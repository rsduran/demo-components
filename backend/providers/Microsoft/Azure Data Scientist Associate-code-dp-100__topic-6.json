[
  {
    "topic": 7,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/12106-exam-dp-100-topic-6-question-1-discussion/",
    "body": "You need to implement a scaling strategy for the local penalty detection data.<br>Which normalization type should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCosine"
    ],
    "answer": "C",
    "answerDescription": "Post batch normalization statistics (PBN) is the Microsoft Cognitive Toolkit (CNTK) version of how to evaluate the population mean and variance of Batch<br>Normalization which could be used in inference Original Paper.<br>In CNTK, custom networks are defined using the BrainScriptNetworkBuilder and described in the CNTK network description language \"BrainScript.\"<br>Scenario:<br>Local penalty detection models must be written by using BrainScript.<br>Reference:<br>https://docs.microsoft.com/en-us/cognitive-toolkit/post-batch-normalization-statistics",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-01-15T21:24:00.000Z",
        "voteCount": 76,
        "content": "Both the question and answer are difficult to follow."
      },
      {
        "date": "2020-12-29T22:45:00.000Z",
        "voteCount": 9,
        "content": "In case study they have also mentioned \n- All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow\n- The images and videos will have varying sizes and formats\n\nSo Batch normalization is usefull to speedup the process where as Cosine normalization is usefull to handle varying sizes and formats of input data."
      },
      {
        "date": "2021-05-27T23:32:00.000Z",
        "voteCount": 2,
        "content": "any reference for this?"
      },
      {
        "date": "2024-01-17T12:33:00.000Z",
        "voteCount": 1,
        "content": "The best normalization type to use in this case is batch normalization. Batch normalization is a technique that reduces the internal covariate shift of the inputs to each layer of a neural network, making the training faster and more stable. Batch normalization also has the benefit of regularizing the model and reducing the need for dropout."
      },
      {
        "date": "2023-06-04T10:50:00.000Z",
        "voteCount": 1,
        "content": "Terminology from Cognitive Toolkit and Synapse Analytics. It seems doesn't relevant for DP-100 test"
      },
      {
        "date": "2022-06-17T03:40:00.000Z",
        "voteCount": 2,
        "content": "DNN normalization?? I really do not expect this kind of questions ...\nThe most common one is batch, and weight is kind of a batch with some improvements ...\nFor other two, I do not know ..."
      },
      {
        "date": "2022-05-01T19:20:00.000Z",
        "voteCount": 2,
        "content": "is this question really for DP-100? it seems more suitable for AI-102."
      },
      {
        "date": "2022-01-26T05:11:00.000Z",
        "voteCount": 2,
        "content": "any easy way to understand this ?"
      },
      {
        "date": "2022-06-17T12:02:00.000Z",
        "voteCount": 1,
        "content": "The images and videos will have varying sizes and formats. Normalization mean put them into the same dimension and same format images / videos before further processing"
      },
      {
        "date": "2021-11-11T23:40:00.000Z",
        "voteCount": 3,
        "content": "\"Local penalty detection models must be written by using BrainScript.\" BrainScript is used in Microsoft Cognitive Toolkit (CNTK) and it's network definition only supports batch normalization.  So C is correct.\n\nhttps://docs.microsoft.com/en-us/cognitive-toolkit/batchnormalization"
      },
      {
        "date": "2021-07-05T05:52:00.000Z",
        "voteCount": 5,
        "content": "Not able to follow question and answer. This question will take atleast 15 mins to read and summarize :-)."
      },
      {
        "date": "2020-12-29T22:42:00.000Z",
        "voteCount": 1,
        "content": "So Batch normalization is usefull to speedup the process where as normalization is use full to handle varying sizes and formats of input data."
      },
      {
        "date": "2020-04-19T10:52:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/103348-exam-dp-100-topic-6-question-1-discussion/",
    "body": "You create an Azure Machine Learning workspace.<br><br>You must configure an event-driven workflow to automatically trigger upon completion of training runs in the workspace. The solution must minimize the administrative effort to configure the trigger.<br><br>You need to configure an Azure service to automatically trigger the workflow.<br><br>Which Azure service should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Grid subscription\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Automation runbook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Hubs Capture",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Hubs consumer"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-31T13:17:00.000Z",
        "voteCount": 1,
        "content": "The Correct answer is: A. Event Grid subscription\n\nJustification:\nEvent Grid subscription: is the most suitable choice for creating event-driven workflows in Azure. When a training run is completed in an Azure ML workspace, Event Grid can be used to trigger a workflow automatically. This approach requires minimal administrative effort, as you can subscribe to specific events (like training run completion) and respond to them without constant polling or manual intervention.\n\nWrong Answers:\n\nB. Azure Automation runbook: is more suitable for scenarios where you need to automate complex, multi-step processes and often require more administrative effort to set up triggers based on specific events in Azure Machine Learning.\n\nC. Event Hubs Capture: is designed to automatically capture the streaming data in Event Hubs and save it to a storage account. \n\nD. Event Hubs consumer: is part of the Event Hubs service and is used to read and process the stream of events. It is not a tool for triggering workflows based on specific events within Azure Machine Learning workspaces. It requires more effort to configure."
      },
      {
        "date": "2023-03-20T01:57:00.000Z",
        "voteCount": 1,
        "content": "To configure an event-driven workflow to automatically trigger upon completion of training runs in the workspace, you should use Azure Event Grid subscription. Azure Machine Learning emits the following event types: Model registered, Model deployed, Run completed, and Dataset drift detected. When an event is triggered, the Event Grid service sends data about that event to subscribing endpoint. You can set up event-driven applications, processes, or CI/CD workflows based on Azure Machine Learning events, such as failure notification emails or ML pipeline runs, when certain conditions are detected by Azure Event Grid"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/120693-exam-dp-100-topic-6-question-2-discussion/",
    "body": "HOTSPOT<br> -<br><br>You plan to implement an Azure Machine Learning solution.<br><br>You have the following requirements:<br><br>\u2022\tRun a Jupyter notebook to interactively train a machine learning model.<br>\u2022\tDeploy assets and workflows for machine learning proof of concept by using scripting rather than custom programming.<br><br>You need to select a development technique for each requirement.<br><br>Which development technique should you use? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-100/image536.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-100/image537.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-06-04T15:22:00.000Z",
        "voteCount": 1,
        "content": "Train models using SDK:https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-model?view=azureml-api-2&amp;tabs=python\n\nusing studio: https://learn.microsoft.com/en-us/azure/machine-learning/how-to-train-with-ui?view=azureml-api-2\n\nI will pick SDK for the first requirement as you need that to submit a job in a Notebook"
      },
      {
        "date": "2023-11-07T15:11:00.000Z",
        "voteCount": 1,
        "content": "bard says the answeres are correct"
      },
      {
        "date": "2023-10-02T13:12:00.000Z",
        "voteCount": 4,
        "content": "On September 4, 2023 exam."
      },
      {
        "date": "2023-09-13T13:30:00.000Z",
        "voteCount": 3,
        "content": "For the requirement to run a Jupyter notebook to interactively train a machine learning model, you can use the Azure Machine Learning Python SDK. This SDK provides an interactive environment for training machine learning models and it integrates well with Jupyter notebooks.\n\nFor deploying assets and workflows for machine learning proof of concept by using scripting rather than custom programming, you can use the Azure CLI. It allows you to manage Azure resources, including Machine Learning assets, using scripts which is ideal for proof of concept deployments."
      },
      {
        "date": "2023-12-10T11:32:00.000Z",
        "voteCount": 2,
        "content": "well question ask to interactively train so AML studio is correct"
      },
      {
        "date": "2023-12-11T09:21:00.000Z",
        "voteCount": 1,
        "content": "the question specifically mentions using a Jupyter notebook, it does imply that you\u2019ll be writing code. In this case, using the Azure Machine Learning Python SDK would be more appropriate."
      },
      {
        "date": "2024-02-25T19:49:00.000Z",
        "voteCount": 1,
        "content": "we can also write code via notebook in azure ml studio. I believe both sdk and ml studio are correct options."
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/36794-exam-dp-100-topic-6-question-2-discussion/",
    "body": "HOTSPOT -<br>You need to use the Python language to build a sampling strategy for the global penalty detection models.<br>How should you complete the code segment? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04274/0032400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04274/0032500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: import pytorch as deeplearninglib<br>Box 2: ..DistributedSampler(Sampler)..<br>DistributedSampler(Sampler):<br>Sampler that restricts data loading to a subset of the dataset.<br>It is especially useful in conjunction with class:`torch.nn.parallel.DistributedDataParallel`. In such case, each process can pass a DistributedSampler instance as a<br>DataLoader sampler, and load a subset of the original dataset that is exclusive to it.<br>Scenario: Sampling must guarantee mutual and collective exclusively between local and global segmentation models that share the same features.<br>Box 3: optimizer = deeplearninglib.train. GradientDescentOptimizer(learning_rate=0.10)<br>Incorrect Answers: ..SGD..<br>Scenario: All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.<br>Box 4: .. nn.parallel.DistributedDataParallel..<br>DistributedSampler(Sampler): The sampler that restricts data loading to a subset of the dataset.<br>It is especially useful in conjunction with :class:`torch.nn.parallel.DistributedDataParallel`.<br>Reference:<br>https://github.com/pytorch/pytorch/blob/master/torch/utils/data/distributed.py",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-28T00:00:00.000Z",
        "voteCount": 12,
        "content": "TF supports static computational graph while pytorch supports  dynamic Computational Graph. So the answer to the first question  is pytorch since we are asked to use dynamic runtime graph computation\nthe 2nd and 4th option are as described in the given solution\nThe 3rd option is confusing, since SGD is offered by pytorch and gradient descent optimizer is offered by tensorflow. I will go with SGD, because it goes with the rest of the answers even though there is this \"All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow\""
      },
      {
        "date": "2020-12-28T05:05:00.000Z",
        "voteCount": 7,
        "content": "Box3: train.GradientDescentOptimizer belongs to TensorFlow, but the other boxes use Pytorch. \nhttps://www.tensorflow.org/api_docs/python/tf/compat/v1/train/GradientDescentOptimizer"
      },
      {
        "date": "2023-07-31T08:22:00.000Z",
        "voteCount": 1,
        "content": "-import pytorch as deeplearninglib\nf-train sampler = deeplearninglib.WeightedRandomSampler.(penalty video dataset)\nh-optimizer = deeplearninglib.optim. SGD(model. parameters).Ir=0,01)\nk-model = deeplearninglib.nn.parallel. DistributedDataParallelCPU(model)\nThese options support the requirements of dynamic runtime graph computation, handling imbalance in the penalty detection classes, applying Stochastic Gradient Descent (SGD) optimizer, and employing parallel computations for the model respectively."
      },
      {
        "date": "2023-02-24T14:52:00.000Z",
        "voteCount": 1,
        "content": "Box 1: A) import pytorch as deeplearninglib\n\nExplanation: Since the feature mentioned, dynamic runtime graph computation, is a feature of PyTorch, we should import PyTorch in this case.\n\nBox 2: C) train_sampler= deeplearninglib.WeightedRnadomSampler.(penalty_video_dataset)\n\nExplanation: A sampling strategy is required for the global penalty detection models. The WeightedRandomSampler allows for weighted sampling, which may be useful for ensuring that rarer samples are not overlooked in the training process.\n\nBox 3: A) optimizer= deeplearninglib.optim.SGD(model.parameters().lr=0.01)\n\nExplanation: The SGD optimizer is mentioned specifically for the penalty detection models, and the learning rate is set to 0.01.\n\nBox 4: A) model= deeplearninglib.parallel.DistributedDataParallel(model)\n\nExplanation: The DistributedDataParallel module allows for parallel processing of a single model across multiple devices or nodes, which can significantly speed up the training process. This is useful for the global penalty detection models, which are mentioned to have slow inference times."
      },
      {
        "date": "2022-06-17T03:52:00.000Z",
        "voteCount": 1,
        "content": "No clue, the only thing I know of is that \nDistributedSampler, Optim.SGD, and nn.Parallel ... are all pytouch packages or classes ..."
      },
      {
        "date": "2022-05-01T19:22:00.000Z",
        "voteCount": 4,
        "content": "is this question really for DP-100?? seems like it is for AI-102"
      },
      {
        "date": "2021-09-27T05:43:00.000Z",
        "voteCount": 5,
        "content": "so hard to answer"
      },
      {
        "date": "2021-09-23T20:18:00.000Z",
        "voteCount": 3,
        "content": "its all messed up ......"
      },
      {
        "date": "2021-07-25T03:43:00.000Z",
        "voteCount": 1,
        "content": "Why Box 4 uses CPU?"
      },
      {
        "date": "2021-06-20T22:02:00.000Z",
        "voteCount": 1,
        "content": "Box 2 is not correct either, it says 'deeplearming' instead of 'deeplearning'..."
      },
      {
        "date": "2021-01-22T05:34:00.000Z",
        "voteCount": 3,
        "content": "I think, box3: optimizer = deeplearninglib.optim.SGD(model.parameters().lr=0,01)\n\nhttps://analyticsindiamag.com/how-ml-frameworks-like-tensorflow-and-pytorch-handle-gradient-descent/"
      },
      {
        "date": "2021-03-07T20:21:00.000Z",
        "voteCount": 1,
        "content": "In this case say \"All penalty detection models show inference phases using a Stochastic Gradient Descent (SGD) are running too slow.\""
      },
      {
        "date": "2021-01-04T08:28:00.000Z",
        "voteCount": 1,
        "content": "Which is the correct one?"
      },
      {
        "date": "2020-12-20T01:03:00.000Z",
        "voteCount": 1,
        "content": "why pytorch not tensorflow? they both support Python"
      },
      {
        "date": "2021-09-07T02:16:00.000Z",
        "voteCount": 3,
        "content": "I might be wrong, but I think the \"to.device()\" code reveals that it must be PyTorch"
      },
      {
        "date": "2020-12-28T03:49:00.000Z",
        "voteCount": 8,
        "content": "we need to use dynamic runtime graph computation thus pytorch"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/114741-exam-dp-100-topic-6-question-3-discussion/",
    "body": "You have an Azure Machine Learning (ML) model deployed to an online endpoint.<br><br>You need to review container logs from the endpoint by using Azure ML Python SDK v2. The logs must include the console log from the inference server, with print/log statements from the model\u2019s scoring script.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect by using SSH to the inference server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance of the MLCIient class.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect by using Docker tools to the inference server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an instance of the OnlineDeploymentOperations class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T07:59:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/machine-learning/how-to-troubleshoot-online-endpoints?view=azureml-api-2&amp;tabs=python#get-container-logs\nhttps://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.operations.onlinedeploymentoperations?view=azure-python\nYou should not instantiate this class directly. Instead, you should create an MLClient instance that instantiates it for you and attaches it as an attribute."
      },
      {
        "date": "2024-06-22T02:28:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-02-06T07:53:00.000Z",
        "voteCount": 2,
        "content": "you create a OnlineDeploymentOperation by referencing 'ml_client.online_deployments'. OnlineDeploymentOperation has then the method 'get_logs(...)' \n\nReferences: \nhttps://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.mlclient?view=azure-python\nhttps://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.operations.onlinedeploymentoperations?view=azure-python#azure-ai-ml-operations-onlinedeploymentoperations-be"
      },
      {
        "date": "2024-01-24T13:01:00.000Z",
        "voteCount": 2,
        "content": "I think the answer should be B because you need an instance of ml_client class to call get_logs on online_deployment instance."
      },
      {
        "date": "2024-01-17T12:24:00.000Z",
        "voteCount": 2,
        "content": "To review the container logs from an online endpoint by using the Azure ML Python SDK v2, you need to do the following steps:\nCreate an instance of the OnlineDeploymentOperations class by passing the workspace object and the name of the online endpoint.\nCall the get_logs method on the OnlineDeploymentOperations object by passing the name of the deployment. This method returns a dictionary with the logs for the deployment, including the console log from the inference server and the print/log statements from the model\u2019s scoring script."
      },
      {
        "date": "2023-11-07T15:14:00.000Z",
        "voteCount": 1,
        "content": "chatgpt and bard say it should be D"
      },
      {
        "date": "2023-07-10T13:22:00.000Z",
        "voteCount": 4,
        "content": "Correct.\nml_client is the instance for MLCLient class, and online_deployment is the instance for either ManagedOnlineDeployment class or KubernetesOnlineDeployment class.\nhttps://github.com/MicrosoftDocs/azure-docs/blob/main/articles/machine-learning/how-to-troubleshoot-online-endpoints.md#get-container-logs"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30866-exam-dp-100-topic-6-question-3-discussion/",
    "body": "DRAG DROP -<br>You need to define an evaluation strategy for the crowd sentiment models.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04274/0032700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04274/0032700002.png\" class=\"in-exam-image\">",
    "answerDescription": "Scenario:<br>Experiments for local crowd sentiment models must combine local penalty detection data.<br>Crowd sentiment models must identify known sounds such as cheers and known catch phrases. Individual crowd sentiment models will detect similar sounds.<br>Note: Evaluate the changed in correlation between model error rate and centroid distance<br>In machine learning, a nearest centroid classifier or nearest prototype classifier is a classification model that assigns to observations the label of the class of training samples whose mean (centroid) is closest to the observation.<br>Reference:<br>https://en.wikipedia.org/wiki/Nearest_centroid_classifier<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/sweep-clustering",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-08T07:17:00.000Z",
        "voteCount": 31,
        "content": "does this question and answer make sense? i dont have any idea at all.  could any one kindly give explain"
      },
      {
        "date": "2020-10-28T08:05:00.000Z",
        "voteCount": 14,
        "content": "The best I could gather was that: they would like to do crowd segmentation which would help them target certain people for their ad campaigns, using clustering based on videos and audios of the people in the crowd. The question wants us to create an evaluation strategy for the models they created. In the problem description, they said they noticed 47 features were not performing rightly and they would engineer 10 independent features from them before retraining our model. This gives us the first answer \"Add new features for retraining...\"."
      },
      {
        "date": "2023-12-20T10:30:00.000Z",
        "voteCount": 1,
        "content": "A - This will be the first one. I think this is part of Cluster-then-classification model. Based on my exp, I will use cluster result as a new feature for later classification model, that's reason we say \"Add new features for retraining supervised models\".\nE makes sense to me as well, but have no idea for C"
      },
      {
        "date": "2023-12-20T10:36:00.000Z",
        "voteCount": 1,
        "content": "It looks like C is kind of Error check. For example, when using KMeans, we need to plot SSE vs. k to determine which k value is better. In this case, this is a classification, but doing similar things. It switches from SSE vs. K to \"Shortest Dis. from Centroid\" vs. \"Model Error Rate\"."
      },
      {
        "date": "2023-08-09T09:56:00.000Z",
        "voteCount": 1,
        "content": "A,C,B could be"
      },
      {
        "date": "2023-07-31T09:10:00.000Z",
        "voteCount": 1,
        "content": "- Filter labeled cases for retraining using the shortest distance from centroids: Start by identifying the labeled cases that are closest to the centroids of their respective clusters. These would typically be the most representative samples of their classes and would form a solid base for initial model training.\nC- Evaluate the changes in correlation between model error rate and centroid distance: After retraining the model with the selected cases, evaluate how the model's error rate correlates with the distance of samples from the centroids. This will provide insights into how well the model is performing and whether samples farther from the centroids are more likely to be misclassified.\nE- Filter labeled cases for retraining using the longest distance from centroids: Based on the evaluation in step 2, it may be observed that samples farther from the centroids are not being accurately classified. To improve the model's performance on these cases, they should be included in the training set for retraining."
      },
      {
        "date": "2023-02-24T23:18:00.000Z",
        "voteCount": 2,
        "content": "The three actions that should be performed in sequence to define an evaluation strategy for the crowd sentiment models are:\n\nC) Evaluate the changes in correlation between model error rate and centroid distance: This step involves evaluating the correlation between the model's error rate and the distance from the centroid. It helps in identifying if the model is overfitting or underfitting the data.\n\nB) Filter labeled cases for retraining using the shortest distance from centroids: This step involves filtering the labeled cases for retraining based on the shortest distance from the centroids. This helps in selecting the cases that are closer to the centroids and are more representative of the cluster.\n\nA) Add new features for retraining supervised models: This step involves adding new features for retraining supervised models. The new features can help improve the performance of the models and capture important information from the data.\nTherefore, the correct order of actions is C, B, A."
      },
      {
        "date": "2023-06-04T11:08:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT3.5?"
      },
      {
        "date": "2022-11-17T00:33:00.000Z",
        "voteCount": 6,
        "content": "Did this appear in any of the previous exams?"
      },
      {
        "date": "2023-02-22T14:37:00.000Z",
        "voteCount": 3,
        "content": "Writing on Friday, will let you know."
      },
      {
        "date": "2023-06-04T11:06:00.000Z",
        "voteCount": 1,
        "content": "What's the news?"
      },
      {
        "date": "2022-06-17T03:21:00.000Z",
        "voteCount": 2,
        "content": "I cannot really follow this case study overall ...\nAfter compare with all options, I think the answer is logically sound ...\nNo other comments ..."
      },
      {
        "date": "2021-09-18T17:13:00.000Z",
        "voteCount": 2,
        "content": "the question is complicated but i say that's a comparisation between existing sound and new sound so the first thing 1) add new features , seconde 2)use correlation to now how much new and old feure are correlated 3)evaluate"
      },
      {
        "date": "2021-09-18T17:18:00.000Z",
        "voteCount": 1,
        "content": "sorry i mean  3)filter based on short distance"
      },
      {
        "date": "2021-05-28T08:45:00.000Z",
        "voteCount": 6,
        "content": "I couldn't make head or tails of this question.  Clueless...."
      },
      {
        "date": "2020-10-19T22:33:00.000Z",
        "voteCount": 4,
        "content": "no idea about this."
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/143021-exam-dp-100-topic-6-question-4-discussion/",
    "body": "You use an Azure Machine Learning workspace.<br><br>You must monitor cost at the endpoint and deployment level.<br><br>You have a trained model that must be deployed as an online endpoint. Users must authenticate by using Microsoft Entra ID.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to Azure Kubernetes Service (AKS). During deployment, set the token_auth_mode parameter of the target configuration object to true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to Azure Kubernetes Service (AKS). During deployment, set the auth_mode parameter to configure the authentication type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to a managed online endpoint. During deployment, set the auth_mode parameter to configure the authentication type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model to a managed online endpoint. During deployment, set the token_auth_mode parameter of the target configuration object to true."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-06-28T06:46:00.000Z",
        "voteCount": 2,
        "content": "C.\n\nAttributes: Diagnostics and Monitoring and Cost \nManaged online endpoints (v2): \n- Local endpoint debugging possible with Docker and Visual Studio Code \n- Advanced metrics and logs analysis with chart/query to compare between deployments \n- Cost breakdown down to deployment level \n-Azure Monitor and Log Analytics powered (includes key metrics and log tables for endpoints and deployments) \n\nACI or AKS(v1): No easy local debugging \n\nhttps://learn.microsoft.com/en-us/azure/machine-learning/concept-endpoints-online?view=azureml-api-2"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53725-exam-dp-100-topic-6-question-4-discussion/",
    "body": "You need to implement a feature engineering strategy for the crowd sentiment local models.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply an analysis of variance (ANOVA).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a Pearson correlation coefficient.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a Spearman correlation coefficient.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a linear discriminant analysis."
    ],
    "answer": "D",
    "answerDescription": "The linear discriminant analysis method works only on continuous variables, not categorical or ordinal variables.<br>Linear discriminant analysis is similar to analysis of variance (ANOVA) in that it works by comparing the means of the variables.<br>Scenario:<br>Data scientists must build notebooks in a local environment using automatic feature engineering and model building in machine learning pipelines.<br>Experiments for local crowd sentiment models must combine local penalty detection data.<br>All shared features for local models are continuous variables.<br>Incorrect Answers:<br>B: The Pearson correlation coefficient, sometimes called Pearson's R test, is a statistical value that measures the linear relationship between two variables. By examining the coefficient values, you can infer something about the strength of the relationship between the two variables, and whether they are positively correlated or negatively correlated.<br>C: Spearman's correlation coefficient is designed for use with non-parametric and non-normally distributed data. Spearman's coefficient is a nonparametric measure of statistical dependence between two variables, and is sometimes denoted by the Greek letter rho. The Spearman's coefficient expresses the degree to which two variables are monotonically related. It is also called Spearman rank correlation, because it can be used with ordinal variables.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/fisher-linear-discriminant-analysis https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/compute-linear-correlation",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-20T10:45:00.000Z",
        "voteCount": 1,
        "content": "A,B,C are methods of Filter Feature Selection, while LDA is a Dimensionality Reduction method and works for categorical target. In this case, I will take D."
      },
      {
        "date": "2023-11-07T15:22:00.000Z",
        "voteCount": 1,
        "content": "chatgpt says D"
      },
      {
        "date": "2023-07-31T09:56:00.000Z",
        "voteCount": 2,
        "content": "In the context of feature engineering for the crowd sentiment local models, which include audio data and need to detect similar sounds, a Pearson correlation coefficient (B) would be a suitable strategy.\n\nThe Pearson correlation coefficient measures the linear relationship between two datasets, which could be valuable in this scenario to understand which features most strongly correlate with positive or negative crowd sentiment. This could involve correlations between specific sound features in the audio data and the sentiment label.\n\nWhile the other techniques mentioned (ANOVA, Spearman correlation coefficient, and linear discriminant analysis) can be useful in certain circumstances, the Pearson correlation coefficient is more relevant in this scenario where you might be dealing with continuous features (like sound frequencies or volumes) and you are interested in linear relationships with the target variable (sentiment)."
      },
      {
        "date": "2022-06-17T03:26:00.000Z",
        "voteCount": 1,
        "content": "MLP combined with LDA ...\nAs mentioned in question, MLP is used for sentiment analysis, multiple layers ...\nThen my guess will be LDA for feature reduction ...\nWhich here is called feature engineering ...\nNo other things are related with feature reduction ..."
      },
      {
        "date": "2021-05-28T08:49:00.000Z",
        "voteCount": 2,
        "content": "these questions seems to be based on Machine Learning Studio (classic). is this still in the syllabus"
      },
      {
        "date": "2021-05-28T08:51:00.000Z",
        "voteCount": 2,
        "content": "This method is often used for dimensionality reduction, because it projects a set of features onto a smaller feature space while preserving the information that discriminates between classes. This not only reduces computational costs for a given classification task, but can help prevent overfitting.\n\nTo generate the scores, you provide a label column and set of numerical feature columns as inputs. The algorithm determines the optimal combination of the input columns that linearly separates each group of data while minimizing the distances within each group. The module returns a dataset containing the compact, transformed features, along with a transformation that you can save and apply to another dataset."
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37583-exam-dp-100-topic-6-question-5-discussion/",
    "body": "DRAG DROP -<br>You need to define a modeling strategy for ad response.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04274/0033000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04274/0033000002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Implement a K-Means Clustering model<br>Step 2: Use the cluster as a feature in a Decision jungle model.<br>Decision jungles are non-parametric models, which can represent non-linear decision boundaries.<br>Step 3: Use the raw score as a feature in a Score Matchbox Recommender model<br>The goal of creating a recommendation system is to recommend one or more \"items\" to \"users\" of the system. Examples of an item could be a movie, restaurant, book, or song. A user could be a person, group of persons, or other entity with item preferences.<br>Scenario:<br>Ad response rated declined.<br>Ad response models must be trained at the beginning of each event and applied during the sporting event.<br>Market segmentation models must optimize for similar ad response history.<br>Ad response models must support non-linear boundaries of features.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/multiclass-decision-jungle https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/score-matchbox-recommender",
    "votes": [],
    "comments": [
      {
        "date": "2023-02-24T23:37:00.000Z",
        "voteCount": 1,
        "content": "The three actions to perform in sequence for defining a modeling strategy for ad response are as follows:\n\nImplement a K-Means clustering model\nUse the cluster as a feature in a decision jungle model\nUse the raw score as a feature in a logistic regression model\nTherefore, the correct order of actions is A, C, and D."
      },
      {
        "date": "2022-06-17T04:03:00.000Z",
        "voteCount": 1,
        "content": "No idea, guess the answer is OK, thought process ...\nK-means --&gt; we do not what will come, so clustering ...\nNeed a score as input for recommendation --&gt; Tree Forest Regression --&gt; Whatever score indicates what action we should take ????"
      },
      {
        "date": "2022-06-18T05:35:00.000Z",
        "voteCount": 1,
        "content": "On a second thought, it might not be the case, this is very confusing ...\nResponse rate is just response vs no response, no it is binary classification ...\nAlso, it mentioned the distribution of features for training and production data are different ...\nSo my bet is :\n1. sweep clustering\n2. Tree forest\n3. Logistic regression"
      },
      {
        "date": "2020-11-22T21:11:00.000Z",
        "voteCount": 1,
        "content": "Is there anybody who can explain this? Thanks"
      },
      {
        "date": "2021-01-20T01:57:00.000Z",
        "voteCount": 26,
        "content": "This question is challenging. The answer looks right.\n\n1. The ad response model needs to support 'non-linear decision boundaries'. This is supported by the decision jungle model. So now we have the decision jungle model as one of the three steps.\n2. The decision jungle model uses K-means clusters as inputs, so K-Means comes before the decision jungle model. K-means clustering can be used to identify 'market/customer segments'.\n3. There remains a third step either before K-means or after decision jungle model. The ad response model needs to be 'applied during the event'. It does seem that a recommender model is the more appropriate choice to apply onto the live dataset and serve recommendations during the event. And the recommender model would logically be at the last step.\n4. So the overall flow seems to be identifying market/customer segments using KMeans -&gt; Non-linear decisions using Decision Jungle (frankly I still wonder how this model fits in the end-to-end flow) -&gt; serve recommendations using the recommender model."
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40935-exam-dp-100-topic-6-question-6-discussion/",
    "body": "DRAG DROP -<br>You need to define an evaluation strategy for the crowd sentiment models.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04274/0033200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04274/0033200002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Define a cross-entropy function activation<br>When using a neural network to perform classification and prediction, it is usually better to use cross-entropy error than classification error, and somewhat better to use cross-entropy error than mean squared error to evaluate the quality of the neural network.<br>Step 2: Add cost functions for each target state.<br>Step 3: Evaluated the distance error metric.<br>Reference:<br>https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-28T16:27:00.000Z",
        "voteCount": 6,
        "content": "Is there anyone could explain why we should use distance error metric not classification error metric (e.g. F-score, etc) as last step? Given this is classification not clustering problem"
      },
      {
        "date": "2021-01-20T02:02:00.000Z",
        "voteCount": 1,
        "content": "The technique we are using here is KMeans clustering to identify market segments/clusters. As we are not solving a supervised classification problem, the classification error metric is not applicable."
      },
      {
        "date": "2023-07-31T10:21:00.000Z",
        "voteCount": 1,
        "content": "Therefore, the sequence might look like this:\n\na- Define a cross-entropy function activation: This function is more suitable for multi-class classification problems. It quantifies the difference between two probability distributions: the predictions made by the model and the actual distribution.\nb- Add cost functions for each target state: By adding cost functions for each class, the model can optimize its predictions for each category.\nc- Evaluate the classification error metric: After the model has been trained, evaluate its performance using a classification error metric. This metric will tell you how often the model's predictions are incorrect.\nSo, the sequence would be A -&gt; B -&gt; C"
      },
      {
        "date": "2023-02-24T23:55:00.000Z",
        "voteCount": 2,
        "content": "C) Evaluate the classification error metric: This is the first step to evaluate the performance of a classification model, such as the crowd sentiment models. The classification error metric measures the proportion of misclassified samples in the dataset and provides a general idea of how well the model is performing.\n\nB) Add cost functions for each target state: Cost functions are used to penalize the model for incorrect predictions and to optimize the model's parameters during training. In the case of the crowd sentiment models, adding cost functions for each target state would enable the model to learn the differences between them and adjust its predictions accordingly.\n\nF) Define a sigmoid loss function activation: The sigmoid loss function is a popular choice for binary classification problems such as sentiment analysis. It returns a value between 0 and 1, which can be interpreted as the probability of a sample belonging to a particular class. Defining this function as the activation function for the output layer of the model would help to improve its performance."
      },
      {
        "date": "2022-06-18T05:24:00.000Z",
        "voteCount": 1,
        "content": "Sentiment analysis is a multi-class classification, so relevant answer for how to evaluate result is 1, 2, 3 that is 100% sure, what is the correct order??? I do not know ..."
      },
      {
        "date": "2021-11-15T11:52:00.000Z",
        "voteCount": 1,
        "content": "Cross entropy is a loss function not an activation function but also, in my opinion, defining a cost function for all target states is not correct (shouldn't the cost function be one?); I think at most we can define a cost function for each component (clustering, classification) and evaluate distance metric and classification metric."
      },
      {
        "date": "2021-09-19T05:21:00.000Z",
        "voteCount": 3,
        "content": "ans: 1,2,3"
      },
      {
        "date": "2021-01-05T18:29:00.000Z",
        "voteCount": 1,
        "content": "sorry 3,2,4 boxes"
      },
      {
        "date": "2021-01-05T18:28:00.000Z",
        "voteCount": 2,
        "content": "3,2,1 boxes"
      },
      {
        "date": "2021-01-05T18:28:00.000Z",
        "voteCount": 1,
        "content": "should be 3rd option."
      },
      {
        "date": "2022-01-28T03:37:00.000Z",
        "voteCount": 9,
        "content": "chutiye kuch aata bhi hai tujhe...sirf confuse kar rha hai logo ko..haiiiinnnn"
      },
      {
        "date": "2023-01-15T06:16:00.000Z",
        "voteCount": 6,
        "content": "exactly what I was thinking"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18924-exam-dp-100-topic-6-question-7-discussion/",
    "body": "You need to implement a model development strategy to determine a user's tendency to respond to an ad.<br>Which technique should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Relative Expression Split module to partition the data based on centroid distance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Relative Expression Split module to partition the data based on distance travelled to the event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Split Rows module to partition the data based on distance travelled to the event.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Split Rows module to partition the data based on centroid distance."
    ],
    "answer": "A",
    "answerDescription": "Split Data partitions the rows of a dataset into two distinct sets.<br>The Relative Expression Split option in the Split Data module of Azure Machine Learning Studio is helpful when you need to divide a dataset into training and testing datasets using a numerical expression.<br>Relative Expression Split: Use this option whenever you want to apply a condition to a number column. The number could be a date/time field, a column containing age or dollar amounts, or even a percentage. For example, you might want to divide your data set depending on the cost of the items, group people by age ranges, or separate data by a calendar date.<br>Scenario:<br>Local market segmentation models will be applied before determining a user's propensity to respond to an advertisement.<br>The distribution of features across training and production data are not consistent<br>Reference:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/split-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-22T07:45:00.000Z",
        "voteCount": 9,
        "content": "Seems the answer is indeed \"Relative Expression Split\""
      },
      {
        "date": "2020-10-17T12:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/split-data-using-regular-expression"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  },
  {
    "topic": 7,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76903-exam-dp-100-topic-6-question-8-discussion/",
    "body": "You need to implement a new cost factor scenario for the ad response models as illustrated in the performance curve exhibit.<br>Which technique should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the threshold to 0.5 and retrain if weighted Kappa deviates +/- 5% from 0.45.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the threshold to 0.05 and retrain if weighted Kappa deviates +/- 5% from 0.5.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the threshold to 0.2 and retrain if weighted Kappa deviates +/- 5% from 0.6.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the threshold to 0.75 and retrain if weighted Kappa deviates +/- 5% from 0.15."
    ],
    "answer": "A",
    "answerDescription": "Scenario:<br>Performance curves of current and proposed cost factor scenarios are shown in the following diagram:<br><img src=\"/assets/media/exam-media/04274/0033400001.png\" class=\"in-exam-image\"><br>The ad propensity model uses a cut threshold is 0.45 and retrains occur if weighted Kappa deviated from 0.1 +/- 5%.",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-15T23:28:00.000Z",
        "voteCount": 2,
        "content": "Makes sense according to the provided explanation."
      },
      {
        "date": "2022-06-18T05:06:00.000Z",
        "voteCount": 1,
        "content": "Kappa = (P0 - Pe) / (1 - Pe)"
      }
    ],
    "examNameCode": "dp-100",
    "topicNumber": "6"
  }
]