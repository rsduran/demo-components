[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79961-exam-pl-300-topic-1-question-1-discussion/",
    "body": "HOTSPOT -<br>You plan to create the Power BI model shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"/assets/media/exam-media/04331/0000600001.jpg\" class=\"in-exam-image\"><br>The data has the following refresh requirements:<br>\u2711 Customer must be refreshed daily.<br>\u2711 Date must be refreshed once every three years.<br>\u2711 Sales must be refreshed in near real time.<br>\u2711 SalesAggregate must be refreshed once per week.<br>You need to select the storage modes for the tables. The solution must meet the following requirements:<br>\u2711 Minimize the load times of visuals.<br>\u2711 Ensure that the data is loaded to the model based on the refresh requirements.<br>Which storage mode should you select for each table? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04331/0000700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0000800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Dual -<br>Customer should use the dual storage mode.<br>Dual: Tables with this setting can act as either cached or not cached, depending on the context of the query that's submitted to the Power BI dataset. In some cases, you fulfill queries from cached data. In other cases, you fulfill queries by executing an on-demand query to the data source.<br>Note: You set the Storage mode property to one of these three values: Import, DirectQuery, and Dual.<br><br>Box 2: Dual -<br>You can set the dimension tables (Customer, Geography, and Date) to Dual to reduce the number of limited relationships in the dataset, and improve performance.<br><br>Box 3: DirectQuery -<br>Sales should use the DirectQuery storage mode.<br>DirectQuery: Tables with this setting aren't cached. Queries that you submit to the Power BI dataset\u05d2\u20ac\"for example, DAX queries\u05d2\u20ac\"and that return data from<br>DirectQuery tables can be fulfilled only by executing on-demand queries to the data source. Queries that you submit to the data source use the query language for that data source, for example, SQL.<br><br>Box 4: Import -<br>Import: Imported tables with this setting are cached. Queries submitted to the Power BI dataset that return data from Import tables can be fulfilled only from cached data.<br>Reference:<br>https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-storage-mode",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-20T07:47:00.000Z",
        "voteCount": 187,
        "content": "Technically Yes, Correct \n\nDual (Composite) Mode:\nThe dual storage mode is between Import and DirectQuery. it is a hybrid approach, Like importing data, the dual storage mode caches the data in the table. However, it leaves it up to Power BI to determine the best way to query the table depending on the query context.\n\n1) Sales Must be Refreshed in Near real time so \"Direct Query\"\n2) Sales Aggregate is once per week so \"Import\" (performance also required)\n3) Both Date and Customer has relationship with both Sales and SalesAggregate tables so \"Dual\"\nbecause to support performance for DirectQuery(Sales) and Import(SalesAggregate)"
      },
      {
        "date": "2023-08-17T02:30:00.000Z",
        "voteCount": 1,
        "content": "thanks for sharing your comment so I would ask if in case the Sales and the Sales Aggregate fact tables are both in near real-time will be a Direct Query not dual? thanks in advance"
      },
      {
        "date": "2023-08-17T02:32:00.000Z",
        "voteCount": 2,
        "content": "I mean for Both Date and Customer dimension tables"
      },
      {
        "date": "2023-02-09T17:41:00.000Z",
        "voteCount": 30,
        "content": "Correct. IF someone still unable to understand I would highly recommend going through this link. Excellent explanation \n"
      },
      {
        "date": "2024-01-18T21:56:00.000Z",
        "voteCount": 1,
        "content": "the link provided is on spot"
      },
      {
        "date": "2024-09-27T08:37:00.000Z",
        "voteCount": 1,
        "content": "If the tables are from the same database in this model. Then there is only one frequency of refresh possible. It cannot be weekly , since it says \"Customer must be refreshed daily.\" \nAlso Import mode generally minimizes the load time of visuals compared to Dual mode. This is because Import mode caches all the data in-memory, allowing for very fast query performance and quick rendering of visuals\n\nCustomer: Import - refresh daily requirement\nDate: Dual - just because Sales is direct query and might be more efficient to have as Dual.\nSales: Direct Query - Real Time by requirements\nSales Aggregate: Imported, is weekly,  but there is no saving in refresh, since the refresh is daily because the Customers table."
      },
      {
        "date": "2024-08-16T00:55:00.000Z",
        "voteCount": 3,
        "content": "I dont agree, sales are to be done near real time so direct query is right. Sales aggregates once a week, then dual is ok as we want performance too. Customer is dual too. while Date, updated once every 3 yrs can be import."
      },
      {
        "date": "2024-08-08T06:31:00.000Z",
        "voteCount": 2,
        "content": "In PBI there are 3 connection modes:\n1 - Import Mode, all the data is loaded into the model once and you need to manually refresh data before get latest updates\n2 -  Direct query mode (or similar live  for Azure Analysis Service or SSAS), in which PBI sends queries to the sources each time a visualization is refreshed or interacted with\n3 - Dual, it is an hybrid mode\n\nSaying that, we need for  sure a direct query for the sales (since it is near real time) then the sales aggregate should be put to Import since there is an once per week update. Finally, as Microsoft suggest, the dimension tables should be set to dual (date and customers in this case)"
      },
      {
        "date": "2024-07-08T04:14:00.000Z",
        "voteCount": 1,
        "content": "Dual Mode"
      },
      {
        "date": "2024-03-17T19:27:00.000Z",
        "voteCount": 3,
        "content": "My interpretation of the answer:\n\nthere are 2 fact tables connected to 2 dim tables. Sales fact table needs DirectQuery.. Sales  Aggregate table needs to weekly refreshes hence Import mode. But both dimension tables have to serve these fact tables and hence they will be in DUAL MODE. Am I correct?"
      },
      {
        "date": "2024-03-10T04:49:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct"
      },
      {
        "date": "2024-03-06T10:43:00.000Z",
        "voteCount": 1,
        "content": "Why does  Sales Aggregate use Import as per the solution? Where as Date uses Dual.\nIn both the cases there is no real time refresh required, why are we going with Import mode for sales aggregate??\n\nBTW, I understand logic behind selecting Dual for Date as it is dimension table and we intend  to reduce number of limited relationships."
      },
      {
        "date": "2024-03-06T10:32:00.000Z",
        "voteCount": 2,
        "content": "Sales need to be real time so a DirectQueary storage mode.\nSales aggregate as is a weekly load you can use Import Mode and save on performance.\n\nAs you have one of each, the other table connected to this source should be Dual. If they where not connected they would be Import mode."
      },
      {
        "date": "2023-11-08T05:38:00.000Z",
        "voteCount": 2,
        "content": "Dual, dual, direct query, import"
      },
      {
        "date": "2023-11-05T23:57:00.000Z",
        "voteCount": 2,
        "content": "Give answer is correct. The tables that are connected to both Direct Query and Import should be set as Dual."
      },
      {
        "date": "2023-10-26T12:28:00.000Z",
        "voteCount": 1,
        "content": "Customer(dual), date(dual), sales(direct query) , salesaggregate(import)"
      },
      {
        "date": "2023-09-04T10:07:00.000Z",
        "voteCount": 2,
        "content": "1. Customer table: Import mode should be used because it needs to be refreshed daily, and importing the data will provide better performance for visuals.\n2. Date table: Import mode should be used because even though it is refreshed once every three years, importing it will not significantly affect performance, and it ensures that visual loads quickly.\n 3. For Sales table: Dual mode should be used for this table because it needs to be refreshed in near real-time.\n4. SalesAggregate Tabe: Import mode should be used because it needs to be refreshed once per week, and importing the data will insure better visual performance."
      },
      {
        "date": "2023-09-04T10:05:00.000Z",
        "voteCount": 1,
        "content": "3. Date table: Import mode should be used because even though it is refreshed once every three years, importing it will not significantly affect performance, and it ensures that visual loads quickly."
      },
      {
        "date": "2023-05-01T04:57:00.000Z",
        "voteCount": 1,
        "content": "Dual\nDual\nDQ\nImport"
      },
      {
        "date": "2023-04-29T11:34:00.000Z",
        "voteCount": 2,
        "content": "This link is helpful in describing when to use which import mode. The requirement of the data (real-time refresh vs. performance) and the relationships in the exhibit are what this question comes down to.\nhttps://learn.microsoft.com/en-us/training/modules/choose-power-bi-model-framework/1-introduction"
      },
      {
        "date": "2023-03-23T07:40:00.000Z",
        "voteCount": 3,
        "content": "Dual\nDual\nDirectQuery\nImport"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79816-exam-pl-300-topic-1-question-2-discussion/",
    "body": "You have a project management app that is fully hosted in Microsoft Teams. The app was developed by using Microsoft Power Apps.<br>You need to create a Power BI report that connects to the project management app.<br>Which connector should you select?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Teams Personal Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataverse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflows"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 39,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-03T07:52:00.000Z",
        "voteCount": 29,
        "content": "You can use the Microsoft Power BI template to import data into Power BI from Project for the web and Project Online. When you're using the template, you're connected to your Microsoft Dataverse instance, where your Microsoft Project web app data is stored. \n\n"
      },
      {
        "date": "2024-03-06T10:44:00.000Z",
        "voteCount": 1,
        "content": "Good anwser"
      },
      {
        "date": "2023-04-11T07:27:00.000Z",
        "voteCount": 1,
        "content": "Amazing stuff! This is exactly what I needed :D\nGreat Q&amp;A material I can test my knowledge on. \n@Abasifreke Thank you for the short summary and providing us with the link."
      },
      {
        "date": "2023-04-29T12:52:00.000Z",
        "voteCount": 8,
        "content": "Microsoft Dataverse for Teams is a built-in, low-code data platform for Microsoft Teams that lets users build custom apps, bots, and flows in Microsoft Teams by using Power Apps, Power Virtual Agents, and Power Automate. Dataverse for Teams\u2014built on Microsoft Dataverse\u2014provides relational data storage, rich data types, enterprise-grade governance, and one-click solution deployment to the Microsoft Teams app store.\n"
      },
      {
        "date": "2024-08-08T06:36:00.000Z",
        "voteCount": 1,
        "content": "Dataverse is the correct one"
      },
      {
        "date": "2024-07-10T07:08:00.000Z",
        "voteCount": 1,
        "content": "c option"
      },
      {
        "date": "2024-07-08T04:15:00.000Z",
        "voteCount": 1,
        "content": "Dataverse"
      },
      {
        "date": "2024-03-06T10:44:00.000Z",
        "voteCount": 4,
        "content": "Dataflows is the configuration of the maniuplation of data. Microsoft Dataverse for Teams is a built-in, low-code data platform for Microsoft Teams that lets users build custom apps, bots, and flows in Microsoft Teams by using Power Apps.I f your project management app is built using Power Apps and stores its data in Dataverse."
      },
      {
        "date": "2024-02-22T23:32:00.000Z",
        "voteCount": 2,
        "content": "When using Microsoft Power BI template , you're connected to your Microsoft Dataverse instance"
      },
      {
        "date": "2023-12-12T12:35:00.000Z",
        "voteCount": 3,
        "content": "Dataverse"
      },
      {
        "date": "2023-11-05T23:57:00.000Z",
        "voteCount": 2,
        "content": "Dataverse."
      },
      {
        "date": "2023-10-26T12:27:00.000Z",
        "voteCount": 1,
        "content": "Dataverse"
      },
      {
        "date": "2023-09-04T10:19:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is C (Dataverse) because it is a database platform often used in conjunction with power apps. In other words, if your project management app is built using Power Apps and stores its data in Dataverse."
      },
      {
        "date": "2023-08-29T00:28:00.000Z",
        "voteCount": 1,
        "content": "C. Dataverse"
      },
      {
        "date": "2023-07-28T09:10:00.000Z",
        "voteCount": 3,
        "content": "Dataverse"
      },
      {
        "date": "2023-06-19T09:06:00.000Z",
        "voteCount": 1,
        "content": "c only"
      },
      {
        "date": "2023-05-01T05:02:00.000Z",
        "voteCount": 4,
        "content": "Dataverse"
      },
      {
        "date": "2023-04-01T00:00:00.000Z",
        "voteCount": 2,
        "content": "Dataverse"
      },
      {
        "date": "2023-03-23T09:15:00.000Z",
        "voteCount": 2,
        "content": "The answer is C"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80518-exam-pl-300-topic-1-question-3-discussion/",
    "body": "For the sales department at your company, you publish a Power BI report that imports data from a Microsoft Excel file located in a Microsoft SharePoint folder.<br>The data model contains several measures.<br>You need to create a Power BI report from the existing data. The solution must minimize development effort.<br>Which type of data source should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI dataset\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta SharePoint folder",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI dataflows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Excel workbook"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 94,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 27,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T07:49:00.000Z",
        "voteCount": 125,
        "content": "It should be dataset, because the case states there is already a report published and the datamodel contains measures. therefore and to be able to use the measures in the datamodel you should connect to the existing dataset (which was created when you plublished the report) instead of starting from scratch with the files in the SharePoint folder."
      },
      {
        "date": "2022-11-10T08:15:00.000Z",
        "voteCount": 4,
        "content": "After reading the question multiple times, the biggest takeaway is that its asking directly for data. A SharePoint folder HOLDS data, but it is not data itself. I agree with this and think its the existing dataset"
      },
      {
        "date": "2022-11-28T08:55:00.000Z",
        "voteCount": 9,
        "content": "The question is confusing because it doesn't tell clearly that there are two reports. So the second report can reuse the dataset of the first one."
      },
      {
        "date": "2023-01-10T23:12:00.000Z",
        "voteCount": 1,
        "content": "Yea, I think so."
      },
      {
        "date": "2023-07-13T18:00:00.000Z",
        "voteCount": 8,
        "content": "I think the question is clear:\nPublish the first report, and then create the second report from dataset of the first report"
      },
      {
        "date": "2024-03-06T10:48:00.000Z",
        "voteCount": 1,
        "content": "The question also implies that there is a report already created, so using the PBI dataset will include the calculated metrics"
      },
      {
        "date": "2023-03-14T02:58:00.000Z",
        "voteCount": 1,
        "content": "can you provide documentation/reference links on this please?"
      },
      {
        "date": "2024-10-04T12:10:00.000Z",
        "voteCount": 1,
        "content": "You can check the semantic model, they changed the name from dataset to semantic model."
      },
      {
        "date": "2023-10-12T07:52:00.000Z",
        "voteCount": 6,
        "content": "The answer is Sharepoint folder. The data model was for the previously published PowerBi report. Creating a new report from existing data refers to all Sales data on the sharepoint folder."
      },
      {
        "date": "2022-09-16T06:04:00.000Z",
        "voteCount": 24,
        "content": "reuse the existing dataset."
      },
      {
        "date": "2024-08-14T01:36:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A-Power BI dataset (now has benn renamed as Power BI semantic models)"
      },
      {
        "date": "2024-08-01T16:42:00.000Z",
        "voteCount": 1,
        "content": "Dataset has been renamed to semantic model, so PBI dataset should be equivalent to PBI Semantic model.\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-rename\nBack to the question, for the purpose of minimizing development effort, by using an existing semantic model may save time from clear, transform and load the data source."
      },
      {
        "date": "2024-07-18T01:46:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-07-08T04:16:00.000Z",
        "voteCount": 1,
        "content": "a sharepoint folder"
      },
      {
        "date": "2024-06-26T06:05:00.000Z",
        "voteCount": 2,
        "content": "The first two sentences were meant to confuse you. The actual question is which \"data source\""
      },
      {
        "date": "2024-06-20T01:14:00.000Z",
        "voteCount": 1,
        "content": "I agree. the correct answer should be Dataset, as it refers to create a new PBI based on data (and a dataset) already built"
      },
      {
        "date": "2024-06-10T05:39:00.000Z",
        "voteCount": 2,
        "content": "The correct answer should be Dataset. A connection with the Share folder was initiated for the first report. After publishing the report a Power BI dataset was created and by connecting with it we minimize the development."
      },
      {
        "date": "2024-05-24T21:26:00.000Z",
        "voteCount": 1,
        "content": "Agree with Nomios!\nAnswer I am going with: Existing Dataset bc the qs clearly states that the report is already published and the data model of that published report contains measures. Therefore, to be able to use those measures in the already published dataset, use: Existing dataset)"
      },
      {
        "date": "2024-05-02T01:44:00.000Z",
        "voteCount": 1,
        "content": "The solution must minimize development effort."
      },
      {
        "date": "2024-04-26T17:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. Even though there was a published page before, they are trying to create a NEW report which you have to use the sharepoint folder url to get access to the files. lol"
      },
      {
        "date": "2024-04-10T06:46:00.000Z",
        "voteCount": 2,
        "content": "Hi , im preparing for the 'Microsoft Certified: Power BI Data Analyst Associate' cirt.  Ive completed all modules that were on the Syllabus twice and there was no mention of the dataverse. I can see its Power app module thats not on the syllabus...does anyone have a comprehensive of all modules required?"
      },
      {
        "date": "2024-04-01T02:12:00.000Z",
        "voteCount": 5,
        "content": "The question is: \"Which type of data source should you use?\"\nThe Power BI dataset is NOT a data source. \nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-data-sources\nThus, since it should have minimal developing impact, it should be answer B \"Sharepoint folder\"."
      },
      {
        "date": "2024-04-23T05:43:00.000Z",
        "voteCount": 4,
        "content": "Power BI dataset is litteraly the 2nd option of your link. Answer is A"
      },
      {
        "date": "2024-03-23T21:14:00.000Z",
        "voteCount": 1,
        "content": "Sharepoint"
      },
      {
        "date": "2024-03-06T10:46:00.000Z",
        "voteCount": 1,
        "content": "Dataset included calculated tables, while a data flow is a collection of tables."
      },
      {
        "date": "2024-02-29T05:59:00.000Z",
        "voteCount": 1,
        "content": "A is the answer:\nI am not convinced with B, the published dataset is already connected to the existing data, and to minimize effort of modeling and creating all the measures you will just reuse everything"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80907-exam-pl-300-topic-1-question-4-discussion/",
    "body": "You import two Microsoft Excel tables named Customer and Address into Power Query. Customer contains the following columns:<br>\u2711 Customer ID<br>\u2711 Customer Name<br>\u2711 Phone<br>\u2711 Email Address<br>\u2711 Address ID<br>Address contains the following columns:<br>\u2711 Address ID<br>\u2711 Address Line 1<br>\u2711 Address Line 2<br>\u2711 City<br>\u2711 State/Region<br>\u2711 Country<br>\u2711 Postal Code<br>Each Customer ID represents a unique customer in the Customer table. Each Address ID represents a unique address in the Address table.<br>You need to create a query that has one row per customer. Each row must contain City, State/Region, and Country for each customer.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge the Customer and Address tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup the Customer and Address tables by the Address ID column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTranspose the Customer and Address tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend the Customer and Address tables."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-04T09:52:00.000Z",
        "voteCount": 52,
        "content": "Remember Merge is JOIN, APPEND is UNION"
      },
      {
        "date": "2024-09-15T14:54:00.000Z",
        "voteCount": 1,
        "content": "Merge is correct."
      },
      {
        "date": "2024-08-08T06:39:00.000Z",
        "voteCount": 3,
        "content": "The right answer is A. Merge the Customer and Address tables.\nBecause we need to merge (join) the customer table with the address table via the addressId"
      },
      {
        "date": "2024-07-18T01:51:00.000Z",
        "voteCount": 1,
        "content": "Merge is correct it combine two table &amp; act as join"
      },
      {
        "date": "2024-07-08T04:16:00.000Z",
        "voteCount": 1,
        "content": "Merge the Customer and Address tables."
      },
      {
        "date": "2024-05-02T01:47:00.000Z",
        "voteCount": 1,
        "content": "Merge will add the additional columns needed."
      },
      {
        "date": "2024-01-25T05:42:00.000Z",
        "voteCount": 1,
        "content": "Merge the tables"
      },
      {
        "date": "2024-01-24T07:39:00.000Z",
        "voteCount": 2,
        "content": "A. Merge - you essentially want to join the columns together, which is what Merge does."
      },
      {
        "date": "2024-01-20T05:58:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer."
      },
      {
        "date": "2023-12-12T12:38:00.000Z",
        "voteCount": 1,
        "content": "joining tables is called \"MERGE\"."
      },
      {
        "date": "2023-11-05T23:55:00.000Z",
        "voteCount": 4,
        "content": "Merge is the right choice, as you are \"joining\" both tables based on Address Id."
      },
      {
        "date": "2023-09-14T09:50:00.000Z",
        "voteCount": 2,
        "content": "The answer is A, because in order to query the column we need to join the two different table based on their common column."
      },
      {
        "date": "2023-09-04T10:36:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A (Merge the Customer and Address tables), it is because merging in power query is used to combine data from multiple tables or queries into a single query based on one or more common columns. Merging is typically used to bring related data together, enrich or denormalize data, or create new composite datasets."
      },
      {
        "date": "2023-08-29T10:09:00.000Z",
        "voteCount": 2,
        "content": "Merge = JOIN tables"
      },
      {
        "date": "2023-08-17T03:17:00.000Z",
        "voteCount": 2,
        "content": "merging in power bi is basically joining of 2 tables."
      },
      {
        "date": "2023-06-19T20:21:00.000Z",
        "voteCount": 1,
        "content": "merging in power bi is basically joining of 2 tables. \n\nIn this case, Customer [LEFT JOIN] ADDRESSES"
      },
      {
        "date": "2023-05-22T00:57:00.000Z",
        "voteCount": 1,
        "content": "We should use Merge."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/83068-exam-pl-300-topic-1-question-5-discussion/",
    "body": "HOTSPOT -<br>You have two Azure SQL databases that contain the same tables and columns.<br>For each database, you create a query that retrieves data from a table named Customer.<br>You need to combine the Customer tables into a single table. The solution must minimize the size of the data model and support scheduled refresh in powerbi.com.<br>What should you do? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04331/0001400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0001400002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Append Queries as New -<br>When you have additional rows of data that you'd like to add to an existing query, you append the query.<br>There are two append options:<br>* Append queries as new displays the Append dialog box to create a new query by appending multiple tables.<br>* Append queries displays the Append dialog box to add additional tables to the current query.<br>Incorrect: When you have one or more columns that you'd like to add to another query, you merge the queries.<br>Box 2: Disable loading the query to the data model<br>By default, all queries from Query Editor will be loaded into the memory of Power BI Model.  You can disable the load for some queries, especially queries that used as intermediate transformation to produce the final query for the model.<br>Disabling Load doesn't mean the query won't be refreshed, it only means the query won't be loaded into the memory. When you click on Refresh model in Power<br>BI, or when a scheduled refresh happens even queries marked as Disable Load will be refreshed, but their data will be used as intermediate source for other queries instead of loading directly into the model. This is a very basic performance tuning tip, but very important when your Power BI model grows bigger and bigger.<br>Reference:<br>https://docs.microsoft.com/en-us/power-query/append-queries<br>https://radacad.com/performance-tip-for-power-bi-enable-load-sucks-memory-up",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-21T05:26:00.000Z",
        "voteCount": 60,
        "content": "Correct\n- Append Queries as New\n- Disable loading the query to the data model"
      },
      {
        "date": "2023-07-02T09:01:00.000Z",
        "voteCount": 8,
        "content": "When you append queries as new, you have 2 queries to disable, but answer says \"Disable loading query\" not \"Disable loading queries\", but when you append queries, you have 1 query to disable"
      },
      {
        "date": "2023-10-14T19:24:00.000Z",
        "voteCount": 9,
        "content": "The second option says \"Action to perform on original 2 database queries\""
      },
      {
        "date": "2023-10-24T14:20:00.000Z",
        "voteCount": 4,
        "content": "Amazing catch."
      },
      {
        "date": "2023-10-03T20:34:00.000Z",
        "voteCount": 1,
        "content": "I strongly agree with you, the answer must be A for the first box."
      },
      {
        "date": "2023-12-30T13:44:00.000Z",
        "voteCount": 1,
        "content": "I understand, we cannot select all the queries at once to disable the load, that is why its \"query\" instead of \"queries\"."
      },
      {
        "date": "2022-10-05T13:24:00.000Z",
        "voteCount": 19,
        "content": "Answer is correct. However just Append is also valid. Its just that due to the two part answer box's given and needing an answer, then it means the first box must be using Append (as new)\n"
      },
      {
        "date": "2024-09-09T17:33:00.000Z",
        "voteCount": 1,
        "content": "Also agree!"
      },
      {
        "date": "2022-10-11T11:35:00.000Z",
        "voteCount": 1,
        "content": "Agreed."
      },
      {
        "date": "2024-09-18T03:04:00.000Z",
        "voteCount": 1,
        "content": "I have a different view point, what if the two table has the same row data from 2 different table database users, the combine option \"append query\" may lead to duplication of records ! The Question statement does not state anything about Field as Unique CUST_ID or other,  User may need to use Trim function for (ID Field Name), the merge table option as New table may avoid duplicates and can generate clean table ! with minimum size"
      },
      {
        "date": "2024-08-14T06:04:00.000Z",
        "voteCount": 2,
        "content": "Since the second is:\n- Disable loading the query to the data model\nthen we need to append (same structure) as new\n- Append Queries as New"
      },
      {
        "date": "2024-06-17T07:59:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-05-10T12:12:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is right because  if you use \"Append Queries\" you will have issues with the second option \"Disable loading into the model\", because this action must be performed in both table. That is why I think."
      },
      {
        "date": "2024-04-01T05:51:00.000Z",
        "voteCount": 3,
        "content": "Whenever you read that the tables have same structure use append. \nAppend as new or not depends on if you want to keep the original table or not. Generally you don't want to keep it in the data model. This will not increase the data model size so you should not load it in the data model."
      },
      {
        "date": "2024-03-12T10:57:00.000Z",
        "voteCount": 1,
        "content": "Interesting"
      },
      {
        "date": "2024-03-06T10:53:00.000Z",
        "voteCount": 1,
        "content": "Append is also Correct. Its just that due to the two part answer box's given and needing an answer, then it means the first box must be using Append"
      },
      {
        "date": "2024-05-10T12:12:00.000Z",
        "voteCount": 1,
        "content": "But if you use \"Append Queries\" you will have issues with the second option \"Disable loading into the model\", because this action must be performed in both table. That is why I think."
      },
      {
        "date": "2024-02-15T01:11:00.000Z",
        "voteCount": 4,
        "content": "Taking into account the fact that it is strictly about 2 tables to append that have exactly the same structure (number of columns, their names) why would you do \"as new\"? Why do \"disable loading to the data model\" for 2 tables, when you can only do it for one? We also notice \"query\" in the answers.\n\nAnd in the documentation, the example for \"Append queries as new\" is used when more than 3 tables are involved in the query and which do not have the same structure (number of columns, names).\n\nI think the correct answer is : \n- Append Queries\n- Disable loading the query to the data model"
      },
      {
        "date": "2024-05-10T12:12:00.000Z",
        "voteCount": 1,
        "content": "But if you use \"Append Queries\" you will have issues with the second option \"Disable loading into the model\", because this action must be performed in both table. That is why I think."
      },
      {
        "date": "2024-01-28T20:46:00.000Z",
        "voteCount": 1,
        "content": "what is the need to retain the original two queries after we have created a new query by appending them ? Wouldn't append as new create a standalone query ?"
      },
      {
        "date": "2023-11-05T23:59:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct.\n\nYou must Append as New and then disable load from the original tables."
      },
      {
        "date": "2023-09-11T16:42:00.000Z",
        "voteCount": 5,
        "content": "By default, all queries from Query Editor are loaded into the memory of the Power BI model. However, you have the option to disable loading for specific queries, particularly those used as intermediate transformations to generate the final query for the model.\nDisabling the load does not mean the query won't be refreshed; it simply prevents the query from being loaded into memory. Even if a query is marked as \"Disable Load,\" it will still be refreshed when you manually click \"Refresh model\" in Power BI or during a scheduled refresh. However, the data from these queries will be utilized as intermediate sources for other queries, instead of being loaded directly into the model. This performance tuning tip is particularly valuable as your Power BI model grows larger."
      },
      {
        "date": "2023-09-10T16:23:00.000Z",
        "voteCount": 3,
        "content": "There are different ways to come to the same conclusion BUT the answer is correct in this case based on the need to disable the TWO ORIGINAL QUERIES. The only way that you can ended up with three queries (two originals and one new, thus need to disable two original queries) was if the APPEND AS NEW was selected in the first step."
      },
      {
        "date": "2023-09-04T10:50:00.000Z",
        "voteCount": 2,
        "content": "The correct Answers are:\n* Append Queries: because it is used to combine the Customer table into as single table. This will stack the data from both tables on top of each other, creating a unified Customer Table.\n* Disable loading the query to the query to the data model: since we want to minimize the size of the data model and avoid duplicating the data from the SQL databases, we should prevent these queries from being loaded into the Power BI data model."
      },
      {
        "date": "2023-08-22T03:25:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct because when you use \"Append query\" the original queries will no longer be there. but it is only when you use \"append query as new\" that you will need to perform the next question that has to do with disabling the loading of queries"
      },
      {
        "date": "2023-07-02T08:45:00.000Z",
        "voteCount": 3,
        "content": "Appending 2 Queries (Query-B to Query-A) , and Disable Loading Query-B is same size with \nAppending 2 Queries as New (Query-C) and Disable Loading Queries-A and Query-B.\nBecause Answer says \"Disable loading the query\" , not \"Queries\", It is :\nAppend Queries and Disable Loading the Query"
      },
      {
        "date": "2023-07-06T07:56:00.000Z",
        "voteCount": 1,
        "content": "I finally understood this, thanks sm for the explanation!"
      },
      {
        "date": "2023-07-08T12:04:00.000Z",
        "voteCount": 6,
        "content": "Actually I re-read it and it says \n\"Action to perform on the original *TWO* SQL database queries\" \nso it makes sense to append AS NEW."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81881-exam-pl-300-topic-1-question-6-discussion/",
    "body": "DRAG DROP -<br>In Power Query Editor, you have three queries named ProductCategory, ProductSubCategory, and Product.<br>Every Product has a ProductSubCategory.<br>Not every ProductsubCategory has a parent ProductCategory.<br>You need to merge the three queries into a single query. The solution must ensure the best performance in Power Query.<br>How should you merge the tables? To answer, drag the appropriate merge types to the correct queries. Each merge type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04331/0001600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0001600002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Inner -<br>Every Product has a ProductSubCategory.<br>A standard join is needed.<br>One of the join kinds available in the Merge dialog box in Power Query is an inner join, which brings in only matching rows from both the left and right tables.<br><br>Box 2: Left outer -<br>Not every ProductsubCategory has a parent ProductCategory.<br>One of the join kinds available in the Merge dialog box in Power Query is a left outer join, which keeps all the rows from the left table and brings in any matching rows from the right table.<br>Reference:<br>https://docs.microsoft.com/en-us/power-query/merge-queries-inner https://docs.microsoft.com/en-us/power-query/merge-queries-left-outer",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-12T13:51:00.000Z",
        "voteCount": 40,
        "content": "Answer is correct"
      },
      {
        "date": "2022-10-04T00:25:00.000Z",
        "voteCount": 31,
        "content": "Answer is correct: 1. Inner join, 2. Left outer join\nIf each row in table A has a matching row in table B, always use inner join because it has the best performance."
      },
      {
        "date": "2022-10-08T11:03:00.000Z",
        "voteCount": 7,
        "content": "Question 1. You in all likelihood have to say 10 products each with a parent category, But your \n subcategories are eg 3 because product 1, 2 and 3 are subcategory socks, product 4, 5 and 6 are subcategory shoes and 7, 8 and 9 are shirts. Sure every Product has a SubCategory but they aren't duplicates. I think the answer to Question 1 is Left Outer.  Question 2 is also Left outer"
      },
      {
        "date": "2022-10-20T04:14:00.000Z",
        "voteCount": 3,
        "content": "When you join tables (inner join) you'll get all rows from T1 and all rows from T2 that meet the join and where conditions. It is not relevant if the cardinality is 1 or many on one or both sides.\nIn your example the result would be:\nproduct 1 - socks\nproduct 2 - socks\nproduct 3 - socks\nproduct 4 - shoes\nproduct 5 - shoes\n... and so on"
      },
      {
        "date": "2024-08-23T05:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct: \n1. Inner Join - Because both table have matching rows\n2. Left Outer - We can get all the matching and unmatching records from left table"
      },
      {
        "date": "2024-08-14T06:09:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct: we need first an INNER JOIN between product and productSubcategory (in this way we will optimize since every product has a  product subcategory) and we also need a left outer join between subcategory and category (left because not every product subcategory has a product category)"
      },
      {
        "date": "2024-01-22T08:42:00.000Z",
        "voteCount": 1,
        "content": "My biggest challenge with this specific question is paying attention the the clearly labeled headers vs. ignoring the order in which the tables were stated within the question itself."
      },
      {
        "date": "2023-12-14T08:27:00.000Z",
        "voteCount": 3,
        "content": "How are you supposed to know that you shouldnt conserve all table information?"
      },
      {
        "date": "2023-11-19T23:36:00.000Z",
        "voteCount": 1,
        "content": "How you get product attribute from D option? is there right information you have in 2nd table regarding product?"
      },
      {
        "date": "2023-11-06T01:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct.\n1. Inner Join\n2. Left Outer Join"
      },
      {
        "date": "2023-09-22T00:20:00.000Z",
        "voteCount": 7,
        "content": "A. Inner Join because we have only one row in Product for each ProductSubCategory.\nB. Left Join because we do not need rows from ProductCategory which do not match any ProductSubCategory"
      },
      {
        "date": "2023-09-11T18:00:00.000Z",
        "voteCount": 5,
        "content": "1: either Left outer or inner (same results)\n2: left outer \nTo fully understand this question, there are a few points:\n(1) Which to keep and which can be ignored. At the end of the day, we need to analyze the sales so all products need to be kept but unused subcategory and category can be ignored.\n(2) best performance : according to point1, we keep all products and ignore unused subcategories and categories. If it doesn't mention \"best performance\" and we want to keep all the subcategories and categories( which might be the case in real world cases),we can do full outer join on 3 tables. \n(3) left outer vs inner in question 1:\nAs \"Every Product has a ProductSubCategory\", Let's say if there are subcategory 1,2,3 in product query and 1,2,3,4 in ProductSubCategory table, either left outer or inner join results in  1,2,3.\n(4)  Let's say we have SubCategory 1,2 and we have  Categories Sport,Drinks. SubCategory 1 is under Category Sport but Subcategory 2 is not under any Category. As we need to keep SubCategory 1,2, we need to do a Left outer join. If we keep Category Drinks then it defeats the purpose of \"Best performance\"."
      },
      {
        "date": "2023-09-04T11:04:00.000Z",
        "voteCount": 4,
        "content": "* The correct answer for the first option is Inner Join: it is because we want to keep only the rows where there's matching ProductSubCategory for each Product.\n* The correct answer for the second option is Left outer join: it is because not every ProductSubCategory has a parent ProductCategor, so we want to keep all ProductSubCategories while matching them with any available ProductCategories."
      },
      {
        "date": "2023-08-26T10:53:00.000Z",
        "voteCount": 2,
        "content": "If I am building something from the client why would I take risk of missing out data from ProdSubCategory table in the Event say a new item in any table not present in other comes in? Answer is not practical for \"Inner Join\""
      },
      {
        "date": "2023-08-26T10:50:00.000Z",
        "voteCount": 3,
        "content": "Please can somebody share source where it states Inner join better than Left join?"
      },
      {
        "date": "2023-07-02T11:17:00.000Z",
        "voteCount": 3,
        "content": "When Product and ProductSubCategory are merged, it will be still named as Product table.\nSo second line should show Product, instead of ProductSubCategory. \nSo confusing."
      },
      {
        "date": "2023-03-23T07:45:00.000Z",
        "voteCount": 1,
        "content": "1. Inner\n2. Left Outer"
      },
      {
        "date": "2023-03-20T01:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2023-02-28T18:21:00.000Z",
        "voteCount": 4,
        "content": "Answer: \n- Inner join is the best option if you only want the matching rows from both tables.\n- Left outer join is useful if you want all the records from one of the tables. In this scenario, we want all the records from the \"product subcategory\" table, so we should use a left outer join."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81994-exam-pl-300-topic-1-question-7-discussion/",
    "body": "You are building a Power BI report that uses data from an Azure SQL database named erp1.<br>You import the following tables.<br><img src=\"/assets/media/exam-media/04331/0001700001.jpg\" class=\"in-exam-image\"><br>You need to perform the following analyses:<br>\u2711 Orders sold over time that include a measure of the total order value<br>Orders by attributes of products sold<br><img src=\"/assets/media/exam-media/04331/0001700003.png\" class=\"in-exam-image\"><br>The solution must minimize update times when interacting with visuals in the report.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Power Query, merge the Order Line Items query and the Products query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a calculated column that adds a list of product categories to the Orders table by using a DAX function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCalculate the count of orders per product by using a DAX function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Power Query, merge the Orders query and the Order Line Items query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 145,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 51,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-05T15:13:00.000Z",
        "voteCount": 88,
        "content": "I'm very sure it's D. It's the Header/Detail Schema, and the most optimal way is to flatten the header into the detail table.\n"
      },
      {
        "date": "2023-09-26T23:43:00.000Z",
        "voteCount": 8,
        "content": "Totally agree, in Star Schema, we should only have one FACT table of ONE object (here is order). So, in this example, we should combine Order and Order Detail into one FACT table."
      },
      {
        "date": "2022-10-08T11:12:00.000Z",
        "voteCount": 8,
        "content": "D. doesn't have a common field. The answer has to be A"
      },
      {
        "date": "2022-10-11T11:43:00.000Z",
        "voteCount": 10,
        "content": "I agree that it's not clearly stated in the question that Order and Order Line tables have common field (for example: order ID)\nIf there is no common fields, there is no way to implement the requirements (calculating order value from Order line)."
      },
      {
        "date": "2023-08-06T14:01:00.000Z",
        "voteCount": 8,
        "content": "There is no way D doesnt have a common field. There wouldn't be an order line if there was no Order ID in it. so just because it is not stated doesnt mean it dont exist."
      },
      {
        "date": "2023-08-12T00:59:00.000Z",
        "voteCount": 3,
        "content": "I was first going with A but from the explanations I got from his source and re-reading the question between the lines, it is obvious that the answer is D."
      },
      {
        "date": "2024-02-26T02:30:00.000Z",
        "voteCount": 2,
        "content": "My option here was A. Nothing in the question makes me think I need any High level Info on Orders (i.e. Orders table). In the 'Order Line Item' I have everything I need to calculate 'Total Order Value' (i.e. Quantity * Price) plus the Product ID, which I'd use to get the Product Attributes required.\nIn my opinion and based on the requirements, I don't think we even need/should to load 'Orders' table, as this would not be a requirement for the task in hand, thus we would be loading extra info that in not needed."
      },
      {
        "date": "2022-09-13T06:10:00.000Z",
        "voteCount": 39,
        "content": "Should be A, because we need to get \" Orders sold over time that include a measure of the total order value Orders by attributes of products sold\"\n\nOrder line detail for quantities ordered, and product for product's attribute"
      },
      {
        "date": "2022-11-28T22:50:00.000Z",
        "voteCount": 16,
        "content": "I think you're forgetting about the \"over time\" part of the objective. You cannot show a distribution of sales over time without having a date column which does not seem to be  present in Products or Order Line Items."
      },
      {
        "date": "2022-12-02T10:31:00.000Z",
        "voteCount": 4,
        "content": "Exactly"
      },
      {
        "date": "2023-07-04T05:16:00.000Z",
        "voteCount": 1,
        "content": "The date column is also not specified in the Orders, so this argument doesn't make sense."
      },
      {
        "date": "2023-07-23T05:41:00.000Z",
        "voteCount": 7,
        "content": "What do you mean? High-level info about orders includes date of the order."
      },
      {
        "date": "2022-12-05T18:31:00.000Z",
        "voteCount": 19,
        "content": "Price is also an attribute to the product, which is present in Order line detail. The key word here is a product sold. The sold items are present only in the Order line detail. So A is INCORRECT"
      },
      {
        "date": "2023-05-01T05:32:00.000Z",
        "voteCount": 6,
        "content": "it is not good idea to merge dim table with fact table!"
      },
      {
        "date": "2023-07-04T05:18:00.000Z",
        "voteCount": 1,
        "content": "It is not a good idea, but in this case it is the only way to aggregate the order value per product attributes."
      },
      {
        "date": "2024-10-03T01:58:00.000Z",
        "voteCount": 2,
        "content": "Hello everyone, to make it clear we must to know:\n--- PBI will do the best aggregation base on Star Schema model, we now have 1 Fact table (Order Line Items) and 2 Dim tables (Products, Orders). Orders has common field with Products (ProductID), and pretty sure time series field (OrderDate); Orders Line Items has Price and Quanity.\n--- We need summarize some values like \"price\" and \"quantity\" over-time by attributes product. But we only have common field in Dim table (Orders) so we need to merge Dim (Orders) and Fact (Order Line Items) to new single Fact table to design the right Star Schema model.\n\n=&gt; So that D is correct"
      },
      {
        "date": "2024-09-23T06:19:00.000Z",
        "voteCount": 1,
        "content": "Might be option A. How about the orders by attribute without accessing the Product datasource ?"
      },
      {
        "date": "2024-08-14T06:14:00.000Z",
        "voteCount": 1,
        "content": "We need first, in such way to join orders and order line query. So the right answer is:\nD. From Power Query, merge the Orders query and the Order Line Items query."
      },
      {
        "date": "2024-06-13T10:49:00.000Z",
        "voteCount": 2,
        "content": "D. Since Order and Order Lines tables are parts of the fact table, so combine them first and make the combination the centre of the star schema, then join the DIM table Product."
      },
      {
        "date": "2024-05-22T10:17:00.000Z",
        "voteCount": 3,
        "content": "D. From Power Query, merge the Orders query and the Order Line Items query.\n\nBy merging these queries, you create a single table that contains all the necessary information about orders and their line items. This simplifies the data model and reduces the overhead of handling multiple tables and relationships, leading to more efficient interactions with visuals in your report."
      },
      {
        "date": "2024-05-09T19:20:00.000Z",
        "voteCount": 2,
        "content": "D - Consolidate the order and order line items query first - you will need information from both tables"
      },
      {
        "date": "2024-04-05T07:43:00.000Z",
        "voteCount": 1,
        "content": "The main advantage of D over A is that Merging Orders with Order Line Items creates a fact table in a star schema, instead of having two fact table."
      },
      {
        "date": "2024-03-27T07:39:00.000Z",
        "voteCount": 2,
        "content": "Here me out, in able to fulfill both requirements, A and D are correct.\nD fulfill the first requirement, and A fulfill the second requirement. But the question ask \"what should you do FIRST\".\n\nThat was why the answer should be D. Because it's the first thing the requirement ask us to do."
      },
      {
        "date": "2024-01-20T06:13:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer should be D. The question asks about the amount of the order over time. The first table only contains the Product Catalog. it doesn't have anything to do with the requirements in the question."
      },
      {
        "date": "2024-01-02T12:20:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer!"
      },
      {
        "date": "2023-11-19T23:36:00.000Z",
        "voteCount": 2,
        "content": "How you get product attribute from D option? is there right information you have in 2nd table regarding product?"
      },
      {
        "date": "2023-09-04T21:57:00.000Z",
        "voteCount": 2,
        "content": "Orders table has high-level order info, while OrderLines has more granular order info. The only way to link OrderLines (which also contains ProductID and price of each order) to Orders would be via an FK reference i.e. OrderID."
      },
      {
        "date": "2023-09-04T11:23:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is D.\nTo perform the analyses of orders sold over time and orders by attributes of products sold in a Power BI report while minimizing update times when interacting with visuals, we should do, From Power Query, merge the Orders query and the Order Line items query. Because:\nBy merging the Orders query and the Order Line items query in Power Query, we combine the relevant data into a single table. After merging the tables in Power Query, you can establish relationships between the tables, ensuring that they are related properly in the data model. Once tables are merged and relationships are set up, we can then create measures in DAX to calculate the total order value and analyze orders by attributes of products sold. This calculation will perform more efficient because we have reduced the data complexity through merging and proper data modeling."
      },
      {
        "date": "2023-12-25T12:48:00.000Z",
        "voteCount": 1,
        "content": "Hi, can you explain again why we have to merge two tables? I'm not sure for star scheme, should it be the case as long as the relationship between two tables has been established, and that would be fine?"
      },
      {
        "date": "2024-02-23T00:25:00.000Z",
        "voteCount": 2,
        "content": "he cannot, it's a chatGpt copy/paste answer"
      },
      {
        "date": "2023-08-25T07:18:00.000Z",
        "voteCount": 1,
        "content": "Should be A, because we need to get \" Orders sold over time that include a measure of the total order value Orders by attributes of products sold\""
      },
      {
        "date": "2023-08-05T06:27:00.000Z",
        "voteCount": 1,
        "content": "As many have said, its not great that the details aren't there but.\n1) I believe its 2 different requirements. The first one has the little picture and doesn't require us to merge anything to answer as Order will have the total amount so we dont need so SUM anything in order details\n\n2) requires details about the products that won't be on the orders table. Someone mentioned product ID would be on Order. No it would not, or you would simply have ALREADY merged orders and order lines since you can have multiple products on an order.\nSo I am going with D. Although I dont like any of them"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79387-exam-pl-300-topic-1-question-8-discussion/",
    "body": "You have a Microsoft SharePoint Online site that contains several document libraries.<br>One of the document libraries contains manufacturing reports saved as Microsoft Excel files. All the manufacturing reports have the same data structure.<br>You need to use Power BI Desktop to load only the manufacturing reports to a table for analysis.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet data from a SharePoint folder and enter the site URL Select Transform, then filter by the folder path to the manufacturing reports library.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet data from a SharePoint list and enter the site URL. Select Combine &amp; Transform, then filter by the folder path to the manufacturing reports library.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet data from a SharePoint folder, enter the site URL, and then select Combine &amp; Load.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet data from a SharePoint list, enter the site URL, and then select Combine &amp; Load."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-04T01:22:00.000Z",
        "voteCount": 27,
        "content": "We have to import Excel files from SharePoint, so we need the connector SharePoint folder which is used to get access to the files stored in the library. SharePoint list is a collection of content that has rows and columns (like a table) and is used for task lists, calendars, etc. \nSince we have to filter only on manufacturing reports, we have to select Transform and then filter by the corresponding folder path."
      },
      {
        "date": "2024-09-20T23:25:00.000Z",
        "voteCount": 9,
        "content": "The Correct Answer is A. \nTo load only the manufacturing reports (Microsoft Excel Files) from a specified SharePoint document library into Power BI Desktop for analysis, we should do, Geta data from a SharePoint folder and enter the site URL. Select Transform, then filter by the folder path to the manufacturing reports library. Option B is not the right answer choice because \"SharePoint list\" is selected, and you need to access files in a document library, not a list. Option C and Option D both include \"Combine &amp; Load\", which typically combines multiple queries or tables and loads them into Power BI. However, since we only want to load files from a specific folder within the SharePoint library, we should use the \"Transform\" option to filter and select the desired data before loading it into Power BI."
      },
      {
        "date": "2024-09-20T23:25:00.000Z",
        "voteCount": 3,
        "content": "Correct the answer A\n\nExplanation: Since the manufacturing reports are saved as Excel files in a specific document library within SharePoint Online, the best option is to use Power BI Desktop to get data from the SharePoint folder that contains the manufacturing reports. After entering the site URL, select Transform to open Power Query Editor, and then filter the folder path to the manufacturing reports library so that only the Excel files in that library are loaded to the table for analysis. Option A is the correct choice for this scenario.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2024-08-14T06:18:00.000Z",
        "voteCount": 1,
        "content": "A is the correct one. You need to select sharepoint folder as source, then insert the sharepoint URL and, as every source, select Transform:\n\nA. Get data from a SharePoint folder and enter the site URL Select Transform, then filter by the folder path to the manufacturing reports library. OK"
      },
      {
        "date": "2023-11-01T10:18:00.000Z",
        "voteCount": 2,
        "content": "Someone please explain this to me, B says combine and transform which is mentioned in the explanation but why is that not the answer?"
      },
      {
        "date": "2023-11-11T12:24:00.000Z",
        "voteCount": 1,
        "content": "B says SharePoint list"
      },
      {
        "date": "2023-07-14T06:04:00.000Z",
        "voteCount": 2,
        "content": "I guess answer is b, but not sure why it is A. Can someone please assist me with explanation"
      },
      {
        "date": "2023-05-24T16:12:00.000Z",
        "voteCount": 1,
        "content": "Why not C ? are the same stes as what is explained as solution,,,"
      },
      {
        "date": "2023-07-26T01:22:00.000Z",
        "voteCount": 1,
        "content": "Yes, I think it's c too. And I check others' comments, maybe C is missing the step to choose files."
      },
      {
        "date": "2023-05-01T05:54:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2023-03-23T07:47:00.000Z",
        "voteCount": 1,
        "content": "I gree the expication."
      },
      {
        "date": "2023-02-28T18:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. Once you import the Sharepoint folder in Power BI, you can clean the data by transforming it and then filter it to show only the data you need based on the specific path."
      },
      {
        "date": "2023-01-05T10:56:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2022-12-27T06:10:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-20T22:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is Get data from a SharePoint Online folder and enter the site URL. Select Combine &amp; Transform, then filter by the folder path to the manufacturing reports library.\n\n"
      },
      {
        "date": "2022-12-15T19:59:00.000Z",
        "voteCount": 1,
        "content": "A is correct!"
      },
      {
        "date": "2022-12-12T10:40:00.000Z",
        "voteCount": 1,
        "content": "C. Get data from a SharePoint folder, enter the site URL, and then select Combine &amp; Load."
      },
      {
        "date": "2022-11-28T11:56:00.000Z",
        "voteCount": 1,
        "content": "The answer is D\nOnce the site URL is entered, the user selects \"Combine &amp; Transform Data\" or \"Combine &amp; Load\". But not just \"Transform\". Also there is no need to filter by the folder path, because the folder path is already in the URL.\n"
      },
      {
        "date": "2022-12-12T10:39:00.000Z",
        "voteCount": 1,
        "content": "I mean: the answer is: C. Get data from a SharePoint folder, enter the site URL, and then select Combine &amp; Load.\nOf course it is about a SharePoint FOLDER"
      },
      {
        "date": "2023-09-21T06:58:00.000Z",
        "voteCount": 1,
        "content": "&gt; Also there is no need to filter by the folder path, because the folder path is already in the URL\n\n...except for the fact that the SharePoint folder dialog explicitly requires the site root URL *without* the folder. \n\n"
      },
      {
        "date": "2022-11-04T03:48:00.000Z",
        "voteCount": 1,
        "content": "correct, A should be the correct answer"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79390-exam-pl-300-topic-1-question-9-discussion/",
    "body": "DRAG DROP -<br>You have a Microsoft Excel workbook that contains two sheets named Sheet1 and Sheet2.<br>Sheet1 contains the following table named Table1.<br><img src=\"/assets/media/exam-media/04331/0001900001.png\" class=\"in-exam-image\"><br>Sheet2 contains the following table named Table2.<br><img src=\"/assets/media/exam-media/04331/0002000001.png\" class=\"in-exam-image\"><br>You need to use Power Query Editor to combine the products from Table1 and Table2 into the following table that has one column containing no duplicate values.<br><img src=\"/assets/media/exam-media/04331/0002000002.png\" class=\"in-exam-image\"><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04331/0002100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0002100002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-shape-and-combine-data",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-02T05:46:00.000Z",
        "voteCount": 426,
        "content": "Import From Excel\nAppend Table 2 to Table 1\nRemove Duplicates"
      },
      {
        "date": "2024-10-09T14:01:00.000Z",
        "voteCount": 1,
        "content": "Sorry about my newbie ignorance, but appending Table 2 to Table 1 creates two columns in Table 1, which have to be merged after appending. It seems to me that only after merging the two columns it makes sense and it is possible to remove duplicates. Much appreciated if errors in my conclusion could be pointed out."
      },
      {
        "date": "2024-04-02T00:58:00.000Z",
        "voteCount": 3,
        "content": "Performed these actions and get the correct result. This should be the answer."
      },
      {
        "date": "2022-10-12T05:11:00.000Z",
        "voteCount": 2,
        "content": "Agreed, that's the correct"
      },
      {
        "date": "2022-09-03T12:52:00.000Z",
        "voteCount": 5,
        "content": "Correct"
      },
      {
        "date": "2022-09-05T00:45:00.000Z",
        "voteCount": 68,
        "content": "Import From Excel since it has not been loaded to Powerbi initially\nAppend Table 2 to Table 1\nRemove Duplicates from the table appended to (Table1)"
      },
      {
        "date": "2024-09-29T02:00:00.000Z",
        "voteCount": 2,
        "content": "D E A, pls revise the answer"
      },
      {
        "date": "2024-08-25T10:03:00.000Z",
        "voteCount": 3,
        "content": "Load both tables into Power Query Editor:\n\nOpen Power Query Editor and import Sheet1 and Sheet2 from the Excel workbook. This will make Table1 and Table2 available for manipulation.\nAppend Queries:\n\nAppend Table1 and Table2 to combine the rows from both tables into a single table. This creates a unified table with all products listed.\nRemove Duplicates:\n\nApply the \"Remove Duplicates\" transformation to the appended table to ensure that only unique products are kept in the final result."
      },
      {
        "date": "2024-08-14T06:23:00.000Z",
        "voteCount": 1,
        "content": "In my opinion it is wrong!\nWe need first of all to import the two tables from excel, then we have to append queries and finally we need to remove duplicates. \nSo the solution would be: D-E-B"
      },
      {
        "date": "2024-08-11T20:41:00.000Z",
        "voteCount": 2,
        "content": "actually it would be append table 1 to table 2"
      },
      {
        "date": "2024-08-04T08:07:00.000Z",
        "voteCount": 2,
        "content": "why we need to append these tables, because to append the table the structure of the tables should be same. But here the structure of the tables are not same"
      },
      {
        "date": "2024-06-17T08:29:00.000Z",
        "voteCount": 3,
        "content": "Import From Excel\nAppend\nRemove Duplicates"
      },
      {
        "date": "2024-05-12T02:59:00.000Z",
        "voteCount": 5,
        "content": "Answer is incorrect... who decided that it was correct? This is misleading and confusing.\n1) Import from Excel\n2) Append \n3) Remove Duplicates"
      },
      {
        "date": "2024-04-05T23:46:00.000Z",
        "voteCount": 2,
        "content": "Import\nAppend\nRemove duplicates"
      },
      {
        "date": "2024-02-25T01:45:00.000Z",
        "voteCount": 9,
        "content": "You need to import the tables first.\nAnd removing Error is not a necessary step specially in this case, there won't be any errors to remove.\n\nI think the best answer is:\n\n1. From Power BI Desktop, import the data from Excel, and select Tabel1 and Table2.\n2. From Power Query Editor, append Tale2 and Table1.\n3. From Power Query Editor, select Table1, and then Remove duplicates."
      },
      {
        "date": "2024-03-12T11:18:00.000Z",
        "voteCount": 1,
        "content": "exactly"
      },
      {
        "date": "2024-02-23T02:42:00.000Z",
        "voteCount": 1,
        "content": "Solution IS CORRECT because:\n\ntested and validated solution:\ncreate an excel sheet with one table in each sheet.\nTHEY MUST BE TABLES (ctrl+t)\n\nIn PBI Desktop create New report\n-&gt; click transform data: opens query editor\n-&gt; New Source : excel -&gt; select Table1 &amp; Table2 (DO NOT SELECT SHEETS!!)\n-&gt;Power Query &gt; Home &gt; Combine &gt; Append Queries &gt; table to append : select Table1\n\nThe major difference between Append table 1 to table 2 is the order of data after append.\nTo get the right order as in the question, you select Table2 and you append Table1 to it.\n\nRight click Products Header &gt; Remove Duplicates\nResult is exactly what is shown is question:\nProducts\nabc\nxyz\ntuv\nmno\npqr\nstu\ndef\nghi\njkl"
      },
      {
        "date": "2024-02-23T02:43:00.000Z",
        "voteCount": 2,
        "content": "typo error : NOT CORRECT"
      },
      {
        "date": "2024-02-23T02:48:00.000Z",
        "voteCount": 1,
        "content": "sorry... IS CORRECT because the question asks for 3 actions and you can solve it with 2 actions (append tables, remove duplicates) So I assume you have to choose an action that will not harm your solution and bring some benefits to it. The only benefit being removing errors..."
      },
      {
        "date": "2024-02-06T07:01:00.000Z",
        "voteCount": 7,
        "content": "Who is deciding these answers are correct? The answer marked here as 'correct' is simply not. There are no errors to remove. \n\nThe question is also horribly ambiguous, as it doesn't suggest whether you should start In Power Bi or Excel?"
      },
      {
        "date": "2024-01-31T09:40:00.000Z",
        "voteCount": 6,
        "content": "Import From Excel\nAppend Table 2 to Table 1\nRemove Duplicates\n100% tested"
      },
      {
        "date": "2024-01-25T06:15:00.000Z",
        "voteCount": 2,
        "content": "1. Import Excel from the get data option.\n2. Append Table 2 to Table 1.\n3. Remove the Duplicates in the Table 1 Column."
      },
      {
        "date": "2024-01-30T22:12:00.000Z",
        "voteCount": 1,
        "content": "I thought the same but answer they provided is different. How do we know if the data is imported into PBI Desktop or not from the question?"
      },
      {
        "date": "2024-01-23T09:06:00.000Z",
        "voteCount": 2,
        "content": "D then  E then B"
      },
      {
        "date": "2024-01-22T10:23:00.000Z",
        "voteCount": 3,
        "content": "Are we supposed to assume every table will have errors? because these tables would not have thrown errors"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80557-exam-pl-300-topic-1-question-10-discussion/",
    "body": "You have a CSV file that contains user complaints. The file contains a column named Logged. Logged contains the date and time each complaint occurred. The data in Logged is in the following format: 2018-12-31 at 08:59.<br>You need to be able to analyze the complaints by the logged date and use a built-in date hierarchy.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a transformation to extract the last 11 characters of the Logged column and set the data type of the new column to Date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data type of the Logged column to Date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the Logged column by using at as the delimiter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply a transformation to extract the first 11 characters of the Logged column."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 179,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T02:02:00.000Z",
        "voteCount": 93,
        "content": "Answer C is best approach\nSplit the Logged column by using \"at\" as the delimiter."
      },
      {
        "date": "2022-12-27T20:45:00.000Z",
        "voteCount": 3,
        "content": "Agreed with you Jay"
      },
      {
        "date": "2022-11-22T12:04:00.000Z",
        "voteCount": 2,
        "content": "agreed"
      },
      {
        "date": "2023-04-16T18:54:00.000Z",
        "voteCount": 9,
        "content": "If you choose to split it will create 2 columns but extract will give 1 column."
      },
      {
        "date": "2024-01-27T04:40:00.000Z",
        "voteCount": 1,
        "content": "wont that give an extra space at the end?"
      },
      {
        "date": "2022-12-28T05:19:00.000Z",
        "voteCount": 36,
        "content": "C,\n\nYou should split the Logged column by using \"at\" as the delimiter. This will allow you to separate the date and time into separate columns, which will enable you to analyze the complaints by date and use a built-in date hierarchy. Alternatively, you could also use a transformation to extract the date and time from the Logged column and set the data type of the new columns to Date and Time, respectively. Option A is incorrect because it only extracts the last 11 characters of the Logged column, which would not include the date. Option B is incorrect because the data in the Logged column is in a non-standard date format and cannot be directly converted to the Date data type. Option D is incorrect because it only extracts the first 11 characters of the Logged column, which would not include the time."
      },
      {
        "date": "2023-02-10T22:41:00.000Z",
        "voteCount": 3,
        "content": "delimiter uses only one character, so \"at\" is not valid"
      },
      {
        "date": "2023-03-14T02:28:00.000Z",
        "voteCount": 10,
        "content": "You actually can do that if you click on the \"Select or enter delimiter\" in the \"Split Column by Delimiter\" window that pops up after you click on \"Split Column\" in the \"Transform\" tab on top of your Power BI window. After you select the --Custom-- option from the drop down menu in the \"Select or enter delimiter\" drop down list, you can write \"at\" in the text box that appears below the drop down list."
      },
      {
        "date": "2023-07-12T05:30:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2024-10-07T00:07:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-09-27T13:53:00.000Z",
        "voteCount": 1,
        "content": "CCCCCCCCCCCCCCCCC! No Brainer!"
      },
      {
        "date": "2024-09-20T23:30:00.000Z",
        "voteCount": 1,
        "content": "answer is D, apply a TRANSFORMATION; \nusing a demo. create a csv with complaint id, complaint date(use the date format described in the example)\nCreate new report -&gt; transform data : opens Power Query editor\nNew Sources -&gt; csv\n\nHere comes the tricky part: when you import a csv, you can apply TRANSFORMATION on the fly;\nthere is a button at the bottom left part of the window : Extract table using examples\n\nclick that button, here comes the TRANSFORMATION\nYou are presented with a form allowing you to pick up the fields and the data you want by example.\nName your headers according to the csv headers and in the first data row, type the kind of data you want.\n\nFor the logged column, you will type the date only.\nAfter import you will see that Query editor imported and converted your field in Date format.\n\nFinally, go to report view and expand your imported csv in the Data pane.\nYou will see that Power BI created a date hierarchy"
      },
      {
        "date": "2024-09-20T23:30:00.000Z",
        "voteCount": 2,
        "content": "there are two major issues with both C and D\nC have the wrong delimeter. \"at\" needs you to have a trim operation after the split. \" at \" (with a space before and after) will do the trick for us\nD  we cannot be sure that different dates with month or days with just one units will be written as i.e. 2024-12-03 or 2024-12-3 making the fixed value of 11 character a bit confusing. mostly in dates like 2024-1-1 at 00:00:0000 where the first 11 characters are 2024-1-1 a\ni think that C will be better. in both cases the resulted columns need to have other transformation to be interpreted as date and not string"
      },
      {
        "date": "2024-09-16T01:09:00.000Z",
        "voteCount": 1,
        "content": "correct is C"
      },
      {
        "date": "2024-09-13T04:49:00.000Z",
        "voteCount": 1,
        "content": "While Option C (splitting by \"at\") could work, Option D is the better answer because it provides a more direct and efficient method to extract the date. It avoids the extra step of splitting and discarding the time part, which is unnecessary when you only need the date for analysis."
      },
      {
        "date": "2024-09-05T20:12:00.000Z",
        "voteCount": 1,
        "content": "Splitting the column with the custom delimiter - at works well as the datatype is automatically recognized as Date"
      },
      {
        "date": "2024-08-28T05:16:00.000Z",
        "voteCount": 1,
        "content": "there is no need for the time, only for the date"
      },
      {
        "date": "2024-08-25T22:52:00.000Z",
        "voteCount": 1,
        "content": "According to me c is correcct because D option will capture space in the 11th character and there should not be space in date."
      },
      {
        "date": "2024-08-17T23:36:00.000Z",
        "voteCount": 1,
        "content": "Answer: D. Apply a transformation to extract the first 11 characters of the Logged column.\n\nReason:\n\nThe date part of the Logged column is contained in the first 11 characters (\"2018-12-31\"). To analyze the complaints by date and use the built-in date hierarchy in Power BI, you need to isolate the date from the time. By extracting the first 11 characters, you capture just the date part, which can then be converted to a Date data type. This allows Power BI to recognize it as a date and automatically generate a date hierarchy for analysis.\n\nOption A (extracting the last 11 characters) would give you only the time, not the date.\nOption B would not work directly because the column has both date and time, and the format includes additional text (\"at\").\nOption C (splitting the column) is more complex and unnecessary when extracting the first 11 characters achieves the goal efficiently."
      },
      {
        "date": "2024-08-14T06:25:00.000Z",
        "voteCount": 1,
        "content": "If you get the first 11 characters then there should be a space and you need a trim operation before transform as date. The best option is to use \" at\" as delimiter (also in this case notice the space). So,\nC. Split the Logged column by using at as the delimiter is correct"
      },
      {
        "date": "2024-08-13T06:02:00.000Z",
        "voteCount": 2,
        "content": "C and D both correct answers to extract the date, but after reading the question several times, in no case they mentioned the need of the time to proceed the analysis. So as an optimized solution i go with tranformation of the current column because i don't want an unused column (time) to be loaded in my model."
      },
      {
        "date": "2024-07-20T12:56:00.000Z",
        "voteCount": 1,
        "content": "Technically, this is the wrong answer. If you select answer C, you will split the data by \"at\", but you must also convert the date into a date format to use the hierarchy. As mentioned in option A, using 11 characters to split the data might not work in case there is extra space in some of the rows. So, the right answer should be to extract the date using \"at\" as a custom delimiter to split the columns, then convert the split column to the date data type, and then use this column for visualization."
      },
      {
        "date": "2024-06-26T06:51:00.000Z",
        "voteCount": 1,
        "content": "you should split the logged column."
      },
      {
        "date": "2024-06-24T09:07:00.000Z",
        "voteCount": 1,
        "content": "o analyze the complaints by the logged date and utilize Power BI\u2019s built-in date hierarchy, you should apply a transformation to extract the first 11 characters of the Logged column. This will give you the date portion, which can then be used effectively for your analysis"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80588-exam-pl-300-topic-1-question-11-discussion/",
    "body": "You have a Microsoft Excel file in a Microsoft OneDrive folder.<br>The file must be imported to a Power BI dataset.<br>You need to ensure that the dataset can be refreshed in powerbi.com.<br>Which two connectors can you use to connect to the file? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExcel Workbook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText/CSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFolder",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSharePoint folder\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeb\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 171,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "AD",
        "count": 15,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-29T09:40:00.000Z",
        "voteCount": 82,
        "content": "We can import an excel file from multiple connectors (excel workbook, folder, web, sharepoint) but if we must refresh the data from the service with no gateways then We must use web and sharepoint connectors"
      },
      {
        "date": "2024-04-17T11:59:00.000Z",
        "voteCount": 1,
        "content": "It would depend if there is authentication involved with the SharePoint folder or the Web address, what if they are using basic auth? Anyway, either of the answers, a,b,c,d all offer difficulty for me to answer as the correct one.  So, I guess it is which is the most correct?"
      },
      {
        "date": "2023-05-01T06:17:00.000Z",
        "voteCount": 4,
        "content": "but OneDrive is cloud and we do not need a gateway."
      },
      {
        "date": "2022-10-14T03:27:00.000Z",
        "voteCount": 2,
        "content": "Try it. D and E won't work. Its looking for a URL"
      },
      {
        "date": "2022-10-16T23:53:00.000Z",
        "voteCount": 18,
        "content": "I tried both and they work perfectly, and of course, you need the path (in this case the URL of the excel file on One Drive) of the file, so I don\u00b4t see the problem you say..."
      },
      {
        "date": "2022-11-02T10:10:00.000Z",
        "voteCount": 15,
        "content": "Works just fine, this is how you do it :) "
      },
      {
        "date": "2022-12-27T21:04:00.000Z",
        "voteCount": 5,
        "content": "Agreed KobeData"
      },
      {
        "date": "2022-10-04T02:22:00.000Z",
        "voteCount": 35,
        "content": "A, B, C: wrong! Would work technically, but the connection will be only to the local copy of the file, no refresh from the online version stored on OneDrive\nD: correct, but more complicated than option E\nE: correct, this is the best option to import from OneDrive"
      },
      {
        "date": "2024-10-03T02:13:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer A and D\n\nA. Excel Workbook\nD. SharePoint folder\n\nYou can use the \"Excel Workbook\" connector to connect to the Excel file stored in OneDrive and import it into a Power BI dataset. This connector allows you to select the OneDrive folder where the file is located and specify the file name.\n\nYou can also use the \"SharePoint folder\" connector to connect to the OneDrive folder and import the Excel file into a Power BI dataset. This connector allows you to specify the URL of the OneDrive folder and navigate to the Excel file within the folder.\n\nUsing either of these connectors ensures that the dataset can be refreshed in Power BI, as the connection to the OneDrive folder will remain active even if the Excel file is updated or moved within the folder.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2024-02-06T07:47:00.000Z",
        "voteCount": 2,
        "content": "https://www.youtube.com/watch?v=CYSMZxaXNLk"
      },
      {
        "date": "2024-10-03T02:13:00.000Z",
        "voteCount": 3,
        "content": "Correct Ansers\n\nA. Excel Workbook\nD. SharePoint folder\n\nTo ensure that the dataset can be refreshed in powerbi.com, you can use the Excel Workbook connector or the SharePoint folder connector to connect to the Excel file in Microsoft OneDrive.\n\nThe Excel Workbook connector allows you to connect to an Excel file in OneDrive and create a Power BI dataset directly from the file. This connector also enables you to schedule automatic refreshes of the dataset to ensure that the data is always up-to-date.\n\nThe SharePoint folder connector allows you to connect to a SharePoint folder where the Excel file is stored, and then you can select the Excel file to create a dataset. This connector also supports automatic refreshes of the dataset.\n\nUsing either of these connectors will ensure that the dataset can be refreshed in powerbi.com, as long as the OneDrive file is accessible and the credentials used to connect to the file are valid.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2024-09-21T03:54:00.000Z",
        "voteCount": 1,
        "content": "I believe A &amp; C are the right answers. I have my excel file in Onedrive and I connect to the dataset through this means and refresh my dashboards. We can import excel files directly from Onedrive which is cloud. gateway connection isnt needed."
      },
      {
        "date": "2024-09-05T20:13:00.000Z",
        "voteCount": 1,
        "content": "schedule refresh without a gateway"
      },
      {
        "date": "2024-08-20T06:45:00.000Z",
        "voteCount": 1,
        "content": "Answer: A. Excel Workbook and D. SharePoint folder\n\nReason:\n\nExcel Workbook (A): Directly connects to an Excel file stored in OneDrive. Power BI supports this connection type, and the dataset can be refreshed in Power BI service.\n\nSharePoint folder (D): Connects to files stored in SharePoint, which includes OneDrive for Business. This allows Power BI to access and refresh the Excel file as long as it's in a SharePoint or OneDrive for Business folder.\n\nText/CSV (B): Used for CSV files, not Excel files.\nFolder (C): Used for multiple files in a folder, not specifically for a single Excel file.\nWeb (E): Used for web URLs, not for files stored in OneDrive."
      },
      {
        "date": "2024-09-18T05:27:00.000Z",
        "voteCount": 1,
        "content": "that is how I understood it as well. As mentioned we have an Excel file in one drive. \nSo we would first connect with the SharePoint folder, and then get the Excel from that folder."
      },
      {
        "date": "2024-08-14T06:29:00.000Z",
        "voteCount": 1,
        "content": "You need to ensure that the dataset can be refreshed in powerbi.com"
      },
      {
        "date": "2024-07-20T09:55:00.000Z",
        "voteCount": 1,
        "content": "a and c are correct. it requires physical copies to be imported into the power bi dataset and not some data gateway as in the case of web and sharepoint."
      },
      {
        "date": "2024-06-18T01:05:00.000Z",
        "voteCount": 1,
        "content": "The correct Answer that i am very sure of is E, i always connect my google sheet file to power BI using web option and it always get refreshed on schedule refresh"
      },
      {
        "date": "2024-06-14T15:12:00.000Z",
        "voteCount": 1,
        "content": "Tested."
      },
      {
        "date": "2024-04-22T01:21:00.000Z",
        "voteCount": 1,
        "content": "Since the File is in excel format and residing in cloud (OneDrive), we can connect to it easily either by SharePoint or Web Connecters."
      },
      {
        "date": "2024-04-06T08:06:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt says the answer is sharepointfolder and web and I agree"
      },
      {
        "date": "2024-04-17T11:43:00.000Z",
        "voteCount": 3,
        "content": "ChatGPT is wrong A TON!!! Be very careful with that thing."
      },
      {
        "date": "2024-04-03T06:47:00.000Z",
        "voteCount": 4,
        "content": "\"The file must be imported to a Power BI dataset.\" that may be the key of this question, as the 4 options can connect to the online file, if want this file can be imported to a Power BI dataset, Sharepoint folder and Web will have some limitations, but Excel Workbook and Folder can definitely be imported to this Power BI dataset. \n\nExcel Workbook and Folder related documents: \nConnect to a folder from Power Query Online: \nhttps://learn.microsoft.com/en-us/power-query/connectors/folder#connect-to-a-folder-from-power-query-online \nConnect to an Excel workbook from Power Query Online: \nhttps://learn.microsoft.com/en-us/power-query/connectors/excel#connect-to-an-excel-workbook-from-power-query-online"
      },
      {
        "date": "2024-02-23T04:26:00.000Z",
        "voteCount": 4,
        "content": "A &amp; C connectors present you the common dialog window allowing you to pickup the file from your ONE DRIVE FOLDER(available from windows explorer).\nNo credentials needed, no extra authentication"
      },
      {
        "date": "2024-01-15T17:34:00.000Z",
        "voteCount": 4,
        "content": "option D&amp;E - The SharePoint folder connector also work for files hosted on OneDrive for Business."
      },
      {
        "date": "2024-04-02T01:06:00.000Z",
        "voteCount": 1,
        "content": "thnx was looking for this :)"
      },
      {
        "date": "2023-12-24T07:39:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is E - Web\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-use-onedrive-business-links"
      },
      {
        "date": "2024-01-14T05:38:00.000Z",
        "voteCount": 2,
        "content": "2 answers brah"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81899-exam-pl-300-topic-1-question-12-discussion/",
    "body": "HOTSPOT -<br>You are profiling data by using Power Query Editor.<br>You have a table named Reports that contains a column named State. The distribution and quality data metrics for the data in State is shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04331/0002500001.jpg\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04331/0002600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0002700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 69 -<br>69 distinct/different values.<br>Note: Column Distribution allows you to get a sense for the overall distribution of values within a column in your data previews, including the count of distinct values (total number of different values found in a given column) and unique values (total number of values that only appear once in a given column).<br><br>Box 2: 4 -<br>Reference:<br>https://systemmanagement.ro/2018/10/16/power-bi-data-profiling-distinct-vs-unique/",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-20T00:52:00.000Z",
        "voteCount": 137,
        "content": "69 is always the right choice! ;)"
      },
      {
        "date": "2024-08-08T04:04:00.000Z",
        "voteCount": 4,
        "content": "69 and 4"
      },
      {
        "date": "2023-07-17T18:20:00.000Z",
        "voteCount": 5,
        "content": "Yes, best answer"
      },
      {
        "date": "2022-09-12T17:48:00.000Z",
        "voteCount": 28,
        "content": "Answer is correct"
      },
      {
        "date": "2024-09-18T05:25:00.000Z",
        "voteCount": 1,
        "content": "Actually, I tested and \"unique\" also counts a unique blank cell if you have it"
      },
      {
        "date": "2024-08-14T06:32:00.000Z",
        "voteCount": 1,
        "content": "69 and 4 is the correct option:\nThere are 69 different values in State including nulls\nThere are 4 non-null values that occur only once in State"
      },
      {
        "date": "2024-08-14T06:31:00.000Z",
        "voteCount": 1,
        "content": "69 and 4 is the correct option:"
      },
      {
        "date": "2024-04-22T01:23:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct."
      },
      {
        "date": "2024-04-02T06:32:00.000Z",
        "voteCount": 11,
        "content": "For everyone confused, the rule to remember is that Unique values are a subset of distinct values. So, every unique value is a distinct value but distinct values are not always unique."
      },
      {
        "date": "2024-03-03T23:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2023-11-06T04:54:00.000Z",
        "voteCount": 6,
        "content": "Given answers are correct. 69 distinct, 4 unique."
      },
      {
        "date": "2023-09-26T19:13:00.000Z",
        "voteCount": 4,
        "content": "69 and 4"
      },
      {
        "date": "2023-09-13T02:06:00.000Z",
        "voteCount": 2,
        "content": "69 , 4 : )"
      },
      {
        "date": "2023-09-04T12:29:00.000Z",
        "voteCount": 9,
        "content": "69 and 4 are correct.\nThe column has 69 distinct values, which includes all unique values (4) plus the empty values(nulls).\nThere are 4 unique values in the column, those are non-null values that occur only once in the \"State\" column."
      },
      {
        "date": "2023-08-14T09:32:00.000Z",
        "voteCount": 3,
        "content": "69 and 4"
      },
      {
        "date": "2023-08-05T09:22:00.000Z",
        "voteCount": 1,
        "content": "Why the 4 unique values are not considered district values as well? I didn't get it! Does anyone could help me please? Ty.  Answer: 73 and 4"
      },
      {
        "date": "2023-08-12T01:07:00.000Z",
        "voteCount": 9,
        "content": "This Column Distribution feature allows you to get a sense for the overall distribution of values within a column in your data previews, including the count of distinct values (total number of different values found in a given column) and unique values (total number of values that only appear once in a given column). In other words, distinct values represent the total number of different values found in a given column, while unique values represent the total number of values that only appear once in a given column.\nCheers!"
      },
      {
        "date": "2023-09-20T08:26:00.000Z",
        "voteCount": 1,
        "content": "Nice explanation!!!"
      },
      {
        "date": "2024-01-22T11:29:00.000Z",
        "voteCount": 1,
        "content": "Perfect explanation, but correct me if I am wrong this data set would then have 65 null values?"
      },
      {
        "date": "2023-08-17T04:23:00.000Z",
        "voteCount": 1,
        "content": "every unique value is considered as distinct so sum with distinct my friend :)"
      },
      {
        "date": "2023-07-04T13:53:00.000Z",
        "voteCount": 1,
        "content": "69 DIstinct (DIfferent) values (including null) / 4 UNIque (1 (UNI) appearance each) values"
      },
      {
        "date": "2023-06-28T06:10:00.000Z",
        "voteCount": 3,
        "content": "Can anyone share the downloaded pdf for the exam. Because I am writing the exam tomorrow"
      },
      {
        "date": "2023-06-07T18:33:00.000Z",
        "voteCount": 2,
        "content": "69 y 4 esta bn"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80990-exam-pl-300-topic-1-question-13-discussion/",
    "body": "HOTSPOT -<br>You have two CSV files named Products and Categories.<br>The Products file contains the following columns:<br>\u2711 ProductID<br>\u2711 ProductName<br>\u2711 SupplierID<br>\u2711 CategoryID<br>The Categories file contains the following columns:<br>\u2711 CategoryID<br>\u2711 CategoryName<br>\u2711 CategoryDescription<br>From Power BI Desktop, you import the files into Power Query Editor.<br>You need to create a Power BI dataset that will contain a single table named Product. The Product will table includes the following columns:<br>\u2711 ProductID<br>\u2711 ProductName<br>\u2711 SupplierID<br>\u2711 CategoryID<br>\u2711 CategoryName<br>\u2711 CategoryDescription<br>How should you combine the queries, and what should you do on the Categories query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04331/0002900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0002900002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Merge -<br>There are two primary ways of combining queries: merging and appending.<br>* When you have one or more columns that you'd like to add to another query, you merge the queries.<br>* When you have additional rows of data that you'd like to add to an existing query, you append the query.<br><br>Box 2: Disable the query load -<br><br>Managing loading of queries -<br>In many situations, it makes sense to break down your data transformations in multiple queries. One popular example is merging where you merge two queries into one to essentially do a join. In this type of situations, some queries are not relevant to load into Desktop as they are intermediate steps, while they are still required for your data transformations to work correctly. For these queries, you can make sure they are not loaded in Desktop by un-checking 'Enable load' in the context menu of the query in Desktop or in the Properties screen:<br>Reference:<br>https://docs.microsoft.com/en-us/power-bi/connect-data/desktop-shape-and-combine-data https://docs.microsoft.com/en-us/power-bi/connect-data/refresh-include-in-report-refresh",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-07T11:51:00.000Z",
        "voteCount": 60,
        "content": "Ok for me"
      },
      {
        "date": "2023-08-17T04:28:00.000Z",
        "voteCount": 2,
        "content": "Ok for me too \ud83d\udc4d\ud83c\udffb"
      },
      {
        "date": "2022-10-03T00:48:00.000Z",
        "voteCount": 22,
        "content": "The given answer is correct.\nCombine the queries by performing a: Merge.\nOn the Categories query: Disable the query load."
      },
      {
        "date": "2024-09-27T14:03:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-08-14T06:37:00.000Z",
        "voteCount": 1,
        "content": "Of course we need to merge the two queries (join them) and we have also to disable the query load. THe asnwer is OK for me"
      },
      {
        "date": "2023-09-26T19:16:00.000Z",
        "voteCount": 1,
        "content": "answer is correct"
      },
      {
        "date": "2023-09-04T12:42:00.000Z",
        "voteCount": 4,
        "content": "Merge &amp; Exclude the query from report refresh are correct answers. \nUse the merge to combine the \"Products\" and \"Categories\" queries based on the \"CategoryID\" column.\nOnce we have combined the queries, we don't need the standalone \"Categories\" query because we have merged its data into the \"Product\" table. We \"Exclude the query from report refresh\" to ensure it doesn't unnecessarily reload when refreshing the report."
      },
      {
        "date": "2023-07-04T13:55:00.000Z",
        "voteCount": 1,
        "content": "Merge / Disable query load"
      },
      {
        "date": "2023-06-07T18:37:00.000Z",
        "voteCount": 1,
        "content": "La respuesta es correcta Combinar, pq se requiere complementar la primera tabla y Deshabilitar la carga de la consulta, para mejor rendimiento."
      },
      {
        "date": "2023-03-23T08:28:00.000Z",
        "voteCount": 1,
        "content": "1. Combine las consultas realizando: Merge - Combinar\n2. En la consulta Categor\u00edas: Deshabilite la carga de consultas."
      },
      {
        "date": "2023-01-05T11:05:00.000Z",
        "voteCount": 5,
        "content": "This is correct\n- Merge\n- Disable the query load"
      },
      {
        "date": "2022-12-27T21:26:00.000Z",
        "voteCount": 3,
        "content": "I totaly agree with the answer, Merge and disable the category query"
      },
      {
        "date": "2022-12-21T20:29:00.000Z",
        "voteCount": 4,
        "content": "correct  \n- Merge\n- Disable the query load"
      },
      {
        "date": "2022-12-16T20:09:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2022-12-16T05:52:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-12-15T20:32:00.000Z",
        "voteCount": 1,
        "content": "-Merge\n-Disable load"
      },
      {
        "date": "2022-11-23T03:19:00.000Z",
        "voteCount": 3,
        "content": "I understand the merge and the disable query concept but why don't you delete the categories table after merge"
      },
      {
        "date": "2022-11-29T04:06:00.000Z",
        "voteCount": 6,
        "content": "Usually the import is not a one time excercise and you will want to be able to refresh the datamodel with updated sources. Then you will need the Categories QUERY again.\nThis first option is about deleting the Categories QUERY, not the Categories TABLE."
      },
      {
        "date": "2023-04-30T02:02:00.000Z",
        "voteCount": 1,
        "content": "Thanks \ud83d\ude4f, very clear now"
      },
      {
        "date": "2023-07-03T15:53:00.000Z",
        "voteCount": 2,
        "content": "You should not thank him, because this answer didn't answer any question.\nThis is a good explanation of \"disable query update\", but not explanation of \"delete the query\"\nin fact, the answer is simple. If i delete query, i got nothing to upload to Power BI, think about this."
      },
      {
        "date": "2022-11-22T12:18:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct for me"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80547-exam-pl-300-topic-1-question-14-discussion/",
    "body": "You have an Azure SQL database that contains sales transactions. The database is updated frequently.<br>You need to generate reports from the data to detect fraudulent transactions. The data must be visible within five minutes of an update.<br>How should you configure the data connection?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a SQL statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Command timeout in minutes setting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Data Connectivity mode to Import.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Data Connectivity mode to DirectQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 52,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-22T12:19:00.000Z",
        "voteCount": 26,
        "content": "D is correct for me"
      },
      {
        "date": "2024-10-14T03:06:00.000Z",
        "voteCount": 1,
        "content": "For near real time data, always use direct query \nWhen it is not feasible to cache data in power BI, due to size constraines , always user direct query"
      },
      {
        "date": "2024-09-27T14:04:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-08-14T06:39:00.000Z",
        "voteCount": 3,
        "content": "Also for me is D. Set the connectivity mode to Direct Query. Because the data is refreshed each 5 minutes and the data in this way is not loaded into a cache but it is always refreshed"
      },
      {
        "date": "2024-03-10T02:34:00.000Z",
        "voteCount": 2,
        "content": "It'll not load the data in power query but go back to the original data source (SQL DB) which is updated frequently."
      },
      {
        "date": "2023-10-06T06:26:00.000Z",
        "voteCount": 2,
        "content": "D is correct for me"
      },
      {
        "date": "2023-09-04T12:52:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer.\nDirectQuery model allows Power BI to directly query the data source (Azure SQL database, in this case) in real-time or near real-time. When data is updated in the database, DirectQuery ensures that the reports reflect the most current data without the need to import and refresh the data into the Power BI model."
      },
      {
        "date": "2023-07-29T03:49:00.000Z",
        "voteCount": 1,
        "content": "dIRECT QUERY IS BEST"
      },
      {
        "date": "2023-07-04T13:57:00.000Z",
        "voteCount": 2,
        "content": "Near real-time data needed, so DirectQuery is needed."
      },
      {
        "date": "2023-05-22T21:41:00.000Z",
        "voteCount": 1,
        "content": "Correct Ans"
      },
      {
        "date": "2023-05-01T08:09:00.000Z",
        "voteCount": 2,
        "content": "Direct Query is the best choice!"
      },
      {
        "date": "2023-04-29T14:14:00.000Z",
        "voteCount": 1,
        "content": "DirectQuery - best for real-time, or if you have large datasets to pull from\n\nhttps://learn.microsoft.com/en-us/training/modules/get-data/6-storage-mode"
      },
      {
        "date": "2023-03-23T08:30:00.000Z",
        "voteCount": 1,
        "content": "D. Establezca el modo de conectividad de datos en DirectQuery."
      },
      {
        "date": "2023-01-07T15:31:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer"
      },
      {
        "date": "2022-12-19T09:28:00.000Z",
        "voteCount": 1,
        "content": "D is correct because the database is updated frequently."
      },
      {
        "date": "2022-11-15T20:56:00.000Z",
        "voteCount": 4,
        "content": "Yup! D seems most appropriate."
      },
      {
        "date": "2022-11-01T14:08:00.000Z",
        "voteCount": 3,
        "content": "D. Set Data Connectivity mode to DirectQuery because the data is accessed frequently."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80566-exam-pl-300-topic-1-question-15-discussion/",
    "body": "DRAG DROP -<br>You have a folder that contains 100 CSV files.<br>You need to make the file metadata available as a single dataset by using Power BI. The solution must NOT store the data of the CSV files.<br>Which three actions should you perform in sequence. To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04331/0003100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04331/0003100002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: From Power BI Desktop, Select Get Data, and then Select Folder.<br>Open Power BI Desktop and then select Get Data\\More\u05d2\u20ac\u00a6 and choose Folder from the All options on the left.<br><img src=\"/assets/media/exam-media/04331/0003300001.jpg\" class=\"in-exam-image\"><br>Enter the folder path, select OK, and then select Transform data to see the folder's files in Power Query Editor.<br>Step 2: From Power Query Editor, expand the Attributes column.<br>Step 3: From Power Query Editor, combine the Content column.<br><img src=\"/assets/media/exam-media/04331/0003400001.png\" class=\"in-exam-image\"><br><br>Combine files behavior -<br>To combine binary files in Power Query Editor, select Content (the first column label) and select Home &gt; Combine Files. Or you can just select the Combine Files icon next to Content.<br>Reference:<br>https://docs.microsoft.com/en-us/power-bi/transform-model/desktop-combine-binaries",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-06T03:53:00.000Z",
        "voteCount": 217,
        "content": "I think the correct flow is\nGet data then select folder\nRemove content Colum\nExpand Attribute Colum"
      },
      {
        "date": "2022-09-14T15:06:00.000Z",
        "voteCount": 36,
        "content": "These are right answer\n1.Get data the select folder\n2.Remove attribute column (because this column contain information about file which not needed).\n3.Combine Content column (which contain actual data which needed for us)"
      },
      {
        "date": "2024-10-09T14:24:00.000Z",
        "voteCount": 1,
        "content": "emmanuelkech has the right answer here, content column is not required, so it should be removed. Attritbutes column is to be expanded."
      },
      {
        "date": "2022-11-29T04:18:00.000Z",
        "voteCount": 23,
        "content": "See the requirement \"The solution must NOT store the data of the CSV files.\"\nSo the content column must be removed."
      },
      {
        "date": "2022-12-21T09:32:00.000Z",
        "voteCount": 10,
        "content": "The question is not to store data of files"
      },
      {
        "date": "2023-01-03T01:55:00.000Z",
        "voteCount": 21,
        "content": "what about:\n1) get data from folder\n2) expand attribute\n3) remove content column\n\nwhy should this order be wrong?"
      },
      {
        "date": "2023-01-28T18:57:00.000Z",
        "voteCount": 18,
        "content": "It's not wrong, but the rule of thumb is \"Filter left. Format right.\". Removing columns is vertical filtering, so it should be on top."
      },
      {
        "date": "2023-02-08T01:18:00.000Z",
        "voteCount": 1,
        "content": "What do u mean with vertical filtering?"
      },
      {
        "date": "2023-09-22T06:44:00.000Z",
        "voteCount": 5,
        "content": "Horizontal filtering = removing rows we don't want.\nVertical filtering = removing columns we don't want."
      },
      {
        "date": "2023-04-22T06:11:00.000Z",
        "voteCount": 3,
        "content": "I have tested and works.\nSome ppl said to \"combine attribute\", but its not possible.The options is not avaliable."
      },
      {
        "date": "2022-10-11T09:11:00.000Z",
        "voteCount": 7,
        "content": "I agree that this is the requirement. The thing that bothers me is WHY? Why would you want to create a dataset with only the metadata?"
      },
      {
        "date": "2022-12-27T16:04:00.000Z",
        "voteCount": 12,
        "content": "audit purpose. Not everything is about the business results, for big corps you'd care about how it's run too"
      },
      {
        "date": "2022-09-06T05:40:00.000Z",
        "voteCount": 52,
        "content": "It should be remove Content not combine, since the file data is NOT to be stored."
      },
      {
        "date": "2022-09-08T09:16:00.000Z",
        "voteCount": 7,
        "content": "I agree"
      },
      {
        "date": "2022-10-26T05:10:00.000Z",
        "voteCount": 3,
        "content": "Tested here and it works. Thankyou!"
      },
      {
        "date": "2024-10-12T01:24:00.000Z",
        "voteCount": 1,
        "content": "I don't see why we combine the content column if it should not be stored.  It should be removed surely"
      },
      {
        "date": "2024-10-02T23:10:00.000Z",
        "voteCount": 1,
        "content": "how are is this type of question graded? does the order have to be correct to get the whole points or is it graded per correct selection?"
      },
      {
        "date": "2024-09-26T06:55:00.000Z",
        "voteCount": 1,
        "content": "Get data then select folder\nRemove content Colum\nExpand Attribute Colum"
      },
      {
        "date": "2024-08-16T08:07:00.000Z",
        "voteCount": 1,
        "content": "Not sure about that but in my opinion it is:\nA - From Power BI Desktop, select Get Data and then select Folder\nB - From Power Query Editor, expand the Attributes column\nC - From Power Query Editor, remove the Content column"
      },
      {
        "date": "2024-08-02T01:16:00.000Z",
        "voteCount": 3,
        "content": "answer:\nFrom Power BI Desktop, select Get Data, and then select Folder.\nFrom Power Query Editor, remove the Content column.\nFrom Power Query Editor, expand the Attributes column."
      },
      {
        "date": "2024-07-29T12:28:00.000Z",
        "voteCount": 1,
        "content": "The question mentioned do NOT store data of csv files. This means we have to remove Attributes. So the correct order is:\n1. Get data, select folder\n2. Remove Attribute\n3. Combine Content"
      },
      {
        "date": "2024-02-21T20:24:00.000Z",
        "voteCount": 1,
        "content": "the solution would be:\n1- Get Data from Folder\n2- Expand (Priority) the Attributes Column\n3- Combine the Content Column"
      },
      {
        "date": "2024-02-19T07:22:00.000Z",
        "voteCount": 1,
        "content": "I have the feeling this question is a voluntary trap of Microsoft :-) \nWhen you test the conflictual solutions in PBI Desktop you will see that combining content will add a bunch of crap in your semantic model..including the data that we are forbidden to store, it's common sense. It is written in the question itself, metadata only and as metadata is also included in Attributes, you need to expand attributes to get the entire metadata context."
      },
      {
        "date": "2024-02-17T01:20:00.000Z",
        "voteCount": 1,
        "content": "Tricky and Interesting"
      },
      {
        "date": "2024-02-03T05:45:00.000Z",
        "voteCount": 20,
        "content": "\nIts explained here which makes sense\nGet data then select folder\nRemove content Colum\nExpand Attribute Colum"
      },
      {
        "date": "2024-03-24T08:17:00.000Z",
        "voteCount": 2,
        "content": "Thank you!"
      },
      {
        "date": "2024-06-07T23:10:00.000Z",
        "voteCount": 1,
        "content": "Totally agree"
      },
      {
        "date": "2024-01-15T17:53:00.000Z",
        "voteCount": 3,
        "content": "I go with below instead of Combine the Content column for 3)\n\n1) get data from folder\n2) expand attribute\n3) remove content column\n\nCombine Content column will have all data of the csv files"
      },
      {
        "date": "2023-11-20T21:09:00.000Z",
        "voteCount": 2,
        "content": "the correct sequence of actions is: From Power BI Desktop, select Get Data, and then select Folder -&gt; From Power Query Editor, remove the Content column -&gt; From Power Query Editor, remove the Attributes column\n\nsince attributes column contains extra info about files we should remove that and only keep clean meta data."
      },
      {
        "date": "2023-09-29T04:11:00.000Z",
        "voteCount": 4,
        "content": "Only meta data must be shown and NO data must be stored.\nTry this out practical, following steps work.\n1) \nGet Data from folder\n2) Expand Attribute columns\n3) Remove Content column"
      },
      {
        "date": "2023-09-28T17:59:00.000Z",
        "voteCount": 3,
        "content": "expand the attributes.\n\nExpanding the attributes means that you create columns in your dataset to capture the metadata about each file, such as file names, file paths, file sizes, and other relevant attributes. This approach keeps the metadata in your dataset without storing the actual data from the CSV files.\n\nBy expanding the attributes, you ensure that you have the necessary information to work with the file metadata in your Power BI reports and visualizations while adhering to the requirement of not storing the CSV file data within your Power BI dataset."
      },
      {
        "date": "2023-09-04T13:09:00.000Z",
        "voteCount": 8,
        "content": "First step: From Power BI desktop, select get data, and then select folder. This is the first step to access the files within the folder.\nSecond Step:  From Power Query Editor, expand the attributes column. This step is necessary to access the file metadata.\nThird Step: From Power BI Query Editor, remove the content column. This step is crucial to exclude the actual file data and keep only the metadata.\nAre the correct steps. Others will not work because of the following reasons :\nRemoving the \"attributes\" column would prevent us from accessing the file metadata.\n\nSelecting \"text/csv\" from the \"Get Data\" menu would be necessary if we want to import the data from the CSV files themeselves, but our requirement is to extract metadata only, so this step in not needed.\n\nCombining the \"Content\" column is not necessary because we want to remove it to avoid storing the data, not combine it."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/82494-exam-pl-300-topic-1-question-16-discussion/",
    "body": "A business intelligence (BI) developer creates a dataflow in Power BI that uses DirectQuery to access tables from an on-premises Microsoft SQL server. The<br>Enhanced Dataflows Compute Engine is turned on for the dataflow.<br>You need to use the dataflow in a report. The solution must meet the following requirements:<br>\u2711 Minimize online processing operations.<br>\u2711 Minimize calculation times and render times for visuals.<br>\u2711 Include data from the current year, up to and including the previous day.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflows connection that has DirectQuery mode selected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflows connection that has Import mode selected and schedule a daily refresh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflows connection that has Import mode selected and create a Microsoft Power Automate solution to refresh the data hourly."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 73,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-04T21:18:00.000Z",
        "voteCount": 46,
        "content": "Correct Answer C\n\nBased on the requirements mentioned, the best option would be to choose option C: Create a dataflows connection that has Import mode selected and schedule a daily refresh.\n\nOption A is not the best choice as it requires online processing operations, which goes against one of the requirements.\n\nOption B is not necessary since the SQL Server is on-premises and not in a cloud environment. Gateway connections are typically used for cloud-based data sources that require access to on-premises data.\n\nOption D refreshes the data too frequently and might lead to unnecessary processing operations, which goes against one of the requirements.\n\nTherefore, the best approach is to use Import mode with daily scheduled refreshes to include data from the current year, up to and including the previous day. This would minimize online processing operations and also reduce calculation times and render times for visuals.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2023-10-16T13:34:00.000Z",
        "voteCount": 6,
        "content": "I would think Gateways are need for ON premises DB."
      },
      {
        "date": "2022-09-28T07:59:00.000Z",
        "voteCount": 44,
        "content": "C, because one of the requirements is 'Minimize online processing operations'. Although the dataflow uses DirectQuery, the Dataset can be refreshed with Import.https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-directquery"
      },
      {
        "date": "2022-10-06T09:42:00.000Z",
        "voteCount": 3,
        "content": "Need a gateway"
      },
      {
        "date": "2022-10-10T05:02:00.000Z",
        "voteCount": 18,
        "content": "The BI developer has already created the dataflow, so the gateway must be present. Import and daily scheduled refresh should do the trick."
      },
      {
        "date": "2023-05-01T08:37:00.000Z",
        "voteCount": 1,
        "content": "in all options, it says :\" create a dataflow....\" it means we already do not have the dataflow"
      },
      {
        "date": "2023-07-23T08:08:00.000Z",
        "voteCount": 2,
        "content": "No, it's already created by the developer. Your task is \"to use the dataflow in a report\""
      },
      {
        "date": "2023-09-11T01:24:00.000Z",
        "voteCount": 2,
        "content": "direct query is to on prem database, does it still use online processing operation?"
      },
      {
        "date": "2022-12-06T07:35:00.000Z",
        "voteCount": 6,
        "content": "\"Although the dataflow uses DirectQuery, the Dataset can be refreshed with Import.\" -&gt; I dont understand this point. Can you help explain more details?"
      },
      {
        "date": "2023-06-05T00:26:00.000Z",
        "voteCount": 6,
        "content": "Image the Dataflow like the Common Data Model which has been ETL from the external data sources and PBI Desktop will connect to Dataflow by Import mode to create its dataset"
      },
      {
        "date": "2024-09-12T18:35:00.000Z",
        "voteCount": 3,
        "content": "must be A: Since the Enhanced Dataflows Compute Engine is turned on, you can use DirectQuery to access the dataflow. This allows you to query the data in real-time without importing it into Power BI, minimizing online processing operations"
      },
      {
        "date": "2024-08-14T07:10:00.000Z",
        "voteCount": 1,
        "content": "\"Include data from the current year, up to and including the previous day.\" and \"Minimize online processing operations\"\nC. Create a dataflows connection that has Import mode selected and schedule a daily refresh"
      },
      {
        "date": "2024-07-15T01:22:00.000Z",
        "voteCount": 1,
        "content": "I don't understand that we already have a dataflow using DirectQuery to access tables, why do we need to create a dataflow connection with Import mode?"
      },
      {
        "date": "2024-05-06T03:50:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\n"
      },
      {
        "date": "2024-05-05T01:11:00.000Z",
        "voteCount": 1,
        "content": "What is the correct one finally"
      },
      {
        "date": "2024-04-25T02:50:00.000Z",
        "voteCount": 3,
        "content": "The right answer is\nB. Create a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.\nThis option allows you to use DirectQuery mode, minimizing online processing operations, while also ensuring that you can access on-premises data from the Microsoft SQL server using a gateway connection."
      },
      {
        "date": "2024-05-13T05:50:00.000Z",
        "voteCount": 1,
        "content": "I also think that B is the correct answer. For those who have voted for C is possible to use import mode from Dataflow without gateway ?"
      },
      {
        "date": "2024-03-07T04:10:00.000Z",
        "voteCount": 1,
        "content": "Option C.\n\nAltought the dataflow (witch is a collection of tables) gets its data from Direct Query. The Dataset (Model of those tables) can use import, and because of that, save recourses.\n\nSo that discarts options A and B.\n\nAlso for option B a Gateway connection is not needed since the SQL sever is ON PREMISE and a gateway is used for cloud envarioment that require access for on premise data.\n\nThen you can also think that since we are trying to minimize resource, the hourly refresh is more expensive that option C\n\nSo option C is correct"
      },
      {
        "date": "2024-03-06T15:03:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-02T14:56:00.000Z",
        "voteCount": 1,
        "content": "C: Import will:\n1. Minimize online processing operations.\n2. Minimize calculation times and render times for visuals.\nthe daily scheduled refresh will keep data updated and thus Include data from the current year, up to and including the previous day."
      },
      {
        "date": "2023-11-10T19:28:00.000Z",
        "voteCount": 7,
        "content": "This question was in Exam today."
      },
      {
        "date": "2023-11-07T05:22:00.000Z",
        "voteCount": 1,
        "content": "C because it meets all requirements"
      },
      {
        "date": "2023-09-04T13:22:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is C. \nIt is because:\n\nImport mode allows us to load and store the data from the DirectQuery source in the Power BI service. This minimizes online processing operations, as calculations are performed during data refresh rather than in real-time during report rendering.\n\nScheduling a daily refresh ensures that our dataflow data is up to date while minimizing the frequency of refresh operations. Since we only need data up to and including the previous day, a daily refresh is sufficient."
      },
      {
        "date": "2023-07-29T04:08:00.000Z",
        "voteCount": 1,
        "content": "SINCE WE ARE MINIMIZING ONLINE PROCESSING"
      },
      {
        "date": "2023-07-09T13:09:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-05-16T04:19:00.000Z",
        "voteCount": 4,
        "content": "The best option to meet the given requirements would be:\n\nB. Create a dataflows connection that has DirectQuery mode selected and configure a gateway connection for the dataset.\n\nExplanation:\n\nThe requirement to minimize online processing operations suggests that DirectQuery mode should be used. DirectQuery allows Power BI to directly query the on-premises SQL server without importing the data into Power BI.\nThe requirement to minimize calculation times and render times for visuals is also achieved through DirectQuery mode. With DirectQuery, calculations are performed on the SQL server, reducing the computational load on Power BI.\nThe requirement to include data from the current year, up to and including the previous day can be handled by configuring a gateway connection. The gateway allows Power BI to establish a secure connection to the on-premises SQL server and retrieve the required data.\nOption B fulfills all the requirements by using DirectQuery mode and configuring a gateway connection for the dataset."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94598-exam-pl-300-topic-1-question-17-discussion/",
    "body": "DRAG DROP<br> -<br><br>You publish a dataset that contains data from an on-premises Microsoft SQL Server database.<br><br>The dataset must be refreshed daily.<br><br>You need to ensure that the Power BI service can connect to the database and refresh the dataset.<br><br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/pl-300/image279.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image280.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-10T19:55:00.000Z",
        "voteCount": 48,
        "content": "Current sequence looks good"
      },
      {
        "date": "2023-04-04T21:55:00.000Z",
        "voteCount": 29,
        "content": "Given Answer is correct\n\n\nSet up an on-premises data gateway: Download and install an on-premises data gateway on a machine that has access to the SQL Server database. Make sure that the gateway is registered to the same workspace as the dataset.\n\nConfigure a data source: In the Power BI service, go to the dataset settings, and select the data source. Then, enter the necessary details, including the server name, database name, and credentials.\n\nSchedule refresh: In the dataset settings, go to the \"Scheduled refresh\" tab, and set up a refresh schedule. Ensure that the gateway is selected as the \"Data source credentials\" option.\n\nPublish the dataset: Finally, publish the dataset to the Power BI service. The dataset will be refreshed according to the schedule you set up, and the on-premises data gateway will allow the service to connect to the SQL Server database.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2023-04-29T14:40:00.000Z",
        "voteCount": 2,
        "content": "3rd step should be add dataset owner to the data source"
      },
      {
        "date": "2023-04-29T14:40:00.000Z",
        "voteCount": 2,
        "content": "then the 4th step should be schedule the refresh"
      },
      {
        "date": "2023-06-02T04:55:00.000Z",
        "voteCount": 10,
        "content": "Publish the dataset isn't one of the answers tho"
      },
      {
        "date": "2024-10-07T03:32:00.000Z",
        "voteCount": 1,
        "content": "Configure an on-premises data gateway\nInstall and set up the on-premises data gateway to enable secure data transfer between the on-premises SQL Server and Power BI service.\n\nAdd a data source\nAdd the SQL Server database as a data source within the configured gateway, providing necessary connection details (e.g., server name, database name, and credentials).\n\nAdd the dataset owner to the data source\nEnsure the dataset owner has access to the data source in the gateway, granting them permission to use the connection for refreshing the dataset.\n\nConfigure a scheduled refresh\nSet up the dataset in Power BI to refresh on a daily schedule, using the gateway to pull updated data from the SQL Server.\n\nNote : adding a data source, adding the dataset owner to the data source, and configuring a scheduled refresh\u2014are performed within Power BI Service"
      },
      {
        "date": "2024-10-03T23:05:00.000Z",
        "voteCount": 1,
        "content": "What if I add the Data Source before setting up the Data Gateway?"
      },
      {
        "date": "2024-08-16T08:11:00.000Z",
        "voteCount": 1,
        "content": "The answers seems OK to me:\n1 - Configure an om-prem gateway\n2 - Add a data source\n3 - Add the dataset owner to the data source\n4 - Configure a scheduled refresh"
      },
      {
        "date": "2024-03-09T14:55:00.000Z",
        "voteCount": 1,
        "content": "correct sequence"
      },
      {
        "date": "2024-02-26T12:05:00.000Z",
        "voteCount": 4,
        "content": "This question is dumb AF, like it doesnt really matter if you create dataset or gateway first, all you need to remember is to have it ready before running refresh,"
      },
      {
        "date": "2024-02-23T07:45:00.000Z",
        "voteCount": 5,
        "content": "This was on the exam on 22/2/2024 (:"
      },
      {
        "date": "2024-01-20T01:12:00.000Z",
        "voteCount": 3,
        "content": "I don't understand what \"add dataset owner to the data source\" does. If the \"data source\" is the on-prem SQL DB and \"you\" are or are going to be the owner of the dataset, shouldn't you be added to the SQL DB first to be able to set it up in the gateway?"
      },
      {
        "date": "2023-09-04T13:36:00.000Z",
        "voteCount": 4,
        "content": "Here are the correct Sequences: \nFirst Step : Add a data source. This involves specifying the connection details, such as the server's name, database name, and authentication credentials.\n\nSecond Step: Configure an on-premises data gateway. The gateway acts as a bridge between our on-premises data sources and the Power BI service in the cloud. It allows secure data transfer and access.\n\nThird Step: Add the dataset owner to the data source. Ensure that the dataset owner (the Power BI user or service account) has appropriate permissions to access the on-premises SQL server database. This is important for successful data retrieval during refresh.\n\nFourth Step: Configure a scheduled refresh. Schedule the refresh to occur daily to keep the dataset up to date."
      },
      {
        "date": "2023-09-27T02:29:00.000Z",
        "voteCount": 3,
        "content": "Nope, the first step should be Configure data gateway. then, in second step we could have ability to add data source.\n"
      },
      {
        "date": "2024-04-15T00:43:00.000Z",
        "voteCount": 2,
        "content": "That was on video..."
      },
      {
        "date": "2023-07-29T04:12:00.000Z",
        "voteCount": 5,
        "content": "SETUP ON PREMISES DATA GATEWAY\nADD DATA SOURCE\nADD DATASET OWNER\nSCHEDULE REFRESH"
      },
      {
        "date": "2023-07-04T23:34:00.000Z",
        "voteCount": 1,
        "content": "comparing to the given answer, i would select add data owner as the last one - https://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/5-dataset-refresh"
      },
      {
        "date": "2024-01-07T22:26:00.000Z",
        "voteCount": 1,
        "content": "You cannot schedule a refresh if the data owner is not established."
      },
      {
        "date": "2023-03-23T18:56:00.000Z",
        "voteCount": 4,
        "content": "One thing I didn't understand is that first we need to add data source and then configure an on-premises data gateway. Someone please help me understand why we are not following this order?"
      },
      {
        "date": "2024-07-20T11:52:00.000Z",
        "voteCount": 1,
        "content": "FIRST DONWLOAD THE DATEGATEWAY FROM POWER BI .COM"
      },
      {
        "date": "2023-03-23T09:02:00.000Z",
        "voteCount": 3,
        "content": "1. Configure una puerta de enlace de datos local.\n2. Agregue un origen de datos.\n3. Agregue el propietario del conjunto de datos al origen de datos.\n4. Configure una actualizaci\u00f3n programada."
      },
      {
        "date": "2023-02-28T01:12:00.000Z",
        "voteCount": 4,
        "content": "Could anyone provide a link to this? Seem like this requires pragmatic experience"
      },
      {
        "date": "2023-04-21T04:10:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/4-power-bi-gateway\nand the study page after that\nhttps://learn.microsoft.com/en-us/training/modules/manage-datasets-power-bi/5-dataset-refresh\nIt does not necessarily talk about this order, but it does help to understand how this works. With those pages I was able to logically put the four things in the correct order."
      },
      {
        "date": "2023-02-18T06:55:00.000Z",
        "voteCount": 2,
        "content": "Any good link for this topic?"
      },
      {
        "date": "2023-02-14T22:25:00.000Z",
        "voteCount": 1,
        "content": "Why Add a data source must be before Add dataset owner to data source?"
      },
      {
        "date": "2023-03-16T10:21:00.000Z",
        "voteCount": 6,
        "content": "You first need to add a source to the gateway and then give permission to that source.\nWithout adding the source to the gw list there is nothing to give access to"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94599-exam-pl-300-topic-1-question-18-discussion/",
    "body": "You attempt to connect Power BI Desktop to a Cassandra database.<br><br>From the Get Data connector list, you discover that there is no specific connector for the Cassandra database.<br><br>You need to select an alternate data connector that will connect to the database.<br><br>Which type of connector should you choose?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tODBC\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOLE DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOData"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 59,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-10T05:40:00.000Z",
        "voteCount": 51,
        "content": "B is Correct because, B\u00b4cause it allows you to connect to data sources that aren't identified in the Get Data lists.\n\nThe ODBC connector lets you import data from any third-party ODBC driver simply by specifying a Data Source Name (DSN) or a connection string. As an option, you can also specify a SQL statement to execute against the ODBC driver.\nList details a few examples of data sources to which Power BI Desktop can connect by using the generic ODBC interface:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-using-generic-interfaces"
      },
      {
        "date": "2023-03-13T06:09:00.000Z",
        "voteCount": 32,
        "content": "The anwer is B\n* Cassandra has an ODBC driver available that can be used to connect to the database using the ODBC connector in Power BI.\n\n* Microsoft SQL Server database is specifically designed to connect to SQL Server databases, \n\n* OLE DB is designed to connect to Microsoft databases and other third-party databases, \n\n* OData is designed to connect to web-based data sources"
      },
      {
        "date": "2024-08-14T07:12:00.000Z",
        "voteCount": 1,
        "content": "ODBC is correct"
      },
      {
        "date": "2024-07-21T03:15:00.000Z",
        "voteCount": 1,
        "content": "ODBC allows to connect other DB"
      },
      {
        "date": "2024-03-07T06:00:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-04T13:42:00.000Z",
        "voteCount": 4,
        "content": "B is the correct answer.\nODBC (Open Database Connectivity) is a general-purpose data access that allows us to connect to a wide range of databases, including Cassandra, using ODBC drivers."
      },
      {
        "date": "2023-08-26T08:10:00.000Z",
        "voteCount": 1,
        "content": "Open database connector"
      },
      {
        "date": "2023-07-29T04:14:00.000Z",
        "voteCount": 1,
        "content": "odbc allows connection to third party database"
      },
      {
        "date": "2023-07-07T00:15:00.000Z",
        "voteCount": 4,
        "content": "This was on the exam"
      },
      {
        "date": "2023-07-06T06:58:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-06-10T04:17:00.000Z",
        "voteCount": 4,
        "content": "To connect Power BI Desktop to a Cassandra database when there is no specific connector available, you should choose option B: ODBC (Open Database Connectivity) connector.\n\nODBC is a widely-used standard for connecting to various types of databases, including Cassandra. It provides a common interface that allows applications like Power BI to communicate with different database systems using the same API. By leveraging the ODBC connector, you can establish a connection to the Cassandra database and retrieve data for analysis and visualization in Power BI."
      },
      {
        "date": "2023-06-08T11:54:00.000Z",
        "voteCount": 1,
        "content": "La respuesta es correcta :)"
      },
      {
        "date": "2023-05-17T07:00:00.000Z",
        "voteCount": 8,
        "content": "Awful question. This is a prime example of why the current exam, is not fit for purpose and needs revamped. How is this relevant to the day-to-day role of a data analyst?"
      },
      {
        "date": "2024-06-26T01:29:00.000Z",
        "voteCount": 1,
        "content": "Totally agree. There are too many questions should be related to the data engineer instead of data analyst...I don't need to know this to provide a good analysis to solve clients' business problems..."
      },
      {
        "date": "2023-05-01T09:14:00.000Z",
        "voteCount": 1,
        "content": "Microsoft open database connectivity"
      },
      {
        "date": "2023-05-01T09:13:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-03-23T09:06:00.000Z",
        "voteCount": 1,
        "content": "B. ODBC"
      },
      {
        "date": "2023-01-09T08:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-connect-using-generic-interfaces"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94600-exam-pl-300-topic-1-question-19-discussion/",
    "body": "DRAG DROP<br> -<br><br>You receive annual sales data that must be included in Power BI reports.<br><br>From Power Query Editor, you connect to the Microsoft Excel source shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image281.png\"><br><br>You need to create a report that meets the following requirements:<br><br>\u2022\tVisualizes the Sales value over a period of years and months<br>\u2022\tAdds a slicer for the month<br>\u2022\tAdds a slicer for the year<br><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/pl-300/image282.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image283.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-10T05:47:00.000Z",
        "voteCount": 57,
        "content": "Correct Answer!"
      },
      {
        "date": "2023-09-04T14:36:00.000Z",
        "voteCount": 29,
        "content": "Here are the correct three actions should be performed in sequence:\nAction 1: Select the Month and MonthNumber Columns. These columns will be used for the slicers to filter the data by month.\nAction 2: Select unpivot other columns. This action will transform the 2019, 2020, and 2021 columns into rows, creating a column called \"Attribute\" that contains the years and a column called \"Value\" that contains the sales data. This step makes the data more suitable for visualization and filtering by year.\nAction 3: Rename the Attribute column as Year and the value column as sales. Renaming the columns provides a more descriptive and meaningful structure for your data.\nAfter performing these actions, your data will be in a format that allows you to create visuals and add slicers for the month and year in Power BI."
      },
      {
        "date": "2024-09-10T19:50:00.000Z",
        "voteCount": 2,
        "content": "on the exam 11/9/24 and I got this right surprisingly. Pivot does my head in."
      },
      {
        "date": "2024-08-14T07:17:00.000Z",
        "voteCount": 1,
        "content": "In my opinion:\n1 - Select Month and MonthNumber columns\n2 - Select Unpivot other columns\n3 - Rename the attribute column as year and the value column as sales"
      },
      {
        "date": "2024-07-21T03:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2023-09-11T01:37:00.000Z",
        "voteCount": 5,
        "content": "i was tricked by \"unpivot OTHER COLUMNS\" do take note!"
      },
      {
        "date": "2023-07-04T14:30:00.000Z",
        "voteCount": 1,
        "content": "Select Month &amp; MonthNo &gt; Unpivot other columns &gt; Rename attribute (Year) and values (Sales)"
      },
      {
        "date": "2023-05-01T09:16:00.000Z",
        "voteCount": 3,
        "content": "answer is correct!"
      },
      {
        "date": "2023-04-21T04:35:00.000Z",
        "voteCount": 11,
        "content": "Answer is correct! Transpose would cause the rows and columns to swap, so the months would become columns and the years would become rows. And, as another person here already stated, for an Unpivot you select the columns that you would like to remain the same, not the rows you want transformed."
      },
      {
        "date": "2023-03-23T09:16:00.000Z",
        "voteCount": 2,
        "content": "1. Seleccione las columnas Mes y MonthNumber.\n2. Seleccione Despivotar otras columnas.\n3. Cambie el nombre de la columna de atributos as A\u00f1o y la columna Valor a Ventas."
      },
      {
        "date": "2023-02-07T17:06:00.000Z",
        "voteCount": 1,
        "content": "The first action is to select the columns for 2019, 2020, and 2021."
      },
      {
        "date": "2023-02-08T02:35:00.000Z",
        "voteCount": 4,
        "content": "Not really. You first select the first two columns, then unpivot other columns (which are the year columns) Then you rename the output"
      },
      {
        "date": "2023-01-22T12:29:00.000Z",
        "voteCount": 1,
        "content": "That's correct"
      },
      {
        "date": "2023-01-09T12:53:00.000Z",
        "voteCount": 2,
        "content": "It's correct!"
      },
      {
        "date": "2023-01-09T09:06:00.000Z",
        "voteCount": 1,
        "content": "Provided answer is correct."
      },
      {
        "date": "2023-01-09T08:56:00.000Z",
        "voteCount": 1,
        "content": "Correct. A, B and C"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94587-exam-pl-300-topic-1-question-20-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are using Power BI Desktop to connect to an Azure SQL database.<br><br>The connection is configured as shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image284.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br><br>NOTE: Each correct solution is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image285.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image286.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-10T06:35:00.000Z",
        "voteCount": 64,
        "content": "10min\nOnly tables with data\nIf navigate using full hierarchy is unchecked you can see only tables(rows n columns) with data. Otherwise you can see all tables"
      },
      {
        "date": "2024-01-28T20:21:00.000Z",
        "voteCount": 7,
        "content": "If you only follow the documentation from Microsoft, it is open to interpretation unless you stepped through the process.\n\nI cannot add a screenshot in this post but use the this link to use as a reference on how exactly the Navigator would look like using with the  \"Navigate using full hierarchy\" is checked and unchecked.\n\nI connected to an Azure SQL Database with tables containing data and others are blank.  Marking the \"Navigate using full hierarchy\" will list the Schemas first and the hierarchy for each object under each schema (e.g. dbo, db_onwer, etc.).\n\nUnmarking the \"Navigate using full hierarchy\" will show all the tables whether with data or without data."
      },
      {
        "date": "2023-01-10T06:09:00.000Z",
        "voteCount": 25,
        "content": "The defaut time out is 10 minutes, but if it takes more than it you can enter another value in minutes to keep the connection open longer.\n\n1. 10 minutes\n2. All the tables\n\nReference:\nhttps://learn.microsoft.com/en-us/power-query/connectors/azuresqldatabase"
      },
      {
        "date": "2024-05-19T12:30:00.000Z",
        "voteCount": 3,
        "content": "Wrong, as per Microsoft-&gt;  If cleared, the navigator displays only the tables whose columns and rows contain data (copied from MS)"
      },
      {
        "date": "2024-03-11T03:36:00.000Z",
        "voteCount": 5,
        "content": "it says in your reference link for \"Navigate using full hierarchy\"  : \nIf checked, the navigator displays the complete hierarchy of tables in the database you're connecting to. If cleared, the navigator displays only the tables whose columns and rows contain data. \nWhich means, it'll only show the tables with data and not all tables."
      },
      {
        "date": "2023-01-10T23:45:00.000Z",
        "voteCount": 15,
        "content": "Navigate using full hierarchy is unchecked. Only table with data will be displayed."
      },
      {
        "date": "2023-01-20T17:36:00.000Z",
        "voteCount": 28,
        "content": "Only tables with data. \nFrom your reference link, under Connect using advanced options, it clearly shows that  \"Navigate using full hierarchy \tIf checked, the navigator displays the complete hierarchy of tables in the database you're connecting to. If cleared, the navigator displays only the tables whose columns and rows contain data.\""
      },
      {
        "date": "2023-04-03T00:46:00.000Z",
        "voteCount": 3,
        "content": "True, I have seen it"
      },
      {
        "date": "2024-08-14T07:18:00.000Z",
        "voteCount": 1,
        "content": "10 minutes, only tables that contain data"
      },
      {
        "date": "2024-07-21T03:23:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2024-06-19T02:47:00.000Z",
        "voteCount": 1,
        "content": "10 MINUTES \nONLY TABLES WITH DATA"
      },
      {
        "date": "2024-02-25T10:10:00.000Z",
        "voteCount": 4,
        "content": "The solution is correct. \n10 minutes default timeout\nOnly tables that contain data\n\nreference to \nhttps://learn.microsoft.com/en-us/power-query/connectors/azure-sql-database"
      },
      {
        "date": "2023-10-19T11:56:00.000Z",
        "voteCount": 2,
        "content": "10 Min.\n \nAll Tables.\n\nI work with DirectQuery in my job."
      },
      {
        "date": "2023-09-14T05:09:00.000Z",
        "voteCount": 2,
        "content": "and what does fialeover do?"
      },
      {
        "date": "2023-09-04T18:02:00.000Z",
        "voteCount": 2,
        "content": "The correct answers are: \n** 10 minutes . The default timeout for the connection from Power BI Desktop to Azure SQL database is 10 minutes.\n** All the tables : the navigator will display all the tables available in the specified database, regardless of whether they contain data or hierarchies."
      },
      {
        "date": "2023-08-22T01:33:00.000Z",
        "voteCount": 4,
        "content": "Correct!: \n\nCommand timeout in minutes:\tIf your connection lasts longer than 10 minutes (the default timeout), you can enter another value in minutes to keep the connection open longer. This option is only available in Power Query Desktop.\n\nNavigate using full hierarchy:\tIf checked, the navigator displays the complete hierarchy of tables in the database you're connecting to. If cleared, the navigator displays only the tables whose columns and rows contain data.\n\nInclude relationship columns:\tIf checked, includes columns that might have relationships to other tables. If this box is cleared, you won\u2019t see those columns.\n\nhttps://learn.microsoft.com/en-us/power-query/connectors/azure-sql-database"
      },
      {
        "date": "2023-08-12T01:51:00.000Z",
        "voteCount": 6,
        "content": "Answer is correct:\n10 minutes (default)\nOnly tables with Data\n\nNavigate using full hierarchy: If checked, the navigator displays the complete hierarchy of tables in the database you're connecting to. If cleared, the navigator displays only the tables whose columns and rows contain data.\n\nhttps://learn.microsoft.com/en-us/power-query/connectors/azure-sql-database"
      },
      {
        "date": "2023-07-23T00:58:00.000Z",
        "voteCount": 2,
        "content": "Is the answer ALL Tables or Only tables that contain data? I think the real confusing is there?"
      },
      {
        "date": "2023-05-13T04:14:00.000Z",
        "voteCount": 4,
        "content": "Yes answer is 10 min , only tables with data as this is azure sql server not on premises sql server  \nhttps://learn.microsoft.com/en-us/power-query/connectors/azure-sql-database"
      },
      {
        "date": "2023-05-07T15:22:00.000Z",
        "voteCount": 5,
        "content": "Please ignore my first answer, after reviewing in detail\n\nCorrect answer is 10 Minutes and Only Table with Data\n\nIf \"Navigate using full hierarchy\" is unchecked in the Fields pane in Power BI Desktop, then only the tables and fields that contain data will be displayed in the report view. Any tables or fields that do not have data will be hidden from view.\n\nHowever, it's important to note that this option controls the behavior of the Fields pane in the report view, and does not affect the data that is loaded into the report. Even if the option is unchecked, all tables and fields in the data model will still be available for use in the report. They just won't be visible in the Fields pane unless they contain data.\n\nSo, if you want to ensure that only the tables and fields with data are shown in the report view, you can uncheck \"Navigate using full hierarchy\" in the Fields pane. But if you want to include all tables and fields in the data model, even if they don't currently contain data, you should leave the option checked."
      },
      {
        "date": "2023-05-07T15:21:00.000Z",
        "voteCount": 1,
        "content": "So, if you want to ensure that only the tables and fields with data are shown in the report view, you can uncheck \"Navigate using full hierarchy\" in the Fields pane. But if you want to include all tables and fields in the data model, even if they don't currently contain data, you should leave the option checked."
      },
      {
        "date": "2023-05-01T09:35:00.000Z",
        "voteCount": 1,
        "content": "Navigate using full hierarchy\tIf checked, the navigator displays the complete hierarchy of tables in the database you're connecting to. If cleared, the navigator displays only the tables whose columns and rows contain data."
      },
      {
        "date": "2023-05-01T09:36:00.000Z",
        "voteCount": 1,
        "content": "Include relationship columns\tIf checked, includes columns that might have relationships to other tables. If this box is cleared, you won\u2019t see those columns."
      },
      {
        "date": "2023-04-04T22:23:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer 10 Minutes and All tables\n\nThe Navigator window in Power BI will display all tables available in the data source, regardless of whether or not they contain data. However, when you preview the data in the Navigator window, only the tables that have data will display data in the preview.\n\nWhen you connect to a data source in Power BI, the Navigator window will typically display a list of tables, views, and other objects available in the data source. This list may include tables that are empty or have no data.\nNo confusion, and no need to further discuss"
      },
      {
        "date": "2023-04-21T04:45:00.000Z",
        "voteCount": 2,
        "content": "I would say there is a need to further discuss it. The documentation about Azure SQL database states that if full hierarchies is unchecked, 'the navigator displays only the tables whose columns and rows contain data', as others have already stated and linked to. What you describe is my experience with Microsoft SQL Server as well, but Azure SQL database appears to be an exception."
      },
      {
        "date": "2023-04-21T04:48:00.000Z",
        "voteCount": 4,
        "content": "I think the real confusion is, should you answer what the documentation tells you or should you answer based on experience (because some have apparently tested it and agree with you) :)"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104626-exam-pl-300-topic-1-question-21-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have the Azure SQL databases shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image312.png\"><br><br>You plan to build a single PBIX file to meet the following requirements:<br><br>\u2022\tData must be consumed from the database that corresponds to each stage of the development lifecycle.<br>\u2022\tPower BI deployment pipelines must NOT be used.<br>\u2022\tThe solution must minimize administrative effort.<br><br>What should you do? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image313.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image314.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-01T06:22:00.000Z",
        "voteCount": 97,
        "content": "Given answer is correct \n\nTo meet the requirements specified, we can use a single parameter in the PBIX file that controls which database is used for data consumption based on the stage of the development lifecycle.\n\nWe can use a Text parameter type in Power BI to achieve this. The parameter can be used to switch between the different database connections when a user interacts with the report. The text parameter could include values such as \"Development\", \"Staging\", and \"Production\", which correspond to the different databases shown in the table.\n\nThe parameter can then be used in the queries to dynamically filter the data based on the selected stage of the development lifecycle. By using a single parameter, we can minimize administrative effort and ensure that the report works with each stage of the development lifecycle.\n\nNo confusion, and no need to discuss further"
      },
      {
        "date": "2024-09-05T15:31:00.000Z",
        "voteCount": 1,
        "content": "no not correct"
      },
      {
        "date": "2023-06-07T19:32:00.000Z",
        "voteCount": 4,
        "content": "Gracias por tu explicaci\u00f3n :)"
      },
      {
        "date": "2023-05-19T01:00:00.000Z",
        "voteCount": 8,
        "content": "Why not two parameters, one for the name and other for the server?"
      },
      {
        "date": "2023-06-11T03:23:00.000Z",
        "voteCount": 3,
        "content": "How will give this parameter in the source step of the queries for tables? we need to give server and DB name?"
      },
      {
        "date": "2024-01-12T21:45:00.000Z",
        "voteCount": 54,
        "content": "It is a trick question.  You have to pay attention to the names of the databases and the servers.\n\nDatabases:\ndb-powerbi-dev\ndb-powerbi-uat\ndb-powerbi-prod\n\nServers:\ndev.database.windows.net\nuat.database.windows.net\nprod.database.windows.net\n\nThe only differences among the 3 names are \"dev\", \"uat\", and \"prod\".\n\nCreate a single parameter as Text and Suggested value as List of Values.\n\nYour list of values will be:\ndev\nuat\nprod\n\nIn the Power Query editor use the parameter value and concatenate it with the rest of the database and server name.\n\nSource = Sql.Database(#\"ParameterName\"&amp;\".database.windows.net\", \"db-powerbi-\"&amp;#\"ParameterName\")\n\nIf the Parameter Value selected from the list is \"uat\", the connection is interpreted as:\nSource = Sql.Database(\"uat.database.windows.net\", \"db-powerbi-uat\")\n\nI hope that clarifies the answer."
      },
      {
        "date": "2023-04-13T02:26:00.000Z",
        "voteCount": 21,
        "content": "Create: 2 parameters\nParameter type: text\n"
      },
      {
        "date": "2023-09-01T13:40:00.000Z",
        "voteCount": 2,
        "content": "why do you think about server name? just deal with three different database name with one parameter. That's it!"
      },
      {
        "date": "2023-04-21T04:59:00.000Z",
        "voteCount": 3,
        "content": "I would agree if this table was not available. In this case, both the server and the database name are tied to the test stage, so that column encompasses both. SanaCanada explains very well how to accomplish the connection with just one parameter."
      },
      {
        "date": "2023-05-02T00:00:00.000Z",
        "voteCount": 2,
        "content": "would you please explain more?"
      },
      {
        "date": "2024-09-05T15:30:00.000Z",
        "voteCount": 1,
        "content": "The given database names are different from each environment as well as the server name.\nSo it needs two parameters, if the given database names are same then we only need one parameter for the server name."
      },
      {
        "date": "2024-08-14T07:24:00.000Z",
        "voteCount": 1,
        "content": "We need one-text parameter in order to switch the various environments (dev, test, prod)"
      },
      {
        "date": "2024-07-21T03:25:00.000Z",
        "voteCount": 1,
        "content": "Correct , create Parameter for 2 Environment"
      },
      {
        "date": "2024-06-19T02:52:00.000Z",
        "voteCount": 1,
        "content": "To meet the requirements specified, we can use a single parameter in the PBIX file that controls which\ndatabase is used for data consumption based on the stage of the development lifecycle.\nWe can use a Text parameter type in Power BI to achieve this. The parameter can be used to switch between\nthe different database connections when a user interacts with the report. The text parameter could include\nvalues such as \"Development\", \"Staging\", and \"Production\", which correspond to the different databases\nshown in the table.\nThe parameter can then be used in the queries to dynamically filter the data based on the selected stage of\nthe development lifecycle. By using a single parameter, we can minimize administrative effort and ensure that\nthe report works with each stage of the development lifecycle"
      },
      {
        "date": "2024-01-04T07:17:00.000Z",
        "voteCount": 3,
        "content": "one param approach will work only if , each full connection string can be prepared like  &lt;database&gt;. &lt;other part of URL&gt;.  in that case,  3 env.s can be switched as per need.  if both cannot be combined ina single entity, then 2 params are needed.  ( I dont have exact idea abot how the connection gets created in textuual form internally ). kindly confirm if i am wrong and upvote if correct."
      },
      {
        "date": "2023-12-24T08:33:00.000Z",
        "voteCount": 1,
        "content": "I think coz db name is an optional param, one would do for the server name alone"
      },
      {
        "date": "2023-12-12T13:48:00.000Z",
        "voteCount": 1,
        "content": "Connection is done with two different fields \"Server\" and \"Database\", so you will need also two parameters for that. Answer should be 2 parameters, both text."
      },
      {
        "date": "2023-09-16T16:53:00.000Z",
        "voteCount": 4,
        "content": "Correct Answers: One Parameter / Text\n\nIn Power Query, in Advance Editor you can use 1 parameter to update the stage of the server and the database by modifying the M Code as follows:\n\n'=Sql.Database(parameter&amp;\"ServerUrl\", \"DatabaseName\"&amp;parameter)'"
      },
      {
        "date": "2024-01-04T07:07:00.000Z",
        "voteCount": 1,
        "content": "But in such solution, when the database name changes from dev to uat to prod, paramter vales will place the URL of dev uat and prod, but how database name will change automatically? ( i.e. suppose you statically give database name as dev db name, then param value comes as uat env URL.   then how this will work ?  hencce I feel 2 params are needed."
      },
      {
        "date": "2023-09-04T18:17:00.000Z",
        "voteCount": 1,
        "content": "The correct answers are Two Parameters and Text. \n** Two Parameters: it is because we have three stages; Development, Test, and Production, which means we need parameters to dynamically switch between these stages. We have two columns in the table that are relevant for the scenario, \"Stage\" (to determine the stage) and \"Server URL\" (to specify the database server URL).\n** Text: Text parameter is suitable for the scenario because we want to use them to switch between different server URLs based on the selected stage."
      },
      {
        "date": "2024-03-14T18:23:00.000Z",
        "voteCount": 1,
        "content": "No NO NO, the answer is right.: 1 Parameter type text, and then when you configure it show an option to LIST, in the list you add each environment."
      },
      {
        "date": "2024-01-17T10:42:00.000Z",
        "voteCount": 1,
        "content": "You can accomplish this with 1 parameter instead. It would contain \"Dev\" \"uat\" and \"Prod\", and you can apply it to both columns"
      },
      {
        "date": "2023-07-30T01:43:00.000Z",
        "voteCount": 1,
        "content": "I think we just need 1 parameter because all the databases have different names and url to represent each of the stages. Using 2 parameters is waste of resources as just 1 parameter can serve the purpose. In this case we can chose to use either the database name or url.  \nThe stage column is only an additional information to clarify each database name or url.\n\nI hope this is clear."
      },
      {
        "date": "2023-07-03T02:02:00.000Z",
        "voteCount": 2,
        "content": "1 Parameter with \"List of Values\""
      },
      {
        "date": "2023-06-16T23:54:00.000Z",
        "voteCount": 4,
        "content": "SanaCanada is correct here only one parameter is needed. With one parameter with the parameter type of \"Text\" and Suggested Values to be \"List of values\" you can set three values as dev, uat, and prod.\n\nIn the Azure they contain the same structure for the Name and Server URL.\n\"db-powerbi-\" &amp; \".database.windows.net\"\n\nThe only thing that changes between the stages is dev, uat, and prod and for each of the stages the Name and Server URL is the same value for that stage. E.g, \"db-powerbi-dev\" &amp; \"dev.database.windows.net\""
      },
      {
        "date": "2023-06-29T07:17:00.000Z",
        "voteCount": 1,
        "content": "To be a bit more precise, when modifying the source in Power Query,  we can click on \"Advanced\" and modify the source URL for a URL that contains both text and parameters."
      },
      {
        "date": "2023-06-30T00:09:00.000Z",
        "voteCount": 1,
        "content": "Those of you who are saying one, have you tried it yourself?\nI am not able to edit the code in advanced editor using one parameter.\nI would go with 2 parameters."
      },
      {
        "date": "2023-05-13T15:09:00.000Z",
        "voteCount": 2,
        "content": "Please share resources from where this is referenced."
      },
      {
        "date": "2023-04-17T06:17:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-05-02T00:00:00.000Z",
        "voteCount": 2,
        "content": "why? can you explain why not 2?"
      },
      {
        "date": "2023-03-31T11:34:00.000Z",
        "voteCount": 1,
        "content": "I think we need to create three parameter for, dev test, prod ?"
      },
      {
        "date": "2023-05-01T13:28:00.000Z",
        "voteCount": 4,
        "content": "No, we just need one text parameter, that will reflect what is the stage of development. One parameter can have multiple values (dev, test, prod)"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104453-exam-pl-300-topic-1-question-22-discussion/",
    "body": "You are creating a query to be used as a Country dimension in a star schema.<br><br>A snapshot of the source data is shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image315.png\"><br><br>You need to create the dimension. The dimension must contain a list of unique countries.<br><br>Which two actions should you perform? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the Country column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove duplicates from the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove duplicates from the City column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the City column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove duplicates from the Country column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-30T03:24:00.000Z",
        "voteCount": 33,
        "content": "DE is correct we only need the countries"
      },
      {
        "date": "2023-03-30T02:23:00.000Z",
        "voteCount": 10,
        "content": "Agree\nThe table has tocontain unique values for \"Country\" column, so\n- delete the city column --&gt; in fact this column is not requested\n- Remove dupicates from the Country column"
      },
      {
        "date": "2024-09-27T23:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-08-14T07:25:00.000Z",
        "voteCount": 1,
        "content": "DE is correct we first remove the city column than we remove duplicates from the Country column"
      },
      {
        "date": "2024-08-13T08:03:00.000Z",
        "voteCount": 1,
        "content": "Correct answers are D and E, as they mentionned in the question, the need to create a \"list of unique countries\" so it will not make sense in removing only duplicates from country and leaving a column of uncompleted cities, so it's necessary to delete this column."
      },
      {
        "date": "2024-07-23T02:57:00.000Z",
        "voteCount": 2,
        "content": "Remove duplicate from a specific column does Not exist in Power BI, the action that does exist is Remove Duplicates."
      },
      {
        "date": "2024-07-21T03:28:00.000Z",
        "voteCount": 1,
        "content": "De is correct"
      },
      {
        "date": "2024-02-22T02:57:00.000Z",
        "voteCount": 6,
        "content": "DB should also be accepted since the table has only one row after deleting City column."
      },
      {
        "date": "2024-10-03T17:37:00.000Z",
        "voteCount": 1,
        "content": "DE will work in whichever order it is executed. But, DB will give expected result only if D is executed first. As the order of execution is open ended in this question, I'd say DE would be the better answer."
      },
      {
        "date": "2024-02-01T19:23:00.000Z",
        "voteCount": 4,
        "content": "DE is correct, but feel like BD could also work, since it technically does the same thing"
      },
      {
        "date": "2023-11-22T00:10:00.000Z",
        "voteCount": 2,
        "content": "It was so easy that I was suspicious..."
      },
      {
        "date": "2024-03-18T23:51:00.000Z",
        "voteCount": 2,
        "content": "one could say it was stress free"
      },
      {
        "date": "2023-11-10T03:52:00.000Z",
        "voteCount": 1,
        "content": "CONFUSING OPTIONS....BEST OPTION IS \nNEWCOUNTRY=ALLNOBLANKROW(COUNTRY[COUNTRY])"
      },
      {
        "date": "2024-02-01T19:23:00.000Z",
        "voteCount": 1,
        "content": "yeah no"
      },
      {
        "date": "2023-11-09T02:42:00.000Z",
        "voteCount": 1,
        "content": "DE is correct"
      },
      {
        "date": "2023-11-07T05:35:00.000Z",
        "voteCount": 1,
        "content": "piece of cake d, e"
      },
      {
        "date": "2023-11-06T05:25:00.000Z",
        "voteCount": 2,
        "content": "D and E are correct."
      },
      {
        "date": "2023-10-19T12:01:00.000Z",
        "voteCount": 1,
        "content": "DE and BE Would Work. !!"
      },
      {
        "date": "2023-09-04T18:57:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E are the correct answers.\nB. Remove duplicates from the table: this action will ensure that only unique rows (combinations of Country and City) remain in the table, effectively removing any duplicate countries. \nE. Remove duplicates from the country column: This action specifically removes duplicates from the \"Country\" column, ensuring that we have a list of unique countries.\nB &amp; E actions, when performing in sequence, will give you a dimension with a list of unique countries, which is what we need for the star schema."
      },
      {
        "date": "2023-09-04T04:47:00.000Z",
        "voteCount": 2,
        "content": "DE or ED give you absolutely the same result. I hate questions like this because the test has only one correct answer :(\nEven more, \"delete the city column\" and \"remove duplicates from the table\" (step: Table.Distinct) it still correct.\nSo, we have three correct answers."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105081-exam-pl-300-topic-1-question-23-discussion/",
    "body": "DRAG DROP<br> -<br><br>You use Power Query Editor to preview the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image331.png\"><br><br>You need to clean and transform the query so that all the rows of data are maintained, and error values in the discount column are replaced with a discount of 0.05. The solution must minimize administrative effort.<br><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/pl-300/image332.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image333.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-11T23:59:00.000Z",
        "voteCount": 52,
        "content": "Order is correct, we need to correct the errors first. Thus we select the column and then replace errors then change the data type."
      },
      {
        "date": "2024-07-28T14:02:00.000Z",
        "voteCount": 4,
        "content": "I disagree with the order; we should change data type before replacing errors. Errors may arise from changing the data type and if this is done last the errors will remain on load."
      },
      {
        "date": "2023-07-04T14:55:00.000Z",
        "voteCount": 19,
        "content": "Select Discount &gt; Replace errors with 0.05 &gt; Set Discount type to Decimal"
      },
      {
        "date": "2024-08-14T07:28:00.000Z",
        "voteCount": 1,
        "content": "First of all: Select discount column,\nThen: Replace errors to replace each error value with 0.05 \nFinally: For the discount column change data type to decimal number"
      },
      {
        "date": "2024-07-21T03:36:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2024-05-02T03:27:00.000Z",
        "voteCount": 1,
        "content": "Change data type last because if the error is relating to the column not being convertable to decimal then that action will fail. If you replace to 0.05 first you are guaranteed it is convertable to decimal."
      },
      {
        "date": "2024-04-09T23:38:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is: Select Discount &gt; Replace errors with 0.05 &gt; Set Discount type to Decimal\n\nThis sequence does not work in P-BI: Select Discount &gt; Set Discount type to Decimal &gt; Replace errors with 0.05"
      },
      {
        "date": "2023-12-29T23:08:00.000Z",
        "voteCount": 3,
        "content": "This was so logical. No argument for this one."
      },
      {
        "date": "2023-09-13T20:11:00.000Z",
        "voteCount": 5,
        "content": "Is it incorrect if I set the Discount type to Decimal before replacing the errors?"
      },
      {
        "date": "2023-09-04T19:18:00.000Z",
        "voteCount": 2,
        "content": "The correct three consecutive actions are:\nAction 1: Select the discount column. This is the column we need to transform. \nAction 3: From the discount column, change data type to decimal Number. This step is necessary to work with numeric values.\nAction 5: Select replace error to replace each error value with 0.05. This will address the requirement of replacing error values with the desired discount value.\n\nAction 2: select the price column. Selecting the price column is not relevant to the requirement of cleaning and transforming the discount column.\nAction 4: From the discount column, change data type to whole number. Changing the data type of the discount column to a whole number is not appropriate since the discount values are decimal numbers, and you want to replace errors with 0.05, which is not a whole number."
      },
      {
        "date": "2023-12-09T09:23:00.000Z",
        "voteCount": 5,
        "content": "This order is correct:\n- Select the discount column\n- Change data type to decimal\n- Select Replace Errors\nTESTED"
      },
      {
        "date": "2023-09-20T23:38:00.000Z",
        "voteCount": 1,
        "content": "are you sure about the order?\nI mean, if i modify the type after the replacement, isn't the same?\nOr even better, because you're not causing other problems of types before the casting"
      },
      {
        "date": "2023-06-08T12:13:00.000Z",
        "voteCount": 2,
        "content": "De acuerdo con la respuesta"
      },
      {
        "date": "2023-05-11T05:44:00.000Z",
        "voteCount": 3,
        "content": "Why do we need to change the datatype?"
      },
      {
        "date": "2023-06-02T05:06:00.000Z",
        "voteCount": 1,
        "content": "its currently stored as text\nits a decimal value so store it as a decimal"
      },
      {
        "date": "2023-10-16T15:56:00.000Z",
        "voteCount": 1,
        "content": "yes , it is required to change the datatype of the column"
      },
      {
        "date": "2023-04-11T10:34:00.000Z",
        "voteCount": 1,
        "content": "Yes please. It would be great if someone explain the 5 &amp; 3 answer order. Assuming the answers are 1-5 top to bottom. Could/Should the answer be 3 &amp; 5 instead?"
      },
      {
        "date": "2023-04-11T10:34:00.000Z",
        "voteCount": 2,
        "content": "Oh no, I get it now. Of course, you want to get rid of the errors first. :)"
      },
      {
        "date": "2023-09-04T19:18:00.000Z",
        "voteCount": 1,
        "content": "The correct three consecutive actions are:\nAction 1: Select the discount column. This is the column we need to transform. \nAction 3: From the discount column, change data type to decimal Number. This step is necessary to work with numeric values.\nAction 5: Select replace error to replace each error value with 0.05. This will address the requirement of replacing error values with the desired discount value.\n\nAction 2: select the price column. Selecting the price column is not relevant to the requirement of cleaning and transforming the discount column.\nAction 4: From the discount column, change data type to whole number. Changing the data type of the discount column to a whole number is not appropriate since the discount values are decimal numbers, and you want to replace errors with 0.05, which is not a whole number."
      },
      {
        "date": "2023-04-04T21:35:00.000Z",
        "voteCount": 1,
        "content": "given answers are correct"
      },
      {
        "date": "2023-04-04T04:57:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105082-exam-pl-300-topic-1-question-24-discussion/",
    "body": "HOTSPOT<br> -<br><br>You attempt to use Power Query Editor to create a custom column and receive the error message shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image334.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image335.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image336.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-12T06:43:00.000Z",
        "voteCount": 38,
        "content": "Given answers are correct"
      },
      {
        "date": "2023-09-04T19:35:00.000Z",
        "voteCount": 14,
        "content": "Mismatched data types and A1 are the correct answers. The custom column expression is trying to concatenate (use the \"&amp;\" operator) a text value and a number value, which are mismatched data types. In this case, the left side of the operator is a text value (e.g.,\"A\"), and the right side is a number value (e.g.,1).\nTo achieve the desired outcome of the custom column as \"A1\", you should ensure that both sides of the \"&amp;\" operator have the same data type, which is text in this case."
      },
      {
        "date": "2024-10-02T18:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-08-14T07:31:00.000Z",
        "voteCount": 2,
        "content": "The error is caused by mismatched data types (text and number). The desired outcome of the custom column is A1. The given answer is OK"
      },
      {
        "date": "2024-07-21T03:40:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2023-12-20T12:54:00.000Z",
        "voteCount": 2,
        "content": "I tried \"A\"&amp;1 to create custom column . It gives the same error as shown in the picture."
      },
      {
        "date": "2023-05-13T15:13:00.000Z",
        "voteCount": 2,
        "content": "can someone please share the resource from where this is referenced?"
      },
      {
        "date": "2023-05-02T00:10:00.000Z",
        "voteCount": 5,
        "content": "Given answer is correct."
      },
      {
        "date": "2023-04-30T07:04:00.000Z",
        "voteCount": 2,
        "content": "Can someone please explain why is it A1?"
      },
      {
        "date": "2023-05-01T13:30:00.000Z",
        "voteCount": 19,
        "content": "In the error message, you can see that we are trying to concatenate two \"parts\", the left one (which is \"A\") and the right one (which is the number 1). The \"&amp;\" is how we are concatenating (in Power Query the syntax \"abc\"&amp;\"def\" will result in \"abcdef\""
      },
      {
        "date": "2023-04-29T15:59:00.000Z",
        "voteCount": 4,
        "content": "can someone explain the syntax? where did the \"&amp;\" come from? what does the { } mean\""
      },
      {
        "date": "2023-04-04T21:36:00.000Z",
        "voteCount": 3,
        "content": "given answers are correct"
      },
      {
        "date": "2023-04-04T04:58:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117349-exam-pl-300-topic-1-question-25-discussion/",
    "body": "From Power Query Editor, you attempt to execute a query and receive the following error message.<br><br>Datasource.Error: Could not find file.<br><br>What are two possible causes of the error? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou do not have permissions to the file.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn incorrect privacy level was used for the data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe file is locked.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe referenced file was moved to a new location.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-04T11:24:00.000Z",
        "voteCount": 50,
        "content": "So, the correct selections would be:\nA. You do not have permissions to the file.\nD. The referenced file was moved to a new location.\n\nOther options cause another message:\n\nB. If an incorrect privacy level is set for a data source, you might receive an error related to data privacy like: \"Formula.Firewall: Query 'QueryName' references other queries or steps, so it may not directly access a data source. Please rebuild this data combination.\"\n\nC. If a file is locked, for example because it is open in another application and that application has locked the file for exclusive access, the error message might be something like: \"DataSource.Error: The process cannot access the file because it is being used by another process.\""
      },
      {
        "date": "2023-08-11T07:16:00.000Z",
        "voteCount": 5,
        "content": "https://learn.microsoft.com/en-us/power-query/dealing-with-errors"
      },
      {
        "date": "2024-01-15T19:08:00.000Z",
        "voteCount": 1,
        "content": "Do you have any tips for navigating Microsoft Learn to find the right article in the exam environment?"
      },
      {
        "date": "2024-08-14T07:32:00.000Z",
        "voteCount": 1,
        "content": "Can be either \nA. You do not have permissions to the file. \nOR\nD. The referenced file was moved to a new location."
      },
      {
        "date": "2024-07-21T03:41:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2024-03-20T17:29:00.000Z",
        "voteCount": 2,
        "content": "AD Seems correct"
      },
      {
        "date": "2024-03-11T04:54:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-query/dealing-with-errors"
      },
      {
        "date": "2024-03-08T05:07:00.000Z",
        "voteCount": 2,
        "content": "Missmatched Data -&gt; A1"
      },
      {
        "date": "2024-02-19T08:12:00.000Z",
        "voteCount": 2,
        "content": "A: no permission on file or above because of heritance(folder/drive)\nD: file does not exist at mentioned location"
      },
      {
        "date": "2023-12-18T21:55:00.000Z",
        "voteCount": 2,
        "content": "A, D correct. references"
      },
      {
        "date": "2024-01-15T19:08:00.000Z",
        "voteCount": 1,
        "content": "Do you have any tips for navigating Microsoft Learn to find the right article in the exam environment?"
      },
      {
        "date": "2023-12-12T10:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2023-11-07T05:43:00.000Z",
        "voteCount": 1,
        "content": "A,D correct in B and C cases you will receive different errors"
      },
      {
        "date": "2023-09-04T19:42:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D are correct answers.\nA. You do not have permission to the file: if there is no permission to access the file, you will encounter this error. \nD. The referenced file was moved to a new location: if the file that that query is trying to access has been moved to a different location or path, Power Query won't be able to find it, resulting in this error. We should verify the file's location and update the data source settings accordingly."
      },
      {
        "date": "2023-08-06T03:59:00.000Z",
        "voteCount": 2,
        "content": "why A ? because error will be \"denied\" and not \"not found\""
      },
      {
        "date": "2023-08-05T15:34:00.000Z",
        "voteCount": 1,
        "content": "I would say A and D. "
      },
      {
        "date": "2023-08-05T07:05:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-08-04T12:18:00.000Z",
        "voteCount": 1,
        "content": "Two possible causes of the error Datasource.Error: Could not find file in Power BI are:\nThe referenced file was moved to a new location: If the file you are trying to connect to as a data source has been moved or renamed, Power BI will not be able to find it and will display this error message.\nThe file is locked: If the file is open by another person, it may be locked and cannot be accessed until it is closed."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117350-exam-pl-300-topic-1-question-26-discussion/",
    "body": "You have data in a Microsoft Excel worksheet as shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image341.png\"><br><br>You need to use Power Query to clean and transform the dataset. The solution must meet the following requirements:<br><br>\u2022\tIf the discount column returns an error, a discount of 0.05 must be used.<br>\u2022\tAll the rows of data must be maintained.<br>\u2022\tAdministrative effort must be minimized.<br><br>What should you do in Power Query Editor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Replace Errors.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the query in the Query Errors group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Remove Errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Keep Errors."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-04T20:03:00.000Z",
        "voteCount": 11,
        "content": "A. Select Replace Errors. is the correct answer. Because selecting \"Replace Errors\" allows you to replace any errors in the discount column with a specified value, which in the case is 0.05 as per the requirement.\nOption B is not necessary for this specific task. Option C would remove rows with errors entirely, which is not in line with the requirement to maintain all rows of data. Option D would keep rows with errors as they are, which is not what we want since we want to replace errors with a specific value."
      },
      {
        "date": "2023-08-12T10:06:00.000Z",
        "voteCount": 9,
        "content": "THE ANSWERE IS A"
      },
      {
        "date": "2024-08-14T07:34:00.000Z",
        "voteCount": 2,
        "content": "We can directly replace errors with 0.05. It is the quickest way to do that.\nA -Select Replace Errors is correct"
      },
      {
        "date": "2024-07-21T03:45:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2024-05-08T04:30:00.000Z",
        "voteCount": 2,
        "content": "A for me"
      },
      {
        "date": "2024-03-11T13:01:00.000Z",
        "voteCount": 1,
        "content": "Replace Errors"
      },
      {
        "date": "2023-12-15T01:04:00.000Z",
        "voteCount": 3,
        "content": "Replace the errors, pretty straight forward"
      },
      {
        "date": "2023-10-02T06:12:00.000Z",
        "voteCount": 4,
        "content": "c'est correct mon reuf"
      },
      {
        "date": "2023-08-06T04:22:00.000Z",
        "voteCount": 3,
        "content": "The correct option to achieve the requirements is:\n\nA. Select Replace Errors.\n\nBy selecting \"Replace Errors\" in Power Query Editor, you can specify a value (in this case, 0.05) to be used whenever an error is encountered in the specified column (discount column). This ensures that if the discount column returns an error, a discount of 0.05 will be used, while maintaining all rows of data and minimizing administrative effort."
      },
      {
        "date": "2023-08-05T03:44:00.000Z",
        "voteCount": 2,
        "content": "A - Select Replace Errors is correct"
      },
      {
        "date": "2023-08-04T11:33:00.000Z",
        "voteCount": 4,
        "content": "A. Select Replace Errors - is correct.  \n\nC&amp;D will remove some rows\nOption B, \"Edit the query in the Query Errors group\", would technically also allow to achieve the required result. However, this would not be the optimal solution given the constraints provided in the scenario, which specifies that administrative effort must be minimized."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122527-exam-pl-300-topic-1-question-27-discussion/",
    "body": "You have a CSV file that contains user complaints. The file contains a column named Logged. Logged contains the date and time each complaint occurred. The data in Logged is in the following format: 2018-12-31 at 08:59.<br><br>You need to be able to analyze the complaints by the logged date and use a built-in date hierarchy.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApply the Parse function from the Data transformations options to the Logged column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data type of the Logged column to Date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the Logged column by using at as the delimiter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a column by example that starts with 2018-12-31."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-05T07:28:00.000Z",
        "voteCount": 18,
        "content": "A: cause error. \nC: correct, although it lacks step change data type"
      },
      {
        "date": "2024-03-02T01:28:00.000Z",
        "voteCount": 4,
        "content": "Datatype is changed automatically you need not do it manually :)"
      },
      {
        "date": "2023-10-15T07:52:00.000Z",
        "voteCount": 10,
        "content": "Change Data Type step is not required since the data type gets updated automatically after splitting the column by delimiter"
      },
      {
        "date": "2023-12-11T20:23:00.000Z",
        "voteCount": 3,
        "content": "Yep, the answer is C and using delimiter \"at\" is the easiest path and will automatically adjust the column to date format upon hitting Ok.  Also, this method retains the time column in time format.  If time was not a requirement, then you might be better using Extract Table Using Examples to minimize the data imported."
      },
      {
        "date": "2024-01-02T15:26:00.000Z",
        "voteCount": 12,
        "content": "Question appears multiple times with different answer options, in this set of answers, C is correct."
      },
      {
        "date": "2024-10-03T02:45:00.000Z",
        "voteCount": 2,
        "content": "D. Create a column by example that starts with 2018-12-31 \nis the correct answer bcz if you add column from  example(i.e. 2018-12-31) then it gives only data format.\n\nAnswer C is wrong bcz if we use \"at as delimiter then the space before at will be added in the data format so the column will not automatically detect as a date format."
      },
      {
        "date": "2024-05-02T19:31:00.000Z",
        "voteCount": 4,
        "content": "Have you even tried? The space before 'at' will be removed automatically, that's the beauty of intelligence in Power BI. I've tried, both C and D options work. But for option D, the data type will be set to text and we need to add additional step to change data type, wherein for option C, Power BI automatically recognizes and changes the data type."
      },
      {
        "date": "2024-08-14T07:36:00.000Z",
        "voteCount": 2,
        "content": "This is similar to another question in which \"C - Split the Logged column by using at as the delimiter\" was the right answer. But why in this case \"D. Create a column by example that starts with 2018-12-31\" is wrong???"
      },
      {
        "date": "2024-07-21T03:47:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2024-05-27T15:20:00.000Z",
        "voteCount": 2,
        "content": "Tested. Data type as Date is set auto BUT\nCreate by example -  data type is not set to Date."
      },
      {
        "date": "2024-05-09T21:02:00.000Z",
        "voteCount": 1,
        "content": "C is the more reliable option"
      },
      {
        "date": "2024-05-08T04:30:00.000Z",
        "voteCount": 1,
        "content": "C for me"
      },
      {
        "date": "2024-05-04T21:24:00.000Z",
        "voteCount": 2,
        "content": "Microsoft Fabric said choice D."
      },
      {
        "date": "2024-05-02T03:47:00.000Z",
        "voteCount": 2,
        "content": "ANS. C   I tested D and it didn't change the data type to Date, it remained as text. I also tested C without using spaces before and after the 'at' and it DID change the data type to date.  The fact that there are two questions like this but C is common to both questions makes extra sure it is correct.  C!"
      },
      {
        "date": "2024-04-14T05:00:00.000Z",
        "voteCount": 2,
        "content": "d"
      },
      {
        "date": "2024-03-14T18:45:00.000Z",
        "voteCount": 1,
        "content": "be careful, this is the same question #10, and in 10 the right answer was D and here C. crazy"
      },
      {
        "date": "2024-03-26T10:06:00.000Z",
        "voteCount": 1,
        "content": "They've got different answers - In #10, D is the simplest, in this Q, C is"
      },
      {
        "date": "2024-02-24T10:32:00.000Z",
        "voteCount": 4,
        "content": "There is another question verbatim like this one and the answer is, \"extract the first 11 characters.\" Delimiter answer appears on both. Is it saying that extracting the characters is a better option if available? I don't understand."
      },
      {
        "date": "2024-02-19T08:33:00.000Z",
        "voteCount": 4,
        "content": "Tested in PBI with dummy data :\nA &amp; B will cause an error in the data --&gt; you will read \"error\" instead of original data\nD will truncate the date correctly but will keep the original string format (no hierarchy possible)\nC will split correctly and apply correct format on the fly, date for date and time for time"
      },
      {
        "date": "2024-01-01T20:26:00.000Z",
        "voteCount": 4,
        "content": "I tried. D is correct.\nIt is easy to create a new column only contains the date with Examples"
      },
      {
        "date": "2024-01-02T15:29:00.000Z",
        "voteCount": 2,
        "content": "D adds another column, C does not"
      },
      {
        "date": "2024-01-26T08:57:00.000Z",
        "voteCount": 2,
        "content": "C divise la colonne en 2 donc il y a bien une nouvelle colonne"
      },
      {
        "date": "2023-12-29T00:10:00.000Z",
        "voteCount": 5,
        "content": "Same question with #10, Topic 1. So the answer should be same which is C."
      },
      {
        "date": "2023-12-18T21:57:00.000Z",
        "voteCount": 3,
        "content": "C is correct. By splitting the Logged column, you can separate the date and time into two columns."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122512-exam-pl-300-topic-1-question-28-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have two Microsoft Excel workbooks in a Microsoft OneDrive folder.<br><br>Each workbook contains a table named Sales. The tables have the same data structure in both workbooks.<br><br>You plan to use Power BI to combine both Sales tables into a single table and create visuals based on the data in the table. The solution must ensure that you can publish a separate report and dataset.<br><br>Which storage mode should you use for the report file and the dataset file? To answer, drag the appropriate modes to the correct files. Each mode may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image354.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image355.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-10-07T02:06:00.000Z",
        "voteCount": 40,
        "content": "Report file: Import: In Power BI, when you import data, it means that the data is loaded into the Power BI Desktop file. In this case, you would import the data from both Excel workbooks into your Power BI Desktop report file. This allows you to create visuals and reports based on the imported data. Importing the data ensures that you can work with the data even when you're not connected to OneDrive.\n\nDataset: DirectQuery: To keep the data in OneDrive and maintain a live connection to the source, you should use DirectQuery for the dataset. DirectQuery allows Power BI to retrieve and query data from the original data source (in this case, the Excel workbooks in OneDrive) in real-time without importing it into the dataset. This ensures that your dataset is always up-to-date and reflects changes made to the source data."
      },
      {
        "date": "2024-09-16T07:12:00.000Z",
        "voteCount": 1,
        "content": "Wrong!!!"
      },
      {
        "date": "2024-07-28T12:33:00.000Z",
        "voteCount": 17,
        "content": "Your answer is wrong. When we read the question we understand that we need to import Excel workbooks using web connector and first of all our source data is Excel. For Excel data source, Direct Query mode is not supported even if the connector is web. Please read Microsoft's direct query supported resources article.\nTHE CORRECT ANSWER IS DEFINITELY REPORT FILE:IMPORT DATASET FILE:IMPORT\n\nNOTE: FRIENDS, PLEASE DON'T FORGET. HIGHLY VOTED ANSWERS ARE NOT ALWAYS CORRECT."
      },
      {
        "date": "2024-05-20T09:12:00.000Z",
        "voteCount": 9,
        "content": "Wrong answer, THERE IS NO DIRECT QUERY FOR EXCEL FILES, ONLY IMPORT MODE"
      },
      {
        "date": "2024-06-19T03:10:00.000Z",
        "voteCount": 1,
        "content": "Dataset: Direct Query.\nTo keep the data in OneDrive and maintain a live connection to the source, you should use Direct Query for the\ndataset. Direct Query allows Power BI to retrieve and query data from the original data source (in this case,\nthe Excel workbooks in OneDrive) in real-time without importing it into the dataset. This ensures that your\ndataset is always up-to-date and reflects changes made to the source data."
      },
      {
        "date": "2024-01-15T19:52:00.000Z",
        "voteCount": 4,
        "content": "Thanks for your answer! I am a bit confused by the need for direct query.\nI see this requirement: \"The solution must ensure that you can publish a separate report and dataset.\" So does using direct query stop the two semantic models from merging or something?\n\nThe question doesn't say anything about required real-time elements so I don't understand why direct query is needed unless it relates to the separation of report and dataset. \n\nOtherwise seems like something you would decide based on size of data source and the real time accuracy requirement from the business...\n\nBut I have never published a dataset without a report so I am not knowledgeable on the subject.."
      },
      {
        "date": "2024-03-25T23:09:00.000Z",
        "voteCount": 1,
        "content": "Load and Transform Data in Power Query. Close and Apply your power query Logic in PBI. Build your semantic model and publish it on service without any reports. Open a new PbI instance and connect to your pulished semantic model (via Get Data Menu and select Power BI semantic Models) and a connected live mode will be etablished. This is the best way to achive this requirement. Hope this will help."
      },
      {
        "date": "2023-11-03T06:14:00.000Z",
        "voteCount": 27,
        "content": "The question is not very clear.\nI would do it this way:\n1 Import by Dataset\n2 Live connect (to dataset) for report(s)."
      },
      {
        "date": "2024-10-07T18:24:00.000Z",
        "voteCount": 1,
        "content": "Excel file stored in one drive is not same as an excel file on your desktop. Live connect can be used with onedrive url to source the excel data. So the answer is live Connect for Dataset, and  import for Report file"
      },
      {
        "date": "2024-10-02T02:37:00.000Z",
        "voteCount": 1,
        "content": "Report file: Import\nDataset file: Import\n\nThe question said there are two Microsoft Excel workbooks in a Microsoft OneDrive Folder which requires an Excel connector (or Web). NOTE: Excel connector only supports the Import storage mode"
      },
      {
        "date": "2024-08-16T08:18:00.000Z",
        "voteCount": 1,
        "content": "Import, DirectQuery"
      },
      {
        "date": "2024-07-19T04:05:00.000Z",
        "voteCount": 3,
        "content": "1. Report file: LiveConnect\n2. Dataset file: Import"
      },
      {
        "date": "2024-06-27T04:50:00.000Z",
        "voteCount": 1,
        "content": "It's import to the dataset and then live connection to report\nthat's how we build it at work"
      },
      {
        "date": "2024-06-04T04:57:00.000Z",
        "voteCount": 1,
        "content": "Copilot says: B"
      },
      {
        "date": "2024-05-29T07:31:00.000Z",
        "voteCount": 2,
        "content": "You cannot create a DirectQuery / Live connection to an Excel spreadsheet per Microsoft's documentation on Power BI data sources. For excel it seems the mode is import."
      },
      {
        "date": "2024-05-09T21:05:00.000Z",
        "voteCount": 4,
        "content": "You need to publish the report and dataset separately, i.e. they are not in the same pbix.  The dataset would use import but once it is published into the Power BI Service the report uses a live connection to the dataset."
      },
      {
        "date": "2024-04-21T11:06:00.000Z",
        "voteCount": 9,
        "content": "In this video it says both are : Import\nI don't see anyone given this answer in the discussion."
      },
      {
        "date": "2024-04-17T13:57:00.000Z",
        "voteCount": 3,
        "content": "The answer provided is rubbish! \nA standalone report file uses a liveconnect stoage mode!!  The dataset imports."
      },
      {
        "date": "2024-03-30T10:39:00.000Z",
        "voteCount": 3,
        "content": "I would say its:\n\nreport file: import\ndataset file: liveconnect\n\nreport: this is because there were no requirement for real-time and due to the requirement's complexity (combining 2 tables into 1 table) THEN visualize the data. we use import to reduce their high complexity by setting report storage mode as import.\n\ndataset: due to the question's requirement \"create visuals based on the data in the table\", there were no semantic model in the picture. so, liveconnect will be the most suitable because liveconnect can create the visuals/report directly from the existing data without the need of semantic model (unlike directquery where it connects the semantic model to data).\n\nthis is my reference: https://learn.microsoft.com/en-us/power-bi/connect-data/service-live-connect-dq-datasets\n\ncorrect me if I'm wrong!"
      },
      {
        "date": "2024-04-17T13:57:00.000Z",
        "voteCount": 4,
        "content": "You have this backwards, the report uses the liveconnect!"
      },
      {
        "date": "2024-03-07T22:59:00.000Z",
        "voteCount": 3,
        "content": "Report either Live or Direct and dataset should be import because you can create daxs in the dataset and publish it and for the report you can connect it with the dataset via live or direct or dual and publish it"
      },
      {
        "date": "2024-02-25T11:13:00.000Z",
        "voteCount": 1,
        "content": "I think the write answer is:\nReport file: Import\nDataset file: LiveConnect\n\nRemember that we want to have a 'separate report' and dataset.\nFor this to work we first the two Excel files to the Power Bi Desktop (do whatever we need to do to transform and prepare it). and this without any reports publish it to the Power Bi Services.\n\nNow we have a semantic model and a report (and empty report) on the workspace of Power BI services. we don't need the report and we can just delete it.\n\nNow in power BI we open a new document and select 'Get Data'&gt;'Power BI datasets' and choose the semantic model that we just published. This is going to be a Liveconnect and I think this is the only way we can connect to a semantic model on Power BI services (if anyone knows any other way, tell me).\n"
      },
      {
        "date": "2024-04-16T03:45:00.000Z",
        "voteCount": 1,
        "content": "shouldn't it the other way round? you wouldn't import the data into the report. you would just connect to the published dataset\n\nReport = Live Connection\nDataset = Import\n\n\"Live connection is a way of connecting a Power BI report to a published Power BI semantic model.\"\n\nSource = https://learn.microsoft.com/en-us/power-bi/connect-data/service-live-connect-dq-datasets"
      },
      {
        "date": "2024-01-29T05:08:00.000Z",
        "voteCount": 8,
        "content": "It is not possible to DirectQuery an excel file - this is only for databases or APIs which support it.\nYour dataset should use import - it will import the excel data into the PBI service.\nYour Report should connect to that dataset, and by default that's done with DirectQuery mode since the data is already in the native PBI format after it's been imported by the service. (you can also import in your report, but it would be redundant)"
      },
      {
        "date": "2024-01-21T10:40:00.000Z",
        "voteCount": 4,
        "content": "The Storage mode is for TABLES in your model. The question asks that you must be able to publish a separate report and dataset. That means the 2 tables must be imported to the model. However, report and dataset are the instances that you publish to Power BI Service workspace. There's no storage mode property that you can set for report and dataset."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122553-exam-pl-300-topic-1-question-29-discussion/",
    "body": "You use Power Query to import two tables named Order Header and Order Details from an Azure SQL database. The Order Header table relates to the Order Details table by using a column named Order ID in each table.<br><br>You need to combine the tables into a single query that contains the unique columns of each table.<br><br>What should you select in Power Query Editor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge queries\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCombine files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend queries"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T09:01:00.000Z",
        "voteCount": 13,
        "content": "There are two primary ways of combining queries: merging and appending.\n\nFor one or more columns that you\u2019d like to add to another query, you merge the queries.\nFor one or more rows of data that you\u2019d like to add to an existing query, you append the query. \nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-shape-and-combine-data"
      },
      {
        "date": "2024-08-29T08:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-08-14T07:41:00.000Z",
        "voteCount": 1,
        "content": "Of course A, merging (join) queries"
      },
      {
        "date": "2024-07-21T03:55:00.000Z",
        "voteCount": 1,
        "content": "Merge is correct"
      },
      {
        "date": "2024-05-27T00:54:00.000Z",
        "voteCount": 2,
        "content": "Merge is correct, because of the distinct individual columns which should remain"
      },
      {
        "date": "2024-03-23T03:03:00.000Z",
        "voteCount": 3,
        "content": "As per my understanding both the tables are related to orders, one is header data i.e is dimension, another one is Fact, so column's will be different in both the tables, in such case we need to user Merge"
      },
      {
        "date": "2024-03-11T13:23:00.000Z",
        "voteCount": 4,
        "content": "Merge = SQL join"
      },
      {
        "date": "2024-01-25T08:01:00.000Z",
        "voteCount": 2,
        "content": "Merge Queries"
      },
      {
        "date": "2023-12-28T20:15:00.000Z",
        "voteCount": 3,
        "content": "I hope I get this one on my exam..."
      },
      {
        "date": "2023-12-18T22:04:00.000Z",
        "voteCount": 1,
        "content": "A is correct, hands down."
      },
      {
        "date": "2023-11-06T05:48:00.000Z",
        "voteCount": 3,
        "content": "Answer A is correct."
      },
      {
        "date": "2023-10-16T17:35:00.000Z",
        "voteCount": 3,
        "content": "Answer A is correct"
      },
      {
        "date": "2023-10-09T13:29:00.000Z",
        "voteCount": 4,
        "content": "A. Merge queries"
      },
      {
        "date": "2023-10-09T01:29:00.000Z",
        "voteCount": 3,
        "content": "Seriously, who even proposes C?"
      },
      {
        "date": "2023-10-16T12:45:00.000Z",
        "voteCount": 5,
        "content": "People who\u00b4re learning"
      },
      {
        "date": "2023-10-07T02:05:00.000Z",
        "voteCount": 2,
        "content": "To combine two tables in Power Query Editor into a single query that contains the unique columns of each table, we should select option C, which is \"Append queries.\""
      },
      {
        "date": "2023-10-08T03:55:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A. Merge combines columns. Append combines rows. The question is about related tables."
      },
      {
        "date": "2023-12-09T03:31:00.000Z",
        "voteCount": 2,
        "content": "You can also think of Append like a Union in SQL, it just puts datasets beneath eachother but does not merge any unique values together. Sorry if the explanation is unclear I am still learning myself as well."
      },
      {
        "date": "2023-10-07T00:51:00.000Z",
        "voteCount": 3,
        "content": "Option A is correct"
      },
      {
        "date": "2023-10-07T00:18:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134893-exam-pl-300-topic-1-question-32-discussion/",
    "body": "You have a PBIX file that imports data from a Microsoft Excel data source stored in a file share on a local network.<br><br>You are notified that the Excel data source was moved to a new location.<br><br>You need to update the PBIX file to use the new location.<br><br>What are three ways to achieve the goal? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Datasets settings of the Power BI service, configure the data source credentials.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Data source settings in Power BI Desktop, configure the file path.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Current File in Power BI Desktop, configure the Data Load settings.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Power Query Editor, use the formula bar to configure the file path for the applied step.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Advanced Editor in Power Query Editor, configure the file path in the M code.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T04:14:00.000Z",
        "voteCount": 17,
        "content": "The answer is correct! \nB, D and E are the correct options!"
      },
      {
        "date": "2024-10-03T02:48:00.000Z",
        "voteCount": 3,
        "content": "B. From the Data source settings in Power BI Desktop, configure the file path.\nThis option allows you to update the connection information for the Excel data source, including changing the file path to the new location.\n\nD. From Power Query Editor, use the formula bar to configure the file path for the applied step.\nIf the source step in Power Query shows the file path, you can change it directly in the formula bar to reflect the new file location.\n\nE. From Advanced Editor in Power Query Editor, configure the file path in the M code.\nThe Advanced Editor lets you edit the M code directly, which includes updating the file path to the new location of the Excel data source."
      },
      {
        "date": "2024-08-14T07:50:00.000Z",
        "voteCount": 2,
        "content": "B. From the Data source settings in Power BI Desktop, configure the file path. \nD. From Power Query Editor, use the formula bar to configure the file path for the applied step.\nE. From Advanced Editor in Power Query Editor, configure the file path in the M code\nGiven answer is correct"
      },
      {
        "date": "2024-07-21T15:08:00.000Z",
        "voteCount": 1,
        "content": "corrrect"
      },
      {
        "date": "2024-07-21T04:07:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2024-03-25T01:50:00.000Z",
        "voteCount": 1,
        "content": "Checked: Correct"
      },
      {
        "date": "2024-03-12T05:21:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-03-10T03:26:00.000Z",
        "voteCount": 1,
        "content": "Not sure about A though"
      },
      {
        "date": "2024-03-10T03:34:00.000Z",
        "voteCount": 4,
        "content": "Actually I did a quick research and apparently YOU CAN EDIT THE CREDENTIALS FROM SERVICE but YOU CANNOT EDIT THE INPUT PATH FROM SERVICE... Meaning you can only do it from Desktop. So BDE are correct."
      },
      {
        "date": "2024-03-07T07:12:00.000Z",
        "voteCount": 1,
        "content": "Correct anwser BDE"
      },
      {
        "date": "2024-03-02T21:34:00.000Z",
        "voteCount": 2,
        "content": "B,C,E IS CORRECT"
      },
      {
        "date": "2024-04-11T05:12:00.000Z",
        "voteCount": 2,
        "content": "C is INCORRECT!"
      },
      {
        "date": "2024-03-07T07:12:00.000Z",
        "voteCount": 2,
        "content": "c IS NOT CORRECT"
      },
      {
        "date": "2024-03-01T11:59:00.000Z",
        "voteCount": 1,
        "content": "c is correct as well IMO. Data Source settings has the \"change source option\"."
      },
      {
        "date": "2024-03-27T06:28:00.000Z",
        "voteCount": 1,
        "content": "options C is not about changing it through 'transform data' and then 'data source settings'. (where the path can be changed indeed). In the answer is mentioned the 'data load' which is under 'current file' when you go to 'option and settings' from file menu. And there you don't have the possibility to change the path"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134892-exam-pl-300-topic-1-question-31-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a folder that contains 50 JSON files.<br><br>You need to use Power BI Desktop to make the metadata of the files available as a single dataset. The solution must NOT store the data of the JSON files.<br><br>Which type of data source should you use, and which transformation should you perform? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image361.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image362.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-29T04:10:00.000Z",
        "voteCount": 27,
        "content": "The answer is correct ! \n- Folder\n- Delete the Content column"
      },
      {
        "date": "2024-03-07T23:02:00.000Z",
        "voteCount": 8,
        "content": "buddy you will not be able to see the meta data if you don't expand the attribute"
      },
      {
        "date": "2024-05-02T19:56:00.000Z",
        "voteCount": 3,
        "content": "I agree with you, but this question is a little hypothetical. Assume that the table has 5000 columns, what would you prefer the first step to be?\nOption 1:\n-Expand attribute column\n- Delete content column\nIn this case, after expanding, you'll have to scroll a lot to the left, little tough, and then delete content column, end result would be storing metadata.\n\nOption 2:\n- Delete content column\n-Expand attribute column\nThis is most preferable, as you'll see content column in the first column position, delete it and then expand attribute column, end result would be storing metadata.\n\nI'd still go with option 2 in terms of ease. My answer would be \"Delete content column\" for 2nd question."
      },
      {
        "date": "2024-03-17T12:45:00.000Z",
        "voteCount": 3,
        "content": "I think she is right because the solution says you should get the metadata and \"solution must NOT store the data of the JSON files\". You won't satisfy the second part of the question till you delete the content column first."
      },
      {
        "date": "2024-05-07T08:01:00.000Z",
        "voteCount": 13,
        "content": "I saw video and the answer is\n- Folder\n - Combine the files of Content column\n"
      },
      {
        "date": "2024-09-27T17:58:00.000Z",
        "voteCount": 1,
        "content": "In the video, the author explains the answer to 2 different questions: the first question requires the steps to store the metada, in which case, the solution is folder/expand the atributes column. The second question requires the steps to store the data, in which case your answer it is the correct one. For the purpose of the question on discussion, the correct answer, according to the video is folder/expand atribute column."
      },
      {
        "date": "2024-08-08T20:13:00.000Z",
        "voteCount": 1,
        "content": "It\u2019s wrong. We do not need content."
      },
      {
        "date": "2024-07-22T21:09:00.000Z",
        "voteCount": 2,
        "content": "You're awesome! Many thanks!"
      },
      {
        "date": "2024-05-08T03:24:00.000Z",
        "voteCount": 2,
        "content": "you saved my time , god bless you dear"
      },
      {
        "date": "2024-10-12T01:42:00.000Z",
        "voteCount": 1,
        "content": "If we dont also expand the attributes column we wont get all the metadata will we?"
      },
      {
        "date": "2024-10-02T17:59:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is Folder and expand the attribute column because the attribute column stores the metadata."
      },
      {
        "date": "2024-08-27T11:28:00.000Z",
        "voteCount": 1,
        "content": "Folder y Delete Content:\n\nclaramente indica que no debe guardar los datos de JSON, posteriormente que se expanda los atributos o no es secundario"
      },
      {
        "date": "2024-08-27T01:43:00.000Z",
        "voteCount": 2,
        "content": "Just tested it. Should be Folder and Expand attribute. The content column will not expand or even show up  when you don't delete the the content column."
      },
      {
        "date": "2024-08-16T08:27:00.000Z",
        "voteCount": 1,
        "content": "We need first to load from  folder in order to get all the metadata files from it (50 JSON) then we need to remove the content column which is binary"
      },
      {
        "date": "2024-07-21T04:02:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct !\n- Folder\n- Delete the Content column"
      },
      {
        "date": "2024-07-19T05:15:00.000Z",
        "voteCount": 4,
        "content": "I tested and the correct option is to \"Expand the Attribute column\". The \"Content\" column is automatically removed when we close the PowerQuery. But if we don't click \"Expand the Attribute column\" we loose these information."
      },
      {
        "date": "2024-05-06T23:49:00.000Z",
        "voteCount": 2,
        "content": "Tested - it's folder, expand attribute.  There are no contents in the file in the data pane, only attributes. This is because the column is 'binary'"
      },
      {
        "date": "2024-05-02T00:09:00.000Z",
        "voteCount": 2,
        "content": "The thing here is that the question is not  about the order or de number of actions hat you have to follow in order to achieve the solution. The question is,\"which action you have to perform?\", so independently of the order, you are going to have to delete the content column. So the answer given is correct."
      },
      {
        "date": "2024-03-31T02:33:00.000Z",
        "voteCount": 9,
        "content": "I tested it. The correct answer is \n1) Folder\n2) Expand the Attribute column.\nI tested with Deleting the Content column and without deleting it, in both cases the content is not visible when you load the data in Power BI Desktop. In order to see the full metadata  you should expand the Attributes column."
      },
      {
        "date": "2024-03-16T03:51:00.000Z",
        "voteCount": 1,
        "content": "\u0130mage is not shown?"
      },
      {
        "date": "2024-03-12T05:12:00.000Z",
        "voteCount": 1,
        "content": "I believe the correct transformation steps would be :\n1. Expand attributes\n2. Delete content\n3. Combine files, getting just 1 table with metadata"
      },
      {
        "date": "2024-03-11T12:00:00.000Z",
        "voteCount": 6,
        "content": "The answer is incorrect !\n- Folder\n- Expand the attribute column\nDone this in Power BI Desktop"
      },
      {
        "date": "2024-03-10T03:23:00.000Z",
        "voteCount": 2,
        "content": "To get METADATA (and not JSON data) I would get data from FOLDER , EXPAND the Attibute column and DELETE the Content column"
      },
      {
        "date": "2024-03-09T15:27:00.000Z",
        "voteCount": 3,
        "content": "Data source type: Folder\nTransformation: Delete the Content column.\n\nThis approach will allow you to access the metadata for all the JSON files in the folder without importing the actual data from the JSON files into the dataset."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/146977-exam-pl-300-topic-1-question-32-discussion/",
    "body": "Case Study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>General Overview -<br><br>Northwind Traders is a specialty food import company.<br><br>The company recently implemented Power BI to better understand its top customers, products, and suppliers.<br><br><br>Business Issues -<br><br>The sales department relies on the IT department to generate reports in Microsoft SQL Server Reporting Services (SSRS). The IT department takes too long to generate the reports and often misunderstands the report requirements.<br><br>Existing Environment. Data Sources<br><br>Northwind Traders uses the data sources shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image373.png\"><br><br>Source2 is exported daily from a third-party system and stored in Microsoft SharePoint Online.<br><br>Existing Environment. Customer Worksheet<br><br>Source2 contains a single worksheet named Customer Details. The first 11 rows of the worksheet are shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image374.png\"><br><br>All the fields in Source2 are mandatory.<br><br>The Address column in Customer Details is the billing address, which can differ from the shipping address.<br><br>Existing Environment. Azure SQL Database<br><br>Source1 contains the following tables:<br><br>\u2022\tOrders<br>\u2022\tProducts<br>\u2022\tSuppliers<br>\u2022\tCategories<br>\u2022\tOrder Details<br>\u2022\tSales Employees<br><br>The Orders table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image375.png\"><br><br>The Order Details table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image376.png\"><br><br>The address in the Orders table is the shipping address, which can differ from the billing address.<br><br>The Products table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image377.png\"><br><br>The Categories table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image378.png\"><br><br>The Suppliers table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image379.png\"><br><br>The Sales Employees table contains the following columns.<br><br><img src=\"https://img.examtopics.com/pl-300/image380.png\"><br><br>Each employee in the Sales Employees table is assigned to one sales region. Multiple employees can be assigned to each region.<br><br>Requirements. Report Requirements<br><br>Northwind Traders requires the following reports:<br><br>\u2022\tTop Products<br>\u2022\tTop Customers<br>\u2022\tOn-Time Shipping<br><br>The Top Customers report will show the top 20 customers based on the highest sales amounts in a selected order month or quarter, product category, and sales region.<br><br>The Top Products report will show the top 20 products based on the highest sales amounts sold in a selected order month or quarter, sales region, and product category. The report must also show which suppliers provide the top products.<br><br>The On-Time Shipping report will show the following metrics for a selected shipping month or quarter:<br><br>\u2022\tThe percentage of orders that were shipped late by country and shipping region<br>\u2022\tCustomers that had multiple late shipments during the last quarter<br><br>Northwind Traders defines late orders as those shipped after the required shipping date.<br><br>The warehouse shipping department must be notified if the percentage of late orders within the current month exceeds 5%.<br><br>The reports must show historical data for the current calendar year and the last three calendar years.<br><br>Requirements. Technical Requirements<br><br>Northwind Traders identifies the following technical requirements:<br><br>\u2022\tA single dataset must support all three reports.<br>\u2022\tThe reports must be stored in a single Power BI workspace.<br>\u2022\tReport data must be current as of 7 AM Pacific Time each day.<br>\u2022\tThe reports must provide fast response times when users interact with a visualization.<br>\u2022\tThe data model must minimize the size of the dataset as much as possible, while meeting the report requirements and the technical requirements.<br><br>Requirements. Security Requirements<br><br>Access to the reports must be granted to Azure Active Directory (Azure AD) security groups only. An Azure AD security group exists for each department.<br><br>The sales department must be able to perform the following tasks in Power BI:<br><br>\u2022\tCreate, edit, and delete content in the reports.<br>\u2022\tManage permissions for workspaces, datasets, and reports.<br>\u2022\tPublish, unpublish, update, and change the permissions for an app.<br>\u2022\tAssign Azure AD groups role-based access to the reports workspace.<br><br>Users in the sales department must be able to access only the data of the sales region to which they are assigned in the Sales Employees table.<br><br>Power BI has the following row-level security (RLS) Table filter DAX expression for the Sales Employees table.<br><br>[EmailAddress] = USERNAME()<br><br>RLS will be applied only to the sales department users. Users in all other departments must be able to view all the data.<br><br><br>You need to create the semantic model.<br><br>Which storage mode should you use for the tables in the semantic model?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDual\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlive connection"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-24T09:52:00.000Z",
        "voteCount": 1,
        "content": "Isn't there a 1 GB limit for the import mode. Source 1 is 2 GB. So therefore must be dual?"
      },
      {
        "date": "2024-09-29T22:33:00.000Z",
        "voteCount": 1,
        "content": "There is no mentioning of Pro in the question. So, it could be any capacity. Also, we could limit the data to 3 calendar years while importing and the data is not expected to have real time data."
      },
      {
        "date": "2024-09-19T04:14:00.000Z",
        "voteCount": 2,
        "content": "I think it should be B"
      },
      {
        "date": "2024-09-14T05:47:00.000Z",
        "voteCount": 1,
        "content": "OPTION B: DUAL"
      },
      {
        "date": "2024-09-10T00:05:00.000Z",
        "voteCount": 4,
        "content": "Import for the excel file, DirectQuery for the tables on SQL server"
      },
      {
        "date": "2024-09-06T06:30:00.000Z",
        "voteCount": 3,
        "content": "C: Import"
      },
      {
        "date": "2024-09-05T07:25:00.000Z",
        "voteCount": 3,
        "content": "Very small datasets updated once a day, also for me is C"
      },
      {
        "date": "2024-10-07T16:18:00.000Z",
        "voteCount": 1,
        "content": "There doen't seem to be anything in the case study material or the answers given here to support the use of Dual. Everything seems to support import.\n--&gt; Ans : C"
      },
      {
        "date": "2024-09-04T22:04:00.000Z",
        "voteCount": 4,
        "content": "I agree with C as reports are updated at 7am only and needs fast response times."
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/146978-exam-pl-300-topic-1-question-33-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI semantic model that contains the data sources shown in the following table.<br><br><img src=\"https://img.examtopics.com/pl-300/image381.png\"><br><br>You need to configure the privacy level s of the data sources.<br><br>What should you configure for each data source? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image382.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image383.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-05T05:07:00.000Z",
        "voteCount": 7,
        "content": "Answer is correct. Employee review data seems to be private since it contains sensitive information. And sales opportunities is organizational since it contains less informative information but still internal"
      },
      {
        "date": "2024-09-16T09:30:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2024-09-04T22:47:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2024-09-04T22:05:00.000Z",
        "voteCount": 1,
        "content": "private and organisational"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/146979-exam-pl-300-topic-1-question-34-discussion/",
    "body": "You plan to use Power BI Desktop to create a bug tracking dashboard that will pull data from Analytics in Azure DevOps.<br><br>From Power BI Desktop, you need to configure a data connector to authenticate to Azure DevOps. The solution must meet the following requirements:<br><br>\u2022\tUse Analytics views.<br>\u2022\tFilter data from the cloud.<br><br>Which connector should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOData queries\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure DevOps (Boards only)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure DevOps Server (Boards only)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOData Feed"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-04T22:13:00.000Z",
        "voteCount": 8,
        "content": "I think it is B because this link - has this point - Select Get Data &gt; Online Services, Azure DevOps (Boards only) for cloud services or select Azure DevOps Server (Boards only) for on-premises. Then select Connect. Analytics views only support queries against work items and test cases.  I am assuming this is about DevOps on the cloud"
      },
      {
        "date": "2024-09-12T18:44:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2024-09-12T18:44:00.000Z",
        "voteCount": 1,
        "content": "Sorry B, you are right"
      },
      {
        "date": "2024-10-16T03:01:00.000Z",
        "voteCount": 1,
        "content": "This is explained in https://learn.microsoft.com/en-us/azure/devops/report/powerbi/odataquery-connect?view=azure-devops, there is no such thing as an OData query connector, it is called an OData Feed connector in Power BI."
      },
      {
        "date": "2024-10-08T19:48:00.000Z",
        "voteCount": 1,
        "content": "OData Queries, refer to the document. https://learn.microsoft.com/en-us/azure/devops/report/powerbi/odataquery-connect?view=azure-devops\n----&gt; \"All filtering is done server-side.\" &lt;-------\nPower BI can execute OData queries. OData queries are powerful and can filter and aggregate data before returning it to Power BI"
      },
      {
        "date": "2024-10-08T19:51:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind, it's still B, this is the only one mentioned analytics views and in the sample part it mentioned the data can be filtered in analytics view.\n\n\"Return to the view under the Analytics view in the web portal and adjust the filters to decrease the size of the dataset.\"\nThis line explains all.\nhttps://learn.microsoft.com/en-us/azure/devops/report/powerbi/data-connector-connect?view=azure-devops\n\nhttps://learn.microsoft.com/en-us/azure/devops/report/powerbi/data-connector-connect?view=azure-devops"
      },
      {
        "date": "2024-10-02T18:27:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT says the correct answer is D - OData feed as its the correct connector to use Analytical views and filter data from the cloud."
      },
      {
        "date": "2024-10-02T07:35:00.000Z",
        "voteCount": 2,
        "content": "OData Queries so that it has the ability to filter"
      },
      {
        "date": "2024-09-30T21:40:00.000Z",
        "voteCount": 1,
        "content": "Belive its B"
      },
      {
        "date": "2024-09-30T08:20:00.000Z",
        "voteCount": 1,
        "content": "Copilot says it is A Azure DevOps (Boards only), is designed specifically for connecting to Azure DevOps Boards. While it can be useful for accessing work items and board-related data, it does not support Analytics views or filtering data from the cloud as required for your bug tracking dashboard.\nFor your needs, OData queries remain the best choice because they allow you to leverage Analytics views and filter data directly from the cloud."
      },
      {
        "date": "2024-09-26T05:43:00.000Z",
        "voteCount": 1,
        "content": "I think B\nTo connect Power BI Desktop to Azure DevOps for creating a bug tracking dashboard, you should use the Azure DevOps (Boards only) data connector. Here are the steps to configure it:\n\nOpen Power BI Desktop.\nSign in to the service. First-time access requires you to sign in and authenticate your credentials.\nGo to Get Data &gt; Online Services.\nSelect Azure DevOps (Boards only) for cloud services or Azure DevOps Server (Boards only) for on-premises1.\nConnect and specify the necessary parameters, such as the Collection URL and Team Project name"
      },
      {
        "date": "2024-09-24T02:48:00.000Z",
        "voteCount": 1,
        "content": "B is the right answer"
      },
      {
        "date": "2024-09-24T01:31:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2024-09-17T04:06:00.000Z",
        "voteCount": 1,
        "content": "Using OData, you can directly query Analytics for Azure DevOps from a supported browser. Use the returned JSON data as you like. Enterprise organizations can generate queries that span multiple projects or an entire organization or project collection."
      },
      {
        "date": "2024-09-11T03:02:00.000Z",
        "voteCount": 3,
        "content": "OData queries\nIt can filter data"
      },
      {
        "date": "2024-09-14T05:50:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/146922-exam-pl-300-topic-1-question-35-discussion/",
    "body": "HOTSPOT<br> -<br><br>You use Power Query Editor to preview the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image391.png\"><br><br>You confirm that the data will always start on row 3, and row 3 will always contain the column names.<br><br>How should you shape the query? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image392.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image393.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-02T18:29:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct \n Remove Top 2 rows\nUse first rows as headers"
      },
      {
        "date": "2024-09-05T05:16:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct.\nWe need to remove top 2 rows first and then user first row as headers"
      },
      {
        "date": "2024-09-04T11:33:00.000Z",
        "voteCount": 1,
        "content": "-remove top rows\n-use first row as headers"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/147043-exam-pl-300-topic-1-question-36-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a data source that contains a column. The column contains case sensitive data.<br><br>You have a Power BI semantic model in DirectQuery mode.<br><br>You connect to the model and discover that it contains undefined values and errors.<br><br>You need to resolve the issue.<br><br>Solution: You implicitly convert the values into the required type.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-11T03:17:00.000Z",
        "voteCount": 2,
        "content": "You have to correct errors before setting the data type"
      },
      {
        "date": "2024-09-05T16:52:00.000Z",
        "voteCount": 3,
        "content": "I believe the answer is correct as case sensitive data would need to be checked in the source system or explicitly handled in a query with an UPPER() or similiar function"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/147003-exam-pl-300-topic-1-question-37-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a data source that contains a column. The column contains case sensitive data.<br><br>You have a Power BI semantic model in DirectQuery mode.<br><br>You connect to the model and discover that it contains undefined values and errors.<br><br>You need to resolve the issue.<br><br>Solution: You change the semantic model mode.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T06:21:00.000Z",
        "voteCount": 1,
        "content": "My answer would be A. Yes: \nSwitching from DirectQuery to Import mode would resolve the issue with case sensitivity. Power Query can handle case sensitivity effectively, but its functions are limited when operating in DirectQuery mode. By switching to Import mode, you gain full access to Power Query's data transformation capabilities, allowing you to standardize the case of your data before it is loaded into the model."
      },
      {
        "date": "2024-09-11T03:19:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2024-09-05T01:28:00.000Z",
        "voteCount": 2,
        "content": "B. I dont think changing the semantic mode will solve the issue."
      },
      {
        "date": "2024-09-05T16:54:00.000Z",
        "voteCount": 1,
        "content": "agree - case sensitivtiy should be handled with a function etc"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/147321-exam-pl-300-topic-1-question-38-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a data source that contains a column. The column contains case sensitive data.<br><br>You have a Power BI semantic model in DirectQuery mode.<br><br>You connect to the model and discover that it contains undefined values and errors.<br><br>You need to resolve the issue.<br><br>Solution: You normalize casing in the source query or Power Query Editor.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-16T03:30:00.000Z",
        "voteCount": 1,
        "content": "This exact scenario is explained in the docs (at the \"Case sensitivity\" part): https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-data-types. According to Microsoft, normalize casing in the source query should be the solution."
      },
      {
        "date": "2024-09-16T00:58:00.000Z",
        "voteCount": 2,
        "content": "The questions says \"OR\":\nSo, either \"Power Query Editor\" (which is IMO wrong due to Direct Query mode) OR\n\"You normalize casing in the source query\" (which is true, again IMO)\n\nI would say that the answer is \"A - Yes\""
      },
      {
        "date": "2024-09-13T09:36:00.000Z",
        "voteCount": 3,
        "content": "Answer is no, direct query mode cannot be edited in power query"
      },
      {
        "date": "2024-09-15T23:45:00.000Z",
        "voteCount": 1,
        "content": "Agree with you."
      },
      {
        "date": "2024-09-11T03:24:00.000Z",
        "voteCount": 1,
        "content": "I think the given answer is correct"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/147044-exam-pl-300-topic-1-question-39-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a data source that contains a column. The column contains case sensitive data.<br><br>You have a Power BI semantic model in DirectQuery mode.<br><br>You connect to the model and discover that it contains undefined values and errors.<br><br>You need to resolve the issue.<br><br>Solution: You add an index key and normalize casing in the data source.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-05T16:57:00.000Z",
        "voteCount": 6,
        "content": "I think the answer is A.  adding the index key won't fix the problem by itself but you are also normalising the case which will resolve it."
      },
      {
        "date": "2024-09-06T05:10:00.000Z",
        "voteCount": 2,
        "content": "Actually i dont understand that how normalising the case will resolve the issue as undefined values and errors can also be related to missing values and unmatched data type..?"
      },
      {
        "date": "2024-10-07T19:20:00.000Z",
        "voteCount": 1,
        "content": "aCCORDING TO PERPLEXITY, the answer is still NO, as this does not solve the issue of undefined values and errors, and also direct query does not really work well with normalizing teh case either."
      },
      {
        "date": "2024-10-07T14:15:00.000Z",
        "voteCount": 1,
        "content": "I believe the option A is correct"
      },
      {
        "date": "2024-10-06T12:56:00.000Z",
        "voteCount": 1,
        "content": "According to Copilot, the answer is A. Here is the explanation I got: \n\nYes, adding an index key and normalizing the casing in the data source can help resolve the issue of undefined values and errors in a Power BI semantic model in DirectQuery mode.\n\nHere\u2019s why:\nIndex Key: Adding an index key can improve query performance and ensure that each row is uniquely identifiable, which helps in avoiding errors related to duplicate or missing data1.\nNormalizing Casing: Normalizing the casing ensures consistency in the data, which can prevent mismatches and errors that arise from case sensitivity issues1.\nThese steps should help in cleaning up the data and making it more reliable for analysis in Power BI.\nhttps://learn.microsoft.com/en-us/power-bi/guidance/directquery-model-guidance"
      },
      {
        "date": "2024-09-13T09:39:00.000Z",
        "voteCount": 4,
        "content": "A is right"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/147015-exam-pl-300-topic-1-question-40-discussion/",
    "body": "You have a Microsoft Excel file in a Microsoft OneDrive folder.<br><br>The file must be imported to a Power BI semantic model.<br><br>You need to ensure that the semantic model can be refreshed in PowerBi.com.<br><br>Which two connectors can you use to connect to the file? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeb\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExcel Workbook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFolder",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText/CSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSharePoint folder\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-05T05:32:00.000Z",
        "voteCount": 10,
        "content": "For me is AE"
      },
      {
        "date": "2024-09-05T16:58:00.000Z",
        "voteCount": 1,
        "content": "agree - this exact question is back at the beginning and it is Web and Sharepoint"
      },
      {
        "date": "2024-09-05T15:08:00.000Z",
        "voteCount": 1,
        "content": "I share your point of view."
      },
      {
        "date": "2024-09-16T01:29:00.000Z",
        "voteCount": 4,
        "content": "A and E are the correct answers. Just tested it now end to end and it works. Refreshing is performed as well.\n\nFor B/C to be refreshed online, you'd need a Gateway which is not mentioned here."
      },
      {
        "date": "2024-09-05T23:24:00.000Z",
        "voteCount": 1,
        "content": "AE in my opinion"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148501-exam-pl-300-topic-1-question-41-discussion/",
    "body": "You use Power Query Editor to preview a column named Date as shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/pl-300/image423.png\"><br><br>You need to change the Date column to contain only the year. The solution must minimize administrative effort.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the column by delimiter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the column by number of characters.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExtract the text after the delimiter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform the column to contain only the year.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-14T10:51:00.000Z",
        "voteCount": 1,
        "content": "D it is."
      },
      {
        "date": "2024-10-02T07:44:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer - Transform the column to contain only the year. This approach directly modifies the column to display only the year, without the need for additional steps like splitting or extracting text."
      },
      {
        "date": "2024-10-02T19:07:00.000Z",
        "voteCount": 1,
        "content": "I agree."
      },
      {
        "date": "2024-10-01T04:51:00.000Z",
        "voteCount": 3,
        "content": "Why A? the column is Date type, so D is possible, D in my opinion"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148529-exam-pl-300-topic-1-question-42-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are designing the data model for a Power BI semantic model.<br><br>You have the following tables in the star schema.<br><br><img src=\"https://img.examtopics.com/pl-300/image424.png\"><br><br>Which table is the fact table of the star schema, and which column in the Patient table is the surrogate key of the star schema? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/pl-300/image425.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/pl-300/image426.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-04T11:36:00.000Z",
        "voteCount": 1,
        "content": "Given anwsers are correct."
      },
      {
        "date": "2024-10-01T14:59:00.000Z",
        "voteCount": 1,
        "content": "Given answer is what I thought as well. \nAns : D, C"
      }
    ],
    "examNameCode": "pl-300",
    "topicNumber": "1"
  }
]