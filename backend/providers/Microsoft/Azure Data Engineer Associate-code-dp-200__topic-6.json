[
  {
    "topic": 6,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60970-exam-dp-200-topic-6-question-1-discussion/",
    "body": "You have to create a new single database instance of Microsoft Azure SQL database. You must ensure that client connections are accepted via a workstation.<br>The workstation will use SQL Server Management Studio to connect to the database instance.<br>Which of the following Powershell commands would you execute to create and configure the database? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew-AzureRmSqlElasticPool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew-AzureRmSqlServerFirewallRule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew-AzureRmSqlServer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew-AzureRmSqlServerVirtualNetworkRule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNew-AzureRmSqlDatabase"
    ],
    "answer": "BCE",
    "answerDescription": "The Microsoft documentation clearly gives the steps to create and configure the database. Please note the below snippet shows the new powershell commands, but you can also use the older Azure PowerShell commands.<br><img src=\"/assets/media/exam-media/03872/0048300001.png\" class=\"in-exam-image\"><br>Since this is clearly given in the documentation, all other options are incorrect.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/scripts/sql-database-create-and-configure-database-powershell",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-29T11:57:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58769-exam-dp-200-topic-6-question-7-discussion/",
    "body": "A company has an Azure SQL data warehouse. They want to use PolyBase to retrieve data from an Azure Blob storage account and ingest into the Azure SQL data warehouse. The files are stored in parquet format. The data needs to be loaded into a table called XYZ_sales.<br>Which of the following actions need to be performed to implement this requirement? (Choose four.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external file format that would map to the parquet-based files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into a staging table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external table called XYZ_sales_details",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external data source for the Azure Blob storage account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a master key on the database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Polybase to use the Azure Blob storage account"
    ],
    "answer": "BCDE",
    "answerDescription": "There is an article on github as part of the Microsoft documentation that provides details on how to load data into an Azure SQL data warehouse from an Azure<br>Blob storage account. The key steps are:<br>Creating a master key in the database.<br>Creating an external data source for the Azure Blob storage account:<br>3. Create a master key for the MySampleDataWarehouse database. You only need to create a master key once per database.<br>CREATE MASTER KEY;<br>4. Run the following CREATE EXTERNAL DATA SOURCE statement to define the location of the Azure blob. This is the location of the external taxi cab data. To run a command that you have appended to the query window, highlight the commands you wish to run and click Execute.<br><img src=\"/assets/media/exam-media/03872/0050100001.png\" class=\"in-exam-image\"><br>Next you load the data. But it is always beneficial to load the data into a staging table first:<br>Load the data into your data warehouse.<br>This section uses the external tables you just defined to load the sample data from Azure Storage Blob to SQL Data Warehouse.<br>[!NOTE] This tutorial loads the data directly into the final table. In a production environment, you will usually use CREATE TABLE AS SELECT to load into a staging table. While data is in the staging table you can perform any necessary transformations. To append the data in the staging table to a production table, you can use the INSERT...SELECT statement. For more information, see Inserting data into a production table.<br>Since this is clearly provided in the documentation, all other options are incorrect.",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-22T11:18:00.000Z",
        "voteCount": 6,
        "content": "How can you read the parquet files without defining the format?\nSo A should be part of the answer."
      },
      {
        "date": "2021-07-27T02:06:00.000Z",
        "voteCount": 3,
        "content": "the question literally said the data needs to be load to xyz_sails. Yet the answer select xyz_sails_Details !!\nthe answer is not correct"
      },
      {
        "date": "2021-11-30T03:54:00.000Z",
        "voteCount": 2,
        "content": "I agree. Although the external tables are required but because it's clearly a different name, I guess the answer is ABDE.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver15#create-external-tables-for-azure-data-lake-store"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58790-exam-dp-200-topic-6-question-51-discussion/",
    "body": "You need to deploy a Microsoft Azure Stream Analytics job for an IoT based solution. The solution must minimize latency. The solution must also minimize the bandwidth usage between the job and the IoT device.<br>Which of the following actions must you perform for this requirement? (Choose four.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure to configure routes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Blob storage container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Streaming Units",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an IoT Hub and add the Azure Stream Analytics modules to the IoT Hub namespace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Stream Analytics edge job and configure job definition save location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Stream Analytics cloud job and configure job definition save location"
    ],
    "answer": "ABDF",
    "answerDescription": "There is an article in the Microsoft documentation on configuring Azure Stream Analytics on IoT Edge devices.<br>You need to have a storage container for the job definition:<br><br>Installation instructions -<br>The high-level steps are described in the following table. More details are given in the following sections.<br><img src=\"/assets/media/exam-media/03872/0055800001.jpg\" class=\"in-exam-image\"><br>You also need to create a cloud part job definition:<br><img src=\"/assets/media/exam-media/03872/0055900001.jpg\" class=\"in-exam-image\"><br>You also need to set the modules for your IoT edge device:<br>Deployment ASA on your IoT Edge device(s)<br><br>Add ASA to your deployment -<br>\u05d2\u20ac\u00a2 In the Azure portal, open IoT Hub, navigate to IoT Edge and click on the device you want to target for this deployment.<br>\u05d2\u20ac\u00a2 Select Set modules, then select + Add and choose Azure Stream Analytics Module.<br>\u05d2\u20ac\u00a2 Select the subscription and the ASA Edge job that you created. Click Save.<br><img src=\"/assets/media/exam-media/03872/0056000001.jpg\" class=\"in-exam-image\"><br>You also need to configure the Routes:<br><br>Configure routes -<br>IoT Edge provides a way to declaratively route messages between modules, and between modules and IoT Hub. The full syntax is described here. Names of the inputs and outputs created in the ASA job can be used as endpoints for routing.<br>Since this is clear from the Microsoft documentation, all other options are incorrect.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-27T06:46:00.000Z",
        "voteCount": 7,
        "content": "The Correct Answer is :\n1) storage\n2) Edge\n3) IOT\n4) route\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/59756-exam-dp-200-topic-6-question-64-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>XYZ is an online training provider.<br><br>Current Environment -<br>The company currently has Microsoft SQL databases that are split into different categories or tiers. Some of the databases are used by Internal users, some by external partners and external distributions.<br>Below is the List of applications, tiers and their individual requirements:<br><img src=\"/assets/media/exam-media/03872/0059000001.jpg\" class=\"in-exam-image\"><br>Below are the current requirements of the company:<br>* For Tier 4 and Tier 5 databases, the backup strategy must include the following:<br>- Transactional log backup every hour<br>- Differential backup every day<br>- Full backup every week<br>* Backup strategies must be in place for all standalone Azure SQL databases using methods available with Azure SQL databases<br>* Tier 1 database must implement the following data masking logic:<br>- For Data type XYZ-A `\" Mask 4 or less string data type characters<br>- For Data type XYZ-B `\" Expose the first letter and mask the domain<br>- For Data type XYZ-C `\" Mask everything except characters at the beginning and the end<br>* All certificates and keys are internally managed in on-premise data stores<br>* For Tier 2 databases, if there are any conflicts between the data transfer from on-premise, preference should be given to on-premise data.<br>* Monitoring must be setup on every database<br>* Applications with Tiers 6 through 8 must ensure that unexpected resource storage usage is immediately reported to IT data engineers.<br>* Azure SQL Data warehouse would be used to gather data from multiple internal and external databases.<br>* The Azure SQL Data warehouse must be optimized to use data from its cache<br>* The below metrics must be available when it comes to the cache:<br>- Metric XYZ-A `\" Low cache hit %, high cache usage %<br>- Metric XYZ-B `\" Low cache hit %, low cache usage %<br>- Metric XYZ-C `\" high cache hit %, high cache usage %<br>* The reporting data for external partners must be stored in Azure storage. The data should be made available during regular business hours in connecting regions.<br>* The reporting for Tier 9 needs to be moved to Event Hubs.<br>* The reporting for Tier 10 needs to be moved to Azure Blobs.<br>The following issues have been identified in the setup:<br>* The External partners have control over the data formats, types and schemas<br>* For External based clients, the queries can't be changed or optimized<br>* The database development staff are familiar with T-SQL language<br>* Because of the size and amount of data, some applications and reporting features are not performing at SLA levels.<br>You have to implement logging for monitoring the data warehousing solution.<br>Which of the following would you log?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequeststeps",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDmWorkers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQLRequest",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecRequest"
    ],
    "answer": "C",
    "answerDescription": "Since the SQL requests would affect the cache, these requests need to be monitored<br>The Microsoft documentation mentions the following on caching:<br><br>Cache hit and used percentage -<br>The matrix below describes scenarios based on the values of the cache metrics:<br><img src=\"/assets/media/exam-media/03872/0059200001.png\" class=\"in-exam-image\"><br>Scenario 1: You are optimally using your cache. Troubleshoot other areas which may be slowing down your queries.<br>Scenario 2: Your current working data set cannot fit into the cache which causes a low cache hit percentage due to physical reads. Consider scaling up your performance level and rerun your workload to populate the cache.<br>Scenario 3: It is likely that your query is running slow due to reasons unrelated to the cache. Troubleshoot other areas which may be slowing down your queries.<br>You can also consider scaling down your instance to reduce your cache size to save costs.<br>Scenario 4: You had a cold cache which could be the reason why your query was slow. Consider rerunning your query as your working dataset should now be in cached.<br>Since this is the ideal metric to monitor, all other options are incorrect.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-14T17:06:00.000Z",
        "voteCount": 1,
        "content": "Here are the logs emitted by dedicated SQL pools:\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/monitoring/how-to-monitor-using-azure-monitor#dedicated-sql-pool-logs"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56714-exam-dp-200-topic-6-question-70-discussion/",
    "body": "You have to implement Azure Stream Analytics Functions as part of your data streaming solution.<br>The solution has the following requirements:<br>- Segment the data stream into distinct time segments that do not repeat or overlap<br>- Segment the data stream into distinct time segments that repeat and can overlap<br>- Segment the data stream to produce an output when an event occurs<br>Which of the following windowing function would you use for the following requirement?<br>`Segment the data stream into distinct time segments that do not repeat or overlap`<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHopping",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSliding",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTumbling"
    ],
    "answer": "D",
    "answerDescription": "You need to use the Tumbling windowing function for this requirement.<br>The Microsoft documentation mentions the following:<br><br>Tumbling window -<br>Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.<br><img src=\"/assets/media/exam-media/03872/0059900001.png\" class=\"in-exam-image\"><br>Since this is clearly given in the documentation, all other options are incorrect.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-16T13:15:00.000Z",
        "voteCount": 1,
        "content": "Session window: Neither overlaps nor repeats\nTumbling window: Repeats and does not overlap\nHopping: Repeats and can overlap\nSliding: Does not repeat but overlaps"
      },
      {
        "date": "2021-07-01T02:46:00.000Z",
        "voteCount": 1,
        "content": "Hopping"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58792-exam-dp-200-topic-6-question-74-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account. You have a number of CSV files loaded in the account. Each file has a header row. After the header row is a property that is formatted by carriage return (/r) and line feed (/n).<br>You need to load the files daily as a batch into Azure SQL Data warehouse using Polybase. You have to skip the header row when the files are imported.<br>Which of the following actions would you take to implement this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external data source and ensure to use the abfs location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external data source and ensure to use the Hadoop location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external file format and set the First_row option",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a database scoped credential that uses OAuth2 token and a key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CREATE EXTERNAL TABLE AS SELECT and create a view that removes the empty row"
    ],
    "answer": "ACD",
    "answerDescription": "The Microsoft documentation highlights the steps required to load data from Azure Data Lake Gen2 to an Azure SQL Data warehouse.<br>One of the steps is to create a database scoped credential:<br><img src=\"/assets/media/exam-media/03872/0060500001.jpg\" class=\"in-exam-image\"><br>Another step is to create the external data source using 'abfs' as the file location:<br><br>Create the external data source -<br>Use this CREATE EXTERNAL DATA SOURCE command to store the location of the data.<br><img src=\"/assets/media/exam-media/03872/0060600001.jpg\" class=\"in-exam-image\"><br>And you can use the FIRST_ROW parameter to skip the first row of the file.<br><br>FIRST_ROW = First_row_int -<br>Specifies the row number that is read first in all files during a PolyBase load. This parameter can take values 1-15. If the value is set to two, the first row in every file (header row) is skipped when the data is loaded. Rows are skipped based on the existence of row terminators (/r/n, /r, /n). When this option is used for export, rows are added to the data to make sure the file can be read with no data loss. If the value is set to &gt;2, the first row exported is the Column names of the external table.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-file-format-transact-sql?view=sql-server-ver15",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-27T07:48:00.000Z",
        "voteCount": 3,
        "content": "correct Answer is ACE"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58174-exam-dp-200-topic-6-question-76-discussion/",
    "body": "A company has an Azure SQL Datawarehouse. They have a table named whizlab_salesfact that contains data for the past 12 months. The data is partitioned by month. The table contains around a billion rows. The table has clustered columnstore indexes. At the beginning of each month you need to remove the data from the table that is older than 12 months.<br>Which of the following actions would you implement for this requirement? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new empty table named XYZ_salesfact_new that has the same schema as XYZ_salesfact",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop the XYZ_salesfact_new table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the data to the new table by using CREATE TABLE AS SELECT (CTAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTruncate the partition containing the stale data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the partition containing the stale data from XYZ_salesfact to XYZ_salesfact_new",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the DELETE statement where the value in the Date column is greater than 12 months"
    ],
    "answer": "BCE",
    "answerDescription": "An example of this is given in a blog post. To achieve this, we first need to copy the data onto a new table using the \u05d2\u20acCREATE TABLE AS SELECT\u05d2\u20ac command.<br>Then we switch the partition and then delete the staging table.<br>Option \u05d2\u20acCreate a new empty table named XYZ_salesfact_new that has the same schema as XYZ_salesfact\u05d2\u20ac is incorrect because we also need to copy the data onto the new table.<br>Option \u05d2\u20acTruncate the partition containing the stale data\u05d2\u20ac is incorrect because we need to switch the partition.<br>Option \u05d2\u20acExecute the DELETE statement where the value in the Date column is greater than 12 months\u05d2\u20ac is incorrect because issuing the DELETE statement would take time.<br>Reference:<br>https://blogs.msdn.microsoft.com/apsblog/2018/06/18/azure-sql-dw-performance-ctaspartition-switching-vs-updatedelete/",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-19T07:55:00.000Z",
        "voteCount": 8,
        "content": "I would say A instead of C because there's no need to copy data, just 'delete' the last month by sending it to the new table wich later is gonna be dropped. \nSo for me the correct answer is A, B E."
      },
      {
        "date": "2021-08-14T18:11:00.000Z",
        "voteCount": 1,
        "content": "Provided answer BCE is correct.\n\"\"The preferred method is to utilize a methodology of CTAS and partition switching in lieu of UPDATE and DELETE operations wherever possible.\"\"\n\nhttps://docs.microsoft.com/en-gb/archive/blogs/apsblog/azure-sql-dw-performance-ctaspartition-switching-vs-updatedelete"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/57636-exam-dp-200-topic-6-question-90-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>XYZ is an online training provider. They also provide a yearly gaming competition for their students. The competition is held every month in different locations.<br><br>Current Environment -<br>The company currently has the following environment in place:<br>* The racing cars for the competition send their telemetry data to a MongoDB database. The telemetry data has around 100 attributes.<br>* A custom application is then used to transfer the data from the MongoDB database to a SQL Server 2017 database. The attribute names are changed when they are sent to the SQL Server database.<br>* Another application named \"XYZ workflow\" is then used to perform analytics on the telemetry data to look for improvements on the racing cars.<br>* The SQL Server 2017 database has a table named \"cardata\" which has around 1 TB of data. \"XYZ workflow\" performs the required analytics on the data in this table. Large aggregations are performed on a column of the table.<br><br>Proposed Environment -<br>The company now wants to move the environment to Azure. Below are the key requirements:<br>* The racing car data will now be moved to Azure Cosmos DB and Azure SQL database. The data must be written to the closest Azure data center and must converge in the least amount of time.<br>* The query performance for data in the Azure SQL database must be stable without the need of administrative overhead<br>* The data for analytics will be moved to an Azure SQL Data warehouse<br>* Transparent data encryption must be enabled for all data stores wherever possible<br>* An Azure Data Factory pipeline will be used to move data from the Cosmos DB database to the Azure SQL database. If there is a delay of more than 15 minutes for the data transfer, then configuration changes need to be made to the pipeline workflow.<br>* The telemetry data must be monitored for any sort of performance issues.<br>* The Request Units for Cosmos DB must be adjusted to maintain the demand while also minimizing costs.<br>* The data in the Azure SQL Server database must be protected via the following requirements:<br>- Only the last four digits of the values in the column CarID must be shown<br>- A zero value must be shown for all values in the column CarWeight<br>Which of the following would you use for the consistency level for the database?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEventual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStrong",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsistent prefix"
    ],
    "answer": "A",
    "answerDescription": "Since there is a requirement for data to be written to the closest data center for Cosmos DB, we need to ensure there is a multi-master setup for Cosmos DB wherein data can be written from multiple regions. For such accounts, we can't set the consistency level to Strong.<br>The Microsoft documentation mentions the following:<br>Strong consistency and multi-master<br>Cosmos accounts configured for multi-master cannot be configured for strong consistency as it is not possible for a distributed system to provide an RPO of zero and an RTO of zero. Additionally, there are no write latency benefits for using strong consistency with multi-master as any write into any region must be replicated and committed to all configured regions within the account. This results in the same write latency as a single master account.<br>Hence if we want data to converge in the least amount of time, we need to use Eventual consistency. This offers the least latency in terms of consistency.<br>The Microsoft documentation mentions the following on the consistency levels.<br>With Azure Cosmos DB, developers can choose from five well-defined consistency models on the consistency spectrum. From strongest to more relaxed, the models include strong, bounded staleness, session, consistent prefix, and eventual consistency. The models are well-defined and intuitive and can be used for specific real-world scenarios. Each model provides availability and performance tradeoffs and is backed by the SLAs. The following image shows the different consistency levels as a spectrum.<br><img src=\"/assets/media/exam-media/03872/0062700001.png\" class=\"in-exam-image\"><br>Because of the proposed logic to the consistency level, all other options are incorrect.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels-tradeoffs https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-20T14:56:00.000Z",
        "voteCount": 2,
        "content": "As you cannot use Strong due to the multi-master model, the next higher availability in the answers is Session"
      },
      {
        "date": "2021-07-11T08:39:00.000Z",
        "voteCount": 2,
        "content": "In my view the answer should be \"Session\" as data must converge in the least amount of time"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60326-exam-dp-200-topic-6-question-98-discussion/",
    "body": "A development team is making use of Azure Stream Analytics. They have defined the following SQL query:<br>WITH step1 AS (SELECT * FROM input1 PARTITION BY XYZID INTO 10), step2 AS (SELECT * FROM input2 PARTITION BY XYZID INTO 10)<br>SELECT * INTO output FROM step1 PARTITION BY XYZID UNION step2 PARTITION BY XYZID<br>Does the query represent the joining of two streams of repartitioned data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Yes, this query does represent the joining of two streams of repartitioned data.<br>An example of this is given in the Microsoft documentation:<br>The following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.<br><img src=\"/assets/media/exam-media/03872/0064000001.jpg\" class=\"in-exam-image\"><br>The output scheme should match the stream scheme key and count so that each substream can be flushed independently. The stream could also be merged and repartitioned again by a different scheme before flushing, but you should avoid that method because it adds to the general latency of the processing and increases resource utilization.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/repartition",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-22T23:44:00.000Z",
        "voteCount": 1,
        "content": "This is a union of 2 streams, not a join."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/57639-exam-dp-200-topic-6-question-108-discussion/",
    "body": "You have to deploy resources on Azure HDInsight for a batch processing job. The batch processing must run daily and must scale to minimize costs. You also be able to monitor cluster performance.<br>You need to decide on a tool that will monitor the clusters and provide information on suggestions on how to scale.<br>You decide on monitoring the cluster load by using the Ambari Web UI.<br>Would this fulfill the requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Yes, this will give you a good idea on the load on the Azure HDInsight cluster.<br>The Microsoft documentation mentions the following:<br><br>Monitor cluster load -<br>Hadoop clusters can deliver the most optimal performance when the load on cluster is evenly distributed across all the nodes. This enables the processing tasks to run without being constrained by RAM, CPU, or disk resources on individual nodes.<br>To get a high-level look at the nodes of your cluster and their loading, sign in to the Ambari Web UI, then select the Hosts tab. Your hosts are listed by their fully qualified domain names. Each host's operating status is shown by a colored health indicator:<br><img src=\"/assets/media/exam-media/03872/0064900001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-key-scenarios-to-monitor",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-11T08:51:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B . Ambari web ui provides monitoring but is not able to provides insights on scaling"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/57640-exam-dp-200-topic-6-question-116-discussion/",
    "body": "A company plans to store hundreds of files in an Azure Storage account and in Azure Data Lake Storage account. The files will be stored in the parquet format. A solution must be in place that would adopt the following requirements:<br>- Provide the ability to process the data every 5 hours<br>- Give the ability for interactive data analysis<br>- Give the ability to process data using solid-state drive caching<br>- Make use of Directed Acyclic Graph processing mechanisms<br>- Provide support for REST API calls for monitoring purposes<br>- Ensure support for Python and Integration with Microsoft Power BI<br>Which of the following would you consider for the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Datawarehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Apache storm cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Spark cluster"
    ],
    "answer": "B",
    "answerDescription": "All of these features are provided with the HDInsight Apache Storm cluster. The Microsoft documentation mentions the following:<br>Why use Apache Storm on HDInsight?<br>Storm on HDInsight provides the following features:<br>\u05d2\u20ac\u00a2 99% Service Level Agreement (SLA) on Storm uptime: For more information, see the SLA information for HDInsight document.<br>\u05d2\u20ac\u00a2 Supports easy customization by running scripts against a Storm cluster during or after creation. For more information, see Customize HDInsight clusters using script action.<br>\u05d2\u20ac\u00a2 Create solutions in multiple languages: You can write Storm components in the language of your choice, such as Java, C#, and Python.<br>- Integrates Visual Studio with HDInsight for the development, management, and monitoring of C# topologies. For more information, see Develop C# Storm topologies with the HDInsight Tools for Visual Studio.<br>- Supports the Trident Java interface. You can create Storm topologies that support exactly once processing of messages, transactional datastore persistence, and a set of common stream analytics operations.<br>\u05d2\u20ac\u00a2 Dynamic scaling: You can add or remove worker nodes with no impact to running Storm topologies.<br>- You must deactivate and reactivate running topologies to take advantage of new nodes added through scaling operations.<br>\u05d2\u20ac\u00a2 Create streaming pipelines using multiple Azure services: Storm on HDInsight integrates with other Azure services such as Event Hubs, SQL Database, Azure<br>Storage, and Azure Data Lake Storage.<br>All of the other options are incorrect because they don't provide all of the capabilities.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-22T02:03:00.000Z",
        "voteCount": 1,
        "content": "\"Ensure support for Python \" this is seems you have to use spark"
      },
      {
        "date": "2021-07-11T09:02:00.000Z",
        "voteCount": 1,
        "content": "should be D .. Spark Cluster has all the requirements as mentioned in the reference link : https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  },
  {
    "topic": 6,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/57641-exam-dp-200-topic-6-question-124-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an<br>All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>XYZ is an online training provider.<br><br>Current Environment -<br>The company currently has Microsoft SQL databases that are split into different categories or tiers. Some of the databases are used by Internal users, some by external partners and external distributions.<br>Below is the List of applications, tiers and their individual requirements:<br><img src=\"/assets/media/exam-media/03872/0066600001.jpg\" class=\"in-exam-image\"><br>Below are the current requirements of the company:<br>* For Tier 4 and Tier 5 databases, the backup strategy must include the following:<br>- Transactional log backup every hour<br>- Differential backup every day<br>- Full backup every week<br>* Backup strategies must be in place for all standalone Azure SQL databases using methods available with Azure SQL databases<br>* Tier 1 database must implement the following data masking logic:<br>- For Data type XYZ-A `\" Mask 4 or less string data type characters<br>- For Data type XYZ-B `\" Expose the first letter and mask the domain<br>- For Data type XYZ-C `\" Mask everything except characters at the beginning and the end<br>* All certificates and keys are internally managed in on-premise data stores<br>* For Tier 2 databases, if there are any conflicts between the data transfer from on-premise, preference should be given to on-premise data.<br>* Monitoring must be setup on every database<br>* Applications with Tiers 6 through 8 must ensure that unexpected resource storage usage is immediately reported to IT data engineers.<br>* Azure SQL Data warehouse would be used to gather data from multiple internal and external databases.<br>* The Azure SQL Data warehouse must be optimized to use data from its cache<br>* The below metrics must be available when it comes to the cache:<br>- Metric XYZ-A `\" Low cache hit %, high cache usage %<br>- Metric XYZ-B `\" Low cache hit %, low cache usage %<br>- Metric XYZ-C `\" high cache hit %, high cache usage %<br>* The reporting data for external partners must be stored in Azure storage. The data should be made available during regular business hours in connecting regions.<br>* The reporting for Tier 9 needs to be moved to Event Hubs.<br>* The reporting for Tier 10 needs to be moved to Azure Blobs.<br>The following issues have been identified in the setup:<br>* The External partners have control over the data formats, types and schemas.<br>* For External based clients, the queries can't be changed or optimized.<br>* The database development staff are familiar with T-SQL language.<br>* Because of the size and amount of data, some applications and reporting features are not performing at SLA levels.<br>The data for the external applications needs to be encrypted at rest. You decide to implement the following steps:<br>- Use the Always Encrypted Wizard in SQL Server Management Studio<br>- Select the column that needs to be encrypted<br>- Set the encryption type to Randomized<br>- Configure the master key to be used from the Windows Certificate Store<br>- Confirm the configuration and deploy the solution<br>Would these steps fulfill the requirement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "As per the documentation, the encryption type needs to set as Deterministic when enabling Always Encrypted:<br><br>Column Selection -<br>Click Next on the Introduction page to open the Column Selection page. On this page, you will select which columns you want to encrypt, the type of encryption, and what column encryption key (CEK) to use.<br>Encrypt SSN and BirthDate information for each patient. The SSN column will use deterministic encryption, which supports equality lookups, joins, and group by.<br>The BirthDate column will use randomized encryption, which does not support operations.<br>Set the Encryption Type for the SSN column to Deterministic and the BirthDate column to Randomized. Click Next.<br><img src=\"/assets/media/exam-media/03872/0066800001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-always-encrypted",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-11T09:09:00.000Z",
        "voteCount": 1,
        "content": "i think answer should be yes ..but deterministic and randomized can be chosen"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "6"
  }
]