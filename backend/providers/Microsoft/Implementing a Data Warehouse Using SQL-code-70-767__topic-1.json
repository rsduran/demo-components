[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18238-exam-70-767-topic-1-question-1-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft Azure SQL Data Warehouse instance that must be available six months a day for reporting.<br>You need to pause the compute resources when the instance is not being used.<br>Solution: You use SQL Server Configuration Manager.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "To pause a SQL Data Warehouse database, use any of these individual methods.<br><br>Pause compute with Azure portal -<br><br>Pause compute with PowerShell -<br><br>Pause compute with REST APIs -<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-17T12:05:00.000Z",
        "voteCount": 3,
        "content": "I'm goint to say \"No\", since according to my google search, to pause compute resource you do it via the Azure Portal.  See the following link:  https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/pause-and-resume-compute-portal"
      },
      {
        "date": "2020-04-26T08:29:00.000Z",
        "voteCount": 1,
        "content": "Or, I'll take another stab; and, say he's on another planet.  If my memory serves me correctly from HS physical science, Venus' year is shorter than its day. :&gt;)"
      },
      {
        "date": "2020-04-11T11:39:00.000Z",
        "voteCount": 1,
        "content": "Six months a day ?"
      },
      {
        "date": "2020-04-15T07:53:00.000Z",
        "voteCount": 1,
        "content": "Yeah; that's some clock they're using.  I take a stab and say the author meant 6 hours a day."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20782-exam-70-767-topic-1-question-2-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft Azure SQL Data Warehouse instance that must be available six months a day for reporting.<br>You need to pause the compute resources when the instance is not being used.<br>Solution: You use the Azure portal.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "To pause a SQL Data Warehouse database, use any of these individual methods.<br><br>Pause compute with Azure portal -<br><br>Pause compute with PowerShell -<br><br>Pause compute with REST APIs -<br>Note: To pause a database:<br>1. Open the Azure portal and open your database. Notice that the Status is Online.<br><img src=\"/assets/media/exam-media/02783/0000400001.png\" class=\"in-exam-image\"><br><br>2. To suspend compute and memory resources, click Pause, and then a confirmation message appears. Click yes to confirm or no to cancel.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-overview https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-portal#pause-compute-bk",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-17T12:12:00.000Z",
        "voteCount": 4,
        "content": "I think the answer should be \"Yes\""
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20784-exam-70-767-topic-1-question-4-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a data warehouse that stores information about products, sales, and orders for a manufacturing company. The instance contains a database that has two tables named SalesOrderHeader and SalesOrderDetail. SalesOrderHeader has 500,000 rows and SalesOrderDetail has 3,000,000 rows.<br>Users report performance degradation when they run the following stored procedure:<br><img src=\"/assets/media/exam-media/02783/0000500001.png\" class=\"in-exam-image\"><br><br>You need to optimize performance.<br>Solution: You run the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0000600001.png\" class=\"in-exam-image\"><br><br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "100 out of 500,000 rows is a too small sample size.<br>References: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-statistics",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-12T01:44:00.000Z",
        "voteCount": 6,
        "content": "No is correct -To few rows sampled"
      },
      {
        "date": "2020-05-17T12:42:00.000Z",
        "voteCount": 3,
        "content": "The answer should be \"No\".  Can someone confirm?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/34206-exam-70-767-topic-1-question-5-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a data warehouse that stores information about products, sales, and orders for a manufacturing company. The instance contains a database that has two tables named SalesOrderHeader and SalesOrderDetail. SalesOrderHeader has 500,000 rows and SalesOrderDetail has 3,000,000 rows.<br>Users report performance degradation when they run the following stored procedure:<br><img src=\"/assets/media/exam-media/02783/0000700001.png\" class=\"in-exam-image\"><br><br>You need to optimize performance.<br>Solution: You run the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0000700002.png\" class=\"in-exam-image\"><br><br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Microsoft recommend against specifying 0 PERCENT or 0 ROWS in a CREATE STATISTICS..WITH SAMPLE statement. When 0 PERCENT or ROWS is specified, the statistics object is created but does not contain statistics data.<br>References: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-statistics-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-26T06:23:00.000Z",
        "voteCount": 1,
        "content": "The answer is NO. It's correct."
      },
      {
        "date": "2020-10-11T09:02:00.000Z",
        "voteCount": 1,
        "content": "We should not specify 0 PERCENT or 0 Rows to update the statistics because it just updates the statistics object, but it does not contain statistics data.\nSource: https://www.sqlshack.com/sql-server-statistics-and-how-to-perform-update-statistics-in-sql/"
      },
      {
        "date": "2021-01-06T13:25:00.000Z",
        "voteCount": 1,
        "content": "So, What is the answer?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23429-exam-70-767-topic-1-question-6-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a data warehouse that stores information about products, sales, and orders for a manufacturing company. The instance contains a database that has two tables named SalesOrderHeader and SalesOrderDetail. SalesOrderHeader has 500,000 rows and SalesOrderDetail has 3,000,000 rows.<br>Users report performance degradation when they run the following stored procedure:<br><img src=\"/assets/media/exam-media/02783/0000800001.png\" class=\"in-exam-image\"><br><br>You need to optimize performance.<br>Solution: You run the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0000800002.png\" class=\"in-exam-image\"><br><br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "You can specify the sample size as a percent. A 5% statistics sample size would be helpful.<br>References: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-statistics",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-18T01:23:00.000Z",
        "voteCount": 8,
        "content": "For less than 1 billion it should be at least the default sampling (20 percent). So shouldn't it be No?"
      },
      {
        "date": "2021-01-29T13:32:00.000Z",
        "voteCount": 1,
        "content": "Correct, the answer is NO."
      },
      {
        "date": "2020-08-11T10:12:00.000Z",
        "voteCount": 7,
        "content": "Less than 1 billion rows, use default sampling (20 percent).\nWith more than 1 billion rows, use sampling of two percent.\n\nSince we have less than 1 billion rows, it must be 20 percent, not 5 percent. I think the answer is NO."
      },
      {
        "date": "2020-11-03T15:50:00.000Z",
        "voteCount": 2,
        "content": "why this is yes, when the optimal would be 20% unless 1 bil ?"
      },
      {
        "date": "2020-07-30T01:41:00.000Z",
        "voteCount": 1,
        "content": "One best practice is to update statistics on date columns each day as new dates are added. Each time new rows are loaded into the SQL pool, new load dates or transaction dates are added. These additions change the data distribution and make the statistics out of date.\n\nStatistics on a country/region column in a customer table might never need to be updated since the distribution of values doesn't generally change. Assuming the distribution is constant between customers, adding new rows to the table variation isn't going to change the data distribution.\n\nHowever, if your SQL pool only contains one country/region, and you bring in data from a new country/region, resulting in data from multiple countries/regions being stored, then you need to update statistics on the country/region column.\n\nThe following are recommendations updating statistics:\n\nUPDATE STATISTICS\nFrequency of stats updates\tConservative: Daily\nAfter loading or transforming your data\nSampling\tLess than 1 billion rows, use default sampling (20 percent).\nWith more than 1 billion rows, use sampling of two percent."
      },
      {
        "date": "2020-07-25T07:53:00.000Z",
        "voteCount": 2,
        "content": "in the reference link it says \"The following are RECOMMENDATIONS updating statistics:\nSampling\tLess than 1 billion rows, use default sampling (20 percent)\". That's different that it should be neccesary at least 20%. \nSo maybe 5% would be helpful."
      },
      {
        "date": "2020-12-16T22:02:00.000Z",
        "voteCount": 1,
        "content": "So what are you saying? Is it Yes or No? Seems your reference suggests NO but you\u2019re saying the answer is YES. You\u2019re confusing people."
      },
      {
        "date": "2020-07-12T01:45:00.000Z",
        "voteCount": 3,
        "content": "it is No  5% is too few"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28450-exam-70-767-topic-1-question-7-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the following line-of-business solutions:<br>\u2711 ERP system<br>\u2711 Online WebStore<br>\u2711 Partner extranet<br>One or more Microsoft SQL Server instances support each solution. Each solution has its own product catalog. You have an additional server that hosts SQL<br>Server Integration Services (SSIS) and a data warehouse. You populate the data warehouse with data from each of the line-of-business solutions. The data warehouse does not store primary key values from the individual source tables.<br>The database for each solution has a table named Products that stored product information. The Products table in each database uses a separate and unique key for product records. Each table shares a column named ReferenceNr between the databases. This column is used to create queries that involve more than once solution.<br>You need to load data from the individual solutions into the data warehouse nightly. The following requirements must be met:<br>\u2711 If a change is made to the ReferenceNr column in any of the sources, set the value of IsDisabled to True and create a new row in the Products table.<br>\u2711 If a row is deleted in any of the sources, set the value of IsDisabled to True in the data warehouse.<br>Solution: Perform the following actions:<br>\u2711 Enable the Change Tracking for the Product table in the source databases.<br>\u2711 Query the CHANGETABLE function from the sources for the updated rows.<br>\u2711 Set the IsDisabled column to True for the listed rows that have the old ReferenceNr value.<br>\u2711 Create a new row in the data warehouse Products table with the new ReferenceNr value.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We must check for deleted rows, not just updates rows.<br>References: https://www.timmitchell.net/post/2016/01/18/getting-started-with-change-tracking-in-sql-server/",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-10T09:18:00.000Z",
        "voteCount": 2,
        "content": "I'm going with YES. \"SQL Server 2008 introduces two tracking features that enable applications to determine the DML changes (insert, update, and delete operations) that were made to user tables in a database. Before these features were available, custom tracking mechanisms had to be implemented in applications. Change tracking captures the fact that rows in a table were changed, but does not capture the data that was changed. This enables applications to determine the rows that have changed with the latest row data being obtained directly from the user tables.\"\n\nhttps://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/cc280519(v=sql.105)?redirectedfrom=MSDN"
      },
      {
        "date": "2021-01-28T04:45:00.000Z",
        "voteCount": 1,
        "content": "Aww so the answer is YES"
      },
      {
        "date": "2021-01-29T13:50:00.000Z",
        "voteCount": 1,
        "content": "Wrong. The answer is NO. CDC is what you want not simply Change Tracking."
      },
      {
        "date": "2020-09-27T22:39:00.000Z",
        "voteCount": 2,
        "content": "\"Enable the Change Tracking\" - This only lets you know that a particular row has changed since your last query. You have no idea - how many times it\u2019s changed, the various change values over time.\n\nSo, you need a solution with CDC.\n\nhttps://www.sqlservercentral.com/blogs/the-difference-between-change-tracking-and-change-data-capture"
      },
      {
        "date": "2020-09-27T22:43:00.000Z",
        "voteCount": 1,
        "content": "Question 9 has the cdc.fn_cdc_get_all_changes_capture_dbo_products function but it is not doing this - \"Set the IsDIsabled column to True on the data warehouse Products table for the listed rows.\""
      },
      {
        "date": "2020-09-14T02:42:00.000Z",
        "voteCount": 1,
        "content": "if we query the change table won't it show all changed records?(updates and deletes?"
      },
      {
        "date": "2020-08-16T03:55:00.000Z",
        "voteCount": 4,
        "content": "While querying we need to get deleted records too, to update the IsDisabled column to TRUE. I think the answer is correct NO."
      },
      {
        "date": "2020-08-13T04:59:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is YES. I do not see any reason why it must be NO. Any hint?"
      },
      {
        "date": "2020-10-06T10:31:00.000Z",
        "voteCount": 3,
        "content": "you can see it from the decision - its only about updated rows, but the question is also about deleted rows"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37718-exam-70-767-topic-1-question-9-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the following line-of-business solutions:<br>\u2711 ERP system<br>\u2711 Online WebStore<br>\u2711 Partner extranet<br>One or more Microsoft SQL Server instances support each solution. Each solution has its own product catalog. You have an additional server that hosts SQL<br>Server Integration Services (SSIS) and a data warehouse. You populate the data warehouse with data from each of the line-of-business solutions. The data warehouse does not store primary key values from the individual source tables.<br>The database for each solution has a table named Products that stored product information. The Products table in each database uses a separate and unique key for product records. Each table shares a column named ReferenceNr between the databases. This column is used to create queries that involve more than once solution.<br>You need to load data from the individual solutions into the data warehouse nightly. The following requirements must be met:<br>\u2711 If a change is made to the ReferenceNr column in any of the sources, set the value of IsDisabled to True and create a new row in the Products table.<br>\u2711 If a row is deleted in any of the sources, set the value of IsDisabled to True in the data warehouse.<br>Solution: Perform the following actions:<br>\u2711 Enable the Change Tracking for the Product table in the source databases.<br>\u2711 Query the cdc.fn_cdc_get_all_changes_capture_dbo_products function from the sources for updated rows.<br>\u2711 Set the IsDisabled column to True for rows with the old ReferenceNr value.<br>\u2711 Create a new row in the data warehouse Products table with the new ReferenceNr value.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We must also handle the deleted rows, not just the updated rows.<br>References: https://solutioncenter.apexsql.com/enable-use-sql-server-change-data-capture/",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-24T18:18:00.000Z",
        "voteCount": 3,
        "content": "\u2711 Query the cdc.fn_cdc_get_all_changes_capture_dbo_products function from the sources for updated rows.\n\nWe dont have CDC... The answer its NO"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18324-exam-70-767-topic-1-question-14-discussion/",
    "body": "DRAG DROP -<br>Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in this series.<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>You have the following requirements:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>\u2711 Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>\u2711 Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>Maximize the performance during the data loading process for the Fact.Order partition.<br><img src=\"/assets/media/exam-media/02783/0001800006.png\" class=\"in-exam-image\"><br>\u2711 Ensure that historical data remains online and available for querying.<br>\u2711 Reduce ongoing storage costs while maintaining query performance for current data.<br>You are not permitted to make changes to the client applications.<br>You need to configure the Fact.Order table.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0001900003.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0002000001.png\" class=\"in-exam-image\">",
    "answerDescription": "From scenario: Partition the Fact.Order table and retain a total of seven years of data. Maximize the performance during the data loading process for the<br>Fact.Order partition.<br>Step 1: Create a partition function.<br>Using CREATE PARTITION FUNCTION is the first step in creating a partitioned table or index.<br>Step 2: Create a partition scheme based on the partition function.<br>To migrate SQL Server partition definitions to SQL Data Warehouse simply:<br>\u2711 Eliminate the SQL Server partition scheme.<br>\u2711 Add the partition function definition to your CREATE TABLE.<br>Step 3: Execute an ALTER TABLE command to specify the partition function.<br>References: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-partition",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-12T14:34:00.000Z",
        "voteCount": 5,
        "content": "The third step in the answer area is different than the explanation. Scheme/Function"
      },
      {
        "date": "2020-06-01T18:54:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct. We use Schema to partition the table."
      },
      {
        "date": "2020-07-25T08:11:00.000Z",
        "voteCount": 5,
        "content": "1. create partition function\n2. create partition scheme based on partition function\n3. alter table to specify partition scheme"
      },
      {
        "date": "2020-07-25T08:17:00.000Z",
        "voteCount": 3,
        "content": "https://docs.microsoft.com/en-us/sql/relational-databases/partitions/create-partitioned-tables-and-indexes?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18398-exam-70-767-topic-1-question-15-discussion/",
    "body": "DRAG DROP -<br>Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in this series.<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>You have the following requirements:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 - Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 - Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>\u2711 - Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>\u2711 - Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>\u2711 - Maximize the performance during the data loading process for the Fact.Order partition.<br>\u2711 - Ensure that historical data remains online and available for querying.<br>\u2711 - Reduce ongoing storage costs while maintaining query performance for current data.<br>You are not permitted to make changes to the client applications.<br>You need to optimize data loading for the Dimension.Customer table.<br>Which three Transact-SQL segments should you use to develop the solution? To answer, move the appropriate Transact-SQL segments from the list of Transact-<br>SQL segments to the answer area and arrange them in the correct order.<br>NOTE: You will not need all of the Transact-SQL segments.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0002200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0002300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: USE DB1 -<br>From Scenario: All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment.<br>Step 2: EXEC sys.sp_cdc_enable_db<br>Before you can enable a table for change data capture, the database must be enabled. To enable the database, use the sys.sp_cdc_enable_db stored procedure. sys.sp_cdc_enable_db has no parameters.<br>Step 3: EXEC sys.sp_cdc_enable_table<br>@source schema = N 'schema' etc.<br>Sys.sp_cdc_enable_table enables change data capture for the specified source table in the current database.<br>Partial syntax:<br>sys.sp_cdc_enable_table<br>[ @source_schema = ] 'source_schema',<br>[ @source_name = ] 'source_name' , [,[ @capture_instance = ] 'capture_instance' ]<br>[,[ @supports_net_changes = ] supports_net_changes ]<br>Etc.<br>References: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-enable-table-transact-sql https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-enable-db-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-02T02:10:00.000Z",
        "voteCount": 2,
        "content": "Of course, It's tricky question which clearly says Data Ware House (DB1) supports Client applications (might be Tabluea Server, BI Reporting) &amp; we're not permitted to do any change on client applications, so either we enable CDC on DB1 or DB2 we're not touching Client Application, so, I would say answer is correct for DB1 because it doesn't make any sense to enable CDC on Development environment"
      },
      {
        "date": "2020-12-01T23:12:00.000Z",
        "voteCount": 2,
        "content": "i would say db2 since  You are not permitted to make changes to the client applications."
      },
      {
        "date": "2020-04-13T22:50:00.000Z",
        "voteCount": 2,
        "content": "Since you should delevop a solution: isn't it better to do that on the DB2? Thus, Use DB2, right?"
      },
      {
        "date": "2020-04-19T08:10:00.000Z",
        "voteCount": 9,
        "content": "Good thought, Dieter; but DB2 is a development environment.  A developer, in an exercise of caution, might want to do a dry run on the development platform; but ultimately, this change is going to land on the production system, DB1."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31248-exam-70-767-topic-1-question-16-discussion/",
    "body": "Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in this series.<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>You have the following requirements:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>\u2711 Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>\u2711 Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>\u2711 Maximize the performance during the data loading process for the Fact.Order partition.<br>\u2711 Ensure that historical data remains online and available for querying.<br>\u2711 Reduce ongoing storage costs while maintaining query performance for current data.<br>You are not permitted to make changes to the client applications.<br>You need to implement the data partitioning strategy.<br>How should you partition the Fact.Order table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate 17,520 partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a granularity of two days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate 2,557 partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate 730 partitions."
    ],
    "answer": "C",
    "answerDescription": "We create on partition for each day, which means that a granularity of one day is used. 7 years times 365 days is 2,555. Make that 2,557 to provide for leap years.<br>From scenario: Partition the Fact.Order table and retain a total of seven years of data.<br>The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily.<br>Maximize the performance during the data loading process for the Fact.Order partition.<br>Reference: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-partition",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-07T04:48:00.000Z",
        "voteCount": 2,
        "content": "Food for thought: https://docs.microsoft.com/en-us/archive/blogs/sqlcat/top-10-best-practices-for-building-a-large-scale-relational-data-warehouse\nPrior versions of SQL allowed a maximum 1000 partitions.  Nowadays the limit is 15000.\n\nAnswer of 2557 looks good to me."
      },
      {
        "date": "2020-10-18T07:48:00.000Z",
        "voteCount": 3,
        "content": "7(years) * 365(partition per day) = 2,555\n2,555 + 1(additional partition for a leap year) = 2,556\n2,556 + 1(additional parition for a new data) = 2,557"
      },
      {
        "date": "2020-09-14T03:26:00.000Z",
        "voteCount": 1,
        "content": "Don't we need to add an empty partition for new data?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/35987-exam-70-767-topic-1-question-17-discussion/",
    "body": "Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in this series.<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>You have the following requirements:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br><img src=\"/assets/media/exam-media/02783/0002500004.png\" class=\"in-exam-image\"><br>\u2711 Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>\u2711 Maximize the performance during the data loading process for the Fact.Order partition.<br>\u2711 Ensure that historical data remains online and available for querying.<br>\u2711 Reduce ongoing storage costs while maintaining query performance for current data.<br>You are not permitted to make changes to the client applications.<br>You need to optimize the storage for the data warehouse.<br>What change should you make?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the Fact.Order table, and move historical data to new filegroups on lower-cost storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate new tables on lower-cost storage, move the historical data to the new tables, and then shrink the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the historical data from the database to leave available space for new data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove historical data to new tables on lower-cost storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement row compression for the Fact.Order table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the index for the Fact.Order table to lower-cost storage."
    ],
    "answer": "A",
    "answerDescription": "Create the load staging table in the same filegroup as the partition you are loading.<br>Create the unload staging table in the same filegroup as the partition you are deleting.<br>From scenario: The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>References:<br>https://blogs.msdn.microsoft.com/sqlcat/2013/09/16/top-10-best-practices-for-building-a-large-scale-relational-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-17T12:23:00.000Z",
        "voteCount": 3,
        "content": "B context doesn't mention a table partitioning. In data warehouses, the process of determining historical data in non-partitioned tables and transferring data can be costly and less efficient than e.g. sliding window approach.\nI would prefer to choose A."
      },
      {
        "date": "2020-11-03T19:41:00.000Z",
        "voteCount": 1,
        "content": "can smb explain why it is not B ?\nI was struggling between A and B"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21045-exam-70-767-topic-1-question-18-discussion/",
    "body": "HOTSPOT -<br>You manage an inventory system that has a table named Products. The Products table has several hundred columns.<br>You generate a report that relates two columns named ProductReference and ProductName from the Products table. The result is sorted by a column named<br>QuantityInStock from largest to smallest.<br>You need to create an index that the report can use.<br>How should you complete the Transact-SQL statement? To answer, select the appropriate Transact-SQL segments in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0002700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0002800001.png\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-26T06:48:00.000Z",
        "voteCount": 1,
        "content": "Answer provided by the question is correct"
      },
      {
        "date": "2020-10-06T11:47:00.000Z",
        "voteCount": 2,
        "content": "the answer doesn't fit the question - as Products table doesn't have Quantity column"
      },
      {
        "date": "2020-10-06T11:51:00.000Z",
        "voteCount": 2,
        "content": "the answer should be:\n-nonclustered (as the table is too big to use clustered index, 100 columns)\n-ProductReference\n-ProductName"
      },
      {
        "date": "2020-12-17T04:33:00.000Z",
        "voteCount": 4,
        "content": "This answer doesnt make sense."
      },
      {
        "date": "2020-12-17T04:34:00.000Z",
        "voteCount": 2,
        "content": "The answer provided in the question is correct as is."
      },
      {
        "date": "2020-05-20T16:22:00.000Z",
        "voteCount": 2,
        "content": "I thought Clustered Indexes were more efficient.  Does anybody know the difference between a clustered index and a nonclustered index?"
      },
      {
        "date": "2020-05-20T17:33:00.000Z",
        "voteCount": 1,
        "content": "For those interested, I found the following info. on the differences between clustered and nonclustered indexes:  There can be only one clustered index per table. However, you can create multiple non-clustered indexes on a single table.\nClustered indexes only sort tables. Therefore, they do not consume extra storage. Non-clustered indexes are stored in a separate place from the actual table claiming more storage space.\nClustered indexes are faster than non-clustered indexes since they don\u2019t involve any extra lookup step."
      },
      {
        "date": "2020-06-12T00:26:00.000Z",
        "voteCount": 8,
        "content": "Clustered Index is used when you are working with the key of the table. \nSince in this scenario, they want to order it by StockQuantity which is not a key, we should create a nonclustered index and include those columns which we want to return in the select part"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28637-exam-70-767-topic-1-question-19-discussion/",
    "body": "HOTSPOT -<br>You manage a data warehouse in a Microsoft SQL Server instance. Company employee information is imported from the human resources system to a table named Employee in the data warehouse instance. The Employee table was created by running the query shown in the Employee Schema exhibit. (Click the<br>Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0002900001.png\" class=\"in-exam-image\"><br>The personal identification number is stored in a column named EmployeeSSN. All values in the EmployeeSSN column must be unique.<br>When importing employee data, you receive the error message shown in the SQL Error exhibit. (Click the Exhibit button.).<br><img src=\"/assets/media/exam-media/02783/0002900002.jpg\" class=\"in-exam-image\"><br>You determine that the Transact-SQL statement shown in the Data Load exhibit is the cause of the error. (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0002900003.png\" class=\"in-exam-image\"><br>You remove the constraint on the EmployeeSSN column. You need to ensure that values in the EmployeeSSN column are unique.<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0003000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0003000002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "With the ANSI standards SQL:92, SQL:1999 and SQL:2003, an UNIQUE constraint must disallow duplicate non-NULL values but accept multiple NULL values.<br>In the Microsoft world of SQL Server however, a single NULL is allowed but multiple NULLs are not.<br>From SQL Server 2008, you can define a unique filtered index based on a predicate that excludes NULLs.<br>References: https://stackoverflow.com/questions/767657/how-do-i-create-a-unique-constraint-that-also-allows-nulls",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-26T06:49:00.000Z",
        "voteCount": 1,
        "content": "No, Yes is the correct answer"
      },
      {
        "date": "2020-11-23T11:07:00.000Z",
        "voteCount": 3,
        "content": "It was this link :\nhttps://www.itprotoday.com/sql-server/whats-good-use-unique-filtered-index"
      },
      {
        "date": "2020-11-23T11:04:00.000Z",
        "voteCount": 2,
        "content": "Have a look:\nhttps://blog.sqlauthority.com/2008/09/01/sql-server-2008-introduction-to-filtered-index-improve-performance-with-filtered-index/#:~:text=Filtered%20Index%20is%20a%20new,compared%20with%20full%2Dtable%20indexes."
      },
      {
        "date": "2020-08-15T02:14:00.000Z",
        "voteCount": 1,
        "content": "I ma wondering why isn't it YES for both? :/"
      },
      {
        "date": "2020-08-27T14:22:00.000Z",
        "voteCount": 6,
        "content": "The table has already a clustered index because EmployeeID is an identity column and as we know a table can't have more then one clustered index"
      },
      {
        "date": "2020-08-31T03:40:00.000Z",
        "voteCount": 4,
        "content": "yes, you are right, thank you lahmed"
      },
      {
        "date": "2020-10-06T12:05:00.000Z",
        "voteCount": 2,
        "content": "if we make that clustered index there will be no nulls, but the table should have nulls"
      },
      {
        "date": "2020-12-18T06:50:00.000Z",
        "voteCount": 1,
        "content": "This is the real reason. You can always change the existing Primary Key to be non-clustered, but it wouldn't help in the case."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33824-exam-70-767-topic-1-question-20-discussion/",
    "body": "DRAG DROP -<br>You have a data warehouse.<br>You need to move a table named Fact.ErrorLog to a new filegroup named LowCost.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0003200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0003300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Add a filegroup named LowCost to the database.<br>First create a new filegroup.<br>Step 2:<br>The next stage is to go to the \"\u02dcFiles' page in the same Properties window and add a file to the filegroup (a filegroup always contains one or more files)<br>Step 3:<br>To move a table to a different filegroup involves moving the table's clustered index to the new filegroup. While this may seem strange at first this is not that surprising when you remember that the leaf level of the clustered index actually contains the table data. Moving the clustered index can be done in a single statement using the DROP_EXISTING clause as follows (using one of the AdventureWorks2008R2 tables as an example) :<br>CREATE UNIQUE CLUSTERED INDEX PK_Department_DepartmentID<br>ON HumanResources.Department(DepartmentID)<br>WITH (DROP_EXISTING=ON,ONLINE=ON) ON SECONDARY<br>This recreates the same index but on the SECONDARY filegroup.<br>References: http://www.sqlmatters.com/Articles/Moving%20a%20Table%20to%20a%20Different%20Filegroup.aspx",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-23T11:13:00.000Z",
        "voteCount": 2,
        "content": "I tried it and the solution works... The table partition is changed"
      },
      {
        "date": "2020-10-18T08:28:00.000Z",
        "voteCount": 1,
        "content": "In my opinion, the last step is: Create a new Fact.ErrorLog table on the LowCost filegroup. Rebuilding or reorganizing indexes don't make sense at this phase."
      },
      {
        "date": "2020-11-17T12:45:00.000Z",
        "voteCount": 2,
        "content": "My bad. I didn't read the instruction carefully. The solution is right."
      },
      {
        "date": "2020-10-06T12:29:00.000Z",
        "voteCount": 1,
        "content": "rebuild clustered index in the new filegroup? alter index... rebuild?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28645-exam-70-767-topic-1-question-21-discussion/",
    "body": "HOTSPOT -<br>Your company has a Microsoft SQL Server data warehouse instance. The human resources department assigns all employees a unique identifier. You plan to store this identifier in a new table named Employee.<br>You create a new dimension to store information about employees by running the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0003400001.png\" class=\"in-exam-image\"><br>You have not added data to the dimension yet. You need to modify the dimension to implement a new column named [EmployeeKey]. The new column must use unique values.<br>How should you complete the Transact-SQL statements? To answer, select the appropriate Transact-SQL segments in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0003500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0003600001.png\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-25T10:47:00.000Z",
        "voteCount": 1,
        "content": "employeID it can be business key or something else so they use employeKey"
      },
      {
        "date": "2020-11-03T21:32:00.000Z",
        "voteCount": 1,
        "content": "Yeah for me it is also not clear why EmployeeID is not Primary, given we extract EmployeeKey from source"
      },
      {
        "date": "2020-12-17T04:47:00.000Z",
        "voteCount": 1,
        "content": "If you do what you\u2019re saying, it means EmployeeKey is only defined as NOT NULL. But the requirement says it has to be UNIQUE. With the provided options, we have to make it Primary to make sure it is unique. So the provided answer is therefore correct."
      },
      {
        "date": "2020-12-19T03:24:00.000Z",
        "voteCount": 2,
        "content": "We are creating EmployeeKey as an IDENTITY column, it's NOT coming from source"
      },
      {
        "date": "2020-10-18T08:36:00.000Z",
        "voteCount": 1,
        "content": "The second option is not correct. The primary key should be defined for EmployeeID column, because is a surrogate key. In our case, EmployeeKey is an alternate key. \nIn my opinion in the second box is:\nPRIMARY KEY CLUSTERED (EmployeeID)"
      },
      {
        "date": "2020-12-19T03:26:00.000Z",
        "voteCount": 2,
        "content": "EmployeeKey is acting as a surrogate key, it is an identity column after all"
      },
      {
        "date": "2020-12-17T04:48:00.000Z",
        "voteCount": 2,
        "content": "If you do what you\u2019re saying, it means EmployeeKey is only defined as NOT NULL. But the requirement says it has to be UNIQUE. With the provided options, we have to make it Primary to make sure it is unique. So the provided answer is therefore correct."
      },
      {
        "date": "2020-08-15T03:50:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28651-exam-70-767-topic-1-question-22-discussion/",
    "body": "HOTSPOT -<br>You deploy a Microsoft Azure SQL Data Warehouse instance. The instance must be available eight hours each day.<br>You need to pause Azure resources when they are not in use to reduce costs.<br>What will be the impact of pausing resources? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0003700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0003700002.png\" class=\"in-exam-image\">",
    "answerDescription": "To save costs, you can pause and resume compute resources on-demand. For example, if you won't be using the database during the night and on weekends, you can pause it during those times, and resume it during the day. You won't be charged for DWUs while the database is paused.<br>When you pause a database:<br>Compute and memory resources are returned to the pool of available resources in the data center<br>Data Warehouse Unit (DWU) costs are zero for the duration of the pause.<br>Data storage is not affected and your data stays intact.<br>SQL Data Warehouse cancels all running or queued operations.<br>When you resume a database:<br>SQL Data Warehouse acquires compute and memory resources for your DWU setting.<br>Compute charges for your DWUs resume.<br>Your data will be available.<br>You will need to restart your workload queries.<br>References: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-manage-compute-rest-api",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-15T05:43:00.000Z",
        "voteCount": 4,
        "content": "Correct. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/pause-and-resume-compute-portal"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19629-exam-70-767-topic-1-question-24-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has three databases as described in the following table.<br><img src=\"/assets/media/exam-media/02783/0004100001.png\" class=\"in-exam-image\"><br><br>You plan to load at least one million rows of data each night from DB1 into the OnlineOrder table. You must load data into the correct partitions using a parallel process.<br>You create 24 Data Flow tasks. You must place the tasks into a component to allow parallel load. After all of the load processes compete, the process must proceed to the next task.<br>You need to load the data for the OnlineOrder table.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "H",
    "answerDescription": "The Parallel Loop Task is an SSIS Control Flow task, which can execute multiple iterations of the standard Foreach Loop Container concurrently.<br>References: http://www.cozyroc.com/ssis/parallel-loop-task",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-08T00:06:00.000Z",
        "voteCount": 8,
        "content": "A standard for each loop container cannot process items in parallel! In a sequentiell container you can process tasks in parallel."
      },
      {
        "date": "2020-05-16T04:24:00.000Z",
        "voteCount": 3,
        "content": "Thanks, you are right. Sequence container can do tasks within in sequence (when connected) or in parallel (when not connected)."
      },
      {
        "date": "2020-10-08T01:09:00.000Z",
        "voteCount": 2,
        "content": "Parallel Loop Task is not native SSIS task. So G"
      },
      {
        "date": "2020-09-27T23:00:00.000Z",
        "voteCount": 2,
        "content": "It is G, here is the explanation with some examples - https://social.msdn.microsoft.com/Forums/sqlserver/en-US/3adbd2f6-b611-4f69-a904-64f993b2af29/is-it-possible-to-run-multiple-data-flow-tasks-simultaneously?forum=sqlintegrationservices"
      },
      {
        "date": "2020-08-15T07:29:00.000Z",
        "voteCount": 1,
        "content": "Agree G is correct"
      },
      {
        "date": "2020-08-04T04:05:00.000Z",
        "voteCount": 2,
        "content": "Correct answer: G"
      },
      {
        "date": "2020-07-14T06:13:00.000Z",
        "voteCount": 1,
        "content": "So the answer is H \"Foreach loop Container\" ?"
      },
      {
        "date": "2020-07-25T08:49:00.000Z",
        "voteCount": 4,
        "content": "I think the right answer is Sequential container. \nA standard foreach loop cannot process items in parallel. \nWith a sequence container any tasks that are not preceded by a precedence constraint will run in parallel."
      },
      {
        "date": "2020-05-04T22:21:00.000Z",
        "voteCount": 1,
        "content": "Sure? The answer talks about parallel loop container which is not standard in SSIS (see Reference). Although, putting all 24 data flows in one for each loop container should do the work."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/12481-exam-70-767-topic-1-question-25-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has the databases described in the following table.<br><img src=\"/assets/media/exam-media/02783/0004200001.png\" class=\"in-exam-image\"><br><br>Each day, you publish a Microsoft Excel workbook that contains a list of product names and current prices to an external website. Suppliers update pricing information in the workbook. Each supplier saves the workbook with a unique name.<br>Each night, the Products table is deleted and refreshed from MDS by using a Microsoft SQL Server Integration Services (SSIS) package. All files must be loaded in sequence.<br>You need to add a data flow in an SSIS package to perform the Excel files import in the data warehouse.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "A",
    "answerDescription": "If you're familiar with SSIS and don't want to run the SQL Server Import and Export Wizard, create an SSIS package that uses the Excel Source and the SQL<br>Server Destination in the data flow.<br><img src=\"/assets/media/exam-media/02783/0004300001.png\" class=\"in-exam-image\"><br><br>References: https://docs.microsoft.com/en-us/sql/integration-services/import-export-data/import-data-from-excel-to-sql",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-21T19:58:00.000Z",
        "voteCount": 10,
        "content": "H. Foreach Loop container\n\nForeach File enumerator to enumerate files in a folder. The enumerator can traverse subfolders. For example, you can read all the files that have the *.log file name extension in the Windows folder and its subfolders. Note that the order in which the files are retrieved cannot be specified.\nhttps://docs.microsoft.com/en-us/sql/integration-services/control-flow/foreach-loop-container?view=sql-server-ver15\nLoad multiple files with SSIS Foreach Loop Container\nhttps://jimsalasek.com/2017/09/29/load-multiple-files-with-ssis-foreach-loop-container/"
      },
      {
        "date": "2020-01-21T19:02:00.000Z",
        "voteCount": 7,
        "content": "How is the \"Lookup transformation\" concept related to the answer below described (Import data from Excel to SQL Server or Azure SQL Database)?"
      },
      {
        "date": "2020-09-27T23:19:00.000Z",
        "voteCount": 1,
        "content": "Facts from the question - Every DAY - multiple xls files with the Product data, after that at night we need to load all xls files SEQUENTIALLY into EMPTY PRODUCT table using A (ONE) DATA FLOW. \"You need to add a data flow.... What should you use?\" - If we have only answers from the list above, then it is For Each Loop (but it's describing the container). If in the exam you'll see the \"Excel Source\" then go for it, this is more related to the DataFlow thing.\n\nHere is the link with the description of this process - https://www.encorebusiness.com/blog/import-data-from-multiple-excel-files-sql-ssis/"
      },
      {
        "date": "2020-08-15T21:30:00.000Z",
        "voteCount": 1,
        "content": "and more over the \"Products table\" is  not in DB2 from description . From other similar questions the Product table is from DB1"
      },
      {
        "date": "2020-08-15T21:19:00.000Z",
        "voteCount": 6,
        "content": "Even I feel the answer is H, but  \"You need to add a data flow in an SSIS package to perform the Excel files import in the data warehouse.\"  As we can't use Foreach loop in data flow we can agree on \"Lookup transformation\""
      },
      {
        "date": "2020-08-04T04:09:00.000Z",
        "voteCount": 2,
        "content": "Correct answer: H"
      },
      {
        "date": "2020-05-07T23:56:00.000Z",
        "voteCount": 3,
        "content": "\"Each night, the Products table is deleted and refreshed\", you cannot lookup a empty table.  Foreach Loop container is correct."
      },
      {
        "date": "2020-05-17T19:46:00.000Z",
        "voteCount": 1,
        "content": "The product table not empty because refreshed from DB3 (MDS) after deleted and we have a single sheet should load so the answer A is correct."
      },
      {
        "date": "2020-06-15T09:32:00.000Z",
        "voteCount": 2,
        "content": "Each supplier saves the workbook with a unique name. that mean we will have multiple excel files. Foeach loop makes more sense."
      },
      {
        "date": "2020-07-19T12:54:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why the answer is lookup. We have multiple excel files. With a lookup you can find the reference for products in one of the files. \nwe have a single sheet? If that's correct what's the reason of \"All files must be loaded in sequence\"?"
      },
      {
        "date": "2020-02-07T04:12:00.000Z",
        "voteCount": 6,
        "content": "There is a Product table in the DW.  \n\nPrice is loaded into the DB by making reference to the produce table and looking up the same value in the excel files, so the answer is correct."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/9618-exam-70-767-topic-1-question-26-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has the databases described in the following table.<br><img src=\"/assets/media/exam-media/02783/0004400001.png\" class=\"in-exam-image\"><br><br>Each week, you import a product catalog from a partner company to a staging table in DB2.<br>You need to create a stored procedure that will update the staging table by inserting new products and deleting discontinued products.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "G",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-02T10:44:00.000Z",
        "voteCount": 20,
        "content": "D. merge statement ??"
      },
      {
        "date": "2020-02-03T00:53:00.000Z",
        "voteCount": 8,
        "content": "D. MERGE statement is correct"
      },
      {
        "date": "2020-07-12T10:55:00.000Z",
        "voteCount": 2,
        "content": "MERGE Statement is the correct answer."
      },
      {
        "date": "2020-01-21T00:18:00.000Z",
        "voteCount": 4,
        "content": "Indeed, Merge statement."
      },
      {
        "date": "2019-12-15T10:53:00.000Z",
        "voteCount": 5,
        "content": "I thought the same DanM...What the use of the sequential container on that?!"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19632-exam-70-767-topic-1-question-27-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has the databases described in the following table.<br><img src=\"/assets/media/exam-media/02783/0004500001.png\" class=\"in-exam-image\"><br><br>Each day, data from the table OnlineOrder in DB2 must be exported by partition. The tables must not be locked during the process.<br>You need to write a Microsoft SQL Server Integration Services (SSIS) package that performs the data export.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "E",
    "answerDescription": "The Union All transformation combines multiple inputs into one output. For example, the outputs from five different Flat File sources can be inputs to the Union All transformation and combined into one output.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/union-all-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-08T00:01:00.000Z",
        "voteCount": 9,
        "content": "You need to export a table to 24 destinations and not import 24 sources into one table. Balanced Data Distributor transformation is correct!"
      },
      {
        "date": "2020-05-16T05:07:00.000Z",
        "voteCount": 1,
        "content": "The only thing I am confused about is that BDD does the data distribution using no special rules to make sure that data gets to the correct partition, doesn't it? Thus, this requirement cannot be fulfilled, can it?"
      },
      {
        "date": "2020-06-15T09:41:00.000Z",
        "voteCount": 1,
        "content": "After a day data from 24 hourly partitions need to be moved to a table with monthly or yearly data, which is why union or merge appear more logical?"
      },
      {
        "date": "2020-07-19T13:35:00.000Z",
        "voteCount": 6,
        "content": "it's possible that \"data from the table OnlineOrder in DB2 must be exported by partition\" means that we have to export all data from the 24 partitions to one destination? in that case union all should be correct.\n\nsource partition1      source partition2 ..... source partition24\n            |____________________|_______________________|\n                                                |\n                                         union all\n                                               |\n                                       destination"
      },
      {
        "date": "2020-11-09T09:42:00.000Z",
        "voteCount": 2,
        "content": "No, it clearly says that there are 24 destinations. Union All is a bad answer."
      },
      {
        "date": "2020-12-19T03:53:00.000Z",
        "voteCount": 2,
        "content": "The table is set to Lock Escalation of AUTO. This means that SQL Server will escalate to a PARTITION lock rather than a TABLE lock. I do think that it can START as a TABLE lock under these conditions\n\nWe are to extract all the partitions, without locking the table. So we can't extract everything in query, we have to split it out by partition, there the ForEach Loop container seems correct.\n\nA UNION ALL would allow us to lock ALL the partitions at once, effectively locking the TABLE, and join the results into a single stream. So I don't think it's correct. \nA Balanced Data Distributor would take the data from a single stream and split it up. If that single stream is locking the table, it can't the be the answer"
      },
      {
        "date": "2020-11-25T14:29:00.000Z",
        "voteCount": 1,
        "content": "you have table with 24 partition, you need EXPORT ALL DATA from this 24 partition to ONE DESTINATION, --&gt;union all is correct"
      },
      {
        "date": "2020-12-08T05:27:00.000Z",
        "voteCount": 1,
        "content": "Data flow has 24 Destinations\nCorrect answer is F : Balanced Data Distributor transformation"
      },
      {
        "date": "2020-12-19T03:47:00.000Z",
        "voteCount": 2,
        "content": "And how would you avoid locking the entire table when you do that? By doing each partition one after the other, with the FOREACH container"
      },
      {
        "date": "2020-11-19T13:27:00.000Z",
        "voteCount": 1,
        "content": "Going with ForEach.\n\nIf there is an explicit transaction in place the select..union becomes part of that and the locks are held until the COMMIT or ROLLBACK. If you have an implicit transaction, by which I mean autocommit, not SET IMPLICIT_TRANSACTIONS, it spans all of the selects. Locks will be held for however long they would be held absent the UNION, governed by isolation level, query options (TABLOCKX etc.), lock escalation, trace flags .. and all the other things that affect this."
      },
      {
        "date": "2020-11-03T22:09:00.000Z",
        "voteCount": 3,
        "content": "How the hell Union all is correct ?\nI didn't even consider this answer"
      },
      {
        "date": "2020-09-29T14:21:00.000Z",
        "voteCount": 3,
        "content": "H. Foreach Loop container\nYou have to execute select 24 times, one time for each partition to not fire lock escalation process."
      },
      {
        "date": "2020-10-06T22:50:00.000Z",
        "voteCount": 1,
        "content": "good reason"
      },
      {
        "date": "2020-08-04T04:17:00.000Z",
        "voteCount": 2,
        "content": "Answer which makes most sense is E"
      },
      {
        "date": "2020-05-04T22:39:00.000Z",
        "voteCount": 1,
        "content": "Sorry. I dont get it. When we want to export _one_ source by partition, the result will be multiple targets, right? Thus, why would UNION ALL be useful for this step?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17686-exam-70-767-topic-1-question-28-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has the databases described in the following table.<br><img src=\"/assets/media/exam-media/02783/0004600001.png\" class=\"in-exam-image\"><br><br>Product prices are updated and are stored in a table named Products on DB1. The Products table is deleted and refreshed each night from MDS by using a<br>Microsoft SQL Server Integration Services (SSIS) package. None of the data sources are sorted.<br>You need to update the SSIS package to add current prices to the Products table.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "D",
    "answerDescription": "In the current release of SQL Server Integration Services, the SQL statement in an Execute SQL task can contain a MERGE statement. This MERGE statement enables you to accomplish multiple INSERT, UPDATE, and DELETE operations in a single statement.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/control-flow/merge-in-integration-services-packages",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-30T16:39:00.000Z",
        "voteCount": 9,
        "content": "It should be Lookup Transformation since it is requiring update of the SSIS packages."
      },
      {
        "date": "2020-06-15T10:02:00.000Z",
        "voteCount": 2,
        "content": "Merge is correct as products can be discontinued/deleted"
      },
      {
        "date": "2020-11-07T08:06:00.000Z",
        "voteCount": 3,
        "content": "Look up is correct. The question specifically says NONE OF THE DATA SOURCES ARE SORTED. This negates the possibility of MERGE. \n\na merge join requires both inputs to be sorted the same way\nlookup does not require either input to be sorted.\n\nhttps://stackoverflow.com/questions/6735733/what-are-the-differences-between-merge-join-and-lookup-transformations-in-ssis#:~:text=A%20Merge%20Join%20is%20designed,work%20like%20a%20SQL%20JOIN.&amp;text=With%20a%20Merge%20Join%20you,rows%20exist%20in%20input%202."
      },
      {
        "date": "2020-11-25T14:41:00.000Z",
        "voteCount": 1,
        "content": "merge join requires both inputs to be sorted. THE ANSWER IS NOT merge join is SQL TASK MERGE STATEMENT"
      },
      {
        "date": "2020-08-04T04:20:00.000Z",
        "voteCount": 4,
        "content": "It has to be a lookup as the SSIS package which takes the latest mds records need to have the correct prices."
      },
      {
        "date": "2020-07-05T11:47:00.000Z",
        "voteCount": 4,
        "content": "Lookup!\nProducts are updated every night from MDS. Commonly used view. But prices need to be taken from another place through Lookup transformation."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33874-exam-70-767-topic-1-question-29-discussion/",
    "body": "HOTSPOT -<br>You have the Microsoft SQL Server Integration Services (SSIS) package shown in the Control flow exhibit. (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0004700001.jpg\" class=\"in-exam-image\"><br>The package iterates over 100 files in a local folder. For each iteration, the package increments a variable named loop as shown in the Expression task exhibit.<br>(Click the Exhibit button) and then imports a file. The initial value of the variable loop is 0.<br><img src=\"/assets/media/exam-media/02783/0004800001.jpg\" class=\"in-exam-image\"><br>You suspect that there may be an issue with the variable value during the loop. You define a breakpoint on the Expression task as shown in the BreakPoint exhibit. (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0004900001.jpg\" class=\"in-exam-image\"><br>You need to check the value of the loop variable value.<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0005000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0005000002.png\" class=\"in-exam-image\">",
    "answerDescription": "Break condition: When the task or container receives the OnPreExecute event.<br>Called when a task is about to execute. This event is raised by a task or a container immediately before it runs.<br>The loop variable does not reset.<br>With the debugger, you can break, or suspend, execution of your program to examine your code, evaluate and edit variables in your program, etc.",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-17T09:29:00.000Z",
        "voteCount": 2,
        "content": "I believe that the first answer is correct, i will go with \"yes\".\nHit count is set to 3 which means the execution would be suspended every third time (1st loop : 0, Second loop : 10, Third loop /breakpoint : 20)\nReference : https://docs.microsoft.com/en-us/sql/integration-services/troubleshooting/debugging-control-flow?view=sql-server-ver15"
      },
      {
        "date": "2020-11-25T15:28:00.000Z",
        "voteCount": 3,
        "content": "before first =0\nbefore second=10\nbefore third =20"
      },
      {
        "date": "2020-10-06T23:01:00.000Z",
        "voteCount": 3,
        "content": "the first is no, the variable value before each iteration will be:\n- 0\n- 10\n- 20\nso the breakpoint triggers before the second iteration and the variable value is 10"
      },
      {
        "date": "2020-10-12T01:54:00.000Z",
        "voteCount": 3,
        "content": "so the answer is:\n-no\n-no\n-yes"
      },
      {
        "date": "2020-11-25T15:28:00.000Z",
        "voteCount": 3,
        "content": "Its not before the second iteration ITS BEFORE THIRD , the answer is correct its YES, NO, YES. before third iteration the value is 20."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37245-exam-70-767-topic-1-question-30-discussion/",
    "body": "You have a Microsoft SQL Server Integration Services (SSIS) package that includes the control flow shown in the following diagram.<br><img src=\"/assets/media/exam-media/02783/0005100001.jpg\" class=\"in-exam-image\"><br><br>You need to choose the enumerator for the Foreach Loop container.<br>Which enumerator should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach SMO Enumerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach ADO.Net Schema Rowset Enumerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach NodeList Enumerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach ADO Enumerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach HDS File Enumerator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach File Enumerator"
    ],
    "answer": "D",
    "answerDescription": "Use the Foreach ADO enumerator to enumerate rows in tables. For example, you can get the rows in an ADO recordset.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/control-flow/foreach-loop-container?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-18T13:20:00.000Z",
        "voteCount": 1,
        "content": "Can someone explain how ADO enumerator relates to web service task?"
      },
      {
        "date": "2020-12-19T07:14:00.000Z",
        "voteCount": 2,
        "content": "Actually the important bit is the \"Get Table\" task before it. \nIt's being called to generate the list (of web services) to be looped over. Having populated that list, as a \"recordset\", you will then use the ADO enumerator to work through it\n\nNone of the others make sense. The SMO and ADO.Net Schema are great for getting a list of database objects you want to enumerate over. If you then want to perform the same admin task on them. No way this would map to a web service\n\nThe File and HDFS File are great if you want to enumerate over a list of files, and say import that data. No way this would map to a web service"
      },
      {
        "date": "2020-11-25T15:34:00.000Z",
        "voteCount": 4,
        "content": "its not important, the important thing is that there sey ROW by ROW in the name of containter, ado enumeratos is using for row by row processing"
      },
      {
        "date": "2020-12-06T06:48:00.000Z",
        "voteCount": 2,
        "content": "Thank you for your explanation. Now I see the point."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/41024-exam-70-767-topic-1-question-31-discussion/",
    "body": "DRAG DROP -<br>You have a Microsoft SQL Server Integration Services (SSIS) package that loads data into a data warehouse each night from a transactional system. The package also loads data from a set of Comma-Separated Values (CSV) files that are provided by your company's finance department.<br>The SSIS package processes each CSV file in a folder. The package reads the file name for the current file into a variable and uses that value to write a log entry to a database table.<br>You need to debug the package and determine the value of the variable before each file is processed.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0005300001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0005400001.png\" class=\"in-exam-image\">",
    "answerDescription": "You debug control flows.<br>The Foreach Loop container is used for looping through a group of files. Put the breakpoint on it.<br>The Locals window displays information about the local expressions in the current scope of the Transact-SQL debugger.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/troubleshooting/debugging-control-flow http://blog.pragmaticworks.com/looping-through-a-result-set-with-the-foreach-loop",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-29T23:01:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33876-exam-70-767-topic-1-question-32-discussion/",
    "body": "HOTSPOT -<br>You create a Microsoft SQL Server Integration Services (SSIS) package as shown in the SSIS Package exhibit. (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0005600001.jpg\" class=\"in-exam-image\"><br>The package uses data from the Products table and the Prices table. Properties of the Prices source are shown in the OLE DB Source Editor exhibit (Click the<br>Exhibit Button.) and the Advanced Editor for Prices exhibit (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0005700001.jpg\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0005800001.png\" class=\"in-exam-image\"><br>You join the Products and Prices tables by using the ReferenceNr column.<br>You need to resolve the error with the package.<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0005900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0006000001.png\" class=\"in-exam-image\">",
    "answerDescription": "There are two important sort properties that must be set for the source or upstream transformation that supplies data to the Merge and Merge Join transformations:<br>The Merge Join Transformation requires sorted data for its inputs.<br>\u2711 The IsSorted property of the output that indicates whether the data has been sorted. This property must be set to True.<br>\u2711 The SortKeyPosition property of output columns that indicates whether a column is sorted, the column's sort order, and the sequence in which multiple columns are sorted. This property must be set for each column of sorted data.<br>If you do not use a Sort transformation to sort the data, you must set these sort properties manually on the source or the upstream transformation.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/sort-data-for-the-merge-and-merge-join-transformations",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-07T09:23:00.000Z",
        "voteCount": 6,
        "content": "YES  - In the source, an ORDER BY clause is used in the statement that is used to load the data.\nYES - The Source is sorted by RefernceNr therefore need to set the IsSorted and  SortKeyPosition\nNO - Obvious\nNO - A Lookup can only accept 1 input\nhttps://stackoverflow.com/questions/6735733/what-are-the-differences-between-merge-join-and-lookup-transformations-in-ssis\nThe IsSorted property of the output that indicates whether the data has been sorted. This property must be set to True.\nThe SortKeyPosition property of output columns that indicates whether a column is sorted, the column's sort order, and the sequence in which multiple columns are sorted. This property must be set for each column of sorted data.\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/sort-data-for-the-merge-and-merge-join-transformations?view=sql-server-ver15"
      },
      {
        "date": "2020-10-06T23:22:00.000Z",
        "voteCount": 2,
        "content": "the last is Yes"
      },
      {
        "date": "2020-10-06T23:34:00.000Z",
        "voteCount": 1,
        "content": "the second is No, SortKeyPosition and IsSorted just indicate if the column is sorted and the sort order, they do not provide the sorting"
      },
      {
        "date": "2020-12-19T07:26:00.000Z",
        "voteCount": 1,
        "content": "Yes and No to your comment\nYes because we don't know anything about the Product stream\nNo because we know the Prices stream is sorted, we just need to tell SSIS"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40342-exam-70-767-topic-1-question-33-discussion/",
    "body": "HOTSPOT -<br>You are testing a Microsoft SQL Server Integration Services (SSIS) package. The package includes the Control Flow task shown in the Control Flow exhibit (Click the Exhibit button) and the Data Flow task shown in the Data Flow exhibit. (Click the Exhibit button.)<br><img src=\"/assets/media/exam-media/02783/0006100001.jpg\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0006200001.jpg\" class=\"in-exam-image\"><br>You declare a variable named Seed as shown in the Variables exhibit. (Click the Exhibit button.) The variable is changed by the Script task during execution.<br><img src=\"/assets/media/exam-media/02783/0006300001.jpg\" class=\"in-exam-image\"><br>You need to be able to interrogate the value of the Seed variable after the Script task completes execution.<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0006400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0006500001.png\" class=\"in-exam-image\">",
    "answerDescription": "\u2711 The Locals window displays variable that are local to the current statement, as well as three statements behind and in front of the current statement.<br>\u2711 To use the Watch window you need to specify which variable you should watch.<br>\u2711 MessageBox.Show can be used to display variables.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/variables-window",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T07:35:00.000Z",
        "voteCount": 2,
        "content": "To use the OnVariableValueChange event you would also need to set the RaiseChangeEvent on the variable"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20024-exam-70-767-topic-1-question-34-discussion/",
    "body": "HOTSPOT -<br>You have a database named DB1. You create a Microsoft SQL Server Integration Services (SSIS) package that incrementally imports data from a table named<br>Customers. The package uses an OLE DB data source for connections to DB1. The package defines the following variables.<br><img src=\"/assets/media/exam-media/02783/0006600001.png\" class=\"in-exam-image\"><br><br>To support incremental data loading, you create a table by running the following Transact-SQL segment:<br><img src=\"/assets/media/exam-media/02783/0006600002.png\" class=\"in-exam-image\"><br><br>You need to create a DML statements that updates the LastKeyByTable table.<br>How should you complete the Transact-SQL statement? To answer, select the appropriate Transact-SQL segments in the dialog box in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0006700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0006800001.png\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-25T16:20:00.000Z",
        "voteCount": 3,
        "content": "its \"?\" TESTED"
      },
      {
        "date": "2020-10-14T20:09:00.000Z",
        "voteCount": 1,
        "content": "DML statements use \"?\" or \"@\" i don't think so... the answer is correct."
      },
      {
        "date": "2020-10-14T20:11:00.000Z",
        "voteCount": 1,
        "content": "dml statenment use \"@\"       *"
      },
      {
        "date": "2020-08-10T10:30:00.000Z",
        "voteCount": 2,
        "content": "If you're using the OLE DB source you have to specify ? else you get an error.\nIf you're using the ADO.NET source you can use the \"@LastKey\" and map this.\n(i tried this on a SSIS package)\n\nI don't really get from the question what they're using to perform the update, it specifies only what source is used to fetch the data."
      },
      {
        "date": "2020-08-10T10:35:00.000Z",
        "voteCount": 2,
        "content": "Personally i would go for the \"?\", because the question refers only to OLE DB and since that is the default source connection type in a SSIS Sql Task. They did not specify otherwise."
      },
      {
        "date": "2020-06-12T08:48:00.000Z",
        "voteCount": 2,
        "content": "Because of DML statements...the answer is correct."
      },
      {
        "date": "2020-07-26T09:34:00.000Z",
        "voteCount": 2,
        "content": "why? can you be more specific? \nTo use variables within a query in a ssis package you use \"?\" and then you map the parameters to the variables.\nIf inside data flow task, you need to use OLEDB Command, and if outside you need to use execute sql task, but with \"?\""
      },
      {
        "date": "2020-08-10T10:32:00.000Z",
        "voteCount": 2,
        "content": "All update statements are DML statements, but if it's used within ssis it depends on the source that is used to connect to the db (OLEDB or ADO.NET)."
      },
      {
        "date": "2020-05-10T01:35:00.000Z",
        "voteCount": 4,
        "content": "I think for DML (outside data warehouse Control Flow and Data Tasks) the variables are specified by \"@\", while OLE DB source is \"?\""
      },
      {
        "date": "2020-07-26T09:35:00.000Z",
        "voteCount": 1,
        "content": "I agree. Within a package you should use \"?\" and the map parameters-variables"
      },
      {
        "date": "2020-05-08T00:28:00.000Z",
        "voteCount": 3,
        "content": "Do the variables in OLEDB sources not have to be specified with \"?\" ?\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/ole-db-source?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/34531-exam-70-767-topic-1-question-35-discussion/",
    "body": "DRAG DROP -<br>You deploy a Microsoft Server database that contains a staging table named EmailAddress_Import. Each night, a bulk process will import customer information from an external database, cleanse the data, and then insert it into the EmailAddress table. Both tables contain a column named EmailAddressValue that stores the email address.<br>You need to implement the logic to meet the following requirements:<br>\u2711 Email addresses that are present in the EmailAddress_Import table but not in the EmailAddress table must be inserted into the EmailAddress table.<br>\u2711 Email addresses that are not in the EmailAddress_Import but are present in the EmailAddress table must be deleted from the EmailAddress table.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate Transact-SQL segments to the correct locations. Each Transact-SQL segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0006900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0006900002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: EmailAddress -<br>The EmailAddress table is the target.<br><br>Box 2: EmailAddress_import -<br>The EmailAddress_import table is the source.<br><br>Box 3: NOT MATCHED BY TARGET -<br><br>Box 4: NOT MATCHED BY SOURCE -<br>References: https://docs.microsoft.com/en-us/sql/t-sql/statements/merge-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-14T21:01:00.000Z",
        "voteCount": 1,
        "content": "\"Box 4: NOT MATCHED BY SOURCE\"\ndelete data from target that not match... I think its not correct..."
      },
      {
        "date": "2020-10-14T21:03:00.000Z",
        "voteCount": 3,
        "content": "Its correct, sorry"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13718-exam-70-767-topic-1-question-38-discussion/",
    "body": "You have a data quality project that focuses on the Products catalog for the company. The data includes a product reference number.<br>The product reference should use the following format: Two letters followed by an asterisk and then four or five numbers. An example of a valid number is<br>XX*55522. Any reference number that does not conform to the format must be rejected during the data cleansing.<br>You need to add a Data Quality Services (DQS) domain rule in the Products domain.<br>Which rule should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tvalue matches pattern ZA*9876[5]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tvalue matches pattern AZ[*]1234[5]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tvalue matches regular expression AZ[*]1234[5]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tvalue matches pattern [a-zA-Z][a-zA-Z]*[0-9][0-9] [0-9][0-9] [0-9]?"
    ],
    "answer": "A",
    "answerDescription": "For a pattern matching rule:<br>Any letter (A\"\u00a6Z) can be used as a pattern for any letter; case insensitive<br>Any digit (0\"\u00a69) can be used as a pattern for any digit<br>Any special character, except a letter or a digit, can be used as a pattern for itself<br>Brackets, [], define optional matching<br><br>Example: ABC:0000 -<br>This rule implies that the data will contain three parts: any three letters followed by a colon (:), which is again followed by any four digits.",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-16T22:34:00.000Z",
        "voteCount": 6,
        "content": "Answer A is correct. Please refer to : https://docs.microsoft.com/en-us/sql/data-quality-services/create-a-domain-rule?view=sql-server-ver15. There you find examples for the matching patterns."
      },
      {
        "date": "2020-02-10T01:23:00.000Z",
        "voteCount": 5,
        "content": "I think It should be D"
      },
      {
        "date": "2020-04-02T08:54:00.000Z",
        "voteCount": 7,
        "content": "Using the regular expression, D is correct. But A is the correct answer using the matching pattern."
      },
      {
        "date": "2021-01-02T00:37:00.000Z",
        "voteCount": 1,
        "content": "A is the one"
      },
      {
        "date": "2020-11-08T01:26:00.000Z",
        "voteCount": 2,
        "content": "Answer is A:\nFor a pattern matching rule:\n\n\n\nAny  letter (A\u2026Z)  can be used as a pattern for any letter; case insensitive\n\nAny digit (0\u20269) can be used as a pattern for any digit\n\nAny special character, except a letter or a digit, can be used as a pattern for itself\n\nBrackets, [], define optional matching\n\nhttps://techcommunity.microsoft.com/t5/sql-server-integration-services/pattern-matching-in-dqs-domain-rules/ba-p/388103"
      },
      {
        "date": "2020-10-07T02:33:00.000Z",
        "voteCount": 1,
        "content": "the only difference between A and D is that A rejects letters \"a-z\", only capitals, so its hard to choose as its not clearly specified by the question, I think I would choose rather D"
      },
      {
        "date": "2020-10-07T02:36:00.000Z",
        "voteCount": 1,
        "content": "sorry, A accepts lowercases letters too, so A and D are equals"
      },
      {
        "date": "2020-10-07T02:38:00.000Z",
        "voteCount": 1,
        "content": "finaly, I think its A as in D answer its said that its  a pattern, but its not a pattern, but reg ex"
      },
      {
        "date": "2020-08-25T05:45:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is D"
      },
      {
        "date": "2020-08-31T07:24:00.000Z",
        "voteCount": 1,
        "content": "buuut the problem is with * it mus be [*]"
      },
      {
        "date": "2020-08-31T07:40:00.000Z",
        "voteCount": 2,
        "content": "I regret, after viewing it more and more now I understand that D is not pattern but as regular expression, so I will go with A as correct answer."
      },
      {
        "date": "2020-07-30T02:51:00.000Z",
        "voteCount": 1,
        "content": "I will always stick with D"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13900-exam-70-767-topic-1-question-39-discussion/",
    "body": "HOTSPOT -<br>You have a series of analytic data models and reports that provide insights into the participation rates for sports at different schools. Users enter information about sports and participants into a client application. The application stores this transactional data in a Microsoft SQL Server database. A SQL Server Integration<br>Services (SSIS) package loads the data into the models.<br>When users enter data, they do not consistently apply the correct names for the sports. The following table shows examples of the data entry issues.<br><img src=\"/assets/media/exam-media/02783/0007900001.png\" class=\"in-exam-image\"><br><br>You need to create a new knowledge base to improve the quality of the sport name data.<br>How should you configure the knowledge base? To answer, select the appropriate options in the dialog box in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0008000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0008100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Spot 1: Create Knowledge base from: None<br>Select None if you do not want to base the new knowledge base on an existing knowledge base or data file.",
    "votes": [],
    "comments": [
      {
        "date": "2020-02-12T04:33:00.000Z",
        "voteCount": 17,
        "content": "The second selection is Domain Management"
      },
      {
        "date": "2020-12-26T21:26:00.000Z",
        "voteCount": 1,
        "content": "Select Domain Management to create the knowledge base and enter the screens that you use to modify the domains in the knowledge base.\nSelect Knowledge Discovery to create the knowledge base and enter the wizard that you use to analyze a data sample and populate the domains of the knowledge base with the results.\nSelect Matching Policy to create a matching policy and add it to the knowledge base.\n\nhttps://docs.microsoft.com/en-us/sql/data-quality-services/create-a-knowledge-base?view=sql-server-ver15"
      },
      {
        "date": "2020-11-18T16:17:00.000Z",
        "voteCount": 1,
        "content": "After reviewing my cbt nuggets lab, I'm going with NONE and Knowledge Discovery. Domain Management takes place AFTER creating your Knowledge Base"
      },
      {
        "date": "2020-11-21T11:15:00.000Z",
        "voteCount": 1,
        "content": "Not exactly. You need to add a mapping between the source fields and domains during creating new \"Knowledge Discovery\". So, you have to create all needed domains at first in \"Domain Management\" and add rules or just add domains without rules in \"Knowledge Discovery\". Anyway, I choose Domain Management as the first step to create Knowledge Base."
      },
      {
        "date": "2020-11-08T01:33:00.000Z",
        "voteCount": 1,
        "content": "None - The scenarios does not specify an existing Knowledge Base or DQS File.\nDomain Management - A Knowledge base consists of domains. Each domain represents the data in a data field. Each value in a data field or domain is known as a domain value. DQS provides the ability to validate, cleanse, match and deduplicate values from any dataset against domain values in the DQS Knowledge base. P 209 in \"Implementing a SQL Data Warehouse."
      },
      {
        "date": "2020-10-15T08:05:00.000Z",
        "voteCount": 1,
        "content": "Some simple explanation of the last 3?"
      },
      {
        "date": "2020-10-12T02:41:00.000Z",
        "voteCount": 1,
        "content": "the second is Knowledge Discovery\n(see - https://docs.microsoft.com/en-us/sql/data-quality-services/dqs-knowledge-bases-and-domains?view=sql-server-ver15)"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13905-exam-70-767-topic-1-question-40-discussion/",
    "body": "DRAG DROP -<br>You have a series of analytic data models and reports that provide insights into the participation rates for sports at different schools. Users enter information about sports and participants into a client application. The application stores this transactional data in a Microsoft SQL Server database. A SQL Server Integration<br>Services (SSIS) package loads the data into the models.<br>When users enter data, they do not consistently apply the correct names for the sports. The following table shows examples of the data entry issues.<br><img src=\"/assets/media/exam-media/02783/0008200001.png\" class=\"in-exam-image\"><br><br>You need to improve the quality of the data.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0008300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0008400001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References: https://docs.microsoft.com/en-us/sql/data-quality-services/perform-knowledge-discovery",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-23T09:07:00.000Z",
        "voteCount": 9,
        "content": "The first step shouldn't be create a DQS knowledge base? And after that, import raw data to perform discovery, map alternate values, and publish KB and modify the ETL..."
      },
      {
        "date": "2020-02-12T04:58:00.000Z",
        "voteCount": 7,
        "content": "I think the correct actions will be like this:\n\n1- Create a Data Quality Service (DQS) knowledge base.\n2- Map alternate values for entries ...\n3- Publish the knowledge base and modify the ETL package to call ...\n4- Add a lookup transformation to ETL package ..."
      },
      {
        "date": "2020-07-26T10:07:00.000Z",
        "voteCount": 1,
        "content": "why step 4 is a lookup?? The DQS Cleansing component returns corrected data"
      },
      {
        "date": "2020-09-26T04:43:00.000Z",
        "voteCount": 1,
        "content": "Hmmm... this is fom Microsoft:\n- First step: Start Knowledge Discovery\n- Mapping Stage (In the Mappings table, map each source column that you want knowledge discovery to be performed on to a domain in the knowledge base)\n- Discover Stage (Discovery is performed on the columns that were entered in the Mappings table on the Map page. )\n- Manage Data Discovery Results Stage\n\nhttps://docs.microsoft.com/en-us/sql/data-quality-services/perform-knowledge-discovery?view=sql-server-ver15"
      },
      {
        "date": "2020-09-26T04:52:00.000Z",
        "voteCount": 4,
        "content": "So, we don't have a mapping stage here, the option we have is about mapping (correcting) the result in the last step.  So, looks like this is the order:\n- Create DQS\n(- we don't have mapping step in the list)\n- Import raw data for the discovery \n- Map alternate values for incorrect entries (DQS Result Management step)\n- Publish - DQS Client transformation step\n\nAny thoughts?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33897-exam-70-767-topic-1-question-42-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to deploy a Microsoft SQL server that will host a data warehouse named DB1.<br>The server will contain four SATA drives configured as a RAID 10 array.<br>You need to minimize write contention on the transaction log when data is being loaded to the database.<br>Solution: You replace the SATA disks with SSD disks.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "A data warehouse is too big to store on an SSD.<br>Instead you should place the log file on a separate drive.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/policy-based-management/place-data-and-log-files-on-separate-drives?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "We've had RAID arrays of SSDs for a while now."
      },
      {
        "date": "2021-01-29T10:01:00.000Z",
        "voteCount": 1,
        "content": "Yes, but this test is from several years ago. So, we need to take that into consideration, too."
      },
      {
        "date": "2020-10-07T03:01:00.000Z",
        "voteCount": 3,
        "content": "I think the solution from the answer is applicable"
      },
      {
        "date": "2020-11-21T11:48:00.000Z",
        "voteCount": 2,
        "content": "I agree with you. The content doesn't say how huge is a data warehouse. I met in my career small data warehouses that were stored in SSD disks. Replacing disks to SSD might not be the perfect solution, but definitely minimize write contention on the transaction log comparing with SATA disks."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29618-exam-70-767-topic-1-question-45-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft Azure SQL Data Warehouse instance. You run the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0008800001.png\" class=\"in-exam-image\"><br>The query fails to return results.<br>You need to determine why the query fails.<br>Solution: You run the following Transact-SQL statements:<br><img src=\"/assets/media/exam-media/02783/0008900001.png\" class=\"in-exam-image\"><br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "To use submit_time we must use sys.dm_pdw_exec_requests table, which holds information about all requests currently or recently active in SQL Data<br>Warehouse. It lists one row per request/query.<br>Label is an Optional label string associated with some SELECT query statements.<br>References: https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql?view=aps- pdw-2016-au7",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-26T05:27:00.000Z",
        "voteCount": 1,
        "content": "I don't fail to get data. I executed in SQL Server Management Studio and got data. Also from the solution I cannot even execute it:\nInvalid object name 'sys.dm_pdw_exec_requests'."
      },
      {
        "date": "2020-09-26T04:57:00.000Z",
        "voteCount": 2,
        "content": "This view is from DW..."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23433-exam-70-767-topic-1-question-48-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>Your company uses Microsoft SQL Server to deploy a data warehouse to an environment that has a SQL Server Analysis Services (SSAS) instance. The data warehouse includes the Fact.Order table as shown in the following table definition. The table has no indexes.<br><img src=\"/assets/media/exam-media/02783/0009200001.jpg\" class=\"in-exam-image\"><br>You must minimize the amount of space that indexes for the Fact.Order table consume. You run the following queries frequently. Both queries must be able to use a columnstore index:<br><img src=\"/assets/media/exam-media/02783/0009300001.png\" class=\"in-exam-image\"><br>You need to ensure that the queries complete as quickly as possible.<br>Solution: You create a measure for the Fact.Order table.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "You should use a columnstore index.<br>Columnstore indexes are the standard for storing and querying large data warehousing fact tables. This index uses column-based data storage and query processing to achieve gains up to 10 times the query performance in your data warehouse over traditional row-oriented storage.<br>References: https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-12T23:30:00.000Z",
        "voteCount": 1,
        "content": "48, 49 and 50 are part of one subset and 50 is the right one"
      },
      {
        "date": "2020-08-04T07:40:00.000Z",
        "voteCount": 1,
        "content": "This is a tricky question"
      },
      {
        "date": "2020-06-18T03:52:00.000Z",
        "voteCount": 1,
        "content": "Why B? the question didn't mention that the table does not have column store index. And we add measures to improve the sum operation performance. So I think the answer is A"
      },
      {
        "date": "2020-07-29T05:52:00.000Z",
        "voteCount": 7,
        "content": "The question mentions this though \"Both queries must be able to use a columnstore index\" and it also says \"the table has no indexes\" so in my understanding that's why creating a measure isn't a sufficient solution and a columnstore index has to be created."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20456-exam-70-767-topic-1-question-51-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft SQL server that has Data Quality Services (DQS) installed.<br>You need to review the completeness and the uniqueness of the data stored in the matching policy.<br>Solution: You profile the data.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Use a matching rule.<br>References: https://docs.microsoft.com/en-us/sql/data-quality-services/create-a-matching-policy?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-12T22:32:00.000Z",
        "voteCount": 7,
        "content": "Following the link in the solution in my opinion the answer is yes. \n\"Profiling gives insights on completeness and uniqueness. ...For more information, see Profiler and Results Tabs\". =&gt; thus, the answer is that profiling your data will give you insights into uniqueness and completeness. Right?"
      },
      {
        "date": "2020-05-12T22:33:00.000Z",
        "voteCount": 3,
        "content": "In addition: in the DQS client tool the profiler shows exactly the required information: uniqueness and completeness...."
      },
      {
        "date": "2020-06-15T11:20:00.000Z",
        "voteCount": 1,
        "content": "The Profiler and Results tab contain statistics for both the Matching Policy and the Matching Results pages."
      },
      {
        "date": "2020-09-09T09:42:00.000Z",
        "voteCount": 1,
        "content": "Profiling gives insights on completeness and uniqueness. Consider completeness and uniqueness in tandem. Use completeness and uniqueness data to determine what weight to give a field in the matching process"
      },
      {
        "date": "2020-08-04T04:36:00.000Z",
        "voteCount": 5,
        "content": "Correct answer: A"
      },
      {
        "date": "2020-08-04T04:38:00.000Z",
        "voteCount": 3,
        "content": "The next question even suggests that using a matching rule is wrong"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20458-exam-70-767-topic-1-question-52-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft SQL server that has Data Quality Services (DQS) installed.<br>You need to review the completeness and the uniqueness of the data stored in the matching policy.<br>Solution: You create a matching rule.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Use a matching rule, and use completeness and uniqueness data to determine what weight to give a field in the matching process.<br>If there is a high level of uniqueness in a field, using the field in a matching policy can decrease the matching results, so you may want to set the weight for that field to a relatively small value. If you have a low level of uniqueness for a column, but low completeness, you may not want to include a domain for that column.<br>References: https://docs.microsoft.com/en-us/sql/data-quality-services/create-a-matching-policy?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-19T14:28:00.000Z",
        "voteCount": 5,
        "content": "The previous questions suggests to use a matching rule as a correct solution.  Its' confusing !"
      },
      {
        "date": "2020-08-26T06:16:00.000Z",
        "voteCount": 1,
        "content": "yes, the previous suggested to use a matching rule and not just to create a matching rule ;)"
      },
      {
        "date": "2020-11-18T16:36:00.000Z",
        "voteCount": 3,
        "content": "Correction: This Question is B, the PRVEIOUS question is A."
      },
      {
        "date": "2020-11-18T16:35:00.000Z",
        "voteCount": 2,
        "content": "A. \nNext  question is B. The Matching Policy is already created, you need to Profile the data for uniqueness/completeness"
      },
      {
        "date": "2020-08-04T02:44:00.000Z",
        "voteCount": 2,
        "content": "I think creating the rule before is essential. But using it would be the answer"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20539-exam-70-767-topic-1-question-53-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Microsoft SQL server that has Data Quality Services (DQS) installed.<br>You need to review the completeness and the uniqueness of the data stored in the matching policy.<br>Solution: You modify the weight of the domain in the matching rule.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Use a matching rule, and use completeness and uniqueness data to determine what weight to give a field in the matching process.<br>If there is a high level of uniqueness in a field, using the field in a matching policy can decrease the matching results, so you may want to set the weight for that field to a relatively small value. If you have a low level of uniqueness for a column, but low completeness, you may not want to include a domain for that column.<br>References: https://docs.microsoft.com/en-us/sql/data-quality-services/create-a-matching-policy?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-13T22:25:00.000Z",
        "voteCount": 13,
        "content": "Answer ist yes? I dont think so .Why should the adaption of the weight help to review the completeness and uniqueness. In my opinion: this is no since we need to profile the data by means of the profiler which can be started after creating a matching rule"
      },
      {
        "date": "2020-11-18T16:37:00.000Z",
        "voteCount": 4,
        "content": "Agreed"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26233-exam-70-767-topic-1-question-54-discussion/",
    "body": "You have a server that has Data Quality Services (DQS) installed.<br>You create a matching policy that contains one matching rule.<br>You need to configure the Similarity of Similar percentage that defines a match.<br>Which similarity percentage will always generate a similarity score of 0?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t55",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t80",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t70",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t75"
    ],
    "answer": "A",
    "answerDescription": "The minimum similarity between the values of a field is 60%. If the calculated matching score for a field of two records is less than 60, the similarity score is automatically set to 0.<br>References: https://docs.microsoft.com/en-us/sql/data-quality-services/create-a-matching-policy?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-20T07:25:00.000Z",
        "voteCount": 1,
        "content": "Field of two records? do they mean a record of two fields.  As far as I know a field is a column."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20753-exam-70-767-topic-1-question-57-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You have a database named DB1 that has change data capture enabled.<br>A Microsoft SQL Server Integration Services (SSIS) job runs once weekly. The job loads changes from DB1 to a data warehouse by querying the change data capture tables.<br>Users report that an application that uses DB1 is suddenly unresponsive.<br>You discover that the Integration Services job causes severe blocking issues in the application.<br>You need to ensure that the users can run the application as quickly as possible.<br>Your SQL Server login is a member of only the ssis_admin database role.<br>Which stored procedure should you execute?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.deploy_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.restore_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.stop_operation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_add_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_change_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_disable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_enable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_stop_job"
    ],
    "answer": "E",
    "answerDescription": "sys.sp_cdc_change_job modifies the configuration of a change data capture cleanup or capture job in the current database.<br>References: https://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-change-job-transact-sql?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-08T02:29:00.000Z",
        "voteCount": 2,
        "content": "I'm going with C. \n1) Membership to the ssis_admin database role\n2) Stops a validation or instance of execution in the Integration Services catalog.\n\nhttps://docs.microsoft.com/en-us/sql/integration-services/system-stored-procedures/catalog-stop-operation-ssisdb-database?view=sql-server-ver15\n\nWrong answers:\n\n\n\nChanges to a job do not take effect until the job is stopped by using sp_cdc_stop_job and restarted by using sp_cdc_start_job.\n\nPermissions\nRequires membership in the db_owner fixed database role.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-change-job-transact-sql?view=sql-server-2017"
      },
      {
        "date": "2020-09-09T10:11:00.000Z",
        "voteCount": 1,
        "content": "catalog.stop_operation - Stops a validation or instance of execution in the Integration Services catalog,\n\nThis stored procedure requires one of the following permissions:\n\nREAD and MODIFY permissions on the validation or instance of execution\n\nMembership to the ssis_admin database role\n\nMembership to the sysadmin server role"
      },
      {
        "date": "2020-08-26T13:28:00.000Z",
        "voteCount": 1,
        "content": "C. catalog.stop_operation Stops a validation or instance of execution in the Integration Services catalog\nE. sys.sp_cdc_change_job Modifies the configuration of a change data capture cleanup or capture job in the current database.\nH. sys.sp_cdc_stop_job Stops a change data capture cleanup or capture job for the current database.\n\nI ma going with C, looks the most appropriate answer here"
      },
      {
        "date": "2020-08-21T02:12:00.000Z",
        "voteCount": 2,
        "content": "The correct answer must be C, it stops the execution of SSIS job using that proc. What H option does is - Stops a change data capture cleanup or capture job for the current database."
      },
      {
        "date": "2020-08-04T04:44:00.000Z",
        "voteCount": 3,
        "content": "Correct answer C"
      },
      {
        "date": "2020-05-27T08:22:00.000Z",
        "voteCount": 3,
        "content": "\"Your SQL Server login is a member of only the ssis_admin database role.\"\nHow is it possible then to execute the sp_cdc_change_job when it needs the db_owner role? Is there something that I am missing?\n\nsee https://docs.microsoft.com/fr-fr/sql/relational-databases/system-stored-procedures/sys-sp-cdc-change-job-transact-sql?view=sql-server-ver15#permissions"
      },
      {
        "date": "2020-05-17T07:30:00.000Z",
        "voteCount": 1,
        "content": "In my opinion the answer should be  H (sys.sp_cdc_stop_job) to fulfill requirements that the users can access the application as quickly as possible. The task does not say how to change the cdc job. Any thoughts to that?"
      },
      {
        "date": "2020-05-17T07:34:00.000Z",
        "voteCount": 9,
        "content": "Retought my answer: since \"Your SQL Server login is a member of only the ssis_admin database role.\" it has to be catalog.stop_operation. \nhttps://docs.microsoft.com/en-us/sql/integration-services/system-stored-procedures/catalog-stop-operation-ssisdb-database?view=sql-server-2017"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20752-exam-70-767-topic-1-question-58-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You have a database named DB1 that has change data capture enabled.<br>A Microsoft SQL Server Integration Services (SSIS) job runs once weekly. The job loads changes from DB1 to a data warehouse by querying the change data capture tables.<br>You remove the Integration Services job.<br>You need to stop tracking changes to the database temporarily. The solution must ensure that tracking changes can be restored quickly in a few weeks.<br>Which stored procedure should you execute?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.deploy_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.restore_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.stop_operation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_add_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_change_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_disable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_enable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_stop_job"
    ],
    "answer": "C",
    "answerDescription": "catalog.stop_operation stops a validation or instance of execution in the Integration Services catalog.<br>Incorrect Answers:<br>H: sys.sp_cdc_stop_job stops a change data capture cleanup or capture job for the current database.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/system-stored-procedures/catalog-stop-operation-ssisdb-database?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-21T14:26:00.000Z",
        "voteCount": 2,
        "content": "To stop the CDC job, use:\n\nEXEC sys.sp_cdc_stop_job \nAnd to start the job again, use:\n\nEXEC sys.sp_cdc_start_job"
      },
      {
        "date": "2020-08-04T04:47:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: H\nYou just need to stop the capturing and later on you can start it again.\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-stop-job-transact-sql?view=sql-server-ver15"
      },
      {
        "date": "2020-08-07T01:32:00.000Z",
        "voteCount": 1,
        "content": "Sorry, I'm confused. Could you stop the capturing job after removing the job? The question says \"you remove the job\" so I think you don't have any job to stop or I'm understanding something wrong. Therefore in my opinion the right answer is catalog.stop_operation."
      },
      {
        "date": "2020-08-27T13:02:00.000Z",
        "voteCount": 1,
        "content": "But since you remove the Integration Service job how can you Stop a validation or instance of execution in the Integration Services catalog? Also, you need to TRACK Changes, which means that CDC must be used since Change Data Capture was implemented to track changes. \nSo, In my opinion the answer is C."
      },
      {
        "date": "2020-10-03T14:04:00.000Z",
        "voteCount": 1,
        "content": "Romove != Removed, so you remove the job but first, you need to stop it."
      },
      {
        "date": "2020-06-23T03:36:00.000Z",
        "voteCount": 1,
        "content": "The correct answer in this case should probably be F."
      },
      {
        "date": "2020-08-03T10:13:00.000Z",
        "voteCount": 3,
        "content": "I don't think so, sys.sp_cdc_disable_db drop all objects related to CDC, and the question says \"the solution must ensure that tracking changes can be restored quickly\"... \nIt also says \"you remove the Integration service job\" so you cannot execute sys.sp_cdc_stop_job. \nI think catalog.stop_operation is the right answer."
      },
      {
        "date": "2020-05-17T07:29:00.000Z",
        "voteCount": 3,
        "content": "In my opinion the answer should be  H (sys.sp_cdc_stop_job) to fulfill requirements that the users can access the application as quickly as possible. The task does not say how to change the cdc job. Any thoughts to that?"
      },
      {
        "date": "2020-05-17T07:30:00.000Z",
        "voteCount": 2,
        "content": "wrong question :) this description should be for the former question..."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20759-exam-70-767-topic-1-question-60-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You have a database named DB1 that has change data capture enabled.<br>A Microsoft SQL Server Integration Services (SSIS) job runs once weekly. The job loads changes from DB1 to a data warehouse by querying the change data capture tables.<br>You discover that the job loads changes from the previous three days only.<br>You need to ensure that the job loads changes from the previous week.<br>Which stored procedure should you execute?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.deploy_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.restore_project",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcatalog.stop_operation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_add_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_change_job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_disable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_enable_db",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.sp_cdc_stop_job"
    ],
    "answer": "A",
    "answerDescription": "catalog.deploy_project deploys a project to a folder in the Integration Services catalog or updates an existing project that has been deployed previously.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/system-stored-procedures/catalog-deploy-project-ssisdb-database",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-17T07:46:00.000Z",
        "voteCount": 7,
        "content": "Answer is E ( sys.sp_cdc_change_job) since there it says the concrete requirements for the job."
      },
      {
        "date": "2020-08-04T04:49:00.000Z",
        "voteCount": 2,
        "content": "Answer E\nYou need to change the job to specify you want to capture more than 3 days only\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sys-sp-cdc-change-job-transact-sql?view=sql-server-ver15"
      },
      {
        "date": "2020-05-27T08:30:00.000Z",
        "voteCount": 2,
        "content": "Hi!\nShouldn't it be E. sys.sp_cdc_change_job to modify the retention period of the capture job so that it covers 7 days instead of only 3 ?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/22569-exam-70-767-topic-1-question-62-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are designing a data warehouse and the load process for the data warehouse.<br>You have a source system that contains two tables named Table1 and Table2. All the rows in each table have a corresponding row in the other table.<br>The primary key for Table1 is named Key1. The primary key for Table2 is named Key2.<br>You need to combine both tables into a single table named Table3 in the data warehouse. The solution must ensure that all the nonkey columns in Table1 and<br>Table2 exist in Table3.<br>Which component should you use to load the data to the data warehouse?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Slowly Changing Dimension transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Conditional Split transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Merge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Data Conversion transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Execute SQL task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Aggregate transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Lookup transformation"
    ],
    "answer": "G",
    "answerDescription": "The Lookup transformation performs lookups by joining data in input columns with columns in a reference dataset. You use the lookup to access additional information in a related table that is based on values in common columns.<br>You can configure the Lookup transformation in the following ways:<br>Specify joins between the input and the reference dataset.<br>Add columns from the reference dataset to the Lookup transformation output.<br>Etc.<br>Incorrect Answers:<br>F: The Aggregate transformation applies aggregate functions, such as Average, to column values and copies the results to the transformation output. Besides aggregate functions, the transformation provides the GROUP BY clause, which you can use to specify groups to aggregate across.<br>References: https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/lookup-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-04T04:55:00.000Z",
        "voteCount": 4,
        "content": "Correct answer G"
      },
      {
        "date": "2020-06-08T05:40:00.000Z",
        "voteCount": 3,
        "content": "I think the correct answer is C, because is the same than former question. You have 2 tables that have a relationship by a key and you want to load all columns nonkeys in table destination.\nAnyone have a different opinion?"
      },
      {
        "date": "2020-07-07T06:12:00.000Z",
        "voteCount": 4,
        "content": "No, it says \"The solution must ensure that all the nonkey columns in Table1 and\nTable2 exist in Table3.\" It wants you to specify the existence/availability of records among source tables and the destination table. The Lookup transformation looks fine."
      },
      {
        "date": "2020-07-25T03:15:00.000Z",
        "voteCount": 1,
        "content": "I don't understand.\nWith a merge transformation you can use a join between the two tables and select all the columns of both sources. \nYou can also retrieve the columns of the second table using a lookup, but why do you think a merge transformation is wrong?"
      },
      {
        "date": "2020-07-30T05:50:00.000Z",
        "voteCount": 6,
        "content": "No, Merge Transformation doesn't join two tables. Merge Transformation just puts all rows from two tables into one table. Since the rows are corresponding, each of them would be in the resulting table twice (one with the columns from table 1 filled and one with the columns from table 2 filled) instead of just once with all non-key columns from both tables. \nTo join two tables you need to use a \"Merge Join Transformation\", but since that's not an chosable answer it has to be \"Lookup Transformation\".\nAt least that's my understanding, correct me if I'm wrong pls :D"
      },
      {
        "date": "2020-08-01T07:06:00.000Z",
        "voteCount": 2,
        "content": "Oh yes! when I was talking about \"merge transformation\" I was really thinking in a \"merge join transformation\", but as you refer that's not a chosable answer. In this case, I agree, lookup is fine."
      },
      {
        "date": "2020-07-25T04:06:00.000Z",
        "voteCount": 1,
        "content": "I agree. Merge transformation combines data from 2 datasets into one dataset. Lookup can find matching rows but cannot combine data from two sources"
      },
      {
        "date": "2020-07-30T05:56:00.000Z",
        "voteCount": 4,
        "content": "It is not about combining data in the sense of adding the rows though, cause \"All the rows in each table have a corresponding row in the other table.\" So you really just need to take the rows from one table and add the columns and data from the other table and that is exactly what a Lookup Tansformation does."
      },
      {
        "date": "2020-08-01T07:09:00.000Z",
        "voteCount": 2,
        "content": "Thanks for your answer. I agree."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15436-exam-70-767-topic-1-question-65-discussion/",
    "body": "You have a data warehouse named DW1.<br>In DW1, you plan to create a table named Table1 that will be partitioned by hour. Table1 will contain the last three hours of data.<br>You plan to implement a sliding window process for inserting data into Table1.<br>You need to recommend the minimum number of partitions that must be included in Table1 to support the planned implementation. The solution must minimize the number of transaction log records created during the insert process.<br>How many partitions should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t5",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t9",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t24"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-09T01:36:00.000Z",
        "voteCount": 8,
        "content": "Table1 contain three hours of data, add two boundary partitions with it."
      },
      {
        "date": "2020-07-21T17:16:00.000Z",
        "voteCount": 1,
        "content": "Are we assumming that within 9 hour (3 + the 2 boundry partitions) there will be a processing of partition every 3 hours.  I thought the answer was 24/3 = 8 +1 = 9"
      },
      {
        "date": "2020-08-06T04:40:00.000Z",
        "voteCount": 7,
        "content": "You have 3 partitions (3hours -&gt;partitioned hourly). When you define 3 boundery values on a partition function it creates 4. (N+1 known as the catch-all partition). You need one additional partition for the archive. Hence 5."
      },
      {
        "date": "2020-10-29T06:30:00.000Z",
        "voteCount": 1,
        "content": "\"The minimum number of partitions that must be included in Table1 to support the planned implementation\".\nBased on official 70-767 Microsoft book, archive data is keeping in the separated table, e.g. Table1_Archive.\nTherefore partition containing older data than 3 hours shouldn't be considered to calculate the number of separations in Table1.\n\nAssuming it's 8:00 a.m. right now and the sliding window partition process has just been completed.\n\nTable 1:\n1 partition: &lt;6:00, 7:00)\n2 partition: &lt;7:00, 8:00)\n3 partition (newly created with no data): &lt;8:00, 9:00)\n\nTable1_Archive\nSwitched partition &lt;5:00, 6:00)\n\nSo I would choose A."
      },
      {
        "date": "2020-10-07T07:30:00.000Z",
        "voteCount": 2,
        "content": "\"recommend the minimum number of partitions that must be included in Table1\"\n-the minimum number of partitions is 3 - one partition for each hour, two boundaries\nSo I would choose A"
      },
      {
        "date": "2020-03-04T08:24:00.000Z",
        "voteCount": 1,
        "content": "Anyone knows why is 5? thanks a lot"
      },
      {
        "date": "2020-10-07T07:34:00.000Z",
        "voteCount": 2,
        "content": "3 is too simple"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26098-exam-70-767-topic-1-question-67-discussion/",
    "body": "You plan to deploy several Microsoft SQL Server Integration Services (SSIS) packages to a highly available SQL Server instance. The instance is configured to use an AlwaysOn availability group that has two replicas.<br>You need to identify which deployment method must be used to ensure that the packages are always accessible from all the nodes in the availability group.<br>Which deployment method should you use for the packages?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy to the msdb database on the secondary replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy to the msdb database on the primary replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy to a file on the hard drive of the primary replica.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy to a shared folder on a file server."
    ],
    "answer": "A",
    "answerDescription": "Before you can configure SSIS to enable support of AlwaysOn on the new added secondary Replicas, you must connect to all new added secondary replicas.<br><img src=\"/assets/media/exam-media/02783/0011200001.jpg\" class=\"in-exam-image\"><br>Note: To use SSIS with AlwaysOn, you'll need to add the SSIS Catalog (SSISDB) into an Availability Group. You'll need to do the following steps:<br>\u2711 Make sure you meet the prerequisites for using AlwaysOn<br>\u2711 Connect to every node and create the SSISDB catalog. We need to create the catalog even on secondary nodes to create the other server-level objects<br>(cleanup jobs, keys, accounts etc) that are used by SSIS.<br>\u2711 Delete the SSISDB databases on secondary nodes.<br>\u2711 Create an availability group, specifying SSISDB as the user database<br>\u2711 Specify secondary replicas.<br>References: https://chrislumnah.com/2017/05/09/enabling-alwayson-for-ssisdb/",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-02T07:43:00.000Z",
        "voteCount": 4,
        "content": "Both A &amp; B are incorrect, we can't deploy SSIS package on msdb (this is related to SQL Jobs), We deploy packages throught .ISPac file as project deployment model which goes inside SSISDB database &amp; we can view through SSISDB Catalog, therefore, here we need to use Package Deployment Model because SSISDB is not an option here, so, either it would be C or D, I think D says shared location which seems correct but I'm not 100% sure"
      },
      {
        "date": "2021-01-02T07:47:00.000Z",
        "voteCount": 1,
        "content": "So, it clearly D because there are several packages so we have to deploy on Folder not on a single file, in addition, as per the link we can deploy package on shared location: https://social.msdn.microsoft.com/Forums/sqlserver/en-US/27e1e6f6-321e-440f-b931-093c11d68ace/how-to-configuredeploy-ssis-package-the-shared-folder-path?forum=sqlintegrationservices"
      },
      {
        "date": "2020-08-28T04:28:00.000Z",
        "voteCount": 3,
        "content": "In a second thought, after reading more regarding Always On availability group I thing the answer A is right. See links below:\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15\nhttps://docs.microsoft.com/en-us/sql/database-engine/availability-groups/windows/availability-modes-always-on-availability-groups?view=sql-server-ver15"
      },
      {
        "date": "2020-10-30T03:42:00.000Z",
        "voteCount": 1,
        "content": "Can you pinpoint why the A is right? I read the attached articles carefully and still think that B is the correct answer. I would agree with you if instead of word \"Deploy\" there was \"Replicate\". All databases within one particular availability group are deployed to the primary replica in the first node and then based on configuration the secondary databases are synchronized with the current state of the primary replica in synchronous-commit or asynchronous-commit mode."
      },
      {
        "date": "2020-12-13T02:57:00.000Z",
        "voteCount": 2,
        "content": "System databases are not mirrored nor replicated in both mirroring and alwayson availability groups. Msdb is a system database and thus it must be deployed separatelly in other nodes\nA is the correct answer"
      },
      {
        "date": "2020-07-19T00:15:00.000Z",
        "voteCount": 3,
        "content": "I am not sure to undestand why it's not B (Deploy to the msdb database on the primary replica.) \nI thought that If you deployed in the primary replica, it would have been replicate in the second replica."
      },
      {
        "date": "2020-08-27T13:42:00.000Z",
        "voteCount": 2,
        "content": "I agree"
      },
      {
        "date": "2020-08-28T04:08:00.000Z",
        "voteCount": 1,
        "content": "And I think its B\nhttps://www.rathishkumar.in/2019/06/step-by-step-guide-to-add-ssidb-to-alwayson-ag.html"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19229-exam-70-767-topic-1-question-70-discussion/",
    "body": "DRAG DROP -<br><br>Table1 -<br><br>Table1 -<br>You are designing an indexing strategy for a data warehouse. The data warehouse contains a table named<br>. Data is bulk inserted into<br>.<br>You plan to create the indexes configured as shown in the following table.<br><img src=\"/assets/media/exam-media/02783/0011500001.png\" class=\"in-exam-image\"><br>Which type of index should you use to minimize the query times of each index? To answer, drag the appropriate index types to the correct indexes. Each index type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>Select and Place:<br><img src=\"/assets/media/exam-media/02783/0011500002.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0011600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Clustered columnstore -<br>A clustered columnstore index is the physical storage for the entire table.<br>With columnstore index, SQL Server processes aggregate in BatchMode thereby delivering order of magnitude better performance when compared to rowstore.<br>SQL Server 2016 takes the aggregate performance to the next level by pushing aggregate computations to the SCAN node.<br><br>Box 2: Nonclustered columnstore -<br>A nonclustered columnstore index and a clustered columnstore index function the same. The difference is that a nonclustered index is a secondary index that's created on a rowstore table, but a clustered columnstore index is the primary storage for the entire table.<br>The nonclustered index contains a copy of part or all of the rows and columns in the underlying table. The index is defined as one or more columns of the table and has an optional condition that filters the rows.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-06T07:36:00.000Z",
        "voteCount": 22,
        "content": "Hi, in my opinion the correct answers are Clustered Columnstore and nonClustered index"
      },
      {
        "date": "2020-08-09T02:00:00.000Z",
        "voteCount": 1,
        "content": "could anyone explain why the second index should be nonclustered index instead of nonclustered columnstore index? thanks"
      },
      {
        "date": "2020-08-09T02:07:00.000Z",
        "voteCount": 1,
        "content": "How do I choose between a rowstore index and a columnstore index?\nRowstore indexes perform best on queries that seek into the data, when searching for a particular value, or for queries on a small range of values. Use rowstore indexes with transactional workloads because they tend to require mostly table seeks instead of table scans.\n\nColumnstore indexes give high performance gains for analytic queries that scan large amounts of data, especially on large tables. Use columnstore indexes on data warehousing and analytics workloads, especially on fact tables, because they tend to require full table scans rather than table seeks.\n\nhttps://docs.microsoft.com/es-es/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver15"
      },
      {
        "date": "2020-08-28T05:35:00.000Z",
        "voteCount": 1,
        "content": "And why not Clustered Columnstore and nonClustered Columnstore?"
      },
      {
        "date": "2020-11-14T16:14:00.000Z",
        "voteCount": 4,
        "content": "Answer is \nClustered Columnstore\nNonclustered\n\nIf you try to create another columnstore index you get:\nMsg 35339, Level 16, State 1, Line 3\nMultiple columnstore indexes are not supported.\nhttps://blog.sqlauthority.com/2016/01/22/sql-server-2016-creating-additional-indexes-with-clustered-columnstore-indexes/"
      },
      {
        "date": "2020-10-07T08:10:00.000Z",
        "voteCount": 1,
        "content": "hash index is even better for point lookups then Btree, so if Table1 is a memory-optimized (the question doesn't say yes or no) it's better to use hash index rather then Btree non-clustered"
      },
      {
        "date": "2020-10-07T08:12:00.000Z",
        "voteCount": 1,
        "content": "Btree index performs  the best for a range selections"
      },
      {
        "date": "2020-09-13T02:42:00.000Z",
        "voteCount": 1,
        "content": "Can have only one columnstore index on table.\nIndex 1 - Clustered columnstore because 'contains all the data and there are aggregates'\nIndex 2 - Non Clustered index because of 'point lookups in Table1' - https://www.sqlshack.com/top-10-questions-answers-sql-server-indexes/"
      },
      {
        "date": "2020-05-27T12:32:00.000Z",
        "voteCount": 1,
        "content": "Hi,\n\nShouldn't it be the following answers?\n1/ Non clustered columnstore; and\n2/ Non clustered\n\nAnd by Non clustered, for me here it implicitly means Non clustered rowstore."
      },
      {
        "date": "2020-04-28T03:15:00.000Z",
        "voteCount": 1,
        "content": "The first index shouldn't it be just a clustered Index? You can't put more than 2 columnstore indexes on one table.\n\nYou can only add one columnstore index per table.) This option is typically used when most of the queries against a table return large aggregations, but another subset does a lookup by a specific value"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33925-exam-70-767-topic-1-question-71-discussion/",
    "body": "HOTSPOT -<br>Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in the series.<br><br>Start of repeated scenario -<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>The following requirements must be met:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>\u2711 Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>\u2711 Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>\u2711 Maximize the performance during the data loading process for the Fact.Order partition.<br>\u2711 Ensure that historical data remains online and available for querying.<br>\u2711 Reduce ongoing storage costs while maintaining query performance for current data.<br>You are not permitted to make changes to the client applications.<br><br>End of repeated scenario -<br>You need to optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>Which technology should you use for each table?<br>To answer, select the appropriate technologies in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0011800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0011900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Temporal table -<br><br>Box 2: Temporal table -<br>Compared to CDC, Temporal tables are more efficient in storing historical data as it ignores insert actions.<br>Box 3: Change Data Capture (CDC)<br>By using change data capture, you can track changes that have occurred over time to your table. This kind of functionality is useful for applications, like a data warehouse load process that need to identify changes, so they can correctly apply updates to track historical changes over time.<br>CDC is good for maintaining slowly changing dimensions.<br>Scenario: Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>The Dimension.SalesTerritory and Dimension.Customer tables are frequently updated.<br>References:<br>https://www.mssqltips.com/sqlservertip/5212/sql-server-temporal-tables-vs-change-data-capture-vs-change-tracking--part-2/ https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-table-usage-scenarios?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-07T08:31:00.000Z",
        "voteCount": 3,
        "content": "-for Dimension.Date obviously there is no need to track any changes, it is enough to take all dates from the last date in DW\n-for Dimension.SalesTerritory and Dimension.Customer CDC or ChangeTracking as there are updates, deletes and insertes"
      },
      {
        "date": "2020-10-07T08:39:00.000Z",
        "voteCount": 1,
        "content": "see difference between CDC and change data capture - https://littlekendra.com/2010/06/23/cdcvsct/"
      },
      {
        "date": "2020-11-25T19:49:00.000Z",
        "voteCount": 3,
        "content": "CDC = change data capture"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21584-exam-70-767-topic-1-question-75-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You configure a new matching policy Master Data Services (MDS) as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/02783/0012500001.png\" class=\"in-exam-image\"><br>You review the Matching Results of the policy and find that the number of new values matches the new values.<br>You verify that the data contains multiple records that have similar address values, and you expect some of the records to match.<br>You need to increase the likelihood that the records will match when they have similar address values.<br>Solution: You increase the relative weights for Address Line 1 of the matching policy.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Decrease the Min. matching score.<br>A data matching project consists of a computer-assisted process and an interactive process. The matching project applies the matching rules in the matching policy to the data source to be assessed. This process assesses the likelihood that any two rows are matches in a matching score. Only those records with a probability of a match greater than a value set by the data steward in the matching policy will be considered a match.<br>References:<br>https://docs.microsoft.com/en-us/sql/data-quality-services/data-matching",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-29T08:11:00.000Z",
        "voteCount": 6,
        "content": "Hi,\n\nWhy wouldn't it be A - Yes?\nGiving more weight to Address should increase the likelihood for records with the same Address to match, right?\n\nIf anyone could help, it would be great!\nThanks"
      },
      {
        "date": "2020-07-31T05:45:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2020-11-14T18:27:00.000Z",
        "voteCount": 1,
        "content": "UPDATE: The higher the weight, the  more exact the record would need to be so 100% Weight = Exact Match.Correct answer is decrease  the min. matching score. Microsoft is just testing that we know the fact an admin can decrease below 80%"
      },
      {
        "date": "2020-11-22T09:57:00.000Z",
        "voteCount": 1,
        "content": "An additional important point is that we cannot increase weight for Address Line 1 without decreasing weight in another domain (currently the sum equals 100%)."
      },
      {
        "date": "2020-11-10T01:18:00.000Z",
        "voteCount": 1,
        "content": "UPDATE: The higher the weight, the MORE exact the data must be. 100% weight means it needs to be exact."
      },
      {
        "date": "2020-11-08T07:46:00.000Z",
        "voteCount": 1,
        "content": "Weight: For each domain in the rule, enter a numerical weight that determines how the matching analysis for the domain will be compared to that for each other domain in the rule. The weight indicates the contribution of the field's score to the overall matching score between two records.\n\nThe way I read it is: \nExample \n153 E. Mian St \n153 E. Main St  \n\nIf we weight this field higher in the equation, it would seem more likely to accept this as similar. \n\nhttps://docs.microsoft.com/en-us/sql/data-quality-services/data-matching"
      },
      {
        "date": "2020-09-11T10:57:00.000Z",
        "voteCount": 1,
        "content": "The minimum matching score  minimum value is 80. If the matching score is below 80, the two records are not considered a match. You cannot change the range of the minimum matching score in this page. The lowest min. matching score is 80. You can, however, change the lowest minimum matching score within the Administration page"
      },
      {
        "date": "2020-08-04T06:14:00.000Z",
        "voteCount": 1,
        "content": "I think A is right too, you cannot set the \"Min. matching score\" under 80%."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27291-exam-70-767-topic-1-question-77-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You configure a new matching policy Master Data Services (MDS) as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/02783/0012700001.png\" class=\"in-exam-image\"><br>You review the Matching Results of the policy and find that the number of new values matches the new values.<br>You verify that the data contains multiple records that have similar address values, and you expect some of the records to match.<br>You need to increase the likelihood that the records will match when they have similar address values.<br>Solution: You decrease the minimum matching score of the matching policy.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "We decrease the Min. matching score.<br>A data matching project consists of a computer-assisted process and an interactive process. The matching project applies the matching rules in the matching policy to the data source to be assessed. This process assesses the likelihood that any two rows are matches in a matching score. Only those records with a probability of a match greater than a value set by the data steward in the matching policy will be considered a match.<br>References:<br>https://docs.microsoft.com/en-us/sql/data-quality-services/data-matching",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-14T18:29:00.000Z",
        "voteCount": 2,
        "content": "Update: Increasing the score means it has to be closer to exact match. 100% wieght = exact match. Correct answer is Decrease MIn Matching Score."
      },
      {
        "date": "2020-11-08T07:52:00.000Z",
        "voteCount": 1,
        "content": "that's why if you INCREASE weight the address Line 1, leave the Min Matching score as low as it will go (80%) you will increase the likelihood that the records will match when they have similar address values."
      },
      {
        "date": "2020-10-06T23:38:00.000Z",
        "voteCount": 4,
        "content": "Passed the exam and had a slightly different exhibit. In the Min. matching score dropbox after the % symbol, there are arrows up and down one, here the down one is faded gray but on the exam's exhibit it was black coloured.."
      },
      {
        "date": "2020-08-05T08:41:00.000Z",
        "voteCount": 1,
        "content": "The minimum value is 80. If the matching score is below 80, the two records are not considered a match. You cannot change the range of the minimum matching score in this page. The lowest min. matching score is 80. You can, however, change the lowest minimum matching score within the Administration page (if you are a DQS administrator).\n\nhttps://docs.microsoft.com/en-us/sql/data-quality-services/create-a-matching-policy?view=sql-server-ver15"
      },
      {
        "date": "2020-08-07T03:37:00.000Z",
        "voteCount": 1,
        "content": "so if I understood well, the answer is NO because you can change the lowest minimum within the administration page but below 80 two records are not considered a match, right?"
      },
      {
        "date": "2020-08-10T11:01:00.000Z",
        "voteCount": 1,
        "content": "What's the point of being able to change the min matching score under 80 on the admin page, if it won't do anything and stick to the 80% ruling. In my opnion the answer should be Yes."
      },
      {
        "date": "2020-08-28T09:20:00.000Z",
        "voteCount": 2,
        "content": "yes, but because there is no any positive change in changing the minimum score under 80, the answer must be NO, since there is no value decreasing the minimum matching score. You are a little confusing othoman_ee."
      },
      {
        "date": "2020-08-04T05:49:00.000Z",
        "voteCount": 1,
        "content": "The minimum value for the matching score is 80%, you cannot enter a value lower than 80%."
      },
      {
        "date": "2020-08-05T11:39:00.000Z",
        "voteCount": 1,
        "content": "I've read that too, but there's a possibility to lower the 80% within the administrator page, so I'm confused with the answer.\n\nThe minimum matching score is the threshold at or above which two records are considered to be a match (and the status for the records is set to \"Matched\"). Enter an integer value in increments of \"1\" or click the up or down arrow to increase or decrease the value in increments of \"10\". The minimum value is 80. If the matching score is below 80, the two records are not considered a match. You cannot change the range of the minimum matching score in this page. The lowest min. matching score is 80. You can, however, change the lowest minimum matching score within the Administration page (if you are a DQS administrator).\nhttps://docs.microsoft.com/es-es/sql/data-quality-services/create-a-matching-policy?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/34001-exam-70-767-topic-1-question-79-discussion/",
    "body": "HOTSPOT -<br>You are a data warehouse developer.<br>You need to create a Microsoft SQL Server Integration Services (SSIS) catalog on a production SQL Server instance.<br>Which features are needed? To answer, select the appropriate options in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0013000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0013100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Yes -<br>\"Enable CLR Integration\" must be selected because the catalog uses CLR stored procedures.<br><br>Box 2: Yes -<br>Once you have selected the \"Enable CLR Integration\" option, another checkbox will be enabled named \"Enable automatic execution of Integration Services stored procedure at SQL Server startup\". Click on this check box to enable the catalog startup stored procedure to run each time the SSIS server instance is restarted.<br><img src=\"/assets/media/exam-media/02783/0013200001.png\" class=\"in-exam-image\"><br><br>Box 3: No -<br>References:<br>https://www.mssqltips.com/sqlservertip/4097/understanding-the-sql-server-integration-services-catalog-and-creating-the-ssisdb-catalog/",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-24T14:33:00.000Z",
        "voteCount": 1,
        "content": "Task mentions \"Automatic SSIS Package Execution\" but this is not a name of a feature or setting of SSIS. The screenshot show an option \"automatic execution of Integration Services stored procedure\", which is a different thing."
      },
      {
        "date": "2020-12-12T06:57:00.000Z",
        "voteCount": 1,
        "content": "The instruction relates to indicating features needed to CREATE an SSIS catalog. \"To create and use the catalog only CLR integration must be enabled .\"\n-Yes\n-No\n-No\nAn \"Automatic SSIS Package Execution\" is not needed to create a catalog. Tested locally and only was required CLR and password."
      },
      {
        "date": "2020-10-08T01:56:00.000Z",
        "voteCount": 3,
        "content": "-Yes\n-No (the feature is optional)\n-No"
      },
      {
        "date": "2020-11-26T19:06:00.000Z",
        "voteCount": 1,
        "content": "But you need it..."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20010-exam-70-767-topic-1-question-81-discussion/",
    "body": "DRAG DROP -<br>You are developing a Microsoft SQL Server Integration Services (SSIS) package to incrementally load new and changed records from a data source.<br>The SSIS package must load new records into Table1 and updated records into Table1_Updates. After loading records, the package must call a Transact-SQL statement to process updated rows according to existing business logic.<br>You need to complete the design of the SSIS package.<br>Which tasks should you use? To answer, drag the appropriate SSIS objects to the correct targets. Each SSIS object may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Step 1: CDC Control Task Get Processing Range<br><br>Step 2: Mark Processed Range -<br><br>Step 3: Data Flow -<br>The Data Flow task encapsulates the data flow engine that moves data between sources and destinations, and lets the user transform, clean, and modify data as it is moved. Addition of a Data Flow task to a package control flow makes it possible for the package to extract, transform, and load data.<br><br>Step 4: CDC Source -<br>The CDC source reads a range of change data from SQL Server 2017 change tables and delivers the changes downstream to other SSIS component.<br><br>Step 5: CDC Splitter -<br>The CDC splitter splits a single flow of change rows from a CDC source data flow into different data flows for Insert, Update and Delete operations.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/control-flow/cdc-control-task https://docs.microsoft.com/en-us/sql/integration-services/control-flow/data-flow-task https://docs.microsoft.com/en-us/sql/integration-services/data-flow/cdc-splitter?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-07T22:18:00.000Z",
        "voteCount": 8,
        "content": "Although there is no complete question here is my answer: \nControl Flow: \nStep 1: CDC Control Task Get Processing Range\nStep 2: Data Flow\nStep : CDC Control Task Mark Processed Range\nData Flow: \nStep 1: CDC Source\nStep 2: CDC Splitter"
      },
      {
        "date": "2020-07-12T23:35:00.000Z",
        "voteCount": 2,
        "content": "switch step 2 and 3 in Control Flow"
      },
      {
        "date": "2020-07-28T06:05:00.000Z",
        "voteCount": 2,
        "content": "No you should not switch them https://social.msdn.microsoft.com/Forums/sqlserver/en-US/7a436274-1244-4d3f-8233-67216eb91a48/ssis-and-cdc-incorrect-state-at-end-of-8220mark-processed-range8221?forum=sqlintegrationservices"
      },
      {
        "date": "2020-08-28T14:54:00.000Z",
        "voteCount": 3,
        "content": "CONTROL FLOW\nStep 1: CDC Control Task Get Processing Range\nStep 2: Data Flow \nStep 3: Mark Processed Range \nDATA FLOW\nStep 4: CDC Source \nStep 5: CDC Splitter"
      },
      {
        "date": "2020-07-26T11:13:00.000Z",
        "voteCount": 1,
        "content": "Step 1: CDC Control Task Get Processing Range\n\nStep 2: Mark Processed Range -\n\nStep 3: Data Flow -\nThe Data Flow task encapsulates the data flow engine that moves data between sources and destinations, and lets the user transform, clean, and modify data as it is moved. Addition of a Data Flow task to a package control flow makes it possible for the package to extract, transform, and load data.\n\nStep 4: CDC Source -\nThe CDC source reads a range of change data from SQL Server 2017 change tables and delivers the changes downstream to other SSIS component.\n\nStep 5: CDC Splitter -\nThe CDC splitter splits a single flow of change rows from a CDC source data flow into different data flows for Insert, Update and Delete operations."
      },
      {
        "date": "2020-08-03T12:31:00.000Z",
        "voteCount": 7,
        "content": "I correct my previous answer, according to the comments of Dieter and pieter94.\n\ncontrol flow\n1. get processing range\n2. data flow\n3. mark processed range\n\ndata flow\n1. cdc source\n2. cdc splitter\n\n\"MarkProcessedRange\t: This operation is executed after the CDC data flow is completed successfully, to record the last LSN that was fully processed in the CDC run.\"\nhttps://docs.microsoft.com/en-us/sql/integration-services/control-flow/cdc-control-task?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26297-exam-70-767-topic-1-question-82-discussion/",
    "body": "You have a data warehouse named DW1 that contains 20 years of data. DW1 contains a very large fact table. New data is loaded to the fact table monthly.<br>Many reports query DW1 for the past year of data.<br>Users frequently report that the reports are slow.<br>You need to modify the fact table to minimize the amount of time it takes to run the reports. The solution must ensure that other reports can continue to be generated from DW1.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the historical data to SAS disks and move the data from the past year to SSD disks. Run the ALTER TABLE statement.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove all the data to SSD disks. Load and archive the data by using partition switching.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove all the data to SAS disks. Load and archive the data by using partition switching.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the historical data to SAS disks and move the data for the past year to SSD disks. Create a distributed partitioned view."
    ],
    "answer": "A",
    "answerDescription": "We use ALTER TABLE to partition the table.<br>Incorrect Answers:<br>D: A Distributed Partitioned View contains participating tables from multiple SQL Server instances, which can be used to distribute the data processing load across multiple servers. Another advantage for the SQL Server Partitioned Views is that the underlying tables can participate in more than one Partitioned View, which could be helpful in some implementations.",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-20T23:09:00.000Z",
        "voteCount": 2,
        "content": "Not sure to understand why it's not D."
      },
      {
        "date": "2020-09-14T02:24:00.000Z",
        "voteCount": 1,
        "content": "One table only, making it a partition table is enough. Other reports must be able to use the table. If you create a view and optimise it somehow reports will use the same table anyway, which you didn't \"improve\"."
      },
      {
        "date": "2020-10-08T02:34:00.000Z",
        "voteCount": 1,
        "content": "it's harder to manage data using partitioned views, for example move partitions boundaries"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37263-exam-70-767-topic-1-question-83-discussion/",
    "body": "DRAG DPOP -<br>You have a database named OnlineSales that contains a table named Customers. You plan to copy incremental changes from the Customers table to a data warehouse every hour.<br>You need to enable change tracking for the Customers table.<br>How should you complete the Transact-SQL statements? To answer, drag the appropriate Transact-SQL segments to the correct locations. Each Transact-SQL segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Box 1: DATABASE [OnlineSales]<br>Before you can use change tracking, you must enable change tracking at the database level. The following example shows how to enable change tracking by using ALTER DATABASE.<br>ALTER DATABASE AdventureWorks2012<br><br>SET CHANGE_TRACKING = ON -<br>(CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)<br><br>Box 2: CHANGE_TRACKING = ON -<br><br>ALTER SET CHANGE_RETENTION -<br>Box 3: ALTER TABLE [dbo].[Customers]<br>Change tracking must be enabled for each table that you want tracked. When change tracking is enabled, change tracking information is maintained for all rows in the table that are affected by a DML operation.<br>The following example shows how to enable change tracking for a table by using ALTER TABLE.<br><br>ALTER TABLE Person.Contact -<br><br>ENABLE CHANGE_TRACKING -<br>WITH (TRACK_COLUMNS_UPDATED = ON)<br><br>Box 4: ENABLE CHANGE_TRACKING -<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-18T16:59:00.000Z",
        "voteCount": 2,
        "content": "ALTER DATABASE AdventureWorks2012  \nSET CHANGE_TRACKING = ON  \n(CHANGE_RETENTION = 2 DAYS, AUTO_CLEANUP = ON)  \n\nALTER TABLE Person.Contact  \nENABLE CHANGE_TRACKING  \nWITH (TRACK_COLUMNS_UPDATED = ON)\nhttps://docs.microsoft.com/en-us/sql/relational-databases/track-changes/enable-and-disable-change-tracking-sql-server?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18646-exam-70-767-topic-1-question-85-discussion/",
    "body": "Note: This question is part of a series of questions that use the same scenario. For your convenience, the scenario is repeated in each question. Each question presents a different goal and answer choices, but the text of the scenario is exactly the same in each question in this series.<br>You have a Microsoft SQL Server data warehouse instance that supports several client applications.<br>The data warehouse includes the following tables: Dimension.SalesTerritory, Dimension.Customer, Dimension.Date, Fact.Ticket, and Fact.Order. The<br>Dimension.SalesTerritory and Dimension.Customer tables are frequently updated. The Fact.Order table is optimized for weekly reporting, but the company wants to change it to daily. The Fact.Order table is loaded by using an ETL process. Indexes have been added to the table over time, but the presence of these indexes slows data loading.<br>All data in the data warehouse is stored on a shared SAN. All tables are in a database named DB1. You have a second database named DB2 that contains copies of production data for a development environment. The data warehouse has grown and the cost of storage has increased. Data older than one year is accessed infrequently and is considered historical.<br>You have the following requirements:<br>\u2711 Implement table partitioning to improve the manageability of the data warehouse and to avoid the need to repopulate all transactional data each night. Use a partitioning strategy that is as granular as possible.<br>\u2711 Partition the Fact.Order table and retain a total of seven years of data.<br>\u2711 Partition the Fact.Ticket table and retain seven years of data. At the end of each month, the partition structure must apply a sliding window strategy to ensure that a new partition is available for the upcoming month, and that the oldest month of data is archived and removed.<br>\u2711 Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br>\u2711 Incrementally load all tables in the database and ensure that all incremental changes are processed.<br>\u2711 Maximize the performance during the data loading process for the Fact.Order partition.<br>\u2711 Ensure that historical data remains online and available for querying.<br>Reduce ongoing storage costs while maintaining query performance for current data.<br><img src=\"/assets/media/exam-media/02783/0013700008.png\" class=\"in-exam-image\"><br>You are not permitted to make changes to the client applications.<br>You need to configure data loading for the tables.<br>Which data loading technology should you use for each table? To answer, select the appropriate options in the answer area.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Scenario: The Dimension.SalesTerritory and Dimension.Customer tables are frequently updated<br>Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.<br><br>Box 1: Change Tracking -<br><br>Box 2: Change Tracking -<br><br>Box 3: Temporal Table -<br>Temporal Tables are generally useful in scenarios that require tracking history of data changes.<br>We recommend you to consider Temporal Tables in the following use cases for major productivity benefits.<br>* Slowly-Changing Dimensions<br>Dimensions in data warehousing typically contain relatively static data about entities such as geographical locations, customers, or products.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-table-usage-scenarios",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-04T22:40:00.000Z",
        "voteCount": 2,
        "content": "Check out question 15, it is from the same subset and the task there is - \"You need to optimize data loading for the Dimension.Customer table....Which three Transact-SQL segments should you use to develop the solution?\" \nAnd there are hotboxes and they are about CDC. So, CDC is the correct answer for Customer and SalesTerritoy."
      },
      {
        "date": "2020-05-17T09:13:00.000Z",
        "voteCount": 3,
        "content": "Strange. The link in the answer says that Temporal Tables are better for SCD like Customers or Geography data - so why is the answer change tracking for these two tables? It has to be temporal tables."
      },
      {
        "date": "2020-08-19T01:00:00.000Z",
        "voteCount": 1,
        "content": "Also this question is the same as #71 right? According to the answer there it should be Temporal Tables for SalesTerritory and Customer and CDC for Date."
      },
      {
        "date": "2020-08-21T05:59:00.000Z",
        "voteCount": 1,
        "content": "The questions are different between #71 and this one. #71 question is to Optimize data loading. And here it is Configure data loading."
      },
      {
        "date": "2020-08-23T05:19:00.000Z",
        "voteCount": 1,
        "content": "The scenario says \"Optimize data loading for the Dimension.SalesTerritory, Dimension.Customer, and Dimension.Date tables.\" as a requirement in both questions though."
      },
      {
        "date": "2020-08-29T12:56:00.000Z",
        "voteCount": 2,
        "content": "Temporal tables is for optimizing data loading (question 71) but this case is for CONFIGURE"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/34009-exam-70-767-topic-1-question-86-discussion/",
    "body": "You have a data warehouse named DW1. All data files are located on drive E.<br>You expect queries that pivot hundreds of millions of rows for each report.<br>You need to modify the data files to minimize latency.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more data files to DW1 on drive E.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd more data files to tempdb on drive E.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove data files from tempdb",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove data files from DW1."
    ],
    "answer": "B",
    "answerDescription": "The number of files depends on the number of (logical) processors on the machine. As a general rule, if the number of logical processors is less than or equal to eight, use the same number of data files as logical processors. If the number of logical processors is greater than eight, use eight data files and then if contention continues, increase the number of data files by multiples of 4 until the contention is reduced to acceptable levels or make changes to the workload/code.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/databases/tempdb-database",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-29T06:01:00.000Z",
        "voteCount": 1,
        "content": "Create as many files as needed to maximize disk bandwidth. Using multiple files reduces tempdb storage contention and yields significantly better scalability. However, do not create too many files because this can reduce performance and increase management overhead. As a general guideline, create one data file for each CPU on the server.\nhttps://docs.microsoft.com/en-us/previous-versions/sql/sql-server-2008-r2/ms175527(v=sql.105)?redirectedfrom=MSDN"
      },
      {
        "date": "2020-12-16T14:23:00.000Z",
        "voteCount": 1,
        "content": "When PSSSQL first came up with this advice, it was really for a particular use-case, when lots of objects are being created in tempdb in succession by different processes. But they didn't make it clear. And now it's baked into the installer *sigh*\nThe contention that *could* occur in tempdb is on the SGAM page. The first extent of a new table is a mixed extent and SQL Server will check the 1st SGAM of the file, and this page potentially becomes a bottle neck. With having multiple files, object creation is done round-robin. So 2 processed go into 2 different tempdb files and can create new objects without contention. If we are pivoting lots of data, we could be spilling to tempDB. Adding files will not solve (work) table creation issues, how many work tables are going to get created in tempdb. Putting files on different spindles would potentially help"
      },
      {
        "date": "2020-11-29T02:08:00.000Z",
        "voteCount": 1,
        "content": "Finally, what is the correct response here?"
      },
      {
        "date": "2020-11-18T17:06:00.000Z",
        "voteCount": 2,
        "content": "DAtabase 101 says to ensure you have 1 file per PROC in temp DB up to 8."
      },
      {
        "date": "2020-10-08T03:24:00.000Z",
        "voteCount": 1,
        "content": "-the question is not about tempdb, so it's not B or C\n-I would choose rather A\nsee - https://www.sqlskills.com/blogs/paul/benchmarking-do-multiple-data-files-make-a-difference/"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18647-exam-70-767-topic-1-question-88-discussion/",
    "body": "DRAG DROP -<br>You are designing the data warehouse to import data from three different environments. The sources for the data warehouse will be loaded every hour.<br>Scenario A includes tables in a Microsoft Azure SQL Database:<br>\u2711 Millions of updates and inserts occur per hour<br>\u2711 A periodic query of the current state of rows that have changed is needed.<br>\u2711 The change detection method needs to be able to ignore changes to some columns in a table.<br>\u2711 The source database is a member of an AlwaysOn Availability group.<br>Scenario B includes tables with status update changes:<br>\u2711 Tracking the duration between workflow statuses.<br>\u2711 All transactions must be captured, including before/after values for UPDATE statements.<br>\u2711 To minimize impact to performance, the change strategy adopted should be asynchronous.<br>Scenario C includes an external source database:<br>\u2711 Updates and inserts occur regularly.<br>\u2711 No changes to the database should require code changes to any reports or applications.<br>\u2711 Columns are added and dropped to tables in the database periodically. These schema changes should not require any interruption or reconfiguration of the change detection method chose.<br>\u2711 Data is frequently queried as the entire row appeared at a past point in time.<br>All tables have primary keys.<br>You need to load each data source. You must minimize complexity, disk storage, and disruption to the data sources and the existing data warehouse.<br>Which change detection method should you use for each scenario? To answer, drag the appropriate loading methods to the correct scenarios. Each source may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Box A: System-Versioned Temporal Table<br>System-versioned temporal tables are designed to allow users to transparently keep the full history of changes for later analysis, separately from the current data, with the minimal impact on the main OLTP workload.<br><br>Box B: Change Tracking -<br><br>Box C: Change Data Capture -<br>Change data capture supports tracking of historical data, while that is not supported by change tracking.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/track-changes/track-data-changes-sql-server https://docs.microsoft.com/en-us/sql/relational-databases/tables/temporal-table-usage-scenarios",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-27T08:05:00.000Z",
        "voteCount": 17,
        "content": "A: Change Tracking because \"AlwaysOn Availability group\", current state of rows\nB: CDC because -&gt; All transactions must be captured, including before/after values for UPDATES, \nasynchronous!\nC: System-Versioned Temporal Table because \"queried as the entire row appeared at a past point in time\""
      },
      {
        "date": "2020-11-18T17:20:00.000Z",
        "voteCount": 2,
        "content": "CDC\nCDC\nTemporal \nif you reference Lil Kendra's resource:\nhttps://littlekendra.com/2010/06/23/cdcvsct"
      },
      {
        "date": "2020-11-18T17:11:00.000Z",
        "voteCount": 3,
        "content": "Change Tracking (CT)\nChange Tracking is a synchronous mechanism which modifies change tracking tables as part of ongoing transactions to indicate when a row has been changed. It does not record past or intermediate versions of the data itself, only that a change has occurred. It is recommended to use snapshot isolation with Change Tracking! (See the links below for details on why.)\n\nChange Data Capture (CDC)\nChange Data Capture is asynchronous and uses the transaction log in a manner similar to replication. Past values of data are maintained and are made available in change tables by a capture process, managed by the SQL Agent, which regularly scans the T-Log. As with replication, this can prevent re-use of parts of the log.\nhttps://littlekendra.com/2010/06/23/cdcvsct/\n\nTemporal tables are the only option that allows to query historical data. Where were these when I started with SQL in HR? lol"
      },
      {
        "date": "2020-05-07T22:55:00.000Z",
        "voteCount": 2,
        "content": "Join in with als2kool. In addition: \nScenario A: \nA periodic query of the current state of rows that have changed is needed. =&gt; if only the current state is needed, Change tracking would be enought IMHO. \n\nScenario B: \nAll transactions must be captured, including before/after values for UPDATE statements. =&gt; that is only possible with CDC (not CT). \n\u2711 To minimize impact to performance, the change strategy adopted should be asynchronous. =&gt; another point for CDC.\n\nI am unsure about scenario C. who can help?"
      },
      {
        "date": "2020-04-18T08:53:00.000Z",
        "voteCount": 1,
        "content": "CDC is asynchronous so what gives with B?"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14637-exam-70-767-topic-1-question-89-discussion/",
    "body": "HOTSPOT -<br>You are developing a data warehouse. You run the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0014200001.png\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that answers each question based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-27T08:02:00.000Z",
        "voteCount": 9,
        "content": "The Name of the table created: TransactionHistoryArchive\nThe Name of the primary key: TransactionID"
      },
      {
        "date": "2020-03-30T09:07:00.000Z",
        "voteCount": 6,
        "content": "There are no drop-down menus   for this and 81-84.  The site needs to be updated.  As it stands now, I'm trying to paint a picture and guess at an answer.  That's no the way I prefer to review for a test."
      },
      {
        "date": "2020-04-24T13:44:00.000Z",
        "voteCount": 2,
        "content": "Talked to support on this topic.  To be brief, their answer was, tough luck.  I guess they are buying the content from someone else and have no technical people on staff to handle these problems.  I subtract the number of questions from the total, 130, and divide that into the number of questions I got right.  When I hit 90% range, I figure I'll have made up for the questions I couldn't answer like this one and the others you mentioned."
      },
      {
        "date": "2021-01-20T06:46:00.000Z",
        "voteCount": 1,
        "content": "https://vceguide.com/hotspot-862/"
      },
      {
        "date": "2020-11-18T17:12:00.000Z",
        "voteCount": 2,
        "content": "The Name of the table created: TransactionHistoryArchive\nThe Name of the primary key: TransactionID"
      },
      {
        "date": "2020-11-07T01:50:00.000Z",
        "voteCount": 2,
        "content": "The Name of the table created: TransactionHistoryArchive\nThe Name of the primary key: TransactionID"
      },
      {
        "date": "2020-09-26T15:35:00.000Z",
        "voteCount": 5,
        "content": "Q1 - What is the name of the table created? - AdventureWorks, Production, TransactionHistoryArchive\nQ2 - What is the name of PK? - Identity, Production, TransactionID\n\neasy as\n\nHere is the link with an image - https://vceguide.com/hotspot-862/"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15387-exam-70-767-topic-1-question-90-discussion/",
    "body": "HOTSPOT -<br>You are designing a data transformation process using Microsoft SQL Server Integration Services (SSIS).<br>You need to ensure that every row is compared with every other row during transformation.<br>What should you configure? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "When you configure the Fuzzy Grouping transformation, you can specify the comparison algorithm that the transformation uses to compare rows in the transformation input. If you set the Exhaustive property to true, the transformation compares every row in the input to every other row in the input. This comparison algorithm may produce more accurate results, but it is likely to make the transformation perform more slowly unless the number of rows in the input is small.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/fuzzy-grouping-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-03T23:58:00.000Z",
        "voteCount": 6,
        "content": "Fuzzy Grouping\nExhaustive"
      },
      {
        "date": "2020-03-27T08:07:00.000Z",
        "voteCount": 5,
        "content": "Transformation Type: Fuzzy Grouping\nTransformation Property: Exhaustive"
      },
      {
        "date": "2021-01-26T07:22:00.000Z",
        "voteCount": 1,
        "content": "Dear Admin - Can you complete the question"
      },
      {
        "date": "2020-05-05T11:19:00.000Z",
        "voteCount": 5,
        "content": "Fuzzy Grouping\nExhausive\n\nRead question 2 here\n\nhttps://www.studocu.com/en-us/document/massachusetts-institute-of-technology/computer-networks/coursework/2020-pass4itsure-microsoft-implementing-a-data-warehouse-using-sql-70-767-exam-dumps-new-pdf/6758472/view"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17558-exam-70-767-topic-1-question-92-discussion/",
    "body": "DRAG DROP -<br>You need to build a knowledge base in Data Quality Services (DQS).<br>You need to ensure that the data is validated by using a third-party data source before DQS processes the data.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Building a DQS knowledge base involves the following processes and components:<br>Step 1: Perform Knowledge Discovery<br>A computer-assisted process that builds knowledge into a knowledge base by processing a data sample<br>Step 2: Perform Domain Management<br>An interactive process that enables the data steward to verify and modify the knowledge that is in knowledge base domains, each of which is associated with a data field. This can include setting field-wide properties, creating rules, changing specific values, using reference data services, or setting up term-based or cross- field relationships.<br>Step 3: Configure reference Data Services<br>A process of domain management that enables you to validate your data against data maintained and guaranteed by a reference data provider.<br>Step 4: Configure a Matching Policy<br>A policy that defines how DQS processes records to identify potential duplicates and non-matches, built into the knowledge base in a computer-assisted and interactive process.<br>References:<br>https://docs.microsoft.com/en-us/sql/data-quality-services/dqs-knowledge-bases-and-domains",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-27T08:22:00.000Z",
        "voteCount": 6,
        "content": "Building a DQS knowledge base involves the following processes and components: Step 1: Perform Knowledge Discovery\nA computer-assisted process that builds knowledge into a knowledge base by processing a data sample Step 2: Perform Domain Management\nAn interactive process that enables the data steward to verify and modify the knowledge that is in knowledge base domains, each of which is associated with a data field. This can include setting field-wide properties, creating rules, changing specific values, using reference data services, or setting up term-based or cross-field relationships.\nStep 3: Configure reference Data Services\nA process of domain management that enables you to validate your data against data maintained and guaranteed by a reference data provider.\nStep 4: Configure a Matching Policy\nA policy that defines how DQS processes records to identify potential duplicates and non-matches, built into the knowledge base in a computer-assisted and interactive process.\nReferences: https://docs.microsoft.com/en-us/sql/data-quality-services/dqs-knowledge-bases-and-domains"
      },
      {
        "date": "2020-09-14T23:09:00.000Z",
        "voteCount": 1,
        "content": "Configure DQS to Use Reference Data from Direct Online Third-Party Reference Data Providers - https://docs.microsoft.com/en-us/sql/data-quality-services/configure-dqs-to-use-reference-data?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15740-exam-70-767-topic-1-question-93-discussion/",
    "body": "You are developing a Microsoft SQL Server Master Data Services (MDS) solution.<br>The model contains an entity named Product. The Product entity has three user-defined attributes named Category, Subcategory, and Price, respectively.<br>You need to ensure that combinations of values stored in the Category and Subcategory attributes are unique.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attribute group that consists of the Category and Subcategory attributes. Create a custom index for the attribute group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish a business rule that will be used by the Product entity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a derived hierarchy based on the Category and Subcategory attributes. Use the Category attribute as the top level for the hierarchy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an attribute group that consists of the Category and Subcategory attributes. Publish a business rule for the attribute group."
    ],
    "answer": "D",
    "answerDescription": "In Master Data Services, business rule actions are the consequence of business rule condition evaluations. If a condition is true, the action is initiated.<br>The Validation action \"must be unique\": The selected attribute must be unique independently or in combination with defined attributes.<br>Incorrect Answers:<br>A: In Master Data Services, attribute groups help organize attributes in an entity. When an entity has lots of attributes, attribute groups improve the way an entity is displayed in the Master Data Manager web application.<br>C: A Master Data Services derived hierarchy is derived from the domain-based attribute relationships that already exist between entities in a model.<br>References:<br>https://docs.microsoft.com/en-us/sql/master-data-services/business-rule-actions-master-data-services",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-06T23:13:00.000Z",
        "voteCount": 7,
        "content": "The correct answer is B"
      },
      {
        "date": "2021-01-02T22:21:00.000Z",
        "voteCount": 1,
        "content": "We can't create Business Rule &amp; can't create Index on Attribute Group, they are just created for format the view on Explorer section so that user can easily view the attributes (like tabs in web applications), B looks closely match answer"
      },
      {
        "date": "2021-01-02T22:27:00.000Z",
        "voteCount": 1,
        "content": "I can also confirm that Derived Hierarchy is also not true because in SQL Server 2016 they have introduced Many-To-Many relationship, therefore sub-category can be part of multiple categories &amp; one category obviously would have many sub-categories. So, B is correct answer but i'm not sure if they change on exam"
      },
      {
        "date": "2020-10-12T10:47:00.000Z",
        "voteCount": 1,
        "content": "I would say A, we need index"
      },
      {
        "date": "2020-10-06T23:40:00.000Z",
        "voteCount": 3,
        "content": "In the exam, they've replaced B with something like that, 'creating a unique attribute for the Product entity'. There was nothing about the Business Rules."
      },
      {
        "date": "2020-11-05T15:10:00.000Z",
        "voteCount": 2,
        "content": "What is the correct answer?"
      },
      {
        "date": "2020-05-13T22:56:00.000Z",
        "voteCount": 3,
        "content": "According to the other two comments. correct is B. Attribute groups are not necessary for the requirement. Business rule is enough."
      },
      {
        "date": "2020-04-06T11:23:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct.  Business rules apply to attributes"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17559-exam-70-767-topic-1-question-96-discussion/",
    "body": "DRAG DROP -<br>You need to load data from a CSV file to a table.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate Transact-SQL segments to the correct locations. Each Transact-SQL segment may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Example:<br><br>BULK INSERT Sales.Orders -<br>FROM '\\\\SystemX\\DiskZ\\Sales\\data\\orders.csv'<br>WITH ( FORMAT='CSV');",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-02T12:45:00.000Z",
        "voteCount": 19,
        "content": "Look for scenario here\n\nhttps://pdf.testsdumps.com/70-767.pdf\n\nQuestion 6"
      },
      {
        "date": "2020-03-27T08:34:00.000Z",
        "voteCount": 15,
        "content": "Transact-SQL segments\nbulk, insert, from, with, merge\n\nAnswer area\nxxxx yyyy SALES.INVOICES\nxxxx '\\\\share\\data\\file1.csv'\nxxxx (FORMAT = 'CSV')\n\nSolution:\nBULK INSERT\nFROM\nWITH"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/12463-exam-70-767-topic-1-question-98-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are developing a Microsoft SQL Server Integration Services (SSIS) package. The package design consists of two differently structured sources in a single data flow. The Sales source retrieves sales transactions from a SQL Server database, and the Product source retrieves product details from an XML file.<br>You need to combine the two data flow sources into a single output dataset.<br>Which SSIS Toolbox item should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCDC Control task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCDC Splitter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tXML task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFuzzy Grouping",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join"
    ],
    "answer": "G",
    "answerDescription": "The Merge Join transformation provides an output that is generated by joining two sorted datasets using a FULL, LEFT, or INNER join. For example, you can use a LEFT join to join a table that includes product information with a table that lists the country/region in which a product was manufactured. The result is a table that lists all products and their country/region of origin.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/merge-join-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-26T07:27:00.000Z",
        "voteCount": 1,
        "content": "Union all or Union is wrong as they are two different structures - Merge is out too as it is not a Ssis Component - Hence its Merge Join as nothing is mentioned about being sorted or unsorted."
      },
      {
        "date": "2020-11-19T09:45:00.000Z",
        "voteCount": 2,
        "content": "Merge Join, due to the 2 data sources:\n\"\nMerge Join is same as JOIN in t-sql, you can choose between different types of Inner join, left outer join and outer join the difference is that with Merge join transformation you can support two inputs from two different data source, for example one from flat file and another from oracle DB, but with join in t-sql you can only join from one data source.\"\n\nhttps://stackoverflow.com/questions/46664399/ssis-difference-between-merge-and-merge-join"
      },
      {
        "date": "2020-09-28T01:55:00.000Z",
        "voteCount": 2,
        "content": "It is a Merge Join, here is the detailed example - https://www.red-gate.com/simple-talk/sql/ssis/ssis-basics-using-the-merge-join-transformation/"
      },
      {
        "date": "2020-09-20T03:35:00.000Z",
        "voteCount": 2,
        "content": "Use can't use Merge or Union when you have Data with different structures.  Columns structure needs to align. I guess Merge Join still works"
      },
      {
        "date": "2020-08-31T11:57:00.000Z",
        "voteCount": 3,
        "content": "From the options we can judge between Union All, Merge and Merge Join.\n\n...\"The Merge transformation is similar to the Union All transformations. Use the Union All transformation instead of the Merge transformation in the following situations:\nThe transformation inputs are not sorted.\nThe combined output does not need to be sorted.\nThe transformation has more than two inputs.\"...\nlink: https://simplebiinsights.com/ssis-difference-between-merge-and-merge-join/#:~:text=Merge%20Join%20transformation%20merge%20the,or%20LEFT%20or%20INNER%20JOIN.&amp;text=Merge%20Join%20Transformation%20requires%20sorted,joined%20columns%20have%20matching%20metadata.\n\nWe don't know if our inputs are sorted but we know that we have two different structured sources and in this situation we cannot use Union All, so the best answer is to use Merge or Merge Join. I would choose Merge since it is more similar to Union All and also in its definition: \"Merge data from two data sources, such as tables and files\", which is our case. \n\nSo, answer: F"
      },
      {
        "date": "2020-01-21T10:37:00.000Z",
        "voteCount": 4,
        "content": "C. Union All\n\nThe Union All transformation combines multiple inputs into one output. For example, the outputs from five different Flat File sources can be inputs to the Union All transformation and combined into one output.\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/union-all-transformation?view=sql-server-ver15"
      },
      {
        "date": "2020-02-19T01:57:00.000Z",
        "voteCount": 14,
        "content": "Two differently structured sources can't be using Union all.\nSales table and Products table can't be union, it should be merge join as Products is a lookup table"
      },
      {
        "date": "2020-04-16T06:16:00.000Z",
        "voteCount": 1,
        "content": "I'll take your point a step further,  mohroshdy.  Merge join expects the inputs to be joined.  Nowhere in the description does it say that the sources are sorted.  Therefore, I chose merge."
      },
      {
        "date": "2020-04-17T00:33:00.000Z",
        "voteCount": 1,
        "content": "Merge expects your input to be sorted as well"
      },
      {
        "date": "2020-08-31T10:36:00.000Z",
        "voteCount": 1,
        "content": "Both Merge and Merge Join require that Inputs have their data sorted, so you cannot judge on this."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13661-exam-70-767-topic-1-question-99-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are developing a Microsoft SQL Server Integration Services (SSIS) package.<br>You are importing data from databases at retail stores into a central data warehouse. All stores use the same database schema.<br>The query being executed against the retail stores is shown below:<br><img src=\"/assets/media/exam-media/02783/0015000001.png\" class=\"in-exam-image\"><br>The data source property named IsSorted is set to True. The output of the transform must be sorted.<br>You need to add a component to the data flow.<br>Which SSIS Toolbox item should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCDC Control task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCDC Splitter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tXML task",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFuzzy Grouping",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-02-09T06:01:00.000Z",
        "voteCount": 16,
        "content": "This should be F. Merge"
      },
      {
        "date": "2020-12-16T06:15:00.000Z",
        "voteCount": 1,
        "content": "It is Merge, not Union All\nWhile Union All will accept Sorted Input, the output would not necessary be Sorted, rather the streams will be \"randomly interleaved\". Merge requires Sorted input and will interleave the results based on the Sort Keys"
      },
      {
        "date": "2020-04-14T23:41:00.000Z",
        "voteCount": 15,
        "content": "Guys...it multiple data sources its 2+ data sources.. and they are already sorted by query\nand their sort of property.. so union all and merge are the same.. but union all is the correct answer because it can accept more than 2 data sources .. eg. 3 store's, 4 store's tables... etc.. so union all is correct"
      },
      {
        "date": "2020-04-17T00:34:00.000Z",
        "voteCount": 3,
        "content": "Output must be sorted. Union all doesn't sort output"
      },
      {
        "date": "2020-08-31T12:54:00.000Z",
        "voteCount": 1,
        "content": "Even Merge doesn't sort output, it requires input to be sorted and we have it by using ORDER BY. So, your argument doesn't stand. And since we must join more than one data source only Union All fits here."
      },
      {
        "date": "2020-09-22T06:41:00.000Z",
        "voteCount": 1,
        "content": "All stores use the same database schema. -  We don't know how many stores in there .If it's more than 2 then definitely Union all Which is not provided in the question.\n\nUnion all doesn't sort the  output even if the Source is Sorted. It just provides a random order output irrespective of input data is sorted or Not. \nSo I am going for  \"MERGE\""
      },
      {
        "date": "2020-09-15T00:48:00.000Z",
        "voteCount": 1,
        "content": "In DW you will need to know which store provided the info, so there is a Store_ID and all data from each store is sorted. Microsoft is saying that Merge and Merge Join are limited to 2 sources only, so let's say you prepared the data as two sources and now you want to sort it by what? by ID? 100 shops can have the same ID 101... so, I think there are Store_IDs as well and Union All will do the job"
      },
      {
        "date": "2020-08-21T02:19:00.000Z",
        "voteCount": 1,
        "content": "\"you need to add *a* component\" - answer is merge"
      },
      {
        "date": "2020-08-04T12:14:00.000Z",
        "voteCount": 6,
        "content": "Correct Answer F:\nunder the explanation of Union you can find:\nThe transformation inputs are added to the transformation output one after the other; no reordering of rows occurs. If the package requires a sorted output, you should use the Merge transformation instead of the Union All transformation.\n\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/union-all-transformation?view=sql-server-ver15"
      },
      {
        "date": "2020-08-04T09:52:00.000Z",
        "voteCount": 4,
        "content": "I think the question is a bit confusing...\nWith sorted sources we can use a Merge, that will return a sorted output, but this component only accepts two sources, and the question says \"all stores\" so maybe there are more than 2 sources. In that case a possible option is to use more Merge components to continue merging other sources in pairs, and the output will remain sorted.\nIf we use a Union All component we can join data from more than 2 sources but we'll need a sort component to sort the output.\nDepending on the point of view, I think that Union all and Merge both could be correct.\nThe question says \"which item should you use?\" to be strict with this, using only one type of item, the correct answer should be merge (more than one merge components if we have more than 2 sources)"
      },
      {
        "date": "2020-08-04T09:53:00.000Z",
        "voteCount": 3,
        "content": "solution with union all:\nsrc1       src2      src3\n\u2800|_______|________|\n\u2800\u2800\u2800\u2800\u2800 |\n\u2800\u2800\u2800union all\n\u2800\u2800\u2800\u2800\u2800 |\n             sort\n\u2800\u2800\u2800\u2800\u2800|\n\u2800\u2800\u2800destination\n\nsolution with merge components:\nsrc1      src2      src3\n  |________|\u2800\u2800\u2800\u2800|\n\u2800\u2800\u2800 |\u2800\u2800\u2800\u2800\u2800\u2800  |\n\u2800\u2800merge \u2800\u2800\u2800\u2800 |\n\u2800\u2800\u2800 | __________|\n\u2800\u2800\u2800\u2800\u2800\u2800 |\n\u2800\u2800\u2800\u2800merge\n\u2800\u2800\u2800\u2800\u2800\u2800 |\n\u2800\u2800\u2800destination\n\nwhat do you think??"
      },
      {
        "date": "2020-11-07T06:34:00.000Z",
        "voteCount": 1,
        "content": "Your are writing about merge join, not merge."
      },
      {
        "date": "2020-02-19T02:18:00.000Z",
        "voteCount": 3,
        "content": "The output of the transform must be sorted so we have to use Merge"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/7937-exam-70-767-topic-1-question-101-discussion/",
    "body": "You have a data warehouse that contains a fact table named Table1 and a Product table named Dim1. Dim1 is configured as shown in the following table.<br><img src=\"/assets/media/exam-media/02783/0015100001.png\" class=\"in-exam-image\"><br>You are adding a second OLTP system to the data warehouse as a new fact table named Table2. The Product table of the OLTP system is configured as shown in the following table<br><img src=\"/assets/media/exam-media/02783/0015200001.png\" class=\"in-exam-image\"><br>You need to modify Dim1 to ensure that the table can be used for both fact tables.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the data type of the Weight column in Dim1 to decimal (19, 2).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the SalesUnit column to Dim1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the data type of the Name column in Dim1 to varchar (85).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop the ProductKey column from Dim1 and replace the column with the ProductIdentifier column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop the Color column from Dim1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the data type of the ProductKey column in Dim1 to char (18)."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-05T07:32:00.000Z",
        "voteCount": 17,
        "content": "I think should be AB"
      },
      {
        "date": "2020-02-25T00:45:00.000Z",
        "voteCount": 5,
        "content": "right answers are A, B"
      },
      {
        "date": "2021-01-25T11:29:00.000Z",
        "voteCount": 1,
        "content": "We must use Dim1 with new Table2. For that we must have a key. We must drop productkey, that we can insert a ProductIdentifier and it is a key for both Table1 and Table2. So, D is correct"
      },
      {
        "date": "2020-11-18T17:51:00.000Z",
        "voteCount": 2,
        "content": "AB\nFor decimal and numeric data types, SQL Server considers each combination of precision and scale as a different data type. For example, decimal(5,5) and decimal(5,0) are considered different data types."
      },
      {
        "date": "2020-10-08T06:05:00.000Z",
        "voteCount": 1,
        "content": "I think BD"
      },
      {
        "date": "2020-10-08T06:19:00.000Z",
        "voteCount": 1,
        "content": "there is no need for A"
      },
      {
        "date": "2020-03-27T09:43:00.000Z",
        "voteCount": 3,
        "content": "ProductIdentifier varchar(8) of table2  can fit into ProductKey varchar(10) of Dim1. I don't have to drop ProductKey\nI have to use SalesUnit, then add it. \nA B"
      },
      {
        "date": "2020-03-08T03:33:00.000Z",
        "voteCount": 2,
        "content": "\u00d7\nI think sales unit might not be an attribute for a  Product dim? Nevertheless, if you drop the initial key(char 10) for the new one(char 8), the dim will not be able to be used for the original fact table. Thus i would tend to agree with A and B as correct answers"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/34028-exam-70-767-topic-1-question-103-discussion/",
    "body": "You are designing a warehouse named DW1.<br>A table named Table1 is partitioned by using the following partitioning scheme and function.<br><img src=\"/assets/media/exam-media/02783/0015400001.png\" class=\"in-exam-image\"><br>Reports are generated from the data in Table1.<br>You need to ensure that queries to DW1 return results as quickly as possible.<br>Which column should appear in the WHERE statement clause of the query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccountNumber",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMyId",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDueDate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrderDate"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-09T08:35:00.000Z",
        "voteCount": 4,
        "content": "1) First of all the CREATE PARTITION FUNCTION [FUNCTION_NAME] (COLUMN_NAME) is missing from the syntax so we cannot know for sure which column the partition function is compiled to use.\n2) Let's move to the Composite Primary Key: (MyId, OrderDate)\n\nAnswer is OrderDate. why? 1)It's part of the Primary Key 2) it lends to Partition Elimination assuming the Function is built using the OrderDate Column"
      },
      {
        "date": "2020-10-08T06:33:00.000Z",
        "voteCount": 1,
        "content": "Interesting question\n-it can be MyID or OrderDate\n-if choose MyID several (as the index is aligned by default, so every partition index) indexes will be seeked, but there will be no scans\n-i choose OrderDate  the index can't be used, but only one partition will be scanned"
      },
      {
        "date": "2020-10-08T06:35:00.000Z",
        "voteCount": 1,
        "content": "I would say rather B, but the answer depends on how much records are in the partitions"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15575-exam-70-767-topic-1-question-109-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are the administrator of a Microsoft SQL Server Master Data Services (MDS) instance. The instance contains a model named Geography and a model named Customer. The Geography model contains an entity named CountryRegion.<br>You need to ensure that the CountryRegion entity members are available in the customer model.<br>Solution: Create a CountryRegion entity in the Customer model. In the Geography model, create a subscription view and load data to the entity-based staging table that contains the CountryRegion entity.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead configure an entity sync relationship to replicate the CountryRegion entity.<br>References:<br>https://docs.microsoft.com/en-us/sql/master-data-services/entity-sync-relationship-master-data-services",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-05T05:12:00.000Z",
        "voteCount": 3,
        "content": "I think that the question is not well put. The solution does meet the goal, its just that Entity Sync is a much better solution."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21097-exam-70-767-topic-1-question-111-discussion/",
    "body": "DRAG DROP -<br>Note: This question is part of a series of questions that present the same scenario. For your convenience, the scenario is repeated in each question.<br>Each question presents a different goal and answer choices, but the text of the scenarios is exactly the same in each question in this series.<br><img src=\"/assets/media/exam-media/02783/0016100002.png\" class=\"in-exam-image\"><br>You are developing a Master Data Management (MDM) solution for a company by using Microsoft SQL Server Integration Services (SSIS), SQL Server Master<br>Data Services (MDS), and SQL Server Data Quality Services (DQS).<br>You have an MDS model named Geography that contains the entities described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016200001.png\" class=\"in-exam-image\"><br>You define a domain-based attribute in the State entity that references the CountryRegion entity. You define another domain-based attribute in the city entity that references the State and CountryRegion entities. A single derived hierarchy named Geography supports navigation between the CountryRegion,<br>, and City levels. Subscription views exist for all entities. The subscription views have the same name as the entity on which they are based.<br><br>State -<br>You initialize each entity member. New City entity members are imported daily based on customer city values in a Customer Relationship Management (CRM) database. The CRM database is a SQL Server relational database. When new cities are imported from the CRM database, the state codes must be standardized to those already defined in the State entity.<br>In the CRM database, sales managers and assigned to countries/regions. A sales manager may be assigned to one or more countries/regions. A country/region may have one or more assigned sales managers. The CRM database contains a table named ManagerCountryRegion that stores a row for each manager- country/region relationship.<br>You create the following MDS users and map each user to an Active Directory Domain Services (AD DS) user account: User1, User2, and User3. Both User1 and<br>User2 belong to the Explorer functional area.<br>Users must be able to complete the tasks described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016200002.png\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0016200003.png\" class=\"in-exam-image\"><br>You need to perform the initial data loading process of the DQS knowledge base, based on the CountryRegion Entity in the MDS model.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/sql/data-quality-services/perform-knowledge-discovery",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-21T09:27:00.000Z",
        "voteCount": 9,
        "content": "Use the Open Knowledge Base feature\nSelect Activity / Knowledge Discovery\nUse the MDS database and select the view mdm.CountryRegion"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11217-exam-70-767-topic-1-question-112-discussion/",
    "body": "DRAG DROP -<br>Note: This question is part of a series of questions that present the same scenario. For your convenience, the scenario is repeated in each question.<br>Each question presents a different goal and answer choices, but the text of the scenarios is exactly the same in each question in this series.<br><img src=\"/assets/media/exam-media/02783/0016300001.png\" class=\"in-exam-image\"><br>You are developing a Master Data Management (MDM) solution for a company by using Microsoft SQL Server Integration Services (SSIS), SQL Server Master<br>Data Services (MDS), and SQL Server Data Quality Services (DQS).<br>You have an MDS model named Geography that contains the entities described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016300002.png\" class=\"in-exam-image\"><br>You define a domain-based attribute in the State entity that references the CountryRegion entity. You define another domain-based attribute in the city entity that references the State and CountryRegion entities. A single derived hierarchy named Geography supports navigation between the CountryRegion,<br>, and City levels. Subscription views exist for all entities. The subscription views have the same name as the entity on which they are based.<br><br>State -<br>You initialize each entity member. New City entity members are imported daily based on customer city values in a Customer Relationship Management (CRM) database. The CRM database is a SQL Server relational database. When new cities are imported from the CRM database, the state codes must be standardized to those already defined in the State entity.<br>In the CRM database, sales managers and assigned to countries/regions. A sales manager may be assigned to one or more countries/regions. A country/region may have one or more assigned sales managers. The CRM database contains a table named ManagerCountryRegion that stores a row for each manager- country/region relationship.<br>You create the following MDS users and map each user to an Active Directory Domain Services (AD DS) user account: User1, User2, and User3. Both User1 and<br>User2 belong to the Explorer functional area.<br>Users must be able to complete the tasks described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016400001.png\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0016400002.png\" class=\"in-exam-image\"><br>You need to add a top level to the Geography hierarchy to navigate from manager to CountryRegion.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Derived hierarchies come from the relationships between entities in a model. These are domain-based attribute relationships.<br>A domain-based attribute contains values that are populated by members from an entity and can be used as attribute values for other entities.<br>References:<br>https://docs.microsoft.com/en-us/sql/master-data-services/master-data-services-overview-mds",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-03T22:41:00.000Z",
        "voteCount": 10,
        "content": "1. Add the Manager as the first level of the Geography attribute\n2. Add a new entity ManagerCountryRegion based on the ManagerCountryRegion table\n3.Create a domain based attr on the Manager entity referencing the ManagerCountryRegion entity"
      },
      {
        "date": "2020-01-01T15:13:00.000Z",
        "voteCount": 5,
        "content": "I don't quite understand this solution."
      },
      {
        "date": "2020-10-08T22:03:00.000Z",
        "voteCount": 1,
        "content": "-add City attribute to Manager entity, so manager can be in the Manager-City-CountryRegion hierarchy\n-change the Geography hierarchy - add Manager"
      },
      {
        "date": "2020-08-06T02:15:00.000Z",
        "voteCount": 3,
        "content": "I think that the third action should be : create two domain-based attributes on the ManagercountryRegion , one referencing the manager entity and another referencing country region entity"
      },
      {
        "date": "2020-08-31T13:22:00.000Z",
        "voteCount": 2,
        "content": "I agree. \nHere is the full question: https://www.certbus.com/online-pdf/70-767.pdf"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27500-exam-70-767-topic-1-question-113-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. For your convenience, the scenario is repeated in each question.<br>Each question presents a different goal and answer choices, but the text of the scenarios is exactly the same in each question in this series.<br><img src=\"/assets/media/exam-media/02783/0016500001.png\" class=\"in-exam-image\"><br>You are developing a Master Data Management (MDM) solution for a company by using Microsoft SQL Server Integration Services (SSIS), SQL Server Master<br>Data Services (MDS), and SQL Server Data Quality Services (DQS).<br>You have an MDS model named Geography that contains the entities described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016500002.png\" class=\"in-exam-image\"><br>You define a domain-based attribute in the State entity that references the CountryRegion entity. You define another domain-based attribute in the city entity that references the State and CountryRegion entities. A single derived hierarchy named Geography supports navigation between the CountryRegion,<br>, and City levels. Subscription views exist for all entities. The subscription views have the same name as the entity on which they are based.<br><br>State -<br>You initialize each entity member. New City entity members are imported daily based on customer city values in a Customer Relationship Management (CRM) database. The CRM database is a SQL Server relational database. When new cities are imported from the CRM database, the state codes must be standardized to those already defined in the State entity.<br>In the CRM database, sales managers and assigned to countries/regions. A sales manager may be assigned to one or more countries/regions. A country/region may have one or more assigned sales managers. The CRM database contains a table named ManagerCountryRegion that stores a row for each manager- country/region relationship.<br>You create the following MDS users and map each user to an Active Directory Domain Services (AD DS) user account: User1, User2, and User3. Both User1 and<br>User2 belong to the Explorer functional area.<br>Users must be able to complete the tasks described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016600001.png\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0016600002.png\" class=\"in-exam-image\"><br>You need to complete the development of the SSIS data flow to load cities into the City staging table.<br><img src=\"/assets/media/exam-media/02783/0016700001.png\" class=\"in-exam-image\"><br>Which transformation type should you use for Component 3?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport Column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCDC Splitter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConditional Split",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSlowly Changing Dimension (SCD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup"
    ],
    "answer": "A",
    "answerDescription": "\"When new cities are imported from the CRM database, the state codes must be standardized to those already defined in the State entity.\"<br>The Import Column transformation reads data from files and adds the data to columns in a data flow. This transformation has one input, one output, and one error output.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/import-column-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-03T00:01:00.000Z",
        "voteCount": 1,
        "content": "Import Column is used to load files / images from folder into the database e.g. text file has image path c:\\anette\\bilel\\image.png &amp; we need to load into database in the binary form than we use that Import Column. on the other hand, Lookup can't be used as we need second source inside lookup for comparison otherwise it shows red sign &amp; only Conditional Split make sense &amp; it also has Configure Error Output setting."
      },
      {
        "date": "2020-08-06T02:29:00.000Z",
        "voteCount": 1,
        "content": "lookup"
      },
      {
        "date": "2020-08-30T03:49:00.000Z",
        "voteCount": 3,
        "content": "Indeed I think A is correct. Import Column transformation reads data from files and adds the data to columns in a data flow. Using this transformation, a package can add text and images stored in separate files to a data flow."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27501-exam-70-767-topic-1-question-114-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. For your convenience, the scenario is repeated in each question.<br>Each question presents a different goal and answer choices, but the text of the scenarios is exactly the same in each question in this series.<br><img src=\"/assets/media/exam-media/02783/0016800001.png\" class=\"in-exam-image\"><br>You are developing a Master Data Management (MDM) solution for a company by using Microsoft SQL Server Integration Services (SSIS), SQL Server Master<br>Data Services (MDS), and SQL Server Data Quality Services (DQS).<br>You have an MDS model named Geography that contains the entities described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016800002.png\" class=\"in-exam-image\"><br>You define a domain-based attribute in the State entity that references the CountryRegion entity. You define another domain-based attribute in the city entity that references the State and CountryRegion entities. A single derived hierarchy named Geography supports navigation between the CountryRegion,<br>, and City levels. Subscription views exist for all entities. The subscription views have the same name as the entity on which they are based.<br><br>State -<br>You initialize each entity member. New City entity members are imported daily based on customer city values in a Customer Relationship Management (CRM) database. The CRM database is a SQL Server relational database. When new cities are imported from the CRM database, the state codes must be standardized to those already defined in the State entity.<br>In the CRM database, sales managers and assigned to countries/regions. A sales manager may be assigned to one or more countries/regions. A country/region may have one or more assigned sales managers. The CRM database contains a table named ManagerCountryRegion that stores a row for each manager- country/region relationship.<br>You create the following MDS users and map each user to an Active Directory Domain Services (AD DS) user account: User1, User2, and User3. Both User1 and<br>User2 belong to the Explorer functional area.<br>Users must be able to complete the tasks described in the following table.<br><img src=\"/assets/media/exam-media/02783/0016900001.png\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/02783/0016900002.png\" class=\"in-exam-image\"><br>You need to configure the City entity to meet User1 and User2 access requirements.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Approval Required property to True for the City entity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the transaction Log Type Property to member for the City entity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish a business rule for the City entity which starts a workflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the value of the Enable Change tracking property to true for the Code attribute of the City entity."
    ],
    "answer": "C",
    "answerDescription": "Changes made by a business rule bypass the approval.<br>In Master Data Services, a business rule is a rule that you use to ensure the quality and accuracy of your master data. You can use a business rule to automatically update data, to send email, or to start a business process or workflow.<br>In Master Data Services, you can use change tracking groups to take action when an attribute value changes. Use change tracking when you don't know what the new value will be, but instead want to know if any change occurred.<br>Incorrect Answers:<br>A: In Master Data Services, the administrator can set an entity to Approval Required. All the changes on this entity would require one of the entity administrators to review and approve the changes.<br>References:<br>https://docs.microsoft.com/en-us/sql/master-data-services/business-rules-master-data-services https://docs.microsoft.com/en-us/sql/master-data-services/approval-required-master-data-services",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-06T02:34:00.000Z",
        "voteCount": 6,
        "content": "According to this article : https://docs.microsoft.com/en-us/sql/master-data-services/approval-required-master-data-services?view=sql-server-ver15  ,the response should be A"
      },
      {
        "date": "2020-08-30T03:52:00.000Z",
        "voteCount": 3,
        "content": "But see the explanation in answer why A is not correct. I think C is more correct"
      },
      {
        "date": "2020-12-10T06:52:00.000Z",
        "voteCount": 2,
        "content": "Business rule does not provide approval functionality. A is correct answer"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29264-exam-70-767-topic-1-question-119-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are implementing a Microsoft SQL Server data warehouse with a multi-dimensional data model.<br>Business users observe that the value displayed for the profit margin is the average profit margin across all products. The profit margin must be calculated per product.<br>You need to modify the existing model without changing the overall structure of the data model.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstar schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsnowflake schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconformed dimension",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tslowly changing dimension (SCD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfact table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsemi-additive measure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnon-additive measure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdimension table reference relationship"
    ],
    "answer": "G",
    "answerDescription": "There are three types of facts:<br>Non-Additive: Non-additive facts are facts that cannot be summed up for any of the dimensions present in the fact table.<br>Additive: Additive facts are facts that can be summed up through all of the dimensions in the fact table.<br>Semi-Additive: Semi-additive facts are facts that can be summed up for some of the dimensions in the fact table, but not the others.<br>References:<br>https://www.1keydata.com/datawarehousing/fact-table-types.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-30T02:18:00.000Z",
        "voteCount": 1,
        "content": "Should that be semi-additive?"
      },
      {
        "date": "2020-09-26T19:34:00.000Z",
        "voteCount": 3,
        "content": "nope, 1 product has a margin and it has been calculated somehow and we should keep it like that \"The profit margin must be calculated per product.\", so we need to \"turn off\" any aggregations on this column -&gt; non-additive"
      },
      {
        "date": "2020-08-22T05:49:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why the answer is Non-additive. Actually we have to calculate the profit margin per product correct."
      },
      {
        "date": "2020-08-30T07:25:00.000Z",
        "voteCount": 1,
        "content": "Yes, it looks like it must be adaptive since Additive facts are facts that can be summed up through all of the dimensions in the fact table and we must calculate margin per product for all products. But since Adaptive is not an answer the next best option is F (semi-adaptive). \nAny hint why it is non-adaptive??"
      },
      {
        "date": "2020-08-31T14:28:00.000Z",
        "voteCount": 4,
        "content": "after reading the link in explanation I agree to non-additive."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27296-exam-70-767-topic-1-question-120-discussion/",
    "body": "HOTSPOT -<br>You are using Microsoft SQL Server data Tools (SSID) to create a SQL Server Information Services (SSIS) package. The package contains a single Data Flow task as shown in the Data Flow exhibit. (Click the Data Flow tab.)<br><img src=\"/assets/media/exam-media/02783/0017700001.png\" class=\"in-exam-image\"><br>Project properties are provided in the Project Properties exhibit. (Click the Project Properties tab.)<br><img src=\"/assets/media/exam-media/02783/0017800001.jpg\" class=\"in-exam-image\"><br>The data flow imports a remote Microsoft Excel file if it exists. You have all of the appropriate permissions to access the file.<br>The package fails to complete. The error message is shown in the Execution Results exhibit. (Click the Execution Results tab.)<br><br>Execution Results -<br><img src=\"/assets/media/exam-media/02783/0017900001.png\" class=\"in-exam-image\"><br>You need to resolve the issue and ensure that you can run the package on a scheduled basis.<br>For each of the following statements, select Yes if the statement is true. otherwise, select No.<br>Hot Area:<br><img src=\"/assets/media/exam-media/02783/0017900002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/02783/0018000001.png\" class=\"in-exam-image\">",
    "answerDescription": "These steps should be tried in the following order until the issue is resolved.<br>1. Make sure the Jet Data Manager and Excel have the same bit-rate. If Excel is 32-bit, you will need to install the 32-bit version of the Jet Data Manager.  If<br>Excel is 64-bit, you will need to install the 64-bit version of the Jet Data Manager.<br>2. Install Microsoft Access Database Engine 2010 Redistributable.  These components can be found here: Microsoft Access Database Engine 2010<br><br>Redistributable -<br>Ensure that you explicitly run the executable as an administrator<br>3. Install 2007 Office System Driver: Data Connectivity Components. These components can be automatically downloaded here: 2007 Office System Driver: Data<br><br>Connectivity Components -<br>Ensure that you explicitly run the executable as an administrator<br>References:<br>https://support.jetglobal.com/hc/en-us/articles/219401847-Error-The-Microsoft-ACE-OLEDB-12-0-provider-is-not-registered",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-09T10:24:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct, read the link: https://support.jetglobal.com/hc/en-us/articles/219401847-Error-The-Microsoft-ACE-OLEDB-12-0-provider-is-not-registered \nMake sure the Jet Data Manager and Excel have the same bit-rate. If Excel is 32-bit, you will need to install the 32-bit version of the Jet Data Manager.  If Excel is 64-bit, you will need to install the 64-bit version of the Jet Data Manager.\n\nInstall Microsoft Access Database Engine 2010 Redistributable.  These components can be found here: Microsoft Access Database Engine 2010 Redistributable\nEnsure that you explicitly run the executable as an administrator\nInstall 2007 Office System Driver: Data Connectivity Components. These components can be automatically downloaded here: 2007 Office System Driver: Data Connectivity Components \nEnsure that you explicitly run the executable as an administrator"
      },
      {
        "date": "2020-12-14T16:09:00.000Z",
        "voteCount": 1,
        "content": "When you install driver, it must be done with Administrator right\nBut the answer here is running SSIS with administrator right, which has nothing to do with the installation. \nI think the correct answer is No, Yes, No"
      },
      {
        "date": "2020-10-09T00:26:00.000Z",
        "voteCount": 1,
        "content": "the error description clearly says that the problem is with connection to Excel - \"MicrosoftJet.Oledb4 provider is not registered\"\n-MicrosoftJet.Oledb4 provider is only for 32 bit systems (https://social.msdn.microsoft.com/Forums/ru-RU/d5b29496-d6a1-4ecf-b1a4-5550d80b84b6/microsoftjetoledb40-32bit-and-64bit?forum=adodotnetdataproviders)\n- I am not 100% shure if changing the package to 32 bit can help, I think in that case its better to use 64 bit driver \nSo my resume:\n-No\n-rather No than Yes\n-No"
      },
      {
        "date": "2020-08-30T08:33:00.000Z",
        "voteCount": 1,
        "content": "I think change the value from 32 bit to 64 is ok. So, its not enough to relaunch ssdt with administrative rights, if we are in 32 bit the error will still occur. \nAs conclusion answers are:\nNo\nYes\nNo"
      },
      {
        "date": "2020-08-22T05:54:00.000Z",
        "voteCount": 2,
        "content": "As per my experience in SSIS, the answer must be Yea for change value from 64 bit to 32 bit. Rest 2 options can be No. The package can run without admin rights and get the data and load to the destination table."
      },
      {
        "date": "2020-08-04T07:12:00.000Z",
        "voteCount": 1,
        "content": "Why relaunch SSDT with administrative rights? You must install the components with administrative rights."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25546-exam-70-767-topic-1-question-122-discussion/",
    "body": "You are developing a Microsoft SQL Server Integration Service (SSIS) package. You enable the SSIS log provider for the Windows event log. You configure the package to use the ScriptTaskLogEntry event. You create a custom Script task.<br>You need to ensure that when the script completes, it writes the execution status to the event log on the server that hosts SSIS.<br>Which code segment should you add to the Script task?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSystem.Diagnostics.EventLog.WriteEntry(\"SSIS\", \"Script executed with return result\" + ScriptResults.Success, System.Diagnostics.EventLogEntrytype.Information)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSystem.Diagnostics.EventLog.WriteEntry(\"SSIS\", \"Script executed with return result\" + Dts.TaskResult, System.Diagnostics.EventLogEntryType.Information)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDts.TaskResult = (int)ScriptResults.Failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDts.Events.FireInformation(0, \"SSIS\", \"Script executed with return result\" + Dts.TaskResult, String.Empty, 0)"
    ],
    "answer": "B",
    "answerDescription": "References:<br>https://support.microsoft.com/en-gb/help/906560/how-to-write-information-to-the-application-event-log-by-using-a-scrip",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-18T18:11:00.000Z",
        "voteCount": 5,
        "content": "answer is correct"
      },
      {
        "date": "2020-12-15T12:09:00.000Z",
        "voteCount": 1,
        "content": "which  answer"
      },
      {
        "date": "2020-07-13T00:02:00.000Z",
        "voteCount": 2,
        "content": "I think correct answer is C"
      },
      {
        "date": "2020-07-27T08:49:00.000Z",
        "voteCount": 7,
        "content": "why?? could you be more specific?\n\"You need to ensure that when the script completes, it writes the execution status to the event log on the server that hosts SSIS\". \nif you use Dts.TaskResult = (int)ScriptResults.Failure the task will always return failure, not the real execution status."
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/microsoft/view/42697-exam-70-767-topic-1-question-124-discussion/",
    "body": "HOTSPOT -<br>You have a database that includes a table named dbo.Sales. The table contains two billion rows. You created the table by running the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0018400001.png\" class=\"in-exam-image\"><br>You run the following queries against the dbo.Sales. All the queries perform poorly.<br><img src=\"/assets/media/exam-media/02783/0018500001.png\" class=\"in-exam-image\"><br>The ETL process that populates the table uses bulk insert to load 10 million rows each day. The process currently takes six hours to load the records.<br>The value of the Refund column is equal to 1 for only 0.01 percent of the rows in the table. For all other rows, the value of the Refund column is equal to 0.<br>You need to maximize the performance of queries and the ETL process.<br>Which index type should you use for each query? To answer, select the appropriate index types in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "Query1: Nonclustered Index -<br>The query include a date range.<br>If you have included columns in your index, then the leaf level page of your non-clustered index contains the columns as defined in the nonclustered index the clustering key column(s) all those additional columns as defined in your INCLUDE statement.<br>Query2: Clustered columnstore index<br>Columnstore index is a new type of index introduced in SQL Server 2012. It is a column-based non-clustered index geared toward increasing query performance for workloads that involve large amounts of data, typically found in data warehouse fact tables.<br>Query3: Filtered nonclustered index<br>* When a column only has a small number of relevant values for queries, you can create a filtered index on the subset of values. For example, when the values in a column are mostly NULL and the query selects only from the non-NULL values, you can create a filtered index for the non-NULL data rows. The resulting index will be smaller and cost less to maintain than a full-table nonclustered index defined on the same key columns.<br>When a table has heterogeneous data rows, you can create a filtered index for one or more categories of data. This can improve the performance of queries on these data rows by narrowing the focus of a query to a specific area of the table. Again, the resulting index will be smaller and cost less to maintain than a full- table nonclustered index.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/indexes/create-filtered-indexes https://logicalread.com/sql-server-columnstore-index-w02/#.XRo06egzaUk",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-25T00:31:00.000Z",
        "voteCount": 2,
        "content": "We already have a clustered index because of the SalesID Primary Key. So how is it possible to have a clustered columnstore index for Query2?? I think it should be Non Clustered Columnstore index for Query2."
      },
      {
        "date": "2021-01-18T11:10:00.000Z",
        "voteCount": 1,
        "content": "Query 1 - see explanation\n\nQuery 2 - Columnstore\n\t- So to make SELECT COUNT(*) queries fast, here\u2019s what to do ... and put a columnstore index on the table.\n\t\nQuery 3 - Filtered nonclustered index \n        - The benefit of using a filtered index is apparent in the scenario when you only select a subset of records from a huge table\n\t- Refund is only 0.01 percent"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14667-exam-70-767-topic-1-question-126-discussion/",
    "body": "HOTSPOT -<br>You manage the data warehouse for a large retail company. The company has many store locations. Each location runs their own independent sales systems.<br>They report daily sales and receipt information to a central system every night. You plan to load data from all locations into a fact table  named fact.Sales by using<br>SQL Server Integration Services (SSIS) packages. You create the following Transact-SQL statement:<br><img src=\"/assets/media/exam-media/02783/0018800001.png\" class=\"in-exam-image\"><br>You need to select the columns for the primary key and clustered column key. The keys must meet the following requirements:<br>\u2711 Prevent duplicate rows from being entered by a single system.<br>\u2711 Allow for point lookups of a single sale.<br>\u2711 Minimize storage requirements for nonclustered indexes.<br>\u2711 Store rows in the order that they are inserted into the table.<br>How should you configure the keys? To answer, select the appropriate column type in the answer area.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-03T02:10:00.000Z",
        "voteCount": 4,
        "content": "SalesID : primary key column\nSystemID : clustered key column\nReceiptID : clustered key column\nStoredID : clustered key column\nWe can't create Primary Key column on NOT NULL column type, as table definition didn't show NULL / NOT NULL column type, so, if we need to assume default SQL Server Setting of ANSI where Identity column will generate NOT NULL and other columns will generate NULL, therefore, we can only select SaleID as primary key, in addition, there are only two options for rest of the 3 keys so we have to choose clustered key for all 3"
      },
      {
        "date": "2021-01-24T12:52:00.000Z",
        "voteCount": 1,
        "content": "yes, I agree with RaheelCSE"
      },
      {
        "date": "2020-11-28T13:23:00.000Z",
        "voteCount": 1,
        "content": "How can be more than one clustered index in the table?"
      },
      {
        "date": "2020-12-02T08:41:00.000Z",
        "voteCount": 1,
        "content": "One clustered index on multiple columns."
      },
      {
        "date": "2020-12-02T10:24:00.000Z",
        "voteCount": 1,
        "content": "There can't be. i think the answer is:\nSalesID : primary key column\nReceiptID : clustered key column"
      },
      {
        "date": "2020-03-08T08:58:00.000Z",
        "voteCount": 2,
        "content": "Prevent duplicate rows from being entered by a single system/Allow for point lookups of a single sale\n\tStoreId+SaleId: This combo would meet the above requireptments. Each sale generates a receipt, that by law must identity unqiuley a sale transaction.\n\t\t\t\t\tA store should not have duplicate receipt id's. \n\t\t\t\t\tThe column SystemID is a red hering(personal point of view):\" The company has many store locations. Each location runs their own independent sales systems.\"\n\t\t\t\t\tThis can mean that two separate locations use the same \"independent sales system\", thus a combo of StoreID+SystemId does not guranatee uniquness.\n\t\t\t\t\tSafest combo would be StoreId+SaleID+SystemID, but when building a compoiste key we should stick to as few columns as necesary.\nStore rows in the order that they are inserted into the table: \n\tsaleid: clustered index on it. Because it is an identity(1,1) column, oredring the table by it \n\t\t\t\tenforces the above request"
      },
      {
        "date": "2020-04-15T08:32:00.000Z",
        "voteCount": 1,
        "content": "I agree with m8rvil, the question doesn't mention composite keys and it does mention minimize storage requirements for the non-clustered indexes"
      },
      {
        "date": "2020-03-03T23:27:00.000Z",
        "voteCount": 4,
        "content": "SalesID : primary key column\nSystemID : clustered key column\nReceiptID : clustered key column\nStoredID : primary key column"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8012-exam-70-767-topic-1-question-129-discussion/",
    "body": "HOTSPOT -<br>You are the administrator of a database that hosts tables for a data warehouse.<br>The table named Fact1 has data from the start of calendar year 2011 through the end of 2017. The table contains at least 20 million rows of data for each year.<br>You create the table by running the following Transact-SQL statement:<br>CREATE PARTITION FUNCTION PartitionFunc-Fact1(SMALLINT)<br>AS RANGE LIFT VALUES(2012, 2013, 2014, 2015)<br>You need to modify the partition function so that rows for each calendar year are in a separate partition. You must also move all data prior to 2014 to another table named Fact1_old.<br>How should you complete the Transact-SQL statement? To answer, select the appropriate Transact-SQL segment in the dialog box in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br>",
    "options": [],
    "answer": "Explanation",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/alter-partition-function-transact-sql https://docs.microsoft.com/en-us/sql/t-sql/statements/alter-table-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-12T21:42:00.000Z",
        "voteCount": 11,
        "content": "1.Merge\n2.2013\n3.Switch\n4.Split\n5.2016"
      },
      {
        "date": "2020-10-27T15:04:00.000Z",
        "voteCount": 1,
        "content": "In my opinion you should to merge year 2012 because we want to move \"move all data prior to 2014\" so after that you will get 2011,2012 and 2013 in one (first) partition. In your case you would be get:\n1st partition: 2011, 2012\n2nd partition: 2013, 2014\n3rd partition: 2015\n4th partition: 2016, 2017\nIt doesn't make sense.\nIn my proposal it look like:\n1st partition: 2011, 2012, 2013\n2nd partition: 2014\n3rd partition: 2015\n4th partition: 2016, 2017"
      },
      {
        "date": "2020-11-09T11:20:00.000Z",
        "voteCount": 1,
        "content": "100% correct. This question is tricky due to the LEFT range specification and (SMALLINT) instead of (DATE)  \nALTER PARTITION FUNCTION PartitionFunc_FACT ()  MERGE  RANGE (2013)  effectively removes the left boundary of 2013 making a complete range of 2012 and 2013 records.\n\nNow, let's SWITCH all those old records to the fact_old table\nALTER TABLE Fact1 SWITCH PARTITION 1 TO Fact_old\n\nThis leaves us with  2 Ranges: 2014 and 2015-2017\n Let's create one more LEFT boundary  of 2016. This will effectively SPLIT that last range and create the needed partitions (on partition for EACH calendar year):\nALTER PARTITION FUNCTION PartitionFunc_Fact1() SPLIT RANGE (2016)"
      },
      {
        "date": "2020-05-17T23:57:00.000Z",
        "voteCount": 7,
        "content": "Correct in my opinion: \n1. Merge to get all from 2012 to 2014 (via Merge 2013) in Parition 1. \n2. Switch this parition to Fact_old\n3. Split the rest (2015 to 2017)with 2016 into three partitions, 2015, 2016, 2017."
      },
      {
        "date": "2020-09-26T21:56:00.000Z",
        "voteCount": 1,
        "content": "Images from this question - https://vceguide.com/how-should-you-complete-the-transact-sql-statement-50/"
      },
      {
        "date": "2020-02-09T04:28:00.000Z",
        "voteCount": 1,
        "content": "Is that the correct answer?"
      },
      {
        "date": "2020-03-03T02:38:00.000Z",
        "voteCount": 7,
        "content": "I think the answer is:\n1. Split\n2. 2014\n3. Switch\n4. Split\n5. 2015"
      },
      {
        "date": "2020-08-05T12:25:00.000Z",
        "voteCount": 2,
        "content": "I've read  this answer in other dumps but I think it's not correct because in order to switch partition 1 to fact_old you have to merge in partition 1 all data prior to 2014, so the first option should be merge 2013 (to get all data from partitions 2012 to 2014). \nI agree with Andrescarnederes and Dieter"
      },
      {
        "date": "2020-02-06T14:28:00.000Z",
        "voteCount": 2,
        "content": "1. SET \n2. 2012\n3. Merge \n4. Split \n5. 2016"
      },
      {
        "date": "2020-08-05T12:15:00.000Z",
        "voteCount": 1,
        "content": "set is not a possible option in the alter partition function, so this answer is not correct.\n\nALTER PARTITION FUNCTION partition_function_name()  {   \n    SPLIT RANGE ( boundary_value )  \n  | MERGE RANGE ( boundary_value )   \n} [ ; ] \nhttps://docs.microsoft.com/es-es/sql/t-sql/statements/alter-partition-function-transact-sql?view=sql-server-ver15"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11913-exam-70-767-topic-1-question-130-discussion/",
    "body": "Note: This question is part of a series of questions that use the same or similar answer choices. An answer choice may be correct for more than one question in the series. Each question is independent of the other questions in this series. Information and details provided in a question apply only to that question.<br>You are a database administrator for an e-commerce company that runs an online store. The company has the databases described in the following table.<br><img src=\"/assets/media/exam-media/02783/0019100001.png\" class=\"in-exam-image\"><br>Each month, eight partner companies send your company comma-separated values (CSV) files that contain their sales data.<br>You need to create a Microsoft SQL Server Integration Services (SSIS) package to load the CSV files in parallel to a single table in DB2. You must minimize the number of transformation steps that are required.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge Join transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE statement",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnion All transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBalanced Data Distributor transformation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSequential container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"H\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tH.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForeach Loop container"
    ],
    "answer": "F",
    "answerDescription": "The Balanced Data Distributor (BDD) transformation takes advantage of concurrent processing capability of modern CPUs. It distributes buffers of incoming rows uniformly across outputs on separate threads. By using separate threads for each output path, the BDD component improves the performance of an SSIS package on multi-core or multi-processor machines.<br>References:<br>https://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/balanced-data-distributor-transformation",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-13T14:18:00.000Z",
        "voteCount": 9,
        "content": "E. \nSince the scenario is to load the CSV files from eight companies in parallel to a single table in DB2."
      },
      {
        "date": "2020-11-19T14:06:00.000Z",
        "voteCount": 2,
        "content": "Union All:\nThe Union All transformation combines multiple inputs into one output. For example, the outputs from five different Flat File sources can be inputs to the Union All transformation and combined into one output.\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/union-all-transformation?view=sql-server-ver15\n\n if you are importing data to one destination table, BDD may be useless. However, if you are importing data to a staging database, it may improve the data import performance.\nhttps://www.sqlshack.com/ssis-balanced-data-distributor-overview/"
      },
      {
        "date": "2020-06-06T07:48:00.000Z",
        "voteCount": 3,
        "content": "Correct is E"
      },
      {
        "date": "2020-04-16T06:24:00.000Z",
        "voteCount": 3,
        "content": "Balanced Data Distributor spreads data over multiple destinations, quite the opposite of what we're trying to achieve here. I would say E is correct"
      },
      {
        "date": "2020-04-19T07:18:00.000Z",
        "voteCount": 2,
        "content": "MS states you need a Union all after BDD to do send the results to a single database"
      }
    ],
    "examNameCode": "70-767",
    "topicNumber": "1"
  }
]