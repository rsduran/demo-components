[
  {
    "topic": 3,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21838-exam-ai-100-topic-3-question-1-discussion/",
    "body": "You need to build an API pipeline that analyzes streaming data. The pipeline will perform the following:<br>\u2711 Visual text recognition<br>\u2711 Audio transcription<br>\u2711 Sentiment analysis<br>\u2711 Face detection<br>Which Azure Cognitive Services should you use in the pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustom Speech Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFace API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVideo Indexer"
    ],
    "answer": "D",
    "answerDescription": "Azure Video Indexer is a cloud application built on Azure Media Analytics, Azure Search, Cognitive Services (such as the Face API, Microsoft Translator, the<br>Computer Vision API, and Custom Speech Service). It enables you to extract the insights from your videos using Video Indexer video and audio models described below:<br>\u2711 Visual text recognition (OCR): Extracts text that is visually displayed in the video.<br>\u2711 Audio transcription: Converts speech to text in 12 languages and allows extensions.<br>\u2711 Sentiment analysis: Identifies positive, negative, and neutral sentiments from speech and visual text.<br>\u2711 Face detection: Detects and groups faces appearing in the video.<br>References:<br>https://docs.microsoft.com/en-us/azure/media-services/video-indexer/video-indexer-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-01T14:08:00.000Z",
        "voteCount": 9,
        "content": "The Question should be \"...which Azure Cognitive Service (without the plural s), otherwise it could be misunderstood that multiple solutions are right."
      },
      {
        "date": "2021-01-16T20:32:00.000Z",
        "voteCount": 2,
        "content": "radio button!!!... can you select more than 1 radio buttons!!!!"
      },
      {
        "date": "2023-06-20T08:23:00.000Z",
        "voteCount": 1,
        "content": "Based on the requirements listed, the appropriate Azure Cognitive Services for the API pipeline would be:\n\nFace API: for face detection\nText Analytics: for sentiment analysis\nTherefore, the correct options are B. Face API and C. Text Analytics."
      },
      {
        "date": "2021-05-15T19:32:00.000Z",
        "voteCount": 2,
        "content": "This question did appear in my exam that I took a few days ago"
      },
      {
        "date": "2021-02-24T20:44:00.000Z",
        "voteCount": 2,
        "content": "As of June 2020 the video indexer is not available for customers. So a high chance that this question will not appear in the exam ;)"
      },
      {
        "date": "2021-06-19T10:20:00.000Z",
        "voteCount": 1,
        "content": "I think, the question might still come up, since they have only changed the name of the service. https://docs.microsoft.com/en-us/azure/azure-video-analyzer/video-analyzer-for-media-docs/video-indexer-overview"
      },
      {
        "date": "2021-05-28T22:14:00.000Z",
        "voteCount": 1,
        "content": "someone took an exam 1 week ago and this question is in the exam as per him, so you're incorrect based on the latest developments."
      },
      {
        "date": "2020-12-06T01:09:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/10050-exam-ai-100-topic-3-question-2-discussion/",
    "body": "You design an AI solution that uses an Azure Stream Analytics job to process data from an Azure IoT hub. The IoT hub receives time series data from thousands of IoT devices at a factory.<br>The job outputs millions of messages per second. Different applications consume the messages as they are available. The messages must be purged.<br>You need to choose an output type for the job.<br>What is the best output type to achieve the goal? More than one answer choice may achieve the goal.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB"
    ],
    "answer": "D",
    "answerDescription": "Stream Analytics can target Azure Cosmos DB for JSON output, enabling data archiving and low-latency queries on unstructured JSON data.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-documentdb-output",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-11T02:04:00.000Z",
        "voteCount": 21,
        "content": "I think Event Hubs may be th best output type  \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-solution-patterns#incorporate-real-time-insights-into-your-application-through-data-stores"
      },
      {
        "date": "2020-05-04T04:41:00.000Z",
        "voteCount": 5,
        "content": "Yes we can implement the auto purging with Cosmos DB TTL feature however when it comes to outputting million message per second, I think Event hub would be best tool, it\u2019s designed for this type of scenario. Event hub also comes with retention period of 24hr by default which can be extended up to 7 days so auto purging is out of the box"
      },
      {
        "date": "2023-06-20T08:25:00.000Z",
        "voteCount": 1,
        "content": "To achieve the goal of processing and purging millions of messages per second from an Azure Stream Analytics job, the best output type would be Azure Event Hubs (option A) or Azure Blob storage (option C). Both options can handle high throughput and scale to accommodate the volume of messages."
      },
      {
        "date": "2021-08-23T07:09:00.000Z",
        "voteCount": 1,
        "content": "Event Hub"
      },
      {
        "date": "2021-02-18T14:52:00.000Z",
        "voteCount": 1,
        "content": "It does state \"more than one answer choice may achieve the goal.\" So A &amp; D are available outputs from Stream and handle the volume\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/event-hubs-output"
      },
      {
        "date": "2020-10-05T15:34:00.000Z",
        "voteCount": 4,
        "content": "It is definitely A."
      },
      {
        "date": "2020-09-15T10:59:00.000Z",
        "voteCount": 2,
        "content": "Event Hub handles millions of messages so I think A. should be the right answer"
      },
      {
        "date": "2020-06-05T18:25:00.000Z",
        "voteCount": 3,
        "content": "I think the right answer is A.\nOne use of an event hub as output is when the output of a Stream Analytics job becomes the input of another streaming job.\nDiffernet applications consume the messages and must delete the messages.\nreference: I think the right answer is A.\nOne use of an event hub as output is when the output of a Stream Analytics job becomes the input of another streaming job.\nDiffernet applications consume the messages and must delete the messages.\nreference: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs"
      },
      {
        "date": "2020-06-01T14:14:00.000Z",
        "voteCount": 1,
        "content": "Keep in mind that more than one answer could achieve the goal, so A and D should be the only possible solutions."
      },
      {
        "date": "2021-02-17T18:35:00.000Z",
        "voteCount": 1,
        "content": "But the question also says, \"What is THE BEST output type to achieve the goal?\". I think this is a radio button answer.\nEvent Hubs is the best choice... but the question doesn't give much room to determine why one should be better than the other (cost / customization / skills required)."
      },
      {
        "date": "2020-05-17T02:52:00.000Z",
        "voteCount": 3,
        "content": "Agree for Event Hubs (A). The question is generically talking about \"messages\", not JSON outputs. Furthermore, on Event Hubs we have the concept of message retention (\"messages must be purged\")\nhttps://docs.microsoft.com/it-it/azure/event-hubs/event-hubs-quotas"
      },
      {
        "date": "2020-03-23T11:36:00.000Z",
        "voteCount": 1,
        "content": "AzureStream Analytics can target Azure Cosmos DB for JSON output,enabling data archiving and low-latency\nqueries on unstructured JSON data.This document covers some best practices for implementing this\nconfiguration.\nIf you're unfamiliar with Azure Cosmos DB, seetheAzure Cosmos DB documentation to get started.\nAt this time, Stream Analytics supports connection to Azure Cosmos DB only through the SQL API. Other Azure Cosmos DB\nAPIs are not yet supported. If you point Stream Analytics to Azure Cosmos DB accounts created with other APIs, the data\nmight not be properly stored.\nfull link: https://opdhsblobprod01.blob.core.windows.net/contents/4a6d75bb3af747de838e6ccc97c5d978/9d8cfa97be0ad7954044efc08765a162?sv=2015-04-05&amp;sr=b&amp;si=ReadPolicy&amp;sig=fvLWtxfiZawJ9WgdkV5137zeBVECogRp9XAMAu34XA8%3D&amp;st=2020-03-23T19%3A21%3A05Z&amp;se=2020-03-24T19%3A31%3A05Z#page=156&amp;zoom=100,38,38\nso, the answer is D"
      },
      {
        "date": "2019-12-11T02:06:00.000Z",
        "voteCount": 2,
        "content": "sorry this should be the right reference link \n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-solution-patterns#incorporate-real-time-insights-into-your-application-with-event-messaging"
      },
      {
        "date": "2020-03-09T17:22:00.000Z",
        "voteCount": 5,
        "content": "I agree, it should be Event hubs. https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs"
      },
      {
        "date": "2019-12-09T12:09:00.000Z",
        "voteCount": 1,
        "content": "I believe that Azure SQL is not correct because it does not provide the automatic purging feature (TTL), which comes with Cosmos DB. Am I right?"
      },
      {
        "date": "2019-12-31T23:50:00.000Z",
        "voteCount": 1,
        "content": "I agree with Danhan. Cosmos DB support TTL and event hub do not support that"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17319-exam-ai-100-topic-3-question-3-discussion/",
    "body": "HOTSPOT -<br>You are designing an AI solution that must meet the following processing requirements:<br>\u2711 Use a parallel processing framework that supports the in-memory processing of high volumes of data.<br>\u2711 Use in-memory caching and a columnar storage engine for Apache Hive queries.<br>What should you use to meet each requirement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0013500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0013600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Apache Spark -<br>Apache Spark is a parallel processing framework that supports in-memory processing to boost the performance of big-data analytic applications. Apache Spark in<br>Azure HDInsight is the Microsoft implementation of Apache Spark in the cloud.<br><br>Box 2: Interactive Query -<br>Interactive Query provides In-memory caching and improved columnar storage engine for Hive queries.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/interactive-query/apache-interactive-query-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-23T12:24:00.000Z",
        "voteCount": 11,
        "content": "Apache Spark : An open-source, parallel-processing framework that supports in-memory processing to boost the performance of big-data analysis applications.\nApache Interactive Query: In-memory caching for interactive and faster Hive queries"
      },
      {
        "date": "2020-12-29T09:06:00.000Z",
        "voteCount": 2,
        "content": "The answer provided is correct."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5785-exam-ai-100-topic-3-question-4-discussion/",
    "body": "You need to deploy cognitive search.<br>You provision an Azure Search service.<br>What should you do next?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch by using the .NET SDK.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch by using the REST API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an index."
    ],
    "answer": "D",
    "answerDescription": "You create a data source, a skillset, and an index. These three components become part of an indexer that pulls each piece together into a single multi-phased operation.<br>Note: At the start of the pipeline, you have unstructured text or non-text content (such as image and scanned document JPEG files). Data must exist in an Azure data storage service that can be accessed by an indexer. Indexers can \"crack\" source documents to extract text from source data.<br>References:<br>https://docs.microsoft.com/en-us/azure/search/cognitive-search-tutorial-blob",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-11T02:41:00.000Z",
        "voteCount": 22,
        "content": "I think the answer is create index, see step 2\n https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search#how-to-use-azure-cognitive-search"
      },
      {
        "date": "2023-06-20T11:42:00.000Z",
        "voteCount": 1,
        "content": "The correct next step after provisioning an Azure Search service would be to\nD. Create an index."
      },
      {
        "date": "2021-05-26T05:18:00.000Z",
        "voteCount": 1,
        "content": "You create a search index before uploading the content. \nGiven answer is correct!\n\nhttps://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search#how-to-use-azure-cognitive-search"
      },
      {
        "date": "2020-12-29T09:09:00.000Z",
        "voteCount": 2,
        "content": "In this article, the data is loaded first then the index is created.\n\nhttps://docs.microsoft.com/en-us/azure/search/search-get-started-portal"
      },
      {
        "date": "2021-02-10T16:10:00.000Z",
        "voteCount": 2,
        "content": "\"In this article\", yes. In Step 3, it says: \"Typically\", index creation is a code-based exercise, completed prior to loading data. \"However\", as this tutorial indicates, the wizard can generate a basic index for any data source it can crawl.\n\nIt's a chicken and egg situation. Pity!\n\nThe wizard can generate a basic index already... so when the search service was provisioned in this case, was a basic index created? Then Load Data... else Create Index.\n\nI so hate this question now."
      },
      {
        "date": "2019-09-27T17:29:00.000Z",
        "voteCount": 1,
        "content": "The answer should be to load the data as the first step is to create a data source, than an index, than run the indexer...  https://docs.microsoft.com/en-us/azure/search/cognitive-search-tutorial-blob#3---create-the-pipeline"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47770-exam-ai-100-topic-3-question-5-discussion/",
    "body": "You need to design an application that will analyze real-time data from financial feeds.<br>The data will be ingested into Azure IoT Hub. The data must be processed as quickly as possible in the order in which it is ingested.<br>Which service should you include in the design?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Queue storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Notification Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs"
    ],
    "answer": "C",
    "answerDescription": "Stream processing can be handled by Azure Stream Analytics. Azure Stream Analytics can run perpetual queries against an unbounded stream of data. These queries consume streams of data from storage or message brokers, filter and aggregate the data based on temporal windows, and write the results to sinks such as storage, databases, or directly to reports in Power BI. Stream Analytics uses a SQL-based query language that supports temporal and geospatial constructs, and can be extended using JavaScript.<br>Incorrect Answers:<br>E: Apache Kafka is used for ingestion, not for stream processing.<br>F: Azure Event Hubs is used for ingestion, not for stream processing.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T11:44:00.000Z",
        "voteCount": 1,
        "content": "To analyze real-time data from financial feeds, ingest it into Azure IoT Hub, and process it as quickly as possible in the order of ingestion, you should include both C. Azure Stream Analytics and F. Azure Event Hubs in the design."
      },
      {
        "date": "2021-06-13T08:07:00.000Z",
        "voteCount": 1,
        "content": "I felt 'processing the data in the order it was ingested' refers to a Queue storage, or is Stream Analytics capable of handling data in a queue manner, preserving the order?"
      },
      {
        "date": "2021-11-12T10:00:00.000Z",
        "voteCount": 1,
        "content": "By default, Stream Analytics processes events by arrival time:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/event-ordering"
      },
      {
        "date": "2021-03-19T13:47:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31540-exam-ai-100-topic-3-question-6-discussion/",
    "body": "You are designing an AI solution that will provide feedback to teachers who train students over the Internet. The students will be in classrooms located in remote areas. The solution will capture video and audio data of the students in the classrooms.<br>You need to recommend Azure Cognitive Services for the AI solution to meet the following requirements:<br>\u2711 Alert teachers if a student seems angry or distracted.<br>\u2711 Identify each student in the classrooms for attendance purposes.<br>\u2711 Allow the teachers to log the text of conversations between themselves and the students.<br>Which Cognitive Services should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision, Text Analytics, and Face API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVideo Indexer, Face API, and Text Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision, Speech to Text, and Text Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics, QnA Maker, and Computer Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVideo Indexer, Speech to Text, and Face API"
    ],
    "answer": "E",
    "answerDescription": "Azure Video Indexer is a cloud application built on Azure Media Analytics, Azure Search, Cognitive Services (such as the Face API, Microsoft Translator, the<br>Computer Vision API, and Custom Speech Service). It enables you to extract the insights from your videos using Video Indexer video and audio models.<br>Face API enables you to search, identify, and match faces in your private repository of up to 1 million people.<br>The Face API now integrates emotion recognition, returning the confidence across a set of emotions for each face in the image such as anger, contempt, disgust, fear, happiness, neutral, sadness, and surprise. These emotions are understood to be cross-culturally and universally communicated with particular facial expressions.<br>Speech-to-text from Azure Speech Services, also known as speech-to-text, enables real-time transcription of audio streams into text that your applications, tools, or devices can consume, display, and take action on as command input. This service is powered by the same recognition technology that Microsoft uses for<br>Cortana and Office products, and works seamlessly with the translation and text-to-speech.<br>Incorrect Answers:<br>Computer Vision or the QnA is not required.<br>References:<br>https://docs.microsoft.com/en-us/azure/media-services/video-indexer/video-indexer-overview https://azure.microsoft.com/en-us/services/cognitive-services/face/ https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-to-text",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-20T17:30:00.000Z",
        "voteCount": 5,
        "content": "Given answer is correct\nYou need speech to text to log text of conversations, text analytics is not really needed here."
      },
      {
        "date": "2023-06-20T11:50:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure Cognitive Services to meet the requirements are:\nB. Video Indexer, Face API, and Text Analytics."
      },
      {
        "date": "2020-12-15T21:52:00.000Z",
        "voteCount": 3,
        "content": "The given answer is correct,\nI want to believe the order is not important."
      },
      {
        "date": "2021-02-10T16:35:00.000Z",
        "voteCount": 3,
        "content": "Not just because of the order, but because Text Analytics is not required. Speech-to-Text (Speech Recognition) is sufficient.\nI'm glad they didn't make it even harder by adding a choice between Video Indexer and Face API. Video Indexer can do Face Detection while Face API can do Face Detection + Face Recognition.\nWe need both here and hence B or E.\nAnd only E because of the above point on the Text part."
      },
      {
        "date": "2020-12-03T04:08:00.000Z",
        "voteCount": 1,
        "content": "am I missing something? the question says nothing about analyzing what people are saying aloud. all it says it to analyze the text.  the mood of the students can be detected by their facial expression. I don't think Speech to text is needed."
      },
      {
        "date": "2020-12-03T04:10:00.000Z",
        "voteCount": 1,
        "content": "turns out I am missing something. \"solution will capture video and audio data\""
      },
      {
        "date": "2021-05-24T06:38:00.000Z",
        "voteCount": 1,
        "content": "\"Allow the teachers to log the text of conversations between themselves and the students.\"\n\nThis is where you will need Speech to Text API"
      },
      {
        "date": "2020-09-22T03:48:00.000Z",
        "voteCount": 2,
        "content": "The correct option is actually B.\nVideo Indexer helps you analyze the student's facial reaction.\nWith Face API, you can identity students based on their gender, age and depending on the data already provided, even their names.\nText Analytics allows the teachers analyze  their conversations with the students."
      },
      {
        "date": "2021-01-17T00:28:00.000Z",
        "voteCount": 2,
        "content": "What do you mean by \"\"Analyze\"\". Question says \"\"Log\"\" thats it.  So given answer is correct. Same answer is marked in Whizlabs Practice tests."
      },
      {
        "date": "2020-09-23T23:06:00.000Z",
        "voteCount": 12,
        "content": "No. The given answer is correct because we don't need to analyse the conversations, just need to log them. Speech-to-text, also known as speech recognition, enables real-time transcription of audio streams into text."
      },
      {
        "date": "2020-09-28T04:32:00.000Z",
        "voteCount": 1,
        "content": "Do you mean, the solutions order in the given option is not important or do you mean to say FaceAPI can be used for text logging?"
      },
      {
        "date": "2020-10-23T21:53:00.000Z",
        "voteCount": 1,
        "content": "order is not important"
      },
      {
        "date": "2020-09-18T01:03:00.000Z",
        "voteCount": 2,
        "content": "I think the correct option is -B"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56186-exam-ai-100-topic-3-question-7-discussion/",
    "body": "You create an Azure Cognitive Services resource.<br>A developer needs to be able to retrieve the keys used by the resource. The solution must use the principle of least privilege.<br>What is the best role to assign to the developer?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSecurity Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSecurity Reader",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCognitive Services Contributor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCognitive Services User"
    ],
    "answer": "D",
    "answerDescription": "The Cognitive Services User lets you read and list keys of Cognitive Services.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T11:53:00.000Z",
        "voteCount": 1,
        "content": "The best role to assign to the developer in order to retrieve the keys used by the Azure Cognitive Services resource, while following the principle of least privilege, is:\n\nB. Security Reader.\n\nThe Security Reader role provides read-only access to security-related information in Azure. It allows the developer to view and retrieve the necessary security-related details, such as keys, without granting unnecessary permissions or access to modify or manage the resource."
      },
      {
        "date": "2023-06-22T08:52:00.000Z",
        "voteCount": 1,
        "content": "To retrieve the keys used by an Azure Cognitive Services resource, the best role to assign to the developer is Cognitive Services User. This role has the necessary permissions to retrieve the keys, but not to modify the resource or access billing information"
      },
      {
        "date": "2021-06-27T10:41:00.000Z",
        "voteCount": 1,
        "content": "The answer could be \"Cognitive Services Contributor\" or \"Cognitive Services User\". Since the solution must use the \"principle of least privilege\" so \"Cognitive Services User\" is right answer . (D)"
      },
      {
        "date": "2021-06-27T10:40:00.000Z",
        "voteCount": 1,
        "content": "The answer could be \"Cognitive Services Contributor\" or \"Cognitive Services User\"."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14531-exam-ai-100-topic-3-question-8-discussion/",
    "body": "Your company plans to deploy an AI solution that processes IoT data in real-time.<br>You need to recommend a solution for the planned deployment that meets the following requirements:<br>\u2711 Sustain up to 50 Mbps of events without throttling.<br>\u2711 Retain data for 60 days.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure IoT Hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Machine Learning"
    ],
    "answer": "A",
    "answerDescription": "Apache Kafka is an open-source distributed streaming platform that can be used to build real-time streaming data pipelines and applications.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T11:58:00.000Z",
        "voteCount": 1,
        "content": "Based on the requirements provided, the most suitable solution for processing IoT data in real-time and meeting the specified criteria would be option A: Apache Kafka.\n\nApache Kafka is a distributed streaming platform designed to handle high volumes of data in real-time. It is known for its scalability, fault-tolerance, and ability to process large amounts of data without throttling. It can handle event throughput of up to terabytes per second, which exceeds the specified requirement of sustaining 50 Mbps of events.\n\nFurthermore, Apache Kafka supports data retention for a specified period. By configuring the appropriate retention settings, you can retain data for 60 days, as required in the scenario."
      },
      {
        "date": "2023-06-22T08:55:00.000Z",
        "voteCount": 1,
        "content": "B. Microsoft Azure IoT Hub\n\nAzure IoT Hub is a fully managed service that enables reliable and secure bi-directional communication between IoT devices and the cloud. It is designed to handle massive amounts of IoT data and provides features such as device-to-cloud telemetry, device management, and bi-directional communication.\n\nWith Azure IoT Hub, you can ingest and process IoT data in real-time without throttling, allowing you to sustain the required 50 Mbps of events. Additionally, IoT Hub supports built-in event retention, allowing you to retain data for the desired 60-day period."
      },
      {
        "date": "2020-11-20T22:07:00.000Z",
        "voteCount": 1,
        "content": "And I think IoT hub also limited by transfer quota (300MB):\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-quotas-throttling"
      },
      {
        "date": "2020-03-24T10:44:00.000Z",
        "voteCount": 1,
        "content": "IoT hub also has this feature, that's why it can be work"
      },
      {
        "date": "2020-03-24T10:44:00.000Z",
        "voteCount": 2,
        "content": "I mean throttling feature"
      },
      {
        "date": "2020-03-24T10:59:00.000Z",
        "voteCount": 4,
        "content": "https://stackoverflow.com/questions/26425513/retaining-data-in-apache-kafka\nkafka, can retain data as long as we want"
      },
      {
        "date": "2020-02-20T08:44:00.000Z",
        "voteCount": 1,
        "content": "Why is IoT Hub not a possible answer?"
      },
      {
        "date": "2020-02-20T22:26:00.000Z",
        "voteCount": 6,
        "content": "because -&gt;24MB/sec/unit  -&gt;https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-quotas-throttling#operation-throttles"
      },
      {
        "date": "2020-05-17T03:19:00.000Z",
        "voteCount": 5,
        "content": "24MB/sec should be enough, as the target rate is expressed in \"bit\" --&gt; 50Mb = 6,25MB\nThe doubt remains on the retention period, as for IoT Hub it is max 7 days\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-devguide-messages-read-builtin"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50114-exam-ai-100-topic-3-question-9-discussion/",
    "body": "You are designing a solution that will use the Azure Content Moderator service to moderate user-generated content.<br>You need to moderate content containing certain phrases without repeatedly scanning the collected content.<br>Which two APIs should you use? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerm List API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Moderation API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImage Moderation API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkflow API"
    ],
    "answer": "AC",
    "answerDescription": "The default global list of terms in Azure Content Moderator is sufficient for most content moderation needs. However, you might need to screen for terms that are specific to your organization. For example, you might want to tag competitor names for further review.<br>Use the List Management API to create custom lists of terms to use with the Text Moderation API. The Text - Screen operation scans your text for profanity, and also compares text against custom and shared blacklists.<br>C: Use Content Moderator's machine-assisted image moderation and human-in-the-loop Review tool to moderate images for adult and racy content.<br>Instead of moderating the same image multiple times, you add the offensive images to your custom list of blocked content. That way, your content moderation system compares incoming images against your custom lists and stops any further processing.<br>Incorrect Answers:<br>B: Use the Text Moderation API in Azure Content Moderator to scan your text content. The operation scans your content for profanity, and compares the content against custom and shared blacklists.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/content-moderator/try-terms-list-api https://docs.microsoft.com/en-us/azure/cognitive-services/content-moderator/image-moderation-api",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-14T05:56:00.000Z",
        "voteCount": 7,
        "content": "\"You need to moderate content containing certain phrases\"\nit's text, not images. I think answer is A and B"
      },
      {
        "date": "2023-06-20T12:00:00.000Z",
        "voteCount": 1,
        "content": "To moderate content containing certain phrases without repeatedly scanning the collected content using Azure Content Moderator service, you should use the following two APIs:\n\nB. Text Moderation API\nD. Workflow API"
      },
      {
        "date": "2021-05-24T06:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is incorrect - It should be Term list API and Text Moderation API.\nEven the reference answer contains text API references."
      },
      {
        "date": "2021-05-15T19:47:00.000Z",
        "voteCount": 1,
        "content": "I selected A and b  - not sure if it was right or wrong"
      },
      {
        "date": "2021-05-03T06:17:00.000Z",
        "voteCount": 2,
        "content": "I also think that answer is A and B"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14691-exam-ai-100-topic-3-question-10-discussion/",
    "body": "You need to configure versioning and logging for Azure Machine Learning models.<br>Which Machine Learning service application should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModels",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExperiments",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeployments"
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio/version-control https://docs.microsoft.com/en-us/azure/machine-learning/how-to-track-experiments#logging-for-deployed-models",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-24T12:03:00.000Z",
        "voteCount": 10,
        "content": "will be E. To retrieve logs from a previously deployed web service, \n\nexperiments are only logging for code to a training script"
      },
      {
        "date": "2020-09-14T23:39:00.000Z",
        "voteCount": 9,
        "content": "It is ML Pipelines https://docs.microsoft.com/en-us/azure/machine-learning/concept-ml-pipelines"
      },
      {
        "date": "2021-05-29T22:39:00.000Z",
        "voteCount": 1,
        "content": "no, the official Microsoft DP-100 reviewer from GITHUB which you can see here https://github.com/MicrosoftLearning/mslearn-dp100 uses experiment to do that. \n\nAlso, the link you shared, which shows the designer + code, still uses the experiment object beneath it , but rather than creating a simple run object, what gets created in the background is  a pipeline_run, which is also still from the experiment object\n\nso answer is EXPERIMENT object"
      },
      {
        "date": "2023-06-20T12:05:00.000Z",
        "voteCount": 1,
        "content": "c. Experiments - Based on the retrieved documents, the Azure Machine Learning service application that you should use to configure versioning and logging for Azure Machine Learning models is Experiments. Experiments allow you to track and manage the versions of your models and log the results of your experiments"
      },
      {
        "date": "2021-08-23T07:10:00.000Z",
        "voteCount": 1,
        "content": "pipelines"
      },
      {
        "date": "2021-06-15T11:59:00.000Z",
        "voteCount": 2,
        "content": "Deployments is the correct answer"
      },
      {
        "date": "2021-05-29T21:13:00.000Z",
        "voteCount": 3,
        "content": "if you download the official Microsoft DP-100 reviewer from GITHUB which you can see here  https://github.com/MicrosoftLearning/mslearn-dp100\n\nyou can navigate to these notebooks\n\nYou can register a trained model using 2 approaches\n\n1. using the run.register_model() method which you get when you call experiment.submit() which was used in this notebook 05 - Train Models.ipynb\n2. the other method using the Model.register() method which was used in this notebook 08 - Create a Pipeline.ipynb\n\nNow regarding logging, you the run object has a number of logging methods\n\n1. run.log()\n2. run.log_image()\n3. run.log_table()\n4. run.log_row()\n5. run.log_list()\n\nthis is all from the \"run\" object that you get when invoking the experiment.submit() method, and we usually use the run.get_metrics() method to get all the logged metrics using the run.log() method when logging accuracy, f1, etc.\n\non the other hand, MODEL doesn't have any method that \"logs\" model performance\n\nso the answer is definitely EXPERIMENT"
      },
      {
        "date": "2021-01-24T07:53:00.000Z",
        "voteCount": 5,
        "content": "The answer is, C. Experiments, \nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio/version-control"
      },
      {
        "date": "2021-01-04T12:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-enable-app-insights\nDeployments??"
      },
      {
        "date": "2020-10-29T16:10:00.000Z",
        "voteCount": 5,
        "content": "It should be Models. 'Model registration allows you to store and version your models in the Azure cloud, in your workspace. The model registry makes it easy to organize and keep track of your trained models. After registration, you can then download or deploy the registered model and receive all the files that were registered.' \nhttps://docs.microsoft.com/en-us/azure/machine-learning/concept-model-management-and-deployment"
      },
      {
        "date": "2021-02-11T08:05:00.000Z",
        "voteCount": 1,
        "content": "Isn't this MLOps page talking more about Pipelines than Models? Model Registration is a process/step. Doesn't say how logging is achieved. Creating pipelines though makes way for both versioning and logging.\nI wouldn't call the question tricky... I call it poorly framed."
      },
      {
        "date": "2020-09-24T02:19:00.000Z",
        "voteCount": 1,
        "content": "for versioning I suppose you have to register the model by using the Model class like shown in the code snippet here https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.model.model?view=azure-ml-py\n\nand for logging you use Run class"
      },
      {
        "date": "2020-09-24T02:19:00.000Z",
        "voteCount": 2,
        "content": "so not sure what to mark here."
      },
      {
        "date": "2020-05-30T06:02:00.000Z",
        "voteCount": 3,
        "content": "The question is very tricky, as it is asking for logging and versioning for models, not experiment runs. Thinking about model versioning, it reminds me to model registry, which keeps multiple models versions and it comes with Model service:\nhttps://docs.microsoft.com/it-it/azure/machine-learning/concept-azure-machine-learning-architecture#models\n\nTalking about logging, it could be referred to logs from deployed models, as stated in the answer.\nAre we sure it is to select only one option? I would have said both Models and Deployments."
      },
      {
        "date": "2020-02-22T06:43:00.000Z",
        "voteCount": 2,
        "content": "I think this would be experiments?\nhttps://docs.microsoft.com/en-gb/azure/machine-learning/how-to-enable-logging#logging-for-deployed-models"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14692-exam-ai-100-topic-3-question-11-discussion/",
    "body": "DRAG DROP -<br>You need to design the workflow for an Azure Machine Learning solution. The solution must meet the following requirements:<br>\u2711 Retrieve data from file shares, Microsoft SQL Server databases, and Oracle databases that in an on-premises network.<br>\u2711 Use an Apache Spark job to process data stored in an Azure SQL Data Warehouse database.<br>Which service should you use to meet each requirement? To answer, drag the appropriate services to the correct requirements. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0014300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0014300002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio/use-data-from-an-on-premises-sql-server https://docs.microsoft.com/en-in/azure/azure-databricks/what-is-azure-databricks",
    "votes": [],
    "comments": [
      {
        "date": "2020-02-22T06:46:00.000Z",
        "voteCount": 5,
        "content": "Seems correct as Databricks uses Spark over databases as moving data is expensive. Data Factory is the orchestration tool"
      },
      {
        "date": "2020-10-10T17:58:00.000Z",
        "voteCount": 1,
        "content": "1. Can we use Azure Stream Analytics to Retrieve the data?\n2. Can we use Azure Logic Apps to Process the data?"
      },
      {
        "date": "2020-10-23T19:35:00.000Z",
        "voteCount": 7,
        "content": "1. No\n2. No"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3405-exam-ai-100-topic-3-question-12-discussion/",
    "body": "DRAG DROP -<br>You need to build a pipeline for an Azure Machine Learning experiment.<br>In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0014400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0014500001.png\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://azure.microsoft.com/en-in/blog/experimentation-using-azure-machine-learning/ https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/machine-learning-modules",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-09T12:47:00.000Z",
        "voteCount": 31,
        "content": "Should be Score and then Evaluate."
      },
      {
        "date": "2020-01-01T00:08:00.000Z",
        "voteCount": 4,
        "content": "Agreed it should be Score then Evaluate. https://docs.microsoft.com/en-gb/azure/machine-learning/studio/tutorial-part2-credit-risk-train"
      },
      {
        "date": "2020-11-20T22:33:00.000Z",
        "voteCount": 1,
        "content": "Agreed. See ML studio cheatsheet."
      },
      {
        "date": "2020-02-20T08:11:00.000Z",
        "voteCount": 2,
        "content": "Please change the solution :)"
      },
      {
        "date": "2020-02-18T07:50:00.000Z",
        "voteCount": 2,
        "content": "Yes, it will be score and then evaluate.\nhttps://docs.microsoft.com/en-gb/azure/machine-learning/studio/tutorial-part2-credit-risk-train#add-the-evaluate-model-module"
      },
      {
        "date": "2020-02-13T00:22:00.000Z",
        "voteCount": 1,
        "content": "I agree, first it's score and then evaluate."
      },
      {
        "date": "2020-02-07T15:10:00.000Z",
        "voteCount": 1,
        "content": "Agreed first score and then evaluate. The output of scored set becomes the input to evaluate."
      },
      {
        "date": "2019-08-29T13:56:00.000Z",
        "voteCount": 3,
        "content": "Agreed."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55386-exam-ai-100-topic-3-question-13-discussion/",
    "body": "You have an app that records meetings by using speech-to-text capabilities from the Speech Services API.<br>You discover that when action items are listed at the end of each meeting, the app transcribes the text inaccurately when industry terms are used.<br>You need to improve the accuracy of the meeting records.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a phrase list",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom wake word",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParse the text by using the Language Understanding (LUIS) API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a custom model by using Custom Translator"
    ],
    "answer": "A",
    "answerDescription": "Phrase Lists are used to identify known phrases in audio data, like a person's name or a specific location. By providing a list of phrases, you improve the accuracy of speech recognition.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/get-started-speech-to-text?tabs=script%2Cbrowser%<br>2Cwindowsinstall&amp;pivots=programming-language-csharp",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T12:12:00.000Z",
        "voteCount": 1,
        "content": "To improve the accuracy of meeting records when industry terms are used, the best option would be:\n\nA. Add a phrase list\n\nAdding a phrase list would allow you to provide specific industry terms that the app should recognize correctly during the transcription process. By including these industry terms in the phrase list, you can enhance the accuracy of the speech-to-text conversion for those specific terms. This approach helps the app understand and transcribe the specialized vocabulary used in the meetings more accurately."
      },
      {
        "date": "2021-07-10T05:57:00.000Z",
        "voteCount": 2,
        "content": "Answerd should be D"
      },
      {
        "date": "2021-11-15T08:06:00.000Z",
        "voteCount": 2,
        "content": "Why? This is Speech to text is not a translation of language."
      },
      {
        "date": "2021-06-15T12:01:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20964-exam-ai-100-topic-3-question-15-discussion/",
    "body": "DRAG DROP -<br>You plan to use the Microsoft Bot Framework to develop bots that will be deployed by using the Azure Bot Service.<br>You need to configure the Azure Bot Service to support the following types of bots:<br>\u2711 Bots that use Azure Functions<br>\u2711 Bots that set a timer-based<br>Which template should you use for each bot type? To answer drag the appropriate templates to the correct bot type. Each template may be used once, more than once or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0015000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0015100001.png\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/bot-service/bot-service-concept-templates?view=azure-bot-service-3.0",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-19T11:52:00.000Z",
        "voteCount": 7,
        "content": "In Version 4 there are no longer this kind of categorizations.\n\nhttps://docs.microsoft.com/en-us/azure/bot-service/bot-service-overview-introduction?view=azure-bot-service-4.0"
      },
      {
        "date": "2021-01-31T00:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct. It's in the link provided."
      },
      {
        "date": "2021-02-11T15:40:00.000Z",
        "voteCount": 3,
        "content": "Yes, but the link is no longer valid. Like Egosyntonic also said. The site has a \"We're no longer updating this content regularly.\" on top. Bot framework templates are no longer a thing. So this question will no longer appear in the exam, and if it still does, Proactive is the answer for both."
      },
      {
        "date": "2020-12-28T04:56:00.000Z",
        "voteCount": 1,
        "content": "I think the answers are basic and proactive"
      },
      {
        "date": "2021-01-17T01:10:00.000Z",
        "voteCount": 7,
        "content": "why? pls justify. \n\n@Moderator : Pls delete the comment  by blackdeath as the comment is missing justification"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5786-exam-ai-100-topic-3-question-16-discussion/",
    "body": "You have Azure IoT Edge devices that collect measurements every 30 seconds.<br>You plan to send the measurements to an Azure IoT hub.<br>You need to ensure that every event is processed as quickly as possible.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics record functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics windowing functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning on the IoT Edge devices"
    ],
    "answer": "D",
    "answerDescription": "Use Azure Notebooks to develop a machine learning module and deploy it to a Linux device running Azure IoT Edge.<br>You can use IoT Edge modules to deploy code that implements your business logic directly to your IoT Edge devices.<br>References:<br>https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-machine-learning",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-02-12T13:31:00.000Z",
        "voteCount": 25,
        "content": "Correct is  Azure Stream Analytics windowing functions\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics"
      },
      {
        "date": "2020-10-23T09:54:00.000Z",
        "voteCount": 1,
        "content": "this is correct. there is another similar question like this in later pages."
      },
      {
        "date": "2020-03-25T09:35:00.000Z",
        "voteCount": 5,
        "content": "I think it should be ASA with windowing function, that perform operation on events within a window, and one of the types is window hopping that can collect this data based on time"
      },
      {
        "date": "2020-07-29T15:11:00.000Z",
        "voteCount": 1,
        "content": "there's no requirement to have to process the data within a timeframe,. the question asks that the data be processed as fast as possible"
      },
      {
        "date": "2023-06-20T12:20:00.000Z",
        "voteCount": 1,
        "content": "To ensure that every event is processed as quickly as possible when sending measurements from Azure IoT Edge devices to an Azure IoT hub, you should use:\n\nB. Azure Stream Analytics record functions\n\nAzure Stream Analytics is a fully managed real-time analytics service in Azure that can process and analyze streaming data from various sources, including IoT devices. By utilizing record functions within Azure Stream Analytics, you can process each event or measurement as it arrives, enabling near real-time processing."
      },
      {
        "date": "2021-11-17T22:36:00.000Z",
        "voteCount": 2,
        "content": "Such a babula"
      },
      {
        "date": "2021-01-28T06:35:00.000Z",
        "voteCount": 1,
        "content": "Answer: D \nUse Azure Notebooks to develop a machine learning module and deploy it to a Linux device running Azure IoT Edge.  You can use IoT Edge modules to deploy code that implements your business logic directly to your IoT Edge devices. \nReferences: https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-machine-learning"
      },
      {
        "date": "2021-02-11T15:58:00.000Z",
        "voteCount": 1,
        "content": "The question doesn't give sufficient details to answer precisely. For instance, there is nothing preventing Kafka as the right fit. ASA windowing function also could work by using the Now option (only output though). I'm more inclined to saying Kafka is the answer.\nHowever, this is an AI exam. Since Kafka does fall under HDInsight documentation, it could be the closest.\nThe explanation you provided is in no way answering the question but the only relevance is that it is ML related."
      },
      {
        "date": "2020-08-28T06:59:00.000Z",
        "voteCount": 3,
        "content": "It's wrong correct ansis \nD. Azure Machine Learning on the IoT Edge devices"
      },
      {
        "date": "2020-02-12T13:45:00.000Z",
        "voteCount": 4,
        "content": "I think Choice B is correct: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions"
      },
      {
        "date": "2019-09-27T17:44:00.000Z",
        "voteCount": 2,
        "content": "If every event should be processed as quickly as possible, it should be done on the edge using a machine learning module.  https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-machine-learning-edge-05-configure-edge-device"
      },
      {
        "date": "2020-01-10T02:54:00.000Z",
        "voteCount": 4,
        "content": "Well  the question asks about fast event processing , not machine learning , so it is possible that the answer is actually Kafka"
      },
      {
        "date": "2020-04-30T07:50:00.000Z",
        "voteCount": 2,
        "content": "agreed, the answer should be Kafka"
      },
      {
        "date": "2020-06-25T07:09:00.000Z",
        "voteCount": 2,
        "content": "Agree as well. Btw, no need to have the winodwing function. The event is already generated every 30s."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39023-exam-ai-100-topic-3-question-17-discussion/",
    "body": "HOTSPOT -<br>You need to build a real-time media bot for Microsoft Skype on an Azure virtual machine. The bot will use the Azure Bot Service. The solution must minimize custom code.<br>Which environment and language should you use to develop the bot? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0015300001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0015400001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/microsoftteams/platform/concepts/calls-and-meetings/requirements-considerations-application-hosted-media-bots",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T08:57:00.000Z",
        "voteCount": 7,
        "content": "This is correct"
      },
      {
        "date": "2021-02-28T18:58:00.000Z",
        "voteCount": 3,
        "content": "Application-hosted media bot development requires C#/.NET and Windows Server\nhttps://docs.microsoft.com/en-us/microsoftteams/platform/bots/calls-and-meetings/requirements-considerations-application-hosted-media-bots"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39024-exam-ai-100-topic-3-question-18-discussion/",
    "body": "Your company recently purchased several hundred hardware devices that contain sensors.<br>You need to recommend a solution to process the sensor data. The solution must provide the ability to write back configuration changes to the devices.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure IoT Hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAPI apps in Microsoft Azure App Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Notification Hubs"
    ],
    "answer": "A",
    "answerDescription": "References:<br>https://azure.microsoft.com/en-us/resources/samples/functions-js-iot-hub-processing/",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T08:57:00.000Z",
        "voteCount": 7,
        "content": "Answer is correct"
      },
      {
        "date": "2023-06-20T12:24:00.000Z",
        "voteCount": 1,
        "content": "To process sensor data and write back configuration changes to the devices, the recommended solution would be:\n\nA. Microsoft Azure IoT Hub\n\nAzure IoT Hub is a fully managed service that provides secure and reliable communication between IoT devices and the cloud. It enables bi-directional communication, allowing you to send data from the devices to the cloud for processing and analysis, as well as send commands or configuration changes from the cloud back to the devices.\n\nBy using Azure IoT Hub, you can easily connect and manage your hundreds of hardware devices, ingest sensor data at scale, and process it using various Azure services such as Azure Stream Analytics, Azure Functions, or Azure Machine Learning. Additionally, Azure IoT Hub supports device twin and direct methods, which allow you to store device configurations and send commands to update them."
      },
      {
        "date": "2022-05-18T15:43:00.000Z",
        "voteCount": 1,
        "content": "write back -&gt; biderectional -&gt; Azure IoT hub"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53263-exam-ai-100-topic-3-question-19-discussion/",
    "body": "DRAG DROP -<br>You have an intelligent edge solution that processes data and outputs the data to an Azure Cosmos DB account that uses the SQL API.<br>You need to ensure that you can perform full text searches of the data.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0015500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0015600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/search/search-howto-index-cosmosdb",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-21T03:09:00.000Z",
        "voteCount": 1,
        "content": "I would expect to first create the indexer before the index can be created."
      },
      {
        "date": "2021-05-30T20:21:00.000Z",
        "voteCount": 4,
        "content": "Create an empty Index, then the indexer to push into the index.\nhttps://docs.microsoft.com/en-us/azure/search/search-indexer-overview#basic-workflow"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38967-exam-ai-100-topic-3-question-20-discussion/",
    "body": "You have thousands of images that contain text.<br>You need to process the text from the images to a machine-readable character stream.<br>Which Azure Cognitive Services service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImage Moderation API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTranslator Text",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision"
    ],
    "answer": "D",
    "answerDescription": "With Computer Vision you can detect text in an image using optical character recognition (OCR) and extract the recognized words into a machine-readable character stream.<br>Incorrect Answers:<br>A: Use Content Moderator's machine-assisted image moderation and human-in-the-loop Review tool to moderate images for adult and racy content. Scan images for text content and extract that text, and detect faces. You can match images against custom lists, and take further action.<br>Reference:<br>https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/ https://docs.microsoft.com/en-us/azure/cognitive-services/content-moderator/image-moderation-api",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T01:06:00.000Z",
        "voteCount": 7,
        "content": "This is correct"
      },
      {
        "date": "2023-06-20T12:35:00.000Z",
        "voteCount": 1,
        "content": "To process text from images and convert it into a machine-readable character stream, the recommended Azure Cognitive Services service is:\n\nD. Computer Vision\n\nComputer Vision is a powerful service that provides advanced image analysis capabilities, including optical character recognition (OCR). It can extract text from images and convert it into a machine-readable format. By leveraging the OCR capabilities of Computer Vision, you can process thousands of images containing text and obtain the text content from them."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56030-exam-ai-100-topic-3-question-21-discussion/",
    "body": "You have Azure IoT Edge devices that collect measurements every 30 seconds.<br>You plan to send the measurements to an Azure IoT hub.<br>You need to process events in the cloud and account for missing data.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics record functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics windowing functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning on the IoT Edge devices"
    ],
    "answer": "D",
    "answerDescription": "Use Azure Notebooks to develop a machine learning module and deploy it to a Linux device running Azure IoT Edge.<br>You can use IoT Edge modules to deploy code that implements your business logic directly to your IoT Edge devices.<br>Use Clean Missing Data module in Azure Machine Learning to to remove, replace, or infer missing values.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-machine-learning",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T12:39:00.000Z",
        "voteCount": 1,
        "content": "To process events in the cloud and account for missing data from Azure IoT Edge devices, you should use option\n\nC: Azure Stream Analytics windowing functions.\n\nAzure Stream Analytics provides powerful real-time data processing capabilities in the Azure cloud. Windowing functions allow you to group and process data within specific time intervals or event counts. By using windowing functions, you can handle missing data by defining time windows and specifying how to handle data that falls within those windows."
      },
      {
        "date": "2021-06-24T17:17:00.000Z",
        "voteCount": 1,
        "content": "Same the question #16 - Aws: C"
      },
      {
        "date": "2021-07-20T07:15:00.000Z",
        "voteCount": 1,
        "content": "Question #16 mentions : \n\"You need to ensure that every event is processed as quickly as possible.\"\nThis question mentions : \n\"You need to process events in the cloud and account for missing data.\""
      },
      {
        "date": "2021-11-15T09:53:00.000Z",
        "voteCount": 1,
        "content": "Yes because of that here the response is D -Azure Machine Learning on the IoT Edge devices (process in the cloud and account on missing data)\nand in question #16 the response is C - Azure Stream Analytics windowing functions ( process the data every 30 seconds when is received is the fastest way to do it)"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60394-exam-ai-100-topic-3-question-22-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are deploying an Azure Machine Learning model to an Azure Kubernetes Service (AKS) container.<br>You need to monitor the scoring accuracy of each run of the model.<br>Solution: You modify the Config.json file.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead update the manifest file.<br>Reference:<br>https://azure.github.io/learnAnalytics-UsingAzureMachineLearningforAIWorkloads/lab07-deploying_a_scoring_service_to_aks/0_README.html",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T12:40:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nModifying the Config.json file alone will not provide a solution for monitoring the scoring accuracy of each run of the model. The Config.json file typically contains configuration settings for the deployment, such as the image version, service endpoint, and resource requirements."
      },
      {
        "date": "2021-08-23T07:12:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "3"
  }
]