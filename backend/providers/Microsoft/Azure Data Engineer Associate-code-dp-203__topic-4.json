[
  {
    "topic": 4,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54544-exam-dp-203-topic-4-question-1-discussion/",
    "body": "You implement an enterprise data warehouse in Azure Synapse Analytics.<br>You have a large fact table that is 10 terabytes (TB) in size.<br>Incoming queries use the primary key SaleKey column to retrieve data as displayed in the following table:<br><img src=\"/assets/media/exam-media/04259/0034000001.png\" class=\"in-exam-image\"><br>You need to distribute the large fact table across multiple nodes to optimize performance of the table.<br>Which technology should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash distributed table with clustered index",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash distributed table with clustered Columnstore index\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround robin distributed table with clustered index",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround robin distributed table with clustered Columnstore index",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\theap table with distribution replicate"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-06T04:35:00.000Z",
        "voteCount": 39,
        "content": "correct B"
      },
      {
        "date": "2021-09-30T21:21:00.000Z",
        "voteCount": 7,
        "content": "For Example:\nCREATE TABLE [dbo].[FactInternetSales]\n(   [ProductKey]            int          NOT NULL\n,   [OrderDateKey]          int          NOT NULL\n,   [CustomerKey]           int          NOT NULL\n,   [PromotionKey]          int          NOT NULL\n,   [SalesOrderNumber]      nvarchar(20) NOT NULL\n,   [OrderQuantity]         smallint     NOT NULL\n,   [UnitPrice]             money        NOT NULL\n,   [SalesAmount]           money        NOT NULL\n)\nWITH\n(   CLUSTERED COLUMNSTORE INDEX\n,  DISTRIBUTION = HASH([ProductKey])\n)\n;"
      },
      {
        "date": "2024-06-05T23:56:00.000Z",
        "voteCount": 1,
        "content": "Should be A because it is said that most query are using the Primary Key (SaleKey) to access data. If you access data with your PK, it's row-oriented and not columned-oriented. So no Column Store Clustered Index but regular Clustered Index."
      },
      {
        "date": "2023-08-31T00:29:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-04-05T14:06:00.000Z",
        "voteCount": 2,
        "content": "Correct B"
      },
      {
        "date": "2022-12-07T19:04:00.000Z",
        "voteCount": 2,
        "content": "Hash on SaleKey distribution column using Columnstore clustered index; Why? (1) petabyte scale data (2) incoming query on SaleKey therefore, SaleKey will be used in WHERE condition and clustered columnstore index will be efficient."
      },
      {
        "date": "2022-11-07T09:40:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-08-12T11:42:00.000Z",
        "voteCount": 2,
        "content": "yes, B is correct"
      },
      {
        "date": "2022-06-30T07:51:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-01-25T03:48:00.000Z",
        "voteCount": 1,
        "content": "Clustered indexes may outperform clustered columnstore tables when a single row needs to be quickly retrieved. For queries where a single or very few row lookup is required to perform with extreme speed, consider a clustered index or nonclustered secondary index. The disadvantage to using a clustered index is that only queries that benefit are the ones that use a highly selective filter on the clustered index column. To improve filter on other columns, a nonclustered index can be added to other columns. However, each index that is added to a table adds both space and processing time to loads."
      },
      {
        "date": "2021-12-25T11:38:00.000Z",
        "voteCount": 1,
        "content": "Clustered columnstore indexes are the most efficient way you can store your data in Azure SQL Data Warehouse. Storing your data in tables that have a clustered columnstore index are the fastest way to query your data. It will give you the greatest data compression and lower your storage costs. \nHash-distributed tables work well for large fact tables in a star schema. They can have very large numbers of rows and still achieve high performance.\nConsider using a hash-distributed table when:\nThe table size on disk is more than 2 GB.\nThe table has frequent insert, update, and delete operations.\nANS B"
      },
      {
        "date": "2021-09-28T15:58:00.000Z",
        "voteCount": 2,
        "content": "A heap is a table without a clustered index. One or more nonclustered indexes can be created on tables stored as a heap. Data is stored in the heap without specifying an order. Usually data is initially stored in the order in which is the rows are inserted into the table, but the Database Engine can move data around in the heap to store the rows efficiently; so the data order cannot be predicted. To guarantee the order of rows returned from a heap, you must use the ORDER BY clause. To specify a permanent logical order for storing the rows, create a clustered index on the table, so that the table is not a heap.\n\nCorrect answer: Non clustered"
      },
      {
        "date": "2021-07-04T11:14:00.000Z",
        "voteCount": 1,
        "content": "Incoming queries use the primary key SaleKey column to retrieve data as displayed in the following table ..doesn't this mean Salekey will be used in where clause , which makes Salekey not suitable for hashkey distribution .\n\nChoosing a distribution column that helps minimize data movement is one of the most important strategies for optimizing performance of your dedicated SQL pool:\n- Is not used in WHERE clauses. This could narrow the query to not run on all the distributions.\n\nwith no obvious choice i feel it should be round robin with column clustered index i.e D"
      },
      {
        "date": "2021-10-13T07:33:00.000Z",
        "voteCount": 1,
        "content": "Consider using a hash-distributed table when:\nThe table size on disk is more than 2 GB\nref:https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choosing-a-distribution-column"
      },
      {
        "date": "2022-06-17T04:58:00.000Z",
        "voteCount": 1,
        "content": "when you don't have any good candidate for hashkey you can also go for composite key. And here the size of the table is huge and using round robin you will never obtain good performance"
      },
      {
        "date": "2021-06-17T22:34:00.000Z",
        "voteCount": 1,
        "content": "I understand that hash distribution mainly for improving the joins and group-by to reduce the data shuffling. In this case, there is no join or group-by mentioned. I think round-robin would be a better option."
      },
      {
        "date": "2021-06-04T15:42:00.000Z",
        "voteCount": 1,
        "content": "If the answer is hash distributed, then what would be the key? If there is no obvious joining key, round-robin should be chosen (https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#round-robin-distributed)"
      },
      {
        "date": "2021-06-10T00:46:00.000Z",
        "voteCount": 15,
        "content": "It says it uses the SaleKey. \nRound-robin is generally not effective at these large scale tables. The 10 tb was a very important hint here."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54909-exam-dp-203-topic-4-question-2-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool that contains a large fact table. The table contains 50 columns and 5 billion rows and is a heap.<br>Most queries against the table aggregate values from approximately 100 million rows and return only two columns.<br>You discover that the queries against the fact table are very slow.<br>Which type of index should you add to provide the fastest query times?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnonclustered columnstore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tclustered columnstore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnonclustered",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tclustered"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-17T13:56:00.000Z",
        "voteCount": 32,
        "content": "correct!"
      },
      {
        "date": "2021-06-08T11:18:00.000Z",
        "voteCount": 14,
        "content": "correct"
      },
      {
        "date": "2024-03-28T19:39:00.000Z",
        "voteCount": 1,
        "content": "clustered columnstore index"
      },
      {
        "date": "2024-03-27T04:13:00.000Z",
        "voteCount": 1,
        "content": "Clustered index\t* Tables with up to 100 million rows\n\n* Large tables (more than 100 million rows) with only 1-2 columns heavily used"
      },
      {
        "date": "2024-04-12T09:35:00.000Z",
        "voteCount": 1,
        "content": "I think you did not read correctly, the queries are aggregating from 100 million rows out of 5 billion, this suggests that there is some form of selection criteria applied, typically through a WHERE clause, to filter the data down to a subset that is relevant to the query. A clustered columnstore index would help in this scenario by efficiently compressing and storing the data in a columnar format, which is optimal for performing large-scale aggregations on a subset of columns. This type of index would indeed support the query patterns described, by enabling faster aggregation and filtering operations on large datasets"
      },
      {
        "date": "2024-01-14T06:58:00.000Z",
        "voteCount": 1,
        "content": "B. clustered columnstore.\n\nA clustered columnstore index is the most efficient type of index for querying large fact tables with a high proportion of aggregation queries. This is because a clustered columnstore index stores data in a columnar format, which is much more efficient for aggregation queries than a row-based format. Additionally, a clustered columnstore index stores data in a compressed format, which further reduces the amount of data that needs to be scanned.\n\nIn this scenario, the fact table contains 50 columns and 5 billion rows, and most queries aggregate values from approximately 100 million rows and return only two columns. This indicates that the queries are primarily interested in summarizing the data in the table, rather than scanning the entire table for specific rows. Therefore, a clustered columnstore index is the most appropriate choice for improving the performance of these queries."
      },
      {
        "date": "2023-12-16T04:52:00.000Z",
        "voteCount": 2,
        "content": "Clustered index\n* Tables with up to 100 million rows\n\n* Large tables (more than 100 million rows) with only 1-2 columns heavily used"
      },
      {
        "date": "2023-08-31T00:40:00.000Z",
        "voteCount": 2,
        "content": "f your table size is less than the recommended 60 million rows for clustered columnstore indexing, consider using heap or\nclustered index tables."
      },
      {
        "date": "2023-08-31T00:47:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2024-01-31T14:19:00.000Z",
        "voteCount": 2,
        "content": "Where did you get this \"60 million rows\" number? I only see the following \n&gt; Clustered columnstore index Great fit for...\tLarge tables (more than 100 million rows)\n\nAnd it seems like our case"
      },
      {
        "date": "2023-08-15T18:06:00.000Z",
        "voteCount": 1,
        "content": "It asks which index to add answer c"
      },
      {
        "date": "2023-08-14T21:52:00.000Z",
        "voteCount": 2,
        "content": "A heap is a table without a clustered index. One or more nonclustered indexes can be created on tables stored as a heap. \nQuestion says already it's a heap table and asks what to add ? So answer is A"
      },
      {
        "date": "2023-07-20T00:42:00.000Z",
        "voteCount": 1,
        "content": "why not a NCCI - why not A ? \n\nNonclustered columnstore index on a disk-based heap or B-tree index\tUse for:\n\n1) An OLTP workload that has some analytics queries. You can drop B-tree indexes created for analytics and replace them with one nonclustered columnstore index.\n\n2) Many traditional OLTP workloads that perform Extract Transform and Load (ETL) operations to move data to a separate data warehouse. You can eliminate ETL and a separate data warehouse by creating a nonclustered columnstore index on some of the OLTP tables.\tNCCI is an additional index that requires 10% more storage on average.\n\nR: https://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-design-guidance?view=sql-server-ver16#choose-the-best-columnstore-index-for-your-needs\n\nEnjoy !"
      },
      {
        "date": "2023-08-10T01:07:00.000Z",
        "voteCount": 1,
        "content": "it is a currently a heap. thus clustered columnstore makes most sense."
      },
      {
        "date": "2023-07-07T00:56:00.000Z",
        "voteCount": 1,
        "content": "Only 2 columns returned"
      },
      {
        "date": "2023-06-26T09:33:00.000Z",
        "voteCount": 1,
        "content": "B of course, there are a few scenarios where clustered columnstore may not be a good option:\n\nColumnstore tables do not support varchar(max), nvarchar(max), and varbinary(max). Consider heap or clustered index instead.\n\nColumnstore tables may be less efficient for transient data. Consider heap and perhaps even temporary tables.\n\nSmall tables with less than 60 million rows. Consider heap tables."
      },
      {
        "date": "2023-06-20T15:03:00.000Z",
        "voteCount": 2,
        "content": "B. clustered columnstore index.\n\nGiven the large fact table with 50 columns and 5 billion rows, and the fact that most queries aggregate values from approximately 100 million rows and return only two columns, a clustered columnstore index would be the most suitable choice. Clustered columnstore indexes are designed for large-scale data warehousing scenarios and provide excellent compression and query performance for analytical workloads.\n\nA clustered columnstore index stores the data in columnar format, enabling efficient data compression and batch-based query execution. It allows for significant query performance improvements, especially for aggregations and large-scale data retrieval."
      },
      {
        "date": "2023-04-29T01:06:00.000Z",
        "voteCount": 4,
        "content": "im really baffled by all the answers here; noone is even considering clustered index, which is what microsoft is recommending for this particular user case scenario;\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet#index-your-table\nfor a table up to 100 mln records and using heavily 1-2 columns and performing queries with lots of joins and aggregations (group by clause) microsoft recommends clustered index; why is this recommendation not applicable here? could someone explain?"
      },
      {
        "date": "2023-05-03T03:00:00.000Z",
        "voteCount": 5,
        "content": "ignore pls; instead of reading watch out if....i read just if, must have been tired?; so clustered index is NOT good when group by operations; its good if you need to retrieve 1 single row or few rows (but aggregate is not just few rows -&gt; its many many rows aggregating to 1 row, which is not the same); by this i believe its indeed clustered columnstore index so the given answer is correct"
      },
      {
        "date": "2023-04-04T16:31:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-02-05T14:39:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct (B) clustered columnstore - This index reordered the physical table data with columnar  format which is stored with index  and compressed. All the query will fetch from index columnstored data and it is designed specially Data warehouse complex query and aggregated data too."
      },
      {
        "date": "2022-11-29T03:00:00.000Z",
        "voteCount": 1,
        "content": "It's B\n\"Do not use a heap when ranges of data are frequently queried from the table. A clustered index on the range column will avoid sorting the entire heap.\"\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/heaps-tables-without-clustered-indexes?toc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Ftoc.json&amp;bc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Fbreadcrumb%2Ftoc.json&amp;view=sql-server-ver15&amp;preserve-view=true#when-not-to-use-a-heap"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56279-exam-dp-203-topic-4-question-3-discussion/",
    "body": "You create an Azure Databricks cluster and specify an additional library to install.<br>When you attempt to load the library to a notebook, the library in not found.<br>You need to identify the cause of the issue.<br>What should you review?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnotebook logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcluster event logs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tglobal init scripts logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tworkspace logs"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-01T08:08:00.000Z",
        "voteCount": 33,
        "content": "I should say Cluster Event logs:\nAzure Databricks provides three kinds of logging of cluster-related activity:\n\nCluster event logs, which capture cluster lifecycle events, like creation, termination, configuration edits, and so on.\nApache Spark driver and worker logs, which you can use for debugging.\nCluster init-script logs, valuable for debugging init scripts.\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage#event-log"
      },
      {
        "date": "2021-06-30T00:22:00.000Z",
        "voteCount": 11,
        "content": "Correct"
      },
      {
        "date": "2024-04-05T03:29:00.000Z",
        "voteCount": 1,
        "content": "B. cluster event logs.\n\nExplanation:\n\nCluster event logs provide information about the cluster's lifecycle events, including the initialization process.\nWhen you specify an additional library to install on the Databricks cluster, the installation process is part of the cluster initialization.\nReviewing the cluster event logs can help you determine whether the library installation process encountered any errors or issues that prevented the library from being installed successfully.\nAny errors or warnings during the library installation process would likely be logged in the cluster event logs, providing insights into the cause of the issue."
      },
      {
        "date": "2024-02-06T17:36:00.000Z",
        "voteCount": 1,
        "content": "B. Cluster Event Logs"
      },
      {
        "date": "2024-01-09T01:10:00.000Z",
        "voteCount": 2,
        "content": "its so simple yet you all are making it hard"
      },
      {
        "date": "2024-01-03T04:58:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B\n\nCluster event logs capture two init script events: INIT_SCRIPTS_STARTED and INIT_SCRIPTS_FINISHED, indicating which scripts are scheduled for execution and which have completed successfully. INIT_SCRIPTS_FINISHED also captures execution duration.\n\nhttps://docs.databricks.com/en/init-scripts/logs.html"
      },
      {
        "date": "2023-12-25T09:33:00.000Z",
        "voteCount": 1,
        "content": "ChatGpt :\nif the library was to be installed through:\n- Standard Databricks library installation methods: Check the cluster event logs (B).\n- A global init script: Check the global init scripts logs (C).\nWithout additional context or explicit mention of an init script being used, option B is typically the more standard choice for initial troubleshooting."
      },
      {
        "date": "2023-08-31T01:09:00.000Z",
        "voteCount": 1,
        "content": "Legacy global init scripts and cluster-named init scripts are deprecated and cannot be used in new workspaces starting February 21, 2023. On September 1st, 2023, Azure Databricks will disable legacy global init scripts for all workspaces."
      },
      {
        "date": "2023-08-31T01:07:00.000Z",
        "voteCount": 1,
        "content": "should be C"
      },
      {
        "date": "2023-07-07T01:00:00.000Z",
        "voteCount": 2,
        "content": "Cluster event logs"
      },
      {
        "date": "2023-06-28T15:56:00.000Z",
        "voteCount": 3,
        "content": "Cluster event logs in Azure Databricks provide detailed information about the cluster's lifecycle events, including the installation and initialization of libraries. By reviewing the cluster event logs, you can examine the events related to library installation and determine if any errors or issues occurred during the process."
      },
      {
        "date": "2023-06-26T07:27:00.000Z",
        "voteCount": 1,
        "content": "Cluster event logs do not log init script events for each cluster node; only one node is selected to represent them all.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts"
      },
      {
        "date": "2023-06-28T15:56:00.000Z",
        "voteCount": 2,
        "content": "Installation and initialization of libraries is not part of init scripts."
      },
      {
        "date": "2023-08-09T06:28:00.000Z",
        "voteCount": 1,
        "content": "That's incorrect. It is a part of init scripts.\nSome examples of tasks performed by init scripts include:\n\nSet system properties and environment variables used by the JVM.\nModify Spark configuration parameters.\nModify the JVM system classpath in special cases.\nInstall packages and libraries not included in Databricks Runtime. To install Python packages, use the Azure Databricks pip binary located at /databricks/python/bin/pip to ensure that Python packages install into the Azure Databricks Python virtual environment rather than the system Python environment. For example, /databricks/python/bin/pip install &lt;package-name&gt;.\nhttps://learn.microsoft.com/en-us/azure/databricks/init-scripts/"
      },
      {
        "date": "2023-04-19T12:19:00.000Z",
        "voteCount": 2,
        "content": "Additional libraries are installed in global init scripts, so correct answer is C.\n\nSome examples of tasks performed by init scripts include:\n- Install packages and libraries not included in Databricks Runtime. To install Python packages, use the Azure Databricks pip binary located at /databricks/python/bin/pip to ensure that Python packages install into the Azure Databricks Python virtual environment rather than the system Python environment. For example, /databricks/python/bin/pip install &lt;package-name&gt;.\n- Modify the JVM system classpath in special cases.\n- Set system properties and environment variables used by the JVM.\n- Modify Spark configuration parameters.\n\nref: https://learn.microsoft.com/en-us/azure/databricks/clusters/init-scripts"
      },
      {
        "date": "2023-06-28T15:58:00.000Z",
        "voteCount": 1,
        "content": "There are two primary ways to install a library on a cluster:\n- Install a workspace library that has been already been uploaded to the workspace.\n- Install a library for use with a specific cluster only."
      },
      {
        "date": "2023-04-06T03:39:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-03-13T04:44:00.000Z",
        "voteCount": 3,
        "content": "the best option in this scenario would be to review the cluster event logs to identify the cause of the issue where an additional library is not found in the Azure Databricks cluster."
      },
      {
        "date": "2023-02-09T01:29:00.000Z",
        "voteCount": 2,
        "content": "Answer C.\nA global init script runs on every cluster created in your workspace. Global init scripts are useful when you want to enforce organization-wide library configurations or security screens. Only admins can create global init scripts. You can create them using either the UI or REST API."
      },
      {
        "date": "2023-01-24T21:23:00.000Z",
        "voteCount": 2,
        "content": "cluster evnet logs only record start and finish event, so C is right, init script logs record the details of running."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55538-exam-dp-203-topic-4-question-4-discussion/",
    "body": "You have an Azure data factory.<br>You need to examine the pipeline failures from the last 60 days.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Activity log blade for the Data Factory resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Monitor &amp; Manage app in Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Resource health blade for the Data Factory resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Monitor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-18T00:39:00.000Z",
        "voteCount": 31,
        "content": "Activity logs show only activities, e.g., trigger the pipeline, stop the pipeline, ...\nResource health check shows only the healthiness of the resource.\nThe monitor app indeed contains the pipeline run failure information. But it keep the data only for 45 days."
      },
      {
        "date": "2022-01-07T09:15:00.000Z",
        "voteCount": 10,
        "content": "\"Data Factory stores pipeline-run data for only 45 days. Use Azure Monitor if you want to keep that data for a longer time.\""
      },
      {
        "date": "2021-06-17T14:06:00.000Z",
        "voteCount": 7,
        "content": "Correct!"
      },
      {
        "date": "2023-08-31T01:16:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-11-07T10:01:00.000Z",
        "voteCount": 3,
        "content": "Agree with D"
      },
      {
        "date": "2022-08-12T11:50:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2021-12-11T11:06:00.000Z",
        "voteCount": 4,
        "content": "CORRECT"
      },
      {
        "date": "2021-11-19T08:27:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2021-11-13T10:09:00.000Z",
        "voteCount": 3,
        "content": "Correct.."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/57163-exam-dp-203-topic-4-question-5-discussion/",
    "body": "You are monitoring an Azure Stream Analytics job.<br>The Backlogged Input Events count has been 20 for the last hour.<br>You need to reduce the Backlogged Input Events count.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrop late arriving events from the job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Azure Storage account to the job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the streaming units for the job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-05T05:20:00.000Z",
        "voteCount": 19,
        "content": "Correct.\n\"Backlogged Input Events\tNumber of input events that are backlogged. A non-zero value for this metric implies that your job isn't able to keep up with the number of incoming events. If this value is slowly increasing or consistently non-zero, you should scale out your job.\"\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-monitoring"
      },
      {
        "date": "2023-08-31T01:18:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-08-12T11:51:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-25T11:05:00.000Z",
        "voteCount": 4,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62056-exam-dp-203-topic-4-question-6-discussion/",
    "body": "You are designing an Azure Databricks interactive cluster. The cluster will be used infrequently and will be configured for auto-termination.<br>You need to ensure that the cluster configuration is retained indefinitely after the cluster is terminated. The solution must minimize costs.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPin the cluster.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure runbook that starts the cluster every 90 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerminate the cluster manually when processing completes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone the cluster after it is terminated."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-19T08:29:00.000Z",
        "voteCount": 11,
        "content": "Correct"
      },
      {
        "date": "2023-03-10T07:54:00.000Z",
        "voteCount": 9,
        "content": "To ensure that the cluster configuration is retained indefinitely after the cluster is terminated while minimizing costs, you should pin the cluster.\n\nPinning a cluster in Azure Databricks prevents it from being terminated by the auto-termination feature. This means that the cluster configuration and installed libraries will be retained even if the cluster is not being used. This is the most efficient and cost-effective way to ensure that the cluster configuration is retained indefinitely after the cluster is terminated.\n\nCreating an Azure runbook to start the cluster every 90 days would require additional resources and would not be a cost-effective solution. Terminating the cluster manually when processing completes would not retain the cluster configuration. Cloning the cluster after it is terminated would create a new cluster with the same configuration, but this would also result in additional costs.  Should be A"
      },
      {
        "date": "2024-04-28T10:03:00.000Z",
        "voteCount": 1,
        "content": "Azure Databricks retains cluster configuration information for up to 200 all-purpose clusters terminated in the last 30 days and up to 30 job clusters recently terminated by the job scheduler. To keep an all-purpose cluster configuration even after it has been terminated for more than 30 days, an administrator can pin a cluster to the cluster list.\nRef : https://docs.databricks.com/api/azure/workspace/clusters"
      },
      {
        "date": "2024-01-18T22:08:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/compute/clusters-manage"
      },
      {
        "date": "2024-01-14T07:42:00.000Z",
        "voteCount": 1,
        "content": "Cloning the cluster after it is terminated is the best option in this case because it will allow the cluster configuration to be retained indefinitely without requiring any ongoing maintenance or incurring unnecessary costs.\n\nHere are some of the benefits of cloning the cluster after it is terminated:\n\nThe cluster configuration will be retained indefinitely.\nThere will be no ongoing maintenance or costs associated with the cluster.\nThe cluster can be easily restarted when it is needed."
      },
      {
        "date": "2023-08-31T01:20:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-07-03T14:05:00.000Z",
        "voteCount": 2,
        "content": "got this question for my exam"
      },
      {
        "date": "2022-08-12T11:53:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2021-09-14T13:49:00.000Z",
        "voteCount": 6,
        "content": "Correct answer!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62062-exam-dp-203-topic-4-question-7-discussion/",
    "body": "You have an Azure data solution that contains an enterprise data warehouse in Azure Synapse Analytics named DW1.<br>Several users execute ad hoc queries to DW1 concurrently.<br>You regularly perform automated data loads to DW1.<br>You need to ensure that the automated data loads have enough memory available to complete quickly and successfully when the adhoc queries run.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHash distribute the large fact tables in DW1 before performing the automated data loads.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a smaller resource class to the automated data load queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a larger resource class to the automated data load queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate sampled statistics for every column in each table of DW1."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T18:28:00.000Z",
        "voteCount": 21,
        "content": "Correct answer!"
      },
      {
        "date": "2023-08-31T01:21:00.000Z",
        "voteCount": 1,
        "content": "ic correct"
      },
      {
        "date": "2023-06-28T16:01:00.000Z",
        "voteCount": 2,
        "content": "Assigning a larger resource class to the automated data load queries prioritizes their resource allocation, allowing them to complete without being heavily impacted by the concurrent ad hoc queries. This helps avoid contention and ensures that the data loads can utilize the necessary resources to complete successfully."
      },
      {
        "date": "2023-04-04T16:32:00.000Z",
        "voteCount": 3,
        "content": "agreed"
      },
      {
        "date": "2022-08-12T11:54:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-04-25T11:07:00.000Z",
        "voteCount": 3,
        "content": "Is correct!"
      },
      {
        "date": "2021-09-30T21:37:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/resource-classes-for-workload-management"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60637-exam-dp-203-topic-4-question-8-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.<br>You need to identify the extent of the data skew in Table1.<br>What should you do in Synapse Studio?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_node_status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-24T04:16:00.000Z",
        "voteCount": 30,
        "content": "The right answer is D, I tested it in Synapse and it's the only one that actually runs without an error"
      },
      {
        "date": "2021-09-08T02:13:00.000Z",
        "voteCount": 18,
        "content": "-- Find data skew for a distributed table\nDBCC PDW_SHOWSPACEUSED('dbo.FactInternetSales');"
      },
      {
        "date": "2021-12-13T11:26:00.000Z",
        "voteCount": 11,
        "content": "This will only work if you connect to the dedicated pool. The answer you've chosen says you are connecting to the built-in (serverless) pool."
      },
      {
        "date": "2023-12-17T01:09:00.000Z",
        "voteCount": 2,
        "content": "dm_pdw_nodes_db_partition_stats because we need to verify it on Pool1 (not built-in pool!)"
      },
      {
        "date": "2023-08-31T01:44:00.000Z",
        "voteCount": 1,
        "content": "dm_pdw_nodes_db_partition_stats"
      },
      {
        "date": "2023-09-09T01:40:00.000Z",
        "voteCount": 2,
        "content": "You can use DBCC PDW_SHOWSPACEUSED to find the skew, however only on dedicated pools."
      },
      {
        "date": "2023-06-20T15:14:00.000Z",
        "voteCount": 1,
        "content": "Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data."
      },
      {
        "date": "2023-06-28T16:22:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet#distributed-or-replicated-tables"
      },
      {
        "date": "2023-04-19T12:49:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D.\n\nA quick way to check for data skew is to use DBCC PDW_SHOWSPACEUSED, but DBCC PDW_SHOWSPACEUSED is not supported by serverless SQL pool in Azure Synapse Analytics. So A option can't  be performed.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\n\nThe only correct option here is to check  sys.dm_pdw_nodes_db_partition_stats using dedicated SQL pool.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2023-06-20T20:49:00.000Z",
        "voteCount": 2,
        "content": "DBCC PDW_SHOWSPACEUSED is a command that can be used to show space usage information for a Database in an Azure Synapse Analytics dedicated SQL pool. However, it is not the best option for identifying data skew in a specific table."
      },
      {
        "date": "2023-01-31T10:40:00.000Z",
        "voteCount": 2,
        "content": "A quick way to check for data skew is to use DBCC PDW_SHOWSPACEUSED.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2023-01-26T03:41:00.000Z",
        "voteCount": 10,
        "content": "This has been explained by others, but not clear enough to get it. I certainly had to look around and ponder for a bit. So, to give a more lucid explanation for why this is D and why the later question is DBCC PDW_SHOWSPACEUSED , it comes down to the small differences.\n\nYou can use DBCC PDW_SHOWSPACEUSED to find the skew, however only on dedicated pools. Well if you are like me, you would be shouting WELLL THE QUESTION SAID DEDICATED POOL DUH. But if you read it carefully, it says connect to the \"built-in pool\" AKA serverless pool and run DBCC PDW_SHOWSPACEUSED. \nWell, we ain't in a serverless pool are we? so that leaves D as the solution.\n\nin the other question the given answers are so\nA. Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED.\nB. Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.\nC. Connect to Pool1 and run DBCC CHECKALLOC.\nD. Connect to the built-in pool and query sys.dm_pdw_sys_info.\n\nHere we see that db_partition_stats is in a built in, which is a no go, so obviously we use PDW_SHOWSPACEUSED.\n\nHopefully this help any airheaded kindred spirits."
      },
      {
        "date": "2023-01-17T14:44:00.000Z",
        "voteCount": 2,
        "content": "A is a quicker way, but you can run DBCC in a serverless SQL pool, the built-in pool."
      },
      {
        "date": "2023-01-13T22:43:00.000Z",
        "voteCount": 1,
        "content": "Right answer is A. DBCC PDW_SHOWSPACEUSED. google it"
      },
      {
        "date": "2022-08-12T23:42:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2022-07-26T05:28:00.000Z",
        "voteCount": 1,
        "content": "I think that first we need to connect to Pool 1,  this excludes the first two options (and especially DBCC PDW_SHOWSPACEUSED). In the other two options, after connecting to Pool1, we execute query sys.dm_pdw_nodes_db_partition_stats."
      },
      {
        "date": "2022-06-27T00:52:00.000Z",
        "voteCount": 2,
        "content": "For dedicated SQL Pool this is the correct answer. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2022-05-04T07:10:00.000Z",
        "voteCount": 4,
        "content": "Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data.\nref: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2022-04-23T23:50:00.000Z",
        "voteCount": 1,
        "content": "DBCC PDW_SHOWSPACEUSED"
      },
      {
        "date": "2022-04-05T18:30:00.000Z",
        "voteCount": 1,
        "content": "Firstly, this is for DEDICATED SQL Pool.\nHere is what both likely outputs give you:\nsys.dm_pdw_nodes_db_partition_stats:\nobject_id, partition_id, in_row_data_page_count, in_row_used_page_count\nThese columns are not useful in identifying skew\n\nHowever, if you're using PDW_SHOWSPACEUSED:\nROWS, RESERVED_SPACE, DATA_SPACE, INDEX_SPACE, UNUSED_SPACE\nThese columns are definitely useful in identifying skew as you can calculate the Space allocation per row and look at any unused space"
      },
      {
        "date": "2022-04-01T05:36:00.000Z",
        "voteCount": 3,
        "content": "A does not work as in this answer we connect to the build in pool NOT the dedicated pool. This leaves D as valid option"
      },
      {
        "date": "2022-04-05T18:32:00.000Z",
        "voteCount": 1,
        "content": "The question specifies dedicated Pool NOT Built-in Pool, so it is A"
      },
      {
        "date": "2022-04-13T02:46:00.000Z",
        "voteCount": 4,
        "content": "Please read the answer options carefully. In options A + B, you connect to the serverless SQL pool, in options C + D, you connect to the dedicated SQL pool."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62480-exam-dp-203-topic-4-question-9-discussion/",
    "body": "HOTSPOT -<br>You need to collect application metrics, streaming query events, and application log messages for an Azure Databrick cluster.<br>Which type of library and workspace should you implement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0034600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0034700001.png\" class=\"in-exam-image\">",
    "answerDescription": "You can send application logs and metrics from Azure Databricks to a Log Analytics workspace. It uses the Azure Databricks Monitoring Library, which is available on GitHub.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-26T18:44:00.000Z",
        "voteCount": 11,
        "content": "Correct!"
      },
      {
        "date": "2021-10-01T17:32:00.000Z",
        "voteCount": 5,
        "content": "Answer is correct\nhttps://docs.microsoft.com/en-us/azure/architecture/databricks-monitoring/application-logs"
      },
      {
        "date": "2024-04-14T02:32:00.000Z",
        "voteCount": 1,
        "content": "And why not Azure Databricks as Workspace?"
      },
      {
        "date": "2023-09-09T01:44:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-12-10T13:27:00.000Z",
        "voteCount": 3,
        "content": "the solution works for databricks runtime 10.x only, though. \nnewer version isn't supported yet"
      },
      {
        "date": "2022-11-07T10:20:00.000Z",
        "voteCount": 2,
        "content": "The given answer is correct"
      },
      {
        "date": "2022-08-12T23:53:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-02-18T09:04:00.000Z",
        "voteCount": 2,
        "content": "Correct!"
      },
      {
        "date": "2021-09-21T04:09:00.000Z",
        "voteCount": 1,
        "content": "is it correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62063-exam-dp-203-topic-4-question-10-discussion/",
    "body": "You have a SQL pool in Azure Synapse.<br>You discover that some queries fail or take a long time to complete.<br>You need to monitor for transactions that have rolled back.<br>Which dynamic management view should you query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_pdw_request_steps",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_pdw_nodes_tran_database_transactions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_pdw_waits",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_pdw_exec_sessions"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T18:41:00.000Z",
        "voteCount": 15,
        "content": "Correct Answer!"
      },
      {
        "date": "2024-07-13T14:22:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer! \nA. sys.dm_pdw_request_steps:\nProvides detailed information about each step of a request executed in Azure Synapse Analytics, including execution status and step type.\n\nB. sys.dm_pdw_nodes_tran_database_transactions:\nDisplays information about active transactions on each node in the distributed database system within Azure Synapse Analytics.\n\nC. sys.dm_pdw_waits:\nShows information about the waiting tasks in Azure Synapse Analytics, including wait types and durations, to help diagnose query performance issues.\n\nD. sys.dm_pdw_exec_sessions:\nLists all active and historic sessions in Azure Synapse Analytics, including session details and activity status."
      },
      {
        "date": "2024-07-09T11:25:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nExplanation:\nsys.dm_pdw_nodes_tran_database_transactions: This view contains information about transactions at the database level, including their state. You can use this view to identify transactions that have been rolled back.\nOther Options:\nsys.dm_pdw_request_steps: Provides details about the steps for each request, useful for understanding the execution plan of queries but not specific to transactions.\nsys.dm_pdw_waits: Contains information about waiting requests, which can help diagnose performance issues but does not specifically track rolled-back transactions.\nsys.dm_pdw_exec_sessions: Provides information about active sessions but not specifically about transaction rollbacks."
      },
      {
        "date": "2024-05-04T04:46:00.000Z",
        "voteCount": 1,
        "content": "I found this question on my exam 30/04/2024, and I put B. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2023-08-31T01:47:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-01-23T16:12:00.000Z",
        "voteCount": 3,
        "content": "Rollback works with transactions.  answer B"
      },
      {
        "date": "2023-01-16T16:41:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer!B. sys.dm_pdw_nodes_tran_database_transactions"
      },
      {
        "date": "2022-11-07T10:22:00.000Z",
        "voteCount": 3,
        "content": "The given answer is correct"
      },
      {
        "date": "2022-10-29T00:00:00.000Z",
        "voteCount": 4,
        "content": "B. sys.dm_pdw_nodes_tran_database_transactions"
      },
      {
        "date": "2022-08-12T23:55:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-01T05:41:00.000Z",
        "voteCount": 2,
        "content": "see https://docs.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-tran-database-transactions-transact-sql?view=sql-server-ver15"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62386-exam-dp-203-topic-4-question-11-discussion/",
    "body": "You are monitoring an Azure Stream Analytics job.<br>You discover that the Backlogged Input Events metric is increasing slowly and is consistently non-zero.<br>You need to ensure that the job can handle all the events.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the compatibility level of the Stream Analytics job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of streaming units (SUs).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove any named consumer groups from the connection and use $default.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional output stream for the existing input stream."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-19T12:03:00.000Z",
        "voteCount": 15,
        "content": "duplicate question. correct answer B"
      },
      {
        "date": "2023-01-23T16:13:00.000Z",
        "voteCount": 12,
        "content": "Money is the answer to all problems. Answer B. increase SU units."
      },
      {
        "date": "2023-08-31T01:53:00.000Z",
        "voteCount": 1,
        "content": "answer B"
      },
      {
        "date": "2023-01-20T16:26:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-job-metrics\n\nThis link is useful"
      },
      {
        "date": "2022-08-12T23:57:00.000Z",
        "voteCount": 1,
        "content": "correct!"
      },
      {
        "date": "2022-01-07T09:49:00.000Z",
        "voteCount": 1,
        "content": "It's just a similar question. Proposed answers are different."
      },
      {
        "date": "2021-09-30T11:02:00.000Z",
        "voteCount": 4,
        "content": "Repeated"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62369-exam-dp-203-topic-4-question-12-discussion/",
    "body": "You are designing an inventory updates table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns:<br><img src=\"/assets/media/exam-media/04259/0034900001.png\" class=\"in-exam-image\"><br>You identify the following usage patterns:<br>\u2711 Analysts will most commonly analyze transactions for a warehouse.<br>\u2711 Queries will summarize by product category type, date, and/or inventory event type.<br>You need to recommend a partition strategy for the table to minimize query times.<br>On which column should you partition the table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEventTypeID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProductCategoryTypeID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEventDate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWarehouseID\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-28T07:09:00.000Z",
        "voteCount": 26,
        "content": "It is recommended to have at least 1 million rows per partition and distribution. Since there are 60 distributions, the number of rows for each partition must exceed 60 millions. Answer is correct"
      },
      {
        "date": "2021-11-29T13:09:00.000Z",
        "voteCount": 2,
        "content": "Partitioning by EventDate does nott mean a partition for each day.  Partitioning by quarter years would be effective."
      },
      {
        "date": "2024-04-02T05:15:00.000Z",
        "voteCount": 1,
        "content": "Perfectly agree! This was exactly my way of reasoning."
      },
      {
        "date": "2021-10-01T06:07:00.000Z",
        "voteCount": 7,
        "content": "I fully Agree!  Answer is correct\nLink below :https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition\n\n\"When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, dedicated SQL pool already divides each table into 60 distributed databases.\n\nAny partitioning added to a table is in addition to the distributions created behind the scenes. Using this example, if the sales fact table contained 36 monthly partitions, and given that a dedicated SQL pool has 60 distributions, then the sales fact table should contain 60 million rows per month, or 2.1 billion rows when all months are populated. If a table contains fewer than the recommended minimum number of rows per partition, consider using fewer partitions in order to increase the number of rows per partition.\""
      },
      {
        "date": "2021-12-28T09:27:00.000Z",
        "voteCount": 17,
        "content": "D is the correct answert.\nAnalysts will most commonly analyze transactions for a warehouse. This mean that warehouseID is always in where clause. Partition filed should in where clause to improve query performace."
      },
      {
        "date": "2021-12-28T09:29:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2023-08-14T07:47:00.000Z",
        "voteCount": 1,
        "content": "However the cheat sheet says: \"In 99 percent of cases, the partition key should be based on date\""
      },
      {
        "date": "2024-01-02T06:20:00.000Z",
        "voteCount": 1,
        "content": "but only when there is more than 1 billion records   \"You might partition your table when you have a large fact table (greater than 1 billion rows). In 99 percent of cases, the partition key should be based on date.\""
      },
      {
        "date": "2024-09-15T21:47:00.000Z",
        "voteCount": 2,
        "content": "There are partitions and distribution. The question is about partition, but let's remember that if we consider distribution, WarehouseID would be the best option. It should not be used for partitioning, so my answer does not go here.\n\nAs documents say (and ChatGPT, and people in other answers), partitioning by date is the way to go in almost all cases. It requires at least 1M records per partition and the question says we have 1M daily rows. Even if we don't want to partition up to the day, we can partition up to the quarter or even the year, if needed."
      },
      {
        "date": "2023-08-31T02:16:00.000Z",
        "voteCount": 2,
        "content": "is correct"
      },
      {
        "date": "2023-01-31T00:14:00.000Z",
        "voteCount": 2,
        "content": "I don't think the answer is right, the answer should be C, EventDate .\nThe total row number in this inventory updates table is determined before it\u2019s created. And here the question is asking us to chose the partition column, not distribution column."
      },
      {
        "date": "2022-11-07T10:34:00.000Z",
        "voteCount": 2,
        "content": "I would go for a date column since positions are most often created for a date column"
      },
      {
        "date": "2022-11-07T10:38:00.000Z",
        "voteCount": 2,
        "content": "Forget it. I agree with the provided answer D"
      },
      {
        "date": "2022-08-28T05:48:00.000Z",
        "voteCount": 1,
        "content": "Tables ? These are the columns, aren't they ?"
      },
      {
        "date": "2022-08-13T00:21:00.000Z",
        "voteCount": 2,
        "content": "D is right"
      },
      {
        "date": "2022-06-07T20:53:00.000Z",
        "voteCount": 1,
        "content": "I will go C. We are querying about warehouses. Therefore I think the distribution column would have to be warehouse. If not then we would most likely have to do a shuffle to aggregate all the transactions for the same warehouse which would be spread out amongst the 60 distibutions."
      },
      {
        "date": "2022-06-17T05:13:00.000Z",
        "voteCount": 2,
        "content": "It's about partition not distribution. Read the question carefully first"
      },
      {
        "date": "2021-11-01T11:19:00.000Z",
        "voteCount": 1,
        "content": "I agree on date column. \"In most cases, table partitions are created on a date column.\" https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      },
      {
        "date": "2022-02-04T07:55:00.000Z",
        "voteCount": 2,
        "content": "But only in most cases. In most cases old data is not needed so date column often shows up in the where clause. This is why partitioning often makes sense on date columns. In this case the \"Analysts will most commonly analyze transactions for a warehouse\", so WarehouseID will be in the where clause and therefore we should partition on this column."
      },
      {
        "date": "2021-10-11T22:50:00.000Z",
        "voteCount": 2,
        "content": "Aren't partition supposed to be done on columns of group by?. So here it's product type on which analysts summarise.so partition should be on productype"
      },
      {
        "date": "2021-10-24T07:56:00.000Z",
        "voteCount": 5,
        "content": "are you thinking of hash distributions instead of partitions?"
      },
      {
        "date": "2021-10-01T04:34:00.000Z",
        "voteCount": 2,
        "content": "For effective partitions its good to have one million rows per partitions for an ideal optimized scenario. This is also mentioned in the Microsoft documentation. C"
      },
      {
        "date": "2021-10-24T08:01:00.000Z",
        "voteCount": 2,
        "content": "You don't have to put each warehouse into it's own partition though so the sizing argument doesnt make sense....Answer is D as you will benefit from partition elimination when you use the warehouseID in the where clause"
      },
      {
        "date": "2021-09-24T01:26:00.000Z",
        "voteCount": 6,
        "content": "WHERE is applied on the WarehouseID, so D"
      },
      {
        "date": "2021-10-14T17:15:00.000Z",
        "voteCount": 2,
        "content": "Nope, don't use WHERE"
      },
      {
        "date": "2021-10-29T05:38:00.000Z",
        "voteCount": 3,
        "content": "it does : \"Analysts will most commonly analyze transactions for a warehouse\""
      },
      {
        "date": "2021-09-23T04:32:00.000Z",
        "voteCount": 2,
        "content": "I think it faster to go by date (C).....Otherwise, the query time will be extremely long since it has wrangled here and there..."
      },
      {
        "date": "2021-09-20T04:04:00.000Z",
        "voteCount": 1,
        "content": "can someone confirm this ?"
      },
      {
        "date": "2021-10-24T08:02:00.000Z",
        "voteCount": 3,
        "content": "It's 100% D"
      },
      {
        "date": "2021-09-19T07:26:00.000Z",
        "voteCount": 3,
        "content": "I will go C"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60841-exam-dp-203-topic-4-question-13-discussion/",
    "body": "You are designing a star schema for a dataset that contains records of online orders. Each record includes an order date, an order due date, and an order ship date.<br>You need to ensure that the design provides the fastest query times of the records when querying for arbitrary date ranges and aggregating by fiscal calendar attributes.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a date dimension table that has a DateTime key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse built-in SQL functions to extract date attributes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a date dimension table that has an integer key in the format of YYYYMMDD.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the fact table, use integer columns for the date fields.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DateTime columns for the date fields."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 41,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-27T15:54:00.000Z",
        "voteCount": 64,
        "content": "Should be C and D"
      },
      {
        "date": "2022-02-02T04:43:00.000Z",
        "voteCount": 4,
        "content": "Yup, that makes sense"
      },
      {
        "date": "2021-09-04T06:53:00.000Z",
        "voteCount": 14,
        "content": "100% CD"
      },
      {
        "date": "2024-08-02T11:30:00.000Z",
        "voteCount": 1,
        "content": "While B is a valid solution, it presents a serious performance issue when SQL Server has to dynamically convert DATE datatypes to INTEGER on-the-fly when joining to a date dimension that uses INTEGER for its surrogate key.  Therefore, options C and D present the most efficient way of querying/filtering this fact table."
      },
      {
        "date": "2024-08-02T11:32:00.000Z",
        "voteCount": 1,
        "content": "Also, the use of built-in SQL functions would require the database to do a full tablescan on the fact table anyways."
      },
      {
        "date": "2024-04-26T03:08:00.000Z",
        "voteCount": 2,
        "content": "C and D but also B - should have asked for 3 steps"
      },
      {
        "date": "2024-03-04T13:51:00.000Z",
        "voteCount": 1,
        "content": "that makes sense , we don't use datetime if it's possible for performance"
      },
      {
        "date": "2024-02-04T02:32:00.000Z",
        "voteCount": 1,
        "content": "Star schema - Fact and Dim tables - they have to be connected using Date Key (int)."
      },
      {
        "date": "2023-08-31T02:19:00.000Z",
        "voteCount": 1,
        "content": "C&amp;D.  You can see some exapmles from modules of  MS' dp-203 training."
      },
      {
        "date": "2023-06-27T00:15:00.000Z",
        "voteCount": 2,
        "content": "Because of fixed date ranges used to query."
      },
      {
        "date": "2023-05-21T02:14:00.000Z",
        "voteCount": 2,
        "content": "Should be A and E"
      },
      {
        "date": "2023-03-16T09:38:00.000Z",
        "voteCount": 2,
        "content": "A. Create a date dimension table that has a DateTime key. A date dimension table that has a DateTime key can provide fast query times when querying for arbitrary date ranges and aggregating by fiscal calendar attributes. The DateTime key allows for easy sorting and filtering of dates, and can be used to join with the fact table on the order date, order due date, and order ship date fields.\n\nB. Use built-in SQL functions to extract date attributes. Using built-in SQL functions to extract date attributes (such as year, quarter, month, week, day) from the DateTime key in the date dimension table can help with aggregating data by fiscal calendar attributes. This can improve query performance by reducing the amount of data that needs to be scanned and aggregated.\n\nTherefore, the correct actions to perform are A and B."
      },
      {
        "date": "2023-05-05T09:50:00.000Z",
        "voteCount": 1,
        "content": "we are not querying against time. the fact table has only dates"
      },
      {
        "date": "2022-12-04T17:29:00.000Z",
        "voteCount": 2,
        "content": "For sure its CD"
      },
      {
        "date": "2022-10-18T14:42:00.000Z",
        "voteCount": 2,
        "content": "CD with no doubt."
      },
      {
        "date": "2022-08-13T01:33:00.000Z",
        "voteCount": 4,
        "content": "correct - C&amp;D agree with  StudentFromAus M"
      },
      {
        "date": "2022-06-27T16:02:00.000Z",
        "voteCount": 5,
        "content": "THe question has many clues, it states fiscal calendar year and then star schema which hints we need proper fact and dim tables and appropriate date keys to link these."
      },
      {
        "date": "2022-06-20T19:24:00.000Z",
        "voteCount": 3,
        "content": "basic knowledge for fact and dim tables"
      },
      {
        "date": "2022-04-05T19:10:00.000Z",
        "voteCount": 7,
        "content": "Who gives these answers??  It's so obviously C and D.  You want a Date Dim with an Integer key and the fact table also with that integer key"
      },
      {
        "date": "2022-03-17T11:19:00.000Z",
        "voteCount": 2,
        "content": "Should be CD!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69121-exam-dp-203-topic-4-question-14-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics cloud job using Azure Portal\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Microsoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-19T12:11:00.000Z",
        "voteCount": 12,
        "content": "Repeated question"
      },
      {
        "date": "2022-01-17T23:31:00.000Z",
        "voteCount": 8,
        "content": "C is correct"
      },
      {
        "date": "2023-08-31T02:21:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-01-14T05:51:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-08-13T01:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-12-30T22:47:00.000Z",
        "voteCount": 4,
        "content": "C is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69247-exam-dp-203-topic-4-question-15-discussion/",
    "body": "You have a SQL pool in Azure Synapse.<br>A user reports that queries against the pool take longer than expected to complete. You determine that the issue relates to queried columnstore segments.<br>You need to add monitoring to the underlying storage to help diagnose the issue.<br>Which two metrics should you monitor? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSnapshot Storage Size",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache used percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU Limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-08-13T01:41:00.000Z",
        "voteCount": 5,
        "content": "seems correct"
      },
      {
        "date": "2024-04-26T02:26:00.000Z",
        "voteCount": 3,
        "content": "Snapshot Storage Size: This metric indicates the size of the snapshot used by the SQL pool. Monitoring this metric can provide information on how much storage space is being utilized by the pool, which can be useful in understanding if there are any storage overutilization issues.\n\nCache hit percentage: This metric indicates the percentage of column cache used for queries. Monitoring this metric can provide insights into how effectively the columns used in queries are being stored in the column cache, which is critical for query performance."
      },
      {
        "date": "2024-08-15T02:35:00.000Z",
        "voteCount": 1,
        "content": "Snapshot size has not effect on query performance. Answer is BD."
      },
      {
        "date": "2023-08-31T02:24:00.000Z",
        "voteCount": 1,
        "content": "go b &amp;D"
      },
      {
        "date": "2023-08-10T02:22:00.000Z",
        "voteCount": 1,
        "content": "This link might be useful. It explains cache hit percentage and cache used percentage:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity"
      },
      {
        "date": "2023-01-21T13:15:00.000Z",
        "voteCount": 3,
        "content": "This article is more relevant here.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache"
      },
      {
        "date": "2022-01-17T23:32:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer"
      },
      {
        "date": "2022-01-02T04:28:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69248-exam-dp-203-topic-4-question-16-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br>You need to monitor resource utilization to determine the source of the performance issues.<br>Which metric should you monitor?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage"
    ],
    "answer": "B",
    "answerDescription": "Monitor and troubleshoot slow query performance by determining whether your workload is optimally leveraging the adaptive cache for dedicated SQL pools.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-02T04:29:00.000Z",
        "voteCount": 6,
        "content": "correct"
      },
      {
        "date": "2023-08-31T02:25:00.000Z",
        "voteCount": 1,
        "content": "repeted"
      },
      {
        "date": "2022-08-13T01:43:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67733-exam-dp-203-topic-4-question-17-discussion/",
    "body": "You have an Azure Databricks resource.<br>You need to log actions that relate to changes in compute for the Databricks resource.<br>Which Databricks services should you log?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tclusters\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tworkspace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDBFS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSH",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjobs"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-12T06:22:00.000Z",
        "voteCount": 23,
        "content": "It shall be A:Clusters, workspace logs does not have any cluster related resource change."
      },
      {
        "date": "2023-06-25T08:22:00.000Z",
        "voteCount": 5,
        "content": "A cluster is defined within the workspace and cluster events are logged at the workspace level.  See \"Cluster Events\" in the following doc:https://docs.databricks.com/administration-guide/account-settings/audit-logs.html"
      },
      {
        "date": "2022-08-13T01:53:00.000Z",
        "voteCount": 5,
        "content": "A is correct"
      },
      {
        "date": "2024-08-09T23:20:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT : clusters\n\nExplanation:\nClusters: In Azure Databricks, clusters are the key compute resource. Changes to clusters, such as creation, termination, and resizing, directly relate to compute changes. Logging actions related to clusters will help you track compute-related events.\nWorkspace: Logging related to workspace typically involves monitoring user activities within the Databricks environment, such as notebook access, but it does not directly relate to compute changes.\nDBFS: The Databricks File System (DBFS) relates to storage, not compute resources.\nSSH: SSH logging is related to Secure Shell (SSH) connections, not specifically to Databricks compute resources.\nJobs: While jobs involve executing tasks on clusters, they don't specifically log changes in the compute resources themselves.\nThus, to monitor and log compute-related changes in Databricks, focusing on the clusters service is the appropriate choice."
      },
      {
        "date": "2024-04-02T05:07:00.000Z",
        "voteCount": 1,
        "content": "You can't take logs from the workspace! Audit Logs are cluster-scoped: you need clusters."
      },
      {
        "date": "2024-01-14T09:03:00.000Z",
        "voteCount": 2,
        "content": "Clusters are the most important service to log for changes in compute, as they represent the compute resources that are used to run workloads. By logging cluster-related actions, you can track when clusters are created, deleted, or modified. This information can be used to troubleshoot performance issues and ensure that compute resources are being used efficiently."
      },
      {
        "date": "2023-11-08T07:28:00.000Z",
        "voteCount": 1,
        "content": "Answer : A.Clusters\n\nwhy not workspace ?\n\t\n\tWorkspace is not a service that you should log to track changes in compute for the Databricks resource because it does not record events related to creating, editing, deleting, starting, or stopping clusters or jobs. Workspace events are related to actions performed on the workspace itself, such as creating, renaming, deleting, or importing notebooks, folders, libraries, or repos1. These events do not affect the compute \nresources used by the Databricks resource, but rather the workspace content and configuration. \nTherefore, workspace is not a relevant service for logging compute changes."
      },
      {
        "date": "2023-08-31T02:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-14T22:47:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/audit-log-delivery\n\nMS document says cluster"
      },
      {
        "date": "2023-07-08T00:17:00.000Z",
        "voteCount": 3,
        "content": "Cluster is the only compute first class resource"
      },
      {
        "date": "2023-06-21T14:43:00.000Z",
        "voteCount": 2,
        "content": "A. Clusters: Databricks clusters are the primary compute resources in Azure Databricks. Monitoring and logging cluster-related actions will help you track changes in cluster creation, termination, resizing, and other cluster-related activities."
      },
      {
        "date": "2023-03-04T02:08:00.000Z",
        "voteCount": 3,
        "content": "100% SURE A IS A CORRECT ANSWER."
      },
      {
        "date": "2022-05-31T08:36:00.000Z",
        "voteCount": 3,
        "content": "definitely A"
      },
      {
        "date": "2022-05-31T07:47:00.000Z",
        "voteCount": 1,
        "content": "Workspace is correct. Detail is here: \nSet-AzDiagnosticSetting -ResourceId $databricks.ResourceId -WorkspaceId $logAnalytics.ResourceId -Enabled $true -name \"&lt;diagnostic setting name&gt;\" -Category &lt;comma separated list&gt;\n\n\nLink: https://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/azure-diagnostic-logs#configure-diagnostic-log-delivery"
      },
      {
        "date": "2022-05-15T19:59:00.000Z",
        "voteCount": 3,
        "content": "I thought compute is related to cluster."
      },
      {
        "date": "2022-05-14T04:27:00.000Z",
        "voteCount": 4,
        "content": "Answer: A (clusters)\nDispite using workspace to enable logging, from there you need to select clusters form the list if you want to satisfy the \"changes in compute for the Databricks resource\" question, hence the service you sould log is clusters. See link from Amsterliese.\nBeware of links to databricks.com vs links to microsoft because they are two slightly different products (i.e. Databricks (on AWS) vs Azure Databricks).\nFor the other comment referencing dp200; the answer description only gives the defintions but no explanation."
      },
      {
        "date": "2022-08-13T01:45:00.000Z",
        "voteCount": 2,
        "content": "agree A should be the answer"
      },
      {
        "date": "2022-04-13T09:20:00.000Z",
        "voteCount": 2,
        "content": "From what I understand from MS documentation, it should be \nA - clusters\nhttps://docs.microsoft.com/en-us/azure/databricks/administration-guide/account-settings/azure-diagnostic-logs#configure-diagnostic-log-delivery\nThe links in previous comments here which support answer B - workspace refer to AWS databricks. I tried to find a similar setup in the MS documentation, but couldn't find anything. Please tell me if my thinking is wrong. (Always happy to learn ;)"
      },
      {
        "date": "2022-02-25T07:27:00.000Z",
        "voteCount": 2,
        "content": "Agreed with clusters!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68471-exam-dp-203-topic-4-question-18-discussion/",
    "body": "You are designing a highly available Azure Data Lake Storage solution that will include geo-zone-redundant storage (GZRS).<br>You need to monitor for replication delays that can affect the recovery point objective (RPO).<br>What should you include in the monitoring solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t5xx: Server Error errors",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAverage Success E2E Latency",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tavailability",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLast Sync Time\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-24T10:35:00.000Z",
        "voteCount": 5,
        "content": "Answer is D. \nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=/azure/storage/blobs/toc.json#check-the-last-sync-time-property"
      },
      {
        "date": "2022-01-18T00:08:00.000Z",
        "voteCount": 5,
        "content": "D is correct as we need to see impact on rpo to know that we need to see when was last sync carried out."
      },
      {
        "date": "2023-08-31T02:29:00.000Z",
        "voteCount": 1,
        "content": "last sync time"
      },
      {
        "date": "2023-06-21T14:44:00.000Z",
        "voteCount": 2,
        "content": "D. Last Sync Time: Monitoring the Last Sync Time metric will provide you with the information about the timestamp of the last successful synchronization between the primary and secondary regions. By monitoring this metric, you can determine if there are any replication delays impacting the RPO. If the Last Sync Time is significantly behind the current time, it indicates a replication delay and potential RPO impact."
      },
      {
        "date": "2022-08-13T01:57:00.000Z",
        "voteCount": 4,
        "content": "agree Last sync time is right"
      },
      {
        "date": "2022-03-08T23:55:00.000Z",
        "voteCount": 5,
        "content": "https://docs.microsoft.com/en-us/azure/storage/common/last-sync-time-get?tabs=azure-powershell"
      },
      {
        "date": "2022-02-25T07:29:00.000Z",
        "voteCount": 4,
        "content": "last sync time"
      },
      {
        "date": "2021-12-23T05:57:00.000Z",
        "voteCount": 3,
        "content": "The key word in this question is \"monitor\", It means that we would have to see the output over time, so the correct answer should be B. Average Success E2E Latency. In this way we can monitor the spent time for each replication\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-scalable-app-verify-metrics"
      },
      {
        "date": "2021-12-27T09:27:00.000Z",
        "voteCount": 4,
        "content": "Answer is D. See below why not B.\n\nAny blob, file, queue, or table operation latency can cause cascading slowdowns in your application. The Success E2E Latency metric measures the total amount of time it takes for requests to be processed by the storage account APIs, sent to the client, and then acknowledged by the client."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69251-exam-dp-203-topic-4-question-19-discussion/",
    "body": "You configure monitoring for an Azure Synapse Analytics implementation. The implementation uses PolyBase to load data from comma-separated value (CSV) files stored in Azure Data Lake Storage Gen2 using an external table.<br>Files with an invalid schema cause errors to occur.<br>You need to monitor for an invalid schema error.<br>For which error should you monitor?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [com.microsoft.polybase.client.KerberosSecureLogin] occurred while accessing external file.'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\". Query aborted- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [Unable to instantiate LoginClass] occurred while accessing external file.'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXTERNAL TABLE access failed due to internal error: 'Java exception raised on call to HdfsBridge_Connect: Error [No FileSystem for scheme: wasbs] occurred while accessing external file.'"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-08T21:55:00.000Z",
        "voteCount": 6,
        "content": "https://techcommunity.microsoft.com/t5/datacat/polybase-setup-errors-and-possible-solutions/ba-p/305297"
      },
      {
        "date": "2023-08-31T02:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-21T14:47:00.000Z",
        "voteCount": 3,
        "content": "To monitor for an invalid schema error when using PolyBase to load data from CSV files in Azure Data Lake Storage Gen2 using an external table in Azure Synapse Analytics, you should monitor the following error:\n\nB. Cannot execute the query \"Remote Query\" against OLE DB provider \"SQLNCLI11\" for linked server \"(null)\". Query aborted- the maximum reject threshold (0 rows) was reached while reading from an external source: 1 rows rejected out of total 1 rows processed.\n\nThis error indicates that a row in the CSV file has been rejected due to an invalid schema. By monitoring for this error, you can identify when data loading fails due to an incompatible or incorrect schema in the CSV files."
      },
      {
        "date": "2022-08-13T02:02:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-01-19T12:28:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2022-01-19T03:07:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2022-01-02T05:24:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69803-exam-dp-203-topic-4-question-20-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br>You run PDW_SHOWSPACEUSED('dbo.FactInternetSales'); and get the results shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0035600001.png\" class=\"in-exam-image\"><br>Which statement accurately describes the dbo.FactInternetSales table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll distributions contain data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table contains less than 10,000 rows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table uses round-robin distribution.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe table is skewed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-13T02:05:00.000Z",
        "voteCount": 5,
        "content": "D is correct"
      },
      {
        "date": "2022-01-10T04:03:00.000Z",
        "voteCount": 5,
        "content": "I think the answer is correct because in some cases the rows are zero."
      },
      {
        "date": "2023-09-09T01:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-31T02:45:00.000Z",
        "voteCount": 2,
        "content": "You can use DBCC PDW_SHOWSPACEUSED to find the skew, however only on dedicated pools."
      },
      {
        "date": "2022-06-27T16:25:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct"
      },
      {
        "date": "2022-02-16T15:30:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-01-19T12:29:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-01-19T03:11:00.000Z",
        "voteCount": 4,
        "content": "correct as few distributions have more data and few have no data at all. The data should be evenly distributed across all the distributions."
      },
      {
        "date": "2022-08-13T02:04:00.000Z",
        "voteCount": 2,
        "content": "Agree !"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68634-exam-dp-203-topic-4-question-21-discussion/",
    "body": "You have two fact tables named Flight and Weather. Queries targeting the tables will be based on the join between the following columns.<br><img src=\"/assets/media/exam-media/04259/0035700001.png\" class=\"in-exam-image\"><br>You need to recommend a solution that maximizes query performance.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the tables use a hash distribution of ArrivalDateTime and ReportDateTime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the tables use a hash distribution of ArrivalAirportID and AirportID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each table, create an IDENTITY column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn each table, create a column as a composite of the other two columns in the table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-19T03:12:00.000Z",
        "voteCount": 5,
        "content": "correct"
      },
      {
        "date": "2024-09-14T05:12:00.000Z",
        "voteCount": 1,
        "content": "Do not HASH where? Do not hash WHERE."
      },
      {
        "date": "2024-08-01T12:07:00.000Z",
        "voteCount": 1,
        "content": "Doing a Hash on the ArrivalAirportID/AirportID ensures that the joined data from each of these tables is located on the same distribution block.  This ensure that no shuffling of data occurs between the 60 distribution blocks."
      },
      {
        "date": "2024-04-23T13:31:00.000Z",
        "voteCount": 1,
        "content": "why not C? for answer B, if some airport is very large and busy, it will contain skewed data"
      },
      {
        "date": "2023-08-31T02:47:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-01-02T13:40:00.000Z",
        "voteCount": 2,
        "content": "Why not D?"
      },
      {
        "date": "2023-02-27T05:26:00.000Z",
        "voteCount": 2,
        "content": "Also, a composite key does not improve performance on its own.\nDistributing on the two columns that are joined, will"
      },
      {
        "date": "2023-01-03T07:16:00.000Z",
        "voteCount": 2,
        "content": "Then you are partly distributing on a date column which is very bad for performance."
      },
      {
        "date": "2022-08-13T02:09:00.000Z",
        "voteCount": 3,
        "content": "B seems correct but not sure what's wrong with D ?"
      },
      {
        "date": "2021-12-27T03:00:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67642-exam-dp-203-topic-4-question-22-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Factory pipeline that has the activities shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0035800001.jpg\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0035800002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0035900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: succeed -<br><br>Box 2: failed -<br>Example:<br>Now let's say we have a pipeline with 3 activities, where Activity1 has a success path to Activity2 and a failure path to Activity3. If Activity1 fails and Activity3 succeeds, the pipeline will fail. The presence of the success path alongside the failure path changes the outcome reported by the pipeline, even though the activity executions from the pipeline are the same as the previous scenario.<br><img src=\"/assets/media/exam-media/04259/0036000001.jpg\" class=\"in-exam-image\"><br>Activity1 fails, Activity2 is skipped, and Activity3 succeeds. The pipeline reports failure.<br>Reference:<br>https://datasavvy.me/2021/02/18/azure-data-factory-activity-failures-and-pipeline-outcomes/",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-13T11:37:00.000Z",
        "voteCount": 46,
        "content": "The answers are correct.\n\nThe second question is \"failed\" because web1 has both a success and failed path. web1 would have to have only a failed path for the second question to be considered successful."
      },
      {
        "date": "2024-06-26T12:05:00.000Z",
        "voteCount": 1,
        "content": "This was on my test on 26-Jun with 930+ score, I chose Succeeded for both. Test it for first one I'm damn sure its succeed and for second one also kind of sure"
      },
      {
        "date": "2022-12-04T07:43:00.000Z",
        "voteCount": 30,
        "content": "The second answer should be \"Succeeded\". You are providing false information to other members. The reason why it is a success is because Set Variable 2 happened because of the failure of Web 1. Therefore, this red pipeline is deedmed a success."
      },
      {
        "date": "2023-09-25T19:11:00.000Z",
        "voteCount": 5,
        "content": "You are incorrect.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-else-block"
      },
      {
        "date": "2023-11-06T01:37:00.000Z",
        "voteCount": 2,
        "content": "Yes, as Wanchihh mentions, you are wrong.\nUpper branch status are fail =&gt; skipped =&gt; skipped. According to the logic in the url below, this is deemed as a failed pipeline."
      },
      {
        "date": "2024-04-26T03:36:00.000Z",
        "voteCount": 1,
        "content": "where did you find skipped?"
      },
      {
        "date": "2022-03-30T22:28:00.000Z",
        "voteCount": 11,
        "content": "Second should also be succeeded."
      },
      {
        "date": "2021-12-15T02:21:00.000Z",
        "voteCount": 4,
        "content": "Agree. Second is \"Fail\" because Success connector presented."
      },
      {
        "date": "2021-12-11T09:40:00.000Z",
        "voteCount": 39,
        "content": "The second answer should be Succeeded as 'Set Variable 2' has failed dependency on Web1."
      },
      {
        "date": "2024-08-10T08:16:00.000Z",
        "voteCount": 1,
        "content": "both are succeded"
      },
      {
        "date": "2024-08-02T15:59:00.000Z",
        "voteCount": 1,
        "content": "Both Succeed"
      },
      {
        "date": "2024-08-01T12:12:00.000Z",
        "voteCount": 1,
        "content": "The arrow between \"Set Variable 1\" and \"Stored Procedure 1\" is green which indicates a \"succeed dependency\".  (a grey arrow indicates skipped).  This means the pipeline will fail on answer 2 since Stored Procedure 1 will never execute."
      },
      {
        "date": "2024-04-26T03:36:00.000Z",
        "voteCount": 1,
        "content": "when PL has a success and fail path if any of those completes the PL is succeeded. So second is succeeded"
      },
      {
        "date": "2024-04-26T03:42:00.000Z",
        "voteCount": 1,
        "content": "as of skip - it is used in debug mode"
      },
      {
        "date": "2024-04-16T12:57:00.000Z",
        "voteCount": 1,
        "content": "Both Success"
      },
      {
        "date": "2024-01-09T05:37:00.000Z",
        "voteCount": 1,
        "content": "Both are success 100% sure about this"
      },
      {
        "date": "2024-01-02T06:53:00.000Z",
        "voteCount": 1,
        "content": "I think both are successful\n\nbcz i think when web activity fail it will pass to the set variable and the purpose of the set varible will beCOMPLETED so pipeline will be success"
      },
      {
        "date": "2023-11-20T06:57:00.000Z",
        "voteCount": 2,
        "content": "Both should be succeed \nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#summary-table"
      },
      {
        "date": "2023-11-12T10:06:00.000Z",
        "voteCount": 1,
        "content": "Do If Skip Else block\nIn this approach, customer defines the business logic, and defines both the Upon Failure path, and Upon Success path, with a dummy Upon Skipped activity attached. This approach renders pipeline succeeds, if Upon Failure path succeeds."
      },
      {
        "date": "2023-08-31T02:55:00.000Z",
        "voteCount": 2,
        "content": "1. Success and 2. Failed"
      },
      {
        "date": "2023-04-26T20:30:00.000Z",
        "voteCount": 11,
        "content": "The answer is correct! It's actually pretty neat how ADF determines that.\n\nIf an activity fails but there was a subsequent OnSuccess activity that never runs, it's a fail. To handle that, you also need an OnSkipped activity to follow the OnSuccess activity in case it never ran!\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-else-block"
      },
      {
        "date": "2023-10-11T15:09:00.000Z",
        "voteCount": 3,
        "content": "There are so many wrong high voted answers. READ THIS."
      },
      {
        "date": "2023-09-26T12:17:00.000Z",
        "voteCount": 2,
        "content": "Thanks @chryckie for this explanation.  Finally it's clear!"
      },
      {
        "date": "2023-04-06T15:32:00.000Z",
        "voteCount": 4,
        "content": "second box should be succeeded\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-skip-else-block"
      },
      {
        "date": "2023-01-16T09:37:00.000Z",
        "voteCount": 6,
        "content": "Using this microsfot doc: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#try-catch-block\nthat claims\n\"\"We determine pipeline success and failures as follows:\n-)Evaluate outcome for all leaves activities. If a leaf activity was skipped, we evaluate its parent activity instead\n-)Pipeline result is success if and only if all nodes evaluated succeed\"\"\n\nI used this logic\n\nWhen web1 activity fails: node setVariable2 succeeds and setVariable1 is skipped and its parent node web1 failed; overall pipeline fails"
      },
      {
        "date": "2022-08-13T09:57:00.000Z",
        "voteCount": 2,
        "content": "In any scenario pipeline will show success status, cause we are catching the failure"
      },
      {
        "date": "2022-06-27T16:22:00.000Z",
        "voteCount": 2,
        "content": "The answers are correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67680-exam-dp-203-topic-4-question-23-discussion/",
    "body": "You have several Azure Data Factory pipelines that contain a mix of the following types of activities:<br>\u2711 Wrangling data flow<br>\u2711 Notebook<br>\u2711 Copy<br>\u2711 Jar<br>Which two Azure services should you use to debug the activities? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 96,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-12T18:57:00.000Z",
        "voteCount": 36,
        "content": "Notebook- azure databricks,  managing activities in pipeline-datafactroy"
      },
      {
        "date": "2024-06-26T12:09:00.000Z",
        "voteCount": 2,
        "content": "This was on my test on 26-Jun with 930+ score, I chose above and its correct 100%"
      },
      {
        "date": "2022-01-12T09:39:00.000Z",
        "voteCount": 16,
        "content": "D &amp; E; Databricks for Wrangling and Notebooks; ADF for Copy and Jar"
      },
      {
        "date": "2022-02-08T22:04:00.000Z",
        "voteCount": 12,
        "content": "Wrangling and Copy = ADF \nJar and Notbooks = Databricks"
      },
      {
        "date": "2024-09-15T02:14:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT confirms Data Factory Monitor for Copy and Wrangling + Databricks for Notebook and Jar."
      },
      {
        "date": "2024-08-02T11:48:00.000Z",
        "voteCount": 1,
        "content": "Examtopics answers are incorrect.\n\n1. Why would you use Synapse Analytics to debug something that it outside of its own system (i.e. Data Factory Pipelines).\n2. Azure Machine Learning describes a type of logic you can implement.  It is not an \"environment\" where you can debug a Data Factory Pipeline."
      },
      {
        "date": "2024-05-04T04:47:00.000Z",
        "voteCount": 1,
        "content": "I found this question on my exam 30/04/2024, and I put DE. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-04-29T19:10:00.000Z",
        "voteCount": 1,
        "content": "notebook-&gt; databricks\ncopy -&gt; adf"
      },
      {
        "date": "2024-04-02T04:44:00.000Z",
        "voteCount": 1,
        "content": "Just a detail: there's another question on examtopics, perfectly identical, with solution DE ..."
      },
      {
        "date": "2024-01-15T01:52:00.000Z",
        "voteCount": 1,
        "content": "he two Azure services you should use to debug the activities are Azure Data Factory and Azure Databricks.\n\nAzure Data Factory provides a comprehensive debugging experience for all types of activities, including wrangling data flows, notebooks, and copy activities. You can use the Data Factory UI or command-line tools to step through activities, inspect data, and identify errors.\n\nAzure Databricks is a cloud-based platform for big data processing that offers a rich debugging experience for Jar activities. You can use Databricks notebooks to debug Jar code, inspect variables, and set breakpoints."
      },
      {
        "date": "2023-08-31T02:59:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-06-27T01:20:00.000Z",
        "voteCount": 2,
        "content": "Wrangling and Copy = ADF\nJar and Notbooks = Databricks"
      },
      {
        "date": "2023-06-21T15:00:00.000Z",
        "voteCount": 3,
        "content": "D. Azure Data Factory: Azure Data Factory itself provides debugging capabilities for its activities. You can monitor and debug the execution of pipeline activities directly within the Azure Data Factory interface. It allows you to view activity run details, input/output data, logs, and diagnose any errors or issues encountered during execution.\n\nE. Azure Databricks: Azure Databricks is a powerful analytics platform that integrates well with Azure Data Factory. You can use it to debug and analyze Notebook activities within the Data Factory pipelines. Azure Databricks provides an interactive environment to run and debug notebooks, allowing you to inspect intermediate data, execute code step-by-step, and troubleshoot any issues."
      },
      {
        "date": "2023-05-26T13:49:00.000Z",
        "voteCount": 3,
        "content": "D - Azure Data Factory\nE - Azure Databricks"
      },
      {
        "date": "2023-05-26T13:22:00.000Z",
        "voteCount": 2,
        "content": "You \"de-bug\" the activity with ML??? Seriously??? come on man??? from where you are getting these answers???"
      },
      {
        "date": "2023-04-30T01:44:00.000Z",
        "voteCount": 2,
        "content": "D &amp; E are correct"
      },
      {
        "date": "2023-01-16T09:41:00.000Z",
        "voteCount": 3,
        "content": "Notebook on azure databricks, rest on pipeline data factroy. No sense for AandC"
      },
      {
        "date": "2023-01-03T17:34:00.000Z",
        "voteCount": 4,
        "content": "Wrangling and Copy = ADF\nJar and Notbooks = Databricks"
      },
      {
        "date": "2022-08-13T02:25:00.000Z",
        "voteCount": 2,
        "content": "should be DE"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74281-exam-dp-203-topic-4-question-24-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.<br>You need to identify the extent of the data skew in Table1.<br>What should you do in Synapse Studio?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run sys.dm_pdw_nodes_db_partition_stats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-23T21:45:00.000Z",
        "voteCount": 10,
        "content": "Correct. See Question 12 topic 4"
      },
      {
        "date": "2023-08-31T03:01:00.000Z",
        "voteCount": 3,
        "content": "Question 8 topic 4"
      },
      {
        "date": "2023-08-31T03:00:00.000Z",
        "voteCount": 1,
        "content": "Correct. See Question 12 topic 4"
      },
      {
        "date": "2023-08-14T08:28:00.000Z",
        "voteCount": 1,
        "content": "Does sys.dm_pdw_nodes_db_partition_stats exist? I found, however, found a reference for sys.dm_db_partition_stats that seems to do the trick."
      },
      {
        "date": "2023-07-03T14:28:00.000Z",
        "voteCount": 2,
        "content": "D .See Question 12 topic 4"
      },
      {
        "date": "2023-06-21T15:07:00.000Z",
        "voteCount": 2,
        "content": "D. Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\nBy connecting to Pool1, which represents the dedicated SQL pool, and querying the sys.dm_pdw_nodes_db_partition_stats system view, you can obtain information about the distribution of data across the compute nodes in the SQL pool. This view provides details on the number of rows and the size of data partitions on each node, allowing you to identify any significant data skew in Table1."
      },
      {
        "date": "2023-05-31T12:17:00.000Z",
        "voteCount": 1,
        "content": "D correct"
      },
      {
        "date": "2023-05-26T13:41:00.000Z",
        "voteCount": 1,
        "content": "Option A is correct"
      },
      {
        "date": "2023-05-26T13:45:00.000Z",
        "voteCount": 3,
        "content": "Sorry, option D is correct."
      },
      {
        "date": "2023-06-21T15:06:00.000Z",
        "voteCount": 2,
        "content": "How could you use built-in (serverless) to query dedicated pool?"
      },
      {
        "date": "2022-08-13T02:26:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74509-exam-dp-203-topic-4-question-25-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br>You need to monitor resource utilization to determine the source of the performance issues.<br>Which metric should you monitor?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocal tempdb percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache used percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU percentage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-25T11:31:00.000Z",
        "voteCount": 6,
        "content": "Is correct"
      },
      {
        "date": "2023-08-31T03:03:00.000Z",
        "voteCount": 1,
        "content": "repeted"
      },
      {
        "date": "2023-06-21T15:08:00.000Z",
        "voteCount": 2,
        "content": "Repeated question."
      },
      {
        "date": "2022-06-27T16:32:00.000Z",
        "voteCount": 4,
        "content": "For already used queries, we need to monitor the adaptive caching"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74510-exam-dp-203-topic-4-question-26-discussion/",
    "body": "You have an Azure data factory.<br>You need to examine the pipeline failures from the last 180 days.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Activity log blade for the Data Factory resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipeline runs in the Azure Data Factory user experience",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Resource health blade for the Data Factory resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory activity runs in Azure Monitor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-25T11:31:00.000Z",
        "voteCount": 7,
        "content": "Correct!"
      },
      {
        "date": "2024-05-04T04:48:00.000Z",
        "voteCount": 1,
        "content": "I found this question on my exam 30/04/2024, and I put D. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2023-08-31T03:04:00.000Z",
        "voteCount": 1,
        "content": "repeted"
      },
      {
        "date": "2023-06-21T15:08:00.000Z",
        "voteCount": 2,
        "content": "If you see anything above 45 days involving logs on ADF, it won't be ADF itself."
      },
      {
        "date": "2023-01-24T18:24:00.000Z",
        "voteCount": 1,
        "content": "Asking to monitor Pipeline failures and D is activity runs. so Cant be D. Looks like they are missing an answer here"
      },
      {
        "date": "2022-08-28T07:34:00.000Z",
        "voteCount": 1,
        "content": "Redundant question"
      },
      {
        "date": "2022-08-13T02:28:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/89228-exam-dp-203-topic-4-question-27-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Edge application using Microsoft Visual Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure Portal"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-08T02:16:00.000Z",
        "voteCount": 1,
        "content": "Correct\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/quick-create-visual-studio-code"
      },
      {
        "date": "2023-10-05T09:00:00.000Z",
        "voteCount": 2,
        "content": "Q14 Topic4\nSame question but different options."
      },
      {
        "date": "2023-08-31T03:04:00.000Z",
        "voteCount": 2,
        "content": "repeted"
      },
      {
        "date": "2023-06-21T15:09:00.000Z",
        "voteCount": 1,
        "content": "Azure IoT Hub = Stream"
      },
      {
        "date": "2023-01-15T06:45:00.000Z",
        "voteCount": 4,
        "content": "Azure Stream Analytics Edge application using Microsoft Visual Studio\n\nAzure Stream Analytics is a real-time data streaming service that allows you to analyze and process data streams in near real-time. The Stream Analytics Edge application can be deployed on IoT devices, such as those used to monitor manufacturing machinery, to enable real-time monitoring and analysis of the data generated by the devices. Stream Analytics Edge allows you to run Stream Analytics jobs on IoT"
      },
      {
        "date": "2022-12-06T19:04:00.000Z",
        "voteCount": 2,
        "content": "Reasons for choosing option B --&gt; IoT devices, streaming data, real-time data requirement"
      },
      {
        "date": "2022-11-29T02:58:00.000Z",
        "voteCount": 1,
        "content": "Ans is D"
      },
      {
        "date": "2023-06-21T15:11:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Factory is primarily a data integration service designed for orchestrating and managing data workflows, such as data movement and transformation.\n\nWhile ADF can be used for data ingestion and processing, it is not optimized for real-time scenarios. Azure Data Factory works based on scheduled or triggered data pipelines rather than real-time streaming data processing."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80943-exam-dp-203-topic-4-question-28-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named SA1 that contains a table named Table1.<br>You need to identify tables that have a high percentage of deleted rows.<br>What should you run?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.pdw_nodes_column_store_segments",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_db_column_store_row_group_operational_stats",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.pdw_nodes_column_store_row_groups\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_db_column_store_row_group_physical_stats"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-15T07:20:00.000Z",
        "voteCount": 7,
        "content": "has a column for the total number of rows physically stored (including those marked as deleted) and a column for the number of rows marked as deleted. Use sys.pdw_nodes_column_store_row_groups to determine which row groups have a high percentage of deleted rows and should be rebuilt"
      },
      {
        "date": "2024-07-09T11:59:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o code:\nWITH RowGroupStats AS (\n    SELECT\n        OBJECT_NAME(t.object_id) AS TableName,\n        t.name AS IndexName,\n        p.partition_number AS PartitionNumber,\n        rg.row_group_id AS RowGroupID,\n        rg.total_rows,\n        rg.deleted_rows,\n        rg.deleted_rows * 1.0 / NULLIF(rg.total_rows, 0) AS DeletedRowPercentage\n    FROM\n        sys.dm_db_column_store_row_group_physical_stats AS rg\n        JOIN sys.partitions AS p ON rg.partition_id = p.partition_id\n        JOIN sys.indexes AS t ON p.object_id = t.object_id AND p.index_id = t.index_id\n)\nSELECT\n    TableName,\n    IndexName,\n    PartitionNumber,\n    AVG(DeletedRowPercentage) AS AvgDeletedRowPercentage\nFROM\n    RowGroupStats\nGROUP BY\n    TableName,\n    IndexName,\n    PartitionNumber\nHAVING\n    AVG(DeletedRowPercentage) &gt; 0.1 -- Adjust this threshold as needed\nORDER BY\n    AvgDeletedRowPercentage DESC;"
      },
      {
        "date": "2024-04-08T02:23:00.000Z",
        "voteCount": 1,
        "content": "The system views starting with \"sys.dm_db_\" are specific to SQL Server and provide information about the database and server activities, while the system views starting with \"sys.pdw_nodes_\" are specific to Azure Synapse and provide information about the distribution and performance in the Parallel Data Warehouse distributed storage environment.\n\nAccording to that, C is correct."
      },
      {
        "date": "2024-04-08T02:25:00.000Z",
        "voteCount": 1,
        "content": "Sorry, D is Correct !"
      },
      {
        "date": "2023-12-17T02:35:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7"
      },
      {
        "date": "2023-08-31T03:05:00.000Z",
        "voteCount": 2,
        "content": "D is corrct"
      },
      {
        "date": "2023-08-31T03:10:00.000Z",
        "voteCount": 4,
        "content": "change to C\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7"
      },
      {
        "date": "2023-07-28T12:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer"
      },
      {
        "date": "2023-07-28T12:43:00.000Z",
        "voteCount": 1,
        "content": "The sys.dm_db_column_store_row_group_physical_stats dynamic management view provides information about the physical characteristics of row groups in columnstore indexes, including the number of deleted rows in each row group. You can use this view to identify tables that have a high percentage of deleted rows by calculating the ratio of deleted rows to total rows for each table. -&gt; D is the answer"
      },
      {
        "date": "2023-06-21T15:14:00.000Z",
        "voteCount": 2,
        "content": "Use sys.pdw_nodes_column_store_row_groups to determine which row groups have a high percentage of deleted rows and should be rebuilt.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-catalog-views/sys-pdw-nodes-column-store-row-groups-transact-sql?view=aps-pdw-2016-au7"
      },
      {
        "date": "2022-11-14T07:27:00.000Z",
        "voteCount": 3,
        "content": "C is the correct Answer !"
      },
      {
        "date": "2022-09-07T09:05:00.000Z",
        "voteCount": 3,
        "content": "C is the correct Answer !"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81913-exam-dp-203-topic-4-question-29-discussion/",
    "body": "You have an enterprise data warehouse in Azure Synapse Analytics.<br>You need to monitor the data warehouse to identify whether you must scale up to a higher service level to accommodate the current workloads.<br>Which is the best metric to monitor?<br>More than one answer choice may achieve the goal. Select the BEST answer.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU used",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-26T20:40:00.000Z",
        "voteCount": 14,
        "content": "It must be DWU percentage. e.g. 95% is bad and 99% is very bad, and you don't need to look at anything else.\n\nIf you looked at DWU used, what can you infer without also knowing the DWU limit (or DWU percentage)?"
      },
      {
        "date": "2023-03-17T08:23:00.000Z",
        "voteCount": 8,
        "content": "C. DWU percentage is the best metric to monitor to identify whether you must scale up to a higher service level to accommodate the current workloads in Azure Synapse Analytics. DWU percentage measures the percentage of Data Warehouse Units (DWUs) in use, which indicates how much processing power is being used. If the DWU percentage consistently exceeds a certain threshold, it may be necessary to scale up to a higher service level to accommodate the workload. DWU used, CPU percentage, and Data IO percentage are also important metrics to monitor, but they do not directly reflect the overall processing power available in the data warehouse."
      },
      {
        "date": "2024-09-02T04:04:00.000Z",
        "voteCount": 1,
        "content": "\"DWU used represents only a high-level representation of usage across the SQL pool and isn't meant to be a comprehensive indicator of utilization. To determine whether to scale up or down, consider all factors which can be impacted by DWU such as concurrency, memory, tempdb, and adaptive cache capacity.\"\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity\n\n\"DWU used percentage\nRepresents a high-level representation of usage across the SQL pool. Measured by taking the maximum between CPU percentage and Data IO percentage.\"\nSource: https://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-metrics/microsoft-synapse-workspaces-sqlpools-metrics\n\nAlthough a little bit confusing at first read, due to the answer misleading options, the correct answer is clearly C."
      },
      {
        "date": "2024-07-16T05:31:00.000Z",
        "voteCount": 1,
        "content": "DWU (Data Warehouse Unit) percentage is the best metric to monitor when determining if you need to scale up to a higher service level. It shows how much of the currently allocated compute resources are being used, indicating whether the current level is sufficient or if scaling up is necessary."
      },
      {
        "date": "2024-01-15T02:36:00.000Z",
        "voteCount": 1,
        "content": "If you are concerned about whether you need to scale up to a higher service level, then DWU used is the most important metric to monitor. If you are concerned about how your workload is using your resources, then DWU percentage can be a helpful secondary metric."
      },
      {
        "date": "2023-12-25T09:54:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nKnowing the absolute number of DWUs used (option A) doesn't provide complete information unless you also know the total DWUs available. On the other hand, the DWU percentage directly indicates how much of the available compute capacity is being used, which is a more informative metric for deciding whether scaling is needed.\n\nTherefore, the best metric to monitor to identify whether you need to scale up to a higher service level to accommodate the current workloads would indeed be:\n\nC. DWU percentage"
      },
      {
        "date": "2023-10-13T05:23:00.000Z",
        "voteCount": 1,
        "content": "C - DWU Percentage"
      },
      {
        "date": "2023-08-31T03:12:00.000Z",
        "voteCount": 1,
        "content": "C is the best."
      },
      {
        "date": "2023-08-31T03:17:00.000Z",
        "voteCount": 1,
        "content": "DWU percentage is the percentage of Data Warehouse Units (DWUs) used by the data warehouse. It is calculated as the maximum of CPU percentage and Data IO percentage. If the DWU percentage is consistently high, it may indicate that you need to scale up to a higher service level to accommodate the current workloads"
      },
      {
        "date": "2023-06-21T15:16:00.000Z",
        "voteCount": 2,
        "content": "Percentage should be way more useful than units itself."
      },
      {
        "date": "2023-05-18T00:06:00.000Z",
        "voteCount": 4,
        "content": "i vote for C because only the % used is meaningful"
      },
      {
        "date": "2023-05-05T21:46:00.000Z",
        "voteCount": 4,
        "content": "I 100% agree with C. How will you know by a UNIT value if it's sufficient or not? You would need to check the percentage consumed out of total capacity, right? Hence, in my logical and design views it must be C --&gt; DWU Percentage."
      },
      {
        "date": "2022-12-06T19:03:00.000Z",
        "voteCount": 2,
        "content": "DWU used is the metric to use, if only one best answer is expected. option A."
      },
      {
        "date": "2022-11-30T02:46:00.000Z",
        "voteCount": 4,
        "content": "As given in the document and explanation, DWU used = DWU limit * DWU percentage,  it comprises limit and percentage.\nThe question also states that more than one answer may achieve the goal and we are supposed to select the best answer, I think DWU used gives the best metric."
      },
      {
        "date": "2022-11-21T02:09:00.000Z",
        "voteCount": 1,
        "content": "Which is the best one, DWU used or DWU percentage? We need to select one."
      },
      {
        "date": "2022-11-06T12:07:00.000Z",
        "voteCount": 1,
        "content": "AC Both are answers"
      },
      {
        "date": "2022-09-12T20:36:00.000Z",
        "voteCount": 2,
        "content": "should be both DWU metrics"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/86576-exam-dp-203-topic-4-question-30-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics cloud job using Azure Portal\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Microsoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-31T21:08:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-11-21T07:50:00.000Z",
        "voteCount": 3,
        "content": "this question is not repeated as options are different. It could appear the first one or this."
      },
      {
        "date": "2022-11-03T02:46:00.000Z",
        "voteCount": 2,
        "content": "Question is a repeat"
      },
      {
        "date": "2022-10-28T05:41:00.000Z",
        "voteCount": 3,
        "content": "Correct. Repeated question."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53900-exam-dp-203-topic-4-question-31-discussion/",
    "body": "HOTSPOT -<br>You have an Azure event hub named retailhub that has 16 partitions. Transactions are posted to retailhub. Each transaction includes the transaction ID, the individual line items, and the payment details. The transaction ID is used as the partition key.<br>You are designing an Azure Stream Analytics job to identify potentially fraudulent transactions at a retail store. The job will use retailhub as the input. The job will output the transaction ID, the individual line items, the payment details, a fraud score, and a fraud indicator.<br>You plan to send the output to an Azure event hub named fraudhub.<br>You need to ensure that the fraud detection solution is highly scalable and processes transactions as quickly as possible.<br>How should you structure the output of the Stream Analytics job? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0036700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0036800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 16 -<br>For Event Hubs you need to set the partition key explicitly.<br>An embarrassingly parallel job is the most scalable scenario in Azure Stream Analytics. It connects one partition of the input to one instance of the query to one partition of the output.<br><br>Box 2: Transaction ID -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-09T23:48:00.000Z",
        "voteCount": 42,
        "content": "Correct.\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\nEmbarrassingly parallel jobs\nStep 3 and 4."
      },
      {
        "date": "2021-10-12T15:20:00.000Z",
        "voteCount": 1,
        "content": "The step 4 you\u2019ve mentioned, @Preben, says: \u201cThe number of input partitions must equal the number of output partitions\u201d. The documentation continues to talk about scenarios that are not embarrassingly parallel like @Maunik has mentioned below"
      },
      {
        "date": "2021-10-12T15:21:00.000Z",
        "voteCount": 2,
        "content": "Disregard my above comment\u2026 meant to respond to another"
      },
      {
        "date": "2023-12-25T09:58:00.000Z",
        "voteCount": 4,
        "content": "Correct cgatpgt :\nFor high scalability and quick processing in Azure Stream Analytics, it's important to align the output event hub partitions with the input source. Since the input event hub `retailhub` has 16 partitions, the output event hub `fraudhub` should also have 16 partitions to match. This ensures that the partitioning scheme is consistent and can handle the volume of transactions efficiently.\n\nThe partition key should be the `Transaction ID`, as this will ensure that all the events for a particular transaction will go to the same partition, maintaining the order of events which is crucial for transactional data and fraud detection scenarios.\n\nSo the correct answers are:\n\nNumber of partitions: 16\nPartition key: Transaction ID"
      },
      {
        "date": "2023-08-31T21:17:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-05T05:03:00.000Z",
        "voteCount": 1,
        "content": "Number of partitions: Since the input event hub retailhub has 16 partitions, it makes sense to have the same number of partitions in the output event hub fraudhub to align the partitions. So the number of partitions should be 16.\n\nPartition key: Since the transaction ID was used as the partition key in the input event hub, using the same partition key in the output event hub ensures that the data for the same transaction ID is processed by the same partition in both event hubs. This makes the flow of data from one event hub to the other more efficient. So the partition key should be the Transaction ID."
      },
      {
        "date": "2022-08-12T11:05:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-06-29T00:50:00.000Z",
        "voteCount": 3,
        "content": "\"A per-device or user unique identity makes a good partition key, but other attributes such as geography can also be used to group related events into a single partition.\""
      },
      {
        "date": "2022-05-26T15:20:00.000Z",
        "voteCount": 2,
        "content": "Event Hub -&gt; Event Hub: x:x partitions\nEvent Hub -&gt; Blob Storage: x:1 partitions or x:y partitions\nBlob Storage -&gt; Event Hub: x:x partitions\nBlob Storage -&gt; Blob Storage: x:1 partitions"
      },
      {
        "date": "2021-09-11T10:05:00.000Z",
        "voteCount": 2,
        "content": "Example of scenarios that are not embarrassingly parallel\nMismatched partition count\nInput: Event hub with 8 partitions\nOutput: Event hub with 32 partitions\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\n\nShould be 8 partitions based on link above"
      },
      {
        "date": "2022-06-09T03:18:00.000Z",
        "voteCount": 1,
        "content": "Maunik it did mention there that it results to \"some level of parallelization\". So I don't think this is the best option to choose if you have equal number of partitions (i.e 16 here) in your options"
      },
      {
        "date": "2021-07-27T02:43:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't the number of partitions only be 8, since the question only asks about the output?"
      },
      {
        "date": "2021-05-31T02:26:00.000Z",
        "voteCount": 2,
        "content": "Why 16? Don't understand..."
      },
      {
        "date": "2021-06-09T05:03:00.000Z",
        "voteCount": 10,
        "content": "Embarrassingly parallel jobs"
      },
      {
        "date": "2021-06-22T22:47:00.000Z",
        "voteCount": 10,
        "content": "It's not THAT embarrassing"
      },
      {
        "date": "2022-06-24T20:54:00.000Z",
        "voteCount": 2,
        "content": "There are 2 eventhub, first has 16 partitions and the number of partitions asked is for the second eventhub, and both must be equals for better performance"
      },
      {
        "date": "2022-02-17T14:43:00.000Z",
        "voteCount": 2,
        "content": "An embarrassingly parallel job is the most scalable scenario in Azure Stream Analytics. It connects one partition of the input to one instance of the query to one partition of the output.\nThe number of input partitions must equal the number of output partitions."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52955-exam-dp-203-topic-4-question-32-discussion/",
    "body": "HOTSPOT -<br>You have an on-premises data warehouse that includes the following fact tables. Both tables have the following columns: DateKey, ProductKey, RegionKey.<br>There are 120 unique product keys and 65 unique region keys.<br><img src=\"/assets/media/exam-media/04259/0036900001.png\" class=\"in-exam-image\"><br>Queries that use the data warehouse take a long time to complete.<br>You plan to migrate the solution to use Azure Synapse Analytics. You need to ensure that the Azure-based solution optimizes query performance and minimizes processing skew.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0037000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0037100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash-distributed -<br><br>Box 2: ProductKey -<br>ProductKey is used extensively in joins.<br>Hash-distributed tables improve query performance on large fact tables.<br><br>Box 3: Hash-distributed -<br><br>Box 4: RegionKey -<br>Round-robin tables are useful for improving loading speed.<br>Consider using the round-robin distribution for your table in the following scenarios:<br>\u2711 When getting started as a simple starting point since it is the default<br>\u2711 If there is no obvious joining key<br>\u2711 If there is not good candidate column for hash distributing the table<br>\u2711 If the table does not share a common join key with other tables<br>\u2711 If the join is less significant than other joins in the query<br>\u2711 When the table is a temporary staging table<br>Note: A distributed table appears as a single table, but the rows are actually stored across 60 distributions. The rows are distributed with a hash or round-robin algorithm.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-03T04:20:00.000Z",
        "voteCount": 104,
        "content": "1. Hash Distributed, ProductKey because &gt;2GB and ProductKey is extensively used in joins\n2. Hash Distributed, RegionKey because \"The table size on disk is more than 2 GB.\" and you have to chose a distribution column which: \"Is not used in WHERE clauses. This could narrow the query to not run on all the distributions.\" \n\nsource: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choosing-a-distribution-column"
      },
      {
        "date": "2021-06-29T08:04:00.000Z",
        "voteCount": 3,
        "content": "I agree with lara_mia1"
      },
      {
        "date": "2021-08-16T10:21:00.000Z",
        "voteCount": 4,
        "content": "Disagree on nr. 1 because of the reason you give for nr. 2. (choose a distribution column that is not used in where clauses. A join is also a where clause"
      },
      {
        "date": "2022-05-26T15:43:00.000Z",
        "voteCount": 4,
        "content": "nah mate, check out his link:\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns. When a table is not used in joins, consider distributing the table on a column that is frequently in the GROUP BY clause.\nIs not used in WHERE clauses. This could narrow the query to not run on all the distributions.\nIs not a date column. WHERE clauses often filter by date. When this happens, all the processing could run on only a few distributions."
      },
      {
        "date": "2021-07-28T02:05:00.000Z",
        "voteCount": 3,
        "content": "i agree"
      },
      {
        "date": "2021-05-17T08:09:00.000Z",
        "voteCount": 28,
        "content": "Both hash as both are &gt; 2GB. In the 2nd table RegionKey cannot be used with round_robin distribution as round_robin does not take a distribution key..."
      },
      {
        "date": "2022-02-04T06:02:00.000Z",
        "voteCount": 1,
        "content": "Correct: \"A round-robin distributed table distributes table rows evenly across all distributions. The assignment of rows to distributions is random. Unlike hash-distributed tables, rows with equal values are not guaranteed to be assigned to the same distribution.\" https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2024-09-23T01:01:00.000Z",
        "voteCount": 1,
        "content": "My 2 cents. if you are choosing the productkey for the sales table and this table is used extensively in joins as stated, and since the regionkey is skewed, we should choose the productkey as a distribution column for the table invoice as well."
      },
      {
        "date": "2024-07-17T05:26:00.000Z",
        "voteCount": 1,
        "content": "Based on the votes, the best agreement for the configuration is:\n\nSales Table: Hash Distributed, ProductKey (134 votes)\nInvoices Table: Hash Distributed, RegionKey (147 votes)"
      },
      {
        "date": "2024-04-26T05:09:00.000Z",
        "voteCount": 1,
        "content": "Hash - Product Key x2"
      },
      {
        "date": "2023-08-31T21:49:00.000Z",
        "voteCount": 2,
        "content": "1. Hash Distributed, ProductKey \n2. Hash Distributed, RegionKey"
      },
      {
        "date": "2023-08-15T21:16:00.000Z",
        "voteCount": 1,
        "content": "When two large fact tables have frequent joins - in this case one is large and another is a small dimension table. Hence highlighted answer is correct"
      },
      {
        "date": "2024-04-26T05:04:00.000Z",
        "voteCount": 1,
        "content": "both are fact tables"
      },
      {
        "date": "2022-09-12T03:54:00.000Z",
        "voteCount": 4,
        "content": "\"Choose a distribution column with data that distributes evenly\"\nProductKey is more relevant in both cases"
      },
      {
        "date": "2022-08-12T11:36:00.000Z",
        "voteCount": 5,
        "content": "1. Hash Distributed, ProductKey because table size &gt;2GB and ProductKey is extensively used in joins . another, region key could have been considered (after join key which is product key) since its being used in grouping but 75% records belongs to one region so - \nNO for region key.\n\n2. Hash Distributed, RegionKey because the table size on disk is more than 2 GB and Its being used in grouping (for this table more than 75% record doesn't fall in same region) and you have to chose a distribution column which is not used in WHERE clause."
      },
      {
        "date": "2022-07-08T01:44:00.000Z",
        "voteCount": 2,
        "content": "To minimize data movement, select a distribution column that:\n\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns. When a table is not used in joins, consider distributing the table on a column that is frequently in the GROUP BY clause.\nIs not used in WHERE clauses. This could narrow the query to not run on all the distributions.\nIs not a date column. WHERE clauses often filter by date. When this happens, all the processing could run on only a few distributions."
      },
      {
        "date": "2022-06-30T07:35:00.000Z",
        "voteCount": 1,
        "content": "the provided aswers are correct"
      },
      {
        "date": "2022-03-15T07:59:00.000Z",
        "voteCount": 1,
        "content": "Generally facts table are hash distributed. so both the table should use hash distribution and distribution key would be product_key for both."
      },
      {
        "date": "2021-08-11T01:24:00.000Z",
        "voteCount": 3,
        "content": "as for me i guess this is the right choice:\n1. Hash Distributed, RegionKey because \n2. Hash Distributed, RegionKey because \n\"When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns\" [Microsoft Documentation]\nIf we use for one ProductKey and for one RegionKey maybe the data movements would increase...or not?"
      },
      {
        "date": "2022-01-04T12:30:00.000Z",
        "voteCount": 4,
        "content": "If we choose RegionKey for Sales, we would have a processing skew."
      },
      {
        "date": "2022-06-18T10:56:00.000Z",
        "voteCount": 1,
        "content": "DarioEtna where in the question is it mentioned that both tables will be used together in a join query? They have different set of columns in where and group by, so why are you so sure that they will be used together? Answers provided are correct here"
      },
      {
        "date": "2021-08-11T01:27:00.000Z",
        "voteCount": 4,
        "content": "But we cannot use ProductKey in both because in Invoice table it is used in WHERE condition"
      },
      {
        "date": "2021-07-26T05:11:00.000Z",
        "voteCount": 2,
        "content": "Regarding the invoces table, we can use the Round-robin distribution because  there is no obvious joining key in the table"
      },
      {
        "date": "2021-07-09T05:31:00.000Z",
        "voteCount": 9,
        "content": "1. Hash on product key\n2. Hash on region key (used on group by and have 65 unique values)"
      },
      {
        "date": "2021-06-11T01:16:00.000Z",
        "voteCount": 3,
        "content": "The sales table makes sense with hashing distribution on ProductKey and since there is no obvious joining key for invoices, you should use round robin distribution on RegionKey. When it would be a smaller table you should use replicated."
      },
      {
        "date": "2021-06-06T22:50:00.000Z",
        "voteCount": 1,
        "content": "When it says 75% of records related to one of the 40 regions, if we partition the Sales by Region, isn't it improve the reading process drastically in compare to productKey?"
      },
      {
        "date": "2021-06-09T23:57:00.000Z",
        "voteCount": 2,
        "content": "That's 75 % of 61 % of the regions that will be done effectively. That's only efficient for 45 % of the queries. Not a whole lot."
      },
      {
        "date": "2021-07-17T06:51:00.000Z",
        "voteCount": 3,
        "content": "No, if 75% relate to one region and we hash on region, that means that those will all be on one node and there will be skew. Correct answers are Hash, Product, Hash, Region."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55298-exam-dp-203-topic-4-question-33-discussion/",
    "body": "You have a partitioned table in an Azure Synapse Analytics dedicated SQL pool.<br>You need to design queries to maximize the benefits of partition elimination.<br>What should you include in the Transact-SQL queries?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJOIN",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWHERE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDISTINCT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGROUP BY"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-23T02:02:00.000Z",
        "voteCount": 8,
        "content": "correct"
      },
      {
        "date": "2021-06-14T01:56:00.000Z",
        "voteCount": 7,
        "content": "Why ??"
      },
      {
        "date": "2021-06-21T04:34:00.000Z",
        "voteCount": 44,
        "content": "Why ?? Because When you add the \"WHERE\" clause to your T-SQL query it allows the query optimizer accesses only the relevant partitions to satisfy the filter criteria of the query - which is what partition elimination is all about."
      },
      {
        "date": "2021-10-29T03:28:00.000Z",
        "voteCount": 2,
        "content": "In question 2, we just mentionned to not use the where condition columns to create partitions.. so the logic is unclear for me.."
      },
      {
        "date": "2024-04-19T21:23:00.000Z",
        "voteCount": 1,
        "content": "I think you mean distributions instead of partitions. For example it is recommended to use the Date column for partitions but not for distributions."
      },
      {
        "date": "2021-11-03T14:14:00.000Z",
        "voteCount": 4,
        "content": "please disregard my comment above. Partitioning is different from hash-column, so the criterias are different"
      },
      {
        "date": "2021-06-21T08:00:00.000Z",
        "voteCount": 1,
        "content": "Maybe this? https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\n\nI think I read somewhere in the docs that you cannot apply complex queries on partition filtering, cannot find it though (not much help I guess, but hopefully better than nothing)"
      },
      {
        "date": "2023-08-31T22:03:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-21T15:29:00.000Z",
        "voteCount": 2,
        "content": "To maximize the benefits of partition elimination in Azure Synapse Analytics dedicated SQL pool, you should include the WHERE clause in your Transact-SQL queries.\n\nThe WHERE clause allows you to specify conditions that filter the rows returned by a query. When designing queries for partitioned tables, you can include predicates in the WHERE clause that align with the partitioning scheme. By doing so, the query optimizer can leverage partition elimination to exclude unnecessary partitions from the query execution plan.\n\nPartition elimination is the process of excluding partitions from query processing based on the predicates specified in the WHERE clause. By eliminating partitions that do not contain relevant data, the query performance can be significantly improved."
      },
      {
        "date": "2022-08-12T11:39:00.000Z",
        "voteCount": 1,
        "content": "correct, agree with okechi"
      },
      {
        "date": "2022-07-10T13:21:00.000Z",
        "voteCount": 4,
        "content": "100% Correct. Think of it this way, you have 36 partitions over Month column for a table. You are interested in a specific month. so in WHERE clause of your select statement, you will give specific month to \"eliminate\" other 35 partitions scan."
      },
      {
        "date": "2022-02-04T06:08:00.000Z",
        "voteCount": 1,
        "content": "A is surely true. But B also. If you have two tables small a and big B and you're joining them on condition a.some_column = b.some_column big table B would be filtered by the values found in a. An if B is partitioned on  \"some_column\" we have the same effect as with the where clause."
      },
      {
        "date": "2022-01-30T10:45:00.000Z",
        "voteCount": 1,
        "content": "B is Correct \nData partition elimination refers to the database server's ability to determine, based on query predicates"
      },
      {
        "date": "2021-12-27T17:10:00.000Z",
        "voteCount": 2,
        "content": "what's the difference between distribution and partition? I don't find any doc online to describe it clearly. \n\n\u2022\tHorizontal partitioning divides a table into multiple tables that contain the same number of columns.\n\u2022\tA distributed table appears as a single table, but the rows are actually stored across 60 distributions.\n\nIf a table have both distribution and Horizontal partition, how are data stored in SQL? For example a customer table, hash-distributed by region and Horizontal Partitioned by year of the activation data."
      },
      {
        "date": "2022-01-04T13:30:00.000Z",
        "voteCount": 4,
        "content": "https://stackoverflow.com/questions/51677471/what-is-a-difference-between-table-distribution-and-table-partition-in-sql/51677595"
      },
      {
        "date": "2022-04-06T23:53:00.000Z",
        "voteCount": 1,
        "content": "distribution is a generally used technique for Massive Distributed Computing. we explicitly decide which distribution pattern to be used in Azure DWH, while Hadoop/Hive automatically distributes the table when created."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/86591-exam-dp-203-topic-4-question-34-discussion/",
    "body": "You have an Azure Stream Analytics query. The query returns a result set that contains 10,000 distinct values for a column named clusterID.<br>You monitor the Stream Analytics job and discover high latency.<br>You need to reduce the latency.<br>Which two actions should you perform? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a pass-through query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of streaming units.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a temporal analytic function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale out the query by using PARTITION BY.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the query to a reference query."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-29T00:26:00.000Z",
        "voteCount": 8,
        "content": "key word: contains 10,000 distinct values for a column named clusterID --&gt; PARTITION.\nreduce the latency --&gt; Increase SU + it refer to PARTITION too."
      },
      {
        "date": "2023-06-21T15:31:00.000Z",
        "voteCount": 8,
        "content": "To reduce latency in an Azure Stream Analytics job with a query returning a result set containing 10,000 distinct values for a column named clusterID, you should perform the following actions:\n\nB. Increase the number of streaming units:\nIncreasing the number of streaming units allocates more resources to your Stream Analytics job, allowing it to handle higher data volumes and processing loads. By increasing the streaming units, you can improve the job's throughput and reduce latency.\n\nD. Scale out the query by using PARTITION BY:\nUsing the PARTITION BY clause in your query allows you to distribute the workload across multiple partitions or parallel processes. By partitioning the data based on relevant criteria, such as clusterID in this case, you can distribute the processing load and reduce latency by enabling parallel processing."
      },
      {
        "date": "2023-09-09T02:08:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-07T02:44:00.000Z",
        "voteCount": 1,
        "content": "B &amp; D are correct"
      },
      {
        "date": "2022-10-28T08:06:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80948-exam-dp-203-topic-4-question-35-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and a database named DB1. DB1 contains a fact table named Table1.<br>You need to identify the extent of the data skew in Table1.<br>What should you do in Synapse Studio?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and query sys.dm_pdw_nodes_db_partition_stats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_node_status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T09:31:00.000Z",
        "voteCount": 10,
        "content": "I don't understand why Exam Topics should be giving different answers for questions they have repeated...like this one!"
      },
      {
        "date": "2024-09-14T04:22:00.000Z",
        "voteCount": 1,
        "content": "I think Exam Topics is not about giving answers, but about hosting Q&amp;As posted by contributors. All answers must be verified."
      },
      {
        "date": "2022-09-07T09:22:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is D."
      },
      {
        "date": "2024-09-15T07:02:00.000Z",
        "voteCount": 1,
        "content": "it D team please check cheat sheet -:https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/cheat-sheet"
      },
      {
        "date": "2024-07-26T12:35:00.000Z",
        "voteCount": 1,
        "content": "Copilot\nSent by Copilot:\nTo identify the extent of the data skew in Table1 within your Azure Synapse Analytics dedicated SQL pool (Pool1), you should:\n\nD. Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats.\n\nThis system view provides detailed information about the distribution of data across the nodes and partitions, which is essential for analyzing data skew."
      },
      {
        "date": "2024-07-16T05:29:00.000Z",
        "voteCount": 1,
        "content": "To identify the extent of data skew in a table within an Azure Synapse Analytics dedicated SQL pool, you should connect to the specific pool (Pool1) and query the sys.dm_pdw_nodes_db_partition_stats dynamic management view. This view provides detailed information about data distribution across nodes, which is essential for identifying skew."
      },
      {
        "date": "2024-06-28T22:00:00.000Z",
        "voteCount": 1,
        "content": "Built-in Pool Limitation: The built-in pool in Synapse Analytics is typically used for on-demand SQL queries and doesn't have access to the detailed statistics of the dedicated SQL pool's partitioned tables."
      },
      {
        "date": "2024-06-27T22:04:00.000Z",
        "voteCount": 1,
        "content": "sys.dm_pdw_nodes_db_partition_stats is a system view in Azure Synapse Analytics dedicated SQL pool that provides statistics about the distribution of data across distributions (similar to nodes or segments) within the pool.\nA is not an option because there's no specific concept of a \"built-in pool\" in Azure Synapse Analytics dedicated SQL pool context. You connect directly to the SQL pool instance (like Pool1) to execute commands."
      },
      {
        "date": "2024-04-02T02:45:00.000Z",
        "voteCount": 1,
        "content": "Correct, you can only launch DMV and DBCC from built-in, not from pool1"
      },
      {
        "date": "2024-02-27T17:18:00.000Z",
        "voteCount": 1,
        "content": "In this series of question, we had the same question and correct answer was D."
      },
      {
        "date": "2024-02-27T17:19:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-02-20T01:47:00.000Z",
        "voteCount": 1,
        "content": "A built-in pool usually means a dedicated SQL pool, I think A will be my choice."
      },
      {
        "date": "2024-02-20T01:56:00.000Z",
        "voteCount": 2,
        "content": "On second thought the word 'pool1' is more definitive than the simple term 'built-in'. So D is correct"
      },
      {
        "date": "2024-01-10T08:17:00.000Z",
        "voteCount": 1,
        "content": "D. Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats. \nbcz its connect to pool1"
      },
      {
        "date": "2023-12-19T15:42:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer!"
      },
      {
        "date": "2023-08-31T22:05:00.000Z",
        "voteCount": 1,
        "content": "repetd"
      },
      {
        "date": "2023-04-29T02:26:00.000Z",
        "voteCount": 1,
        "content": "Its D!\nOfficial Learning path:Returns page and row-count information for every partition in the current database.\nnodes_db_partition_stats\n\nhttps://learn.microsoft.com/en-us/training/modules/analyze-optimize-data-warehouse-storage-azure-synapse-analytics/2-understand-skewed-data-space-usage"
      },
      {
        "date": "2023-03-16T15:40:00.000Z",
        "voteCount": 4,
        "content": "We had the same question before. The correct answer is D"
      },
      {
        "date": "2023-03-01T14:00:00.000Z",
        "voteCount": 1,
        "content": "Option A is confusing as we have different answer for same question."
      },
      {
        "date": "2023-01-17T03:43:00.000Z",
        "voteCount": 5,
        "content": "Answer is not so clear!, because I can't see any refernece on built-in pool. What is built-in pool?\n\nAnyway looking at the doc here: \nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-partition-stats-transact-sql?view=sql-server-ver16\nthat claims: \"This syntax is not supported by serverless SQL pool in Azure Synapse Analytics.\"\n\nSo if built-in pool is serverless SQL pool the correct answer should be D (Connect to Pool1 and query sys.dm_pdw_nodes_db_partition_stats)."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81097-exam-dp-203-topic-4-question-36-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.<br>You need to identify the extent of the data skew in Table1.<br>What should you do in Synapse Studio?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and DBCC PDW_SHOWSPACEUSED.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-08T02:37:00.000Z",
        "voteCount": 8,
        "content": "https://github.com/rgl/azure-content/blob/master/articles/sql-data-warehouse/sql-data-warehouse-manage-distributed-data-skew.md"
      },
      {
        "date": "2022-09-08T13:22:00.000Z",
        "voteCount": 5,
        "content": "Correct, answer is A !"
      },
      {
        "date": "2024-09-14T04:26:00.000Z",
        "voteCount": 1,
        "content": "The only option available for a Dedicated Pool is A. The \"built-in\" always refers to Serverless. PDW_SHOWSPACEUSED is not the best choice to respond the question, but gives a hint about it."
      },
      {
        "date": "2024-09-14T04:25:00.000Z",
        "voteCount": 1,
        "content": "The only option available for a Dedicated Pool is A. The \"built-in\" always refers to Serverless."
      },
      {
        "date": "2024-05-04T04:49:00.000Z",
        "voteCount": 2,
        "content": "I found this question on my exam 30/04/2024, and I put A. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-04-02T02:42:00.000Z",
        "voteCount": 1,
        "content": "Examtopics chooses --&gt; D. Connect to the built-in pool and query sys.dm_pdw_sys_info.\nExamtopics explains --&gt; \"Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data\"\n\nreally, non-sense...\n\nCorrect answer is --&gt; B. Connect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.\nsince you need to use the built-in dedicated SQL pool to launch such a command."
      },
      {
        "date": "2024-04-02T02:44:00.000Z",
        "voteCount": 1,
        "content": "Please notice that \"Use sys.dm_pdw_nodes_db_partition_stats to analyze any skewness in the data\" is the correct solution, but it is not a option, so another solution is PDW_SHOWSPACEUSED which is not the best one, as stated in the official docs"
      },
      {
        "date": "2024-02-27T17:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-25T10:06:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt : A\nOption D suggests querying `sys.dm_pdw_sys_info`, which provides information about the SQL pool nodes and their characteristics, rather than details about data distribution or skew within a table. To investigate data skew specifically, you need to understand how the data is distributed across the distributions or partitions of a table, which is not information that `sys.dm_pdw_sys_info` would provide.\n\nTherefore, while `sys.dm_pdw_sys_info` could give you insights into the overall system, it would not be the right choice for diagnosing data skew within a specific table. For that purpose, `DBCC PDW_SHOWSPACEUSED` is more appropriate, despite it not being a direct indicator of skew, it can still give you an initial indication based on space usage which might suggest further investigation if there are anomalies."
      },
      {
        "date": "2023-12-19T15:44:00.000Z",
        "voteCount": 3,
        "content": "it's dedicated Pool which basically eliminates all the other options except A"
      },
      {
        "date": "2023-08-31T22:06:00.000Z",
        "voteCount": 1,
        "content": "concept repeated , A is correct"
      },
      {
        "date": "2023-06-21T15:37:00.000Z",
        "voteCount": 1,
        "content": "sys.dm_pdw_sys_info actually provides a set of appliance-level counters that reflect overall activity on the appliance. DBCC PDW_SHOWSPACEUSED should be use instead since it displays the number of rows, disk space reserved, and disk space used for a specific table, or for all tables in a Azure Synapse Analytics or Analytics Platform System (PDW) database."
      },
      {
        "date": "2023-05-28T12:39:00.000Z",
        "voteCount": 2,
        "content": "Ok, he did the typo for printing D. It should be \"Connect to the built-in pool and use sys.dm_pdw_nodes_db_partition_stats\""
      },
      {
        "date": "2023-05-26T12:45:00.000Z",
        "voteCount": 4,
        "content": "Read Question 20, Topic 4 - Why examtopics giving 2 different answers for the same question?\n For Q.20, Topic 4 - it says answer is B \nhere for Q.36, Topic 4 -  it says answer is D\n\nExamtopics, you first decide what you want to answer."
      },
      {
        "date": "2023-07-29T04:09:00.000Z",
        "voteCount": 1,
        "content": "Similar to Question 24, Topic 4 - and Q.36, Topic 4 - Answer is different for same question."
      },
      {
        "date": "2023-01-17T03:51:00.000Z",
        "voteCount": 4,
        "content": "-Use DBCC PDW_SHOWSPACEUSED for seeing the skewness (each size in distributions, etc) in a table.\n\n-By using sys.dm_pdw_request_steps table (dynamic management view, DMV) you can see how the operation is really executed and how long it took.\n\nref: https://tsmatz.wordpress.com/2020/10/07/azure-synapse-analytics-sql-dedicated-pool-performance-distribution-hash/"
      },
      {
        "date": "2022-12-03T08:12:00.000Z",
        "voteCount": 3,
        "content": "need to connect Azure Synapse Analytics dedicated SQL pool1 not built-in pool (serverless pool)"
      },
      {
        "date": "2022-12-01T08:19:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\nRead Question 20, Topic 4"
      },
      {
        "date": "2022-10-28T08:08:00.000Z",
        "voteCount": 1,
        "content": "A is the right one!"
      },
      {
        "date": "2022-10-20T05:00:00.000Z",
        "voteCount": 4,
        "content": "Answer is A\n\nA quick way to check for data skew is to use DBCC PDW_SHOWSPACEUSED. The following SQL code returns the number of table rows that are stored in each of the 60 distributions. For balanced performance, the rows in your distributed table should be spread evenly across all the distributions.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93637-exam-dp-203-topic-4-question-37-discussion/",
    "body": "You use Azure Data Lake Storage Gen2.<br>You need to ensure that workloads can use filter predicates and column projections to filter data at the time the data is read from disk.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReregister the Azure Storage resource provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a storage policy that is scoped to a container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReregister the Microsoft Data Lake Store resource provider.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a storage policy that is scoped to a container prefix filter.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister the query acceleration feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-03T00:47:00.000Z",
        "voteCount": 19,
        "content": "E. Register the query acceleration feature.\nD. Create a storage policy that is scoped to a container prefix filter.\n\nTo filter data at the time it is read from disk, you need to use the query acceleration feature of Azure Data Lake Storage Gen2. To enable this feature, you need to register the query acceleration feature in your Azure subscription.\n\nIn addition, you can use storage policies scoped to a container prefix filter to specify which files and directories in a container should be eligible for query acceleration. This can be used to optimize the performance of the queries by only considering a subset of the data in the container."
      },
      {
        "date": "2023-03-13T22:56:00.000Z",
        "voteCount": 5,
        "content": "Option A, reregistering the Azure Storage resource provider, and Option C, reregistering the Microsoft Data Lake Store resource provider, are not necessary to enable filter predicates and column projections in Azure Data Lake Storage Gen2.\n\nOption D, creating a storage policy that is scoped to a container prefix filter, is not a valid option as Azure Data Lake Storage Gen2 does not support storage policies scoped to container prefix filters."
      },
      {
        "date": "2023-08-01T22:04:00.000Z",
        "voteCount": 3,
        "content": "It has, I think"
      },
      {
        "date": "2024-06-27T22:10:00.000Z",
        "voteCount": 1,
        "content": "B. Create a storage policy that is scoped to a container.\nAzure Data Lake Storage Gen2 supports hierarchical namespaces and allows you to define storage policies at various levels, including at the container level.\nE. Register the query acceleration feature.\nAzure Data Lake Storage Gen2 provides a feature called query acceleration, which optimizes query performance by using indexes and metadata caching.\n\nD is not an option because while you can create storage policies scoped to containers or paths within containers, creating a policy scoped to a container prefix filter specifically is not a standard option or action related to optimizing filter predicates and column projections in Azure Data Lake Storage Gen2."
      },
      {
        "date": "2023-12-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "Correction - Chatgpt : DE\nOption A, which suggests re-registering the Azure Storage resource provider, is typically not related to performance tuning or enabling specific features like query acceleration within a storage solution. Re-registering a resource provider is an administrative task that may be necessary when there are issues with the Azure subscription or the resource provider itself, which could affect the provisioning and management of Azure services.\n\nFor the scenario described, where the goal is to filter data at the time it is read from disk to optimize query performance, re-registering the Azure Storage resource provider would not directly impact the ability to use filter predicates and column projections. Instead, enabling features that allow for such optimizations, like query acceleration (E), and setting up policies for how data is stored and accessed (D), are the relevant actions to take."
      },
      {
        "date": "2023-12-25T10:10:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt : AE\n\nOption A, which suggests re-registering the Azure Storage resource provider, is typically not related to performance tuning or enabling specific features like query acceleration within a storage solution. Re-registering a resource provider is an administrative task that may be necessary when there are issues with the Azure subscription or the resource provider itself, which could affect the provisioning and management of Azure services.\n\nFor the scenario described, where the goal is to filter data at the time it is read from disk to optimize query performance, re-registering the Azure Storage resource provider would not directly impact the ability to use filter predicates and column projections. Instead, enabling features that allow for such optimizations, like query acceleration (E), and setting up policies for how data is stored and accessed (D), are the relevant actions to take."
      },
      {
        "date": "2023-11-27T19:03:00.000Z",
        "voteCount": 2,
        "content": "D. Create a storage policy that is scoped to a container prefix filter.\nE. Register the query acceleration feature.\n\nD&amp;E are correct"
      },
      {
        "date": "2023-10-05T20:53:00.000Z",
        "voteCount": 2,
        "content": "Its D &amp; E \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration"
      },
      {
        "date": "2023-10-05T21:01:00.000Z",
        "voteCount": 1,
        "content": "Ignore this chatgpt pointing to B&amp; E"
      },
      {
        "date": "2023-10-05T21:09:00.000Z",
        "voteCount": 1,
        "content": "mod kindly remove both the replies, not relevant."
      },
      {
        "date": "2023-08-31T22:28:00.000Z",
        "voteCount": 1,
        "content": "should be correct"
      },
      {
        "date": "2023-09-09T02:15:00.000Z",
        "voteCount": 1,
        "content": "go to  BE"
      },
      {
        "date": "2023-03-07T12:46:00.000Z",
        "voteCount": 3,
        "content": "D + E = correct"
      },
      {
        "date": "2023-01-15T16:18:00.000Z",
        "voteCount": 3,
        "content": "E. Register the query acceleration feature.\nD. Create a storage policy that is scoped to a container prefix filter."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95047-exam-dp-203-topic-4-question-38-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.<br><br>You need to identify the extent of the data skew in Table1.<br><br>What should you do in Synapse Studio?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and run DBCC PDW_SHOWSPACEUSED.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and run DBCC PDW_SHOWSPACEUSED.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and run DBCC CHECKALLOC.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-28T12:53:00.000Z",
        "voteCount": 21,
        "content": "For the \"Exam Topics\" team: \nTo begin with, your questions vs answers are completely wrong., period. Check your answer for the question#36 in the same page itself!!! Why you are misleading us who are preparing seriously for the exam?? I need an immediate explanation why these questions Q#36 and Q#38 with different answers being at the same question pattern??? Seriously."
      },
      {
        "date": "2023-01-23T16:59:00.000Z",
        "voteCount": 9,
        "content": "This is repeated way too many times."
      },
      {
        "date": "2024-06-27T22:07:00.000Z",
        "voteCount": 1,
        "content": "DBCC PDW_SHOWSPACEUSED is a command that can be executed in the context of the dedicated SQL pool (formerly SQL DW). It provides detailed information about how data is distributed across distributions (similar to shards or segments) in the underlying storage of the dedicated SQL pool.\nBy running this command against Pool1, you can see the distribution of data across the distributions. This includes information about the number of rows per distribution, which helps in identifying data skew.\n\nB is not an option because there's no specific concept of a \"built-in pool\" in Azure Synapse Analytics dedicated SQL pool context"
      },
      {
        "date": "2024-04-02T02:34:00.000Z",
        "voteCount": 1,
        "content": "you need to access the built-in to check out the DMVs and to use DBCCs."
      },
      {
        "date": "2024-01-15T04:20:00.000Z",
        "voteCount": 2,
        "content": "To identify the extent of data skew in Table1, you should connect to Pool1 and run DBCC PDW_SHOWSPACEUSED.\n\nDBCC PDW_SHOWSPACEUSED is a Dynamic Management View (DMV) that provides information about the physical storage of data in a Parallel Data Warehouse (PDW) instance. This includes the distribution of data across partitions and the amount of space used by each partition.\n\nBy running DBCC PDW_SHOWSPACEUSED, you can identify partitions that are storing a disproportionately large amount of data. These partitions may be indicative of data skew."
      },
      {
        "date": "2023-08-31T22:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-09T02:16:00.000Z",
        "voteCount": 1,
        "content": "PDW_SHOWSPACEUSED"
      },
      {
        "date": "2023-05-03T22:28:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2023-01-24T08:39:00.000Z",
        "voteCount": 1,
        "content": "Question 36 from the same topic has the same question but as right answer D. So what is the right answer here?"
      },
      {
        "date": "2023-05-28T12:56:00.000Z",
        "voteCount": 1,
        "content": "Looks like he is misleading us?"
      },
      {
        "date": "2023-01-15T09:56:00.000Z",
        "voteCount": 2,
        "content": "(H)Agreed!"
      },
      {
        "date": "2023-01-15T08:38:00.000Z",
        "voteCount": 4,
        "content": "Its A we need to connect to Pool1"
      },
      {
        "date": "2023-01-15T06:24:00.000Z",
        "voteCount": 5,
        "content": "Connect to Pool1 and run DBCC PDW_SHOWSPACEUSED\n\nAzure Synapse Analytics dedicated SQL pool (formerly known as Azure Synapse Analytics Parallel Data Warehouse) uses a Massively Parallel Processing (MPP) architecture and DBCC PDW_SHOWSPACEUSED is a system stored procedure that can be used to check the distribution of data across the compute nodes. By running this command on Pool1 and specifying the fact table Table1, you can identify the extent of data skew in Table1 and determine if the data is evenly distributed across the compute nodes or if it is skewed towards a specific node"
      },
      {
        "date": "2023-01-13T06:59:00.000Z",
        "voteCount": 4,
        "content": "It's A"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95330-exam-dp-203-topic-4-question-39-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account that contains two folders named Folder1 and Folder2.<br><br>You use Azure Data Factory to copy multiple files from Folder1 to Folder2.<br><br>You receive the following error.<br><br>Operation on target Copy_sks failed: Failure happened on 'Sink' side.<br>ErrorCode=DelimitedTextMoreColumnsThanDefined,<br>'Type=Microsoft.DataTransfer.Common.Snared.HybridDeliveryException,<br>Message=Error found when processing 'Csv/Tsv Format Text' source<br>'0_2020_11_09_11_43_32.avro' with row number 53: found more columns than expected column count 27., Source=Microsoft.DataTransfer.Comnon,'<br><br>What should you do to resolve the error?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Copy activity setting to Binary Copy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLower the degree of copy parallelism.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an explicit mapping.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable fault tolerance to skip incompatible rows."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-06T21:04:00.000Z",
        "voteCount": 15,
        "content": "Correct answer is A. We are just copying files between folders. Selecting binary copy, ADF will not check schema.\nWith D we would discard data\nWith C we would change file contents"
      },
      {
        "date": "2023-04-28T19:27:00.000Z",
        "voteCount": 9,
        "content": "It's tricky.\nNot D, because you don't just throw away data.\nLikely not C, because it doesn't solve for future schema variability. (Avro formats are usually chosen in situations where the schema may evolve over time, because they store both the data and schema in the file itself.)\nA makes most sense, since you're just trying to move files over. Binary preserves everything as-is, and you can read/interpret them as ASCII/UTF-8/whatever later."
      },
      {
        "date": "2023-04-28T19:29:00.000Z",
        "voteCount": 1,
        "content": "Oh! Also, the message says it's trying to process the Avro file as a Csv/Tsv Format Text. That's likely the issue."
      },
      {
        "date": "2024-09-14T04:50:00.000Z",
        "voteCount": 1,
        "content": "The goal is to copy from source to destination. Period.\nBinary is not analysed (images, videos, etc).\nTreat text as binary and you'll ve fine.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide#you-see-a-delimitedtextmorecolumnsthandefined-error-when-copying-a-pipeline"
      },
      {
        "date": "2024-05-13T14:18:00.000Z",
        "voteCount": 2,
        "content": "chatgpt:\nThe best solution to resolve the error is: Add an explicit mapping.\n\nAdding an explicit mapping will ensure that the data from the source file is correctly mapped to the destination columns, thus resolving the error related to the column count mismatch.\n\nChanging the copy activity setting to binary copy may not directly address the root cause of the error, which is a column count mismatch. Binary copy may still encounter the same issue if the source data does not match the expected column count in the destination. Therefore, while changing the copy activity setting to binary copy might be beneficial in some scenarios, it may not effectively resolve the specific error mentioned."
      },
      {
        "date": "2024-04-02T02:24:00.000Z",
        "voteCount": 2,
        "content": "Error explicitly points error to a AVRO file, not CSV. I don't know if it could be possible to have schema mismatch on a AVRO file (I don't think it resembles a CSV at all), but setting up a binary copy activity colves the error if the requirement is just to copy files from A to B. \n\nObviously, it is not the best solution, but it depends on the variety of file formats inside the folder. In case all the files are AVRO formatted, there's a beautiful dataset enabling to properly handle such a format --&gt; https://learn.microsoft.com/en-us/azure/data-factory/format-avro"
      },
      {
        "date": "2024-03-27T04:41:00.000Z",
        "voteCount": 3,
        "content": "Resolution https://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide#you-see-a-delimitedtextmorecolumnsthandefined-error-when-copying-a-pipeline\n\nSelect the Binary Copy option while creating the Copy activity. This way, for bulk copies or migrating your data from one data lake to another, Data Factory won't open the files to read the schema. Instead, Data Factory will treat each file as binary and copy it to the other location."
      },
      {
        "date": "2024-02-20T02:10:00.000Z",
        "voteCount": 2,
        "content": "Adding an explicit mapping allows you to define the exact structure of the data being copied, including the number and names of columns. This ensures that the Copy activity can handle any inconsistencies in the source data and prevent this error from occurring."
      },
      {
        "date": "2024-01-25T09:37:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is A\nhttps://learn.microsoft.com/en-us/azure/data-factory/pipeline-trigger-troubleshoot-guide#you-see-a-delimitedtextmorecolumnsthandefined-error-when-copying-a-pipeline"
      },
      {
        "date": "2024-01-15T06:44:00.000Z",
        "voteCount": 2,
        "content": "Switch the Copy activity to Binary Copy\nBinary Copy can help to resolve this error by copying the data from the source file without any data conversion. This means that the data will be copied as-is, even if it contains more columns than the sink table expects. However, it does not support data transformation.\n\nTo use Binary Copy for this task, you will need to configure the source and sink connections to point to the respective folders. For the source connection, you can use the Delimited Text connector in ADF. For the sink connection, you can also use the Delimited Text connector or another connector that supports the file format in the target folder.\n\nUse an explicit mapping\nIf you want to copy the data from the source file and transform it to match the schema of the sink table, you can use an explicit mapping. This will allow you to map the source columns to the corresponding sink columns."
      },
      {
        "date": "2023-11-22T18:11:00.000Z",
        "voteCount": 7,
        "content": "C. Add an explicit mapping.\n\nExplicit mapping involves specifying the mapping between source and destination columns explicitly. By doing this, you can ensure that each column in the source file is correctly mapped to its corresponding column in the destination file, which helps to address issues related to column count mismatches.\n\nWhile other options may have their use cases, such as changing the copy activity setting to Binary Copy or enabling fault tolerance to skip incompatible rows, adding an explicit mapping (Option C) is specifically designed to handle issues where the source and destination structures do not match in terms of column count or order.\n\nTherefore, in the context of resolving a \"DelimitedTextMoreColumnsThanDefined\" error, adding an explicit mapping is the most appropriate action."
      },
      {
        "date": "2023-11-08T08:50:00.000Z",
        "voteCount": 4,
        "content": "Vote for C\n\nwe have a schema mismatch -) \n\nAlso\n\u2022 Option A: Binary Copy is used for copying non-parseable files like images or videos, \nnot for structured data like CSV."
      },
      {
        "date": "2023-10-05T22:14:00.000Z",
        "voteCount": 3,
        "content": "https://sqlwithmanoj.com/2020/07/29/azure-data-factory-adf-pipeline-failure-found-more-columns-than-expected-column-count-delimitedtextmorecolumnsthandefined/"
      },
      {
        "date": "2023-09-13T01:36:00.000Z",
        "voteCount": 2,
        "content": "A. Change the Copy activity setting to Binary Copy: This would bypass the error by copying the files as-is without interpreting the contents. This method might be suitable if the files are not strictly delimited text files or if you plan to handle the data inconsistency at a later stage or in a different part of the pipeline."
      },
      {
        "date": "2023-08-31T22:22:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-08-18T12:11:00.000Z",
        "voteCount": 2,
        "content": "I would go with Option C- Add an explicit mapping.\nLaying out possible derivations from the question\n1. the actual error says - column mismatch .\n2. Even though the filename is \"filename.avro\" , it could just be a filename, the source file type is CSV/TSV.\nPossible answers\n1. Add an explicit mapping\n2. Enabling Fault tolerance to skip incompatible rows\nI think both would be a possible solution, but to me, skipping incompatible rows is more of  a temporary solution and explicit mapping would be more permanent for this error. I'm also excluding future schema issues that arise after this as there is no information about it."
      },
      {
        "date": "2023-08-15T21:35:00.000Z",
        "voteCount": 2,
        "content": "It says CSV/tsv source but file is avro so A is the answer"
      },
      {
        "date": "2023-06-28T05:08:00.000Z",
        "voteCount": 1,
        "content": "I was pondering a bit about this one, and decided to go with D. Reasoning behind this is because the question was \"how to resolve this error?\" and 100% preservation of source data hasn't been a condition, hence D is the most straightforward."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95331-exam-dp-203-topic-4-question-40-discussion/",
    "body": "A company plans to use Apache Spark analytics to analyze intrusion detection data.<br><br>You need to recommend a solution to analyze network and system activity data for malicious activities and policy violations. The solution must minimize administrative efforts.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T08:39:00.000Z",
        "voteCount": 6,
        "content": "Azure databricks"
      },
      {
        "date": "2024-04-04T21:52:00.000Z",
        "voteCount": 1,
        "content": "I think it is D) Azure Databricks\nhttps://learn.microsoft.com/en-us/azure/databricks/security/privacy/enhanced-security-monitoring"
      },
      {
        "date": "2023-08-31T22:31:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-21T15:49:00.000Z",
        "voteCount": 3,
        "content": "By leveraging Azure Databricks, you can easily perform advanced analytics on the intrusion detection data using Spark's powerful distributed processing capabilities. Databricks provides an interactive and collaborative environment where you can write Spark code, explore and visualize data, and build machine learning models. It also integrates with popular data sources, including Azure Data Lake Storage, for efficient data ingestion and processing."
      },
      {
        "date": "2023-01-14T13:10:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105226-exam-dp-203-topic-4-question-41-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to monitor the database for long-running queries and identify which queries are waiting on resources.<br><br>Which dynamic management view should you use for each requirement? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image285.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image286.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-27T03:29:00.000Z",
        "voteCount": 21,
        "content": "The sys.dm_pdw_lock_waits view is specific to SQL Server and is used to monitor lock waits and lock resources in regular SQL Server environments, not in Azure Synapse Analytics dedicated SQL pools.\n\nMy answers are:\n1. sys.dm_pdw_exec_requests\n2. sys.dm_pdw_waits\nThere is a similar question in the microsoft official practice assessment and the explaination is the following:\nThe sys.dm_pdw_waits view holds information about all wait stats encountered during the execution of a request or query, including locks and waits on a transmission queue"
      },
      {
        "date": "2023-04-29T02:12:00.000Z",
        "voteCount": 8,
        "content": "Its dm_pwd_waits:\nQueries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceTyp\nfrom the official learning path: https://learn.microsoft.com/en-us/training/modules/manage-monitor-data-warehouse-activities-azure-synapse-analytics/6-use-dynamic-management-views-to-identify-troubleshoot-query-performance"
      },
      {
        "date": "2024-07-11T03:25:00.000Z",
        "voteCount": 1,
        "content": "explained here : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor\n1. sys.dm_pdw_exec_requests\n2. sys.dm_pdw_waits"
      },
      {
        "date": "2024-04-02T01:32:00.000Z",
        "voteCount": 1,
        "content": "1. sys.dm_pdw_sql_requests\nIt is more precise about the request made in the question. Please notice that exec_requests contains also all the informations from sql_requests, which is a subset of exec_requests. But we're only interested in SQL queries performances, so even sql_requests is OK. \n\n2. sys.dm_pdw_lock_waits\nwe're specifically referring to time spent in waiting a resource lock, so this is the correct table. It is supported by dedicated SQL pool, but not by serverless."
      },
      {
        "date": "2024-01-15T06:49:00.000Z",
        "voteCount": 2,
        "content": "1. sys.dm_pdw_exec_requests\n\nThis DMV provides information about all active queries in the database, including their query ID, status, execution time, and resource utilization. You can use this DMV to identify long-running queries by filtering for queries that have been running for a long period of time.\n\n2. sys.dm_pdw_waits\n\nThis DMV provides information about the wait stats encountered by active queries. You can use this DMV to identify queries that are waiting for resources, such as CPU, memory, or I/O."
      },
      {
        "date": "2023-11-08T09:02:00.000Z",
        "voteCount": 1,
        "content": "provided answers are correct"
      },
      {
        "date": "2023-08-31T22:33:00.000Z",
        "voteCount": 7,
        "content": "1. sys.dm_pdw_exec_requests\n2.  sys.dm_pdw_waits"
      },
      {
        "date": "2023-06-21T15:51:00.000Z",
        "voteCount": 2,
        "content": "\"Queries in the Suspended state can be queued due to a large number of active running queries. These queries also appear in the sys.dm_pdw_waits waits query with a type of UserConcurrencyResourceType.\""
      },
      {
        "date": "2023-04-04T16:51:00.000Z",
        "voteCount": 3,
        "content": "correct\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/monitoring-with-dmvs?view=azuresql"
      },
      {
        "date": "2023-04-04T16:57:00.000Z",
        "voteCount": 21,
        "content": "box 1: is correct\nbox 2: sys.dm_pdw_waits\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-waits-transact-sql?view=aps-pdw-2016-au7"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/109869-exam-dp-203-topic-4-question-42-discussion/",
    "body": "You have an Azure Data Factory pipeline named pipeline1 that includes a Copy activity named Copy1. Copy1 has the following configurations:<br><br>\u2022\tThe source of Copy1 is a table in an on-premises Microsoft SQL Server instance that is accessed by using a linked service connected via a self-hosted integration runtime.<br>\u2022\tThe sink of Copy1 uses a table in an Azure SQL database that is accessed by using a linked service connected via an Azure integration runtime.<br><br>You need to maximize the amount of compute resources available to Copy1. The solution must minimize administrative effort.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale out the self-hosted integration runtime.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale up the data flow runtime of the Azure integration runtime and scale out the self-hosted integration runtime.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale up the data flow runtime of the Azure integration runtime."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-25T22:04:00.000Z",
        "voteCount": 17,
        "content": "I would answer A.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\nCopying between a cloud data source and a data source in a private network: if either the source or sink linked service points to a self-hosted IR, the copy activity is executed on the self-hosted IR."
      },
      {
        "date": "2023-05-21T21:18:00.000Z",
        "voteCount": 13,
        "content": "Why not B? \n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\nAzure integration runtime provides the native compute to move data between cloud data stores in a secure, reliable, and high-performance manner. You can set how many data integration units to use on the copy activity, and the compute size of the Azure IR is elastically scaled up accordingly without requiring you to explicitly adjust the size of the Azure Integration Runtime.\n\nFor high availability and scalability, you can scale out the self-hosted IR by associating the logical instance with multiple on-premises machines in active-active mode."
      },
      {
        "date": "2024-08-05T14:17:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-07-05T04:43:00.000Z",
        "voteCount": 3,
        "content": "To maximize the amount of compute resources available to the Copy activity (Copy1) in your Azure Data Factory pipeline while minimizing administrative effort, the most appropriate action would be:\n\nA. Scale out the self-hosted integration runtime.\n\nExplanation:\nSelf-Hosted Integration Runtime (SHIR): Since the source of Copy1 is an on-premises SQL Server, the data needs to be transferred using the self-hosted integration runtime. Scaling out the self-hosted integration runtime means adding more nodes to the integration runtime cluster, which increases the number of parallel connections and throughput for data transfer from the on-premises SQL Server to Azure.\n\nAzure Integration Runtime: The sink is an Azure SQL Database accessed via an Azure integration runtime. Azure integration runtime is managed by Azure, and it automatically scales based on the load. Therefore, you generally do not need to manually scale up the Azure integration runtime."
      },
      {
        "date": "2024-06-23T00:05:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-04-27T12:24:00.000Z",
        "voteCount": 1,
        "content": "I chose B option"
      },
      {
        "date": "2024-04-26T05:34:00.000Z",
        "voteCount": 1,
        "content": "A is the answer; as BillMyI quoted microsoft - copy is happening on self-hosted IR"
      },
      {
        "date": "2024-04-26T05:35:00.000Z",
        "voteCount": 2,
        "content": "besides it is a copy activity, there is now Data Flow"
      },
      {
        "date": "2024-04-17T21:28:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2024-03-31T07:06:00.000Z",
        "voteCount": 1,
        "content": "The question explicitly requires to minimize the administrative cost. Every time there's such a request, the solution must be as much automated as possible. Self-Hosted has to be managed by some IT department, so it implies effort ad then administrative costs. Instead, simply scaling up the data flow runtime does not require infrastructural costs, since Azure provides such a machine type in the cloud, so we don't have to handle our machine in the company. \n\nJust a detail: dataflows, even with a giant cluster, have very poor performances in practical situations. Just scaling up the IR could be not enough..."
      },
      {
        "date": "2024-01-15T07:03:00.000Z",
        "voteCount": 3,
        "content": "es, the answer is still the same. Scaling up the data flow runtime of the Azure integration runtime is still the best option to maximize the amount of compute resources available to Copy1.\n\nThe reasoning is that the copy activity is configured to use the self-hosted integration runtime because the source linked service is connected to a self-hosted integration runtime. This means that the copy activity will be executed on the self-hosted integration runtime, and scaling up the Azure integration runtime will have no effect on the copy activity's performance.\n\nScaling up the Azure integration runtime would only help if the source and sink linked services were both connected to the Azure integration runtime. In that case, scaling up the Azure integration runtime would provide more processing power for the copy activity."
      },
      {
        "date": "2024-01-09T04:46:00.000Z",
        "voteCount": 1,
        "content": "Integration runtime is hosted on the location of the sink for copy activity if I am not mistaken."
      },
      {
        "date": "2024-01-02T10:45:00.000Z",
        "voteCount": 2,
        "content": "chat gpt \nC. Scale up the data flow runtime of the Azure integration runtime.\n\nExplanation:\n\nIn Azure Data Factory, when you're copying data between different data stores, the compute resources used by the Copy activity are mainly determined by the data flow involved in the copying process. Azure Data Factory provides two types of integration runtimes:"
      },
      {
        "date": "2023-12-25T10:37:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt:\nThe self-hosted integration runtime can be scaled out by adding additional nodes, which allows it to process more activities simultaneously. This is a way to increase compute resources without a significant administrative overhead since it involves configuration changes rather than physical infrastructure changes.\n\nOptions B and C involve scaling up the data flow runtime, which is not applicable in this context since the Copy activity does not use data flow runtime; it uses the integration runtime for data movement. Therefore, the correct answer to maximize compute resources for Copy1 with minimal administrative effort is:\n\nA. Scale out the self-hosted integration runtime."
      },
      {
        "date": "2023-11-22T18:17:00.000Z",
        "voteCount": 2,
        "content": "For maximizing the amount of compute resources available to the Copy activity in Azure Data Factory, you should consider scaling up the data flow runtime of the Azure integration runtime.\n\nOption C. Scale up the data flow runtime of the Azure integration runtime."
      },
      {
        "date": "2023-08-31T23:03:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#self-hosted-ir-compute-resource-and-scaling\nA should be corrrect."
      },
      {
        "date": "2023-08-31T23:09:00.000Z",
        "voteCount": 4,
        "content": "correct it , C ,Option A, \u201cScale out the self-hosted integration runtime,\u201d is not the best solution to maximize the amount of compute resources available to Copy1 because it would not minimize administrative effort. Scaling out the self-hosted integration runtime would involve adding more nodes to the runtime pool, which would require allocating new virtual machines and registering new nodes on the integration runtime. This process can be time-consuming and would require additional administrative effort1."
      },
      {
        "date": "2023-08-15T21:39:00.000Z",
        "voteCount": 3,
        "content": "if either the source or sink linked service points to a self-hosted IR, the copy activity is executed on the self-hosted IR."
      },
      {
        "date": "2023-07-08T07:13:00.000Z",
        "voteCount": 3,
        "content": "My answer is A due to the precedence criteria among Integration runtimes selection when source and sink linked services are linked to different IRs, as described here \nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108789-exam-dp-203-topic-4-question-43-discussion/",
    "body": "You are designing a solution that will use tables in Delta Lake on Azure Databricks.<br><br>You need to minimize how long it takes to perform the following:<br><br>\u2022\tQueries against non-partitioned tables<br>\u2022\tJoins on non-partitioned columns<br><br>Which two options should you include in the solution? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe clone command",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZ-Ordering\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Spark caching",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic file pruning (DFP)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-09T01:30:00.000Z",
        "voteCount": 9,
        "content": "Seems correct:\nhttps://learn.microsoft.com/en-us/azure/databricks/optimizations/dynamic-file-pruning\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/data-skipping"
      },
      {
        "date": "2023-06-29T15:10:00.000Z",
        "voteCount": 8,
        "content": "Dynamic file pruning, can significantly improve the performance of many queries on Delta Lake tables. Dynamic file pruning is especially efficient for non-partitioned tables, or for joins on non-partitioned columns. The performance impact of dynamic file pruning is often correlated to the clustering of data so consider using Z-Ordering to maximize the benefit."
      },
      {
        "date": "2024-08-17T03:39:00.000Z",
        "voteCount": 1,
        "content": "By sorting the data files based on one or more columns, Z-Ordering can significantly improve the performance of queries that filter on those columns.\nCaching in Apache Spark allows frequently accessed data to be stored in memory, reducing the time it takes to read the data for subsequent operations. This can be particularly useful for speeding up joins and repeated queries against non-partitioned tables, as the data is readily available without having to be read from disk repeatedly."
      },
      {
        "date": "2024-03-31T06:58:00.000Z",
        "voteCount": 1,
        "content": "https://www.databricks.com/blog/2020/04/30/faster-sql-queries-on-delta-lake-with-dynamic-file-pruning.html"
      },
      {
        "date": "2023-08-31T22:53:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108874-exam-dp-203-topic-4-question-44-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account named account1 that contains a container named container1.<br><br>You plan to create lifecycle management policy rules for container1.<br><br>You need to ensure that you can create rules that will move blobs between access tiers based on when each blob was accessed last.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure object replication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable access time tracking\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the hierarchical namespace"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T08:29:00.000Z",
        "voteCount": 9,
        "content": "Answer is correct.\nCustomers stores huge amount of data in Azure blob storage. Sometimes this data is accessed frequently and other times infrequently. Last access time tracking integrates with the lifecycle of Azure blob storage to allow automatic tiering and deletion of data based on when individual blobs are accessed last."
      },
      {
        "date": "2024-03-31T06:37:00.000Z",
        "voteCount": 1,
        "content": "in addition --&gt; https://azure.microsoft.com/en-us/updates/azure-blob-storage-last-access-time-tracking-now-generally-available/"
      },
      {
        "date": "2023-08-31T23:11:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/109167-exam-dp-203-topic-4-question-45-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br><br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br><br>You need to monitor resource utilization to determine the source of the performance issues.<br><br>Which metric should you monitor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU percentage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-31T06:26:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C since the question is referringo to commonly used queries, so, in a good scenario, we'd expect that the cache HITs are high. CPU could make sense, but cache is more relevant."
      },
      {
        "date": "2024-03-20T22:05:00.000Z",
        "voteCount": 1,
        "content": "correct ans c"
      },
      {
        "date": "2023-08-31T23:12:00.000Z",
        "voteCount": 2,
        "content": "Repeated"
      },
      {
        "date": "2023-06-08T04:57:00.000Z",
        "voteCount": 4,
        "content": "Repeat Questons"
      },
      {
        "date": "2023-05-13T09:59:00.000Z",
        "voteCount": 4,
        "content": "Answer is C, and it's a repeated question"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/109009-exam-dp-203-topic-4-question-46-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure data factory named DF1 that contains 10 pipelines.<br><br>The pipelines are executed hourly by using a schedule trigger. All activities are executed on an Azure integration runtime.<br><br>You need to ensure that you can identify trends in queue times across the pipeline executions and activities The solution must minimize administrative effort.<br><br>How should you configure the Diagnostic settings for DF1? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image306.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image307.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-11T17:54:00.000Z",
        "voteCount": 11,
        "content": "1. To identify trends in queue times, you should focus on the Pipeline activity run logs rather than the Pipeline run logs. \nPipeline activity run logs allows you to track the queue times for individual activities within the pipeline. \nWhile Pipeline run logs logs may provide some information about queue times, they do not provide granular details for each activity within the pipeline."
      },
      {
        "date": "2023-05-18T00:26:00.000Z",
        "voteCount": 7,
        "content": "so the provided answer is correct."
      },
      {
        "date": "2024-09-02T07:09:00.000Z",
        "voteCount": 1,
        "content": "The answers are correct. This documentation outlines all the options and describes them clearly. The provided solutions minimize administrative effort, as the goal of these logs is to facilitate further analysis. Sending them to Log Analytics offers a ready-to-use tool for this purpose. In contrast, other solutions store the data as files, such as JSON or Blobs, requiring additional steps to analyze, such as using copy activity in Data Factory to prepare them for further analysis.\n\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/essentials/activity-log?tabs=powershell#send-to-log-analytics-workspace"
      },
      {
        "date": "2024-03-31T06:21:00.000Z",
        "voteCount": 3,
        "content": "collect pipeline activity runs --&gt; the question asks such a granularity, so it must be included\nsend to log analytics workspace --&gt; this minimized administrative effort in storing logs"
      },
      {
        "date": "2024-03-31T06:30:00.000Z",
        "voteCount": 2,
        "content": "there's another better reason for answer 1: we're interested in measuring how much time activities spend waiting in queue, metric that cannot be analyzed using simple pipelines runs. We must have details about each activity, hence the first, \"collect pipeline activity runs\", is correct."
      },
      {
        "date": "2023-12-25T10:43:00.000Z",
        "voteCount": 1,
        "content": "Wrong activity runs dont minimize efforts, chatgpt :\nTo minimize administrative effort while still being able to identify trends in queue times across pipeline executions and activities, you should collect:\n\n- Pipeline runs log: This log provides a high-level overview of each pipeline execution, which is sufficient for identifying trends in queue times without the need for the more granular detail that would come from collecting activity runs logs.\n\nAnd send to:\n\n- Log Analytics workspace: This will allow for centralized logging and analytics, which is effective for trend analysis with minimal administrative effort. \n\nSo the settings should be:\n\nCollect: Pipeline runs log\nSend to: Log Analytics workspace"
      },
      {
        "date": "2024-02-07T16:41:00.000Z",
        "voteCount": 6,
        "content": "when will they start deleting all comments marked \u201cchatgpt\u201d... constant aggregation of delirium"
      },
      {
        "date": "2023-10-14T07:52:00.000Z",
        "voteCount": 1,
        "content": "Did any of you completed DP-203 exam here"
      },
      {
        "date": "2024-01-02T11:02:00.000Z",
        "voteCount": 1,
        "content": "what about you i am preparing for now"
      },
      {
        "date": "2023-08-31T23:14:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111527-exam-dp-203-topic-4-question-47-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br><br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br><br>You need to monitor resource utilization to determine the source of the performance issues.<br><br>Which metric should you monitor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Warehouse Units (DWU) used",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-08T04:57:00.000Z",
        "voteCount": 16,
        "content": "Repeated 4 times"
      },
      {
        "date": "2024-05-09T21:05:00.000Z",
        "voteCount": 2,
        "content": "Seems like this question is time travelling. saw it few pages back couple times already and here it is again. lol."
      },
      {
        "date": "2024-01-23T07:37:00.000Z",
        "voteCount": 4,
        "content": "Question is being repeated multiple times, don't know why people select anything other than B. If it's something to do with \"Commonly used queries\" then it's 99% of the time cache"
      },
      {
        "date": "2024-01-20T08:08:00.000Z",
        "voteCount": 1,
        "content": "The correct metric to monitor is C. Data Warehouse Units (DWU) used.\n\nDWUs are the units of resource consumption in Azure Synapse Analytics. Each DWU represents a certain amount of processing power, memory, and storage capacity. When users run queries, they consume DWUs.\n\nIn this case, users report slow performance when they run commonly used queries. This suggests that the performance issues are likely due to resource contention. If the commonly used queries are consuming a large amount of DWUs, then there may not be enough resources available for other queries to run efficiently.\n\nBy monitoring DWU usage, you can identify which queries are consuming the most resources and take steps to optimize them or to scale your Azure Synapse Analytics workspace."
      },
      {
        "date": "2023-11-08T09:28:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: C\n\nWhy not A ?\nWhile the DWU percentage can provide insights into whether your workload is more CPU or IO intensive, the DWU used can provide a more direct measure of the overall resource utilization1. This can be more helpful in identifying if resource contention (i.e., your workload is demanding more resources than are available) is causing the slow performance of commonly used queries\n\nR: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-concept-resource-utilization-query-activity"
      },
      {
        "date": "2023-09-13T01:45:00.000Z",
        "voteCount": 4,
        "content": "Monitoring DWU used (Option C) can certainly be part of a comprehensive approach to diagnosing the performance issues, focusing on the cache hit percentage (Option B) might offer a more targeted way to address the specific problem described in the scenario."
      },
      {
        "date": "2023-09-09T02:23:00.000Z",
        "voteCount": 2,
        "content": "Repeated"
      },
      {
        "date": "2023-08-31T23:14:00.000Z",
        "voteCount": 1,
        "content": "Repeated"
      },
      {
        "date": "2023-09-14T23:57:00.000Z",
        "voteCount": 2,
        "content": "says the guy with the wrong answer xD\nI know you know the answer. I know you chose C by mistake which you don't know.\nAAAHHHH how do I know so much yet so little!!!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117436-exam-dp-203-topic-4-question-48-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image339.png\"><br><br>You need to ensure that you can run Spark notebooks in ws1.The solution must ensure that you can retrieve secrets from kv1 by using UAMI1.<br><br>What should you do? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image340.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image341.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-06T06:14:00.000Z",
        "voteCount": 33,
        "content": "In the Azure Portal --&gt; Add a role-based access control (RBAC) role to kv1\nIn Synapse Studio --&gt; Create a linked service to kv1"
      },
      {
        "date": "2024-01-23T07:39:00.000Z",
        "voteCount": 2,
        "content": "Yep, that's how we do it at our company."
      },
      {
        "date": "2023-11-06T03:25:00.000Z",
        "voteCount": 10,
        "content": "I'm reporting this comment on purpose so the ExamTopics team can review the answer, as requested below.\n\nExamTopics team, please, do one of the following:\n1) clearly explain and provide the support link or material for the answer you chose, or\n2) correct the answers selected.\nAs stated in previous answers in this discussion, the logic says that first you have to add the role in RBAC to kv1 and then associate the kv1 source as an linked service in synapse."
      },
      {
        "date": "2024-09-02T08:32:00.000Z",
        "voteCount": 1,
        "content": "Azure Portal: RBAC\n\"Authorization determines which operations the caller can perform. Authorization in Key Vault uses Azure role-based access control (Azure RBAC) on management plane and either Azure RBAC or Azure Key Vault access policies on data plane.\"\nSource: https://learn.microsoft.com/en-us/azure/key-vault/general/security-features\n\nSynapse: Linked Services\nRefer to the Steps number 3: \"Create a linked service pointing to your Azure Key Vault.\"\nSource: https://learn.microsoft.com/en-us/azure/data-factory/store-credentials-in-key-vault#steps"
      },
      {
        "date": "2024-04-10T13:09:00.000Z",
        "voteCount": 1,
        "content": "The answer is incorrect. Should be:\n1. Add a role-based access control (RBAC) role to kv\n2. Create a linked service to kv1"
      },
      {
        "date": "2024-03-31T06:02:00.000Z",
        "voteCount": 2,
        "content": "ExamTopics solution is inverted... you cannot set a linked service from portal, just a detail. \n\nCorrect answers: \nfrom portal --&gt; RBAC on kv1\nfrom synapse studio --&gt; linked service on kv1 (this works exactly as in DataFactory)"
      },
      {
        "date": "2024-02-28T08:32:00.000Z",
        "voteCount": 2,
        "content": "Answer is not Correct."
      },
      {
        "date": "2023-12-25T10:48:00.000Z",
        "voteCount": 3,
        "content": "Chatgpt:\nTo enable Spark notebooks in Azure Synapse Analytics workspace `ws1` to retrieve secrets from Azure Key Vault `kv1` using the user-assigned managed identity `UAMI1`, you need to set the appropriate permissions and configurations:\n\nIn the Azure portal:\n- Add a role-based access control (RBAC) role to `kv1`. Assign the user-assigned managed identity `UAMI1` the necessary role, like \"Key Vault Secrets User\", to retrieve secrets from the Key Vault.\n\nIn Synapse Studio:\n- Create a linked service to `kv1`. This linked service should use the user-assigned managed identity `UAMI1` for authentication, allowing the Spark notebooks to use this linked service to access Key Vault secrets.\n\nSo the selections should be:\n\nIn the Azure portal: Add a role-based access control (RBAC) role to `kv1`.\nIn Synapse Studio: Create a linked service to `kv1`."
      },
      {
        "date": "2023-09-06T01:05:00.000Z",
        "voteCount": 2,
        "content": "The boxes should be reversed for the answers as it does not make sense currently"
      },
      {
        "date": "2023-08-31T23:17:00.000Z",
        "voteCount": 3,
        "content": "Box1. Add a role-based access control (RBAC) role to kv\nBox2. Create a linked service to kv1"
      },
      {
        "date": "2023-08-05T11:02:00.000Z",
        "voteCount": 4,
        "content": "In the Azure portal:\nAdd a role-based access control (RBAC) role to kv1 - You need to assign the 'Key Vault Secrets User' role to UAMI1 on kv1. This will grant the managed identity the necessary permissions to retrieve secrets from Key Vault.\n\nIn Synapse Studio:\nCreate a linked service to kv1 - You need to create a linked service in Azure Synapse Studio to connect to kv1. The linked service will use the User-Assigned Managed Identity (UAMI1) to authenticate to the Azure Key Vault."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/118108-exam-dp-203-topic-4-question-49-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Factory pipeline shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image342.png\"><br><br>The execution log for the first pipeline run is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image343.png\"><br><br>The execution log for the second pipeline run is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image344.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image345.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image346.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-08T19:29:00.000Z",
        "voteCount": 14,
        "content": "No yes no"
      },
      {
        "date": "2023-11-06T03:34:00.000Z",
        "voteCount": 11,
        "content": "ExamTopics team, please, do one of the following:\n1) clearly explain and provide the support link or material for the answer you chose, or\n2) correct the answers selected.\nAs stated in answers below in this discussion, the response are N-Y-N or N-N-N, but there's nothing that can point us to the answer given in the resolution."
      },
      {
        "date": "2024-09-14T02:52:00.000Z",
        "voteCount": 1,
        "content": "The common sense leans towards answers being either no/no/no or no/yes/no. Here\u2019s my analysis:\n\nRetry Information: The panel only shows the general status, not specific retry counts. You can't confirm retry settings without checking the Details or activity settings. So, the answer is no if asked if retry is greater than 0.\n\nwaitOnCompletion Property: Comparing activity times suggests waitOnCompletion might be set in the second activity, not the first, which makes sense since the first activity can't have dependencies. Hence, the answer is no for the first activity.\n\nSkipped Activities: The first activity isn\u2019t skipped due to dependency but may be skipped based on parameters or conditions like \"skip if copied less than 3 hours ago.\" Pipelines don\u2019t have retries, only activities do. So, the answer remains no."
      },
      {
        "date": "2024-04-16T13:55:00.000Z",
        "voteCount": 1,
        "content": "No, no, no"
      },
      {
        "date": "2024-03-31T05:56:00.000Z",
        "voteCount": 1,
        "content": "no --&gt; in the first run, pipline failed, so there's no retry \nyes --&gt; it is a default option for nested pipelines; it doesn't seem to be changed here \nno --&gt; in the second run, pipeline has been restarted from failed activities, causing the skip on the first activity, which succeeded in the last pipeline run"
      },
      {
        "date": "2023-08-14T11:35:00.000Z",
        "voteCount": 7,
        "content": "No, No, No\n\nThe Retry Property is not set to one for Web_GetIP: Otherwise, we would see a retry of that activity in the first run.\n\nwaitOnCompletion property is not set to true:  In the second run, Exec_COPY_BLOB takes as long as in the first one, despite being skipped. So, it could not have been waiting for the pipeline that it had triggered to complete.\n\nExec_COPY_BLOB cannot be skipped due to a pipeline dependency since it is the first activity in the pipeline. Most likely, its activity state was manually set to \u201askipped\u2018."
      },
      {
        "date": "2024-01-02T11:13:00.000Z",
        "voteCount": 7,
        "content": "i think waitOnCompletion is yes bcz only after 11 sec next activity get started if it was not set to true all first and second activity both will be started at same time"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117269-exam-dp-203-topic-4-question-50-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1.<br><br>You need to identify the extent of the data skew in Table1.<br><br>What should you do in Synapse Studio?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and query sys.dm_pdw_nodes_db_partition_stats.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and run DBCC PDW_SHOWSPACEUSED.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to Pool1 and query sys.dm_pdw_node_status.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect to the built-in pool and query sys.dm_pdw_sys_info."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-03T11:14:00.000Z",
        "voteCount": 5,
        "content": "I think it is B , same question in the previous pages"
      },
      {
        "date": "2024-09-14T02:50:00.000Z",
        "voteCount": 1,
        "content": "Another explanation to support option B as the most appropriate:\n\nTo analyze data skew specifically in a dedicated SQL pool, you should use sys.dm_pdw_nodes_db_partition_stats. The option says \"connect to the built-in\", but built-in is Serverless and this view is not available for querying from Serverless, even against a Dedicated pool.\n\nOption B (DBCC PDW_SHOWSPACEUSED) is a valid alternative but provides less detailed information about data skew compared to sys.dm_pdw_nodes_db_partition_stats. It is meant to show the space used, as the command implies, but may give an idea about skew."
      },
      {
        "date": "2024-07-18T07:51:00.000Z",
        "voteCount": 1,
        "content": "The built in pool is serverless, we cannot use those A's DMV  is not supported by  serverless pools, https://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-db-partition-stats-transact-sql?toc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Ftoc.json&amp;bc=%2Fazure%2Fsynapse-analytics%2Fsql-data-warehouse%2Fbreadcrumb%2Ftoc.json&amp;view=azure-sqldw-latest&amp;preserve-view=true"
      },
      {
        "date": "2024-07-16T05:19:00.000Z",
        "voteCount": 1,
        "content": "Explanation:\nTo identify the extent of data skew in a table within an Azure Synapse Analytics dedicated SQL pool, the DBCC PDW_SHOWSPACEUSED command is most appropriate. This command provides information about space usage and data distribution, which can help identify skew. You need to connect to the specific pool (Pool1) to run this command."
      },
      {
        "date": "2024-03-11T01:35:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-11-28T05:31:00.000Z",
        "voteCount": 2,
        "content": "Answere B \nTopic 4 #36 and #38"
      },
      {
        "date": "2023-08-31T23:25:00.000Z",
        "voteCount": 3,
        "content": "REPETED,B is correct"
      },
      {
        "date": "2023-08-20T01:44:00.000Z",
        "voteCount": 3,
        "content": "It is indeed B:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#determine-if-the-table-has-data-skew"
      },
      {
        "date": "2024-04-01T23:59:00.000Z",
        "voteCount": 1,
        "content": "...no. Please read better the resource: \n\"Create the view dbo.vTableSizes that is shown in the Tables overview article.\"\n\nthe article --&gt; https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview#table-size-queries\n\nThe cited view indeed uses dm_pdw_nodes_db_partition_stats, so A is the correct answer, not B."
      },
      {
        "date": "2024-04-02T00:00:00.000Z",
        "voteCount": 1,
        "content": "\"However, using DBCC commands can be quite limiting. Dynamic management views (DMVs) show more detail than DBCC commands.\""
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117507-exam-dp-203-topic-4-question-51-discussion/",
    "body": "You have several Azure Data Factory pipelines that contain a mix of the following types of activities:<br><br>\u2022\tPower Query<br>\u2022\tNotebook<br>\u2022\tCopy<br>\u2022\tJar<br><br>Which two Azure services should you use to debug the activities? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T17:40:00.000Z",
        "voteCount": 7,
        "content": "The Copy activity is native to Azure Data Factory, so you would use Azure Data Factory to debug it.\n\nThe Notebook and Jar activities are related to Databricks jobs, so you would use Azure Databricks to debug them.\n\nPower Query is more associated with data wrangling and transformation, and while it can be used in various services, in the context of Azure Data Factory, you'd likely be debugging within Azure Data Factory or Azure Synapse Analytics. However, given the other activities listed, Azure Data Factory is the more probable choice for this scenario.\n\nTherefore, the correct answers are:\n\nB. Azure Data Factory\nE. Azure Databricks"
      },
      {
        "date": "2024-03-11T01:38:00.000Z",
        "voteCount": 1,
        "content": "Data Factory for Copy Activity and Databricks for JAR"
      },
      {
        "date": "2024-02-02T13:26:00.000Z",
        "voteCount": 1,
        "content": "REPETED"
      },
      {
        "date": "2023-12-25T11:00:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt BE :\nTo debug the various types of activities in Azure Data Factory pipelines, you would use:\n\nB. Azure Data Factory - This is the primary environment where you orchestrate and monitor the pipelines and activities including Power Query, Copy, and others.\n\nE. Azure Databricks - For debugging activities related to Notebooks and Jar files, Azure Databricks provides an environment to run and debug these types of activities.\n\nThese services offer the necessary tools and environments to debug the specified activities within Azure Data Factory pipelines."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/119633-exam-dp-203-topic-4-question-52-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br><br>The company must be able to monitor the devices in real-time.<br><br>You need to design the solution.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics cloud job using Azure Portal\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-29T13:54:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-23T07:41:00.000Z",
        "voteCount": 1,
        "content": "Repeted question, always ASA"
      },
      {
        "date": "2023-08-31T23:31:00.000Z",
        "voteCount": 3,
        "content": "repeted"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129339-exam-dp-203-topic-4-question-53-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named pool1.<br><br>You need to perform a monthly audit of SQL statements that affect sensitive data. The solution must minimize administrative effort.<br><br>What should you include in the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tworkload management",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsensitivity labels\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Defender for SQL"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-05T14:45:00.000Z",
        "voteCount": 1,
        "content": "Sensitivity Label\n\nHere the explanation \nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql#what-is-dc"
      },
      {
        "date": "2024-08-31T04:02:00.000Z",
        "voteCount": 1,
        "content": "sensitivity labels doesn't allow to audit statements that affect sensitive data, Microsoft Defender for SQL does\nref: https://learn.microsoft.com/en-us/azure/defender-for-cloud/defender-for-sql-introduction#advanced-threat-protection"
      },
      {
        "date": "2024-07-29T17:47:00.000Z",
        "voteCount": 2,
        "content": "D. Microsoft Defender for SQL\n\nHere\u2019s why:\n\nMicrosoft Defender for SQL: This service provides advanced threat protection and security capabilities for your SQL databases. It includes features like vulnerability assessments, advanced threat detection, and auditing capabilities. By enabling Microsoft Defender for SQL, you can automatically monitor and audit SQL statements that affect sensitive data, ensuring compliance and security with minimal administrative effort1.\nOther Options:\nWorkload Management (A): This is more about managing and optimizing resource allocation and performance rather than auditing SQL statements.\nSensitivity Labels (B): These are used to classify and protect sensitive data, but they do not provide auditing capabilities.\nDynamic Data Masking \u00a9: This feature helps to obfuscate sensitive data in query results but does not audit SQL statements."
      },
      {
        "date": "2024-06-28T22:11:00.000Z",
        "voteCount": 1,
        "content": "Sensitivity Labels"
      },
      {
        "date": "2024-04-10T13:28:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-24T13:00:00.000Z",
        "voteCount": 2,
        "content": "Microsoft Defender for SQL is a tool that provides advanced SQL security capabilities, including vulnerability assessment, threat protection, and detection of anomalous activities that may indicate a security threat to your databases\u200b\u200b. However, for the specific need of auditing SQL statements affecting sensitive data, sensitivity labels are the more directly applicable feature\u200b\u200b."
      },
      {
        "date": "2024-01-15T04:28:00.000Z",
        "voteCount": 1,
        "content": "Sensitivity labels will not help you with audit. It will secure your data but not help you with the SQL statements\n\nMicrosoft Defender for Azure SQL helps you discover and mitigate potential database vulnerabilities and alerts you to anomalous activities that might be an indication of a threat to your databases.\n\nVulnerability assessment: Scan databases to discover, track, and remediate vulnerabilities. Learn more about vulnerability assessment.\nThreat protection: Receive detailed security alerts and recommended actions based on SQL Advanced Threat Protection to provide to mitigate threats. Learn more about SQL Advanced Threat Protection.\nhttps://learn.microsoft.com/en-us/azure/defender-for-cloud/defender-for-sql-introduction"
      },
      {
        "date": "2024-01-09T06:36:00.000Z",
        "voteCount": 1,
        "content": "sensitive in ques= sensitive in options"
      },
      {
        "date": "2023-12-25T11:11:00.000Z",
        "voteCount": 1,
        "content": "Correct, chatgpt\nTo conduct a monthly audit of SQL statements impacting sensitive data in Azure Synapse Analytics while minimizing administrative effort, include the use of Sensitivity Labels to classify sensitive data and utilize Microsoft Defender for SQL for advanced threat detection and monitoring of related activities. These tools will aid in governance and provide alerts for any suspicious access or manipulation of sensitive data. For a comprehensive approach to setting up these features, refer to the official Microsoft documentation for guidance.https://learn.microsoft.com/en-us/azure/synapse-analytics/"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129340-exam-dp-203-topic-4-question-54-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br><br>The company must be able to monitor the devices in real-time.<br><br>You need to design the solution.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Edge application using Microsoft Visual Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-02T09:02:00.000Z",
        "voteCount": 1,
        "content": "Those questions are the same, but they are different versions of it. The previous ones had different options for answers."
      },
      {
        "date": "2024-04-10T13:29:00.000Z",
        "voteCount": 1,
        "content": "it's ok"
      },
      {
        "date": "2024-01-23T07:43:00.000Z",
        "voteCount": 1,
        "content": "Repeated, always ASA"
      },
      {
        "date": "2024-01-09T06:37:00.000Z",
        "voteCount": 1,
        "content": "IOT PORTAL in ques== Azure Stream Analytics Edge application using Microsoft Visual Studio"
      },
      {
        "date": "2023-12-25T11:13:00.000Z",
        "voteCount": 2,
        "content": "Like 52..."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125512-exam-dp-203-topic-4-question-55-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure data factory.<br><br>You execute a pipeline that contains an activity named Activity1. Activity1 produces the following output.<br><br><img src=\"https://img.examtopics.com/dp-203/image351.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image352.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image353.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-11-06T03:41:00.000Z",
        "voteCount": 21,
        "content": "At last an answer from Examtopics team that makes sense with the problem stated and is not based on an alternate wacky multiverse where all the rules and good practices of Azure are broken."
      },
      {
        "date": "2023-12-25T11:22:00.000Z",
        "voteCount": 5,
        "content": "Correct, chatgpt :\n1. Activity1 appears to be a Copy activity given the \"dataRead,\" \"dataWritten,\" and \"copyDuration\" fields.\n2. Activity1 does not use a self-hosted integration runtime; it uses an Azure integration runtime as indicated by \"AutoResolveIntegrationRuntime.\"\n3. The data factory is connected to Microsoft Purview, as evidenced by the \"reportLineageToPurview\" section indicating a successful status.\nHence, the correct answers are:\n- Yes, Activity1 is a Copy activity.\n- No, Activity1 is not executed using a self-hosted integration runtime.\n- Yes, the data factory that executed the pipeline is connected to Microsoft Purview."
      },
      {
        "date": "2024-04-16T13:58:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-02-28T08:58:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-09T06:39:00.000Z",
        "voteCount": 1,
        "content": "yes totally correct i have exam on 11 jan 2023 will post the experience on home page of the examtopics where all questions begin"
      },
      {
        "date": "2024-01-04T10:29:00.000Z",
        "voteCount": 3,
        "content": "Alternate wacky multiverse. Lessond Learned from this comment."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125634-exam-dp-203-topic-4-question-56-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br><br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br><br>You need to monitor resource utilization to determine the source of the performance issues.<br><br>Which metric should you monitor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Warehouse Units (DWU) used"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-23T07:44:00.000Z",
        "voteCount": 1,
        "content": "B, question is here multiple times"
      },
      {
        "date": "2024-01-09T06:39:00.000Z",
        "voteCount": 1,
        "content": "b is correct ans"
      },
      {
        "date": "2023-12-14T16:03:00.000Z",
        "voteCount": 3,
        "content": "this question repeat like 6 or 7 times....."
      },
      {
        "date": "2023-11-24T11:26:00.000Z",
        "voteCount": 2,
        "content": "correct answer B, (repeat to many times)"
      },
      {
        "date": "2023-11-08T09:47:00.000Z",
        "voteCount": 1,
        "content": "correct answer A"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130350-exam-dp-203-topic-4-question-57-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics workspace and a user named User1.<br><br>You need to ensure that User1 can review the Azure Synapse Analytics database templates from the gallery. The solution must follow the principle of least privilege.<br><br>Which role should you assign to User1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Contributor.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSynapse Administrator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSynapse Contributor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSynapse User\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-04T04:49:00.000Z",
        "voteCount": 3,
        "content": "I found this question on my exam 30/04/2024, and I put D. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-01-20T09:33:00.000Z",
        "voteCount": 2,
        "content": "The Synapse User role is the lowest-privileged role that allows users to view Azure Synapse Analytics database templates from the gallery. This role does not allow users to create or manage databases, but it does allow them to explore and review existing templates\n\nStorage Blob Data Contributor: This role allows users to read and write data to Azure Blob Storage. It does not allow users to manage Azure Synapse Analytics databases or templates.\nSynapse Contributor: This role allows users to create and manage Azure Synapse Analytics databases and artifacts. It is a more powerful role than Synapse User, but it is not necessary for simply viewing database templates.\nSynapse Administrator: This role has full control over the Azure Synapse Analytics workspace. It is an excessive role for simply viewing database templates."
      },
      {
        "date": "2024-01-20T09:27:00.000Z",
        "voteCount": 1,
        "content": "The Synapse User role is the lowest-privileged role that allows users to view Azure Synapse Analytics database templates from the gallery. This role does not allow users to create or manage databases, but it does allow them to explore and review existing templates\n\nStorage Blob Data Contributor: This role allows users to read and write data to Azure Blob Storage. It does not allow users to manage Azure Synapse Analytics databases or templates.\nSynapse Contributor: This role allows users to create and manage Azure Synapse Analytics databases and artifacts. It is a more powerful role than Synapse User, but it is not necessary for simply viewing database templates.\nSynapse Administrator: This role has full control over the Azure Synapse Analytics workspace. It is an excessive role for simply viewing database templates."
      },
      {
        "date": "2024-01-09T06:45:00.000Z",
        "voteCount": 1,
        "content": "CHAT GPT\nSynapse User (Option D): This role is designed for users who need to interact with the Synapse workspace and its resources. It includes permissions to view templates, explore resources, and perform user-level actions without providing excessive administrative privileges."
      },
      {
        "date": "2024-01-08T05:53:00.000Z",
        "voteCount": 3,
        "content": "At least Synapse User role permissions are required for exploring a lake database template from Gallery.\n    Synapse Administrator, or Synapse Contributor permissions are required on the Synapse workspace for creating a lake database.\n    Storage Blob Data Contributor permissions are required on data lake when using the create table From data lake option.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/create-lake-database-from-lake-database-templates"
      },
      {
        "date": "2024-01-14T10:26:00.000Z",
        "voteCount": 1,
        "content": "I agree .The Answer is D: Synapse User role"
      },
      {
        "date": "2024-01-04T15:47:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is D. Synapse User as it has less priviledges than contributor.\n\nI am unaware what roles can and cant view templates, some one will have to test this. But as you can just google the templates, i assume its the least priveledged role \n\nSee link showing priveledges:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-synapse-rbac-roles#:~:text=Can%20read%20and%20write%20artifacts%0ACan%20view%20saved%20notebook%20and%20pipeline%20output%0ACan%20do%20all%20actions%20on%20Spark%20activities%0ACan%20view%20Spark%20pool%20logs"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130412-exam-dp-203-topic-4-question-58-discussion/",
    "body": "You have a Log Analytics workspace named la1 and an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 sends logs to la1.<br><br>You need to identify whether a recently executed query on Pool1 used the result set cache.<br><br>What are two ways to achieve the goal? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the sys.dm_pdw_sql_requests dynamic management view in Pool1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the sys.dm_pdw_exec_requests dynamic management view in Pool1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Monitor hub in Synapse Studio.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the AzureDiagnostics table in la1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview the sys.dm_pdw_request_steps dynamic management view in Pool1."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-02T09:36:00.000Z",
        "voteCount": 1,
        "content": "I'll go with B and E because they're both mentioned in this documentation from Microsoft: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching#key-commands"
      },
      {
        "date": "2024-07-05T12:03:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nTo identify whether a recently executed query on Pool1 used the result set cache, you can use the following methods:\n\nB. Review the sys.dm_pdw_exec_requests dynamic management view in Pool1.\n\nD. Review the AzureDiagnostics table in la1."
      },
      {
        "date": "2024-04-26T08:45:00.000Z",
        "voteCount": 1,
        "content": "B and E provides information about query caching"
      },
      {
        "date": "2024-04-20T02:37:00.000Z",
        "voteCount": 1,
        "content": "The Monitor hub in Synapse Studio does not provide information to verify cache utilization."
      },
      {
        "date": "2024-01-13T00:37:00.000Z",
        "voteCount": 1,
        "content": "The sys.dm_pdw_exec_requests dynamic management view provides details about currently or recently executed requests, and the Monitor hub in Synapse Studio can offer insights into the query execution and caching."
      },
      {
        "date": "2024-01-05T09:24:00.000Z",
        "voteCount": 3,
        "content": "Correct\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/monitoring/how-to-monitor-using-azure-monitor\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/set-result-set-caching-transact-sql?view=azure-sqldw-latest\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-pdw-exec-requests-transact-sql?view=aps-pdw-2016-au7"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130413-exam-dp-203-topic-4-question-59-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Sales.Orders. Sales.Orders contains a column named SalesRep.<br><br>You plan to implement row-level security (RLS) for Sales.Orders.<br><br>You need to create the security policy that will be used to implement RLS. The solution must ensure that sales representatives only see rows for which the value of the SalesRep column matches their username.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br><img src=\"https://img.examtopics.com/dp-203/image377.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image378.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-26T08:48:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-02-28T09:32:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-01-13T00:40:00.000Z",
        "voteCount": 4,
        "content": "-- Create schema\nCREATE SCHEMA Security;\nGO\n\n-- Create function\nCREATE FUNCTION Security.tv_securitypredicate (@SalesRep AS nvarchar(50))\nRETURNS TABLE\nWITH SCHEMABINDING\nAS\nRETURN\n    SELECT 1 AS tv_securitypredicate_result\n    WHERE @SalesRep = USER_NAME();\nGO\n\n-- Create security policy\nCREATE SECURITY POLICY SalesFilter\nADD FILTER PREDICATE Security.tv_securitypredicate(SalesRep)\nON Sales.Orders\nWITH (STATE = ON);"
      },
      {
        "date": "2024-01-05T09:28:00.000Z",
        "voteCount": 3,
        "content": "Correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130651-exam-dp-203-topic-4-question-60-discussion/",
    "body": "You have an Azure data factory named DF1. DF1 contains a single pipeline that is executed by using a schedule trigger.<br><br>From Diagnostics settings, you configure pipeline runs to be sent to a resource-specific destination table in a Log Analytics workspace.<br><br>You need to run KQL queries against the table.<br><br>Which table should you query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tADFPipelineRun\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tADFTriggerRun",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tADFActivityRun",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzureDiagnostics"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-25T11:40:00.000Z",
        "voteCount": 7,
        "content": "The answer is not D because the workspace was set to Resource-specific. See doc below.\nI'm torn between A (PipelineRun) vs B (TriggerRun) because the wording is so generic. However, since they mentioned the scheduling of the pipeline, I would lean on B.\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/essentials/resource-logs#send-to-log-analytics-workspace"
      },
      {
        "date": "2024-02-29T09:38:00.000Z",
        "voteCount": 4,
        "content": "I would say it's A, because in the ADFTriggerRun it would only contain information on when was the pipeline executed and which pipeline, the question asks for \"pipeline runs\", but yes it's very generic, because by pipeline runs you can also think that it's about when it was executed, but since it doesn't implicitely say so, I'd go with A"
      },
      {
        "date": "2024-02-24T13:40:00.000Z",
        "voteCount": 6,
        "content": "Remember that you are setting \"pipeline runs\" logs from diagnostics settings to be sent to a Log Analytics workspace, not the \"trigger runs\" or \"activity runs\" logs.\n\nAzure Diagnostics stores resource logs for Azure services that use Azure Diagnostics mode. Resource logs describe the internal operation of Azure resources. \nAzure services that use resource-specific mode store data in a table specific to that service and do not use the AzureDiagnostics table\n\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/azurediagnostics"
      },
      {
        "date": "2024-09-05T15:30:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/azure-monitor/essentials/resource-logs#resource-specific"
      },
      {
        "date": "2024-07-29T18:01:00.000Z",
        "voteCount": 3,
        "content": "A. ADFPipelineRun\n\nHere\u2019s why:\n\nADFPipelineRun: This table contains detailed information about the execution of your ADF pipelines, including start and end times, status, and other relevant metrics. It\u2019s the go-to table for querying pipeline run details1."
      },
      {
        "date": "2024-07-16T05:14:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2024-07-05T18:36:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/azure-monitor/reference/tables/adfpipelinerun"
      },
      {
        "date": "2024-02-16T03:52:00.000Z",
        "voteCount": 3,
        "content": "Chat GPT answer: \n\nTo run KQL queries against the pipeline runs sent to a Log Analytics workspace, you should query the table that corresponds to the pipeline runs. Since you configured pipeline runs to be sent to a resource-specific destination table in the Log Analytics workspace, you should query the table corresponding to pipeline runs.\n\nTherefore, the correct table to query is:\n\nA. ADFPipelineRun"
      },
      {
        "date": "2024-01-20T09:56:00.000Z",
        "voteCount": 4,
        "content": "ADFTriggerRun\n\nThe ADFTriggerRun table stores logs related to trigger runs, which are the events that initiate pipeline executions. This table includes the trigger name, pipeline name, run ID, start time, end time, status, and a message about the trigger run. This table is useful for understanding how triggers are firing and how pipelines are being executed."
      },
      {
        "date": "2024-01-20T03:23:00.000Z",
        "voteCount": 1,
        "content": "Should be D."
      },
      {
        "date": "2024-01-15T04:45:00.000Z",
        "voteCount": 1,
        "content": "Per the other two comments, answer should be D. Adding a voting comment so it can show on \"show answer\""
      },
      {
        "date": "2024-01-13T00:44:00.000Z",
        "voteCount": 2,
        "content": "When you configure pipeline runs to be sent to a Log Analytics workspace in Azure Data Factory, the data is typically stored in the \"AzureDiagnostics\" table. Therefore, you should query the \"AzureDiagnostics\" table to retrieve information about pipeline runs.\n\nSo, the correct answer is:\n\nD. AzureDiagnostics"
      },
      {
        "date": "2024-02-10T15:24:00.000Z",
        "voteCount": 1,
        "content": "It has already been stated that the destination table is sent to 'resource specific', not azure diagnostic"
      },
      {
        "date": "2024-02-10T15:34:00.000Z",
        "voteCount": 1,
        "content": "I would choose Activity Run as a preference as it would show detailed activities of the run.\nHowever, I believe this question may seem to concern with the trigger rather than the pipeline.\n\n https://learn.microsoft.com/en-us/azure/data-factory/monitor-schema-logs-eventsactivities."
      },
      {
        "date": "2024-01-11T17:34:00.000Z",
        "voteCount": 1,
        "content": "Answer is D\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/essentials/resource-logs\nall data from any diagnostic setting is collected in the AzureDiagnostics table"
      },
      {
        "date": "2024-01-08T13:19:00.000Z",
        "voteCount": 1,
        "content": "IMHO activity run should show all the details"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130418-exam-dp-203-topic-4-question-61-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool named sqlpool1 that contains a table named Sales1.<br><br>Each row in the Sales table contains regional sales data and a field that lists the username of a sales analyst.<br><br>You need to configure row-level security (RLS) to ensure that the analysts can view only the rows containing their respective data.<br><br>What should you do? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image379.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image380.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-29T12:58:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2024-01-12T01:30:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-01-05T09:41:00.000Z",
        "voteCount": 4,
        "content": "Correct\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130419-exam-dp-203-topic-4-question-62-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse workspace named WS1 and an Azure Monitor action group named Group1. WS1 has a dedicated SQL pool.<br><br>You plan to archive monitoring data for integration activity runs.<br><br>You need to ensure that you can configure custom alerts based on the archived data that will execute Group1. The solution must minimize administrative effort.<br><br>Which diagnostic setting should you select?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend to Log Analytics workspace\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tArchive to a storage account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStream to an event hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend to a partner solution"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T04:23:00.000Z",
        "voteCount": 1,
        "content": "no doubt A"
      },
      {
        "date": "2024-04-29T03:20:00.000Z",
        "voteCount": 1,
        "content": "Send to Log Analytics workspace"
      },
      {
        "date": "2024-02-01T15:20:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/sentinel/detect-threats-custom"
      },
      {
        "date": "2024-01-05T09:45:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130420-exam-dp-203-topic-4-question-63-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1.<br><br>You have the queries shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image381.png\"><br><br>You are evaluating whether to enable result set caching for Pool1.<br><br>Which query results will be cached if result set caching is enabled?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery1 only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery2 only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery1 and Query2 only\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery1 and Query3 only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery1, Query2, and Query3 only"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T00:47:00.000Z",
        "voteCount": 9,
        "content": "correct \n\nWhat's not cached\nOnce result set caching is turned ON for a database, results are cached for all queries until the cache is full, except for these queries:\n\nQueries with built-in functions or runtime expressions that are non-deterministic even when there\u2019s no change in base tables\u2019 data or query. For example, DateTime.Now(), GetDate().\nQueries using user defined functions\nQueries using tables with row level security\nQueries returning data with row size larger than 64KB\nQueries returning large data in size (&gt;10GB)"
      },
      {
        "date": "2024-01-05T09:49:00.000Z",
        "voteCount": 6,
        "content": "Correct.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching#whats-not-cached"
      },
      {
        "date": "2024-07-20T18:13:00.000Z",
        "voteCount": 1,
        "content": "must be deterministic"
      },
      {
        "date": "2024-03-11T02:56:00.000Z",
        "voteCount": 2,
        "content": "Correct, queries that don't depend on the User"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130422-exam-dp-203-topic-4-question-64-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics workspace name workspace1, workspace1 contains an Azure Synapse Analytics dedicated SQL pool named Pool1.<br><br>You create a mapping data flow in an Azure Synapse pipeline that writes data to Pool1.<br><br>You execute the data flow and capture the execution information.<br><br>You need to identify how long it takes to write the data to Pool1.<br><br>Which metric should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe rows written",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe sink processing time\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe transformation processing time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe post processing time"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T04:27:00.000Z",
        "voteCount": 1,
        "content": "Sink processing time: This metric measures the time taken to write the data to the sink, which in this case is Pool1. It directly answers the question about how long the write operation takes."
      },
      {
        "date": "2024-02-26T07:08:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-05T09:57:00.000Z",
        "voteCount": 4,
        "content": "Correct:\n\nEach transformation stage includes a total time for that stage to complete with each partition execution time totaled together. When you select the Sink, you see \"Sink Processing Time\". This time includes the total of the transformation time plus the I/O time it took to write your data to your destination store. The difference between the Sink Processing Time and the total of the transformation is the I/O time to write the data.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-monitoring#total-sink-processing-time-vs-transformation-processing-time"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/142853-exam-dp-203-topic-4-question-65-discussion/",
    "body": "You have an Azure data factory named DF1. DF1 contains a pipeline that has five activities.<br><br>You need to monitor queue times across the activities by using Log Analytics.<br><br>What should you do in DF1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect DF1 to a Microsoft Purview account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a diagnostic setting that sends activity runs to a Log Analytics workspace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auto refresh for the Activity Logs Insights workbook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a diagnostic setting that sends pipeline runs to a Log Analytics workspace."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-17T04:29:00.000Z",
        "voteCount": 1,
        "content": "B. Add a diagnostic setting that sends activity runs to a Log Analytics workspace: This is the correct option because it ensures that detailed logs of each activity run, including queue times, are sent to Log Analytics. From there, you can use Log Analytics to monitor and analyze these logs."
      },
      {
        "date": "2024-06-23T17:14:00.000Z",
        "voteCount": 3,
        "content": "B is correct: the table ADFActivityRun serves the same purpose. All information about each activity can be found in this table."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/142249-exam-dp-203-topic-4-question-66-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1.<br><br>You need to monitor Pool1. The solution must ensure that you capture the start and end times of each query completed in Pool1.<br><br>Which diagnostic setting should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSql Requests",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest Steps",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDms Workers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExec Requests\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-18T02:46:00.000Z",
        "voteCount": 1,
        "content": "\nSql Requests captures the start and end times of each query executed in the pool, as well as other relevant details like the status, duration, and resource consumption of the query.\nThis diagnostic setting allows you to track query performance, including the time a query starts and completes, which is essential for auditing and performance monitoring."
      },
      {
        "date": "2024-08-17T22:23:00.000Z",
        "voteCount": 1,
        "content": "This diagnostic setting captures detailed information about each SQL request executed in the Synapse dedicated SQL pool, including the start and end times, duration, and status of the queries. This is the most appropriate setting for monitoring query execution details."
      },
      {
        "date": "2024-07-15T00:54:00.000Z",
        "voteCount": 1,
        "content": "Information about SQL requests or queries in an Azure Synapse dedicated SQL pool."
      },
      {
        "date": "2024-06-10T02:30:00.000Z",
        "voteCount": 2,
        "content": "D seems correct, I was wondering if B could also be correct but apparently it only gives start and end times for the steps of a query and not for the query itself. Following link for reference :\nhttps://learn.microsoft.com/en-us/azure/azure-monitor/reference/supported-logs/microsoft-synapse-workspaces-sqlpools-logs"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  },
  {
    "topic": 4,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/143011-exam-dp-203-topic-4-question-67-discussion/",
    "body": "You have an Azure Stream Analytics job named Job1.<br><br>The metrics of Job1 from the last hour are shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image391.png\"><br><br>The late arrival tolerance for Job1 is set to five seconds.<br><br>You need to optimize Job1.<br><br>Which two actions achieve the goal? Each correct answer presents a complete solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of SUs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParallelize the query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResolve errors in output processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResolve errors in input processing."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-15T00:57:00.000Z",
        "voteCount": 1,
        "content": "no error to resolve"
      },
      {
        "date": "2024-07-05T19:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct for sure. C and D are wrong, because there was no error to resolve."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "4"
  }
]