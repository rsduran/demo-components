[
  {
    "topic": 8,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18340-exam-dp-201-topic-8-question-1-discussion/",
    "body": "HOTSPOT -<br>You need to design the data loading pipeline for Planning Assistance.<br>What should you recommend? To answer, drag the appropriate technologies to the correct locations. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0013700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0013700002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: SqlSink Table -<br>Sensor data must be stored in a Cosmos DB named treydata in a collection named SensorData<br><br>Box 2: Cosmos Bulk Loading -<br>Use Copy Activity in Azure Data Factory to copy data from and to Azure Cosmos DB (SQL API).<br>Scenario: Data from the Sensor Data collection will automatically be loaded into the Planning Assistance database once a week by using Azure Data Factory. You must be able to manually trigger the data load process.<br>Data used for Planning Assistance must be stored in a sharded Azure SQL Database.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-13T03:09:00.000Z",
        "voteCount": 77,
        "content": "The data pipeline is moving data from Cosmos DB to SQL Database. So, the 1st box should be Cosmos Bulking Loading and 2nd box should be SQLSink Table."
      },
      {
        "date": "2020-07-27T09:16:00.000Z",
        "voteCount": 22,
        "content": "why Cosmos Bulk Loading when you are reading from Cosmos DB? should be Cosmos Query"
      },
      {
        "date": "2021-06-21T14:49:00.000Z",
        "voteCount": 2,
        "content": "I AGREE ITS ONCE A WEEK  SO \n\nBULK LOADING and SQL DATABASE"
      },
      {
        "date": "2020-06-13T00:56:00.000Z",
        "voteCount": 15,
        "content": "ADF uses Cosmos DB as the source. Shouldn't be Cosmos query?\nCheck: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-source"
      },
      {
        "date": "2020-06-13T00:58:00.000Z",
        "voteCount": 52,
        "content": "Box1: Cosmos query\nBox2: SQLSink Table"
      },
      {
        "date": "2021-04-29T13:36:00.000Z",
        "voteCount": 3,
        "content": "Cosmos Query in Box1 and SqlSink Table in Box2"
      },
      {
        "date": "2021-03-09T21:14:00.000Z",
        "voteCount": 3,
        "content": "I will go with Bulk Loading because of this it happens once a week:\nData from the Sensor Data collection will automatically be loaded into the Planning Assistance database once a week by using Azure Data Factory. You must be able to manually trigger the data load process."
      },
      {
        "date": "2021-01-28T05:03:00.000Z",
        "voteCount": 2,
        "content": "it is Query for the first one. check this: https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db#azure-cosmos-db-sql-api-as-source"
      },
      {
        "date": "2021-01-17T05:52:00.000Z",
        "voteCount": 3,
        "content": "I can't find the adf \"Cosmos Bulk Loading\" anywhere. There is a bulk loading option for loading data INTO cosmos but i can't find the otherway anywhere"
      },
      {
        "date": "2021-01-14T14:00:00.000Z",
        "voteCount": 4,
        "content": "Second box = SQLSink Table\nFirst box: since sensor data are coming in pretty much like a stream, a change feed would be a good solution. If Cosmos query is used, how would you deal with the fact that the endpoint of the data moves every few seconds? \nhttps://devblogs.microsoft.com/cosmosdb/change-feed-unsung-hero-of-azure-cosmos-db/"
      },
      {
        "date": "2021-01-09T20:23:00.000Z",
        "voteCount": 2,
        "content": "So is it Cosmos Bulk loading or cosmos query?"
      },
      {
        "date": "2020-10-01T05:13:00.000Z",
        "voteCount": 7,
        "content": "Box1: Cosmos query\nBox2: SQLSink Table"
      },
      {
        "date": "2020-09-07T14:05:00.000Z",
        "voteCount": 2,
        "content": "Why not cosmos change feed for box 1?"
      },
      {
        "date": "2020-11-08T06:12:00.000Z",
        "voteCount": 2,
        "content": "Sensor data has a time stamp in it, and as it is only inserts, this time stamp will not change. So I guess you could use it, but a regular query would be enough. So my guess for box 1 is Cosmos query."
      },
      {
        "date": "2020-07-15T05:18:00.000Z",
        "voteCount": 3,
        "content": "Sensor data from Cosmos DB &gt; ADF is juist a Source or a Sink (table)  \nso the left answer is correct \nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "8"
  },
  {
    "topic": 8,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8590-exam-dp-201-topic-8-question-2-discussion/",
    "body": "You need to design the runtime environment for the Real Time Response system.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGeneral Purpose nodes without the Enterprise Security package",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemory Optimized Nodes without the Enterprise Security package",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemory Optimized nodes with the Enterprise Security package",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGeneral Purpose nodes with the Enterprise Security package"
    ],
    "answer": "B",
    "answerDescription": "Scenario: You must maximize the performance of the Real Time Response system.",
    "votes": [],
    "comments": [
      {
        "date": "2019-11-19T11:01:00.000Z",
        "voteCount": 58,
        "content": "\"Azure Active Directory must be used for all services where it is available\" so Enterprise Security package is needed.\nCorrect answer is C"
      },
      {
        "date": "2020-05-17T01:36:00.000Z",
        "voteCount": 3,
        "content": "But it also says\nAzure resource costs must be minimized where possible.\nEnterprise Security package incur additional costs on the nodes\nMay be they meant using Azure AD if readily available with the service"
      },
      {
        "date": "2020-06-10T14:24:00.000Z",
        "voteCount": 2,
        "content": "good point! it's  not clear and really confusing"
      },
      {
        "date": "2020-06-01T06:54:00.000Z",
        "voteCount": 3,
        "content": "Reference: https://docs.microsoft.com/fr-fr/azure/hdinsight/domain-joined/hdinsight-security-overview"
      },
      {
        "date": "2020-12-17T17:41:00.000Z",
        "voteCount": 18,
        "content": "why so many answers are incorrect in this exam"
      },
      {
        "date": "2020-12-22T05:22:00.000Z",
        "voteCount": 15,
        "content": "I am really frustrated"
      },
      {
        "date": "2021-06-17T06:15:00.000Z",
        "voteCount": 1,
        "content": "Wondering how do I correct myself since so many answers are not correct?"
      },
      {
        "date": "2021-06-20T04:53:00.000Z",
        "voteCount": 1,
        "content": "what is the correct answer then"
      },
      {
        "date": "2021-04-29T13:38:00.000Z",
        "voteCount": 2,
        "content": "C. Memory Optimized nodes with the Enterprise Security package"
      },
      {
        "date": "2021-04-26T07:39:00.000Z",
        "voteCount": 1,
        "content": "it says \"script performance will be limited by available memory\", so shouldn't it be \"general purpose node\" ?"
      },
      {
        "date": "2021-02-05T11:58:00.000Z",
        "voteCount": 3,
        "content": "The correct answer seems to be C, because of the requirement: Privacy and security policy -\nAzure Active Directory must be used for all services where it is available.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/domain-joined/hdinsight-security-overview\n\nAuthentication\nEnterprise Security Package from HDInsight provides Active Directory-based authentication, multi-user support, and role-based access control. The Active Directory integration is achieved through the use of Azure Active Directory Domain Services. With these capabilities, you can create an HDInsight cluster joined to an Active Directory domain. Then configure a list of employees from the enterprise who can authenticate to the cluster.\n\nWith this setup, enterprise employees can sign in to the cluster nodes by using their domain credentials. They can also use their domain credentials to authenticate with other approved endpoints. Like Apache Ambari Views, ODBC, JDBC, PowerShell, and REST APIs to interact with the cluster."
      },
      {
        "date": "2020-12-10T05:37:00.000Z",
        "voteCount": 1,
        "content": "C looks the most correct option to me"
      },
      {
        "date": "2020-11-02T12:53:00.000Z",
        "voteCount": 3,
        "content": "azure databricks does not need enterprise security package like hdinsight. it is single sign on with Azure AD. Azure databricks is being used here to run pypspak script on cosmos db sensor collection data to to notify if emergency dispatch service is required."
      },
      {
        "date": "2020-10-27T06:30:00.000Z",
        "voteCount": 1,
        "content": "It says that the sensors should only write access to the data storage, meaning there is some sort of service principle authentication and authorization in place. Therefore, to let the Spark Cluster access the data, you need to have the Enterprise Security package installed."
      },
      {
        "date": "2020-08-09T15:30:00.000Z",
        "voteCount": 4,
        "content": "C: Open-source Apache Hadoop relies on the Kerberos protocol for authentication and security. Therefore, HDInsight cluster nodes with Enterprise Security Package (ESP) are joined to a domain that's managed by Azure AD DS. Kerberos security is configured for the Hadoop components on the cluster.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/domain-joined/apache-domain-joined-architecture"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "8"
  },
  {
    "topic": 8,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18341-exam-dp-201-topic-8-question-3-discussion/",
    "body": "HOTSPOT -<br>You need to ensure that emergency road response vehicles are dispatched automatically.<br>How should you design the processing system? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0013900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0013900002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box1: API App -<br><img src=\"/assets/media/exam-media/03774/0014000001.jpg\" class=\"in-exam-image\"><br>1. Events generated from the IoT data sources are sent to the stream ingestion layer through Azure HDInsight Kafka as a stream of messages. HDInsight Kafka stores streams of data in topics for a configurable of time.<br>2. Kafka consumer, Azure Databricks, picks up the message in real time from the Kafka topic, to process the data based on the business logic and can then send to Serving layer for storage.<br>3. Downstream storage services, like Azure Cosmos DB, Azure Synapse Analytics, or Azure SQL DB, will then be a data source for presentation and action layer.<br>4. Business analysts can use Microsoft Power BI to analyze warehoused data. Other applications can be built upon the serving layer as well. For example, we can expose APIs based on the service layer data for third party uses.<br><br>Box 2: Cosmos DB Change Feed -<br>Change feed support in Azure Cosmos DB works by listening to an Azure Cosmos DB container for any changes. It then outputs the sorted list of documents that were changed in the order in which they were modified.<br>The change feed in Azure Cosmos DB enables you to build efficient and scalable solutions for each of these patterns, as shown in the following image:<br><img src=\"/assets/media/exam-media/03774/0014100001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/bs-cyrl-ba/azure/architecture/example-scenario/data/realtime-analytics-vehicle-iot?view=azurermps-4.4.1",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-13T03:26:00.000Z",
        "voteCount": 74,
        "content": "As the Aid Dispatcher is an Azure Logic App, so the data pipe line is from Cosmos DB via Databricks to Azure Logic App. The right answer should be, Box 1 - Cosmos DB Change Feed. Box 2 - Auzre Logic App Connector."
      },
      {
        "date": "2020-07-14T07:55:00.000Z",
        "voteCount": 3,
        "content": "where has Auzre Logic App Connector in Azure Databrick"
      },
      {
        "date": "2020-07-14T07:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/connectors/apis-list#managed-api-connectors"
      },
      {
        "date": "2021-05-19T08:13:00.000Z",
        "voteCount": 2,
        "content": "Indeed, according to logo on the right (sink) it's 'Logic App'."
      },
      {
        "date": "2020-06-05T22:27:00.000Z",
        "voteCount": 18,
        "content": "I think first box should be Cosmos DB change, and second box should be API call"
      },
      {
        "date": "2021-06-09T13:36:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is :  Box 1 - Cosmos DB Change Feed. Box 2 - Azure API.\n* You can check the answer about Box 2 in the second fig."
      },
      {
        "date": "2021-02-15T05:05:00.000Z",
        "voteCount": 2,
        "content": "I think for triggering Logic up we need to use WebHook\nhttps://docs.microsoft.com/en-us/connectors/custom-connectors/create-webhook-trigger"
      },
      {
        "date": "2021-01-29T05:43:00.000Z",
        "voteCount": 1,
        "content": "Change Feed and then API app. https://docs.microsoft.com/en-us/azure/cosmos-db/change-feed"
      },
      {
        "date": "2020-10-10T05:08:00.000Z",
        "voteCount": 2,
        "content": "The first box should be Cosmos Change feed and the second box should be webhook. Logic apps cannot be triggered by API calls"
      },
      {
        "date": "2020-10-16T10:43:00.000Z",
        "voteCount": 2,
        "content": "Logic Apps can be triggered by ADF http activity"
      },
      {
        "date": "2021-03-27T22:49:00.000Z",
        "voteCount": 2,
        "content": "But we have databricks instead of adf in that case ... So I think the webhook should be okay"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "8"
  },
  {
    "topic": 8,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19681-exam-dp-201-topic-8-question-4-discussion/",
    "body": "DRAG DROP -<br>You need to ensure that performance requirements for Backtrack reports are met.<br>What should you recommend? To answer, drag the appropriate technologies to the correct locations. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0014200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0014200002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Cosmos DB indexes -<br>The report for Backtrack must execute as quickly as possible.<br>You can override the default indexing policy on an Azure Cosmos container, this could be useful if you want to tune the indexing precision to improve the query performance or to reduce the consumed storage.<br><br>Box 2: Cosmos DB TTL -<br>This solution reports on all data related to a specific vehicle license plate. The report must use data from the SensorData collection. Users must be able to filter vehicle data in the following ways:<br>\u2711 vehicles on a specific road<br>\u2711 vehicles driving above the speed limit<br>Note: With Time to Live or TTL, Azure Cosmos DB provides the ability to delete items automatically from a container after a certain time period. By default, you can set time to live at the container level and override the value on a per-item basis. After you set the TTL at a container or at an item level, Azure Cosmos DB will automatically remove these items after the time period, since the time they were last modified.<br>Incorrect Answers:<br>Cosmos DB stored procedures: Stored procedures are best suited for operations that are write heavy. When deciding where to use stored procedures, optimize around encapsulating the maximum amount of writes possible. Generally speaking, stored procedures are not the most efficient means for doing large numbers of read operations so using stored procedures to batch large numbers of reads to return to the client will not yield the desired benefit.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/index-policy https://docs.microsoft.com/en-us/azure/cosmos-db/time-to-live<br>Design data processing solutions",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-10T06:01:00.000Z",
        "voteCount": 11,
        "content": "Indexing for reporting is right\nTTL for security is correct\nOther listed options don't tie in"
      },
      {
        "date": "2021-06-29T00:49:00.000Z",
        "voteCount": 1,
        "content": "BackTrack - The solution must report changes in real time.\nBox1 - would this be Cosmos DB change feed?"
      },
      {
        "date": "2021-06-25T21:17:00.000Z",
        "voteCount": 1,
        "content": "Indexing - For retrieving reports fast\nTTL - To purge data once retention period is over"
      },
      {
        "date": "2020-05-05T10:32:00.000Z",
        "voteCount": 4,
        "content": "What does TTL have to do with security?"
      },
      {
        "date": "2020-05-15T06:41:00.000Z",
        "voteCount": 13,
        "content": "Might have to do with this line\n\n\"Data must only be stored for seven years\"\n\nsee\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/time-to-live"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "8"
  }
]