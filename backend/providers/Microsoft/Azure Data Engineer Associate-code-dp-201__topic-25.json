[
  {
    "topic": 25,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16734-exam-dp-201-topic-25-question-1-discussion/",
    "body": "What should you recommend as a batch processing solution for Health Interface?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure CycleCloud",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks"
    ],
    "answer": "B",
    "answerDescription": "Scenario: ADatum identifies the following requirements for the Health Interface application:<br>Support a more scalable batch processing solution in Azure.<br>Reduce the amount of time it takes to add data from new hospitals to Health Interface.<br>Data Factory integrates with the Azure Cosmos DB bulk executor library to provide the best performance when you write to Azure Cosmos DB.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db<br>Design data processing solutions",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-16T03:49:00.000Z",
        "voteCount": 64,
        "content": "How come batch processing using Azure Stream analytics why not Azure data bricks? seems like wrong answer this should be D."
      },
      {
        "date": "2021-04-13T07:04:00.000Z",
        "voteCount": 4,
        "content": "it's actually ADF as per their explanation, they marked it wrong. Bricks would also do I guess, there's little that ADF can do that databricks can't, if anything."
      },
      {
        "date": "2021-04-13T07:06:00.000Z",
        "voteCount": 1,
        "content": "ok, ADF can use copy data from on-premise source, spark, which is used by ADF data fows and data bricks can't do that"
      },
      {
        "date": "2020-04-09T12:23:00.000Z",
        "voteCount": 34,
        "content": "Technology choices for batch processing are\n1. Azure Synapse Analytics\n2. Azure HDInsight\n3. Azure Data Lake Analytics\n4. Azure Databricks\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing"
      },
      {
        "date": "2021-03-06T14:41:00.000Z",
        "voteCount": 2,
        "content": "ADF has Data Flows, why is ADF not listed as part of Batch Processing? Secondly, changing the Units, will scale ADF as well... Sending data from On-Premise cant be done via DataBricks, DataBricks can act on it once data is in Azure, ADF seems to be the option"
      },
      {
        "date": "2021-11-15T14:06:00.000Z",
        "voteCount": 1,
        "content": "for batch processing is databricks"
      },
      {
        "date": "2021-05-26T08:54:00.000Z",
        "voteCount": 2,
        "content": "Why is it that no body is choosing Azure stream analytics as the input of the processing solution is messages generated by the website."
      },
      {
        "date": "2021-05-24T05:50:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: B\nExplanation/Reference:\nExplanation:\nScenario: ADatum identifies the following requirements for the Health Interface application:\nSupport a more scalable batch processing solution in Azure.\nReduce the amount of time it takes to add data from new hospitals to Health Interface.\nData Factory integrates with the Azure Cosmos DB bulk executor library to provide the best performance when you write to Azure Cosmos DB.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-cosmos-db"
      },
      {
        "date": "2021-05-16T02:22:00.000Z",
        "voteCount": 1,
        "content": "I would choose ADF.\nhttps://devblogs.microsoft.com/cosmosdb/migrating-relational-data-into-cosmos-db-using-azure-data-factory-and-azure-databricks/"
      },
      {
        "date": "2021-04-29T14:45:00.000Z",
        "voteCount": 1,
        "content": "D. Azure Databricks"
      },
      {
        "date": "2021-04-13T07:14:00.000Z",
        "voteCount": 1,
        "content": "Not sure if databricks can access on prem data source. If yes, then no question D.\nIf not, then you have to use ADF copy data activity to opy from on prem to staging. But as different hospitals have different data formats then you have to transform it to common format. ADF can use mappng data flow or call databricks notebook to do that (but only from staged data already in Azure). dataflow unfortunately is not auto scalable, you have to redefine how many cores you want to use, so I would call databricks notebook from ADF after copy data in ADF. Cosest anwer seems C - ADF."
      },
      {
        "date": "2021-03-10T07:18:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing\nit seems Databricks"
      },
      {
        "date": "2021-02-18T07:09:00.000Z",
        "voteCount": 2,
        "content": "Don't go by word \"batch\". read this: \nHealth Interface -\nADatum has a critical application named Health Interface that receives hospital messages related to patient care and status updates. So stream analytics seems to be correct."
      },
      {
        "date": "2021-01-14T08:15:00.000Z",
        "voteCount": 3,
        "content": "The more reactions I read, the more confused I get. My 2 cents: in this case, the hospitals send the data in batch. This means not message-by-message, but a file containing several messages or records. Most of the discussion here looks at \"batch processing\", which is another story to do with analysing big data stored in files. To me, batch processing is not the correct context of this case. What we need is to ingest files coming from the hospital from time to time. Azure Data Factory seems right to me. The answer's comment also seems to point to this solution, so the answer itself might be a typo."
      },
      {
        "date": "2020-12-26T20:02:00.000Z",
        "voteCount": 1,
        "content": "Can I use ADF only for solution of both Health Insights and Health Interface?"
      },
      {
        "date": "2020-12-26T18:42:00.000Z",
        "voteCount": 2,
        "content": "Which product would provide the best performance?"
      },
      {
        "date": "2020-12-09T15:10:00.000Z",
        "voteCount": 1,
        "content": "It has B showing as the answer, but then the description underneath implies C where it talks about data Factory and Cosmos DB. Data Factory is scalable."
      },
      {
        "date": "2020-12-08T01:49:00.000Z",
        "voteCount": 1,
        "content": "\"Minimize the number of services required to perform data processing, development, scheduling, monitoring, and the operationalizing of pipelines.\"\nI would pick Data Factory as the answer"
      },
      {
        "date": "2020-12-13T18:13:00.000Z",
        "voteCount": 1,
        "content": "Disregard this; Databricks for batch processing"
      },
      {
        "date": "2020-11-04T22:28:00.000Z",
        "voteCount": 2,
        "content": "The answer should be D: Databricks. Purely because of Scalability factor. ADF can be used but Databricks is better when it comes to scaling."
      },
      {
        "date": "2021-04-13T07:08:00.000Z",
        "voteCount": 1,
        "content": "ADF can call databricks notebook in its pipeline"
      },
      {
        "date": "2020-11-02T10:09:00.000Z",
        "voteCount": 2,
        "content": "They mentioned, health interface application received data in batches (group of messages as batch from existing c# application). If ADF is answer how solution is expecting to receive data (http source / json files on blob store?) with varying schema and perform bulk insert into cosmodb?  It has to be ADB receiving messages / batches on stream and ingesting them into cosmodb."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "25"
  }
]