[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13483-exam-dp-200-topic-1-question-1-discussion/",
    "body": "You are a data engineer implementing a lambda architecture on Microsoft Azure. You use an open-source big data solution to collect, process, and maintain data.<br>The analytical data store performs poorly.<br>You must implement a solution that meets the following requirements:<br>\u2711 Provide data warehousing<br>\u2711 Reduce ongoing management activities<br>\u2711 Deliver SQL query responses in less than one second<br>You need to create an HDInsight cluster to meet the requirements.<br>Which type of cluster should you create?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInteractive Query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Hadoop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache HBase",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Spark"
    ],
    "answer": "D",
    "answerDescription": "Lambda Architecture with Azure:<br>Azure offers you a combination of following technologies to accelerate real-time big data analytics:<br>1. Azure Cosmos DB, a globally distributed and multi-model database service.<br>2. Apache Spark for Azure HDInsight, a processing framework that runs large-scale data analytics applications.<br>3. Azure Cosmos DB change feed, which streams new data to the batch layer for HDInsight to process.<br>4. The Spark to Azure Cosmos DB Connector<br><img src=\"/assets/media/exam-media/03872/0000300001.jpg\" class=\"in-exam-image\"><br>Note: Lambda architecture is a data-processing architecture designed to handle massive quantities of data by taking advantage of both batch processing and stream processing methods, and minimizing the latency involved in querying big data.<br>References:<br>https://sqlwithmanoj.com/2018/02/16/what-is-lambda-architecture-and-what-azure-offers-with-its-new-cosmos-db/",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-24T03:13:00.000Z",
        "voteCount": 9,
        "content": "Could the write answer be D becase Spark has;\n1) Interactive queries through spark-sql\n2) Datawarehousing capabilities through Delta Lake (and also spark-sql creates in memory tables)\n3) Less management because these are out-of-the-box features?"
      },
      {
        "date": "2021-05-11T05:39:00.000Z",
        "voteCount": 2,
        "content": "It also mentions SQL queries which is not Hive ( Interactive Query)"
      },
      {
        "date": "2020-08-18T14:45:00.000Z",
        "voteCount": 8,
        "content": "I think the answer should be A. Interactive Query.\nHere I am implementing Lambda architecture using a open source technology which can be Apache Spark and already in use. The prevailing issue here  Analytical Processing is very slow , in another words queries are slow. So I created an HDInsight Cluster of type \"Interactive Query\" to support Analytical processing/ fast query access, data warehousing etc. We can use HiveQL on Interactive Query. Refer to https://docs.microsoft.com/en-us/azure/hdinsight/interactive-query/apache-interactive-query-get-started"
      },
      {
        "date": "2021-06-27T06:17:00.000Z",
        "voteCount": 2,
        "content": "A - Interactive Query  - \"Deliver SQL query responses in less than one second\"\nhttps://docs.microsoft.com/en-us/azure/hdinsight/interactive-query/apache-interactive-query-get-started"
      },
      {
        "date": "2021-06-19T19:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct based on Data warehousing requirement."
      },
      {
        "date": "2021-04-05T23:25:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-02-09T03:50:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architectur"
      },
      {
        "date": "2021-01-20T09:55:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer in Spark because it is in memory"
      },
      {
        "date": "2020-12-11T13:02:00.000Z",
        "voteCount": 5,
        "content": "hi to all,\n\nanswer:\n\nhttps://azure.microsoft.com/pt-pt/blog/general-availability-of-hdinsight-interactive-query-blazing-fast-data-warehouse-style-queries-on-hyper-scale-data-2/\nsub-second !\n\nSummary\nThis week at Ignite, we are pleased to announce general availability of Azure HDInsight Interactive Query. Backed by our enterprise-grade SLA, HDInsight Interactive Query brings sub-second speed to data warehouse style SQL queries to the hyper-scale data stored in commodity cloud storage.\n\nregards"
      },
      {
        "date": "2020-11-29T16:27:00.000Z",
        "voteCount": 6,
        "content": "Exam was updated on Nov 24, 2020.  Didn't see too many questions from the test on ExamTopics...maybe 20-30% of the test questions. Suggest waiting a bit to take the test so that all the exam prep questions are updated.  Exam definitely requires hands-on knowledge of the products.  A lot of questions on CosmosDB consistency settings, encryption/security, monitoring/metrics."
      },
      {
        "date": "2020-12-01T06:27:00.000Z",
        "voteCount": 4,
        "content": "Do you passed the exam after 24 Nov, is there any difference regarding these questions and the updated ones ? I mean if I prepared the exam with these version of a questions what is my chances to pass it ? thank you"
      },
      {
        "date": "2020-11-08T13:20:00.000Z",
        "voteCount": 3,
        "content": "I also vote for Interactive Query as \"An Interactive Query cluster is different from an Apache Hadoop cluster. It contains only the Hive service.\nRequirements: \n\u2711 Provide data warehousing ( Yes) \n\u2711 Reduce ongoing management activities (Not sure)\n\u2711 Deliver SQL query responses in less than one second ( Yes)"
      },
      {
        "date": "2020-11-05T21:49:00.000Z",
        "voteCount": 1,
        "content": "D: Apache spark"
      },
      {
        "date": "2020-10-18T01:20:00.000Z",
        "voteCount": 1,
        "content": "https://azure.microsoft.com/en-in/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/"
      },
      {
        "date": "2020-10-16T18:39:00.000Z",
        "voteCount": 4,
        "content": "Apache Spark is correct Answer !!\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview"
      },
      {
        "date": "2020-08-10T03:52:00.000Z",
        "voteCount": 1,
        "content": "i think the logic to answer this question is:\nLambda architecture:\nhttps://databricks.com/glossary/lambda-architecture\nAzure implementation:\nhttps://azure.microsoft.com/en-us/services/databricks/\nAzure Databricks = Fast, easy, and collaborative Apache SparkTM based analytics service"
      },
      {
        "date": "2020-06-22T07:49:00.000Z",
        "voteCount": 4,
        "content": "How does Spark meet the requirements? Spark does not provide data warehousing by itself, it is not a data store."
      },
      {
        "date": "2020-06-24T11:26:00.000Z",
        "voteCount": 9,
        "content": "Neither does any of the options, the last part of the question is key: Which type of cluster will you create, hence, Spark"
      },
      {
        "date": "2020-03-02T08:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture"
      },
      {
        "date": "2020-07-13T23:44:00.000Z",
        "voteCount": 1,
        "content": "The link you provided is redirected to What is Azure Synapse Link for Azure Cosmos DB (Preview)?"
      },
      {
        "date": "2020-02-06T01:40:00.000Z",
        "voteCount": 2,
        "content": "Would suggest to use the original link from MS: https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture as better background documentation"
      },
      {
        "date": "2020-07-13T23:43:00.000Z",
        "voteCount": 1,
        "content": "The link you provided is redirected to What is Azure Synapse Link for Azure Cosmos DB (Preview)? wy?"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49506-exam-dp-200-topic-1-question-2-discussion/",
    "body": "DRAG DROP -<br>You develop data engineering solutions for a company. You must migrate data from Microsoft Azure Blob storage to an Azure SQL Data Warehouse for further transformation. You need to implement the solution.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0000400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0000400002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Provision an Azure SQL Data Warehouse instance.<br>Create a data warehouse in the Azure portal.<br>Step 2: Connect to the Azure SQL Data warehouse by using SQL Server Management Studio<br>Connect to the data warehouse with SSMS (SQL Server Management Studio)<br>Step 3: Build external tables by using the SQL Server Management Studio<br>Create external tables for data in Azure blob storage.<br>You are ready to begin the process of loading data into your new data warehouse. You use external tables to load data from the Azure storage blob.<br>Step 4: Run Transact-SQL statements to load data.<br>You can use the CREATE TABLE AS SELECT (CTAS) T-SQL statement to load the data from Azure Storage Blob into new tables in your data warehouse.<br>References:<br>https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/sql-data-warehouse/load-data-from-azure-blob-storage-using-polybase.md",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-18T02:42:00.000Z",
        "voteCount": 15,
        "content": "Step 2 should be \"Connect to SQL data warehouse using SSMS"
      },
      {
        "date": "2021-11-20T03:54:00.000Z",
        "voteCount": 1,
        "content": "also for me \"Connect to SQL data warehouse using SSMS\" on step 2"
      },
      {
        "date": "2021-06-28T03:38:00.000Z",
        "voteCount": 1,
        "content": "nope step 2 is point to Blob Storage (check polybase load steps!)"
      },
      {
        "date": "2021-05-21T03:50:00.000Z",
        "voteCount": 4,
        "content": "Step 2:Connect to the Azure SQL Data warehouse by using SQL Server Management Studio"
      },
      {
        "date": "2021-05-12T02:41:00.000Z",
        "voteCount": 1,
        "content": "Why are we using sql data warehouse instance?"
      },
      {
        "date": "2021-04-07T07:35:00.000Z",
        "voteCount": 3,
        "content": "I've a doubt about step 2. Which is the correct one?\n\"Connect to the Blob Storage container by using SQL Server Management Studio.\"\n or\n\"Connect to the Azure SQL Data Warehouse by using SQL Server Management Studio\"\nThank you in advance."
      },
      {
        "date": "2021-04-08T04:46:00.000Z",
        "voteCount": 9,
        "content": "look at the explanation, it should be Azure SQL Data Warehouse. Blob storage doesn't make sense here."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21645-exam-dp-200-topic-1-question-3-discussion/",
    "body": "You develop data engineering solutions for a company. The company has on-premises Microsoft SQL Server databases at multiple locations.<br>The company must integrate data with Microsoft Power BI and Microsoft Azure Logic Apps. The solution must avoid single points of failure during connection and transfer to the cloud. The solution must also minimize latency.<br>You need to secure the transfer of data between on-premises databases and Microsoft Azure.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a standalone on-premises Azure data gateway at each location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an on-premises data gateway in personal mode at each location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an Azure on-premises data gateway at the primary location",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an Azure on-premises data gateway as a cluster at each location"
    ],
    "answer": "D",
    "answerDescription": "You can create high availability clusters of On-premises data gateway installations, to ensure your organization can access on-premises data resources used in<br>Power BI reports and dashboards. Such clusters allow gateway administrators to group gateways to avoid single points of failure in accessing on-premises data resources. The Power BI service always uses the primary gateway in the cluster, unless it's not available. In that case, the service switches to the next gateway in the cluster, and so on.<br>References:<br>https://docs.microsoft.com/en-us/power-bi/service-gateway-high-availability-clusters",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-31T01:52:00.000Z",
        "voteCount": 9,
        "content": "https://docs.microsoft.com/en-us/data-integration/gateway/service-gateway-high-availability-clusters"
      },
      {
        "date": "2020-12-06T02:43:00.000Z",
        "voteCount": 5,
        "content": "D. Clusters is correct. The answer is ok."
      },
      {
        "date": "2020-11-23T04:42:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2020-11-14T10:44:00.000Z",
        "voteCount": 1,
        "content": "IGNORE MY EARLIER COMMENT, \nThe answer is spark because SPARK has the functionality of INTERACTIVE QUERIES AS WELL.\nSpark\tIn-memory processing, interactive queries, micro-batch stream processing\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-provision-linux-clusters"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27278-exam-dp-200-topic-1-question-4-discussion/",
    "body": "You are a data architect. The data engineering team needs to configure a synchronization of data between an on-premises Microsoft SQL Server database to<br>Azure SQL Database.<br>Ad-hoc and reporting queries are being overutilized the on-premises production instance. The synchronization process must:<br>\u2711 Perform an initial data synchronization to Azure SQL Database with minimal downtime<br>\u2711 Perform bi-directional data synchronization after initial synchronization<br>You need to implement this synchronization solution.<br>Which synchronization method should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttransactional replication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Migration Assistant (DMA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbackup and restore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Agent job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Sync"
    ],
    "answer": "E",
    "answerDescription": "SQL Data Sync is a service built on Azure SQL Database that lets you synchronize the data you select bi-directionally across multiple SQL databases and SQL<br>Server instances.<br>With Data Sync, you can keep data synchronized between your on-premises databases and Azure SQL databases to enable hybrid applications.<br>Compare Data Sync with Transactional Replication<br><img src=\"/assets/media/exam-media/03872/0000700001.png\" class=\"in-exam-image\"><br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-sync-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-09T00:31:00.000Z",
        "voteCount": 2,
        "content": "the answer is E"
      },
      {
        "date": "2020-12-06T02:45:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is E. \nSQL Data Sync is a service built on Azure SQL Database that lets you synchronize the data you select bi-directionally across multiple databases, both on-premises and in the cloud.\n\nSource: https://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database"
      },
      {
        "date": "2021-05-15T11:39:00.000Z",
        "voteCount": 1,
        "content": "keyword is bi-directional"
      },
      {
        "date": "2020-11-23T04:42:00.000Z",
        "voteCount": 1,
        "content": "Can only be E"
      },
      {
        "date": "2020-08-25T08:23:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30688-exam-dp-200-topic-1-question-5-discussion/",
    "body": "An application will use Microsoft Azure Cosmos DB as its data solution. The application will use the Cassandra API to support a column-based database type that uses containers to store items.<br>You need to provision Azure Cosmos DB. Which container name and item name should you use?  Each correct answer presents part of the solutions.<br>NOTE: Each correct answer selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcollection",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgraph",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tentities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttable"
    ],
    "answer": "BE",
    "answerDescription": "B: Depending on the choice of the API, an Azure Cosmos item can represent either a document in a collection, a row in a table or a node/edge in a graph. The following table shows the mapping between API-specific entities to an Azure Cosmos item:<br><img src=\"/assets/media/exam-media/03872/0000800001.png\" class=\"in-exam-image\"><br>E: An Azure Cosmos container is specialized into API-specific entities as follows:<br><img src=\"/assets/media/exam-media/03872/0000800002.png\" class=\"in-exam-image\"><br>References:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/databases-containers-items",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T02:52:00.000Z",
        "voteCount": 23,
        "content": "Correct answer: B (row) &amp; E (table)\n\nCassandra API: Table/Row\nTable API: Table/Item\nMongo API: Collection/Document\nSQL Core: Container/Item\nGremlin: Graph/Node&amp;Edge"
      },
      {
        "date": "2020-09-06T07:22:00.000Z",
        "voteCount": 8,
        "content": "Not directly related but they've updated the screenshot in the question: https://docs.microsoft.com/en-us/azure/cosmos-db/databases-containers-items#azure-cosmos-containers\nSQL API =&gt; Container and not Collection"
      },
      {
        "date": "2020-11-26T14:42:00.000Z",
        "voteCount": 2,
        "content": "B&amp;E are the correct answers."
      },
      {
        "date": "2020-11-23T04:47:00.000Z",
        "voteCount": 2,
        "content": "100% B &amp; E"
      },
      {
        "date": "2020-11-14T11:59:00.000Z",
        "voteCount": 1,
        "content": "its obvious answer is B &amp; E"
      },
      {
        "date": "2020-10-05T15:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is B &amp; E"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23958-exam-dp-200-topic-1-question-6-discussion/",
    "body": "A company has a SaaS solution that uses Azure SQL Database with elastic pools. The solution contains a dedicated database for each customer organization.<br>Customer organizations have peak usage at different periods during the year.<br>You need to implement the Azure SQL Database elastic pool to minimize cost.<br>Which option or options should you configure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNumber of transactions only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\teDTUs per database only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNumber of databases only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCPU usage only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\teDTUs and max data size"
    ],
    "answer": "E",
    "answerDescription": "The best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:<br>\u2711 Maximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of resourcing model).<br>\u2711 Maximum storage bytes utilized by all databases in the pool.<br>Note: Elastic pools enable the developer to purchase resources for a pool shared by multiple databases to accommodate unpredictable periods of usage by individual databases. You can configure resources for the pool based either on the DTU-based purchasing model or the vCore-based purchasing model.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-pool",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-24T11:59:00.000Z",
        "voteCount": 10,
        "content": "An additional note from the reference link:\n\"There is no per-database charge for elastic pools. You are billed for each hour a pool exists at the highest eDTU or vCores, regardless of usage or whether the pool was active for less than an hour.\"\nSo no need to figure out the needs of an individual database to size the pool"
      },
      {
        "date": "2021-02-16T18:51:00.000Z",
        "voteCount": 4,
        "content": "Answer E"
      },
      {
        "date": "2020-12-06T02:59:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct - E.\n\nHow do I choose the correct pool size?\nThe best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:\n\n-&gt; Maximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of purchasing model).\n-&gt; Maximum storage bytes utilized by all databases in the pool.\n\nSource: https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"
      },
      {
        "date": "2020-11-23T04:54:00.000Z",
        "voteCount": 1,
        "content": "Link provided supports E as the right answer"
      },
      {
        "date": "2020-11-16T21:50:00.000Z",
        "voteCount": 1,
        "content": "indeed the correct answer is E"
      },
      {
        "date": "2020-11-14T12:01:00.000Z",
        "voteCount": 1,
        "content": "answer is E"
      },
      {
        "date": "2020-09-02T00:54:00.000Z",
        "voteCount": 3,
        "content": "Answer is E"
      },
      {
        "date": "2020-08-25T03:43:00.000Z",
        "voteCount": 1,
        "content": "so answer here is B i.e  eDTU's per database only"
      },
      {
        "date": "2020-09-01T06:14:00.000Z",
        "voteCount": 2,
        "content": "The best size for a pool depends on the aggregate resources needed for all databases in the pool. This involves determining the following:\n\nMaximum resources utilized by all databases in the pool (either maximum DTUs or maximum vCores depending on your choice of purchasing model).\nMaximum storage bytes utilized by all databases in the pool."
      },
      {
        "date": "2020-09-19T12:58:00.000Z",
        "voteCount": 13,
        "content": "Vijaya...please don't confuse people, answer is E!"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50542-exam-dp-200-topic-1-question-7-discussion/",
    "body": "HOTSPOT -<br>You are a data engineer. You are designing a Hadoop Distributed File System (HDFS) architecture. You plan to use Microsoft Azure Data Lake as a data storage repository.<br>You must provision the repository with a resilient data schema. You need to ensure the resiliency of the Azure Data Lake Storage. What should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0001000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0001100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: NameNode -<br>An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients.<br><br>Box 2: DataNode -<br>The DataNodes are responsible for serving read and write requests from the file system's clients.<br><br>Box 3: DataNode -<br>The DataNodes perform block creation, deletion, and replication upon instruction from the NameNode.<br>Note: HDFS has a master/slave architecture. An HDFS cluster consists of a single NameNode, a master server that manages the file system namespace and regulates access to files by clients. In addition, there are a number of DataNodes, usually one per node in the cluster, which manage storage attached to the nodes that they run on. HDFS exposes a file system namespace and allows user data to be stored in files. Internally, a file is split into one or more blocks and these blocks are stored in a set of DataNodes. The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system's clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.<br>References:<br>https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#NameNode+and+DataNodes",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-27T03:01:00.000Z",
        "voteCount": 22,
        "content": "1.- NameNode (Its asking about data access meaning permission not data provider so name node has this responsibility)\n2.- NameNode (Name Node perform namespace and etc on files system)\n3.- DataNode"
      },
      {
        "date": "2021-06-04T12:11:00.000Z",
        "voteCount": 1,
        "content": "Agree, doc link supports this."
      },
      {
        "date": "2021-11-20T04:05:00.000Z",
        "voteCount": 1,
        "content": "i think that is the correct answer is 1) name node 2) name node 3) data node"
      },
      {
        "date": "2021-07-19T05:56:00.000Z",
        "voteCount": 1,
        "content": "1.- DataNode\n2.- NameNode\n3.- DataNode"
      },
      {
        "date": "2021-06-28T05:35:00.000Z",
        "voteCount": 1,
        "content": "NameNode, a master server that manages the file system namespace and regulates access to files by clients.\n\nNameNode executes file system namespace operations like opening, closing, and renaming files and directories.\n\nDataNodes are responsible for serving read and write requests from the file system\u2019s clients. \nThe DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode."
      },
      {
        "date": "2021-06-28T04:02:00.000Z",
        "voteCount": 2,
        "content": "NDD  (last one &gt;&gt; Data Node, The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode.)"
      },
      {
        "date": "2021-06-11T23:52:00.000Z",
        "voteCount": 2,
        "content": "1)DataNode\n2)NameNode\n3)DataNode\n\nThis is what  Whizlab's exam answer is"
      },
      {
        "date": "2021-06-11T23:36:00.000Z",
        "voteCount": 3,
        "content": "Run operations on Files and Directories.. DataNode has no work on these operations.. It has to serve to create, read,write blocks. Not name space work as per documentation.\n\nIt is Name Node, Name Node, Data Node"
      },
      {
        "date": "2021-05-23T11:37:00.000Z",
        "voteCount": 1,
        "content": "The answer provided as a solution is correct.\n1. Namenode\n2. Datanode\n3. Datanode\nThe apache ref link provided in the answer has the correct pointers"
      },
      {
        "date": "2021-05-31T09:00:00.000Z",
        "voteCount": 4,
        "content": "N, N, D is correct. \nExcerpt from the link provided:\nThe NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes. The DataNodes are responsible for serving read and write requests from the file system\u2019s clients. The DataNodes also perform block creation, deletion, and replication upon instruction from the NameNode."
      },
      {
        "date": "2021-05-03T23:08:00.000Z",
        "voteCount": 4,
        "content": "I believe the answers to be 1.DataNode 2.NameNode 3.DataNode"
      },
      {
        "date": "2021-05-01T06:17:00.000Z",
        "voteCount": 1,
        "content": "As per the line \"A master server that manages the file system namespace and regulates access to files by clients\" so the first option should be NameNode."
      },
      {
        "date": "2021-04-29T09:22:00.000Z",
        "voteCount": 1,
        "content": "So what\u2019s the correct answer?"
      },
      {
        "date": "2021-04-23T00:29:00.000Z",
        "voteCount": 3,
        "content": "NameNode, DataNode, DataNode"
      },
      {
        "date": "2021-05-03T02:26:00.000Z",
        "voteCount": 4,
        "content": "It should be NameNode,NameNode,DataNode.\n\nThe second question is related to file namespace and according to the documentation \"The NameNode executes file system namespace operations like opening, closing, and renaming files and directories. It also determines the mapping of blocks to DataNodes.\"\n\nReference: https://hadoop.apache.org/docs/r1.2.1/hdfs_design.html#NameNode+and+DataNodes"
      },
      {
        "date": "2021-04-20T06:04:00.000Z",
        "voteCount": 3,
        "content": "1.- DataNode\n2.- NameNode\n3.- DataNode"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52814-exam-dp-200-topic-1-question-8-discussion/",
    "body": "DRAG DROP -<br>You are developing the data platform for a global retail company. The company operates during normal working hours in each region. The analytical database is used once a week for building sales projections.<br>Each region maintains its own private virtual network.<br>Building the sales projections is very resource intensive and generates upwards of 20 terabytes (TB) of data.<br>Microsoft Azure SQL Databases must be provisioned.<br>\u2711 Database provisioning must maximize performance and minimize cost<br>\u2711 The daily sales for each region must be stored in an Azure SQL Database instance<br>\u2711 Once a day, the data for all regions must be loaded in an analytical Azure SQL Database instance<br>You need to provision Azure SQL database instances.<br>How should you provision the database instances? To answer, drag the appropriate Azure SQL products to the correct databases. Each Azure SQL product may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0001300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0001300002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure SQL Database elastic pools<br>SQL Database elastic pools are a simple, cost-effective solution for managing and scaling multiple databases that have varying and unpredictable usage demands. The databases in an elastic pool are on a single Azure SQL Database server and share a set number of resources at a set price. Elastic pools in Azure<br>SQL Database enable SaaS developers to optimize the price performance for a group of databases within a prescribed budget while delivering performance elasticity for each database.<br>Box 2: Azure SQL Database Hyperscale<br>A Hyperscale database is an Azure SQL database in the Hyperscale service tier that is backed by the Hyperscale scale-out storage technology. A Hyperscale database supports up to 100 TB of data and provides high throughput and performance, as well as rapid scaling to adapt to the workload requirements. Scaling is transparent to the application \u05d2\u20ac\" connectivity, query processing, and so on, work like any other SQL database.<br>Incorrect Answers:<br>Azure SQL Database Managed Instance: The managed instance deployment model is designed for customers looking to migrate a large number of apps from on- premises or IaaS, self-built, or ISV provided environment to fully managed PaaS cloud environment, with as low migration effort as possible.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-pool https://docs.microsoft.com/en-us/azure/sql-database/sql-database-service-tier-hyperscale-faq",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-15T12:05:00.000Z",
        "voteCount": 6,
        "content": "ANSWER IS CORRECT\n\u2022\tAzure SQL Database Elastic Pools &gt;&gt;Simple, Cost-effective, scaling multiple db, deploy multiple db to a single logical instance\n\u2022\tAzure SQL Database Premium &gt;&gt; better capabilities for the database\n\u2022\tAzure SQL Database Managed Instance &gt;&gt; migrate large numbers of apps on-premise to Azure, use CLR features ,IASS\n\t\t\t\t\t&gt;&gt; Back up and restore\n\u2022\tAzure SQL Database Hyperscale &gt;&gt; Up to 100TB, high performance\n\nSo;\nAzure SQL Database Premium  and Azure SQL Database Managed Instance are irrelavent\nfor Daily Sales &gt;&gt; Azure SQL Database Elastic Pools (from multiple region to single location)\nfor weekly projections &gt;&gt; Azure SQL Database Hyperscale (up to 100TB)"
      },
      {
        "date": "2021-06-11T23:40:00.000Z",
        "voteCount": 2,
        "content": "Daily sales itself will go around 20TB.. I don't see any model is providing that much limit as per the document. \nAnyone have document to prove that \"Elastic Pools\" is the answer ?\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/resource-limits-dtu-elastic-pools"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15767-exam-dp-200-topic-1-question-9-discussion/",
    "body": "A company manages several on-premises Microsoft SQL Server databases.<br>You need to migrate the databases to Microsoft Azure by using a backup process of Microsoft SQL Server.<br>Which data technology should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database single database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Managed Instance"
    ],
    "answer": "D",
    "answerDescription": "Managed instance is a new deployment option of Azure SQL Database, providing near 100% compatibility with the latest SQL Server on-premises (Enterprise<br>Edition) Database Engine, providing a native virtual network (VNet) implementation that addresses common security concerns, and a business model favorable for on-premises SQL Server customers. The managed instance deployment model allows existing SQL Server customers to lift and shift their on-premises applications to the cloud with minimal application and database changes.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-managed-instance",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-28T09:26:00.000Z",
        "voteCount": 26,
        "content": "You can only do backup and restore to a managed instance.  See page 60 https://azure.microsoft.com/mediahandler/files/resourcefiles/choosing-your-database-migration-path-to-azure/Choosing_your_database_migration_path_to_Azure.pdf"
      },
      {
        "date": "2020-06-15T12:30:00.000Z",
        "voteCount": 6,
        "content": "when ever you are choosing to migrate indirectly we would need to do it in a cost effective way which is possible through azure sql database managed instance."
      },
      {
        "date": "2021-01-07T07:47:00.000Z",
        "voteCount": 2,
        "content": "D: es correct"
      },
      {
        "date": "2020-12-13T13:44:00.000Z",
        "voteCount": 1,
        "content": "The managed instance is the resource, which is closest to an on-prem SQL-server, whereas a SQL database is limited (e.g. support for R). The lift and shift is the least effort solution for a SQL server"
      },
      {
        "date": "2020-12-06T03:14:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D: Azure SQL Managed Instance.\n\nAzure SQL Managed Instance is designed for customers looking to migrate a large number of apps from an on-premises or IaaS, self-built, or ISV provided environment to a fully managed PaaS cloud environment, with as low a migration effort as possible."
      },
      {
        "date": "2020-11-22T01:28:00.000Z",
        "voteCount": 2,
        "content": "This Question came in DP-201 in Nov 2020"
      },
      {
        "date": "2020-12-09T03:31:00.000Z",
        "voteCount": 1,
        "content": "On which date u gave the exam?"
      },
      {
        "date": "2020-11-14T12:05:00.000Z",
        "voteCount": 2,
        "content": "the keywords are \"migrating databases\" hence single db is wrong.the answer is SQL Managed Instance"
      },
      {
        "date": "2020-09-23T06:30:00.000Z",
        "voteCount": 4,
        "content": "SQL Managed Instance allows existing SQL Server customers to lift and shift their on-premises applications to the cloud with minimal application and database changes. At the same time, SQL Managed Instance preserves all PaaS capabilities (automatic patching and version updates, automated backups, high availability) that drastically reduce management overhead and TCO."
      },
      {
        "date": "2020-08-31T20:13:00.000Z",
        "voteCount": 2,
        "content": "the given answer is correct because it is several on-prem DB's"
      },
      {
        "date": "2020-03-07T08:47:00.000Z",
        "voteCount": 2,
        "content": "Why isn't it single db? If the question asked a backup and restore then I would prefer to choose managed db."
      },
      {
        "date": "2020-04-07T21:33:00.000Z",
        "voteCount": 5,
        "content": "Because there are several databases. Single db is only good for very few databases."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24765-exam-dp-200-topic-1-question-10-discussion/",
    "body": "The data engineering team manages Azure HDInsight clusters. The team spends a large amount of time creating and destroying clusters daily because most of the data pipeline process runs in minutes.<br>You need to implement a solution that deploys multiple HDInsight clusters with minimal effort.<br>What should you implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Traffic Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Resource Manager templates",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAmbari web user interface"
    ],
    "answer": "C",
    "answerDescription": "A Resource Manager template makes it easy to create the following resources for your application in a single, coordinated operation:<br>\u2711 HDInsight clusters and their dependent resources (such as the default storage account).<br>\u2711 Other resources (such as Azure SQL Database to use Apache Sqoop).<br>In the template, you define the resources that are needed for the application. You also specify deployment parameters to input values for different environments.<br>The template consists of JSON and expressions that you use to construct values for your deployment.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-arm-templates",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-17T02:33:00.000Z",
        "voteCount": 3,
        "content": "C. Azure Resource Manager templates, \nls correct"
      },
      {
        "date": "2020-11-23T05:03:00.000Z",
        "voteCount": 4,
        "content": "C is correct; afterall Templates are meant to make life easier ain't it?"
      },
      {
        "date": "2020-09-25T03:11:00.000Z",
        "voteCount": 3,
        "content": "Are the topics related to HDInsight still part of DP-200 exam?"
      },
      {
        "date": "2020-10-14T09:14:00.000Z",
        "voteCount": 2,
        "content": "yes, they are"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5507-exam-dp-200-topic-1-question-11-discussion/",
    "body": "You are the data engineer for your company. An application uses a NoSQL database to store data. The database uses the key-value and wide-column NoSQL database type.<br>Developers need to access data in the database using an API.<br>You need to determine which API to use for the database model and type.<br>Which two APIs should you use? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMongoDB API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGremlin API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCassandra API"
    ],
    "answer": "BE",
    "answerDescription": "B: Azure Cosmos DB is the globally distributed, multimodel database service from Microsoft for mission-critical applications. It is a multimodel database and supports document, key-value, graph, and columnar data models.<br>E: Wide-column stores store data together as columns instead of rows and are optimized for queries over large datasets. The most popular are Cassandra and<br>HBase.<br>References:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction https://www.mongodb.com/scale/types-of-nosql-databases",
    "votes": [],
    "comments": [
      {
        "date": "2019-11-26T01:42:00.000Z",
        "voteCount": 97,
        "content": "it should ba A and E"
      },
      {
        "date": "2020-08-31T20:15:00.000Z",
        "voteCount": 2,
        "content": "as per documentation mongo API is multi-model and supports document, key-value, graph, and columnar data models so answer is correct ref https://docs.microsoft.com/en-us/azure/cosmos-db/mongodb-introduction"
      },
      {
        "date": "2021-06-11T23:49:00.000Z",
        "voteCount": 1,
        "content": "Answer is clearly A and E. No documentation refers to Mongo API ."
      },
      {
        "date": "2021-05-03T18:10:00.000Z",
        "voteCount": 1,
        "content": "No, the documentation didn't refer to that"
      },
      {
        "date": "2020-03-07T08:50:00.000Z",
        "voteCount": 87,
        "content": "key value --&gt; Table API\ngraph --&gt; Gremlin API\ndocument --&gt; SQL API and MongoDB API\ncolumnar --&gt; Cassandra API\n\nso, the answer is A,E"
      },
      {
        "date": "2021-06-12T04:16:00.000Z",
        "voteCount": 3,
        "content": "right answer is A,E but slight change in your description.                                                       \n key value --&gt; Cassandra API\ngraph --&gt; Gremlin API\ndocument --&gt; SQL API and MongoDB API\ncolumnar --&gt; Table API                                             \nhttps://cloud.netapp.com/blog/azure-cvo-blg-azure-nosql-types-services-and-a-quick-tutorial#H_H1"
      },
      {
        "date": "2021-11-20T04:11:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is  Table Api e Cassandra Api. so A - E"
      },
      {
        "date": "2021-03-29T12:48:00.000Z",
        "voteCount": 1,
        "content": "It's A and E."
      },
      {
        "date": "2021-03-16T10:43:00.000Z",
        "voteCount": 1,
        "content": "Key-Value -&gt; Table API\nWide column -&gt; Cassandra API"
      },
      {
        "date": "2021-02-24T11:27:00.000Z",
        "voteCount": 1,
        "content": "A &amp; E certainly"
      },
      {
        "date": "2021-01-26T09:24:00.000Z",
        "voteCount": 1,
        "content": "Table API and Cassandra API for sure"
      },
      {
        "date": "2021-01-23T05:59:00.000Z",
        "voteCount": 1,
        "content": "It's A (Table API for key-value pairs) and E (Cassandra API for wide-columns)"
      },
      {
        "date": "2020-12-07T10:53:00.000Z",
        "voteCount": 1,
        "content": "AE. Table uses key/value. Not MongoDB. B is wrong."
      },
      {
        "date": "2020-12-06T03:16:00.000Z",
        "voteCount": 1,
        "content": "Should be A (Table API) and E (Cassandra API).\n\nTable API is the best solution for key-value model."
      },
      {
        "date": "2020-11-23T07:44:00.000Z",
        "voteCount": 1,
        "content": "Azure CosmosDB table API is a key-value storage hosted in the cloud. It's a part of Azure Cosmos DB, that is Microsoft's multi-model database. It's a globally distributed, low latency, high throughput solution with client SDKs available for . NET, Java, Python, and Node.  So Answer will be A and E"
      },
      {
        "date": "2020-11-23T05:09:00.000Z",
        "voteCount": 1,
        "content": "It's A &amp; E\nCassandra is wide-table and Table is key"
      },
      {
        "date": "2020-11-22T13:39:00.000Z",
        "voteCount": 1,
        "content": "it should be A and E"
      },
      {
        "date": "2020-11-19T05:51:00.000Z",
        "voteCount": 2,
        "content": "Answer is Table APi and cassandra API"
      },
      {
        "date": "2020-11-09T03:34:00.000Z",
        "voteCount": 1,
        "content": "I think it should be MongoDB, since it uses key-value (which is the json format in document). but not storing the key-value pairs(then should be Table API)"
      },
      {
        "date": "2020-11-13T07:44:00.000Z",
        "voteCount": 1,
        "content": "https://www.mongodb.com/key-value-database"
      },
      {
        "date": "2020-11-13T07:47:00.000Z",
        "voteCount": 1,
        "content": "ignore it. I would go for Table"
      },
      {
        "date": "2020-10-29T09:33:00.000Z",
        "voteCount": 4,
        "content": "It should be D and E.\nBefore Azure Cosmos DB existed, Redis or the Table API might have been a good fit for this kind of data; however, Core (SQL) API is now the better choice, as it offers a richer query experience, with improved indexing over the Table API."
      },
      {
        "date": "2020-09-25T00:40:00.000Z",
        "voteCount": 2,
        "content": "The answer should be Table API for key value and Cassandra API for wide column"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24884-exam-dp-200-topic-1-question-12-discussion/",
    "body": "A company is designing a hybrid solution to synchronize data and on-premises Microsoft SQL Server database to Azure SQL Database.<br>You must perform an assessment of databases to determine whether data will move without compatibility issues. You need to perform the assessment.<br>Which tool should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Migration Assistant (SSMA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Assessment and Planning Toolkit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Vulnerability Assessment (VA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Sync",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Migration Assistant (DMA)"
    ],
    "answer": "E",
    "answerDescription": "The Data Migration Assistant (DMA) helps you upgrade to a modern data platform by detecting compatibility issues that can impact database functionality in your new version of SQL Server or Azure SQL Database. DMA recommends performance and reliability improvements for your target environment and allows you to move your schema, data, and uncontained objects from your source server to your target server.<br>References:<br>https://docs.microsoft.com/en-us/sql/dma/dma-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-30T23:21:00.000Z",
        "voteCount": 24,
        "content": "Correct answer is DMA. Data Migration Assistant is a client-side tool that you can install on a Windows-compatible workstation or server. It has two major functions in the migration of the social database to the Azure SQL Database platform in this module. First, it assesses your existing database and identifies any incompatibilities between that database and Azure SQL Database. It then generates a report of the things you need to fix before you can migrate. As you make changes, you can rerun Data Migration Assistant to generate an updated report of changes that you need to make. This capability helps you to not only track your progress, but also catch any new issues that might have been introduced during your coding phase.  \nRefrence: https://docs.microsoft.com/en-us/learn/modules/migrate-sql-server-relational-data/3-migration-overview"
      },
      {
        "date": "2021-05-15T12:13:00.000Z",
        "voteCount": 1,
        "content": "keyword is &gt;&gt; Assessment of db"
      },
      {
        "date": "2021-03-16T10:44:00.000Z",
        "voteCount": 3,
        "content": "DMA is the correct answer. It is used for compatibility check prior to migration."
      },
      {
        "date": "2021-02-17T06:51:00.000Z",
        "voteCount": 1,
        "content": "Answer E Data Migration Assistant."
      },
      {
        "date": "2020-12-31T10:45:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is definitely the chosen answer E. The question is about assessement not the actual migration and/or synchronization. Azure Data Sync (E) addresses the actual migration and synchronization; however, the question is about assessment before migration"
      },
      {
        "date": "2020-11-23T05:16:00.000Z",
        "voteCount": 2,
        "content": "From link https://docs.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15:\nAssess on-premises SQL Server instance(s) migrating to Azure SQL database(s). The assessment workflow helps you to detect the following issues that can affect Azure SQL database migration and provides detailed guidance on how to resolve them."
      },
      {
        "date": "2020-11-14T12:08:00.000Z",
        "voteCount": 2,
        "content": "\"perform an assessment\" is the keyword here not sync . Data Migration Assistant is the correct one"
      },
      {
        "date": "2020-07-16T14:53:00.000Z",
        "voteCount": 2,
        "content": "I also feel it should be Azure Data Sync"
      },
      {
        "date": "2020-08-10T05:25:00.000Z",
        "voteCount": 6,
        "content": "i think the question is about first migrating data, so we need to address this issue first. The sync comes later."
      },
      {
        "date": "2020-07-06T05:39:00.000Z",
        "voteCount": 2,
        "content": "Should not be D: Azure Data Sync as it is said to  synchronize data ? Data Migration Assistant should be used to migrate data from on-premise to Azure."
      },
      {
        "date": "2020-07-07T09:56:00.000Z",
        "voteCount": 12,
        "content": "It is mentioned that \"perform an assessment of databases to determine whether data will move without compatibility issues\".So E is correct\nRef:https://docs.microsoft.com/en-us/sql/dma/dma-overview?view=sql-server-ver15"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49946-exam-dp-200-topic-1-question-13-discussion/",
    "body": "DRAG DROP -<br>You manage a financial computation data analysis process. Microsoft Azure virtual machines (VMs) run the process in daily jobs, and store the results in virtual hard drives (VHDs.)<br>The VMs product results using data from the previous day and store the results in a snapshot of the VHD. When a new month begins, a process creates a new<br>VHD.<br>You must implement the following data retention requirements:<br>\u2711 Daily results must be kept for 90 days<br>\u2711 Data for the current year must be available for weekly reports<br>\u2711 Data from the previous 10 years must be stored for auditing purposes<br>\u2711 Data required for an audit must be produced within 10 days of a request.<br>You need to enforce the data retention requirements while minimizing cost.<br>How should you configure the lifecycle policy? To answer, drag the appropriate JSON segments to the correct locations. Each JSON segment may be used once, more than once, or not at all. You may need to drag the split bat between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0001800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0001900001.png\" class=\"in-exam-image\">",
    "answerDescription": "The Set-AzStorageAccountManagementPolicy cmdlet creates or modifies the management policy of an Azure Storage account.<br>Example: Create or update the management policy of a Storage account with ManagementPolicy rule objects.<br><img src=\"/assets/media/exam-media/03872/0002000001.png\" class=\"in-exam-image\"><br>Action -BaseBlobAction Delete -daysAfterModificationGreaterThan 100<br>PS C:\\&gt;$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -BaseBlobAction TierToArchive -daysAfterModificationGreaterThan 50<br>PS C:\\&gt;$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -BaseBlobAction TierToCool -daysAfterModificationGreaterThan 30<br>PS C:\\&gt;$action1 = Add-AzStorageAccountManagementPolicyAction -InputObject $action1 -SnapshotAction Delete -daysAfterCreationGreaterThan 100<br>PS C:\\&gt;$filter1 = New-AzStorageAccountManagementPolicyFilter -PrefixMatch ab,cd<br>PS C:\\&gt;$rule1 = New-AzStorageAccountManagementPolicyRule -Name Test -Action $action1 -Filter $filter1<br>PS C:\\&gt;$action2 = Add-AzStorageAccountManagementPolicyAction -BaseBlobAction Delete -daysAfterModificationGreaterThan 100<br>PS C:\\&gt;$filter2 = New-AzStorageAccountManagementPolicyFilter<br>References:<br>https://docs.microsoft.com/en-us/powershell/module/az.storage/set-azstorageaccountmanagementpolicy",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-12T01:22:00.000Z",
        "voteCount": 6,
        "content": "I would rather choose \"delete\" in the last section"
      },
      {
        "date": "2021-05-15T12:17:00.000Z",
        "voteCount": 12,
        "content": "Remember: Data for the current year must be available for weekly reports. \nif you delete after 90 days you cant prepare weekly reports! so you must not delete them before 10 years.\nANSWER IS CORRECT"
      },
      {
        "date": "2021-05-15T12:17:00.000Z",
        "voteCount": 4,
        "content": "Remember: Data for the current year must be available for weekly reports. \nif you delete after 90 days you cant prepare weekly reports! so you must not delete them before 10 years.\nANSWER IS CORRECT"
      },
      {
        "date": "2021-06-22T23:36:00.000Z",
        "voteCount": 1,
        "content": "I think \"data for the current year\" refers to de data on the baseBlob, which is why the json part for the baseBlob is archived after 356 days. It explicitly says that snapshot data should be kept for 90 days, whitch is analogous to saying it can be deleted after 90."
      },
      {
        "date": "2021-05-12T11:09:00.000Z",
        "voteCount": 2,
        "content": "Data should be kept for current year for reporting. So Greater than 90 cannot be deleted."
      },
      {
        "date": "2021-05-01T04:04:00.000Z",
        "voteCount": 2,
        "content": "I agree. The daily snapshots are not needed after the 90-day period, so should be deleted instead of being moved to cool tier."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16975-exam-dp-200-topic-1-question-14-discussion/",
    "body": "A company plans to use Azure SQL Database to support a mission-critical application.<br>The application must be highly available without performance degradation during maintenance windows.<br>You need to implement the solution.<br>Which three technologies should you implement? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPremium service tier",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVirtual machine Scale Sets",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBasic service tier",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Data Sync",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAlways On availability groups",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tZone-redundant configuration"
    ],
    "answer": "AEF",
    "answerDescription": "A: Premium/business critical service tier model that is based on a cluster of database engine processes. This architectural model relies on a fact that there is always a quorum of available database engine nodes and has minimal performance impact on your workload even during maintenance activities.<br>E: In the premium model, Azure SQL database integrates compute and storage on the single node. High availability in this architectural model is achieved by replication of compute (SQL Server Database Engine process) and storage (locally attached SSD) deployed in 4-node cluster, using technology similar to SQL<br>Server Always On Availability Groups.<br><img src=\"/assets/media/exam-media/03872/0002200001.png\" class=\"in-exam-image\"><br><br>F: Zone redundant configuration -<br>By default, the quorum-set replicas for the local storage configurations are created in the same datacenter. With the introduction of Azure Availability Zones, you have the ability to place the different replicas in the quorum-sets to different availability zones in the same region. To eliminate a single point of failure, the control ring is also duplicated across multiple zones as three gateway rings (GW).<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-high-availability",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-21T05:45:00.000Z",
        "voteCount": 19,
        "content": "Look at the Conclusion section in the below link. It clearly mentions Always on and thus the provided answer is correct [A,E,F]\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-high-availability"
      },
      {
        "date": "2020-07-07T07:28:00.000Z",
        "voteCount": 4,
        "content": "I think it is confusing because the Premium tier says \"similar to Always On\" but I agree with the answer because you can apply Always On AGs to the VMs that host the SQL Servers"
      },
      {
        "date": "2021-06-23T00:23:00.000Z",
        "voteCount": 1,
        "content": "The question asks \"how should YOU configure\". The documentation states that \"High availability is implemented using a technology similar to SQL Server Always On availability groups\", but this is an implementation detail of the premium/business critical service tiers, not something you have to configure yourself. That said, the other answers make even less sense."
      },
      {
        "date": "2020-05-18T04:52:00.000Z",
        "voteCount": 9,
        "content": "The given answer[A,E,F] is correct. Same can be understood from given link - https://docs.microsoft.com/en-us/azure/sql-database/sql-database-high-availability"
      },
      {
        "date": "2021-03-13T02:04:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A &amp; F; \"Always On availability groups\" are integral part of (already included in) Premium tier zone redundant availability:\n\"High availability is implemented using a technology similar to SQL Server Always On availability groups.\" \nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla\nFor better understanding of the above once and for all, pls look at the illustrations too."
      },
      {
        "date": "2020-11-23T05:25:00.000Z",
        "voteCount": 4,
        "content": "Between Premium and Basic, Premium would be the better choice\nData Sync is definitely irrelevant for this question\nVM Scale Sets is also wrong for sure\nSo answer is AEF"
      },
      {
        "date": "2020-11-14T12:11:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/sql-database/sql-database-high-availability. here there describe it nicely. A&lt;E&lt;F is the answer"
      },
      {
        "date": "2020-09-27T03:15:00.000Z",
        "voteCount": 1,
        "content": "Re premium-  High availability requirement - As an extra benefit, the premium availability model includes the ability to redirect read-only Azure SQL connections to one of the secondary replicas."
      },
      {
        "date": "2020-07-07T13:02:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is A and F, in here https://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla microsoft says:\nThe underlying database files (.mdf/.ldf) are placed on the attached SSD storage to provide very low latency IO to your workload. High availability is implemented using a technology similar to SQL Server Always On availability groups.\n\nbut it says SIMILAR and you don't implement always on, Microsoft does it for you."
      },
      {
        "date": "2020-09-01T21:29:00.000Z",
        "voteCount": 4,
        "content": "But the question asked for three technologies\u3002\u3002"
      },
      {
        "date": "2020-03-19T05:10:00.000Z",
        "voteCount": 8,
        "content": "You can not implement Alway On Availability Group on Azure Database Premium or any SKU.\n\nhttps://docs.microsoft.com/en-au/sql/database-engine/availability-groups/windows/overview-of-always-on-availability-groups-sql-server?view=sql-server-ver15"
      },
      {
        "date": "2020-03-26T19:34:00.000Z",
        "voteCount": 1,
        "content": "Right, \"Always On Available\" only applicable  to on-prem \"SQL Server\"."
      },
      {
        "date": "2020-04-23T01:56:00.000Z",
        "voteCount": 7,
        "content": "Its on azure too : https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sql/virtual-machines-windows-portal-sql-availability-group-overview\n\nanswer is correct"
      },
      {
        "date": "2020-04-23T01:58:00.000Z",
        "voteCount": 2,
        "content": "actually this is for Azure VM - so my answer is not correct."
      },
      {
        "date": "2020-09-25T05:42:00.000Z",
        "voteCount": 1,
        "content": "For Azure SQL DB it is called \"Failover groups\" so the correct answer is: Premium service tier, Failover groups and Zone-redundant configuration."
      },
      {
        "date": "2020-05-11T17:33:00.000Z",
        "voteCount": 1,
        "content": "This is true, always on availability groups is for on premise. Answer should be Premium service tier, SQL Data Sync and Zone-redundant configuration."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33531-exam-dp-200-topic-1-question-15-discussion/",
    "body": "A company plans to use Azure Storage for file storage purposes. Compliance rules require:<br>\u2711 A single storage account to store all operations including reads, writes and deletes<br>\u2711 Retention of an on-premises copy of historical operations<br>You need to configure the storage account.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the storage account to log read, write and delete operations for service type Blob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the AzCopy tool to download log data from $logs/blob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the storage account to log read, write and delete operations for service-type table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the storage client to download log data from $logs/table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the storage account to log read, write and delete operations for service type queue"
    ],
    "answer": "AB",
    "answerDescription": "Storage Logging logs request data in a set of blobs in a blob container named $logs in your storage account. This container does not show up if you list all the blob containers in your account but you can see its contents if you access it directly.<br>To view and analyze your log data, you should download the blobs that contain the log data you are interested in to a local machine. Many storage-browsing tools enable you to download blobs from your storage account; you can also use the Azure Storage team provided command-line Azure Copy Tool (AzCopy) to download your log data.<br>References:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/enabling-storage-logging-and-accessing-log-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-24T06:04:00.000Z",
        "voteCount": 7,
        "content": "I agree with the answer we have a new feature \"Azure Storage Explorer\" which is explained in detail in the video \"https://www.youtube.com/watch?v=GJYAgi5eYYE\" which confirms the answer with proof"
      },
      {
        "date": "2020-12-21T14:19:00.000Z",
        "voteCount": 5,
        "content": "i agree with the answer"
      },
      {
        "date": "2020-11-09T05:08:00.000Z",
        "voteCount": 2,
        "content": "I DIDN'T AGREE,  When the question is asking for file storage, how the answer is related to blob storage."
      },
      {
        "date": "2020-11-21T12:29:00.000Z",
        "voteCount": 15,
        "content": "Anything can go into blob storage, including files.  However, files cannot go into tables or queues.  Since there is no choice for files, blobs can be the only choice.  AZ Copy is an excellent way to get data into a storage blob as well"
      },
      {
        "date": "2020-11-08T16:12:00.000Z",
        "voteCount": 3,
        "content": "I agree"
      },
      {
        "date": "2020-10-03T11:20:00.000Z",
        "voteCount": 3,
        "content": "I agree with the answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15670-exam-dp-200-topic-1-question-16-discussion/",
    "body": "DRAG DROP -<br>You are developing a solution to visualize multiple terabytes of geospatial data.<br>The solution has the following requirements:<br>\u2711 Data must be encrypted.<br>\u2711 Data must be accessible by multiple resources on Microsoft Azure.<br>You need to provision storage for the solution.<br>Which four actions should you perform in sequence? To answer, move the appropriate action from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0002400003.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0002500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Create a new Azure Data Lake Storage account with Azure Data Lake managed encryption keys<br>For Azure services, Azure Key Vault is the recommended key storage solution and provides a common management experience across services. Keys are stored and managed in key vaults, and access to a key vault can be given to users or services. Azure Key Vault supports customer creation of keys or import of customer keys for use in customer-managed encryption key scenarios.<br>Note: Data Lake Storage Gen1 account Encryption Settings. There are three options:<br>\u2711 Do not enable encryption.<br>\u2711 Use keys managed by Data Lake Storage Gen1, if you want Data Lake Storage Gen1 to manage your encryption keys.<br>\u2711 Use keys from your own Key Vault. You can select an existing Azure Key Vault or create a new Key Vault. To use the keys from a Key Vault, you must assign permissions for the Data Lake Storage Gen1 account to access the Azure Key Vault.<br>References:<br>https://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-17T01:07:00.000Z",
        "voteCount": 16,
        "content": "In Data Lake Gen 2, encryption is enabled by default and can't be disabled. \nhttps://docs.microsoft.com/en-us/learn/modules/secure-azure-storage-account/2-storage-security-features"
      },
      {
        "date": "2020-04-17T10:02:00.000Z",
        "voteCount": 13,
        "content": "I think this question is obsolete as in the answer we can see that they are talking about Gen1. Microsoft recommends using Data Lake Gen2."
      },
      {
        "date": "2020-09-27T03:26:00.000Z",
        "voteCount": 3,
        "content": "Confusing question."
      },
      {
        "date": "2020-03-06T04:24:00.000Z",
        "voteCount": 3,
        "content": "One of the things that are not clear for me in this kind of questions is the right order of some answers. \nIn this case, 3rd and 4th answers couldn't those be switched as keep the right order of answers of this exercise?"
      },
      {
        "date": "2020-04-15T05:37:00.000Z",
        "voteCount": 5,
        "content": "No, Actually you can't. If you don't have a policy pre-configured, you won't be able to  designate the secret."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49112-exam-dp-200-topic-1-question-17-discussion/",
    "body": "You are developing a data engineering solution for a company. The solution will store a large set of key-value pair data by using Microsoft Azure Cosmos DB.<br>The solution has the following requirements:<br>\u2711 Data must be partitioned into multiple containers.<br>\u2711 Data containers must be configured separately.<br>\u2711 Data must be accessible from applications hosted around the world.<br>\u2711 The solution must minimize latency.<br>You need to provision Azure Cosmos DB.<br>Which three actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure account-level throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Azure Cosmos DB account with the Azure Table API. Enable geo-redundancy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure table-level throughput.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicate the data globally by manually adding regions to the Azure Cosmos DB account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvision an Azure Cosmos DB account with the Azure Table API. Enable multi-region writes."
    ],
    "answer": "E",
    "answerDescription": "Scale read and write throughput globally. You can enable every region to be writable and elastically scale reads and writes all around the world. The throughput that your application configures on an Azure Cosmos database or a container is guaranteed to be delivered across all regions associated with your Azure Cosmos account. The provisioned throughput is guaranteed up by financially backed SLAs.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/distribute-data-globally",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-06T08:21:00.000Z",
        "voteCount": 37,
        "content": "The answers are C, D and E. \n\u2022\tC. Configure table-level throughput. Requirements state that containers must be configured separately.\n\u2022\tD. Replicate the data globally by manually adding regions to the Azure Cosmos DB account. By adding extra regions our data is automatically copied to those regions reducing latency.\n\u2022\tE. Provision an Azure Cosmos DB account with the Azure Table API. Enable multi-region writes. By enabling multi-region writes this also reduces latency since we don't have a single master database, but rather would be implementing a multi-master model."
      },
      {
        "date": "2021-05-06T22:52:00.000Z",
        "voteCount": 8,
        "content": "question doesn't mention the requirement for multi-region write, So as per my understanding answer should be B, C, D\nPlease suggest if my understanding is correct"
      },
      {
        "date": "2021-11-20T04:25:00.000Z",
        "voteCount": 1,
        "content": "Cosmos db support multi region write so C-D-E"
      },
      {
        "date": "2021-04-14T07:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is CDE"
      },
      {
        "date": "2021-04-07T01:17:00.000Z",
        "voteCount": 2,
        "content": "B,D,E is the answer"
      },
      {
        "date": "2021-05-05T07:16:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/set-throughput\nFrom the above link CosmosDB allows throughput at two levels \nAzure Cosmos containers\nAzure Cosmos databases\nHence B,D,E is correct answer. Sometimes we have to choose answer from list of provided answers."
      },
      {
        "date": "2021-05-05T22:48:00.000Z",
        "voteCount": 4,
        "content": "Propose solution is C, D and E.\nWhen we say Azure Cosmos containers we are pertaining to the \"CONTAINERS\" of what we chose in the creation of DB. The option C pertains to configuration in \"Cassandra API\" in which the name of the container is \"TABLE\" and there are also other containers such as Container for SQL API, Collection for Mongo DB, graph for Gremlin API and Table for Table API\n\nReference: https://azure.microsoft.com/en-us/blog/sharing-provisioned-throughput-across-multiple-containers-in-azure-cosmosdb/"
      },
      {
        "date": "2021-04-04T18:51:00.000Z",
        "voteCount": 1,
        "content": "There must be 3 answers, any one knows what are those."
      },
      {
        "date": "2021-04-06T22:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is CDE"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30725-exam-dp-200-topic-1-question-18-discussion/",
    "body": "A company has a SaaS solution that uses Azure SQL Database with elastic pools. The solution will have a dedicated database for each customer organization.<br>Customer organizations have peak usage at different periods during the year.<br>Which two factors affect your costs when sizing the Azure SQL Database elastic pools? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmaximum data size",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnumber of databases",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\teDTUs consumption",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnumber of read operations",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnumber of transactions"
    ],
    "answer": "AC",
    "answerDescription": "A: With the vCore purchase model, in the General Purpose tier, you are charged for Premium blob storage that you provision for your database or elastic pool.<br>Storage can be configured between 5 GB and 4 TB with 1 GB increments. Storage is priced at GB/month.<br>C: In the DTU purchase model, elastic pools are available in basic, standard and premium service tiers. Each tier is distinguished primarily by its overall performance, which is measured in elastic Database Transaction Units (eDTUs).<br>References:<br>https://azure.microsoft.com/en-in/pricing/details/sql-database/elastic/",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T03:46:00.000Z",
        "voteCount": 17,
        "content": "Number of databases is not affecting pricing. \nWhat affects is:\nStorage: maximum data size\neDTU: elasitc database throughput unit - unit of measurement used to determine scale in elastic pool.\n\nThus correct answers: A &amp; C"
      },
      {
        "date": "2020-11-23T05:40:00.000Z",
        "voteCount": 5,
        "content": "A &amp; C 100%"
      },
      {
        "date": "2020-09-06T13:45:00.000Z",
        "voteCount": 1,
        "content": "out of scope post July 2020"
      },
      {
        "date": "2020-11-13T21:48:00.000Z",
        "voteCount": 4,
        "content": "nope its not"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49541-exam-dp-200-topic-1-question-19-discussion/",
    "body": "HOTSPOT -<br>You are developing a solution using a Lambda architecture on Microsoft Azure.<br>The data at rest layer must meet the following requirements:<br>Data storage:<br>\u2711 Serve as a repository for high volumes of large files in various formats.<br>\u2711 Implement optimized storage for big data analytics workloads.<br>\u2711 Ensure that data can be organized using a hierarchical structure.<br>Batch processing:<br>\u2711 Use a managed solution for in-memory computation processing.<br>\u2711 Natively support Scala, Python, and R programming languages.<br>\u2711 Provide the ability to resize and terminate the cluster automatically.<br>Analytical data store:<br>\u2711 Support parallel processing.<br>\u2711 Use columnar storage.<br>\u2711 Support SQL-based languages.<br>You need to identify the correct technologies to build the Lambda architecture.<br>Which technologies should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0002900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0003000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Data storage: Azure Data Lake Store<br>A key mechanism that allows Azure Data Lake Storage Gen2 to provide file system performance at object storage scale and prices is the addition of a hierarchical namespace. This allows the collection of objects/files within an account to be organized into a hierarchy of directories and nested subdirectories in the same way that the file system on your computer is organized. With the hierarchical namespace enabled, a storage account becomes capable of providing the scalability and cost-effectiveness of object storage, with file system semantics that are familiar to analytics engines and frameworks.<br>Batch processing: HD Insight Spark<br>Aparch Spark is an open-source, parallel-processing framework that supports in-memory processing to boost the performance of big-data analysis applications.<br>HDInsight is a managed Hadoop service. Use it deploy and manage Hadoop clusters in Azure. For batch processing, you can use Spark, Hive, Hive LLAP,<br>MapReduce.<br>Languages: R, Python, Java, Scala, SQL<br>Analytic data store: Azure Synapse Analytics<br>Azure Synapse Analytics Warehouse is a cloud-based Enterprise Data Warehouse (EDW) that uses Massively Parallel Processing (MPP).<br>Azure Synapse Analytics stores data into relational tables with columnar storage.<br>Note: As of November 2019, Azure SQL Data Warehouse is now Azure Synapse Analytics.<br>References:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-overview-what-is",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-09T03:35:00.000Z",
        "voteCount": 38,
        "content": "I think that in batch processing the answer sould be Azure DataBricks, due to the link provided an it\u00b4s capabilities:\nAzure Databricks is an Apache Spark-based analytics platform. You can think of it as \"Spark as a service.\" It's the easiest way to use Spark on the Azure platform.\n\n    Languages: R, Python, Java, Scala, Spark SQL\n    Fast cluster start times, autotermination, autoscaling.\n    Manages the Spark cluster for you.\n    Built-in integration with Azure Blob Storage, Azure Data Lake Storage (ADLS), Azure Synapse, and other services. See Data Sources.\n    User authentication with Azure Active Directory.\n    Web-based notebooks for collaboration and data exploration.\n    Supports GPU-enabled cluster"
      },
      {
        "date": "2021-04-12T12:18:00.000Z",
        "voteCount": 6,
        "content": "Agree with the comments above. Databricks enables you to autoscale and autoterminate your cluster and enables also to in-memory processing because of the undelying Spark engine."
      },
      {
        "date": "2021-06-15T02:57:00.000Z",
        "voteCount": 1,
        "content": "Batch processing is Spark as it provides in memory operations"
      },
      {
        "date": "2021-06-05T00:00:00.000Z",
        "voteCount": 3,
        "content": "\"terminate the cluster automatically\" - I think this line makes Databricks a more suitable choice\nRest requirements suits both HDInsight and Databricks equally."
      },
      {
        "date": "2021-05-28T14:33:00.000Z",
        "voteCount": 1,
        "content": "ADLS\nHDInsight Spark ( https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-autoscale-clusters ) keyword here is in-memory processing\nAzure Synapse Analytics"
      },
      {
        "date": "2021-06-28T05:53:00.000Z",
        "voteCount": 1,
        "content": "Databricks has in-memory processing"
      },
      {
        "date": "2021-05-10T08:46:00.000Z",
        "voteCount": 1,
        "content": "Why not cosmos for analytical datastore?"
      },
      {
        "date": "2021-05-01T11:25:00.000Z",
        "voteCount": 2,
        "content": "For third one Synapse will work but why will Cosmos not work?"
      },
      {
        "date": "2021-04-18T03:19:00.000Z",
        "voteCount": 2,
        "content": "I think answer should be cosmos db for 3rd one. Azure synapse doesn't support columnar storage right ?"
      },
      {
        "date": "2021-04-25T23:57:00.000Z",
        "voteCount": 2,
        "content": "it does.     https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-overview-what-is"
      },
      {
        "date": "2021-04-14T07:53:00.000Z",
        "voteCount": 2,
        "content": "Batch processing should be Azure Databricks"
      },
      {
        "date": "2021-04-12T16:38:00.000Z",
        "voteCount": 4,
        "content": "that\u00b4s correct so the answers would be: Azure DataBricks, Azure DataLake and ASA (Azure Synapse)"
      },
      {
        "date": "2021-04-07T13:30:00.000Z",
        "voteCount": 5,
        "content": "Batch processing should be Azure Databricks right?"
      },
      {
        "date": "2021-04-15T17:22:00.000Z",
        "voteCount": 2,
        "content": "Yes! HDInsight does not support autotermination natively"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51178-exam-dp-200-topic-1-question-20-discussion/",
    "body": "DRAG DROP -<br>Your company has an on-premises Microsoft SQL Server instance.<br>The data engineering team plans to implement a process that copies data from the SQL Server instance to Azure Blob storage once a day. The process must orchestrate and manage the data lifecycle.<br>You need to create Azure Data Factory to connect to the SQL Server instance.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0003200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0003200002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an Azure Data Factory<br>You need to create a data factory and start the Data Factory UI to create a pipeline in the data factory.<br>Step 2: From the on-premises network, install and configure a self-hosted runtime.<br>To use copy data from a SQL Server database that isn't publicly accessible, you need to set up a self-hosted integration runtime.<br>Step 3: Configure a linked service to connect to the SQL Server instance.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/connector-sql-server https://www.mssqltips.com/sqlservertip/5812/connect-to-onpremises-data-in-azure-data-factory-with-the-selfhosted-integration-runtime--part-1/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T02:33:00.000Z",
        "voteCount": 4,
        "content": "answer with sequences is CORRECT"
      },
      {
        "date": "2021-05-10T08:50:00.000Z",
        "voteCount": 4,
        "content": "linked service ust be created after crating data factory, but SH runtime is independent, so there are many possible sequences here."
      },
      {
        "date": "2021-06-05T00:08:00.000Z",
        "voteCount": 1,
        "content": "step1 and step2 mentioned in are independent of each other , only step 3 is depedent on both of them"
      },
      {
        "date": "2021-04-29T15:36:00.000Z",
        "voteCount": 3,
        "content": "I've worked before with integration runtime and I've followed the steps in this order: Create Data Factory, Configure a link service, install and configure an Integration runtime"
      },
      {
        "date": "2021-05-11T05:27:00.000Z",
        "voteCount": 4,
        "content": "Cannot setup link service to on-premise SQL Server if the self-hosted integration runtime is not configured."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8608-exam-dp-200-topic-1-question-21-discussion/",
    "body": "A company runs Microsoft SQL Server in an on-premises virtual machine (VM).<br>You must migrate the database to Azure SQL Database. You synchronize users from Active Directory to Azure Active Directory (Azure AD).<br>You need to configure Azure SQL Database to use an Azure AD user as administrator.<br>What should you configure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Azure SQL Database, set the Access Control to administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Azure SQL Database server, set the Active Directory to administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Azure SQL Database, set the Active Directory administrator role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Azure SQL Database server, set the Access Control to administrator."
    ],
    "answer": "C",
    "answerDescription": "There are two administrative accounts (Server admin and Active Directory admin) that act as administrators.<br>One Azure Active Directory account, either an individual or security group account, can also be configured as an administrator. It is optional to configure an Azure<br>AD administrator, but an Azure AD administrator must be configured if you want to use Azure AD accounts to connect to SQL Database.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-manage-logins",
    "votes": [],
    "comments": [
      {
        "date": "2019-11-19T13:30:00.000Z",
        "voteCount": 72,
        "content": "Admin user is defined at server level.\nAnswer is B"
      },
      {
        "date": "2021-03-21T12:09:00.000Z",
        "voteCount": 2,
        "content": "+1 for this answer"
      },
      {
        "date": "2021-03-25T02:56:00.000Z",
        "voteCount": 2,
        "content": "+1 for this answer"
      },
      {
        "date": "2021-05-03T19:43:00.000Z",
        "voteCount": 3,
        "content": "Appropriate answer"
      },
      {
        "date": "2020-03-07T09:25:00.000Z",
        "voteCount": 9,
        "content": "All resources --&gt; Active Directory Admin --&gt; Set Admin\n\nAnswer is B"
      },
      {
        "date": "2021-11-20T04:33:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is B 100%"
      },
      {
        "date": "2021-08-05T21:51:00.000Z",
        "voteCount": 1,
        "content": "The question mentions migrating just 'one' database. Do we need to grant admin access to a DB server (group of DBs) ? Doesn't go with the minimal access practice. Don't agree with B."
      },
      {
        "date": "2021-06-25T22:06:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B. I did a practical implementation for the same"
      },
      {
        "date": "2021-06-13T14:59:00.000Z",
        "voteCount": 1,
        "content": "Just created a new Azure SQL server and a new Azure sql DB. Answer it B."
      },
      {
        "date": "2021-05-15T12:23:00.000Z",
        "voteCount": 2,
        "content": "Azure AD admin with a server in SQL Database\nEach server in Azure (which hosts SQL Database or Azure Synapse) starts with a single server administrator account that is the administrator of the entire server. Create a second administrator account as an Azure AD account. This principal is created as a contained database user in the master database of the server. Administrator accounts are members of the db_owner role in every user database, and enter each user database as the dbo user.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-configure?tabs=azure-powershell\n\nThe correct answer is : B"
      },
      {
        "date": "2021-04-02T10:44:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2020-12-29T23:46:00.000Z",
        "voteCount": 1,
        "content": "C is Right , Under SQL Server -&gt; select SQL database -&gt; Select ACTIVE DIRETORY ADMIN -&gt; Set Admin -&gt; Select User or Azure AD group -&gt; Save them as Admin role (Az AD Admin of the SQL database)\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-configure?tabs=azure-powershell#provision-azure-ad-admin-sql-database"
      },
      {
        "date": "2021-01-23T03:27:00.000Z",
        "voteCount": 3,
        "content": "my friend, you have accessed the sql server and not sql db. Answer is B"
      },
      {
        "date": "2020-12-26T22:09:00.000Z",
        "voteCount": 1,
        "content": "As per below link, the answer should be B.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/logins-create-manage"
      },
      {
        "date": "2020-12-13T04:43:00.000Z",
        "voteCount": 1,
        "content": "EDIT. C is correct.\nAs mentioned in the answer there are two accounts, with admin permissions to a database. Server Admin Login and Active Directory Admin. Server Admin is I guess nothing to do with AD. Go to \"Properties\" tab for Azure Database for reference. It is mentioned there"
      },
      {
        "date": "2021-01-23T03:28:00.000Z",
        "voteCount": 1,
        "content": "Well, if you go to the link and go to the properties as you've mentioned --&gt; the option is dimmed and you cannot edit it. you have to do it from the Azure sql server and not Azure sql db"
      },
      {
        "date": "2020-12-13T04:41:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\nAs mentioned in the answer there are two accounts, with admin permissions to a database. Server Admin Login and Active Directory Admin. Server Admin is I guess nothing to do with AD. Go to \"Performance\" tab for Azure Database for reference. It is mentioned there"
      },
      {
        "date": "2020-11-23T05:52:00.000Z",
        "voteCount": 2,
        "content": "From link https://docs.microsoft.com/en-us/azure/azure-sql/database/logical-servers:\nWhen you create a server, you provide a server login account and password that has administrative rights to the master database on that server and all databases created on that server. This initial account is a SQL login account. Azure SQL Database and Synapse Analytics support SQL authentication and Azure Active Directory Authentication for authentication.\nAnswer is B"
      },
      {
        "date": "2020-12-08T02:01:00.000Z",
        "voteCount": 1,
        "content": "B - the pic bellow says it all: server level (This article shows you how to create and populate an Azure Active Directory (Azure AD) instance, and then use Azure AD with Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics)\nhttps://docs.microsoft.com/pt-pt/azure/azure-sql/database/media/authentication-aad-configure/set-admin.png"
      },
      {
        "date": "2020-09-22T12:10:00.000Z",
        "voteCount": 1,
        "content": "C - Practically it makes sense to configure previliges at database level than SQL server level."
      },
      {
        "date": "2020-09-22T12:26:00.000Z",
        "voteCount": 5,
        "content": "eh...Sorry, I cant delete the above. There is no option to configure the AD admin at database level. You can do it only at server level. So answer is B"
      },
      {
        "date": "2020-09-20T12:52:00.000Z",
        "voteCount": 1,
        "content": "Using AD as administrator for SQL Databases should be on a Database level.  (Check diagaram in this page) ---&gt; https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview"
      },
      {
        "date": "2020-09-01T19:00:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\nAn Azure AD administrator must be configured if you want to use Azure AD accounts to connect to SQL Database, SQL Managed Instance, or Azure Synapse. \nRefer https://docs.microsoft.com/en-us/azure/azure-sql/database/logins-create-manage"
      },
      {
        "date": "2020-09-25T06:53:00.000Z",
        "voteCount": 2,
        "content": "To configure Azure AD admin account to a Database, it references to this link: https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-configure?tabs=azure-powershell ... in the screenshots you see that configuration has to be set on the server level. If you check the portal you can see that there is NO option to configure the AD admin at the database level. So the answer is B, but agree that the wording is incorrect."
      },
      {
        "date": "2020-08-10T08:16:00.000Z",
        "voteCount": 4,
        "content": "B: Admin is at server level.\n    not C: as each server in Azure (which hosts SQL Database) starts with a single server administrator account that is the administrator of the entire server. \ni.e. Admin at server level. \n    not D: as role-based access control (RBAC) applies only to the Azure portal and isn't propagated to SQL Database."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/22887-exam-dp-200-topic-1-question-22-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database named DB1 that contains a table named Table1. Table1 has a field named Customer_ID that is varchar(22).<br>You need to implement masking for the Customer_ID field to meet the following requirements:<br>\u2711 The first two prefix characters must be exposed.<br>\u2711 The last four suffix characters must be exposed.<br>\u2711 All other characters must be masked.<br>Solution: You implement data masking and use a credit card function mask.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Must use Custom Text data masking, which exposes the first and last characters and adds a custom padding string in the middle.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T02:38:00.000Z",
        "voteCount": 1,
        "content": "answer is NO"
      },
      {
        "date": "2020-06-28T11:01:00.000Z",
        "voteCount": 3,
        "content": "Email Masking:\nMasking method, which exposes the first letter and replaces the domain with XXX.com using a constant string prefix in the form of an email address.\n\naXX@XXXX.com\n\nReference: https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
      },
      {
        "date": "2021-04-22T13:59:00.000Z",
        "voteCount": 1,
        "content": "In the Question we have column \"Customer_ID\", I guess it is not related to Email\nalso, the first two characters to show."
      },
      {
        "date": "2020-06-11T13:31:00.000Z",
        "voteCount": 2,
        "content": "Either Q22 or Q23 must have an option of 'last for SUFFIX characters must be exposed'. Now both options are the same. In real exam they have 'SUFFIX'. Anyhow, this option also makes the answer 'NO'."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50582-exam-dp-200-topic-1-question-23-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database named DB1 that contains a table named Table1. Table1 has a field named Customer_ID that is varchar(22).<br>You need to implement masking for the Customer_ID field to meet the following requirements:<br>\u2711 The first two prefix characters must be exposed.<br>The last four suffix characters must be exposed.<br><img src=\"/assets/media/exam-media/03872/0003400005.png\" class=\"in-exam-image\"><br>\u2711 All other characters must be masked.<br>Solution: You implement data masking and use an email function mask.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Must use Custom Text data masking, which exposes the first and last characters and adds a custom padding string in the middle.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-25T02:59:00.000Z",
        "voteCount": 1,
        "content": "aXX@XXXX.com =  NO, is correct!"
      },
      {
        "date": "2021-04-20T11:28:00.000Z",
        "voteCount": 2,
        "content": "this is the right answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24289-exam-dp-200-topic-1-question-24-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database named DB1 that contains a table named Table1. Table1 has a field named Customer_ID that is varchar(22).<br>You need to implement masking for the Customer_ID field to meet the following requirements:<br>\u2711 The first two prefix characters must be exposed.<br>\u2711 The last four suffix characters must be exposed.<br>\u2711 All other characters must be masked.<br>Solution: You implement data masking and use a random number function mask.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Must use Custom Text data masking, which exposes the first and last characters and adds a custom padding string in the middle.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-17T08:05:00.000Z",
        "voteCount": 3,
        "content": "Answer: B. No.\nRandom number\tMasking method, which generates a random number according to the selected boundaries and actual data types. If the designated boundaries are equal, then the masking function is a constant number.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
      },
      {
        "date": "2020-11-23T05:56:00.000Z",
        "voteCount": 1,
        "content": "Answer to Qn 22-24 is Custom text (Masking method, which exposes the first and last characters and adds a custom padding string in the middle.)\nFrom https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
      },
      {
        "date": "2020-11-08T01:51:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: You implement data masking and use a custom text mask."
      },
      {
        "date": "2020-06-28T11:03:00.000Z",
        "voteCount": 1,
        "content": "Reference: https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51514-exam-dp-200-topic-1-question-25-discussion/",
    "body": "DRAG DROP -<br>You are responsible for providing access to an Azure Data Lake Storage Gen2 account.<br>Your user account has contributor access to the storage account, and you have the application ID and access key.<br>You plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics.<br>You need to configure PolyBase to connect the data warehouse to the storage account.<br>Which three components should you create in sequence? To answer, move the appropriate components from the list of components to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0003700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0003800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: a database scoped credential<br>To access your Data Lake Storage account, you will need to create a Database Master Key to encrypt your credential secret used in the next step. You then create a database scoped credential.<br><br>Step 2: an external data source -<br>Create the external data source. Use the CREATE EXTERNAL DATA SOURCE command to store the location of the data. Provide the credential created in the previous step.<br><br>Step 3: an external file format -<br>Configure data format: To import the data from Data Lake Storage, you need to specify the External File Format. This object defines how the files are written in<br>Data Lake Storage.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-02T02:55:00.000Z",
        "voteCount": 10,
        "content": "This answer is correct. A database scoped credential contains the authentication info to connect to external resources.  Before creating a database scoped credential, the database must have a master key. https://docs.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48710-exam-dp-200-topic-1-question-26-discussion/",
    "body": "You plan to create a dimension table in Azure Synapse Analytics that will be less than 1 GB.<br>You need to create the table to meet the following requirements:<br>\u2711 Provide the fastest query time.<br>\u2711 Minimize data movement during queries.<br>Which type of table should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash distributed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\theap",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treplicated\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround-robin"
    ],
    "answer": "D",
    "answerDescription": "Usually common dimension tables or tables that doesn't distribute evenly are good candidates for round-robin distributed table.<br>Note: Dimension tables or other lookup tables in a schema can usually be stored as round-robin tables. Usually these tables connect to more than one fact tables and optimizing for one join may not be the best idea. Also usually dimension tables are smaller which can leave some distributions empty when hash distributed.<br>Round-robin by definition guarantees a uniform data distribution.<br>Reference:<br>https://blogs.msdn.microsoft.com/sqlcat/2015/08/11/choosing-hash-distributed-table-vs-round-robin-distributed-table-in-azure-sql-dw-service/",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-02T10:44:00.000Z",
        "voteCount": 56,
        "content": "Answer is Replicated table here."
      },
      {
        "date": "2022-06-05T21:43:00.000Z",
        "voteCount": 1,
        "content": "dimension tables are normally replicated\nFact tables are normally hash due to large size of data"
      },
      {
        "date": "2021-11-20T04:37:00.000Z",
        "voteCount": 2,
        "content": "Absolutely wrong the round robin use all distrbution, the correct answer is replicated"
      },
      {
        "date": "2021-06-28T06:24:00.000Z",
        "voteCount": 3,
        "content": "dimension tables are normally replicated. also given that it is less than 1GB is another reason to use replicated.\nstaging is round-robin unless it's too big, then use hash.\nfact tables are hash distributed."
      },
      {
        "date": "2021-06-27T03:51:00.000Z",
        "voteCount": 2,
        "content": "minimize data movement --&gt; no way it can be round robin \nhas to be replicated"
      },
      {
        "date": "2021-06-16T03:58:00.000Z",
        "voteCount": 3,
        "content": "Replicated should be correct answer"
      },
      {
        "date": "2021-06-13T15:03:00.000Z",
        "voteCount": 4,
        "content": "Answer is Replicated"
      },
      {
        "date": "2021-06-12T08:59:00.000Z",
        "voteCount": 3,
        "content": "Data movement is reduced with Replicated tables because a full copy of data is stored on each Compute node. As a result, queries that required data movement steps to complete now run faster with Replicated tables.\nAnswer is Replicated table\nhttps://azure.microsoft.com/en-in/blog/replicated-tables-now-in-preview-for-azure-sql-data-warehouse/"
      },
      {
        "date": "2021-06-12T00:47:00.000Z",
        "voteCount": 2,
        "content": "Replicated is the right choice"
      },
      {
        "date": "2021-06-05T00:35:00.000Z",
        "voteCount": 1,
        "content": "I think answer should be replicated as size is already given 1GB ( not that big) , plus no data movement required as present on all nodes"
      },
      {
        "date": "2021-06-05T00:42:00.000Z",
        "voteCount": 1,
        "content": "to support it more .. from MS guide -\nWhat is a replicated table?\n\nA replicated table has a full copy of the table accessible on each Compute node. Replicating a table removes the need to transfer data among Compute nodes before a join or aggregation. Since the table has multiple copies, replicated tables work best when the table size is less than 2 GB compressed. 2 GB is not a hard limit. If the data is static and does not change, you can replicate larger tables.\n\n*My addition* - \nIn dimension data doesn't change frequently"
      },
      {
        "date": "2021-04-14T11:24:00.000Z",
        "voteCount": 2,
        "content": "The correct is Replicated\t\u2705 Small-dimension tables in a star schema with less than 2GB of storage after compression (~5x compression)."
      },
      {
        "date": "2021-04-07T04:39:00.000Z",
        "voteCount": 2,
        "content": "Replicated table: A replicated table has a full copy of the table available on every Compute node. Queries run fast on replicated tables because joins on replicated tables don't require data movement. Replication requires extra storage, though, and isn't practical for large tables."
      },
      {
        "date": "2021-04-06T09:15:00.000Z",
        "voteCount": 3,
        "content": "See this link supporting my statement. https://docs.microsoft.com/en-gb/learn/modules/design-azure-sql-data-warehouse/6-table-geometries"
      },
      {
        "date": "2021-04-06T09:11:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is REPLICATED.\n\nFact tables should almost always use hash distribution, dimension tables almost always replicated and staging tables almost always round-robin. If the dimension table is large (2 GB+) and/or used in joins with the fact table on the partition key then it may make sense to also use hash distribution for that particular dimension table."
      },
      {
        "date": "2021-04-01T21:00:00.000Z",
        "voteCount": 2,
        "content": "why not replicated table ?\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-guidance-for-replicated-tables"
      },
      {
        "date": "2021-04-01T18:23:00.000Z",
        "voteCount": 3,
        "content": "In my opinion, the correct answer is Hash-distributed based on the link below:\n\"A hash distributed table distributes rows based on the value in the distribution column. A hash distributed table is designed to achieve high performance for queries on large tables. There are several factors to consider when choosing a distribution column.\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview"
      },
      {
        "date": "2021-05-10T09:10:00.000Z",
        "voteCount": 1,
        "content": "This is not a large table, it;s very small. Hash distributed is good for large fact tables with evenly distributed column that is used in joins."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48834-exam-dp-200-topic-1-question-27-discussion/",
    "body": "You have an enterprise data warehouse in Azure Synapse Analytics.<br>Using PolyBase, you create an external table named [Ext].[Items] to query Parquet files stored in Azure Data Lake Storage Gen2 without importing the data to the data warehouse.<br>The external table has three columns.<br>You discover that the Parquet files have a fourth column named ItemID.<br>Which command should you run to add the ItemID column to the external table?<br><img src=\"/assets/media/exam-media/03872/0004000001.png\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOption A",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOption B",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOption C",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOption D"
    ],
    "answer": "A",
    "answerDescription": "Incorrect Answers:<br>B, D: Only these Data Definition Language (DDL) statements are allowed on external tables:<br>\u2711 CREATE TABLE and DROP TABLE<br>\u2711 CREATE STATISTICS and DROP STATISTICS<br>\u2711 CREATE VIEW and DROP VIEW<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-02T10:47:00.000Z",
        "voteCount": 8,
        "content": "Answer is correct"
      },
      {
        "date": "2022-06-19T03:16:00.000Z",
        "voteCount": 1,
        "content": "Only DML queries are allowed on external tables"
      },
      {
        "date": "2021-06-09T20:14:00.000Z",
        "voteCount": 1,
        "content": "Right Answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48877-exam-dp-200-topic-1-question-28-discussion/",
    "body": "DRAG DROP -<br>You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics:<br>\u2711 Is partitioned by month<br>\u2711 Contains one billion rows<br>\u2711 Has clustered columnstore indexes<br>At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible.<br>Which three actions should you perform in sequence in a stored procedure? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0004200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0004200002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an empty table named SalesFact_work that has the same schema as SalesFact.<br>Step 2: Switch the partition containing the stale data from SalesFact to SalesFact_Work.<br>SQL Data Warehouse supports partition splitting, merging, and switching. To switch partitions between two tables, you must ensure that the partitions align on their respective boundaries and that the table definitions match.<br>Loading data into partitions with partition switching is a convenient way stage new data in a table that is not visible to users the switch in the new data.<br>Step 3: Drop the SalesFact_Work table.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-partition",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-02T20:07:00.000Z",
        "voteCount": 19,
        "content": "The answer is correct"
      },
      {
        "date": "2021-07-19T06:19:00.000Z",
        "voteCount": 2,
        "content": "1.- Copy the data to a new table by using CTAS.\n2.- Switch partition...\n3.- Drop new table"
      },
      {
        "date": "2021-04-20T06:24:00.000Z",
        "voteCount": 2,
        "content": "1.- Copy the data to a new table by using CTAS.\n2.- Switch partition...\n3.- Drop new table"
      },
      {
        "date": "2021-04-25T03:15:00.000Z",
        "voteCount": 4,
        "content": "are you sure? Why copy data?"
      },
      {
        "date": "2021-05-01T20:04:00.000Z",
        "voteCount": 5,
        "content": "No need to copy data, when finally you have to drop the table, additionally, copying will take more time is not needed and is less efficient"
      },
      {
        "date": "2021-05-16T02:51:00.000Z",
        "voteCount": 1,
        "content": "No! you dont have to spend time for copying. to switch is more easy and take less time..."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17537-exam-dp-200-topic-1-question-29-discussion/",
    "body": "You plan to implement an Azure Cosmos DB database that will write 100,000,000 JSON records every 24 hours. The database will be replicated to three regions.<br>Only one region will be writable.<br>You need to select a consistency level for the database to meet the following requirements:<br>\u2711 Guarantee monotonic reads and writes within a session.<br>\u2711 Provide the fastest throughput.<br>\u2711 Provide the lowest latency.<br>Which consistency level should you select?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStrong",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBounded Staleness",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEventual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsistent Prefix"
    ],
    "answer": "D",
    "answerDescription": "Session: Within a single client session reads are guaranteed to honor the consistent-prefix (assuming a single \u05d2\u20acwriter\u05d2\u20ac session), monotonic reads, monotonic writes, read-your-writes, and write-follows-reads guarantees. Clients outside of the session performing writes will see eventual consistency.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-13T23:44:00.000Z",
        "voteCount": 34,
        "content": "I guess Andrea25 could have got this question which i found in another dump where there are three answers but the question is slightly different\nYou plan to deploy an Azure Cosmos DB database that supports multi-master replication.\n\nYou need to select a consistency level for the database to meet the following requirements:\n\n\u2013 Provide a recovery point objective (RPO) of less than 15 minutes.\n\n\u2013 Provide a recovery time objective (RTO) of zero minutes.\n\nWhat are three possible consistency levels that you can select? Each correct answer presents a complete solution. NOTE: Each correct selection is worth one point.\nA . Strong\nB . Bounded Staleness\nC . Eventual\nD . Session\nE . Consistent Prefix"
      },
      {
        "date": "2021-01-23T19:01:00.000Z",
        "voteCount": 2,
        "content": "I saw this question in other dump too.  CDE would be the three options. \nTo this question, Session is the only option. D is the answer."
      },
      {
        "date": "2020-04-10T05:21:00.000Z",
        "voteCount": 18,
        "content": "I had this question in my exam. The question ask to choice three of those:\n- Eventual;\n- Session;\n- Consistent Prefix"
      },
      {
        "date": "2020-04-12T02:04:00.000Z",
        "voteCount": 1,
        "content": "Andrea25 Were all the questions on the exam from this dump?"
      },
      {
        "date": "2020-06-10T12:43:00.000Z",
        "voteCount": 12,
        "content": "No, the actual exam DOES NOT require to provide 3 answers, I know this question. The keyword is 'monotonic', so the correct answer is SESSION. https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels"
      },
      {
        "date": "2021-04-14T11:33:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D: Session ------&gt;&gt;&gt; Session consistency is the most widely used consistency level for both single region as well as globally distributed applications. It provides write latencies, availability, and read throughput comparable to that of eventual consistency but also provides the consistency guarantees that suit the needs of applications written to operate in the context of a user"
      },
      {
        "date": "2020-11-23T06:13:00.000Z",
        "voteCount": 2,
        "content": "Link supports D as the answer"
      },
      {
        "date": "2020-03-26T22:50:00.000Z",
        "voteCount": 14,
        "content": "Keyword here is \"Guarantee monotonic reads and writes within a session.\""
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52859-exam-dp-200-topic-1-question-30-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database named DB1 that contains a table named Table1. Table1 has a field named Customer_ID that is varchar(22).<br>You need to implement masking for the Customer_ID field to meet the following requirements:<br>\u2711 The first two prefix characters must be exposed.<br>\u2711 The last four suffix characters must be exposed.<br>\u2711 All other characters must be masked.<br>Solution: You implement data masking and use a custom text mask.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "We must use Custom Text data masking, which exposes the first and last characters and adds a custom padding string in the middle.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T02:59:00.000Z",
        "voteCount": 3,
        "content": "answer is CORRECT.\nyou should mask in custom... others are not appropriate"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49527-exam-dp-200-topic-1-question-31-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You modify the files to ensure that each row is less than 1 MB.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead convert the files to compressed delimited text files.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-08T14:39:00.000Z",
        "voteCount": 9,
        "content": "The only thing I've found mentioning row size when doing the loads relates to loads using Polybase. Then the row size needs to less than 1 MB. https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"
      },
      {
        "date": "2021-09-06T08:12:00.000Z",
        "voteCount": 1,
        "content": "REcommended approach as per this: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices"
      },
      {
        "date": "2021-09-06T08:08:00.000Z",
        "voteCount": 1,
        "content": "it is not just about the size, it is the approach. the recommended approach is about zipping the file."
      },
      {
        "date": "2021-06-13T15:09:00.000Z",
        "voteCount": 4,
        "content": "Answer is Yes. File size should be less than 1 MB."
      },
      {
        "date": "2021-06-25T17:54:00.000Z",
        "voteCount": 1,
        "content": "I concur. The correct answer is 'yes' since the no size limitation update (PolyBase) applies to only SQL Server 2019+. To date, Azure Synapse Analytics still has a cap at 1 MB."
      },
      {
        "date": "2021-06-13T08:59:00.000Z",
        "voteCount": 1,
        "content": "correct answer is B"
      },
      {
        "date": "2021-04-21T09:45:00.000Z",
        "voteCount": 4,
        "content": "\"Yes.\""
      },
      {
        "date": "2021-04-13T01:11:00.000Z",
        "voteCount": 2,
        "content": "Row size should be still maximum 1 MB. So answer is YES. Also according to latest updates https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver15"
      },
      {
        "date": "2021-04-07T10:51:00.000Z",
        "voteCount": 4,
        "content": "Earlier, the answer to this question was \"Yes\", but now it shows as \"No\". Can someone confirm what is the correct answer ?"
      },
      {
        "date": "2021-05-16T12:24:00.000Z",
        "voteCount": 1,
        "content": "so what is the correct answer"
      },
      {
        "date": "2021-06-02T04:02:00.000Z",
        "voteCount": 1,
        "content": "Answer is \"yes\""
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30501-exam-dp-200-topic-1-question-32-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an Azure SQL data warehouse.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You modify the files to ensure that each row is more than 1 MB.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead modify the files to ensure that each row is less than 1 MB.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-29T19:06:00.000Z",
        "voteCount": 6,
        "content": "one word, huge difference (less/more)"
      },
      {
        "date": "2021-09-16T07:59:00.000Z",
        "voteCount": 1,
        "content": "This answer contradicts the last one"
      },
      {
        "date": "2021-05-16T03:01:00.000Z",
        "voteCount": 4,
        "content": "the answer is correct... NO"
      },
      {
        "date": "2020-09-03T08:55:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. The description is literally what is said in A."
      },
      {
        "date": "2020-09-03T08:57:00.000Z",
        "voteCount": 6,
        "content": "Ignore my comment, the answer is B. I misread."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52860-exam-dp-200-topic-1-question-33-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You copy the files to a table that has a columnstore index.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead convert the files to compressed delimited text files.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T03:09:00.000Z",
        "voteCount": 1,
        "content": "answer is CORRECT!"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52861-exam-dp-200-topic-1-question-34-discussion/",
    "body": "You plan to deploy an Azure Cosmos DB database that supports multi-master replication.<br>You need to select a consistency level for the database to meet the following requirements:<br>\u2711 Provide a recovery point objective (RPO) of less than 15 minutes.<br>\u2711 Provide a recovery time objective (RTO) of zero minutes.<br>What are three possible consistency levels that you can select? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStrong",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBounded Staleness",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEventual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsistent Prefix"
    ],
    "answer": "CDE",
    "answerDescription": "<img src=\"/assets/media/exam-media/03872/0004800001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels-choosing",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T03:10:00.000Z",
        "voteCount": 4,
        "content": "%100 CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48881-exam-dp-200-topic-1-question-35-discussion/",
    "body": "SIMULATION -<br><img src=\"/assets/media/exam-media/03872/0004900001.jpg\" class=\"in-exam-image\"><br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10277521 -<br>You need to ensure that you can recover any blob data from an Azure Storage account named storage 10277521 up to 30 days after the data is deleted.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "1. Open Azure Portal and open the Azure Blob storage account named storage10277521.<br>2. Right-click and select Blob properties<br><img src=\"/assets/media/exam-media/03872/0005000001.jpg\" class=\"in-exam-image\"><br>3. From the properties window, change the access tier for the blob to Cool.<br><img src=\"/assets/media/exam-media/03872/0005100001.jpg\" class=\"in-exam-image\"><br>Note: The cool access tier has lower storage costs and higher access costs compared to hot storage. This tier is intended for data that will remain in the cool tier for at least 30 days.<br>Reference:<br>https://dailydotnettips.com/how-to-update-access-tier-in-azure-storage-blob-level/",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-02T20:43:00.000Z",
        "voteCount": 30,
        "content": "In the Azure portal, navigate to your storage account.\nLocate the Data Protection option under Blob service.\nSet the Blob soft delete property to Enabled.\nUnder Retention policies, specify how long soft-deleted blobs are retained by Azure Storage.\nSave your changes.\nRef link https://docs.microsoft.com/en-us/azure/storage/blobs/soft-delete-blob-enable?tabs=azure-portal"
      },
      {
        "date": "2021-05-16T03:16:00.000Z",
        "voteCount": 3,
        "content": "you should set soft delete"
      },
      {
        "date": "2021-05-28T12:47:00.000Z",
        "voteCount": 2,
        "content": "Soft Delete is the option needed here , not the answer provided"
      },
      {
        "date": "2021-05-06T23:21:00.000Z",
        "voteCount": 4,
        "content": "wrong answer. the question is asking for recovery of deleted data and that can be achieve by setting soft delete property."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51058-exam-dp-200-topic-1-question-36-discussion/",
    "body": "SIMULATION -<br><img src=\"/assets/media/exam-media/03872/0005200001.jpg\" class=\"in-exam-image\"><br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10277521 -<br>You need to replicate db1 to a new Azure SQL server named REPL10277521 in the Central Canada region.<br>To complete this task, sign in to the Azure portal.<br>NOTE: This task might take several minutes to complete. You can perform other tasks while the task completes or ends this section of the exam.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "1. In the Azure portal, browse to the database that you want to set up for geo-replication.<br>2. On the SQL database page, select geo-replication, and then select the region to create the secondary database.<br><img src=\"/assets/media/exam-media/03872/0005400001.jpg\" class=\"in-exam-image\"><br>3. Select or configure the server and for the secondary database.<br><br>Region: Central Canada -<br><br>Target server: REPL10277521 -<br><img src=\"/assets/media/exam-media/03872/0005500001.png\" class=\"in-exam-image\"><br>4. Click Create to add the secondary.<br>5. The secondary database is created and the seeding process begins.<br><img src=\"/assets/media/exam-media/03872/0005600001.png\" class=\"in-exam-image\"><br>6. When the seeding process is complete, the secondary database displays its status.<br><img src=\"/assets/media/exam-media/03872/0005700001.png\" class=\"in-exam-image\"><br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-active-geo-replication-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-06T08:24:00.000Z",
        "voteCount": 3,
        "content": "can i expect these in dP 203?"
      },
      {
        "date": "2021-06-10T00:53:00.000Z",
        "voteCount": 2,
        "content": "is this type of question for now as well?"
      },
      {
        "date": "2021-04-27T05:23:00.000Z",
        "voteCount": 3,
        "content": "In the solution, it is showing, South Central US, shouldn't it be Canada Central ?"
      },
      {
        "date": "2021-05-16T03:19:00.000Z",
        "voteCount": 1,
        "content": "Yes, the showing is wrong. but the explanation is TRUE"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52599-exam-dp-200-topic-1-question-38-discussion/",
    "body": "SIMULATION -<br><img src=\"/assets/media/exam-media/03872/0006000001.jpg\" class=\"in-exam-image\"><br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10277521 -<br>You plan to query db3 to retrieve a list of sales customers. The query will retrieve several columns that include the email address of each sales customer.<br>You need to modify db3 to ensure that a portion of the email addresses is hidden in the query results.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "1. Launch the Azure portal.<br>2. Navigate to the settings page of the database db3 that includes the sensitive data you want to mask.<br>3. Click the Dynamic Data Masking tile that launches the Dynamic Data Masking configuration page.<br>Note: Alternatively, you can scroll down to the Operations section and click Dynamic Data Masking.<br><img src=\"/assets/media/exam-media/03872/0006200001.jpg\" class=\"in-exam-image\"><br>4. In the Dynamic Data Masking configuration page, you may see some database columns that the recommendations engine has flagged for masking.<br><img src=\"/assets/media/exam-media/03872/0006300001.jpg\" class=\"in-exam-image\"><br>5. Click ADD MASK for the EmailAddress column<br>6. Click Save in the data masking rule page to update the set of masking rules in the dynamic data masking policy.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-13T02:35:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't we select masking field format as 'Email'?"
      },
      {
        "date": "2021-05-16T03:46:00.000Z",
        "voteCount": 1,
        "content": "it is not mentioned in question. so in step 5 you can select format as; email, custom or default... it is not important in this question."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49530-exam-dp-200-topic-1-question-40-discussion/",
    "body": "HOTSPOT -<br>You have an enterprise data warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012.<br>You need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must meet the following requirements:<br>\u2711 Create four partitions based on the order date.<br>\u2711 Ensure that each partition contains all the orders placed during a given calendar year.<br>How should you complete the T-SQL command? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0006700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0006800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: LEFT -<br>RANGE LEFT: Specifies the boundary value belongs to the partition on the left (lower values). The default is LEFT.<br>Box 2: 20090101, 20100101, 20110101, 20120101<br>FOR VALUES ( boundary_value [,...n] ) specifies the boundary values for the partition. boundary_value is a constant expression.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-07T09:45:00.000Z",
        "voteCount": 21,
        "content": "Considerations\nCreate four partitions based on the order date.\nEnsure that each partition contains all the orders placed during a given calendar year.\n\n\nIf we chose the left option here, data buckets will be like below\nPartition 1: OrderDateKey &lt;= 20100101\nPartition 2: 20100101 &lt; OrderDateKey &lt;= 20110101\nPartition 3: 20110101 &lt; OrderDateKey &lt;= 20120101\nPartition 4: 20120101 &lt; OrderDateKey\n\nThe problem here is \"Ensure that each partition contains all the orders placed during a given calendar year.\" will Fail in this case. The OrderDateKey first day of the year ex- 20110101 will be in the partition which contains 2010 dates.\n\n\nIf we chose the Right option here, data buckets will be like below\nPartition 1: OrderDateKey &lt; 20100101\nPartition 2: 20100101 &lt;= OrderDateKey &lt; 20110101\nPartition 3: 20110101 &lt;= OrderDateKey &lt; 20120101\nPartition 4: 20120101 &lt;= OrderDateKey\n\nThis will \"Ensure that each partition contains all the orders placed during a given calendar year.\"\n\nSo Option is Right with (20100101,20110101,20120101)"
      },
      {
        "date": "2021-09-21T02:10:00.000Z",
        "voteCount": 2,
        "content": "correct ,  clearly explained at \n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7#PartitionedTable"
      },
      {
        "date": "2021-10-05T11:36:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2021-04-07T11:00:00.000Z",
        "voteCount": 19,
        "content": "The second option should be RIGHT, since we need to store calendar data in each partition.\n\nPartition 1: col &lt; 20090101\nPartition 2: 20090101 &lt;= col &lt; 20100101\nPartition 3: 20100101 &lt;= col &lt; 20110101\nPartition 4: 20110101 &lt;= col &lt; 20120101\nPartition 5: 20120101 &lt;= col"
      },
      {
        "date": "2021-05-20T12:31:00.000Z",
        "voteCount": 2,
        "content": "same question in page 38 with RIGHT answer!!!"
      },
      {
        "date": "2021-04-28T22:18:00.000Z",
        "voteCount": 12,
        "content": "This creates 5 partitions and not 4... Question says to create 4 partitions... If we have to create 4 Partitions, it would be \"LEFT\" and not \"RIGHT\"\nPartition 1: 20090101 &lt;= col &lt; 20100101\nPartition 2: 20100101 &lt;= col &lt; 20110101\nPartition 3: 20110101 &lt;= col &lt; 20120101\nPartition 4: 20120101 &lt;= col"
      },
      {
        "date": "2021-10-05T16:02:00.000Z",
        "voteCount": 1,
        "content": "Box 1 = Right, Box 2 = 20090101, 20100101, 20110101, 20120101\nIn a range left partition function, all boundary values are upper boundaries, they are the last values in the partitions. If you partition by year, you use December 31st. If you partition by month, you use January 31st, February 28th / 29th, March 31st, April 30th and so on. In a range right partition function, all boundary values are lower boundaries, they are the first values in the partitions. If you partition by year, you use January 1st. If you partition by month, you use January 1st, February 1st, March 1st, April 1st and so on:"
      },
      {
        "date": "2021-06-16T18:46:00.000Z",
        "voteCount": 1,
        "content": "LEFT is the Correct answer. We will use RIGHT when the value range is ( 20081231,20091231,20101231,20111231)"
      },
      {
        "date": "2021-06-16T10:54:00.000Z",
        "voteCount": 3,
        "content": "The answer is: LEFT, and 20100101, 20110101, 20120101. This will (1. give 4 partitions (2. Ensure each partitions contain orders made within a calendar year (E.g. 1st Jan 2009 to 31st Dec 2009)\nPARTITION 1: &lt;= 20100101 --------&gt; all orders  before AND on 31st Dec 2009\nPARTION 2: Col &gt; 1st Jan 2010 to 31st Dec 2010\nPARTITION 3: 1st Jan 2011 to 31st Dec 2011\nPARTITION 4: 1st Jan 2012 and above\n\nThis satisfies the 2 conditions of 4 partitions and each partition comprising orders in (within) a calendar year."
      },
      {
        "date": "2021-06-05T02:50:00.000Z",
        "voteCount": 8,
        "content": "Answer should be \nRight\n2010 , 2011, 2012\n\nThis ensures all the dates of one calendar year falls in same partiton and 3 boundaries gives 4 partitons"
      },
      {
        "date": "2021-06-01T13:33:00.000Z",
        "voteCount": 7,
        "content": "Answer should be RIGHT and 20100101,20110101,20120101 \n\nRange Left or Right, both are creating similar partition but there is difference in comparison \n\nFor example: in this scenario, When you use LEFT and 20100101,20110101,20120101 \nPartition will be, datecol&lt;=20100101 , datecol&gt;20100101 and datecol&lt;=20110101 , datecol&gt;20110101 and datecol&lt;=20120101, datecol&gt;20120101 \n\nBut if you use range RIGHT and 20100101,20110101,20120101 \nPartition will be, datecol&lt;20100101 , datecol&gt;=20100101 and datecol&lt;20110101 , datecol&gt;=20110101 and datecol&lt;20120101, datecol&gt;=20120101\n\nIn this example, Range RIGHT will be suitable for calendar comparison Jan 1st to Dec 31st\n\nReference: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql?view=sql-server-ver15"
      },
      {
        "date": "2021-05-12T19:10:00.000Z",
        "voteCount": 3,
        "content": "The total number of partitions is always the total number of boundary values + 1. As the question talks about four partions, second option should be the right answer.. And as it using is lower value of the right parition, it should be right."
      },
      {
        "date": "2021-05-12T10:29:00.000Z",
        "voteCount": 11,
        "content": "Check the link provided in the solution: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7\n\nRANGE LEFT: left boundary is exclusive, while the right boundary is inclusive.\nRANGE RIGHT: left boundary is inclusive, while the right boundary is exclusive.\n\n=&gt; RANGE RIGHT is certainly the correct option because 01-01-xxxx should be included in the right partition together with all the dates from year xxxx. We just need to play with the date values. \n\nPrecisely, IMO the correct answer is RANGE RIGHT FOR VALUES (20100101, 20110101, 20120101).\n- Partition 1: col &lt; 20100101\n- Partition 2: 20100101 &lt;= col &lt; 20110101\n- Partition 3: 20110101 &lt;= col &lt; 20120101\n- Partition 4: 20120101 &lt;= col\n\nIf the empty partitions are discarded, it could also be RANGE RIGHT FOR VALUES (20090101, 20100101, 20110101, 20120101) because the partition &lt;20090101 is empty. But I think this partition still exists even though it's empty, and the question specifies for 4 partitions. Please provide me evidence if I'm wrong about the existing empty partitions."
      },
      {
        "date": "2021-05-15T03:40:00.000Z",
        "voteCount": 1,
        "content": "This is the actual correct Answer."
      },
      {
        "date": "2021-05-16T23:52:00.000Z",
        "voteCount": 2,
        "content": "this is correct answer. since in question they have mentioned 4 partitions imples 3 boundaries   and right"
      },
      {
        "date": "2021-05-11T07:39:00.000Z",
        "voteCount": 1,
        "content": "Answer cannot be RIGHT, because boundary value is INCLUSIVE, so RIGHT would be a range of 2nd Jan one year up to 1st Jan next year.\nBoth 2nd and 3rd option would be valid, because there are no rows with dates &lt; 2009, so crating left boundary on 01.01.2009 will not create fifth partition, because it would had to be empty."
      },
      {
        "date": "2021-05-07T14:20:00.000Z",
        "voteCount": 2,
        "content": "The range should be RIGHT if you follow the link below.\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7#PartitionedTable"
      },
      {
        "date": "2021-05-04T08:39:00.000Z",
        "voteCount": 1,
        "content": "What is the correct answer??"
      },
      {
        "date": "2021-05-12T22:02:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is Left and option 2 .\nPARTITION ( id RANGE LEFT FOR VALUES (10, 20, 30, 40 )),  \n    CLUSTERED COLUMNSTORE INDEX\n\nPartition 1: col &lt;= 10\nPartition 2: 10 &lt; col &lt;= 20\nPartition 3: 20 &lt; col &lt;= 30\nPartition 4: 30 &lt; col &lt;= 40\nPartition 5: 40 &lt; col\n\n4 values creates 5 partitions , so we need to go for 3 values that will create 4 partitions."
      },
      {
        "date": "2021-05-03T03:35:00.000Z",
        "voteCount": 1,
        "content": "This can be a bit confusing if you don't read and interpret what's said in the link below with an open mind; \nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7\n\nscroll to \"Table partition options\"\npartition_column_name\t- Specifies the column that Azure Synapse Analytics will use to partition the rows. This column can be any data type. Azure Synapse Analytics sorts the partition column values in ascending order. The low-to-high ordering goes from LEFT to RIGHT in the RANGE specification.\n\nRANGE LEFT\t- Specifies the boundary value belongs to the partition on the left (lower values). The default is LEFT.\n\nRANGE RIGHT  - Specifies the boundary value belongs to the partition on the right (higher values).\n\nI will go with the answers provided, \"LEFT and the 4 date partitions\" - because the questions state that it's to the end of 2012, so the lower boundary will be the start of that year (2012-01-01), therefore LEFT range."
      },
      {
        "date": "2021-04-30T18:36:00.000Z",
        "voteCount": 2,
        "content": "The answer should be LEFT and 2nd Line."
      },
      {
        "date": "2021-04-28T08:35:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is 'RIGHT'\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse?view=aps-pdw-2016-au7\n(Scroll towards the end)\n\nLEFT/RIGHT is confusing, trick to remember is\n1. Draw a hyphen among the values\n20090101-20100101-20110101-20120101\n2. pick one value (say 20100101)\n3. Now understand, in which range do we want 20100101 to fall? here we want it in 20100101-20110101.\n4. 20110101 is towards its right, Hence 'RIGHT'"
      },
      {
        "date": "2021-04-28T00:49:00.000Z",
        "voteCount": 1,
        "content": "In a range left partition function, all boundary values are upper boundaries, they are the last values in the partitions. If you partition by year, you use December 31st. If you partition by month, you use January 31st, February 28th / 29th, March 31st, April 30th and so on. In a range right partition function, all boundary values are lower boundaries, they are the first values in the partitions. If you partition by year, you use January 1st. If you partition by month, you use January 1st, February 1st, March 1st, April 1st and so on.\n\nReference: https://www.cathrinewilhelmsen.net/table-partitioning-in-sql-server/#:~:text=Range%20Left%20and%20Range%20Right&amp;text=Range%20left%20means%20that%20the,value%20in%20the%20right%20partition."
      },
      {
        "date": "2021-04-24T10:58:00.000Z",
        "voteCount": 1,
        "content": "The range is correct. We need to create 4 partitions, 2009-2010, 2010-2011, 2011-2012, and 2012-2013, because we want data till the end of 2012."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62214-exam-dp-200-topic-1-question-41-discussion/",
    "body": "SIMULATION -<br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10543936 -<br><img src=\"/assets/media/exam-media/03872/0007000001.jpg\" class=\"in-exam-image\"><br>You need to create an elastic pool that contains an Azure SQL database named db2 and a new SQL database named db3.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "Step 1: Create a new SQL database named db3<br>1. Select SQL in the left-hand menu of the Azure portal. If SQL is not in the list, select All services, then type SQL in the search box.<br>2. Select + Add to open the Select SQL deployment option page. Select Single Database. You can view additional information about the different databases by selecting Show details on the Databases tile.<br>3. Select Create:<br><img src=\"/assets/media/exam-media/03872/0007100002.jpg\" class=\"in-exam-image\"><br><img src=\"/assets/media/exam-media/03872/0007100001.png\" class=\"in-exam-image\"><br>4. Enter the required fields if necessary.<br>5. Leave the rest of the values as default and select Review + Create at the bottom of the form.<br>6. Review the final settings and select Create. Use Db3 as database name.<br>On the SQL Database form, select Create to deploy and provision the resource group, server, and database.<br>Step 2: Create your elastic pool using the Azure portal.<br>1. Select Azure SQL in the left-hand menu of the Azure portal. If Azure SQL is not in the list, select All services, then type Azure SQL in the search box.<br>2. Select + Add to open the Select SQL deployment option page.<br>3. Select Elastic pool from the Resource type drop-down in the SQL Databases tile. Select Create to create your elastic pool.<br><img src=\"/assets/media/exam-media/03872/0007200001.jpg\" class=\"in-exam-image\"><br>4. Configure your elastic pool with the following values:<br>Name: Provide a unique name for your elastic pool, such as myElasticPool.<br>Subscription: Select your subscription from the drop-down.<br>ResourceGroup: Select the resource group.<br><br>Server: Select the server -<br><img src=\"/assets/media/exam-media/03872/0007300001.jpg\" class=\"in-exam-image\"><br>5. Select Configure elastic pool<br>6. On the Configure page, select the Databases tab, and then choose to Add database.<br><img src=\"/assets/media/exam-media/03872/0007400001.jpg\" class=\"in-exam-image\"><br>7. Add the Azure SQL database named db2, and the new SQL database named db3 that you created in Step 1.<br>8. Select Review + create to review your elastic pool settings and then select Create to create your elastic pool.<br>Reference:<br>https://docs.microsoft.com/bs-latn-ba/azure/sql-database/sql-database-elastic-pool-failover-group-tutorial",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-16T08:38:00.000Z",
        "voteCount": 1,
        "content": "The answer is obsolete"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49571-exam-dp-200-topic-1-question-42-discussion/",
    "body": "SIMULATION -<br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10543936 -<br><img src=\"/assets/media/exam-media/03872/0007500001.jpg\" class=\"in-exam-image\"><br>You need to create an Azure Storage account named account10543936. The solution must meet the following requirements:<br>\u2711 Minimize storage costs.<br>\u2711 Ensure that account10543936 can store many image files.<br>Ensure that account10543936 can quickly retrieve stored image files.<br><img src=\"/assets/media/exam-media/03872/0007500004.png\" class=\"in-exam-image\"><br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "Create a general-purpose v2 storage account, which provides access to all of the Azure Storage services: blobs, files, queues, tables, and disks.<br>1. On the Azure portal menu, select All services. In the list of resources, type Storage Accounts. As you begin typing, the list filters based on your input. Select<br>Storage Accounts.<br>2. On the Storage Accounts window that appears, choose Add.<br>3. Select the subscription in which to create the storage account.<br>4. Under the Resource group field, select Create new. Enter the name for your new resource group, as shown in the following image.<br><img src=\"/assets/media/exam-media/03872/0007700001.jpg\" class=\"in-exam-image\"><br>5. Next, enter the name account10543936 for your storage account.<br>6. Select a location for your storage account, or use the default location.<br>7. Leave these fields set to their default values:<br>Deployment model: Resource Manager<br><br>Performance: Standard -<br>Account kind: StorageV2 (general-purpose v2)<br>Replication: Read-access geo-redundant storage (RA-GRS)<br><br>Access tier: Hot -<br>8. Select Review + Create to review your storage account settings and create the account.<br>9. Select Create.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-account-create",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-19T08:07:00.000Z",
        "voteCount": 6,
        "content": "LRS is  thee cheapest one for cost."
      },
      {
        "date": "2021-05-02T04:10:00.000Z",
        "voteCount": 6,
        "content": "I agree.  Locally redundant storage is a cheaper option than globally redundant, and does not negatively impact any of the other requirements of the scenario."
      },
      {
        "date": "2021-04-07T22:16:00.000Z",
        "voteCount": 2,
        "content": "I believe Storage tier should be Cool.. in order to minimize the cost"
      },
      {
        "date": "2021-04-08T16:23:00.000Z",
        "voteCount": 2,
        "content": "The question states \"must be able to quickly retrieve...\". Thus, storage has to be hot, BUT it also means we should use RA-GRS not LRS nor GRS."
      },
      {
        "date": "2021-05-21T01:55:00.000Z",
        "voteCount": 1,
        "content": "in the new azure there is no direct RA-GRS. you have to select Geo-zone-redundant storage (GZRS) and then check make read access to data available in the event of regional unavailability"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49429-exam-dp-200-topic-1-question-43-discussion/",
    "body": "SIMULATION -<br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10543936 -<br><img src=\"/assets/media/exam-media/03872/0007900001.jpg\" class=\"in-exam-image\"><br>You need to ensure that users in the West US region can read data from a local copy of an Azure Cosmos DB database named cosmos10543936.<br>To complete this task, sign in to the Azure portal.<br>NOTE: This task might take several minutes to complete. You can perform other tasks while the task completes or end this section of the exam.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "You can enable Availability Zones by using Azure portal when creating an Azure Cosmos account.<br>You can enable Availability Zones by using Azure portal.<br>Step 1: enable the Geo-redundancy, Multi-region Writes<br>1. In Azure Portal search for and select Azure Cosmos DB.<br>2. Locate the Cosmos DB database named cosmos10543936<br>3. Access the properties for cosmos10543936<br>4. enable the Geo-redundancy, Multi-region Writes.<br><br>Location: West US region -<br><img src=\"/assets/media/exam-media/03872/0008000001.jpg\" class=\"in-exam-image\"><br>Step 2: Add region from your database account<br>1. In to Azure portal, go to your Azure Cosmos account, and open the Replicate data globally menu.<br>2. To add regions, select the hexagons on the map with the + label that corresponds to your desired region(s). Alternatively, to add a region, select the + Add region option and choose a region from the drop-down menu.<br><br>Add: West US region -<br><img src=\"/assets/media/exam-media/03872/0008100001.jpg\" class=\"in-exam-image\"><br>3. To save your changes, select OK.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/high-availability https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-manage-database-account",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-13T23:25:00.000Z",
        "voteCount": 1,
        "content": "The question specifies \"users in the West US region can READ data\". Geo redundancy should cut it and Multi region writes are unnecessary."
      },
      {
        "date": "2021-04-06T11:52:00.000Z",
        "voteCount": 2,
        "content": "There is no need for multi-region writes only geo-redundancy. See https://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview You can create the secondary across the country where it would have low latency for someone nearby and it would be readable."
      },
      {
        "date": "2021-04-08T01:37:00.000Z",
        "voteCount": 3,
        "content": "The question is asking the Azure Cosmos DB, not Azure SQL Database in your link."
      },
      {
        "date": "2021-04-08T17:11:00.000Z",
        "voteCount": 3,
        "content": "I am also now wondering why they suggested using the availability zones feature. The way the question is worded, \"ensure that users in the West region can read...\" makes me wonder if \"ensure\" is the key word here and that's why they are also enabling the availability zones which I would not have done if I was only interested in saving money."
      },
      {
        "date": "2021-05-06T19:06:00.000Z",
        "voteCount": 1,
        "content": "It make sense to enable the \"Availability Zones\" as part of the requirement since there is a chance the replicated server could go down."
      },
      {
        "date": "2021-04-08T16:49:00.000Z",
        "voteCount": 4,
        "content": "You're right. Sorry about that. In this link (https://docs.microsoft.com/en-us/azure/cosmos-db/high-availability) look at the section on multi-region accounts with a single write region. There is no need given the stated objectives to enable multi-region writes. Thanks for the catch."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52001-exam-dp-200-topic-1-question-44-discussion/",
    "body": "SIMULATION -<br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10543936 -<br><img src=\"/assets/media/exam-media/03872/0008200001.jpg\" class=\"in-exam-image\"><br>You plan to enable Azure Multi-Factor Authentication (MFA).<br>You need to ensure that User1-10543936@ExamUsers.com can manage any databases hosted on an Azure SQL server named SQL10543936 by signing in using his Azure Active Directory (Azure AD) user account.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "Provision an Azure Active Directory administrator for your managed instance<br>Each Azure SQL server (which hosts a SQL Database or SQL Data Warehouse) starts with a single server administrator account that is the administrator of the entire Azure SQL server. A second SQL Server administrator must be created, that is an Azure AD account. This principal is created as a contained database user in the master database.<br>1. In the Azure portal, in the upper-right corner, select your connection to drop down a list of possible Active Directories. Choose the correct Active Directory as the default Azure AD. This step links the subscription-associated Active Directory with Azure SQL server making sure that the same subscription is used for both<br>Azure AD and SQL Server. (The Azure SQL server can be hosting either Azure SQL Database or Azure SQL Data Warehouse.)<br><img src=\"/assets/media/exam-media/03872/0008400001.jpg\" class=\"in-exam-image\"><br>2. Search for and select the SQL server SQL10543936<br><img src=\"/assets/media/exam-media/03872/0008500001.jpg\" class=\"in-exam-image\"><br>3. In SQL Server page, select Active Directory admin.<br>4. In the Active Directory admin page, select Set admin.<br><img src=\"/assets/media/exam-media/03872/0008600001.jpg\" class=\"in-exam-image\"><br>5. In the Add admin page, search for user User1-10543936@ExamUsers.com, select it, and then select Select. (The Active Directory admin page shows all members and groups of your Active Directory. Users or groups that are grayed out cannot be selected because they are not supported as Azure AD administrators.<br><img src=\"/assets/media/exam-media/03872/0008700001.jpg\" class=\"in-exam-image\"><br>6. At the top of the Active Directory admin page, select SAVE.<br><img src=\"/assets/media/exam-media/03872/0008800001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-aad-authentication-configure?",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-06T09:45:00.000Z",
        "voteCount": 3,
        "content": "Answer here: https://docs.microsoft.com/en-us/azure/active-directory/authentication/tutorial-enable-azure-mfa"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49125-exam-dp-200-topic-1-question-45-discussion/",
    "body": "HOTSPOT -<br>You have the following Azure Stream Analytics query.<br><img src=\"/assets/media/exam-media/03872/0008900001.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0009000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0009000002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: No -<br>Note: You can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.<br>The outcome is a stream that has the same partition scheme. Please see below for an example:<br>WITH step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)<br>SELECT * INTO [output] FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID<br>Note: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.<br><br>Box 2: Yes -<br>When joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count.<br><br>Box 3: Yes -<br>Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.<br>In general, the best practice is to start with 6 SUs for queries that don't use PARTITION BY.<br>Here there are 10 partitions, so 6x10 = 60 SUs is good.<br>Note: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, must be adjusted so the number of physical resources available to the job can fit the partitioned flow. In general, six SUs is a good number to assign to each partition. In case there are insufficient resources assigned to the job, the system will only apply the repartition if it benefits the job.<br>Reference:<br>https://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-01T02:24:00.000Z",
        "voteCount": 13,
        "content": "Answer is No,No,No (source whizlab)\nHere we are using the UNION clause which is different from the JOIN clause\nWe need to match the partition key in the input and output scheme, but not necessarily need to match the count.\nYou can scale up to 6 streaming units for each step in a job. If you have partitions, you need to\nmultiply the number of partitions by 6.\nNow in the query, we have 2 select queries in the input streams. And each has a partition count of 10.\nThat means we can scale the job to the following number of streaming units.\nNumber of SELECT queries * Number of partitions * 6\n= 2*10*6 = 120\nIn the query, we have one SELECT statement in the output with no partition count. Hence the\ncalculation for the maximum number of streaming units is\nNumber of SELECT queries * 6 = 6.\nHence the total number of streaming units that can be assigned to the job is 126."
      },
      {
        "date": "2021-04-04T23:19:00.000Z",
        "voteCount": 10,
        "content": "Yes at 1st Question"
      },
      {
        "date": "2021-11-20T04:48:00.000Z",
        "voteCount": 2,
        "content": "the answer is yes-yes-yes"
      },
      {
        "date": "2021-06-23T04:34:00.000Z",
        "voteCount": 3,
        "content": "The answer is yes, yes, no. The example comes almost directly from the microsoft documentation. Quoting from docs.microsoft.com/en-us/azure/stream-analytics/repartition:\nThe following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.\n\nSQL\nWITH step1 AS (SELECT * FROM input1 PARTITION BY DeviceID),\nstep2 AS (SELECT * FROM input2 PARTITION BY DeviceID)\n\nSELECT * INTO output FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID\n\nThe documentation clearly states that this example consitutes a join of partitioned streams, even though traditionally one might not consider a union a join. \nOthers have explained why 60 stu is not optimal."
      },
      {
        "date": "2021-06-23T04:37:00.000Z",
        "voteCount": 2,
        "content": "I forgot to include the following from the same documetation:\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently. I guess the question states \"must\" while the documentation states \"should\", so I guess \"no\" is the technically correct answer here."
      },
      {
        "date": "2021-06-05T06:05:00.000Z",
        "voteCount": 3,
        "content": "Second Option should be No. \nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently. \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/repartition"
      },
      {
        "date": "2021-06-02T06:35:00.000Z",
        "voteCount": 2,
        "content": "I don't think \"Union step2 By StateID\" it is valid."
      },
      {
        "date": "2021-06-01T09:00:00.000Z",
        "voteCount": 3,
        "content": "so what is the answer??"
      },
      {
        "date": "2021-05-01T12:01:00.000Z",
        "voteCount": 2,
        "content": "3rd question is correct. While 126 RUs might be the most ideal, 60 RUs will still optimize the preformance."
      },
      {
        "date": "2021-04-28T20:55:00.000Z",
        "voteCount": 2,
        "content": "Answer for RU unit is wrong. Ideal RU unit required is 126"
      },
      {
        "date": "2021-04-28T01:20:00.000Z",
        "voteCount": 3,
        "content": "The following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.\n\nWITH step1 AS (SELECT * FROM input1 PARTITION BY DeviceID),\nstep2 AS (SELECT * FROM input2 PARTITION BY DeviceID)\n\nSELECT * INTO output FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID\n\nReference: https://docs.microsoft.com/en-us/azure/stream-analytics/repartition"
      },
      {
        "date": "2021-05-06T19:14:00.000Z",
        "voteCount": 2,
        "content": "Propose solution is No, Yes, Yes\n\nReferencing the query, the \"PARTITION\" keyword is missing after the \"union step2\""
      },
      {
        "date": "2021-04-24T09:41:00.000Z",
        "voteCount": 2,
        "content": "why are some people saying that the first one is also yes? A Union is not a join. A union will combine the rows from both steps together. A join would combine the columns based on some join condition"
      },
      {
        "date": "2021-05-19T08:27:00.000Z",
        "voteCount": 1,
        "content": "and yet Microsoft calls union \"join\" see https://docs.microsoft.com/en-us/azure/stream-analytics/repartition"
      },
      {
        "date": "2021-04-23T08:14:00.000Z",
        "voteCount": 2,
        "content": "In the union query for step2 there is no partition by. so may be the query is wront. thats the reason for No for 1st question"
      },
      {
        "date": "2021-04-23T00:15:00.000Z",
        "voteCount": 1,
        "content": "It should be yes,yes,yes"
      },
      {
        "date": "2021-04-20T07:16:00.000Z",
        "voteCount": 2,
        "content": "No, No and No (source Whizlabs)"
      },
      {
        "date": "2021-04-06T05:04:00.000Z",
        "voteCount": 4,
        "content": "Box 1 is Yes. \n\nWhen joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count. The outcome is a stream that has the same partition scheme. \n\nhttps://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96171-exam-dp-200-topic-1-question-46-discussion/",
    "body": "DRAG DROP -<br>You have an Azure SQL database named DB1 in the East US 2 region.<br>You need to build a secondary geo-replicated copy of DB1 in the West US region on a new server.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0009200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0009300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: From the Geo-replication settings of DB1, select West US<br>The following steps create a new secondary database in a geo-replication partnership.<br>1. In the Azure portal, browse to the database that you want to set up for geo-replication.<br>2. (Step 1) On the SQL database page, select geo-replication, and then select the region to create the secondary database.<br>3. (Step 2-3) Select or configure the server and pricing tier for the secondary database.<br><img src=\"/assets/media/exam-media/03872/0009400001.jpg\" class=\"in-exam-image\"><br>Step 2: Create a target server and select a pricing tier<br>Step 3: On the secondary server, create logins that match the SIDs on the primary server.<br>Incorrect Answers:<br>Not log shipping: Replication is used.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-active-geo-replication-portal",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-20T08:38:00.000Z",
        "voteCount": 2,
        "content": "Answer looks good. Very similar question above in the form of Simulation."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52868-exam-dp-200-topic-1-question-47-discussion/",
    "body": "HOTSPOT -<br>You have an Azure SQL database that contains a table named Employee. Employee contains sensitive data in a decimal (10,2) column named Salary.<br>You need to ensure that nonprivileged users can view the table data, but Salary must display a number from 0 to 100.<br>What should you configure? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0009600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0009700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: SELECT -<br>Users with SELECT permission on a table can view the table data. Columns that are defined as masked, will display the masked data.<br>Incorrect:<br>Grant the UNMASK permission to a user to enable them to retrieve unmasked data from the columns for which masking is defined.<br>The CONTROL permission on the database includes both the ALTER ANY MASK and UNMASK permission.<br><br>Box 2: Random number -<br>Random number: Masking method, which generates a random number according to the selected boundaries and actual data types. If the designated boundaries are equal, then the masking function is a constant number.<br><img src=\"/assets/media/exam-media/03872/0009800001.jpg\" class=\"in-exam-image\">",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T04:09:00.000Z",
        "voteCount": 5,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52887-exam-dp-200-topic-1-question-48-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure subscription that contains an Azure Storage account.<br>You plan to implement changes to a data storage solution to meet regulatory and compliance standards.<br>Every day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.<br>Solution: You apply an Azure policy that tags the storage account.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead apply an Azure Blob storage lifecycle policy.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T08:41:00.000Z",
        "voteCount": 2,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27538-exam-dp-200-topic-1-question-49-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure subscription that contains an Azure Storage account.<br>You plan to implement changes to a data storage solution to meet regulatory and compliance standards.<br>Every day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.<br>Solution: You apply an expired tag to the blobs in the storage account.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead apply an Azure Blob storage lifecycle policy.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-06T22:07:00.000Z",
        "voteCount": 10,
        "content": "Data sets have unique lifecycles. Early in the lifecycle, people access some data often. But the need for access drops drastically as the data ages. Some data stays idle in the cloud and is rarely accessed once stored. Some data expires days or months after creation, while other data sets are actively read and modified throughout their lifetimes. Azure Blob storage lifecycle management offers a rich, rule-based policy for GPv2 and Blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.\n\nThe lifecycle management policy lets you:\n\nTransition blobs to a cooler storage tier (hot to cool, hot to archive, or cool to archive) to optimize for performance and cost\nDelete blobs at the end of their lifecycles\nDefine rules to be run once per day at the storage account level\nApply rules to containers or a subset of blobs (using name prefixes or blob index tags as filters)"
      },
      {
        "date": "2020-11-26T05:43:00.000Z",
        "voteCount": 1,
        "content": "Agree!"
      },
      {
        "date": "2021-05-16T08:41:00.000Z",
        "voteCount": 1,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39304-exam-dp-200-topic-1-question-50-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure subscription that contains an Azure Storage account.<br>You plan to implement changes to a data storage solution to meet regulatory and compliance standards.<br>Every day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.<br>Solution: You apply an Azure Blob storage lifecycle policy.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Azure Blob storage lifecycle management offers a rich, rule-based policy for GPv2 and Blob storage accounts. Use the policy to transition your data to the appropriate access tiers or expire at the end of the data's lifecycle.<br>The lifecycle management policy lets you:<br>\u2711 Transition blobs to a cooler storage tier (hot to cool, hot to archive, or cool to archive) to optimize for performance and cost<br>\u2711 Delete blobs at the end of their lifecycles<br>\u2711 Define rules to be run once per day at the storage account level<br>\u2711 Apply rules to containers or a subset of blobs (using prefixes as filters)<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T08:42:00.000Z",
        "voteCount": 2,
        "content": "answer is CORRECT"
      },
      {
        "date": "2020-12-08T16:05:00.000Z",
        "voteCount": 1,
        "content": "duplicate"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21029-exam-dp-200-topic-1-question-51-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database named DB1 that contains a table named Table1. Table1 has a field named Customer_ID that is varchar(22).<br>You need to implement masking for the Customer_ID field to meet the following requirements:<br>\u2711 The first two prefix characters must be exposed.<br>\u2711 The last four suffix characters must be exposed.<br>\u2711 All other characters must be masked.<br>Solution: You implement data masking and use a custom string function mask.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Must use Custom Text data masking, which exposes the first and last characters and adds a custom padding string in the middle.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-02T20:41:00.000Z",
        "voteCount": 17,
        "content": "For this question the last sentence is  \"You implement data masking and use a custom text mask.\" and the answer is yes."
      },
      {
        "date": "2020-05-20T13:04:00.000Z",
        "voteCount": 7,
        "content": "This question appeared as Topic 1 Question #22 earlier"
      },
      {
        "date": "2021-06-10T17:12:00.000Z",
        "voteCount": 1,
        "content": "Answer is Correct only .It should be \"You implement data masking and use a custom text mask.\""
      },
      {
        "date": "2021-06-05T06:28:00.000Z",
        "voteCount": 3,
        "content": "It should be \"Yes\"; Just test by adding sample database while creating sql database in azure. Goto Dynamic Masking and click any field to add mask and edit the mask.\n\nIf you check, you can see only below types.\n\n1. Default Value\n2. Credit Card\n3. Email\n4. Number\n5. Custom String\n\nCheck the latest document :  03/24/2021\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver15"
      },
      {
        "date": "2021-04-19T12:01:00.000Z",
        "voteCount": 7,
        "content": "Newly updated document (1/25/2021) online Masking function called custom text. The old document online  it was called custom string."
      },
      {
        "date": "2021-04-19T08:17:00.000Z",
        "voteCount": 3,
        "content": "Answer should be Yes"
      },
      {
        "date": "2021-04-08T17:27:00.000Z",
        "voteCount": 1,
        "content": "I think for this answer they were being too literal in their interpretation of first/last characters for the custom mask. That does not mean exactly 1 at front and back, but rather that those are parameters that let you determine how many to expose at the front and back. 0, 1 or more are all permissible."
      },
      {
        "date": "2021-01-23T23:38:00.000Z",
        "voteCount": 2,
        "content": "I was expecting a question with 'Yes' solution which is to use custom text masking. this is repeative."
      },
      {
        "date": "2021-01-07T03:57:00.000Z",
        "voteCount": 1,
        "content": "Repeated question."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52210-exam-dp-200-topic-1-question-52-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure subscription that contains an Azure Storage account.<br>You plan to implement changes to a data storage solution to meet regulatory and compliance standards.<br>Every day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.<br>Solution: You schedule an Azure Data Factory pipeline.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead you can use the Delete Activity in Azure Data Factory to delete files or folders from on-premises storage stores or cloud storage stores or apply an Azure<br>Blob storage lifecycle policy.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/delete-activity https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-09T09:43:00.000Z",
        "voteCount": 5,
        "content": "This should be Yes right?"
      },
      {
        "date": "2021-05-12T11:04:00.000Z",
        "voteCount": 4,
        "content": "I also think \"yes\": https://docs.microsoft.com/en-us/azure/data-factory/delete-activity#clean-up-the-expired-files-that-were-last-modified-before-201811\n\n\"You can create a pipeline to clean up the old or expired files by leveraging file attribute filter: \u201cLastModified\u201d in dataset.\"\n\nAnd it uses blob files: \n\"dataset\": { \n    \"referenceName\":\"BlobFilesLastModifiedBefore201811\", \n    ... \n}"
      },
      {
        "date": "2021-09-21T09:31:00.000Z",
        "voteCount": 1,
        "content": "when it can be done with Azure Blob storage lifecycle policy, why data factory ?"
      },
      {
        "date": "2021-05-19T13:01:00.000Z",
        "voteCount": 2,
        "content": "There is no activity mentioned, it should be No.\nAm I right?"
      },
      {
        "date": "2021-05-27T08:26:00.000Z",
        "voteCount": 1,
        "content": "Correct question is question #58"
      },
      {
        "date": "2021-05-18T02:18:00.000Z",
        "voteCount": 2,
        "content": "It should be Yes."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54604-exam-dp-200-topic-1-question-53-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You convert the files to compressed delimited text files.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "All file formats have different performance characteristics. For the fastest load, use compressed delimited text files.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-06T07:42:00.000Z",
        "voteCount": 1,
        "content": "It should be \"Yes\". \nCompress the source file is good practice in this use case,. Polybase is not mandatory in this question and the data to transfert is not so huge. As mentionned by Microsoft below, we can turn off \"using polybase\" which avoid the overhead of splitting the files. \n\n\"Row size and data type limits\nPolyBase loads are limited to rows smaller than 1 MB. It cannot be used to load to VARCHR(MAX), NVARCHAR(MAX), or VARBINARY(MAX). For more information, see Azure Synapse Analytics service capacity limits.\n\nWhen your source data has rows greater than 1 MB, you might want to vertically split the source tables into several small ones. Make sure that the largest size of each row doesn't exceed the limit. The smaller tables can then be loaded by using PolyBase and merged together in Azure Synapse Analytics.\n\nAlternatively, for data with such wide columns, you can use non-PolyBase to load the data by turning off \"allow PolyBase\" setting.\"\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory"
      },
      {
        "date": "2021-06-05T06:30:00.000Z",
        "voteCount": 1,
        "content": "It should be \"No\" as the row length should be less than 1 MB"
      },
      {
        "date": "2021-06-23T04:58:00.000Z",
        "voteCount": 2,
        "content": "The max 1 MB limit only applies if you plan to use polybase"
      },
      {
        "date": "2021-06-21T17:57:00.000Z",
        "voteCount": 1,
        "content": "Check answer 31. It seems that the 1 MB limit is no longer necessary."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37840-exam-dp-200-topic-1-question-54-discussion/",
    "body": "You are designing an enterprise data warehouse in Azure Synapse Analytics. You plan to load millions of rows of data into the data warehouse each day.<br>You must ensure that staging tables are optimized for data loading.<br>You need to design the staging tables.<br>What type of tables should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRound-robin distributed table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHash-distributed table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicated table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExternal table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-26T05:59:00.000Z",
        "voteCount": 17,
        "content": "From https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview:\nUse round-robin for the staging table. The load with CTAS is fast. Once the data is in the staging table, use INSERT...SELECT to move the data to production tables."
      },
      {
        "date": "2021-05-16T09:13:00.000Z",
        "voteCount": 1,
        "content": "keyword is staging table"
      },
      {
        "date": "2021-06-27T04:15:00.000Z",
        "voteCount": 1,
        "content": "keyword --&gt; staging table --&gt; round robin is best"
      },
      {
        "date": "2021-04-29T21:33:00.000Z",
        "voteCount": 3,
        "content": "round robin is entirely correct for staging tables"
      },
      {
        "date": "2021-01-09T20:41:00.000Z",
        "voteCount": 4,
        "content": "answer is correct its specifically asked for fast loading , not read, which leads to round robin being the correct answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49009-exam-dp-200-topic-1-question-55-discussion/",
    "body": "HOTSPOT -<br>You have a SQL pool in Azure Synapse.<br>You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.<br>You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.<br>How should you configure the table? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0010400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0010500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash -<br>Hash-distributed tables improve query performance on large fact tables. hey can have very large numbers of rows and still achieve high performance.<br>Incorrect:<br>Round-robin tables are useful for improving loading speed.<br><br>Box 2: Clustered columnstore -<br>When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.<br><br>Box 3: Date -<br>Table partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column.<br>Partition switching can be used to quickly remove or replace a section of a table.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-15T04:19:00.000Z",
        "voteCount": 68,
        "content": "Round-Robin\nHeap\nNone"
      },
      {
        "date": "2021-04-03T23:33:00.000Z",
        "voteCount": 10,
        "content": "From https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview:\n\"Use round-robin for the staging table. The load with CTAS is fast. Once the data is in the staging table, use INSERT...SELECT to move the data to production tables.\"\nThis is a staging table, not a fact, so I think the answer is Round robin, Heap, Date."
      },
      {
        "date": "2021-05-18T13:43:00.000Z",
        "voteCount": 2,
        "content": "If Round Robin, then you don't choose column to partition on, it can only be None"
      },
      {
        "date": "2022-06-24T23:47:00.000Z",
        "voteCount": 1,
        "content": "As @TessieB has indicated quoting MS Docs \"Partitioning is also supported on all distribution types, including both hash or round robin distributed.\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      },
      {
        "date": "2024-04-19T08:50:00.000Z",
        "voteCount": 1,
        "content": "round robin, heap, none \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices#load-to-a-staging-table"
      },
      {
        "date": "2021-08-19T02:58:00.000Z",
        "voteCount": 2,
        "content": "Round-Robin , Heap , None"
      },
      {
        "date": "2021-07-24T06:55:00.000Z",
        "voteCount": 1,
        "content": "round-robin, haep, non \n\nWhy should someone hash distribute a stage table?"
      },
      {
        "date": "2021-07-18T00:21:00.000Z",
        "voteCount": 2,
        "content": "I think the answer should be:\nRound robin\nHeap\nDate\n\nSee here: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition \n\nPartitioning and distribution are two different things! Partitioning is supported by all types of distribution and by all types of indexes! Partitioning can speed up the loading process and it's often done by using a date column. In this question it's not clear however, if the Date column is the right fit for the loading process, but since it is often used like that, I'm gonna go and say that Date might be the correct answer! :)"
      },
      {
        "date": "2021-06-12T02:58:00.000Z",
        "voteCount": 4,
        "content": "Round-Robin , Heap , None"
      },
      {
        "date": "2021-05-18T02:29:00.000Z",
        "voteCount": 4,
        "content": "I think it should be Round-RObin, Heap and None. Refer the link\nhttps://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-data-loading-guidance?view=azure-sqldw-latest"
      },
      {
        "date": "2021-05-12T22:04:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is :\nRound-Robin\nClustered columnstore\nData\nTable partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column. Partitioning is supported on all dedicated SQL pool table types; including clustered columnstore, clustered index, and heap. Partitioning is also supported on all distribution types, including both hash or round robin distributed.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      },
      {
        "date": "2021-05-18T13:45:00.000Z",
        "voteCount": 1,
        "content": "Round RObin is random equal distribution, it doesn't include choosing a column to partition on"
      },
      {
        "date": "2021-05-12T12:45:00.000Z",
        "voteCount": 7,
        "content": "The answer should be Round-Robin/ Heap/ None.\n\nReference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/guidance-for-loading-data#loading-to-a-staging-table\n\n\u201cTo achieve the fastest loading speed for moving data into a dedicated SQL pool table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option.\u201d"
      },
      {
        "date": "2021-05-03T09:10:00.000Z",
        "voteCount": 1,
        "content": "If you are partitioning by Date and using Clustered Column Store, Would that be faster than using a Heap, None?"
      },
      {
        "date": "2021-04-29T21:45:00.000Z",
        "voteCount": 3,
        "content": "Round-Robin\nHeap\nNone\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"
      },
      {
        "date": "2021-04-29T21:45:00.000Z",
        "voteCount": 2,
        "content": "round-robin\nheap\nnone\n\nReference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"
      },
      {
        "date": "2021-04-25T01:48:00.000Z",
        "voteCount": 1,
        "content": "Anyone know the correct answer?, I'm between (Round-Robin, Heap and None) the other option (Round-Robin, Heap, Date)"
      },
      {
        "date": "2021-05-03T04:05:00.000Z",
        "voteCount": 1,
        "content": "I'll go with \"Round-Robin, Heap and None\", Round-Robin is the best and obvious choice, then Heap as there's no need for indexing since it's just for loading purposes, and then None as Round-Robin does not support partitioning."
      },
      {
        "date": "2021-04-13T10:52:00.000Z",
        "voteCount": 4,
        "content": "I think that the first is round robin (fastest way to load data) and the third box should be None, since round robin doesn't need partitioning"
      },
      {
        "date": "2021-04-05T06:14:00.000Z",
        "voteCount": 7,
        "content": "Answer should be \"round-robin\""
      },
      {
        "date": "2021-04-04T19:46:00.000Z",
        "voteCount": 3,
        "content": "Consider using the round-robin distribution for your table in the following scenarios:\n\nWhen getting started as a simple starting point since it is the default\nIf there is no obvious joining key\nIf there is no good candidate column for hash distributing the table\nIf the table does not share a common join key with other tables\nIf the join is less significant than other joins in the query\nWhen the table is a temporary staging table"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/41207-exam-dp-200-topic-1-question-56-discussion/",
    "body": "You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1.<br>You are building a SQL pool in Azure Synapse that will use data from the data lake.<br>Your company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the<br>Sales group access to the files in the data lake.<br>You plan to load data to the SQL pool every hour.<br>You need to ensure that the SQL pool can load the sales data from the data lake.<br>Which three actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the shared access signature (SAS) as the credentials for the data load process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the managed identity to the Sales group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your Azure Active Directory (Azure AD) account to the Sales group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shared access signature (SAS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the managed identity as the credentials for the data load process.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACD",
    "answerDescription": "A: The managed identity grants permissions to the dedicated SQL pools in the workspace.<br>Note: Managed identity for Azure resources is a feature of Azure Active Directory. The feature provides Azure services with an automatically managed identity in<br><br>Azure AD -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-identity",
    "votes": [
      {
        "answer": "ACF",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-01-02T03:08:00.000Z",
        "voteCount": 69,
        "content": "Correct answers are A, C, F."
      },
      {
        "date": "2024-07-26T11:06:00.000Z",
        "voteCount": 1,
        "content": "F. Use the managed identity as the credentials for the data load process.\nA. Create a managed identity.\nB. Use the shared access signature (SAS) as the credentials for the data load process."
      },
      {
        "date": "2024-07-26T11:50:00.000Z",
        "voteCount": 1,
        "content": "Sorry for the typo - I meant FAC"
      },
      {
        "date": "2021-11-23T08:04:00.000Z",
        "voteCount": 1,
        "content": "ACF, or FAD in the correct order are the good answers"
      },
      {
        "date": "2021-11-23T08:04:00.000Z",
        "voteCount": 1,
        "content": "I meant FAD"
      },
      {
        "date": "2021-11-23T08:05:00.000Z",
        "voteCount": 1,
        "content": "FAC***"
      },
      {
        "date": "2021-09-06T09:31:00.000Z",
        "voteCount": 1,
        "content": "ACD is fine. data load process has nothing to do with Sales Group. you just need to add sales group to Active directory."
      },
      {
        "date": "2022-06-24T22:55:00.000Z",
        "voteCount": 1,
        "content": "Please Azure AD Group Sales already created: \"All the members of the sales team are in an Azure Active Directory group named Sales.\""
      },
      {
        "date": "2021-05-16T09:19:00.000Z",
        "voteCount": 3,
        "content": "I also think ACF"
      },
      {
        "date": "2021-05-04T00:04:00.000Z",
        "voteCount": 4,
        "content": "Agree with A,C,F\n\nReference: https://docs.microsoft.com/en-us/azure/purview/register-scan-adls-gen2"
      },
      {
        "date": "2021-04-27T10:23:00.000Z",
        "voteCount": 2,
        "content": "Correct answers are C, D, F."
      },
      {
        "date": "2021-04-19T11:14:00.000Z",
        "voteCount": 3,
        "content": "Correct answers are A, C, F."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50232-exam-dp-200-topic-1-question-57-discussion/",
    "body": "HOTSPOT -<br>You have two Azure Storage accounts named Storage1 and Storage2. Each account contains an Azure Data Lake Storage file system. The system has files that contain data stored in the Apache Parquet format.<br>You need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements:<br>\u2711 No transformations must be performed.<br>\u2711 The original folder structure must be retained.<br>How should you configure the copy activity? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0010800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0010900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Parquet -<br>For Parquet datasets, the type property of the copy activity source must be set to ParquetSource..<br><br>Box 2: PreserveHierarchy -<br>PreserveHierarchy (default): Preserves the file hierarchy in the target folder. The relative path of the source file to the source folder is identical to the relative path of the target file to the target folder.<br>Incorrect Answers:<br>FlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.<br>MergeFiles: Merges all files from the source folder to one file. If the file name is specified, the merged file name is the specified name. Otherwise, it's an autogenerated file name.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/format-parquet https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-10T11:32:00.000Z",
        "voteCount": 6,
        "content": "The first box should be \"Binary\". It says - no transformation."
      },
      {
        "date": "2021-06-16T09:30:00.000Z",
        "voteCount": 1,
        "content": "Binary is only for Binary format: https://docs.microsoft.com/en-us/azure/data-factory/format-binary"
      },
      {
        "date": "2021-06-21T18:15:00.000Z",
        "voteCount": 3,
        "content": "Every parquet file is also a binary file. I think the key is \"no transformations\", so why the extra work of interpreting a parquet file?! Binary and preserve hierarchy should do it imo."
      },
      {
        "date": "2021-10-06T02:38:00.000Z",
        "voteCount": 3,
        "content": "First box should be \"Binary\" . I tested it with the 2 options . using paquet i got an error with the following message :\n\"Dataset Parquet1 location is a folder, the wildcard file name is required for Copy data1\""
      },
      {
        "date": "2021-06-20T02:19:00.000Z",
        "voteCount": 4,
        "content": "The given answer is correct Parquet and preserve hierarchy"
      },
      {
        "date": "2021-06-17T11:19:00.000Z",
        "voteCount": 1,
        "content": "You can use Binary dataset in Copy activity, GetMetadata activity, or Delete activity. When using Binary dataset, ADF does not parse file content but treat it as-is. When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset..so the ans should be parquet"
      },
      {
        "date": "2021-06-23T05:29:00.000Z",
        "voteCount": 1,
        "content": "Why does it need to be parquet? Just configure the sink dataset as binary as well. This way ADF doesn't need to parse the files. You just need parquet if you want to do some transformation or when the sink dataset is an existing parquet dataset"
      },
      {
        "date": "2021-05-18T14:00:00.000Z",
        "voteCount": 2,
        "content": "It should be Binary - it copies the files as they are, no need to parse the parquet format if you don't need to transform them."
      },
      {
        "date": "2021-04-29T22:30:00.000Z",
        "voteCount": 4,
        "content": "Agree with the answer as both source and sink can accommodate \"parquet\" extension files using the behavior as seen below. Try working it on ADFv2\n\nFile Format: Parquet (source and sink)\nCopy behavior: Preserve Hierarchy"
      },
      {
        "date": "2021-04-29T09:23:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct. \nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet"
      },
      {
        "date": "2021-04-16T22:32:00.000Z",
        "voteCount": 2,
        "content": "do u have any reference ? and if u cant use parquet to load parquet files whats the point of ever choosing parquet?"
      },
      {
        "date": "2021-04-15T17:43:00.000Z",
        "voteCount": 3,
        "content": "The first box should be \"Binary\". You can't use a parquet data source to load different parquet files."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50098-exam-dp-200-topic-1-question-58-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure subscription that contains an Azure Storage account.<br>You plan to implement changes to a data storage solution to meet regulatory and compliance standards.<br>Every day, Azure needs to identify and delete blobs that were NOT modified during the last 100 days.<br>Solution: You schedule an Azure Data Factory pipeline with a delete activity.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "You can use the Delete Activity in Azure Data Factory to delete files or folders from on-premises storage stores or cloud storage stores.<br>Azure Blob storage is supported.<br>Note: You can also apply an Azure Blob storage lifecycle policy.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/delete-activity https://docs.microsoft.com/en-us/azure/storage/blobs/storage-lifecycle-management-concepts?tabs=azure-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-14T01:04:00.000Z",
        "voteCount": 4,
        "content": "Answer A seems to be correct https://azure.microsoft.com/it-it/blog/clean-up-files-by-built-in-delete-activity-in-azure-data-factory/"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50509-exam-dp-200-topic-1-question-59-discussion/",
    "body": "HOTSPOT -<br>You have the following Azure Stream Analytics query.<br><img src=\"/assets/media/exam-media/03872/0011100001.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0011200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0011200002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Yes -<br>You can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.<br>The outcome is a stream that has the same partition scheme. Please see below for an example:<br>WITH step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)<br>SELECT * INTO [output] FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID<br>Note: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.<br><br>Box 2: Yes -<br>When joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count.<br><br>Box 3: Yes -<br>Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.<br>In general, the best practice is to start with 6 SUs for queries that don't use PARTITION BY.<br>Here there are 10 partitions, so 6x10 = 60 SUs is good.<br>Note: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, must be adjusted so the number of physical resources available to the job can fit the partitioned flow. In general, six SUs is a good number to assign to each partition. In case there are insufficient resources assigned to the job, the system will only apply the repartition if it benefits the job.<br>Reference:<br>https://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-14T23:54:00.000Z",
        "voteCount": 8,
        "content": "Yes , Yes  and No."
      },
      {
        "date": "2023-09-24T21:34:00.000Z",
        "voteCount": 1,
        "content": "From DP 203 discussions, I found out the answer is Yes for all."
      },
      {
        "date": "2021-06-27T17:31:00.000Z",
        "voteCount": 1,
        "content": "What is the correct answer?? Pl advise."
      },
      {
        "date": "2021-07-04T06:50:00.000Z",
        "voteCount": 1,
        "content": "Doesn't matter, no-one can seem to agree on the answer on here either"
      },
      {
        "date": "2021-06-25T03:48:00.000Z",
        "voteCount": 1,
        "content": "What is the answer?? WHy do they not give the correct answer in the first place..what is their problem?\n]"
      },
      {
        "date": "2021-06-17T02:07:00.000Z",
        "voteCount": 3,
        "content": "NO, YES, YES."
      },
      {
        "date": "2021-06-01T09:11:00.000Z",
        "voteCount": 1,
        "content": "After the survey, I have no ideas. Can I know what is the answer?"
      },
      {
        "date": "2021-04-19T16:29:00.000Z",
        "voteCount": 2,
        "content": "This question is repetitive."
      },
      {
        "date": "2021-04-20T07:27:00.000Z",
        "voteCount": 2,
        "content": "It's different. My options are Yes, No and No (Source Whizlabs)"
      },
      {
        "date": "2021-05-11T09:48:00.000Z",
        "voteCount": 7,
        "content": "Its not. \nQuestion 45: The query joins ...\nQuestion 59: The query combines ..."
      },
      {
        "date": "2021-05-11T18:37:00.000Z",
        "voteCount": 2,
        "content": "The query from both items are different. Question # 45 doesn't have \"partition\" syntax that makes it incorrect. On the other hand, there are two select statements that is combined."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49141-exam-dp-200-topic-1-question-60-discussion/",
    "body": "DRAG DROP -<br>You need to create an Azure Cosmos DB account that will use encryption keys managed by your organization.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0011400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0011500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an Azure key vault and enable purge protection<br>Using customer-managed keys with Azure Cosmos DB requires you to set two properties on the Azure Key Vault instance that you plan to use to host your encryption keys: Soft Delete and Purge Protection.<br>Step 2: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI<br>Data stored in your Azure Cosmos account is automatically and seamlessly encrypted with keys managed by Microsoft (service-managed keys). Optionally, you can choose to add a second layer of encryption with keys you manage (customer-managed keys).<br>Step 3: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal<br>Add an access policy to your Azure Key Vault instance<br>Step 4: Generate a new key in the Azure key vault<br>Generate a key in Azure Key Vault<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-19T10:54:00.000Z",
        "voteCount": 37,
        "content": "Step 1 Create an Azure Key vault and enable purge protection\nStep 2 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\nStep 3 Generate a new key in Azure Key Vault\nStep 4 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI"
      },
      {
        "date": "2021-06-05T10:20:00.000Z",
        "voteCount": 1,
        "content": "Perfect. Microsoft.DocumentDB Resouce Provider to be registerd and then all the steps mentioned here."
      },
      {
        "date": "2021-04-25T12:21:00.000Z",
        "voteCount": 2,
        "content": "this solution is correct. Check documentation here: \nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk"
      },
      {
        "date": "2021-04-05T00:54:00.000Z",
        "voteCount": 21,
        "content": "Step 1: Create an Azure key vault and enable purge protection\nStep 2: Generate a new key in the Azure key vault\nStep 3: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI\nStep 4: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal"
      },
      {
        "date": "2021-04-29T23:03:00.000Z",
        "voteCount": 2,
        "content": "this make sense. checked the documentation in the url below\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk"
      },
      {
        "date": "2021-05-06T19:59:00.000Z",
        "voteCount": 4,
        "content": "retracting my feedback here instead go for the solution below\n\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\nStep 3 Generate a new key in Azure Key Vault\nStep 4 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI"
      },
      {
        "date": "2021-06-25T10:21:00.000Z",
        "voteCount": 2,
        "content": "right sequence looks like this :\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Generate a new key in Azure Key Vault\nStep 3 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI\nStep 4 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\n\nIn discussions there is confusion going on whether step 4 should come above step 3 etc..but unless we dont create a cosmos DB resource , how can we create key vault access policy and grant permission to cosmos DB principal.so step 4 should be last"
      },
      {
        "date": "2021-05-16T09:34:00.000Z",
        "voteCount": 1,
        "content": "Step 1:Create an Azure key vault and enable purge protection\nStep 2:Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal\nStep 3:Generate a new key in the Azure key vault\nStep 4:Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI"
      },
      {
        "date": "2021-05-05T02:54:00.000Z",
        "voteCount": 2,
        "content": "Step 1: Create an Azure key vault and enable purge protection\nStep 2: Add an Azure Key Vault access policy to grant permissions to the Azure Cosmos DB principal  (doesn't have to actually exist yet)\nStep 3: Generate a new key in the Azure key vault\nStep 4: Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed Key (Enter key URI), and enter the key URI\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk"
      },
      {
        "date": "2021-04-29T09:53:00.000Z",
        "voteCount": 1,
        "content": "These are the correct steps: \n\nStep 1 Create an Azure Key vault and enable purge protection\nStep 2 Generate a new key in Azure Key Vault\nStep 3 Create a new Azure Cosmos DB account, set Data Encryption to Customer-managed key (Enter Key URI), and enter the key URI \nStep 4 Add an Azure Key Vault access policy to grant permission to the Azure Cosmos DB principal\n\nYou cannot add the key URI before you've even created the key so creating the Cosmos DB account AND inserting the key uri before the key even exists does not make sense. Also, you cannot add an access policy for a resource that does not exist yet so adding the access policy to the key vault before you even created the cosmos DB account does  not make sense."
      },
      {
        "date": "2021-04-05T06:31:00.000Z",
        "voteCount": 10,
        "content": "The Cosmos DB account must be created as last step using previous created key. MS docs states that:\n\n\"When you create a new Azure Cosmos DB account from the Azure portal, choose Customer-managed key in the Encryption step. In the Key URI field, paste the URI/key identifier of the Azure Key Vault key that you copied from the previous step\"\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/how-to-setup-cmk"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62118-exam-dp-200-topic-1-question-61-discussion/",
    "body": "You have an Azure Storage account named storage1 that is configured as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03872/0011700001.png\" class=\"in-exam-image\"><br>You need to ensure that all calls to an Azure Storage REST API operation on storage1 are made over HTTPS.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Secure transfer required to Enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Allow Blob public access to Disabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor the Blob service, create a shared access signature (SAS) that allows HTTPS only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Minimum TLS version to Version 1.2."
    ],
    "answer": "A",
    "answerDescription": "You can configure your storage account to accept requests from secure connections only by setting the Secure transfer required property for the storage account.<br>When you require secure transfer, any requests originating from an insecure connection are rejected. Microsoft recommends that you always require secure transfer for all of your storage accounts.<br>When secure transfer is required, a call to an Azure Storage REST API operation must be made over HTTPS. Any request made over HTTP is rejected.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-require-secure-transfer",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-15T10:53:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer!"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58958-exam-dp-200-topic-1-question-62-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics dedicated SQL pool that contains the users shown in the following table.<br><img src=\"/assets/media/exam-media/03872/0011800001.png\" class=\"in-exam-image\"><br>User1 executes a query on the database, and the query returns the results shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03872/0011900001.jpg\" class=\"in-exam-image\"><br>User1 is the only user who has access to the unmasked data.<br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0012000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0012100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 0 -<br>The YearlyIncome column is of the money data type.<br>The Default masking function: Full masking according to the data types of the designated fields<br>\u2711 Use a zero value for numeric data types (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real).<br>Box 2: the values stored in the database<br>Users with administrator privileges are always excluded from masking, and see the original data without any mask.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-14T05:25:00.000Z",
        "voteCount": 4,
        "content": "The second question involves the admin user, so the answer is correct"
      },
      {
        "date": "2021-07-29T21:59:00.000Z",
        "voteCount": 3,
        "content": "Zero and 01-01-1900\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview"
      },
      {
        "date": "2021-08-14T07:16:00.000Z",
        "voteCount": 2,
        "content": "No. The provided answer is correct. It should show the value stored in the database and based on the graphic, we dont know what that value is"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "1"
  }
]