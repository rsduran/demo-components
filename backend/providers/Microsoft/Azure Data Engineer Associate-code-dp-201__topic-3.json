[
  {
    "topic": 3,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53590-exam-dp-201-topic-3-question-1-discussion/",
    "body": "You are planning a big data solution in Azure.<br>You need to recommend a technology that meets the following requirements:<br>\u2711 Be optimized for batch processing.<br>\u2711 Support autoscaling.<br>\u2711 Support per-cluster scaling.<br>Which technology should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Spark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks"
    ],
    "answer": "D",
    "answerDescription": "Azure Databricks is an Apache Spark-based analytics platform. Azure Databricks supports autoscaling and manages the Spark cluster for you.<br>Incorrect Answers:<br>A, B:<br><img src=\"/assets/media/exam-media/03774/0025000001.png\" class=\"in-exam-image\">",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-29T05:38:00.000Z",
        "voteCount": 1,
        "content": "Based on current capabilities, both B and D are correct. probably this question might asked in multiple choice questions.\nhttps://learn.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing"
      },
      {
        "date": "2021-08-19T00:48:00.000Z",
        "voteCount": 1,
        "content": "The answer provided is 100% Correct. It is Databricks."
      },
      {
        "date": "2021-06-29T06:19:00.000Z",
        "voteCount": 1,
        "content": "Answer B \nhttps://azure.microsoft.com/en-in/blog/drive-higher-utilization-of-azure-hdinsight-clusters-with-autoscale/"
      },
      {
        "date": "2021-06-27T11:33:00.000Z",
        "voteCount": 2,
        "content": "When the question was released it was not possible on HDinsight hence the exam is being shut down! correct answer ADB"
      },
      {
        "date": "2021-06-16T04:21:00.000Z",
        "voteCount": 2,
        "content": "Should be B:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-autoscale-clusters"
      },
      {
        "date": "2021-05-27T10:54:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is D"
      },
      {
        "date": "2021-05-26T01:14:00.000Z",
        "voteCount": 3,
        "content": "It should be B: HDInsight with spark . Key word id Big data processing"
      },
      {
        "date": "2021-06-08T20:54:00.000Z",
        "voteCount": 4,
        "content": "Auto scaling is not possible in HDInsight with spark"
      },
      {
        "date": "2021-08-29T17:52:00.000Z",
        "voteCount": 1,
        "content": "HDInsight Autoscale is supported in Spark and Hadoop (Hive) clusters as a generally available feature. \nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-autoscale-clusters"
      },
      {
        "date": "2021-06-22T06:51:00.000Z",
        "voteCount": 3,
        "content": "NO AUTOSCALING READ DESCRIPTION FIRST! 1000% NOT B!"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48032-exam-dp-201-topic-3-question-2-discussion/",
    "body": "You are designing an enterprise data warehouse in Azure Synapse Analytics that will contain a table named Customers. Customers will contain credit card information.<br>You need to recommend a solution to provide salespeople with the ability to view all the entries in Customers. The solution must prevent all the salespeople from viewing or inferring the credit card information.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata masking",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn-level security",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAlways Encrypted"
    ],
    "answer": "B",
    "answerDescription": "SQL Database dynamic data masking limits sensitive data exposure by masking it to non-privileged users.<br>The Credit card masking method exposes the last four digits of the designated fields and adds a constant string as a prefix in the form of a credit card.<br><br>Example: XXXX-XXXX-XXXX-1234 -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-dynamic-data-masking-get-started",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-26T23:25:00.000Z",
        "voteCount": 34,
        "content": "The correct answer is \"Column-level security\"."
      },
      {
        "date": "2021-06-15T00:23:00.000Z",
        "voteCount": 7,
        "content": "\"You need to recommend a solution to provide salespeople with the ability to view all the entries in Customers\". Column level security does not meet this requirement so I think Data Masking (B) is the right answer."
      },
      {
        "date": "2021-04-28T02:16:00.000Z",
        "voteCount": 3,
        "content": "Yes Column level security is the right answer\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security"
      },
      {
        "date": "2021-05-26T01:13:00.000Z",
        "voteCount": 2,
        "content": "This is the appropriate answer than data masking as the user can still query the said column even if it is mask"
      },
      {
        "date": "2021-03-23T12:57:00.000Z",
        "voteCount": 8,
        "content": "I think the answer should be D, since you can infer data that is masked using brute-force techniques. The question states that the solution must prevent the salespeople from viewing or INFERRING the credit card information.\n\n\"It is appropriate for preventing accidental sensitive data exposure, but will not protect against malicious intent to infer the underlying data.\"\nhttps://docs.microsoft.com/nl-nl/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver15"
      },
      {
        "date": "2021-04-10T02:56:00.000Z",
        "voteCount": 1,
        "content": "I agree, should be Always encrypted"
      },
      {
        "date": "2021-04-11T19:07:00.000Z",
        "voteCount": 3,
        "content": "Disregard my previous post. Always encrypted is not supported by Synapse Analytics so it's Column level security"
      },
      {
        "date": "2021-04-11T04:59:00.000Z",
        "voteCount": 4,
        "content": "Synapse does not support always encrypted only SQL server and Azure SQL DB: https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine?view=sql-server-ver15.\nCorrect answer is Column-level"
      },
      {
        "date": "2022-03-26T19:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is data masking. Please re-read column level security here : https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security\n\nColumn level security is when you need to restrict user access to certain columns. Not where you need to give them access to all data BUT want to stop them from viewing/inferring certain information as is the case in this question (credit card info must not be visible or be inferred i.e. data masking)"
      },
      {
        "date": "2021-06-29T06:24:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nColumn-level security simplifies the design and coding of security in your application, allowing you to restrict column access to protect sensitive data. For example, ensuring that specific users can access only certain columns of a table pertinent to their department."
      },
      {
        "date": "2021-04-29T10:12:00.000Z",
        "voteCount": 2,
        "content": "C. column-level security"
      },
      {
        "date": "2021-04-25T22:11:00.000Z",
        "voteCount": 6,
        "content": "Data masking in this case can not be inferred as it simply hides the characters with XXX-es. There is no hashing logic or something involved. Therefore you can't infer the original value. Therefore the given answer is correct"
      },
      {
        "date": "2021-05-12T03:51:00.000Z",
        "voteCount": 7,
        "content": "Additionally, the requirment states that \"... provide salespeople with the ability to view all the entries in Customers.\" With CLS the salespeople would not be able to see all columns in the Customers table. Therefore, data masking is the correct answer."
      },
      {
        "date": "2021-05-14T03:35:00.000Z",
        "voteCount": 1,
        "content": "I agree."
      },
      {
        "date": "2021-04-13T05:56:00.000Z",
        "voteCount": 1,
        "content": "Given answer is wrong. Data Masking allows information to be inferred. \nRow level security acts as a filter at the row level so not preventing information within a column from being viewed.\n\nThat leaves Always Encrypted and Column Level Encryption. Even column level encryption data can be inferred through the use of divide by zero errors, but, as noted by rahul_t, we cannot use Always Encrypted on Synapse at this time so by process of elimination correct answer is Column Level Encryption."
      },
      {
        "date": "2021-04-23T01:09:00.000Z",
        "voteCount": 1,
        "content": "Creating a mask on a column does not prevent updates to that column."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29023-exam-dp-201-topic-3-question-3-discussion/",
    "body": "HOTSPOT -<br>A company plans to use Azure SQL Database to support a line of business application. The application will manage sensitive employee data.<br>The solution must meet the following requirements:<br>\u2711 Encryption must be performed by the application.<br>\u2711 Only the client application must have access keys for encrypting and decrypting data.<br>\u2711 Data must never appear as plain text in the database.<br>\u2711 The strongest possible encryption method must be used.<br>\u2711 Grouping must be possible on selected data.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0025200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0025300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Always Encrypted with deterministic encryption<br>Deterministic encryption always generates the same encrypted value for any given plain text value. Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns. However, it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column, especially if there is a small set of possible encrypted values, such as True/False, or North/South/East/West region.<br>Deterministic encryption must use a column collation with a binary2 sort order for character columns.<br>Box 2: Always Encrypted with Randomized encryption<br>\u2711 Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns.<br>Note: With Always Encrypted the Database Engine never operates on plaintext data stored in encrypted columns, but it still supports some queries on encrypted data, depending on the encryption type for the column. Always Encrypted supports two types of encryption: randomized encryption and deterministic encryption.<br>Use deterministic encryption for columns that will be used as search or grouping parameters, for example a government ID number. Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-18T23:35:00.000Z",
        "voteCount": 30,
        "content": "This was in my exam. The options were \"Grouping data\" and \"Non-grouping data\" instead of \"Searchable data\" and \"Non-searchable data\"."
      },
      {
        "date": "2020-08-22T20:11:00.000Z",
        "voteCount": 1,
        "content": "and the same options to choose?"
      },
      {
        "date": "2020-08-28T08:39:00.000Z",
        "voteCount": 2,
        "content": "Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables."
      },
      {
        "date": "2021-06-15T07:23:00.000Z",
        "voteCount": 2,
        "content": "Its sads Grouping data and Non grouping data"
      },
      {
        "date": "2020-09-09T02:38:00.000Z",
        "voteCount": 14,
        "content": "Use deterministic encryption for columns that will be used as search or grouping parameters. For example, a government ID number. Use randomized encryption for data such as confidential investigation comments, which aren't grouped with other records and aren't used to join tables."
      },
      {
        "date": "2021-06-29T17:12:00.000Z",
        "voteCount": 3,
        "content": "This was in my exam also..thanks guys"
      },
      {
        "date": "2021-06-30T09:10:00.000Z",
        "voteCount": 1,
        "content": "So the answers are correct right?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40634-exam-dp-201-topic-3-question-4-discussion/",
    "body": "HOTSPOT -<br>You are designing the security for a mission critical Azure SQL database named DB1. DB1 contains several columns that store Personally Identifiable Information<br>(PII) data<br>You need to recommend a security solution that meets the following requirements:<br>\u2711 Ensures that DB1 is encrypted at rest<br>\u2711 Ensures that data from the columns containing PII data is encrypted in transit<br>Which security solution should you recommend for DB1 and the columns? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0025500001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0025600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "DB1: Transparent Data Encryption<br>Azure SQL Database currently supports encryption at rest for Microsoft-managed service side and client-side encryption scenarios.<br>Support for server encryption is currently provided through the SQL feature called Transparent Data Encryption.<br><br>Columns: Always encrypted -<br>Always Encrypted is a feature designed to protect sensitive data stored in Azure SQL Database or SQL Server databases. Always Encrypted allows clients to encrypt sensitive data inside client applications and never reveal the encryption keys to the database engine (SQL Database or SQL Server).<br>Note: Most data breaches involve the theft of critical data such as credit card numbers or personally identifiable information. Databases can be treasure troves of sensitive information. They can contain customers' personal data (like national identification numbers), confidential competitive information, and intellectual property. Lost or stolen data, especially customer data, can result in brand damage, competitive disadvantage, and serious fines--even lawsuits.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest https://docs.microsoft.com/en-us/azure/security/fundamentals/database-security-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-22T21:54:00.000Z",
        "voteCount": 21,
        "content": "correct choice"
      },
      {
        "date": "2021-06-16T05:41:00.000Z",
        "voteCount": 1,
        "content": "Data Masking for \"column\" sensitive data in transit"
      },
      {
        "date": "2021-08-29T18:09:00.000Z",
        "voteCount": 1,
        "content": "Data masking is not encryption"
      },
      {
        "date": "2021-06-16T05:38:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL Databases\nEncryption at rest can be enabled at the database and server levels. As of June 2017, Transparent Data Encryption (TDE) is enabled by default on newly created databases.\n\nhttps://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39107-exam-dp-201-topic-3-question-5-discussion/",
    "body": "You are developing an application that uses Azure Data Lake Storage Gen 2.<br>You need to recommend a solution to grant permissions to a specific application for a limited time period.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) identities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signatures (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccount keys",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trole assignments"
    ],
    "answer": "B",
    "answerDescription": "A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but to whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you can grant them access to a resource for a specified period of time, with a specified set of permissions.<br>Reference:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/delegate-access-with-shared-access-signature",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-07T04:11:00.000Z",
        "voteCount": 23,
        "content": "Limited tine period -&gt; SAS"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21923-exam-dp-201-topic-3-question-6-discussion/",
    "body": "You need to recommend a security solution to grant anonymous users permission to access the blobs in a specific container only.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccess keys for the storage account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signatures (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRole assignments",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe public access level for the container"
    ],
    "answer": "D",
    "answerDescription": "You can enable anonymous, public read access to a container and its blobs in Azure Blob storage. By doing so, you can grant read-only access to these resources without sharing your account key, and without requiring a shared access signature (SAS).<br>Public read access is best for scenarios where you want certain blobs to always be available for anonymous read access.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-manage-access-to-resources",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-09T07:43:00.000Z",
        "voteCount": 35,
        "content": "D is right, in the container clic \"Change access level\" , you can choose between : private, blob , container, select blob to get \"anonymous read access for blob only\""
      },
      {
        "date": "2021-05-26T01:33:00.000Z",
        "voteCount": 2,
        "content": "entirely correct. keyword here is \"specific container\" and SAS doesn't provide permission to specific container but it allows to access the storage."
      },
      {
        "date": "2020-11-19T11:39:00.000Z",
        "voteCount": 8,
        "content": "D is correct- You have to do 2 steps: \n1) Enable \"Allow Blob public access\" for the Storage Account \n2) On the selected container -&gt; Change Access Level -&gt; Public Access Level -must be set on Blobs or Container. \nRef: https://docs.microsoft.com/en-us/azure/storage/blobs/anonymous-read-access-configure?tabs=portal"
      },
      {
        "date": "2021-04-29T10:14:00.000Z",
        "voteCount": 1,
        "content": "B. shared access signatures (SAS)"
      },
      {
        "date": "2021-04-29T10:14:00.000Z",
        "voteCount": 2,
        "content": "Explanation\nThe most secure way is to use a shared access signature\n\nA shared access signature (SAS) provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:\n\nWhat resources the client may access.\n\nWhat permissions they have to those resources.\n\nHow long the SAS is valid.\n\nBlob level access can be provided via Azure portal and folder level access can be provided via Storage Explorer\n\n\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview\n\nIncorrect answers:\n\nAccess keys for the storage account - This would give access to the entire storage account\n\nRole based access control -   This is used to control permissions to the entire storage account\n\nPublic access level for the blob service - This would give access to the entire blob service and to anyone who has the storage link"
      },
      {
        "date": "2021-04-17T12:12:00.000Z",
        "voteCount": 2,
        "content": "correct answer is B."
      },
      {
        "date": "2021-02-07T16:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/storage/blobs/anonymous-read-access-configure?tabs=portal"
      },
      {
        "date": "2020-09-02T14:46:00.000Z",
        "voteCount": 3,
        "content": "I agree with D however as now Storage explorer been integrated with r Storage account, so if you open a container via storage explorer and then right click , you'll get SAS oprion too"
      },
      {
        "date": "2020-07-27T09:33:00.000Z",
        "voteCount": 3,
        "content": "I bring my knowledge of similar scenario from GCP. D is the way to do it"
      },
      {
        "date": "2020-06-25T12:28:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2020-06-21T09:30:00.000Z",
        "voteCount": 4,
        "content": "using SAS, if given at container level, one can access all the containers in a service account, the permission is not specific to a particular container. However, this is possible with by creating a container with public access level set to  'Public access to Anonymous users'"
      },
      {
        "date": "2020-06-02T09:24:00.000Z",
        "voteCount": 2,
        "content": "No, The correct Answer is B"
      },
      {
        "date": "2020-10-11T23:05:00.000Z",
        "voteCount": 1,
        "content": "Verified in SKillCertPro, B is correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26622-exam-dp-201-topic-3-question-7-discussion/",
    "body": "You are designing a solution that will use Azure Databricks and Azure Data Lake Storage Gen2.<br>From Databricks, you need to access Data Lake Storage directly by using a service principal.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signatures (SAS) in Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccess keys in Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan organizational relationship in Azure Active Directory (Azure AD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan application registration in Azure Active Directory (Azure AD)"
    ],
    "answer": "D",
    "answerDescription": "Create and grant permissions to service principal<br>If your selected the access method requires a service principal with adequate permissions, and you do not have one, follow these steps:<br>1. Create an Azure AD application and service principal that can access resources. Note the following properties:<br>\u2711 client-id: An ID that uniquely identifies the application.<br>\u2711 directory-id: An ID that uniquely identifies the Azure AD instance.<br>\u2711 service-credential: A string that the application uses to prove its identity.<br>2. Register the service principal, granting the correct role assignment, such as Storage Blob Data<br>3. Contributor, on the Azure Data Lake Storage Gen2 account.<br>Reference:<br>https://docs.databricks.com/data/data-sources/azure/azure-datalake-gen2.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-31T11:33:00.000Z",
        "voteCount": 22,
        "content": "Given answer is correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53676-exam-dp-201-topic-3-question-8-discussion/",
    "body": "You are designing a storage solution to store CSV files.<br>You need to grant a data scientist access to read all the files in a single container of an Azure Storage account. The solution must use the principle of least privilege and provide the highest level of security.<br>What are two possible ways to achieve the goal? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide an access key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Storage Blob Data Reader role at the container level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Reader role to the storage account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide an account shared access signature (SAS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide a user delegation shared access signature (SAS)."
    ],
    "answer": "BE",
    "answerDescription": "B: When an Azure role is assigned to an Azure AD security principal, Azure grants access to those resources for that security principal. Access can be scoped to the level of the subscription, the resource group, the storage account, or an individual container or queue.<br>The built-in Data Reader roles provide read permissions for the data in a container or queue.<br>Note: Permissions are scoped to the specified resource.<br>For example, if you assign the Storage Blob Data Reader role to user Mary at the level of a container named sample-container, then Mary is granted read access to all of the blobs in that container.<br>E: A user delegation SAS is secured with Azure Active Directory (Azure AD) credentials and also by the permissions specified for the SAS. A user delegation SAS applies to Blob storage only.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-portal https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-27T12:03:00.000Z",
        "voteCount": 6,
        "content": "Answer is right"
      },
      {
        "date": "2021-06-21T07:40:00.000Z",
        "voteCount": 1,
        "content": "TRUE it is"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53392-exam-dp-201-topic-3-question-9-discussion/",
    "body": "You are designing an Azure Synapse solution that will provide a query interface for the data stored in an Azure Storage account. The storage account is only accessible from a virtual network.<br>You need to recommend an authentication mechanism to ensure that the solution can access the source data.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Active Directory (Azure AD) service principal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tanonymous public read access"
    ],
    "answer": "B",
    "answerDescription": "Managed Identity authentication is required when your storage account is attached to a VNet.<br>Regardless of the type of identity chosen a managed identity is a service principal of a special type that may only be used with Azure resources.<br>Note: An Azure service principal is an identity created for use with applications, hosted services, and automated tools to access Azure resources.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-bulk-load-copy-tsql-examples https://docs.microsoft.com/en-us/powershell/azure/create-azure-service-principal-azureps",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T03:13:00.000Z",
        "voteCount": 6,
        "content": "%100... CORRECT"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55374-exam-dp-201-topic-3-question-10-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains an Azure Data Lake Storage account. The storage account contains a data lake named DataLake1.<br>You plan to use an Azure data factory to ingest data from a folder in DataLake1, transform the data, and land the data in another folder.<br>You need to ensure that the data factory can read and write data from any folder in the DataLake1 file system. The solution must meet the following requirements:<br>\u2711 Minimize the risk of unauthorized user access.<br>\u2711 Use the principle of least privilege.<br>\u2711 Minimize maintenance effort.<br>How should you configure access to the storage account for the data factory? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0026100004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0026100005.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Active Directory (Azure AD)<br>On Azure, managed identities eliminate the need for developers having to manage credentials by providing an identity for the Azure resource in Azure AD and using it to obtain Azure Active Directory (Azure AD) tokens.<br><br>Box 2: a managed identity -<br>A data factory can be associated with a managed identity for Azure resources, which represents this specific data factory. You can directly use this managed identity for Data Lake Storage Gen2 authentication, similar to using your own service principal. It allows this designated factory to access and copy data to or from your Data Lake Storage Gen2.<br>Note: The Azure Data Lake Storage Gen2 connector supports the following authentication types.<br>\u2711 Account key authentication<br>\u2711 Service principal authentication<br>\u2711 Managed identities for Azure resources authentication<br>Reference:<br>https://docs.microsoft.com/en-us/azure/active-directory/managed-identities-azure-resources/overview https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage",
    "votes": [],
    "comments": [
      {
        "date": "2022-05-19T16:41:00.000Z",
        "voteCount": 1,
        "content": "Correct!!!!"
      },
      {
        "date": "2021-06-15T07:45:00.000Z",
        "voteCount": 4,
        "content": "100% Correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55217-exam-dp-201-topic-3-question-11-discussion/",
    "body": "You are designing a real-time stream processing solution in Azure Stream Analytics. The solution must read data from a blob container in an Azure Storage account via a service endpoint.<br>You need to recommend an authentication mechanism for the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed identity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta storage access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta user-assigned managed identity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan account key"
    ],
    "answer": "A",
    "answerDescription": "Azure Stream Analytics\u05d2\u20ac\u2030supports\u05d2\u20ac\u2030Managed Identity authentication\u05d2\u20ac\u2030for both Azure Event Hubs\u05d2\u20ac\u2030input and output.<br>Note: First, you create a managed identity for your Azure Stream Analytics job.\u05d2\u20ac\u2030<br>1. In the\u05d2\u20ac\u2030Azure portal, open your Azure Stream Analytics job.\u05d2\u20ac\u2030<br>2. From\u05d2\u20ac\u2030the\u05d2\u20ac\u2030left\u05d2\u20ac\u2030navigation\u05d2\u20ac\u2030menu, select\u05d2\u20ac\u2030Managed Identity\u05d2\u20ac\u2030located under\u05d2\u20ac\u2030Configure. Then, check the box next to\u05d2\u20ac\u2030Use\u05d2\u20ac\u2030System-assigned Managed Identity\u05d2\u20ac\u2030and select\u05d2\u20ac\u2030Save.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/event-hubs-managed-identity",
    "votes": [],
    "comments": [
      {
        "date": "2022-05-19T16:38:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer!!"
      },
      {
        "date": "2021-06-12T23:11:00.000Z",
        "voteCount": 4,
        "content": "Correct answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47944-exam-dp-201-topic-3-question-12-discussion/",
    "body": "You are designing security for administrative access to Azure Synapse Analytics.<br>You need to recommend a solution to ensure that administrators use two-factor authentication when accessing the data warehouse from Microsoft SQL Server<br>Management Studio (SSMS).<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure conditional access policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) Privileged Identity Management (PIM)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Key Vault secrets",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) Identity Protection"
    ],
    "answer": "A",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-conditional-access",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-22T06:24:00.000Z",
        "voteCount": 7,
        "content": "https://docs.microsoft.com/en-us/azure/active-directory/conditional-access/overview"
      },
      {
        "date": "2021-04-29T10:27:00.000Z",
        "voteCount": 5,
        "content": "A. Azure conditional access policies"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18673-exam-dp-201-topic-3-question-13-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database that has columns. The columns contain sensitive Personally Identifiable Information (PII) data.<br>You need to design a solution that tracks and stores all the queries executed against the PII data. You must be able to review the data in Azure Monitor, and the data must be available for at least 45 days.<br>Solution: You add classifications to the columns that contain sensitive data. You turn on Auditing and set the audit log destination to use Azure Log Analytics.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "The default retention for Log Analytics is 31 days only. The Log Analytics retention settings allow you to configure a minimum of 31 days (if not using a free tier) up to 730 days.<br>You would need to reconfigure to at least 45 days, or, for example, use Azure Blob Storage as destination.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-auditing https://blogs.msdn.microsoft.com/canberrapfe/2017/01/25/change-oms-log-analytics-retention-period-in-the-azure-portal/",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-23T11:09:00.000Z",
        "voteCount": 19,
        "content": "The question asks what you\u2019ve done has met the goal or not. So, B"
      },
      {
        "date": "2021-05-24T05:34:00.000Z",
        "voteCount": 2,
        "content": "the question dont need to mention to change/arrange retention period. All we know that it is possible to increase the period to 45. so the answer should be A"
      },
      {
        "date": "2021-05-27T01:20:00.000Z",
        "voteCount": 2,
        "content": "it is a matter how the requirement was understood and I agree with answer B.\n\nReference: https://docs.microsoft.com/en-us/azure/azure-monitor/logs/manage-cost-storage"
      },
      {
        "date": "2021-01-14T04:00:00.000Z",
        "voteCount": 5,
        "content": "Agreed. It is possible to increase the retention period, but the question makes no mention of this step. Simply doing what the question says, will lead to log files being kept for 30 days. This does not satisfy the requirement."
      },
      {
        "date": "2020-04-19T05:12:00.000Z",
        "voteCount": 13,
        "content": "When we can change the default retention to 730 days, why is A wrong answer?"
      },
      {
        "date": "2021-09-21T12:47:00.000Z",
        "voteCount": 1,
        "content": "indeed default is 31 for standalone. so B"
      },
      {
        "date": "2021-06-01T12:48:00.000Z",
        "voteCount": 2,
        "content": "The answer is YES. \nReference 1: Whizlabs course.\nReference 2: https://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?tabs=azure-t-sql"
      },
      {
        "date": "2021-04-10T09:01:00.000Z",
        "voteCount": 9,
        "content": "Answer should be yes, default Azure log analytics retention period is 90 day.\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/logs/manage-cost-storage"
      },
      {
        "date": "2021-01-02T15:46:00.000Z",
        "voteCount": 7,
        "content": "log analytics default is 90 days so answer is A"
      },
      {
        "date": "2020-12-08T07:48:00.000Z",
        "voteCount": 2,
        "content": "Log analytics is correct\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview:\n\"Auditing for Azure SQL Database and Azure Synapse Analytics tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace, or Event Hubs.\""
      },
      {
        "date": "2020-12-05T07:15:00.000Z",
        "voteCount": 1,
        "content": "You have multiple options for configuring where audit logs will be written. You can write logs to an Azure storage account, to a Log Analytics workspace for consumption by Azure Monitor logs (preview), or to event hub for consumption using event hub (preview). You can configure any combination of these options, and audit logs will be written to each.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview\nshould be Log analytics, for sure."
      },
      {
        "date": "2020-11-21T22:01:00.000Z",
        "voteCount": 2,
        "content": "What's the point of switching to another type of storage just because you don't change the default setting that is changeable?"
      },
      {
        "date": "2020-09-26T07:56:00.000Z",
        "voteCount": 1,
        "content": "Previous Similar question to store to blob is yes.  Log analytics is no and have to set retention."
      },
      {
        "date": "2020-08-14T06:15:00.000Z",
        "voteCount": 10,
        "content": "To set up 45 days, you have to change the default retention which is an additional step that is not stated. That's why the answer is B."
      },
      {
        "date": "2020-07-27T10:15:00.000Z",
        "voteCount": 1,
        "content": "All similar pointer from previous indicate yes"
      },
      {
        "date": "2020-06-24T06:48:00.000Z",
        "voteCount": 1,
        "content": "The answer should be A. https://docs.microsoft.com/en-us/azure/azure-monitor/platform/manage-cost-storage#change-the-data-retention-period. \nhttps://azure.microsoft.com/en-gb/pricing/details/monitor/"
      },
      {
        "date": "2020-05-23T07:55:00.000Z",
        "voteCount": 2,
        "content": "The explanation says increasing to 730 days is only possible in the non-free tier, so I guess B is correct?"
      },
      {
        "date": "2020-06-04T23:30:00.000Z",
        "voteCount": 4,
        "content": "The question does not specify the tier. It only ask to design a solution, so: it is possiible to have 45 days ? to me the answer is yes."
      },
      {
        "date": "2020-05-11T07:56:00.000Z",
        "voteCount": 4,
        "content": "It's A, the reference for this question might be old"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53393-exam-dp-201-topic-3-question-14-discussion/",
    "body": "You have an Azure Storage account.<br>You plan to copy one million image files to the storage account.<br>You plan to share the files with an external partner organization. The partner organization will analyze the files during the next year.<br>You need to recommend an external access solution for the storage account. The solution must meet the following requirements:<br>\u2711 Ensure that only the partner organization can access the storage account.<br>Ensure that access of the partner organization is removed automatically after 365 days.<br><img src=\"/assets/media/exam-media/03774/0026400002.png\" class=\"in-exam-image\"><br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared keys",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage lifecycle management policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure policies",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signature (SAS)"
    ],
    "answer": "D",
    "answerDescription": "A shared access signature (SAS) is a URI that grants restricted access rights to Azure Storage resources. You can provide a shared access signature to clients who should not be trusted with your storage account key but to whom you wish to delegate access to certain storage account resources. By distributing a shared access signature URI to these clients, you can grant them access to a resource for a specified period of time, with a specified set of permissions.<br>Reference:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/delegate-access-with-shared-access-signature",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T03:25:00.000Z",
        "voteCount": 3,
        "content": "keyword is --&gt;&gt; removed automatically after 365 days."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30997-exam-dp-201-topic-3-question-15-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database that has columns. The columns contain sensitive Personally Identifiable Information (PII) data.<br>You need to design a solution that tracks and stores all the queries executed against the PII data. You must be able to review the data in Azure Monitor, and the data must be available for at least 45 days.<br>Solution: You execute a daily stored procedure that retrieves queries from Query Store, looks up the column classifications, and stores the results in a new table in the database.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead add classifications to the columns that contain sensitive data and turn on Auditing.<br>Note: Auditing has been enhanced to log sensitivity classifications or labels of the actual data that were returned by the query. This would enable you to gain insights on who is accessing sensitive data.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/announcing-public-preview-of-data-discovery-classification-for-microsoft-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-27T02:16:00.000Z",
        "voteCount": 2,
        "content": "It is definitely incorrect, query store usage is to provide information on the query plan and performance of Azure SQL.\n\nReference: https://docs.microsoft.com/en-us/sql/relational-databases/performance/monitoring-performance-by-using-the-query-store?view=sql-server-ver15"
      },
      {
        "date": "2021-01-06T22:20:00.000Z",
        "voteCount": 1,
        "content": "Correct link referred in answer should be below as questions refers to Azure SQL Database not Azure SQL Warehouse/Azure Synapse Analytics:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview"
      },
      {
        "date": "2020-12-24T05:31:00.000Z",
        "voteCount": 1,
        "content": "The explanation is incomplete : it misses 'you have to set the default retention time to 45 days because by default is 31 days'."
      },
      {
        "date": "2020-09-10T03:26:00.000Z",
        "voteCount": 1,
        "content": "discovery-classification is it in preview?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11626-exam-dp-201-topic-3-question-16-discussion/",
    "body": "A company stores sensitive information about customers and employees in Azure SQL Database.<br>You need to ensure that the sensitive data remains encrypted in transit and at rest.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransparent Data Encryption",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAlways Encrypted with secure enclaves",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Disk Encryption",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server AlwaysOn"
    ],
    "answer": "B",
    "answerDescription": "Incorrect Answers:<br>A: Transparent Data Encryption (TDE) encrypts SQL Server, Azure SQL Database, and Azure Synapse Analytics data files, known as encrypting data at rest. TDE does not provide encryption across communication channels.<br>Reference:<br>https://cloudblogs.microsoft.com/sqlserver/2018/12/17/confidential-computing-using-always-encrypted-with-secure-enclaves-in-sql-server-2019-preview/",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-19T02:57:00.000Z",
        "voteCount": 55,
        "content": "The answer is A. Azure SQL db auto enforces TLS (Transport layer security) which means that the data will be encrypted in transit. Enable TDE (Transparent data encryption) and Azure will encrypt your DB files, log files and backup files (= data at rest)"
      },
      {
        "date": "2020-08-09T08:04:00.000Z",
        "voteCount": 1,
        "content": "A correct: Transparent Data Encryption (TDE) encrypts SQL Server, Azure SQL Database, and Azure Synapse Analytics (SQL Data Warehouse) data files. This encryption is known as encrypting data at rest\n\nBy default, Azure Storage accounts permit clients to send and receive data with the oldest version of TLS, TLS 1.0, and above. To enforce stricter security measures, you can configure your storage account to require that clients send and receive data with a newer version of TLS.\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/transparent-data-encryption?view=sql-server-ver15\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/transport-layer-security-configure-minimum-version?tabs=portal\n\nB not correct, see pingvins11 comment: \n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-enclaves?view=sql-server-ver15"
      },
      {
        "date": "2021-01-18T22:51:00.000Z",
        "voteCount": 7,
        "content": "Wrong, The answer is B cause TDE is only for rest and not for transit."
      },
      {
        "date": "2021-05-31T02:51:00.000Z",
        "voteCount": 5,
        "content": "Appropriate answer is B and the explanation is included in the link below.\n\nReference: https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-enclaves?view=sql-server-ver15"
      },
      {
        "date": "2021-05-18T19:56:00.000Z",
        "voteCount": 2,
        "content": "the provided answer is correct: https://docs.microsoft.com/en-us/azure/azure-sql/database/always-encrypted-azure-key-vault-configure?tabs=azure-powershell"
      },
      {
        "date": "2021-04-07T23:16:00.000Z",
        "voteCount": 16,
        "content": "It\u2019s incredible the highest voted answer is wrong. \nB is correct."
      },
      {
        "date": "2021-09-28T03:23:00.000Z",
        "voteCount": 2,
        "content": "Important *\nTDE doesn't provide encryption across communication channels.  So B is correct"
      },
      {
        "date": "2021-02-01T18:36:00.000Z",
        "voteCount": 2,
        "content": "Actually, after digging more,\nB is the correct option. Ignore my previous post.\nhttps://docs.microsoft.com/en-us/learn/modules/protect-data-transit-rest/5-explain-object-encryption-secure-enclaves"
      },
      {
        "date": "2021-02-01T18:28:00.000Z",
        "voteCount": 1,
        "content": "B - is incorrect, because it is in preview\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-enclaves?view=sql-server-ver15\nAlways Encrypted with secure enclaves is available in SQL Server 2019 (15.x) and in Azure SQL Database (in preview)."
      },
      {
        "date": "2020-12-27T14:52:00.000Z",
        "voteCount": 3,
        "content": "It can\u00b4t be A as TDE doesn\u00b4t support in transit. Best option is: \nTDE as the first line of defense (and to meet common compliance requirements) to encrypt the entire database at rest.\nTLS to protect all traffic to the database.\nAlways Encrypted to protect highly sensitive data from high-privilege users and malware in the database environment.\nhttps://azure.microsoft.com/es-es/blog/transparent-data-encryption-or-always-encrypted/\nFor me only accepted option despite it only works on SQL 2019 is the current answer."
      },
      {
        "date": "2020-12-24T21:11:00.000Z",
        "voteCount": 1,
        "content": "Transparent data encryption - APPLIES TO: Azure SQL Database, Azure SQL Managed Instance, Azure Synapse Analytics \n\nAlways Encrypted with secure enclaves - Applies to: SQL Server 2019 (15.x) - Windows only"
      },
      {
        "date": "2020-12-23T14:13:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B. In SQL server management studio you can do always encrypt which encrypts the data at rest and in transit. TDE and TLS are enabled by default, so TDE alone cannot be the correct answer."
      },
      {
        "date": "2020-12-08T07:41:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-enclaves?view=sql-server-ver15:\n\"Always Encrypted with secure enclaves provides additional functionality to the Always Encrypted feature.\"\nB is correct"
      },
      {
        "date": "2020-11-17T05:34:00.000Z",
        "voteCount": 3,
        "content": "it seems, that we don't have the right answer in the options. Maybe the question is badly worded?"
      },
      {
        "date": "2020-09-27T01:56:00.000Z",
        "voteCount": 4,
        "content": "How A can be right Ans..?\nEncrypting your data at rest, which means encrypting it while it is stored on whatever file storage you use.\nEncrypting your data in transit, which means encrypting data while it travels through private or public network communication channels.\nEncrypting your data in use, which means encrypting it while it is actively used in RAM or CPU caches and registers.\n\nhttps://docs.microsoft.com/en-us/dynamics365/business-central/dev-itpro/security/transparent-data-encryption#:~:text=Encrypting%20your%20data%20in%20transit,or%20CPU%20caches%20and%20registers.\n\nImportant\n\nTDE doesn't provide encryption across communication channels. For more information about how to encrypt data across communication channels, see Enable Encrypted Connections to the Database Engine (SQL Server Configuration Manager)."
      },
      {
        "date": "2020-08-23T19:58:00.000Z",
        "voteCount": 1,
        "content": "Option A\nTransparent data encryption (TDE) helps protect Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics against the threat of malicious offline activity by encrypting data at rest. It performs real-time encryption and decryption of the database, associated backups, and transaction log files at rest without requiring changes to the application. By default, TDE is enabled for all newly deployed SQL Databases and must be manually enabled for older databases of Azure SQL Database, Azure SQL Managed Instance. TDE must be manually enabled for Azure Synapse Analytics"
      },
      {
        "date": "2020-08-21T06:43:00.000Z",
        "voteCount": 1,
        "content": "By looking at this in Documentation answer B is correct:\nAlways Encrypted also protects the data, stored in encrypted columns, at rest and in transit. However, unless your goal is to protect sensitive data in use, TDE is the recommended choice for encryption at rest, and we recommend TLS for protecting data in-transit. In fact, it is often advised to use Always Encrypted, TDE, and TLS together:"
      },
      {
        "date": "2020-08-21T06:41:00.000Z",
        "voteCount": 1,
        "content": "It should be A.\nTransparent Data Encryption\nTDE is intended to add a layer of security to protect data at rest from offline access to raw files or backups, common scenarios include datacenter theft or unsecured disposal of hardware or media such as disk drives and backup tapes. For a deeper look into how TDE protects against the risk of malicious parties trying to recover stolen databases: data, log files, snapshots, copies or backups and to review TDE best practices see Feature Spotlight: Transparent Data Encryption (TDE)."
      },
      {
        "date": "2020-08-21T06:44:00.000Z",
        "voteCount": 1,
        "content": "ignore this"
      },
      {
        "date": "2020-07-27T03:40:00.000Z",
        "voteCount": 1,
        "content": "Common sense people!.. Transparent Data Encryption is a technology employed by Microsoft, IBM and Oracle to encrypt database files. TDE offers encryption at file level. TDE solves the problem of protecting data at rest, encrypting databases both on the hard drive and consequently on backup media.  Answer is A"
      },
      {
        "date": "2020-07-23T01:12:00.000Z",
        "voteCount": 1,
        "content": "TDE doesn't encrypt the data in transit and only at rest.https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/transparent-data-encryption?view=sql-server-ver15\n\nHence, I feel 'A' may not be correct."
      },
      {
        "date": "2020-07-18T05:52:00.000Z",
        "voteCount": 1,
        "content": "Always Encrypted with secure enclaves:10/31/2019\n14 minutes to read\n     \nTHIS TOPIC APPLIES TO: Yes to SQL Server 2019 and later (Windows only)  \nNo to Azure SQL Database, No to Azure Synapse Analytics (SQL DW), No to Parallel Data Warehouse"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53394-exam-dp-201-topic-3-question-17-discussion/",
    "body": "DRAG DROP -<br>You are designing a data warehouse in Azure Synapse Analytics for a financial services company. Azure Active Directory will be used to authenticate the users.<br>You need to ensure that the following security requirements are met:<br>\u2711 Department managers must be able to create new database.<br>\u2711 The IT department must assign users to databases.<br>\u2711 Permissions granted must be minimized.<br>Which role memberships should you recommend? To answer, drag the appropriate roles to the correct groups. Each role may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0026800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0026900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: dbmanager -<br>Members of the dbmanager role can create new databases.<br><br>Box 2: db_accessadmin -<br>Members of the db_accessadmin fixed database role can add or remove access to the database for Windows logins, Windows groups, and SQL Server logins.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-manage-logins",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-25T18:16:00.000Z",
        "voteCount": 1,
        "content": "Why wouldn't IT get db_accessadmin? That is least privilege that allows them to assign user access. db_securityadmin can elevate their own privileges."
      },
      {
        "date": "2021-06-10T00:12:00.000Z",
        "voteCount": 4,
        "content": "Given answers are correct. Refer to the provided link below.\n\nReference: https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/database-level-roles?view=sql-server-ver15"
      },
      {
        "date": "2021-06-01T11:56:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct, &gt; https://docs.microsoft.com/en-us/sql/relational-databases/security/authentication-access/database-level-roles?view=sql-server-ver15"
      },
      {
        "date": "2021-06-01T11:55:00.000Z",
        "voteCount": 3,
        "content": "Wouldn't the IT group get the loginmanager role?"
      },
      {
        "date": "2021-06-09T05:53:00.000Z",
        "voteCount": 2,
        "content": "Assuming users have login access. You are assigning them to different database based on project."
      },
      {
        "date": "2021-05-23T03:33:00.000Z",
        "voteCount": 1,
        "content": "%100 ... CORRECT"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15924-exam-dp-201-topic-3-question-18-discussion/",
    "body": "You plan to use Azure SQL Database to support a line of business app.<br>You need to identify sensitive data that is stored in the database and monitor access to the data.<br>Which three actions should you recommend? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Data Discovery and Classification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Transparent Data Encryption (TDE).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Auditing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun Vulnerability Assessment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Advanced Threat Protection."
    ],
    "answer": "ACE",
    "answerDescription": "A: Data Discovery &amp; Classification is built into Azure SQL Database, Azure SQL Managed Instance, and Azure Synapse Analytics. It provides advanced capabilities for discovering, classifying, labeling, and reporting the sensitive data in your databases.<br>C: An important aspect of the information-protection paradigm is the ability to monitor access to sensitive data. Azure SQL Auditing has been enhanced to include a new field in the audit log called data_sensitivity_information. This field logs the sensitivity classifications (labels) of the data that was returned by a query.<br>E: Data Discovery &amp; Classification is part of the Advanced Data Security offering, which is a unified package for advanced Azure SQL security capabilities. You can access and manage Data Discovery &amp; Classification via the central SQL Advanced Data Security section of the Azure portal.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-10T17:01:00.000Z",
        "voteCount": 77,
        "content": "ACE\nD - is for finding the vulnerability. Question is not for finding the vulnerability but identify the sensitive data and monitor the access."
      },
      {
        "date": "2020-09-14T05:22:00.000Z",
        "voteCount": 2,
        "content": "Vulnerability Assessment is a scanning service built into Azure SQL Database. The service employs a knowledge base of rules that flag security vulnerabilities. It highlights deviations from best practices, such as misconfigurations, excessive permissions, and unprotected sensitive data.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-vulnerability-assessment"
      },
      {
        "date": "2021-06-10T00:14:00.000Z",
        "voteCount": 1,
        "content": "This solution is the appropriate for the requirements."
      },
      {
        "date": "2020-03-16T22:45:00.000Z",
        "voteCount": 44,
        "content": "correct answer - ACE , I believe"
      },
      {
        "date": "2020-08-03T06:59:00.000Z",
        "voteCount": 8,
        "content": "Yes, ACE is correct Answer.. Vulnerability assessment in not fits the purpose"
      },
      {
        "date": "2021-06-01T14:31:00.000Z",
        "voteCount": 3,
        "content": "ACD\nReference Whizlabs Course."
      },
      {
        "date": "2020-12-08T00:13:00.000Z",
        "voteCount": 2,
        "content": "I would go for ACE\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/threat-detection-overview"
      },
      {
        "date": "2020-11-02T00:35:00.000Z",
        "voteCount": 3,
        "content": "https://docs.microsoft.com/en-us/learn/modules/secure-your-azure-sql-database/5-monitor-your-database\nData discovery &amp; classification (currently in preview)"
      },
      {
        "date": "2020-11-23T22:51:00.000Z",
        "voteCount": 1,
        "content": "IDENTIFY , Not Encrypt !"
      },
      {
        "date": "2020-10-26T09:18:00.000Z",
        "voteCount": 1,
        "content": "ACE is correct answer"
      },
      {
        "date": "2020-09-30T04:49:00.000Z",
        "voteCount": 4,
        "content": "**Correct Answer**\nA. Enable Data Discovery and Classification \nC. Enable Auditing\nE. Use Advanced Threat Protection."
      },
      {
        "date": "2020-06-25T14:26:00.000Z",
        "voteCount": 7,
        "content": "ACE\n\nA) reporting the sensitive data in your databases + monitoring (auditing) and alerting on anomalous access to sensitive data.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview\n\nC) Clicking View dashboard at the top of the Audit records page will open a dashboard displaying audit logs info, where you can drill down into Security Insights, Access to Sensitive Data and more.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview\n\nE) detects anomalous activities indicating unusual and potentially harmful attempts to access or exploit databases.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/threat-detection-overview"
      },
      {
        "date": "2020-06-25T12:07:00.000Z",
        "voteCount": 1,
        "content": "ACE: https://docs.microsoft.com/en-us/learn/modules/secure-your-azure-sql-database/5-monitor-your-database"
      },
      {
        "date": "2020-06-21T08:26:00.000Z",
        "voteCount": 1,
        "content": "vulnerability assessment can't be the right choice here, Vulnerability assessment is manual check and it doesn't integrate with monitor. ACE is the best choice here."
      },
      {
        "date": "2021-02-17T22:50:00.000Z",
        "voteCount": 1,
        "content": "You can configure to perform periodic scan automatically"
      },
      {
        "date": "2021-02-17T23:03:00.000Z",
        "voteCount": 1,
        "content": "However, I will also go for ACE because VA is better fit for other use-cases in context of proactive checks of unprotected sensitive data if masking is not configured etc. rather than tracking user activities like anomalous access pattern which is ATP"
      },
      {
        "date": "2020-06-10T11:13:00.000Z",
        "voteCount": 4,
        "content": "I think it's ACE. \nA) for identify sensitive data, C) Threat Detection requires Auditing (see link), and E) - ATP should meet the monitor access to the sensitive data.\n(C) - https://docs.microsoft.com/en-us/azure/azure-sql/database/threat-detection-overview#overview.\n\nThe issue is when you read about \"Vulnerability Assessment\", it seems to encapsulate A &amp; E. The key is 'Run' Vulnerability Assessment, instead of 'Enable' or 'Use' &lt;X&gt;. The run VA seems to be a once of to get reports, where as the requirement is constant and live."
      },
      {
        "date": "2020-06-07T12:17:00.000Z",
        "voteCount": 2,
        "content": "A, D, E\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview"
      },
      {
        "date": "2020-04-23T22:11:00.000Z",
        "voteCount": 3,
        "content": "Vulnerability Assessment is a scanning service built into Azure SQL Database. The service employs a knowledge base of rules that flag security vulnerabilities. It highlights deviations from best practices, such as misconfigurations, excessive permissions, and unprotected sensitive data.\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-vulnerability-assessment"
      },
      {
        "date": "2020-05-05T05:09:00.000Z",
        "voteCount": 2,
        "content": "The scan report also provides a map of sensitive data discovered in your database. It includes recommendations to classify that data by using data discovery and classification.\n\n- It makes sense. Thank you."
      },
      {
        "date": "2020-05-26T12:10:00.000Z",
        "voteCount": 1,
        "content": "So ACDE is probably best answer.  Does Auditing really need to be enabled?"
      },
      {
        "date": "2020-03-10T01:44:00.000Z",
        "voteCount": 8,
        "content": "\"You need to identify sensitive data that is stored in the database and monitor access to the data\" this should be A, Discovery and Classification.\nADE"
      },
      {
        "date": "2020-03-09T03:45:00.000Z",
        "voteCount": 6,
        "content": "but Data Discovery &amp; Classification introduces a new tool built into SQL Server Management Studio (SSMS) for discovering, classifying, labeling &amp; reporting the sensitive data in your databases."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19384-exam-dp-201-topic-3-question-19-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database that has columns. The columns contain sensitive Personally Identifiable Information (PII) data.<br>You need to design a solution that tracks and stores all the queries executed against the PII data. You must be able to review the data in Azure Monitor, and the data must be available for at least 45 days.<br>Solution: You create a SELECT trigger on the table in SQL Database that writes the query to a new table in the database, and then executes a stored procedure that looks up the column classifications and joins to the query text.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead add classifications to the columns that contain sensitive data and turn on Auditing.<br>Note: Auditing has been enhanced to log sensitivity classifications or labels of the actual data that were returned by the query. This would enable you to gain insights on who is accessing sensitive data.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/announcing-public-preview-of-data-discovery-classification-for-microsoft-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-30T10:25:00.000Z",
        "voteCount": 11,
        "content": "There is no such thing as \"Select\" trigger."
      },
      {
        "date": "2021-05-01T23:46:00.000Z",
        "voteCount": 1,
        "content": "Correct. https://www.sqlshack.com/triggers-in-sql-server/"
      },
      {
        "date": "2020-08-21T06:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15961-exam-dp-201-topic-3-question-20-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database that has columns. The columns contain sensitive Personally Identifiable Information (PII) data.<br>You need to design a solution that tracks and stores all the queries executed against the PII data. You must be able to review the data in Azure Monitor, and the data must be available for at least 45 days.<br>Solution: You add classifications to the columns that contain sensitive data. You turn on Auditing and set the audit log destination to use Azure Blob storage.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Auditing has been enhanced to log sensitivity classifications or labels of the actual data that were returned by the query. This would enable you to gain insights on who is accessing sensitive data.<br>Note: You now have multiple options for configuring where audit logs will be written. You can write logs to an Azure storage account, to a Log Analytics workspace for consumption by Azure Monitor logs, or to event hub for consumption using event hub. You can configure any combination of these options, and audit logs will be written to each.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/announcing-public-preview-of-data-discovery-classification-for-microsoft-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-24T06:35:00.000Z",
        "voteCount": 7,
        "content": "Sending logs to blob meets 45 days storage requirement. But how about, \"You must be able to review the data in Azure Monitor\"? I think it should be NO."
      },
      {
        "date": "2021-03-27T01:27:00.000Z",
        "voteCount": 1,
        "content": "In the Azure Log Analytics, which is part of the Azure Monitor Tool, you can add these logs added manually. Just go to the advanced options and configure the connection"
      },
      {
        "date": "2021-06-27T21:05:00.000Z",
        "voteCount": 1,
        "content": "90 days in log analytics. So Log Aalaytics works(bure previous answer was no to this question which is silly). Now about blob storage we can easily connect to Azure Monitor/log analytics and then it will be available in Monitor(to meet the question). But then where is that extra step of connecting to log analytics? Also if you plan to connect to log analytics, why use storage account? The only extra benefit of doing this (Storage + log analytics) is that auditing information(the user) is only mentioned in the storage logs and is masked in log analytics. But such a scenario is not asked in this question. So the answer to this question is a big NO. Log Anlaytics is the correct answer. Dont even think about replying to this text"
      },
      {
        "date": "2021-06-27T21:10:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/66302107/unable-to-get-the-user-id-identity-details-from-log-analytics-workspace-captured\nWell, I might be wrong: \"Log analytics does not capture any PII\" but then the storage to log analytics connection is missing in the question"
      },
      {
        "date": "2021-06-27T21:17:00.000Z",
        "voteCount": 1,
        "content": "now if log analytics really masks the PII then how will it work when storage account is connected to Log Analytics for monitor? So the stackoverflow answer is wrong and my answer aboe is correct or the answer to this question is \"NO\"still"
      },
      {
        "date": "2020-12-08T07:46:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview\nAnswer is yes"
      },
      {
        "date": "2021-03-22T06:56:00.000Z",
        "voteCount": 2,
        "content": "Bro same article the answer IS NO, you can see auditing information is monitor if they are stored in blob storage"
      },
      {
        "date": "2020-07-08T15:17:00.000Z",
        "voteCount": 1,
        "content": "on the page 20 in this dump the same question is answed no and here yes ! just wondering if you urself are not sure of ur answer"
      },
      {
        "date": "2020-07-24T06:04:00.000Z",
        "voteCount": 8,
        "content": "Over there log analytics is used instead of blob."
      },
      {
        "date": "2021-06-15T08:21:00.000Z",
        "voteCount": 1,
        "content": "Nothing on page 20"
      },
      {
        "date": "2020-03-09T08:25:00.000Z",
        "voteCount": 3,
        "content": "But if you need  to use Azure Monitor you should  audit to Log Analytics destination.\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-auditing"
      },
      {
        "date": "2020-03-16T22:13:00.000Z",
        "voteCount": 17,
        "content": "Here the storage is needed for 45 days, Log analytics can store default logs till 31 days. Blob would be a convenient storage medium if longer retention is needed from default."
      },
      {
        "date": "2020-04-25T00:47:00.000Z",
        "voteCount": 7,
        "content": "With free tier is up to 31 days, but you can store it up 730 days for an increased charge, also if you're using Sentinel it's stored for 90 days for free. \nhttps://www.shudnow.io/2019/10/14/increasing-azure-log-analytics-retention-per-data-type/\nhttps://blogs.msdn.microsoft.com/canberrapfe/2017/01/25/change-oms-log-analytics-retention-period-in-the-azure-portal/\n\nI believe the answer is Log Analytics, with storage account you wouldn't be able to set up alerts and monitor it from the Azure Monitor service."
      },
      {
        "date": "2020-04-30T10:27:00.000Z",
        "voteCount": 1,
        "content": "But the solution will work"
      },
      {
        "date": "2020-05-28T01:51:00.000Z",
        "voteCount": 2,
        "content": "Yes, it is a valid solution. It can work with log stored in a Blob or Log Analytics."
      },
      {
        "date": "2020-09-18T10:18:00.000Z",
        "voteCount": 2,
        "content": "I can't find any articles that suggest Azure Monitor can works with log data held in Blob Storage. Can you please confirm why you think this is so?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49797-exam-dp-201-topic-3-question-21-discussion/",
    "body": "You have an Azure subscription that contains an Azure virtual machine and an Azure Storage account. The virtual machine will access the storage account.<br>You are planning the security design for the storage account.<br>You need to ensure that only the virtual machine can access the storage account.<br>Which two actions should you include in the design? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Allow trusted Microsoft services to access this storage account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Allow read access to storage logging from any network.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable a virtual network service endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the Allow access from setting to Selected networks."
    ],
    "answer": "AC",
    "answerDescription": "C: Virtual Network (VNet) service endpoint provides secure and direct connectivity to Azure services over an optimized route over the Azure backbone network.<br>Endpoints allow you to secure your critical Azure service resources to only your virtual networks. Service Endpoints enables private IP addresses in the VNet to reach the endpoint of an Azure service without needing a public IP address on the VNet.<br>A: You must have Allow trusted Microsoft services to access this storage account turned on under the Azure Storage account Firewalls and Virtual networks settings menu.<br>Incorrect Answers:<br>D: Virtual Network (VNet) service endpoint policies allow you to filter egress virtual network traffic to Azure Storage accounts over service endpoint, and allow data exfiltration to only specific Azure Storage accounts. Endpoint policies provide granular access control for virtual network traffic to Azure Storage when connecting over service endpoint.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/virtual-network/virtual-network-service-endpoints-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-06T11:51:00.000Z",
        "voteCount": 23,
        "content": "Answer: CD"
      },
      {
        "date": "2021-06-01T02:29:00.000Z",
        "voteCount": 2,
        "content": "This is the appropriate configuration for the requirement"
      },
      {
        "date": "2021-04-11T07:28:00.000Z",
        "voteCount": 3,
        "content": "Should be A and D: https://docs.microsoft.com/en-us/azure/storage/common/storage-network-security?tabs=azure-portal#grant-access-from-a-virtual-network"
      },
      {
        "date": "2021-04-11T07:29:00.000Z",
        "voteCount": 4,
        "content": "I meant C and D"
      },
      {
        "date": "2021-04-13T11:09:00.000Z",
        "voteCount": 6,
        "content": "I agree with rahul_t.\n\nA. Select Allow trusted Microsoft services to access this storage account.\n\nNo - this setting is too broad and does not restrict access to the VM which is NOT a Microsoft service in this usage\n\nB. Select Allow read access to storage logging from any network.\n\nAgain, too permissive. We want to limit access not allow it from anywhere.\n\nC. Enable a virtual network service endpoint. - First step\nD. Set the Allow access from setting to Selected networks. - Second step"
      },
      {
        "date": "2021-04-10T06:25:00.000Z",
        "voteCount": 1,
        "content": "Hmm, why not D instead of A?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53397-exam-dp-201-topic-3-question-22-discussion/",
    "body": "You are designing an app that will provide a data cleaning and supplementing service for customers. The app will use Azure Data Factory to run a daily process to read and write data from Azure Storage blob containers.<br>You need to recommend an access mechanism for the customers to grant the app access to their data. The solution must meet the following requirements:<br>\u2711 Provide access for a period of three months.<br>\u2711 Restrict the app's access to specific containers.<br>\u2711 Minimize administrative effort.<br>\u2711 Minimize changes to the existing access controls of the customer's Azure Storage accounts.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tanonymous public read access",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed identity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)"
    ],
    "answer": "D",
    "answerDescription": "A shared access signature (SAS) provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data. For example:<br>\u2711 What resources the client may access.<br>\u2711 What permissions they have to those resources.<br>\u2711 How long the SAS is valid.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T04:28:00.000Z",
        "voteCount": 4,
        "content": "keyword is --&gt;&gt; Provide access for a period of three months."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55727-exam-dp-201-topic-3-question-23-discussion/",
    "body": "You need to implement an Azure Storage account that will use a Blob service endpoint that uses zone-redundant storage (ZRS).<br>The storage account must only accept connections from a virtual network over Azure Private Link.<br>What should you include in the implementation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta private endpoint for Azure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta customer-managed key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta firewall rule to allow traffic from the virtual network"
    ],
    "answer": "A",
    "answerDescription": "You can use private endpoints for your Azure Storage accounts to allow clients on a virtual network (VNet) to securely access data over a Private Link.<br>When creating the private endpoint, you must specify the storage account and the storage service to which it connects. You need a separate private endpoint for each storage service in a storage account that you need to access, namely Blobs, Data Lake Storage Gen2, Files, Queues, Tables, or Static Websites.<br>Note: The private endpoint uses an IP address from the VNet address space for your storage account service. Network traffic between the clients on the VNet and the storage account traverses over the VNet and a private link on the Microsoft backbone network, eliminating exposure from the public internet.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-private-endpoints",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-21T08:37:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2021-06-21T02:00:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28193-exam-dp-201-topic-3-question-24-discussion/",
    "body": "You have an Azure Data Lake Storage account that has a virtual network service endpoint configured.<br>You plan to use Azure Data Factory to extract data from the Data Lake Storage account. The data will then be loaded to a data warehouse in Azure Synapse<br>Analytics by using PolyBase.<br>Which authentication method should you use to access Data Lake Storage?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access key authentication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanaged identity authentication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccount key authentication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tservice principal authentication"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse#use-polybase-to-load-data-into-azure-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-11T18:09:00.000Z",
        "voteCount": 27,
        "content": "If your Azure Storage is configured with VNet service endpoint, you must use managed identity authentication"
      },
      {
        "date": "2020-12-08T00:54:00.000Z",
        "voteCount": 5,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage:\n\"A data factory can be associated with a managed identity for Azure resources, which represents this specific data factory. You can directly use this managed identity for Data Lake Storage Gen2 authentication, similar to using your own service principal. It allows this designated factory to access and copy data to or from your Data Lake Storage Gen2.\"\nB is correct"
      },
      {
        "date": "2021-04-07T05:29:00.000Z",
        "voteCount": 4,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage\n\n\"When you use PolyBase or COPY statement to load data into Azure Synapse Analytics, if your source or staging Data Lake Storage Gen2 is configured with an Azure Virtual Network endpoint, you must use managed identity authentication as required by Synapse. See the managed identity authentication section with more configuration prerequisites.\""
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54933-exam-dp-201-topic-3-question-25-discussion/",
    "body": "HOTSPOT -<br>A company plans to use Azure SQL Database to support a line of business application. The application will manage sensitive employee data.<br>The solution must meet the following requirements:<br>\u2711 Encryption must be performed by the application.<br>\u2711 Only the client application must have access keys for encrypting and decrypting data.<br>\u2711 Data must never appear as plain text in the database.<br>\u2711 The strongest possible encryption method must be used.<br>\u2711 Searching must be possible on selected data.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0027600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0027700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Always Encrypted with deterministic encryption<br>Deterministic encryption always generates the same encrypted value for any given plain text value. Using deterministic encryption allows point lookups, equality joins, grouping and indexing on encrypted columns. However, it may also allow unauthorized users to guess information about encrypted values by examining patterns in the encrypted column, especially if there is a small set of possible encrypted values, such as True/False, or North/South/East/West region.<br>Deterministic encryption must use a column collation with a binary2 sort order for character columns.<br>Box 2: Always Encrypted with Randomized encryption<br>\u2711 Randomized encryption uses a method that encrypts data in a less predictable manner. Randomized encryption is more secure, but prevents searching, grouping, indexing, and joining on encrypted columns.<br>Note: With Always Encrypted the Database Engine never operates on plaintext data stored in encrypted columns, but it still supports some queries on encrypted data, depending on the encryption type for the column. Always Encrypted supports two types of encryption: randomized encryption and deterministic encryption.<br>Use deterministic encryption for columns that will be used as search or grouping parameters, for example a government ID number. Use randomized encryption, for data such as confidential investigation comments, which are not grouped with other records and are not used to join tables.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-21T08:39:00.000Z",
        "voteCount": 1,
        "content": "SAME Question (duplication),... Correct!"
      },
      {
        "date": "2021-06-08T20:06:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18092-exam-dp-201-topic-3-question-26-discussion/",
    "body": "You need to recommend a security solution for containers in Azure Blob storage. The solution must ensure that only read permissions are granted to a specific user for a specific container.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signatures (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan RBAC role in Azure Active Directory (Azure AD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpublic read access for blobs only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccess keys"
    ],
    "answer": "A",
    "answerDescription": "You can delegate access to read, write, and delete operations on blob containers, tables, queues, and file shares that are not permitted with a service SAS.<br>Note: A shared access signature (SAS) provides secure delegated access to resources in your storage account without compromising the security of your data.<br>With a SAS, you have granular control over how a client can access your data. You can control what resources the client may access, what permissions they have on those resources, and how long the SAS is valid, among other parameters.<br>Incorrect Answers:<br>C: You can enable anonymous, public read access to a container and its blobs in Azure Blob storage. By doing so, you can grant read-only access to these resources without sharing your account key, and without requiring a shared access signature (SAS).<br>Public read access is best for scenarios where you want certain blobs to always be available for anonymous read access.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-24T09:55:00.000Z",
        "voteCount": 26,
        "content": "Reading this carefully: \"granted to a specific user for a specific container.\" It should be RBAC. SAS is not for a specific user. It is designed to be created and given to ANY authenticated user. On the other hand, RBAC is to authorize the users on the container level."
      },
      {
        "date": "2020-05-17T02:17:00.000Z",
        "voteCount": 8,
        "content": "only if the requirement is to provide access to blob for a specific period of time, sas to be used."
      },
      {
        "date": "2021-06-27T21:32:00.000Z",
        "voteCount": 1,
        "content": "It is pretty simple sent the SAS to just this user. But we cannot prove if someone else accessed it. In AD if the user is added to a role and then given the role access then we know the role accessed but not if other users are in that role!"
      },
      {
        "date": "2021-06-27T21:34:00.000Z",
        "voteCount": 1,
        "content": "and the smart ass did not explain why B is not a correct answer :D"
      },
      {
        "date": "2021-04-29T09:43:00.000Z",
        "voteCount": 3,
        "content": "B. an RBAC role in Azure Active Directory (Azure AD)"
      },
      {
        "date": "2021-04-29T09:44:00.000Z",
        "voteCount": 2,
        "content": "Explanation\nWhile both SAS and RBAC can achieve this. However user delegated SAS is backed by Azure AD, hence RBAC is a preferred way.\n\nAzure Active Directory (Azure AD) authorizes access rights to secured resources through Azure role-based access control (Azure RBAC). Azure Storage defines a set of Azure built-in roles that encompass common sets of permissions used to access blob or queue data.\n\nWhen an Azure role is assigned to an Azure AD security principal, Azure grants access to those resources for that security principal. Access can be scoped to the level of the subscription, the resource group, the storage account, or an individual container or queue.\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-portal"
      },
      {
        "date": "2021-02-09T01:53:00.000Z",
        "voteCount": 1,
        "content": "RBAC: Storage Blob Data Owner: Provides full access to Azure Storage blob containers and data, including assigning POSIX access control. To learn which actions are required for a given data operation: https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#storage-blob-data-reader"
      },
      {
        "date": "2021-01-25T20:06:00.000Z",
        "voteCount": 2,
        "content": "There is no such thing as \"RBAC role in Azure Active Directory\""
      },
      {
        "date": "2021-01-18T11:48:00.000Z",
        "voteCount": 1,
        "content": "An RBAC on the container itself would be the easiest way. But option B states 'an RBAC role in Azure Active Directory (Azure AD)'. You don't create the role itself in AD. I therefore think SAS is the only valid option here."
      },
      {
        "date": "2021-01-02T14:54:00.000Z",
        "voteCount": 4,
        "content": "Answer is B . RBAC . \nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad#resource-scope"
      },
      {
        "date": "2021-01-15T20:46:00.000Z",
        "voteCount": 1,
        "content": "RBAC provides coarse grain access i.e: at account level. RBAC cannot provide access to specific containers . You will need to do ACL for that. Since, ACL is not an option here , the next best choice becomes SAS. Though , SAS is usually used for temporary time bound access."
      },
      {
        "date": "2020-12-29T01:30:00.000Z",
        "voteCount": 1,
        "content": "I think RBAC is correct - because you need to provide access to a specific user for a specific container (using SAS anyone with the URL can access the container)"
      },
      {
        "date": "2020-12-07T21:40:00.000Z",
        "voteCount": 5,
        "content": "The answer is correct\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview"
      },
      {
        "date": "2020-12-07T03:47:00.000Z",
        "voteCount": 1,
        "content": "I would go for B\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad"
      },
      {
        "date": "2020-10-05T10:25:00.000Z",
        "voteCount": 5,
        "content": "Both SAS and RBAC are okay for container level.\nFor folder level should be SAS.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2020-08-14T21:48:00.000Z",
        "voteCount": 5,
        "content": "SAS IS CORRECT"
      },
      {
        "date": "2020-08-05T02:30:00.000Z",
        "voteCount": 1,
        "content": "When your application design requires shared access signatures for access to Blob storage, use Azure AD credentials to create a user delegation SAS when possible for superior security.\nSo SAS is good choice"
      },
      {
        "date": "2021-06-27T21:36:00.000Z",
        "voteCount": 1,
        "content": "there is no \"a user delegation SAS\" in the question."
      },
      {
        "date": "2020-07-15T23:39:00.000Z",
        "voteCount": 1,
        "content": "Create a user delegation SAS for a container or blob with the Azure CLI https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-user-delegation-sas-create-cli"
      },
      {
        "date": "2020-06-21T08:59:00.000Z",
        "voteCount": 3,
        "content": "Either of Delegation SAS  and RBAC can apply for permissions at the container level, the question should be more specific, or options"
      },
      {
        "date": "2020-05-05T05:21:00.000Z",
        "voteCount": 1,
        "content": "\"To get the user delegation key and create the SAS, an Azure AD security principal must be assigned a role-based access control (RBAC) \"\n-&gt; I would say RBAC directly then ?\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-sas-overview"
      },
      {
        "date": "2021-08-22T17:58:00.000Z",
        "voteCount": 1,
        "content": "I think you are right, besides the question start with 'recommend a security solution for containers', maybe SAS works too but RBAC is more secure 'Authorizing requests against Azure Storage with Azure AD provides superior security ...Microsoft recommends using Azure AD authorization with your blob applications when possible to assure access with minimum required privileges.' https://docs.microsoft.com/en-us/azure/storage/blobs/authorize-access-azure-active-directory"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18787-exam-dp-201-topic-3-question-27-discussion/",
    "body": "You are designing the security for an Azure SQL database.<br>You have an Azure Active Directory (Azure AD) group named Group1.<br>You need to recommend a solution to provide Group1 with read access to the database only.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta contained database user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta SQL login",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan RBAC role",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)"
    ],
    "answer": "A",
    "answerDescription": "Create a User for a security group<br>A best practice for managing your database is to use Windows security groups to manage user access. That way you can simply manage the customer at the<br>Security Group level in Active Directory granting appropriate permissions. To add a security group to SQL Data Warehouse, you use the Display Name of the security group as the principal in the CREATE USER statement.<br>CREATE USER [&lt;Security Group Display Name&gt;] FROM EXTERNAL PROVIDER WITH DEFAULT_SCHEMA = [&lt;schema&gt;];<br>In our AD instance, we have a security group called Sales Team with an alias of salesteam@company.com. To add this security group to SQL Data Warehouse you simply run the following statement:<br>CREATE USER [Sales Team] FROM EXTERNAL PROVIDER WITH DEFAULT_SCHEMA = [sales];<br>Reference:<br>https://blogs.msdn.microsoft.com/sqldw/2017/07/28/adding-ad-users-and-security-groups-to-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-07T04:09:00.000Z",
        "voteCount": 7,
        "content": "https://docs.microsoft.com/en-us/sql/relational-databases/security/contained-database-users-making-your-database-portable?view=sql-server-ver15:\n\"Use contained database users to authenticate SQL Server and SQL Database connections at the database level\"\nA is correct"
      },
      {
        "date": "2021-09-21T13:10:00.000Z",
        "voteCount": 1,
        "content": "indeed contained db user needed"
      },
      {
        "date": "2020-04-20T12:44:00.000Z",
        "voteCount": 1,
        "content": "Should be C since we already have an Azure AD group."
      },
      {
        "date": "2020-04-25T01:24:00.000Z",
        "voteCount": 36,
        "content": "You cannot grant access to database access using RBAC, it must be on the database level, so the correct answer is \"contained user access\"."
      },
      {
        "date": "2021-02-06T21:06:00.000Z",
        "voteCount": 3,
        "content": "This is correct... Please see below:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview\n\nStatement: \"Azure AD authentication uses contained database users to authenticate identities at the database level.\""
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18418-exam-dp-201-topic-3-question-28-discussion/",
    "body": "HOTSPOT -<br>You use Azure Data Lake Storage Gen2 to store data that data scientists and data engineers will query by using Azure Databricks interactive notebooks. The folders in Data Lake Storage will be secured, and users will have access only to the folders that relate to the projects on which they work.<br>You need to recommend which authentication methods to use for Databricks and Data Lake Storage to provide the users with the appropriate access. The solution must minimize administrative effort and development effort.<br>Which authentication method should you recommend for each Azure service? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0028000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0028100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Databricks: Personal access tokens<br>To authenticate and access Databricks REST APIs, you use personal access tokens. Tokens are similar to passwords; you should treat them with care. Tokens expire and can be revoked.<br>Data Lake Storage: Azure Active Directory<br>Azure Data Lake Storage Gen1 uses Azure Active Directory for authentication.<br>References:<br>https://docs.azuredatabricks.net/dev-tools/api/latest/authentication.html https://docs.microsoft.com/en-us/azure/data-lake-store/data-lakes-store-authentication-using-azure-active-directory",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-07T12:40:00.000Z",
        "voteCount": 30,
        "content": "Answer is Correct\nhttps://docs.databricks.com/dev-tools/api/latest/authentication.html\nhttps://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2\nhttps://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/azure-datalake-gen2#adls2-aad-credentials"
      },
      {
        "date": "2020-07-11T04:53:00.000Z",
        "voteCount": 22,
        "content": "To minimize the admin effort the best option would be a creating a High Concurrency cluster enable AD credential passthrough, using RBAC to assign contributor role to the AD users (data engineers and data analyst) to the databricks workspace,  apply ACLs to the specific folders for the AD users. Active Directory authentication perfectly works for both."
      },
      {
        "date": "2021-09-21T13:11:00.000Z",
        "voteCount": 1,
        "content": "both shud be AD. ADB auto authenticates with AD"
      },
      {
        "date": "2021-03-09T19:33:00.000Z",
        "voteCount": 4,
        "content": "This seems a lot like Azure Active Directory in both boxes for me. First of all, I am authenticating to the Databrick UI itself to create and run notebooks and not the REST API.  I would rather use the standard AAD to access Databricks and use the same AAD credentials to access ADLS Gen 2 to for the files.  Of course, I will be implementing ACL to restrict access tothe folders each user should be able to access only on AAD.\n\nRef:  https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2021-03-06T23:04:00.000Z",
        "voteCount": 1,
        "content": "This seems a lot like Azure Active Directory in both boxes for me. First of all, I am authenticating to the Databrick UI itself to create and run notebooks and not the REST API.  I would rather use the standard AAD to access Databricks and use the same AAD credentials to access ADLS Gen 2 to for the files.  Of course, I will be implementing ACL to restrict access tothe folders each user should be able to access only on AAD.\n\nRef:  https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2020-12-30T11:25:00.000Z",
        "voteCount": 13,
        "content": "All the answers above are wrong. The correct answers are:\n\n1.Databricks: Azure Active Directory\n2.Data Lake Storage: Azure Active Directory\n\n1. There is no mention of connecting with the Databricks API, instead the descriptions days that users will connect to ADLS using Interactive Notebooks. For that they will have to log in to Databricks itself, which will be done with their AD accounts.\n\n2. Shared access signature or shared access keys do not use ACLs, but RBAC, and are applied on container or storage account level, NOT on directory or file level. I quote from https://docs.microsoft.com/nl-nl/azure/storage/blobs/data-lake-storage-access-control\n\n\n\"ACLs apply only to security principals in the same tenant, and they don't apply to users who use Shared Key or shared access signature (SAS) token authentication. That's because no identity is associated with the caller and therefore security principal permission-based authorization cannot be performed.\""
      },
      {
        "date": "2021-01-23T14:13:00.000Z",
        "voteCount": 5,
        "content": "1.Databricks: Azure Active Directory (minimize administrative effort)\n2.Data Lake Storage: Azure Active Directory"
      },
      {
        "date": "2020-12-07T04:18:00.000Z",
        "voteCount": 1,
        "content": "I would say the answer is correct\nhttps://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/authentication:\n\"To authenticate to and access Databricks REST APIs, you can use Azure Databricks personal access tokens or Azure Active Directory (Azure AD) tokens.\"\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control:\n\"Always use Azure AD security groups as the assigned principal in an ACL entry\""
      },
      {
        "date": "2020-12-30T11:00:00.000Z",
        "voteCount": 3,
        "content": "I see no mention of an API, it's logging in to Databricks and then querying ADLS."
      },
      {
        "date": "2020-10-24T10:23:00.000Z",
        "voteCount": 7,
        "content": "Where in the question does it talk about accessing the REST API? Personal access tokens are used to access the Databricks REST API. For interactive notebooks, AAD is the way to authenticate the users!"
      },
      {
        "date": "2020-12-30T10:59:00.000Z",
        "voteCount": 1,
        "content": "Indeed."
      },
      {
        "date": "2020-08-05T14:13:00.000Z",
        "voteCount": 4,
        "content": "Databricks - Azure Key Vault https://docs.microsoft.com/en-us/azure/databricks/security/secrets/example-secret-workflow\n\nADLS Gen 2 - AAD"
      },
      {
        "date": "2020-05-10T21:24:00.000Z",
        "voteCount": 6,
        "content": "AAD cant do folder level permissions in ADLS, it needs ACL to do that."
      },
      {
        "date": "2020-05-31T07:23:00.000Z",
        "voteCount": 6,
        "content": "Re: ADLS, the question concerns authentication only not authorisation, so AAD for authentication and then RBAC roles for authorisation."
      },
      {
        "date": "2020-05-01T11:30:00.000Z",
        "voteCount": 3,
        "content": "It looks like a bad wording in the question. The requirements are not to secure the Notebook, but only the storage access, so What I do in those cases - define access using KeyVault (so user of that notebook won't see the credentials) and secure ADLS2 with Service Identity in AAD - that allows granular authorization and project scope."
      },
      {
        "date": "2020-04-14T07:05:00.000Z",
        "voteCount": 6,
        "content": "I think for ADLS Gen2, it should use SAS rather than AAD (RBAC). Shared Key is not quite suitable as it make user effectively gains 'super-user' access, meaning full access to all operations on all resources, including setting owner and changing ACLs."
      },
      {
        "date": "2020-04-27T23:43:00.000Z",
        "voteCount": 4,
        "content": "I concur, I also think that AAD should be the authentication method for databricks since personal access token is used to access databricks REST API instead of interactive notebook."
      },
      {
        "date": "2020-05-01T11:24:00.000Z",
        "voteCount": 4,
        "content": "Won't work - ADLS can only have account level SAS, and you need at least container wise"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19045-exam-dp-201-topic-3-question-29-discussion/",
    "body": "You store data in a data warehouse in Azure Synapse Analytics.<br>You need to design a solution to ensure that the data warehouse and the most current data is available within one hour of a datacenter failure.<br>Which three actions should you include in the design? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach day, restore the data warehouse from a geo-redundant backup to an available Azure region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf a failure occurs, update the connection strings to point to the recovered data warehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf a failure occurs, modify the Azure Firewall rules of the data warehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach day, create Azure Firewall rules that allow access to the restored data warehouse.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEach day, restore the data warehouse from a user-defined restore point to an available Azure region."
    ],
    "answer": "BDE",
    "answerDescription": "E: You can create a user-defined restore point and restore from the newly created restore point to a new data warehouse in a different region.<br>Note: A data warehouse snapshot creates a restore point you can leverage to recover or copy your data warehouse to a previous state.<br>A data warehouse restore is a new data warehouse that is created from a restore point of an existing or deleted data warehouse. On average within the same region, restore rates typically take around 20 minutes.<br>Incorrect Answers:<br>A: SQL Data Warehouse performs a geo-backup once per day to a paired data center. The RPO for a geo-restore is 24 hours. You can restore the geo-backup to a server in any other region where SQL Data Warehouse is supported. A geo-backup ensures you can restore data warehouse in case you cannot access the restore points in your primary region.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/backup-and-restore",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-10T21:39:00.000Z",
        "voteCount": 34,
        "content": "It can be BCE instead of BDE Why to create firewall rule each day if we have one hour time to restore after failure"
      },
      {
        "date": "2021-03-06T23:11:00.000Z",
        "voteCount": 2,
        "content": "Actually C and D are a bit confusing to me. probably they are badly worded or it is just me. i would actually be looking out for an option like \nD.  If a failure occurs, create Azure Firewall rules that allow access to the restored data warehouse.\nWhy should I always create a firewall rule to open up the restored database pre-emptively? I believe it is safer to only create the firewall in the event of failures.\n\nOtherwise, an option C that explicitly qualifies the datawarehouse as restored would still have sounded more correct to me than the two options available in the question\nC. If a failure occurs, modify the Azure Firewall rules of the restored data warehouse."
      },
      {
        "date": "2020-05-26T05:02:00.000Z",
        "voteCount": 15,
        "content": "C is a trick question to cause confusion, why would you update firewall rules on the existing data warehouse after it fails, existing being implied since they didn't mention restored. It's D because you're updating the restored database."
      },
      {
        "date": "2020-07-17T22:09:00.000Z",
        "voteCount": 11,
        "content": "In my opinion the correct answer is B,D,E. \nUpdate connection strings -&gt; \n\"Because your recovered database resides in a different server, you need to update your application\u2019s connection string to point to that server.\" \nConfigure firewall rules -&gt; \nYou need to make sure that the firewall rules configured on server and on the database match those that were configured on the primary server and primary database.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/disaster-recovery-guidance?toc=/azure/synapse-analytics/sql-data-warehouse/toc.json&amp;bc=/azure/synapse-analytics/sql-data-warehouse/breadcrumb/toc.json#configure-your-database-after-recovery"
      },
      {
        "date": "2020-12-05T23:30:00.000Z",
        "voteCount": 2,
        "content": "I agree with BDE as the answer based on the link you shared"
      },
      {
        "date": "2021-01-03T19:56:00.000Z",
        "voteCount": 2,
        "content": "Answer given is correct."
      },
      {
        "date": "2020-11-14T15:43:00.000Z",
        "voteCount": 4,
        "content": "Shouldn't it be A, B &amp; D?"
      },
      {
        "date": "2020-10-24T10:37:00.000Z",
        "voteCount": 3,
        "content": "Does ADW requires DAILY manual steps to provide 1 hour of RTO?? Realy??"
      },
      {
        "date": "2020-10-01T07:04:00.000Z",
        "voteCount": 2,
        "content": "what all the suggested answers are all manual operations?? And why user defined restore point is what the most current data we want to restore??"
      },
      {
        "date": "2020-05-01T05:17:00.000Z",
        "voteCount": 1,
        "content": "but why suggest E if you say that a user defined rollback point is hard to establish accurately?  I think A, then D in case there is an issue, then B to direct to working region."
      },
      {
        "date": "2020-04-25T01:37:00.000Z",
        "voteCount": 6,
        "content": "In my opinion the correct answer is A,D,E. Azure synapse takes a snapshot roughly every 4 hours and 1 geo-snapshot every day to a paired region . The question talks about datacenter failure, so unless you can predict when the failure will occur to take a user-defined restore point, you should rely on the automated geo-snapshot."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52291-exam-dp-201-topic-3-question-30-discussion/",
    "body": "HOTSPOT -<br>You are designing an Azure data factory that will copy data from Azure Blob storage to a data warehouse in Azure Synapse Analytics.<br>You need to recommend an authentication mechanism that meet the following requirements:<br>\u2711 Identities must be validated by using Azure Active Directory (Azure AD).<br>\u2711 Development and maintenance effort must be minimized.<br>Which authentication mechanism should you recommend for each service? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0028300003.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0028400001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-msi https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-10T07:28:00.000Z",
        "voteCount": 16,
        "content": "Both boxes should be Managed Identity:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/synapse-workspace-managed-identity"
      },
      {
        "date": "2021-05-28T18:01:00.000Z",
        "voteCount": 5,
        "content": "After carefully read the reference and the requirement(Development and maintenance effort must be minimized) in the question.\nBoth boxes should be Managed Identity. Here is the reason:\nBy using Managed Identity, it takes two steps\nBy using service principal, it needs three steps\nRef. https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse"
      },
      {
        "date": "2021-11-18T07:09:00.000Z",
        "voteCount": 1,
        "content": "both Managed indentity, the moc describe that blob storage supported managed identity"
      },
      {
        "date": "2021-05-20T18:32:00.000Z",
        "voteCount": 2,
        "content": "Azure data factory can use Managed Identity and service principal both as authentication.\nquestion here is :You are designing an Azure data factory that will copy data from Azure Blob storage to a data warehouse in Azure Synapse Analytics.\nAnswer here is correct.\nIf some one pick up Managed Identity on the second box, it should be correct answer too"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54679-exam-dp-201-topic-3-question-31-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account named adls2 that is protected by a virtual network.<br>You are designing a SQL pool in Azure Synapse that will use adls2 as a source.<br>What should you use to authenticate to adls2?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed identity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Active Directory (Azure AD) user"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://medium.com/@nadakkannu.smart/ingest-data-from-an-azure-data-lake-gen-2-into-a-sql-pool-using-azure-synapse-analytics-434517321c61",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-05T19:18:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct.\nManaged identity for Azure resources is a feature of Azure Active Directory. The feature provides Azure services with an automatically managed identity in Azure AD. You can use the Managed Identity capability to authenticate to any service that support Azure AD authentication."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51257-exam-dp-201-topic-3-question-32-discussion/",
    "body": "You plan to create an Azure Synapse Analytics dedicated SQL pool.<br>You need to minimize the time it takes to identify queries that return confidential information as defined by the company's data privacy regulations and the users who executed the queries.<br>Which two components should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking for columns that contain confidential information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsensitivity-classification labels applied to columns that contain confidential information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tresource tags for databases that contain confidential information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taudit logs sent to a Log Analytics workspace"
    ],
    "answer": "AB",
    "answerDescription": "Reference:<br>https://www.sqlshack.com/understanding-azure-synapse-analytics-formerly-sql-dw/<br>Design for high availability and disaster recovery",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-30T07:15:00.000Z",
        "voteCount": 43,
        "content": "The solution needs to identify the users who executed queries, not to hide confidental information.\nI guess that B,D is the answer."
      },
      {
        "date": "2021-11-11T08:20:00.000Z",
        "voteCount": 1,
        "content": "For me B - D is the answer correct"
      },
      {
        "date": "2021-06-09T12:49:00.000Z",
        "voteCount": 3,
        "content": "B and D"
      },
      {
        "date": "2021-06-05T19:21:00.000Z",
        "voteCount": 2,
        "content": "BD is the correct answer , when you need to minimize the time it takes to identify queries that return confidential information as defined by the company data privacy regulations and the users who executed the queries.\nA (dynamic data masking) is usable to mask data."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "3"
  }
]