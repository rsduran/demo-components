[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/36865-exam-dp-201-topic-1-question-1-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an HDInsight/Hadoop cluster solution that uses Azure Data Lake Gen1 Storage.<br>The solution requires POSIX permissions and enables diagnostics logging for auditing.<br>You need to recommend solutions that optimize storage.<br>Proposed Solution: Ensure that files stored are larger than 250MB.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater. If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.<br>Note: POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1. Avoiding small file sizes can have multiple benefits, such as:<br>\u2711 Lowering the authentication checks across multiple files<br>\u2711 Reduced open file connections<br>\u2711 Faster copying/replication<br>\u2711 Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-12T08:56:00.000Z",
        "voteCount": 17,
        "content": "POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1.\naccording to this docs resource, I think the given answer is correct"
      },
      {
        "date": "2021-06-23T23:12:00.000Z",
        "voteCount": 5,
        "content": "We can ignore questions where we see GEN1 as it is out of scope now."
      },
      {
        "date": "2021-06-22T06:34:00.000Z",
        "voteCount": 1,
        "content": "File size is accepted within 256MB to 2GB"
      },
      {
        "date": "2021-05-18T20:00:00.000Z",
        "voteCount": 2,
        "content": "Referencing the provided link the minimum acceptable file size is 256MB whereas the propose solution started at 250MB. I would say the answer is 'NO'\n\nReference: https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices"
      },
      {
        "date": "2021-06-13T08:59:00.000Z",
        "voteCount": 1,
        "content": "That makes not really sense for the this question"
      },
      {
        "date": "2021-06-13T20:37:00.000Z",
        "voteCount": 1,
        "content": "250 MB vs 256 MB gives less than 3% waste in worst-case. So it is acceptable. Answer should be YES"
      },
      {
        "date": "2021-04-21T23:45:00.000Z",
        "voteCount": 1,
        "content": "So is this a trap question? as the guidance is 256MB and they are saying larger than 250MB... a small difference but below we recommended size"
      },
      {
        "date": "2021-02-20T22:43:00.000Z",
        "voteCount": 2,
        "content": "The given solution is correct \nTypically, analytics engines such as HDInsight and Azure Data Lake Analytics have a per-file overhead. If you store your data as many small files, this can negatively affect performance.\npls refer this link https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance#structure-your-data-set \nIn general, organize your data into larger sized files for better performance. As a rule of thumb, organize data sets in files of 256 MB or larger"
      },
      {
        "date": "2020-12-19T00:03:00.000Z",
        "voteCount": 1,
        "content": "Given answer B. No is correct.\n\nIn POSIX-style model it is recommended to avoid small size files, due to following considerations:\n-Lowering the authentication checks across multiple files\n-Reduced open file connections\n-Faster copying/replication\n-Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions"
      },
      {
        "date": "2020-12-06T13:51:00.000Z",
        "voteCount": 1,
        "content": "Is it correct answer?"
      },
      {
        "date": "2020-12-05T04:11:00.000Z",
        "voteCount": 2,
        "content": "Provided link says at least 265 MB but greater than 250 MB seems good enough.\nI would agree with the answer"
      },
      {
        "date": "2020-11-27T09:53:00.000Z",
        "voteCount": 1,
        "content": "Not really, it's a trap. Files should be grater than 256 mb regarding to best practises. So bigger file thant 250 like 251 it's not a solution."
      },
      {
        "date": "2020-11-22T01:02:00.000Z",
        "voteCount": 2,
        "content": "Agree  with @Piiri565"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40084-exam-dp-201-topic-1-question-2-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an HDInsight/Hadoop cluster solution that uses Azure Data Lake Gen1 Storage.<br>The solution requires POSIX permissions and enables diagnostics logging for auditing.<br>You need to recommend solutions that optimize storage.<br>Proposed Solution: Implement compaction jobs to combine small files into larger files.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Depending on what services and workloads are using the data, a good size to consider for files is 256 MB or greater. If the file sizes cannot be batched when landing in Data Lake Storage Gen1, you can have a separate compaction job that combines these files into larger ones.<br>Note: POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1. Avoiding small file sizes can have multiple benefits, such as:<br>\u2711 Lowering the authentication checks across multiple files<br>\u2711 Reduced open file connections<br>\u2711 Faster copying/replication<br>\u2711 Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-16T12:09:00.000Z",
        "voteCount": 7,
        "content": "Correct answer"
      },
      {
        "date": "2021-02-20T22:46:00.000Z",
        "voteCount": 2,
        "content": "Somewhat similar to above qn \nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-performance-tuning-guidance#structure-your-data-set"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40334-exam-dp-201-topic-1-question-3-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an HDInsight/Hadoop cluster solution that uses Azure Data Lake Gen1 Storage.<br>The solution requires POSIX permissions and enables diagnostics logging for auditing.<br>You need to recommend solutions that optimize storage.<br>Proposed Solution: Ensure that files stored are smaller than 250MB.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Ensure that files stored are larger, not smaller than 250MB.<br>You can have a separate compaction job that combines these files into larger ones.<br>Note: The file POSIX permissions and auditing in Data Lake Storage Gen1 comes with an overhead that becomes apparent when working with numerous small files. As a best practice, you must batch your data into larger files versus writing thousands or millions of small files to Data Lake Storage Gen1. Avoiding small file sizes can have multiple benefits, such as:<br>\u2711 Lowering the authentication checks across multiple files<br>\u2711 Reduced open file connections<br>\u2711 Faster copying/replication<br>\u2711 Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T00:04:00.000Z",
        "voteCount": 8,
        "content": "Given answer B. No is correct.\n\nIn POSIX-style model it is recommended to avoid small size files, due to following considerations:\n-Lowering the authentication checks across multiple files\n-Reduced open file connections\n-Faster copying/replication\n-Fewer files to process when updating Data Lake Storage Gen1 POSIX permissions\nIt is recommended that size of files are at least 256 MB.\n\nSource: https://docs.microsoft.com/pl-pl/azure/data-lake-store/data-lake-store-best-practices"
      },
      {
        "date": "2021-02-16T22:42:00.000Z",
        "voteCount": 2,
        "content": "Please check this in the mentioned link \nhttps://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-best-practices\nIt's under \"Improve throughput with parallelism\"\nAvoid Small file sizes. in this heading you need to look out for the below line \n\"you can have a separate compaction job that combines these files into larger ones.\" which states the given stmt is false"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23160-exam-dp-201-topic-1-question-4-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure SQL Database that will use elastic pools. You plan to store data about customers in a table. Each record uses a value for<br>CustomerID.<br>You need to recommend a strategy to partition data based on values in CustomerID.<br>Proposed Solution: Separate data into customer regions by using vertical partitioning.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Vertical partitioning is used for cross-database queries. Instead we should use Horizontal Partitioning, which also is called charding.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-14T16:11:00.000Z",
        "voteCount": 32,
        "content": "Sharding, not charding haha"
      },
      {
        "date": "2020-08-09T01:41:00.000Z",
        "voteCount": 27,
        "content": "Customer scenarios for elastic query are characterized by the following topologies:\n\n\t\u2022 Vertical partitioning - Cross-database queries (Topology 1): The data is partitioned vertically between a number of databases in a data tier. Typically, different sets of tables reside on different databases. That means that the schema is different on different databases. For instance, all tables for inventory are on one database while all accounting-related tables are on a second database. Common use cases with this topology require one to query across or to compile reports across tables in several databases.\n\nHorizontal Partitioning - Sharding (Topology 2): Data is partitioned horizontally to distribute rows across a scaled out data tier. With this approach, the schema is identical on all participating databases. This approach is also called \u201csharding\u201d. Sharding can be performed and managed using (1) the elastic database tools libraries or (2) self-sharding. An elastic query is used to query or compile reports across many shards. Shards are typically databases within an elastic pool. You can think of elastic query as an efficient way for querying all databases of elastic pool at once, as long as databases share the common schema."
      },
      {
        "date": "2021-06-30T22:21:00.000Z",
        "voteCount": 1,
        "content": "'Avoid creating \"hot\" partitions that can affect performance and availability. For example, using the first letter of a customer's name causes an unbalanced distribution, because some letters are more common. Instead, use a hash of a customer identifier to distribute data more evenly across partitions.' - From MS documentation for Horizontal Partitioning."
      },
      {
        "date": "2021-02-16T23:11:00.000Z",
        "voteCount": 1,
        "content": "Here we're storing Customers data in a table and now we want to partition cust region so we need to use sharding as per the right concept as they are performed as long as DBs share common schema as per defn."
      },
      {
        "date": "2021-01-24T21:57:00.000Z",
        "voteCount": 1,
        "content": "Answer : No\nApplicable solution : Horizontal partitioning\nReference : https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning"
      },
      {
        "date": "2021-01-15T09:54:00.000Z",
        "voteCount": 1,
        "content": "Ok i am confused as to the difference between question 4 and question 5 on this site.  Question 4 says to use horizontal partitioning but Question 5 says it recommends to use horizontal partition and the wording is the same but they say that answer should be No still  why?"
      },
      {
        "date": "2020-09-23T12:01:00.000Z",
        "voteCount": 1,
        "content": "Still don't know why  horizontal and not vertical !"
      },
      {
        "date": "2020-09-27T18:45:00.000Z",
        "voteCount": 3,
        "content": "it is because it is to partition customers IDs, so it means it is just 1 database."
      },
      {
        "date": "2020-12-19T00:13:00.000Z",
        "voteCount": 1,
        "content": "Vertical partitioning is to reduce the I/O and performance costs associated with fetching items that are frequently accessed. Vertical partitioning splits table and in the result we have more partitions with different schema instead of 1 big table. This is not what is expected in this scenario.\n\nHorizontal partitioning using sharding is expected. Horizontal sharding will split table row-wise, so we have multiple partitions with the same schema, but based on region in that case.\n\nFor instance split table containing all customers world-wise into multiple partitions based on the regions (customer from Europe, customers from USA etc)"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29334-exam-dp-201-topic-1-question-5-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure SQL Database that will use elastic pools. You plan to store data about customers in a table. Each record uses a value for<br>CustomerID.<br>You need to recommend a strategy to partition data based on values in CustomerID.<br>Proposed Solution: Separate data into customer regions by using horizontal partitioning.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We should use Horizontal Partitioning through Sharding, not divide through regions.<br>Note: Horizontal Partitioning - Sharding: Data is partitioned horizontally to distribute rows across a scaled out data tier. With this approach, the schema is identical on all participating databases. This approach is also called \u05d2\u20acsharding\u05d2\u20ac. Sharding can be performed and managed using (1) the elastic database tools libraries or<br>(2) self-sharding. An elastic query is used to query or compile reports across many shards.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-15T02:34:00.000Z",
        "voteCount": 1,
        "content": "Correct answer  is\nNo\nbecuase \nSeparate data into shards by using horizontal partitioning"
      },
      {
        "date": "2021-03-02T19:46:00.000Z",
        "voteCount": 1,
        "content": "harding can be performed and managed using (1) the elastic database tools libraries or (2) self-sharding. An elastic query is used to query or compile reports across many shards. Shards are typically databases within an elastic pool. You can think of elastic query as an efficient way for querying all databases of elastic pool at once, as long as databases share the common schema."
      },
      {
        "date": "2021-01-15T09:55:00.000Z",
        "voteCount": 2,
        "content": "Ok i am confused as to the difference between question 4 and question 5 on this site. Question 4 says to use horizontal partitioning but Question 5 says it recommends to use horizontal partition and the wording is the same but they say that answer should be No still why?"
      },
      {
        "date": "2021-01-24T21:59:00.000Z",
        "voteCount": 6,
        "content": "Answer : No\nApplicable solution : Horizontal partitioning (based on customerID not region i.e. using sharding concept)\nReference : https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning"
      },
      {
        "date": "2020-08-23T00:41:00.000Z",
        "voteCount": 2,
        "content": "I don't understand why is not recommend horizontal ... Each shard could be the region, no?"
      },
      {
        "date": "2020-08-23T22:39:00.000Z",
        "voteCount": 9,
        "content": "Horizontal partitioning splits one or more tables by row, usually within a single instance of a schema and a database server.\n\nSharding goes beyond this: it partitions the problematic table(s) in the same way, but it does this across potentially multiple instances of the schema. The obvious advantage would be that search load for the large partitioned table can now be split across multiple servers (logical or physical), not just multiple indexes on the same logical server. \n\nRef: https://en.wikipedia.org/wiki/Shard_(database_architecture)#Shards_compared_to_horizontal_partitioning"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39225-exam-dp-201-topic-1-question-6-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure SQL Database that will use elastic pools. You plan to store data about customers in a table. Each record uses a value for<br>CustomerID.<br>You need to recommend a strategy to partition data based on values in CustomerID.<br>Proposed Solution: Separate data into shards by using horizontal partitioning.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Horizontal Partitioning - Sharding: Data is partitioned horizontally to distribute rows across a scaled out data tier. With this approach, the schema is identical on all participating databases. This approach is also called \u05d2\u20acsharding\u05d2\u20ac. Sharding can be performed and managed using (1) the elastic database tools libraries or (2) self- sharding. An elastic query is used to query or compile reports across many shards.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-elastic-query-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-01T03:23:00.000Z",
        "voteCount": 6,
        "content": "Yes is the answer"
      },
      {
        "date": "2021-06-22T09:30:00.000Z",
        "voteCount": 3,
        "content": "No is the answer\nThis solution does not meet the requirements. You need to use sharding, which is partitioning data horizontally to distribute data across multiple databases in a scaled-out design, but CustomerID is not the best choice in this scenario. Sharding by RegionalID will make sorting by geographic location more efficient. \n\nSharding requires that the schema is the same on all of the databases involved. Sharding helps to minimize the size of individual databases, which in turn helps to improve transactional process performance. Hardware support requirements are minimized, which helps to reduce related costs. Elastic queries let you run queries across multiple shards. You can configure and manage sharding through the elastic database tools libraries or through self-sharding"
      },
      {
        "date": "2020-12-08T06:33:00.000Z",
        "voteCount": 4,
        "content": "This is the correct solution"
      },
      {
        "date": "2021-04-12T05:22:00.000Z",
        "voteCount": 2,
        "content": "I disagree. CustomerID will be unique and that means you would have as many shards as you have customers. This would be a poor design. The question is poorly worded and given the wording the answer might be correct, but it is lousy design."
      },
      {
        "date": "2021-05-15T09:22:00.000Z",
        "voteCount": 1,
        "content": "I agree sharding based on region would be a better fit"
      },
      {
        "date": "2021-06-01T02:53:00.000Z",
        "voteCount": 1,
        "content": "this would only be correct if compound shard is created for customerid and region"
      },
      {
        "date": "2021-05-22T12:32:00.000Z",
        "voteCount": 3,
        "content": "Well, the more the shards, the lesser is the likelihood of you facing the hot partition problem. The region will create a hot partition problem."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17789-exam-dp-201-topic-1-question-7-discussion/",
    "body": "HOTSPOT -<br>You are designing a data processing solution that will run as a Spark job on an HDInsight cluster. The solution will be used to provide near real-time information about online ordering for a retailer.<br>The solution must include a page on the company intranet that displays summary information.<br>The summary information page must meet the following requirements:<br>\u2711 Display a summary of sales to date grouped by product categories, price range, and review scope.<br>\u2711 Display sales summary information including total sales, sales as compared to one day ago and sales as compared to one year ago.<br>\u2711 Reflect information for new orders as quickly as possible.<br>You need to recommend a design for the solution.<br>What should you recommend? To answer, select the appropriate configuration in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0005000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0005100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DataFrame -<br><br>DataFrames -<br>Best choice in most situations.<br>Provides query optimization through Catalyst.<br>Whole-stage code generation.<br>Direct memory access.<br>Low garbage collection (GC) overhead.<br>Not as developer-friendly as DataSets, as there are no compile-time checks or domain object programming.<br><br>Box 2: parquet -<br>The best format for performance is parquet with snappy compression, which is the default in Spark 2.x. Parquet stores data in columnar format, and is highly optimized in Spark.<br>Incorrect Answers:<br><br>DataSets -<br>Good in complex ETL pipelines where the performance impact is acceptable.<br>Not good in aggregations where the performance impact can be considerable.<br><br>RDDs -<br>You do not need to use RDDs, unless you need to build a new custom RDD.<br>No query optimization through Catalyst.<br>No whole-stage code generation.<br>High GC overhead.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-perf",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-02T00:11:00.000Z",
        "voteCount": 50,
        "content": "The highighted answer and the explanation differ. Should be dataframe I believe."
      },
      {
        "date": "2020-04-10T03:09:00.000Z",
        "voteCount": 22,
        "content": "I think it should be dataframe as well. In most cases parquet and dataframe are the best choice."
      },
      {
        "date": "2020-05-09T03:34:00.000Z",
        "voteCount": 1,
        "content": "They say Dataset is good for complex ETL situations\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-perf"
      },
      {
        "date": "2021-09-02T17:33:00.000Z",
        "voteCount": 1,
        "content": "1. Dataframe\n2. Parquet\nConfirmed"
      },
      {
        "date": "2021-08-12T06:16:00.000Z",
        "voteCount": 1,
        "content": "Anyone knows why Exam Topics have taken AWS certification questions offline? There is nothing related to AWS certifications which used to be there earlier."
      },
      {
        "date": "2021-11-30T05:37:00.000Z",
        "voteCount": 1,
        "content": "Hi, I found the link to the associate SA exam. \nhttps://www.examtopics.com/exams/amazon/aws-certified-solutions-architect-associate-saa-c02/view/"
      },
      {
        "date": "2021-08-11T09:56:00.000Z",
        "voteCount": 1,
        "content": "The practical combination is Dataframe + Parquet . Here answer clarification is ambiguous."
      },
      {
        "date": "2021-07-27T06:45:00.000Z",
        "voteCount": 1,
        "content": "1- DATFRAME\n2 - Data Format = Avro \nBecause only Avro support Streaming (Against Parquet)"
      },
      {
        "date": "2021-05-21T00:41:00.000Z",
        "voteCount": 5,
        "content": "Data abstraction = Dataframe"
      },
      {
        "date": "2021-05-18T16:30:00.000Z",
        "voteCount": 2,
        "content": "Dataframe is correct , \nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/optimize-data-storage"
      },
      {
        "date": "2021-02-17T01:32:00.000Z",
        "voteCount": 1,
        "content": "It's wrong selection shown in the display. It's actually \n- Data Frame [Reason for elimination Not as developer-friendly as DataSets, as there are no compile-time checks or domain object programming,don't need to use RDDs, unless you need to build a new custom RDD]\nAnyhow \"Parquet\" is selected"
      },
      {
        "date": "2020-12-09T00:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/hdinsight/spark/optimize-data-storage:\n\"Parquet stores data in columnar format, and is highly optimized in Spark.\"\n\"DataFrames\nBest choice in most situations.\""
      },
      {
        "date": "2020-11-22T01:00:00.000Z",
        "voteCount": 3,
        "content": "Dataset is not good for Aggregation, Should be dataframe."
      },
      {
        "date": "2020-11-08T05:35:00.000Z",
        "voteCount": 1,
        "content": "Can some correct the answers??"
      },
      {
        "date": "2020-06-24T18:01:00.000Z",
        "voteCount": 11,
        "content": "The question need quick processing but Dataset add overhead, also the query is aggregation and Dataset not good at that\n\nDataSets : Adds serialization/deserialization overhead, High GC overhead, Not good in aggregations where the performance impact can be considerable.\n\nDataFrames : Best choice in most situations, Direct memory access."
      },
      {
        "date": "2020-06-10T02:18:00.000Z",
        "voteCount": 5,
        "content": "Data set is Not good in aggregations where the performance impact can be considerable.So. I think dataframe should be correct one. Can anyone confirm. Please Thanks."
      },
      {
        "date": "2020-05-27T00:19:00.000Z",
        "voteCount": 4,
        "content": "dataframe for sure"
      },
      {
        "date": "2020-04-23T08:47:00.000Z",
        "voteCount": 7,
        "content": "I think it's dataframe too."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/35326-exam-dp-201-topic-1-question-8-discussion/",
    "body": "You are evaluating data storage solutions to support a new application.<br>You need to recommend a data storage solution that represents data by using nodes and relationships in graph structures.<br>Which data storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlob Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight"
    ],
    "answer": "B",
    "answerDescription": "For large graphs with lots of entities and relationships, you can perform very complex analyses very quickly. Many graph databases provide a query language that you can use to traverse a network of relationships efficiently.<br>Relevant Azure service: Cosmos DB<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/guide/technology-choices/data-store-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-21T00:42:00.000Z",
        "voteCount": 7,
        "content": "CosmosDB with Gremlin API"
      },
      {
        "date": "2021-05-19T21:32:00.000Z",
        "voteCount": 4,
        "content": "the proposed solution is correct as the azure cosmos db has gremlin api that can support the graph requirement.\n\nreference: https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction"
      },
      {
        "date": "2021-05-16T01:53:00.000Z",
        "voteCount": 1,
        "content": "No other option supports Graph Structures. So it should be only Azure Cosmos DB."
      },
      {
        "date": "2021-05-09T18:15:00.000Z",
        "voteCount": 1,
        "content": "101% correct"
      },
      {
        "date": "2020-12-05T04:31:00.000Z",
        "voteCount": 2,
        "content": "It can only be B"
      },
      {
        "date": "2020-11-22T00:59:00.000Z",
        "voteCount": 2,
        "content": "yes agree with Cosmos DB"
      },
      {
        "date": "2020-11-05T22:11:00.000Z",
        "voteCount": 3,
        "content": "Gremlin - API"
      },
      {
        "date": "2020-10-27T16:31:00.000Z",
        "voteCount": 4,
        "content": "I agree with the answer - Cosmos DB."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46530-exam-dp-201-topic-1-question-9-discussion/",
    "body": "HOTSPOT -<br>You have an on-premises data warehouse that includes the following fact tables. Both tables have the following columns: DataKey, ProductKey, RegionKey.<br>There are 120 unique product keys and 65 unique region keys.<br><img src=\"/assets/media/exam-media/03774/0005300001.png\" class=\"in-exam-image\"><br>Queries that use the data warehouse take a long time to complete.<br>You plan to migrate the solution to use Azure Synapse Analytics. You need to ensure that the Azure-based solution optimizes query performance and minimizes processing skew.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0005400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0005500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash-distributed -<br><br>Box 2: ProductKey -<br>ProductKey is used extensively in joins.<br>Hash-distributed tables improve query performance on large fact tables.<br><br>Box 3: Round-robin -<br><br>Box 4: RegionKey -<br>Round-robin tables are useful for improving loading speed.<br>Consider using the round-robin distribution for your table in the following scenarios:<br>\u2711 When getting started as a simple starting point since it is the default<br>\u2711 If there is no obvious joining key<br>\u2711 If there is not good candidate column for hash distributing the table<br>\u2711 If the table does not share a common join key with other tables<br>\u2711 If the join is less significant than other joins in the query<br>\u2711 When the table is a temporary staging table<br>Note: A distributed table appears as a single table, but the rows are actually stored across 60 distributions. The rows are distributed with a hash or round-robin algorithm.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-13T16:09:00.000Z",
        "voteCount": 31,
        "content": "Table sales:\n**Distribution type: Hash-Distributed \nFor 2 Reasons: the table is 600GB and we want to optimize queries\n**Distribution column: \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\n=&gt; Product key is the only possible correct choice\nTable Invoices:\n**Distribution type: Hash-Distributed\nThe table size is more than 2GB and probably growing up.\nConsider using a hash-distributed table when:The table size on disk is more than 2 GB. And  The table has frequent insert, update, and delete operations.\n**Distribution column: for sure it\u2019s regionkey To minimize data movement, select a distribution column that:\u2026 same link"
      },
      {
        "date": "2021-05-19T21:58:00.000Z",
        "voteCount": 9,
        "content": "Distribution for both should be \"hash-distributed\" as we are talking about fact tables while round-robin is mostly use in staging tables. As a rule of the thumb when using hash-distributed it should be applied in the columns that uses JOIN, GROUP BY, DISTINCT, OVER, and HAVING and one shouldn't apply it in WHERE and DATE columns.\n\nSales: Hash-distributed, ProductKey\nInvoices: Hash-distributed, RegiongKey\n\nReference: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2021-06-10T13:09:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is: Sales : Hash-Distributed&gt;&gt;&gt;&gt; Product Key\n                                       Invoices:   Hash-Distributed&gt;&gt;&gt;&gt; Region Key"
      },
      {
        "date": "2021-05-24T00:32:00.000Z",
        "voteCount": 2,
        "content": "i think Distribution should be : \"HASH-DISTRIBUTION\" as both are Fact tables and ProductKey for sales and Region Key for Invoices ."
      },
      {
        "date": "2021-05-19T00:20:00.000Z",
        "voteCount": 2,
        "content": "I don't think there is a distribution column option for Round Robin. The distribution column is available only for Hash Partitioning. So it must be Hash Partitioning &amp; Region Key for Invoice table."
      },
      {
        "date": "2021-05-12T10:44:00.000Z",
        "voteCount": 2,
        "content": "As there are some different answers for table invoices.\nFor sure hash-distributed, as the table size is more than 2 GB.\nExplanation for RegionKey:\nTo minimize data movement, select a distribution column that:\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses. When two large fact tables have frequent joins, query performance improves when you distribute both tables on one of the join columns. When a table is not used in joins, consider distributing the table on a column that is frequently in the GROUP BY clause.\nIs not used in WHERE clauses. This could narrow the query to not run on all the distributions.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2021-03-13T06:37:00.000Z",
        "voteCount": 4,
        "content": "I think that the right answer for the ditribution type at the invoice table would be hash-distributed  with regionkey as the distributed key as it is used for grouping."
      },
      {
        "date": "2021-03-12T13:24:00.000Z",
        "voteCount": 5,
        "content": "I would say Hash distributed and Date key for both tables because date key is used extensively in queries in both tables, region key will result in skewed partitioning as 75% of data falls in one region. Also Hash is best for both because we are optimizing query performance and not loading which Round-Robin is best suited for"
      },
      {
        "date": "2021-04-12T05:49:00.000Z",
        "voteCount": 4,
        "content": "From the provided link we learn that generally we should not use date values as the partitioning key. As noted by H_S the Invoices table is large enough to warrant being hash distributed as well and as noted by you, Mariekumi, RegionKey would result in hot spots/skew. I think hash distributed on product key for both tables makes the most sense."
      },
      {
        "date": "2021-03-11T11:40:00.000Z",
        "voteCount": 1,
        "content": "why round robin for invoices."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17760-exam-dp-201-topic-1-question-10-discussion/",
    "body": "You are designing a data processing solution that will implement the lambda architecture pattern. The solution will use Spark running on HDInsight for data processing.<br>You need to recommend a data storage technology for the solution.<br>Which two technologies should you recommend? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Service Bus",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Storage Queue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Cassandra",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKafka HDInsight"
    ],
    "answer": "AE",
    "answerDescription": "To implement a lambda architecture on Azure, you can combine the following technologies to accelerate real-time big data analytics:<br>\u2711 Azure Cosmos DB, the industry's first globally distributed, multi-model database service.<br>\u2711 Apache Spark for Azure HDInsight, a processing framework that runs large-scale data analytics applications<br>Azure Cosmos DB change feed, which streams new data to the batch layer for HDInsight to process<br><img src=\"/assets/media/exam-media/03774/0005600009.png\" class=\"in-exam-image\"><br>\u2711 The Spark to Azure Cosmos DB Connector<br>E: You can use Apache Spark to stream data into or out of Apache Kafka on HDInsight using DStreams.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-20T21:55:00.000Z",
        "voteCount": 26,
        "content": "for batch processing - cosmos DB ,\nfor Stream processing - Kafka HDinsight"
      },
      {
        "date": "2020-04-01T06:32:00.000Z",
        "voteCount": 11,
        "content": "Is Kafka considered a data storage solution? I thought it was a streaming technology."
      },
      {
        "date": "2020-05-18T07:28:00.000Z",
        "voteCount": 2,
        "content": "https://www.confluent.io/blog/okay-store-data-apache-kafka/ [ it states something like this - \"It is much closer in architecture to a distributed filesystem or database then to traditional message queue.\" ]"
      },
      {
        "date": "2021-05-03T13:05:00.000Z",
        "voteCount": 1,
        "content": "Question here is :You need to recommend a data storage technology for the solution.\nAnswer: cosmos DB and Blob blob. Yet Azure Kafka is for stream processing"
      },
      {
        "date": "2021-05-01T03:25:00.000Z",
        "voteCount": 2,
        "content": "for batch: Cosmos DB\nfor stream: Kafka HD insight"
      },
      {
        "date": "2021-04-28T12:22:00.000Z",
        "voteCount": 3,
        "content": "A. Azure Cosmos DB\nD. Apache Cassandra"
      },
      {
        "date": "2021-02-17T08:39:00.000Z",
        "voteCount": 3,
        "content": "Given solution is right &amp; pls go through this link \nhttps://www.bluegranite.com/blog/exploring-the-lambda-architecture-in-azure \nKafka hdsight is for ingestion \nCosmos DB for processing"
      },
      {
        "date": "2020-12-08T06:42:00.000Z",
        "voteCount": 1,
        "content": "https://www.bluegranite.com/blog/exploring-the-lambda-architecture-in-azure\nKafka for ingestion\nAs for processing, Cosmos DB would be it"
      },
      {
        "date": "2020-04-23T08:57:00.000Z",
        "voteCount": 8,
        "content": "Lambda architecture is usually built with Cassandra as a storage solution and Kafka as a Data stream technology, so Cosmos DB is the correct answer. There is no such thing as Apache Cassandra."
      },
      {
        "date": "2020-12-19T01:03:00.000Z",
        "voteCount": 2,
        "content": "What do you mean? There is Apache Cassandra - a distributed, wide column storage on Apache license. \n\nHowever, Cosmos DB &amp; HDI Kafka are the answers for this question."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/6406-exam-dp-201-topic-1-question-11-discussion/",
    "body": "A company manufactures automobile parts. The company installs IoT sensors on manufacturing machinery.<br>You must design a solution that analyzes data from the sensors.<br>You need to recommend a solution that meets the following requirements:<br>\u2711 Data must be analyzed in real-time.<br>\u2711 Data queries must be deployed using continuous integration.<br>\u2711 Data must be visualized by using charts and graphs.<br>\u2711 Data must be available for ETL operations in the future.<br>\u2711 The solution must support high-volume data ingestion.<br>Which three actions should you recommend? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Azure Analysis Services to query the data. Output query results to Power BI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure an Azure Event Hub to capture data to Azure Data Lake Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an Azure Stream Analytics application that queries the data and outputs to Power BI. Use Azure Data Factory to deploy the Azure Stream Analytics application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an application that sends the IoT data to an Azure Event Hub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an Azure Stream Analytics application that queries the data and outputs to Power BI. Use Azure Pipelines to deploy the Azure Stream Analytics application.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an application that sends the IoT data to an Azure Data Lake Storage container."
    ],
    "answer": "BCD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-10-10T05:28:00.000Z",
        "voteCount": 120,
        "content": "Reading\"Data queries must be deployed using continuous integration\", i think than correct answer it\u00b4s BDE and not BCD."
      },
      {
        "date": "2020-09-19T16:34:00.000Z",
        "voteCount": 14,
        "content": "Pipelines are subset activity  in Azure factory, so C is correct which makes the answer BCD correct."
      },
      {
        "date": "2021-06-03T11:18:00.000Z",
        "voteCount": 1,
        "content": "ADF pipelines are used for ETL jobs and all, not for CI/CD, for that we need to use pipelines in azure devops"
      },
      {
        "date": "2022-05-16T12:24:00.000Z",
        "voteCount": 1,
        "content": "I believe it is BDE"
      },
      {
        "date": "2021-05-21T00:46:00.000Z",
        "voteCount": 1,
        "content": "CI = Azure Pipeline!"
      },
      {
        "date": "2021-05-20T01:56:00.000Z",
        "voteCount": 2,
        "content": "B,D,E are correct\n\nAzure pipeline is related to CI/CD process and one should differentiate the \"pipeline\" of Azure Data Factory. Also the azure stream analytics can output the data into PowerBI dataset. Hence, the Azure Analysis Service is not needed in the solution."
      },
      {
        "date": "2021-04-28T12:25:00.000Z",
        "voteCount": 4,
        "content": "BDE   is the correct answer."
      },
      {
        "date": "2021-04-09T10:41:00.000Z",
        "voteCount": 2,
        "content": "Answer will be BDE"
      },
      {
        "date": "2020-12-08T06:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment:\n\"There are two suggested methods to promote a data factory to another environment:\n\nAutomated deployment using Data Factory's integration with Azure Pipelines\"\nBDE it is"
      },
      {
        "date": "2020-11-04T03:30:00.000Z",
        "voteCount": 2,
        "content": "BDE is the correct answer."
      },
      {
        "date": "2020-08-20T10:59:00.000Z",
        "voteCount": 3,
        "content": "The correct answer should be BDE\nAutomate continuous integration by using Azure Pipelines releases"
      },
      {
        "date": "2020-08-07T00:30:00.000Z",
        "voteCount": 1,
        "content": "In the same time they said :Data must be available for ETL operations in the future.\nSo the response is OK"
      },
      {
        "date": "2020-07-15T08:54:00.000Z",
        "voteCount": 2,
        "content": "Tutorial: Deploy an Azure Stream Analytics job with CI/CD using Azure Pipelines https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-visual-studio-cicd-vsts"
      },
      {
        "date": "2020-07-15T08:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/continuous-integration-deployment"
      },
      {
        "date": "2020-06-12T06:11:00.000Z",
        "voteCount": 3,
        "content": "Continuous integration and deployment using Azure Data Factory | Azure Friday\n\nhttps://www.youtube.com/watch?v=WhUAX8YxxLk"
      },
      {
        "date": "2020-07-05T03:58:00.000Z",
        "voteCount": 2,
        "content": "Right bob"
      },
      {
        "date": "2020-06-10T02:52:00.000Z",
        "voteCount": 3,
        "content": "I believe BDE is the right onec.\nhttps://azure.microsoft.com/en-us/blog/refreshing-reference-data-with-azure-data-factory-for-azure-stream-analytics-jobs-3/"
      },
      {
        "date": "2020-05-28T05:25:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer is ABD\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard"
      },
      {
        "date": "2020-04-07T00:57:00.000Z",
        "voteCount": 3,
        "content": "Should be E instead of C.\nData queries must be deployed using continuous integration.\nAny thoughts?"
      },
      {
        "date": "2020-03-17T21:58:00.000Z",
        "voteCount": 7,
        "content": "Answer - BDE"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24237-exam-dp-201-topic-1-question-12-discussion/",
    "body": "You are designing an Azure Databricks interactive cluster.<br>You need to ensure that the cluster meets the following requirements:<br>\u2711 Enable auto-termination<br>\u2711 Retain cluster configuration indefinitely after cluster termination.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStart the cluster after it is terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPin the cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone the cluster after it is terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerminate the cluster manually at process completion."
    ],
    "answer": "B",
    "answerDescription": "To keep an interactive cluster configuration even after it has been terminated for more than 30 days, an administrator can pin a cluster to the cluster list.<br>Reference:<br>https://docs.azuredatabricks.net/user-guide/clusters/terminate.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-20T11:03:00.000Z",
        "voteCount": 24,
        "content": "Pin a cluster\n30 days after a cluster is terminated, it is permanently deleted. To keep an interactive cluster configuration even after a cluster has been terminated for more than 30 days, an administrator can pin the cluster. Up to 20 clusters can be pinned."
      },
      {
        "date": "2020-06-29T13:40:00.000Z",
        "voteCount": 10,
        "content": "You're trying to keep the configuration, not keep the cluster running. According to Satabricks, the answer is to pin the cluster.\nhttps://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluster"
      },
      {
        "date": "2021-05-20T02:05:00.000Z",
        "voteCount": 3,
        "content": "B. Pin the cluster is the appropriate answer for the requirement\n\nReference: https://docs.databricks.com/clusters/clusters-manage.html#pin-a-cluste"
      },
      {
        "date": "2020-12-19T01:04:00.000Z",
        "voteCount": 2,
        "content": "B. Pin the cluster is an answer. \n\nTo keep a cluster configuration even after a cluster has been terminated (which is after 30 days) administrator must pin the cluster. \n\nSource: https://docs.microsoft.com/en-us/azure/databricks/clusters/clusters-manage"
      },
      {
        "date": "2020-06-30T11:50:00.000Z",
        "voteCount": 4,
        "content": "Databricks documentation points to the use of pinning as the way to keep configurations: https://docs.databricks.com/clusters/index.html - 'Important!' section."
      },
      {
        "date": "2020-06-27T23:53:00.000Z",
        "voteCount": 6,
        "content": "according to the instructor at one of the training sessions provided by MS, the answer is A.\nHere is a screenshot from the session:\nhttps://ibb.co/LgL5gCz"
      },
      {
        "date": "2020-10-01T14:29:00.000Z",
        "voteCount": 3,
        "content": "just watched the session and you are right !"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8467-exam-dp-201-topic-1-question-13-discussion/",
    "body": "You are designing a solution for a company. The solution will use model training for objective classification.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Cognitive Services application",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Spark Streaming job",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tinteractive Spark queries",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI models",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Spark application that uses Spark MLib.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "Spark in SQL Server big data cluster enables AI and machine learning.<br>You can use Apache Spark MLlib to create a machine learning application to do simple predictive analysis on an open dataset.<br>MLlib is a core Spark library that provides many utilities useful for machine learning tasks, including utilities that are suitable for:<br>\u2711 Classification<br>\u2711 Regression<br>\u2711 Clustering<br>\u2711 Topic modeling<br>\u2711 Singular value decomposition (SVD) and principal component analysis (PCA)<br>\u2711 Hypothesis testing and calculating sample statistics<br>Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T09:48:00.000Z",
        "voteCount": 18,
        "content": "ooops... the correct is Spark ML Lib"
      },
      {
        "date": "2020-03-21T09:47:00.000Z",
        "voteCount": 12,
        "content": "one observation: the correct is Apache ML Lib"
      },
      {
        "date": "2022-02-11T05:22:00.000Z",
        "voteCount": 1,
        "content": "one observation: the correct is Apache ML Lib"
      },
      {
        "date": "2021-08-29T15:50:00.000Z",
        "voteCount": 1,
        "content": "A. an Azure Cognitive Services application"
      },
      {
        "date": "2021-05-20T02:11:00.000Z",
        "voteCount": 4,
        "content": "appropriate answer is E\n\nReference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-machine-learning-mllib-ipython"
      },
      {
        "date": "2021-05-01T03:31:00.000Z",
        "voteCount": 3,
        "content": "Spark Mlib should be the correct answer"
      },
      {
        "date": "2021-02-21T03:31:00.000Z",
        "voteCount": 1,
        "content": "the keyword is  Machine Learning \"objective classification\" to choose the ans choice Spark ML Lib"
      },
      {
        "date": "2020-12-19T01:10:00.000Z",
        "voteCount": 3,
        "content": "E. a Spark application that uses Spark MLib. is correct answer"
      },
      {
        "date": "2020-12-08T06:53:00.000Z",
        "voteCount": 2,
        "content": "Model training so Machine Learning is what should come into mind\nE is the answer"
      },
      {
        "date": "2020-09-06T02:14:00.000Z",
        "voteCount": 2,
        "content": "Cognitive services.. it's possibile train model with Custom Vision API"
      },
      {
        "date": "2020-05-28T05:33:00.000Z",
        "voteCount": 3,
        "content": "It is for model training hence Spark ML Lib"
      },
      {
        "date": "2020-05-27T00:29:00.000Z",
        "voteCount": 4,
        "content": "For training model this is Spark MLlib that contains ML models for spark. Cognitive services is not for training a new model but to use some existing pretrained models."
      },
      {
        "date": "2019-12-27T05:08:00.000Z",
        "voteCount": 6,
        "content": "The Computer Vision API in Cognitive Services can do this too, but it's out of scope for this exam."
      },
      {
        "date": "2019-11-18T05:08:00.000Z",
        "voteCount": 4,
        "content": "Why not use Cognitive Services ? it is built to achieve such classification tasks, is'nt it ?"
      },
      {
        "date": "2020-04-20T06:47:00.000Z",
        "voteCount": 17,
        "content": "It says model training, if you need to do model training cognitive services are not the correct solutions. They are already trained. The question refers to a custom scenario."
      },
      {
        "date": "2020-09-16T12:17:00.000Z",
        "voteCount": 4,
        "content": "Cognitive services.. it's possibile train model with Custom Vision API"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/22716-exam-dp-201-topic-1-question-14-discussion/",
    "body": "A company stores data in multiple types of cloud-based databases.<br>You need to design a solution to consolidate data into a single relational database. Ingestion of data will occur at set times each day.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Migration Assistant",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Data Sync",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Database Migration Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Migration Assistant"
    ],
    "answer": "C",
    "answerDescription": "Incorrect Answers:<br>D: Azure Database Migration Service is used to migrate on-premises SQL Server databases to the cloud.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/introduction https://azure.microsoft.com/en-us/blog/operationalize-azure-databricks-notebooks-using-data-factory/ https://azure.microsoft.com/en-us/blog/data-ingestion-into-azure-at-scale-made-easier-with-latest-enhancements-to-adf-copy-data-tool/",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-12T17:59:00.000Z",
        "voteCount": 36,
        "content": "Source data is stored on different cloud storage and you need migrate into  relational database, so only Azure Data Factory can do this task"
      },
      {
        "date": "2020-07-06T02:06:00.000Z",
        "voteCount": 2,
        "content": "The member databases can be either databases in Azure SQL Database or in instances of SQL Server. See https://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database. So it's probably not Data Sync."
      },
      {
        "date": "2020-07-06T02:07:00.000Z",
        "voteCount": 1,
        "content": "Quote from the website: \"The member databases can be either databases in Azure SQL Database or in instances of SQL Server.\""
      },
      {
        "date": "2020-10-15T02:25:00.000Z",
        "voteCount": 8,
        "content": "Here are my thoughts: we know that A and E are obviously incorrect, and since this is a single-selection question, we've got to select the most appropriate answer from the rest choices. SQL Data Sync can only be used to sync data between a Hub Database and a Member Database, and the Hub Database must be an Azure SQL Database. Azure Database Migration Service is mainly used to migrate data from on-prem RDMS to Azure Database or from MongoDB to Azure Cosmos DB (you can still do cloud-to-cloud migrations with it but there are strict network topology requirements applied). Considering that we have various types of databases in Azure and the consolidation requirements are not clear, Azure Data Factory is the most universal solution, so the answer is C."
      },
      {
        "date": "2021-05-01T03:36:00.000Z",
        "voteCount": 2,
        "content": "Azure data factory can be used to connect to multiple databases, hence the ideal solution for sourcing data on multiple cloud databases"
      },
      {
        "date": "2021-02-21T03:36:00.000Z",
        "voteCount": 1,
        "content": "Normally ADF is recommended for ingestion when when mult.. cluod dbs are stored"
      },
      {
        "date": "2020-12-08T07:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview#supported-data-stores-and-formats\nData Factory is correct"
      },
      {
        "date": "2020-06-20T22:20:00.000Z",
        "voteCount": 6,
        "content": "The data is already hosted in cloud data stores, SQL data sync appeal to customers who are considering moving to the cloud and would like to put some of their application in Azure, So Azure Datafactory is appropriate here"
      },
      {
        "date": "2020-06-10T03:05:00.000Z",
        "voteCount": 2,
        "content": "why not Data sync? \nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database"
      },
      {
        "date": "2020-06-10T13:19:00.000Z",
        "voteCount": 2,
        "content": "I think you might be right. https://www.bdo.com/digital/insights/cloud/azure-sql-data-sync-is-now-generally-available \nThis article highlights the possibility"
      },
      {
        "date": "2020-10-15T13:57:00.000Z",
        "voteCount": 1,
        "content": "A company stores data in multiple types of cloud-based databases\n\"Oracle\", \"MySQL\", \"Postgres\""
      },
      {
        "date": "2020-08-07T22:11:00.000Z",
        "voteCount": 6,
        "content": "Though we can achieve this using data sync but it isn't recommended for this scenario. As data sync is mostly used for synchronizing but not transfer related activities."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27582-exam-dp-201-topic-1-question-15-discussion/",
    "body": "HOTSPOT -<br>You manage an on-premises server named Server1 that has a database named Database1. The company purchases a new application that can access data from<br>Azure SQL Database.<br>You recommend a solution to migrate Database1 to an Azure SQL Database instance.<br>What should you recommend? To answer, select the appropriate configuration in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0006000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0006100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-import",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-07T22:18:00.000Z",
        "voteCount": 47,
        "content": "The explaination is\nIf we have an on-premises database and we have a new app(ex: app1). Now app1 can only access data from azure databases. So our plan is to\n1. Import data from onpremises to azure sql db.\nBackup the on premises db into a bacpac file -&gt; Upload the bacpac file to a blob storage container -&gt; Go to azure sql db you created and in overview click on backup and give the information about sa( blob container) and click backup."
      },
      {
        "date": "2020-12-19T01:11:00.000Z",
        "voteCount": 8,
        "content": "Box 1: BACKPACK\nBox2: BLOB"
      },
      {
        "date": "2021-06-08T00:55:00.000Z",
        "voteCount": 1,
        "content": "BACPAC contains the schema and data of the specific database while Azure Blob Storage is the storage that can handle such file and size."
      },
      {
        "date": "2021-05-01T03:38:00.000Z",
        "voteCount": 1,
        "content": "1: Bacpac\n2: Blob"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13227-exam-dp-201-topic-1-question-16-discussion/",
    "body": "You are designing an application. You plan to use Azure SQL Database to support the application.<br>The application will extract data from the Azure SQL Database and create text documents. The text documents will be placed into a cloud-based storage solution.<br>The text storage solution must be accessible from an SMB network share.<br>You need to recommend a data storage solution for the text documents.<br>Which Azure data storage type should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQueue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFiles",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBlob",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable"
    ],
    "answer": "B",
    "answerDescription": "Azure Files enables you to set up highly available network file shares that can be accessed by using the standard Server Message Block (SMB) protocol.<br>Incorrect Answers:<br>A: The Azure Queue service is used to store and retrieve messages. It is generally used to store lists of messages to be processed asynchronously.<br>C: Blob storage is optimized for storing massive amounts of unstructured data, such as text or binary data. Blob storage can be accessed via HTTP or HTTPS but not via SMB.<br>D: Azure Table storage is used to store large amounts of structured data. Azure tables are ideal for storing structured, non-relational data.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-introduction https://docs.microsoft.com/en-us/azure/storage/tables/table-storage-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-31T12:19:00.000Z",
        "voteCount": 13,
        "content": "https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-use-files-windows"
      },
      {
        "date": "2020-06-01T23:27:00.000Z",
        "voteCount": 13,
        "content": "Yes SMB only available with FILES."
      },
      {
        "date": "2021-05-01T03:57:00.000Z",
        "voteCount": 1,
        "content": "SMB is only available with Files as we are using test documents storage"
      },
      {
        "date": "2021-02-21T04:21:00.000Z",
        "voteCount": 2,
        "content": "the keyword is SMB (Server Mesage Block) protocol based on it we can choose the ans choice FILES \nhttps://docs.microsoft.com/en-us/azure/storage/files/storage-files-introduction pls refer this link"
      },
      {
        "date": "2021-02-02T06:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer"
      },
      {
        "date": "2020-12-19T01:11:00.000Z",
        "voteCount": 3,
        "content": "SMB protocol is for Azure Files. Files is the obvious answer."
      },
      {
        "date": "2020-12-05T00:23:00.000Z",
        "voteCount": 4,
        "content": "Can only be files since SMB is mentioned"
      },
      {
        "date": "2020-05-27T00:29:00.000Z",
        "voteCount": 3,
        "content": "SMB is available with FILES."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25910-exam-dp-201-topic-1-question-17-discussion/",
    "body": "You are designing an application that will have an Azure virtual machine. The virtual machine will access an Azure SQL database. The database will not be accessible from the Internet.<br>You need to recommend a solution to provide the required level of access to the database.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an On-premises data gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a virtual network to the Azure SQL server that hosts the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an application gateway to the virtual network that contains the Azure virtual machine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a virtual network gateway to the virtual network that contains the Azure virtual machine."
    ],
    "answer": "B",
    "answerDescription": "When you create an Azure virtual machine (VM), you must create a virtual network (VNet) or use an existing VNet. You also need to decide how your VMs are intended to be accessed on the VNet.<br>Incorrect Answers:<br>C: Azure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications.<br>D: A VPN gateway is a specific type of virtual network gateway that is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public Internet.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/virtual-machines/network-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-07T22:36:00.000Z",
        "voteCount": 23,
        "content": "There are many ways to achieve this\n1. One way is to use a service endpoint along with service end point policies.\n2. Second way is by using an azure private link.\n3. Third way is to go to azure sql server where the db is hosted and add a virtual network using firewall/virtual_network blade."
      },
      {
        "date": "2020-08-09T03:08:00.000Z",
        "voteCount": 19,
        "content": "B correct: agree with Yaswant, but haven't found a source that presents the three options.  I'd say A, C and D are not correct and therefore B is correct.\n\nHere a graph:\n\nhttps://azure.microsoft.com/de-de/blog/vnet-service-endpoints-for-azure-sql-database-now-generally-available/"
      },
      {
        "date": "2020-12-07T04:05:00.000Z",
        "voteCount": 8,
        "content": "B is the best answer\nThe on-premises data gateway acts as a bridge. It provides quick and secure data transfer between on-premises data, which is data that isn't in the cloud, and several Microsoft cloud services\nAzure Application Gateway is a web traffic load balancer that enables you to manage traffic to your web applications\nA VPN gateway is a specific type of virtual network gateway that is used to send encrypted traffic between an Azure virtual network and an on-premises location over the public Internet"
      },
      {
        "date": "2021-03-01T03:17:00.000Z",
        "voteCount": 1,
        "content": "you need an endpoint in a VNet"
      },
      {
        "date": "2021-02-18T04:23:00.000Z",
        "voteCount": 1,
        "content": "the mentioned ans is correct pls check the link https://azure.microsoft.com/de-de/blog/vnet-service-endpoints-for-azure-sql-database-now-generally-available/ and line \nIt's shown in the picture clearly where C&amp; D options dont relate at all"
      },
      {
        "date": "2020-12-19T01:13:00.000Z",
        "voteCount": 4,
        "content": "B. Add a virtual network to the Azure SQL server that hosts the database.\nIs the correct answer."
      },
      {
        "date": "2020-07-16T08:58:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is \"C\""
      },
      {
        "date": "2021-04-16T06:33:00.000Z",
        "voteCount": 2,
        "content": "Please Note the option C and D is for virtual machine not for database."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17979-exam-dp-201-topic-1-question-18-discussion/",
    "body": "HOTSPOT -<br>You are designing an application that will store petabytes of medical imaging data<br>When the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes.<br>You need to select a storage strategy for the data. The solution must minimize costs.<br>Which storage tier should you use for each time frame? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0006400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0006500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "First week: Hot -<br>Hot - Optimized for storing data that is accessed frequently.<br><br>After one month: Cool -<br>Cool - Optimized for storing data that is infrequently accessed and stored for at least 30 days.<br><br>After one year: Cool -<br>Incorrect Answers:<br>Archive: Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements (on the order of hours).<br>References:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-06T06:00:00.000Z",
        "voteCount": 61,
        "content": "This is correct. Look at this from the docs:\n\n\"The archive access tier has the lowest storage cost. But it has higher data retrieval costs compared to the hot and cool tiers. Data in the archive tier can take several hours to retrieve.\""
      },
      {
        "date": "2020-08-07T22:43:00.000Z",
        "voteCount": 15,
        "content": "Hot : Frequent\nCool : Infrequent (30 days)\nArchive : Rare (180 days) -&gt; Data from archive tier can only be accessed by rehydrating the blob which may take up to several hours."
      },
      {
        "date": "2021-05-01T04:00:00.000Z",
        "voteCount": 5,
        "content": "We can't use archive because data must be accessible. So:\nBox 1: Hot\nBox 2: Cold\nBox 3: Cold"
      },
      {
        "date": "2021-02-14T23:30:00.000Z",
        "voteCount": 1,
        "content": "Hot cold cold : Correct"
      },
      {
        "date": "2020-12-19T01:14:00.000Z",
        "voteCount": 3,
        "content": "We can't use archive because data must be accessible. So:\nBox 1: Hot\nBox 2: Cold\nBox 3: Cold"
      },
      {
        "date": "2020-10-27T17:27:00.000Z",
        "voteCount": 1,
        "content": "I agree with the answer by the reasons explained in the comments."
      },
      {
        "date": "2020-07-12T06:22:00.000Z",
        "voteCount": 3,
        "content": "After One year The correct Answer is Archieve"
      },
      {
        "date": "2020-07-16T07:29:00.000Z",
        "voteCount": 22,
        "content": "Data in the archive tier can take several hours to retrieve. But here the question asks to retrieve data in 5 minutes, hence it should be Cool Tire. Given answer is correct."
      },
      {
        "date": "2020-08-09T18:30:00.000Z",
        "voteCount": 5,
        "content": "Apart from the reason given by RajdeepRoy, it costs more to retrieve data from archived data and that would not be in line with the requirement of minimizing the cost."
      },
      {
        "date": "2020-05-27T00:30:00.000Z",
        "voteCount": 5,
        "content": "Yes correct answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38894-exam-dp-201-topic-1-question-19-discussion/",
    "body": "You are designing a data store that will store organizational information for a company. The data will be used to identify the relationships between users. The data will be stored in an Azure Cosmos DB database and will contain several million objects.<br>You need to recommend which API to use for the database. The API must minimize the complexity to query the user relationships. The solution must support fast traversals.<br>Which API should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMongoDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGremlin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCassandra"
    ],
    "answer": "C",
    "answerDescription": "Gremlin features fast queries and traversals with the most widely adopted graph query standard.<br>Reference:<br>https://docs.microsoft.com/th-th/azure/cosmos-db/graph-introduction?view=azurermps-5.7.0",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T01:14:00.000Z",
        "voteCount": 15,
        "content": "C. Gremlin API\nWhen we talk about relationships and/or edges/nodes, Gremin API is the answer"
      },
      {
        "date": "2020-12-05T04:55:00.000Z",
        "voteCount": 7,
        "content": "identify the relationships between users\nAnswer is C"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16139-exam-dp-201-topic-1-question-20-discussion/",
    "body": "HOTSPOT -<br>You are designing a new application that uses Azure Cosmos DB. The application will support a variety of data patterns including log records and social media relationships.<br>You need to recommend which Cosmos DB API to use for each data pattern. The solution must minimize resource utilization.<br>Which API should you recommend for each data pattern? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0006700001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0006800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Log records: SQL -<br><br>Social media mentions: Gremlin -<br>You can store the actual graph of followers using Azure Cosmos DB Gremlin API to create vertexes for each user and edges that maintain the \"A-follows-B\" relationships. With the Gremlin API, you can get the followers of a certain user and create more complex queries to suggest people in common. If you add to the graph the Content Categories that people like or enjoy, you can start weaving experiences that include smart content discovery, suggesting content that those people you follow like, or finding people that you might have much in common with.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/social-media-apps",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T01:15:00.000Z",
        "voteCount": 27,
        "content": "Log records: SQL (because logs are row oriented)\nSocial Media: Gremlin (because it uses relationships)"
      },
      {
        "date": "2021-05-20T17:38:00.000Z",
        "voteCount": 4,
        "content": "propose solution is correct.\n\nCassandra API is mostly used to handle high volume and real time data while the requirement is related to log records, SQL API is sufficient in this terms. Gremlin API is appropriate when we are talking about connection between entities.\n\nReference: https://acloudguru.com/blog/engineering/azure-cosmos-db-apis-use-cases-and-trade-offs"
      },
      {
        "date": "2020-03-10T09:54:00.000Z",
        "voteCount": 3,
        "content": "why  SQL over Cassandra for log records?"
      },
      {
        "date": "2020-03-14T18:23:00.000Z",
        "voteCount": 46,
        "content": "log data are row-oriented so SQL handle them better, Cassandra are mainly for column-oriented data"
      },
      {
        "date": "2020-04-01T03:13:00.000Z",
        "voteCount": 40,
        "content": "It also states in the documentation that Cassandra is only recommend to migrate existing Cassandra databases to CosmosDB.  In all other cases, the SQL Api is recommended.\n\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/3-analyze-the-decision-criteria"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17150-exam-dp-201-topic-1-question-21-discussion/",
    "body": "You need to recommend a storage solution to store flat files and columnar optimized files. The solution must meet the following requirements:<br>\u2711 Store standardized data that data scientists will explore in a curated folder.<br>\u2711 Ensure that applications cannot access the curated folder.<br>\u2711 Store staged data for import to applications in a raw folder.<br>\u2711 Provide data scientists with access to specific folders in the raw folder and all the content the curated folder.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "B",
    "answerDescription": "Azure Blob Storage containers is a general purpose object store for a wide variety of storage scenarios. Blobs are stored in containers, which are similar to folders.<br>Incorrect Answers:<br>C: Azure Data Lake Storage is an optimized storage for big data analytics workloads.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-21T19:36:00.000Z",
        "voteCount": 112,
        "content": "Shouldn't answer be C, there is no concept of folders and folder permissions in Azure storage."
      },
      {
        "date": "2021-09-23T12:25:00.000Z",
        "voteCount": 1,
        "content": "RBAC Security on Azure Blob can be scoped at the container level or above. With two containers (one for raw data and one for curated data) without folders, it would be possible to manage the security. BUT the current use case states that: \" datascientists need access to specifics folders in the \"raw\" folder. You cannot manage security at this level with Azure Blob. You have to use Azure Data Lake with RBAC/ACLs. The right answer is C"
      },
      {
        "date": "2020-04-01T03:15:00.000Z",
        "voteCount": 13,
        "content": "I agree, Azure Data Lake Stroage includes ACLs which can be applied to folder structures, which Blob Storage does not.  Therefore the security requirements mean the answer should be ADLS"
      },
      {
        "date": "2020-04-14T06:19:00.000Z",
        "voteCount": 10,
        "content": "There is the notion of public anonymous access in blob storage as well as shared access signatures, and of course RBAC can be implemented through Azure AD for Blobs and Queues, so the security requirements can be met.\nCheck this doc : https://docs.microsoft.com/en-us/azure/storage/common/storage-auth-aad-rbac-portal"
      },
      {
        "date": "2020-04-22T11:36:00.000Z",
        "voteCount": 3,
        "content": "Agree, especially it is only required to handle standardized data. There is no need to use ADLS."
      },
      {
        "date": "2020-04-23T10:50:00.000Z",
        "voteCount": 16,
        "content": "However, strictly speaking, in BLOB storage, data not stored in folders, just the name of the blob will include the folder name. So if the requirement is to store in folder, it have to be ADLS"
      },
      {
        "date": "2020-04-23T10:56:00.000Z",
        "voteCount": 11,
        "content": "Also, in blob, without RBAC you can only grant permission to the level of container."
      },
      {
        "date": "2020-11-12T02:21:00.000Z",
        "voteCount": 2,
        "content": "When an Azure role is assigned to an Azure AD security principal, Azure grants access to those resources for that security principal. Access can be scoped to the level of the subscription, the resource group, the storage account, or an individual container or queue. An Azure AD security principal may be a user, a group, an application service principal, or a managed identity for Azure resources.\nhttps://docs.microsoft.com/de-de/azure/storage/common/storage-auth-aad"
      },
      {
        "date": "2021-06-18T23:29:00.000Z",
        "voteCount": 2,
        "content": "there is, it is called container."
      },
      {
        "date": "2020-04-26T12:06:00.000Z",
        "voteCount": 45,
        "content": "Answer should definitely be C, Azure Data Lake Storage Gen2."
      },
      {
        "date": "2021-06-18T23:33:00.000Z",
        "voteCount": 3,
        "content": "The given answer is wrong and it should be C. The answer given states container is same as folder but it is not. A folder can have sub folders and access can be given only to sub folder. Where as in containers there are no sub containers hence the answer is wrong. Folder however can be given access in ADLS Gen2 using ACL so when we have a straight forward answer, why go with assumtion that 'container is same as folder'"
      },
      {
        "date": "2021-06-05T06:00:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Lake Store Gen2 is a superset of Azure Blob storage capabilities. In the list below, some of the key differences between ADLS Gen2 and Blob storage are summarized.\n\n ADLS Gen2 supports ACL and POSIX permissions allowing for more granular access control compared to Blob storage.\nADLS Gen2 introduces a hierarchical namespace. This is a true file system, unlike Blob Storage which has a flat namespace. This capability has a significant impact on performance, especially in big data analytics scenarios.\nADLS Gen2 is an HDFS-compatible store. This means that Apache Hadoop services can use data stored in ADLS Gen2. Azure Blob storage is not Hadoop-compatible."
      },
      {
        "date": "2021-06-01T18:34:00.000Z",
        "voteCount": 1,
        "content": "ADLS is the appropriate solution here as it has ACL function."
      },
      {
        "date": "2021-05-27T05:00:00.000Z",
        "voteCount": 1,
        "content": "In Question they mentioned about flat files and columnar optimized files(Binary Files) and Containers are similar to folders, so Azure storage is Correct"
      },
      {
        "date": "2021-05-20T18:34:00.000Z",
        "voteCount": 1,
        "content": "The requirements leads to using ADLS gen 2 as it can manage the folder level using ACL"
      },
      {
        "date": "2021-04-28T12:47:00.000Z",
        "voteCount": 2,
        "content": "C. Azure Data Lake Storage Gen2"
      },
      {
        "date": "2021-04-12T22:40:00.000Z",
        "voteCount": 2,
        "content": "Columnar optimized file for Raw, enriched and curated structure with Folder level access\nAns is ADLS\nref: \nhttps://www.dremio.com/data-lake/adls/\nhttps://medium.com/microsoftazure/building-your-data-lake-on-adls-gen2-3f196fc6b430"
      },
      {
        "date": "2021-02-19T05:52:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct as when you check the below link\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage it's clearly mentioned that ADLS can be used with certain restrictions it can be accessed via az synapse using poly base feature. There are certain performance tuning guidelines but in qn it's asked it need to be easily accessed by data scientistists as per the conditions we can go with blob storage"
      },
      {
        "date": "2021-02-06T05:35:00.000Z",
        "voteCount": 2,
        "content": "I will choose Azure Datalake on the only fact that the question did ask for columnar optimized files and this is available in Datalake and not Azure storage account"
      },
      {
        "date": "2020-12-28T07:33:00.000Z",
        "voteCount": 2,
        "content": "A folder can be created in a blob (e.g. via button \"Create folder\" in the portal) but such a folder is virtual. Using Azure Storage Explorer (presently v1.17.0), one can verify that an SAS can be created on a blob container, but not on a folder within a blob. \nStill, multiple containers could be created where each container maps to one group of users in the security requirements. This is not forbidden in the question. If each container has one or more folders, all requirements would still be met, making answer B a \"minimum viable answer\". \nHowever, I agree that answer C is the best and most flexible. Using Azure Storage Explorer, one can easily verify that the option \"Manage Access Control Lists\" is available on an individual folder."
      },
      {
        "date": "2020-10-23T04:51:00.000Z",
        "voteCount": 3,
        "content": "The given answer is clearly incorrect. All the points that are mentioned in the questions are hints to use ADLS Gen 2."
      },
      {
        "date": "2020-10-11T05:45:00.000Z",
        "voteCount": 3,
        "content": "The answer also mentions that the files are supposed to be explored by Data Scientists in curated folder. ADLS Gen 2 hooked up with Databricks or Azure Synapse Analytics is a ready-made solution for this kind of exploration."
      },
      {
        "date": "2020-10-11T05:40:00.000Z",
        "voteCount": 2,
        "content": "I think correct answer should be ADLS Gen 2."
      },
      {
        "date": "2020-08-29T10:09:00.000Z",
        "voteCount": 6,
        "content": "raw, curated folder, folder level access all characteristics of ADLS"
      },
      {
        "date": "2020-08-13T10:10:00.000Z",
        "voteCount": 1,
        "content": "I believe there is actually only a single layer of containers. You can virtually create a \"file-system\" like layered storage, but in reality everything will be in 1 layer, the container in which it is.\nSo Answer should be DATA LAKE"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17749-exam-dp-201-topic-1-question-22-discussion/",
    "body": "Your company is an online retailer that can have more than 100 million orders during a 24-hour period, 95 percent of which are placed between 16:30 and 17:00.<br>All the orders are in US dollars. The current product line contains the following three item categories:<br>\u2711 Games with 15,123 items<br>\u2711 Books with 35,312 items<br>\u2711 Pens with 6,234 items<br>You are designing an Azure Cosmos DB data solution for a collection named Orders Collection. The following documents is a typical order in Orders Collection.<br><img src=\"/assets/media/exam-media/03774/0007000001.png\" class=\"in-exam-image\"><br>Orders Collection is expected to have a balanced read/write-intensive workload.<br>Which partition key provides the most efficient throughput?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tItem/Category",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrderTime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tItem/Currency",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tItem/id"
    ],
    "answer": "A",
    "answerDescription": "Choose a partition key that has a wide range of values and access patterns that are evenly spread across logical partitions. This helps spread the data and the activity in your container across the set of logical partitions, so that resources for data storage and throughput can be distributed across the logical partitions.<br>Choose a partition key that spreads the workload evenly across all partitions and evenly over time. Your choice of partition key should balance the need for efficient partition queries and transactions against the goal of distributing items across multiple partitions to achieve scalability.<br>Candidates for partition keys might include properties that appear frequently as a filter in your queries. Queries can be efficiently routed by including the partition key in the filter predicate.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-01T03:19:00.000Z",
        "voteCount": 67,
        "content": "Given there are 100 million orders in a 24 hour period, and there are only three catgegories, is Item/Id not a better solution, otherwise the category will cause significant hotspots?"
      },
      {
        "date": "2020-08-04T05:52:00.000Z",
        "voteCount": 7,
        "content": "I think if the id was an integrer (inremental foe exemple ) it can be a good partition key but with this format i think category is the best choice"
      },
      {
        "date": "2020-05-04T11:14:00.000Z",
        "voteCount": 35,
        "content": "2 paragraphs below the link given in microsoft docs, there is an interesting answer :\n\nUsing item ID as the partition key\n\nIf your container has a property that has a wide range of possible values, it is likely a great partition key choice. One possible example of such a property is the item ID. For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.\n\nThe item ID is a great partition key choice for the following reasons:\nThere are a wide range of possible values (one unique item ID per item).\nBecause there is a unique item ID per item, the item ID does a great job at evenly balancing RU consumption and data storage.\nYou can easily do efficient point reads since you'll always know an item's partition key if you know its item ID."
      },
      {
        "date": "2020-08-09T03:27:00.000Z",
        "voteCount": 2,
        "content": "D correct: Source as above quoted https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview"
      },
      {
        "date": "2021-05-17T12:20:00.000Z",
        "voteCount": 5,
        "content": "Answer should be \"item/id\". You can find almost similar example in below link.\nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-\nlesson"
      },
      {
        "date": "2021-06-05T08:09:00.000Z",
        "voteCount": 1,
        "content": "This example is definitely VERY similar to the question and explains why several of the proposed values are not good choices, and also shows \"Item/Id\" to be a decent choice."
      },
      {
        "date": "2021-04-28T12:52:00.000Z",
        "voteCount": 6,
        "content": "D. Item/id   is the answer"
      },
      {
        "date": "2021-02-22T03:12:00.000Z",
        "voteCount": 1,
        "content": "Given solution is right where we choose the item/category. It's explained in detail in the below link https://medium.com/walmartglobaltech/deep-dive-azure-cosmos-partitions-and-partitionkey-14e898f371cd this concept is of major focus as question may not be exactly asked in exam we need to need to know the concept of physical &amp;  logical partitions pre-requisites &amp; Partition key as well."
      },
      {
        "date": "2021-05-19T16:51:00.000Z",
        "voteCount": 1,
        "content": "the item/id is the correct solution, regarding to the explanation in the link that you posted, all the documents related to the item/id will store in same partition."
      },
      {
        "date": "2021-02-19T06:51:00.000Z",
        "voteCount": 14,
        "content": "If you use the Item/Category property as a partition key, then it has a small cardinality. Even if the documents are evenly distributed across the collection, for large collections, any category might outgrow a single partition.\n\nIf the categories aren't evenly distributed across the documents in the collection, then the problem is even worse. The dominant category restricts the ability of Azure Cosmos DB to scale.\n\nItem/Category is not a good choice for the partition key.\nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson"
      },
      {
        "date": "2021-06-01T19:41:00.000Z",
        "voteCount": 2,
        "content": "perfect! the link provided clear states the strategy of optimizing partition."
      },
      {
        "date": "2021-07-05T18:09:00.000Z",
        "voteCount": 2,
        "content": "Superb ! Its crystal clear now. Partition should be on Item/id. Requesting all to go through above link."
      },
      {
        "date": "2021-02-26T06:30:00.000Z",
        "voteCount": 1,
        "content": "Nice link. It is exactly the case from the ex."
      },
      {
        "date": "2021-05-04T22:37:00.000Z",
        "voteCount": 3,
        "content": "this link is the answer to all the confusion here"
      },
      {
        "date": "2020-12-08T07:05:00.000Z",
        "voteCount": 4,
        "content": "From https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey:\n\"Have a high cardinality. In other words, the property should have a wide range of possible values.\"\nD is the answer"
      },
      {
        "date": "2020-11-22T14:11:00.000Z",
        "voteCount": 5,
        "content": "item/id for sure.\nsee the section \"Propose partition key values for the collection\" at: \nhttps://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson"
      },
      {
        "date": "2020-11-12T02:44:00.000Z",
        "voteCount": 1,
        "content": "Candidates for partition keys might include properties that appear frequently as a filter in your queries. Queries can be efficiently routed by including the partition key in the filter predicate.\nItem ID will not appear as a filter most likely"
      },
      {
        "date": "2020-11-12T02:49:00.000Z",
        "voteCount": 1,
        "content": "For small read-heavy containers or write-heavy containers of any size, Item-ID is naturally good choice. In this case, we have balanced read/write workload"
      },
      {
        "date": "2020-10-23T05:08:00.000Z",
        "voteCount": 12,
        "content": "Given the discussion here: https://docs.microsoft.com/en-us/learn/modules/monitor-and-scale-cosmos-db/5-partition-lesson, \"Item/id\" is the correct answer"
      },
      {
        "date": "2020-10-22T01:48:00.000Z",
        "voteCount": 8,
        "content": "I shall go with D. Item/Id\nItem/Category is out. It will only create 3 logical partitions, that also unevenly distributed. A logical distribution has a size cap of 20 GB. With 100 million orders per day, it won't be very hard to reach that limit quickly.\n \nOrderTime is out. 16:30 to 17:00 spike shall create a hotspot problem.\n \nItem/Currency is out. Only 1 value \"USD\" will result in everything cramming up one logical partition.\n \nOnly Item/id is left. So this is the answer."
      },
      {
        "date": "2020-09-26T08:05:00.000Z",
        "voteCount": 1,
        "content": "your partition key should:\n\nBe a property that has a value which does not change. If a property is your partition key, you can't update that property's value.\nHave a high cardinality. In other words, the property should have a wide range of possible values.\nSpread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions."
      },
      {
        "date": "2020-08-11T15:07:00.000Z",
        "voteCount": 3,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/partition-data#logical-partitions\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview\nhttps://www.examtopics.com/exams/microsoft/dp-201/view/6/\n\nD Item/ID\n\nCategory doesn\u2019t distribute RU evenly across partitions. Low cardinality."
      },
      {
        "date": "2020-08-07T23:48:00.000Z",
        "voteCount": 1,
        "content": "Consider we have provisioned a throughput of 1200 request units and we know that throughput can be provisioned in cosmos db only at a container level or at a database level.\nIn our case we consider our online retailer to be Walkart. Now walkart has an account in cosmosdb and they have a document db with coresql api. Now walkart has created a container named orders in their cosmos account and provisioned 1200ru's.\nNow consider the case of choosing a partition key. Considering they have 1200 customer id-s and if they use id as partition key they will have their throughput spread across partitions which makes their unused throughput in vain as customers come buy and go and it makes a hotspot. Now if we choose product category as partition we'll be having a balanced throughput and read-write."
      },
      {
        "date": "2020-07-24T06:39:00.000Z",
        "voteCount": 1,
        "content": "these comments causing further confusing for new bees as it's not able to draw whats final correct answer.. I would go by Item/Category only... this combo may not give repeated values as item would be different in same category.. item/id might create super heavy number of partitions"
      },
      {
        "date": "2020-06-29T09:05:00.000Z",
        "voteCount": 7,
        "content": "In this case A is correct indeed. See the reference and be aware of the read/write balancing. The read is as important as the throuput. \n\nPartition keys for read-heavy containers\nFor most containers, the above criteria is all you need to consider when picking a partition key. For large read-heavy containers, however, you might want to choose a partition key that appears frequently as a filter in your queries. Queries can be efficiently routed to only the relevant physical partitions by including the partition key in the filter predicate.\nIf most of your workload's requests are queries and most of your queries have an equality filter on the same property, this property can be a good partition key choice. For example, if you frequently run a query that filters on UserID, then selecting UserID as the partition key would reduce the number of cross-partition queries"
      },
      {
        "date": "2020-07-14T07:27:00.000Z",
        "voteCount": 1,
        "content": "Ur explanation is correct"
      },
      {
        "date": "2020-06-12T00:53:00.000Z",
        "voteCount": 3,
        "content": "Choose a partition key that has a wide range of values,so the data is evenly spread across logical partitioning. Hence I suggest the answer is item/category"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19060-exam-dp-201-topic-1-question-23-discussion/",
    "body": "You have a MongoDB database that you plan to migrate to an Azure Cosmos DB account that uses the MongoDB API.<br>During testing, you discover that the migration takes longer than expected.<br>You need to recommend a solution that will reduce the amount of time it takes to migrate the data.<br>What are two possible recommendations to achieve this goal? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Request Units (RUs).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off indexing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a write region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate unique indexes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate compound indexes."
    ],
    "answer": "AB",
    "answerDescription": "A: Increase the throughput during the migration by increasing the Request Units (RUs).<br>For customers that are migrating many collections within a database, it is strongly recommend to configure database-level throughput. You must make this choice when you create the database. The minimum database-level throughput capacity is 400 RU/sec. Each collection sharing database-level throughput requires at least 100 RU/sec.<br>B: By default, Azure Cosmos DB indexes all your data fields upon ingestion. You can modify the indexing policy in Azure Cosmos DB at any time. In fact, it is often recommended to turn off indexing when migrating data, and then turn it back on when the data is already in Cosmos DB.<br>Reference:<br>https://docs.microsoft.com/bs-latn-ba/Azure/cosmos-db/mongodb-pre-migration",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-22T02:48:00.000Z",
        "voteCount": 20,
        "content": "Correct"
      },
      {
        "date": "2020-12-19T01:17:00.000Z",
        "voteCount": 12,
        "content": "A. Increase the request units (RUs)\nB. Turn off indexing"
      },
      {
        "date": "2021-05-20T19:27:00.000Z",
        "voteCount": 1,
        "content": "Propose solution is correct, by default azure cosmos db create an index though the feature could be toggle to prevent it from happening.\n\nReference: https://docs.microsoft.com/bs-latn-ba/Azure/cosmos-db/mongodb-post-migration"
      },
      {
        "date": "2021-05-01T04:25:00.000Z",
        "voteCount": 1,
        "content": "turn off indexing and increase the request units - Answer A is correct"
      },
      {
        "date": "2020-11-08T22:37:00.000Z",
        "voteCount": 4,
        "content": "Indexing makes write operation slower. \n\n\"By default, indexing policy is set to automatic. It's achieved by setting the automatic property in the indexing policy to true. Setting this property to true allows Azure CosmosDB to automatically index documents as they are written.\"\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/index-policy"
      },
      {
        "date": "2021-01-05T09:25:00.000Z",
        "voteCount": 2,
        "content": "yes, we can turn that indexing off and turn it back on after data load is complete"
      },
      {
        "date": "2020-10-23T05:34:00.000Z",
        "voteCount": 5,
        "content": "The explanation for B. is incorrect. MongoDB API only creates an index for _id field. From the documentation: \"The Azure Cosmos DB's API for MongoDB server version 3.6 automatically indexes the _id field only. This field can't be dropped. It automatically enforces the uniqueness of the _id field per shard key. To index additional fields, you apply the MongoDB index-management commands. This default indexing policy differs from the Azure Cosmos DB SQL API, which indexes all fields by default.\"\nSo, I think B can not the correct answer."
      },
      {
        "date": "2020-10-21T16:35:00.000Z",
        "voteCount": 3,
        "content": "I think its A &amp; D. The link that is provided in explanation, does not mention about the turning off indexes, however it mentions Creating Unix Indexes. Any suggestion?"
      },
      {
        "date": "2020-12-05T04:30:00.000Z",
        "voteCount": 1,
        "content": "I agree on this one"
      },
      {
        "date": "2020-04-25T08:25:00.000Z",
        "voteCount": 2,
        "content": "Just a thought - If I migrate from Amazon and I have many locations there, it will make sense to have multiple write sites and run migration in parallel from several different locations. That will server as a migration accelerator."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24519-exam-dp-201-topic-1-question-24-discussion/",
    "body": "You need to recommend a storage solution for a sales system that will receive thousands of small files per minute. The files will be in JSON, text, and CSV formats. The files will be processed and transformed before they are loaded into a data warehouse in Azure Synapse Analytics. The files must be stored and secured in folders.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage"
    ],
    "answer": "A",
    "answerDescription": "Azure provides several solutions for working with CSV and JSON files, depending on your needs. The primary landing place for these files is either Azure Storage or Azure Data Lake Store.1<br>Azure Data Lake Storage is an optimized storage for big data analytics workloads.<br>Incorrect Answers:<br>D: Azure Blob Storage containers is a general purpose object store for a wide variety of storage scenarios. Blobs are stored in containers, which are similar to folders.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/scenarios/csv-and-json",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-07T23:54:00.000Z",
        "voteCount": 38,
        "content": "Blob Storage -&gt; Object Storage (Binary / Flatfiles)\nDataLake -&gt; Various formats (CSV, Json, Avro, Parquet.../ Folders)"
      },
      {
        "date": "2021-06-08T01:21:00.000Z",
        "voteCount": 2,
        "content": "ADLS is the appropriate solution for this requirement as it was indicated the use of \"folder\" and is a good holder of big data files."
      },
      {
        "date": "2021-05-25T20:09:00.000Z",
        "voteCount": 1,
        "content": "Even I am thinking to opt ADLS2 but the only thing to mention is several files per minute which might impact adls2 which is good for bigdata work loads as compared to blob? Which one is correct any source?"
      },
      {
        "date": "2021-05-01T04:26:00.000Z",
        "voteCount": 4,
        "content": "Azure datalake storage gen 2"
      },
      {
        "date": "2021-02-22T04:17:00.000Z",
        "voteCount": 4,
        "content": "The k/word to lookout for is \"folders\" which means ADLS Gen 2 which is built on top of Az Blob Strg it's a container - vir dir.. where as ADLS Gen 2 is like accumulation of files &amp; it's like a folder."
      },
      {
        "date": "2020-12-16T19:28:00.000Z",
        "voteCount": 1,
        "content": "ADLS is good for analytics solutions. This requirement has that, that is why ADLS"
      },
      {
        "date": "2020-12-05T04:44:00.000Z",
        "voteCount": 4,
        "content": "The files must be stored and secured in folders.\nA is the answer for sure"
      },
      {
        "date": "2020-10-01T05:49:00.000Z",
        "voteCount": 1,
        "content": "You can also query JSON files directly from Azure Blob Storage without importing them into Azure SQL. For a complete example of this approach, see Work with JSON files with Azure SQL. Currently this option isn't available for CSV files."
      },
      {
        "date": "2020-09-01T20:05:00.000Z",
        "voteCount": 1,
        "content": "I think it will be be ADLS as it supports all file format and will handle any flow of small files as long as we are not retaining them for a longer period there should not be any problem."
      },
      {
        "date": "2020-08-23T17:53:00.000Z",
        "voteCount": 1,
        "content": "Blob is correct"
      },
      {
        "date": "2020-12-16T12:53:00.000Z",
        "voteCount": 5,
        "content": "It is not. Correct answer is Azure Data Lake Gen 2. It is build on top of Blob Storage + hierarchical namespace (folders). The questions includes file in folders as requirement."
      },
      {
        "date": "2020-08-04T22:21:00.000Z",
        "voteCount": 2,
        "content": "I too agree . because of  small files per minute , Which is not ideal for Datalake . Correct answer is BLOB STORAGE."
      },
      {
        "date": "2020-10-15T14:29:00.000Z",
        "voteCount": 5,
        "content": "Question say folder, not containers.  I think correct answer is ADLS"
      },
      {
        "date": "2020-10-23T05:39:00.000Z",
        "voteCount": 2,
        "content": "Small files issue was with DLS Gen 1. I think because Gen 2 is using Blob Storage in the background, it is not the case with Gen 2 any more. So, the given solution is correct."
      },
      {
        "date": "2020-11-04T02:20:00.000Z",
        "voteCount": 7,
        "content": "ADLS Gen 2 is everything Blob is, plus hierarchical capabilities"
      },
      {
        "date": "2020-07-01T04:48:00.000Z",
        "voteCount": 3,
        "content": "There is many small files, and file types are text types, what means the DLS solution, which is BigData like storage is a bad idea. DLS distributed file storage like big files and types of Parquet (columnar optimized). So the correct answer should be Blob Storage in my opinion"
      },
      {
        "date": "2020-07-18T07:06:00.000Z",
        "voteCount": 33,
        "content": "Folders = ADLS"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38885-exam-dp-201-topic-1-question-25-discussion/",
    "body": "You are designing an Azure Cosmos DB database that will support vertices and edges.<br>Which Cosmos DB API should you include in the design?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCassandra",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGremlin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable"
    ],
    "answer": "C",
    "answerDescription": "The Azure Cosmos DB Gremlin API can be used to store massive graphs with billions of vertices and edges.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/graph-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-19T01:18:00.000Z",
        "voteCount": 15,
        "content": "1000000% is C Gremlin API.\nWhen we talk about edges/nodes and relationships, its Gremlin API"
      },
      {
        "date": "2021-02-22T04:21:00.000Z",
        "voteCount": 2,
        "content": "True. It's also used in Socai n/ws , Recommendation engines , Geospatial, IoT"
      },
      {
        "date": "2020-12-05T04:24:00.000Z",
        "voteCount": 4,
        "content": "100% is C"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38886-exam-dp-201-topic-1-question-26-discussion/",
    "body": "You are designing a big data storage solution. The solution must meet the following requirements:<br>\u2711 Provide unlimited account sizes.<br>\u2711 Support a hierarchical file system.<br>\u2711 Be optimized for parallel analytics workloads.<br>Which storage solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache HBase in Azure HDInsight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB"
    ],
    "answer": "A",
    "answerDescription": "Azure Data Lake Storage is optimized performance for parallel analytics workloads<br>A key mechanism that allows Azure Data Lake Storage Gen2 to provide file system performance at object storage scale and prices is the addition of a hierarchical namespace. This allows the collection of objects/files within an account to be organized into a hierarchy of directories and nested subdirectories in the same way that the file system on your computer is organized.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-05T04:32:00.000Z",
        "voteCount": 16,
        "content": "100% is A"
      },
      {
        "date": "2021-07-13T08:37:00.000Z",
        "voteCount": 1,
        "content": "ADLS Gen 2 is the correct"
      },
      {
        "date": "2021-05-17T12:24:00.000Z",
        "voteCount": 2,
        "content": "Keyword is \"Hierarchical\", so it should be ADLS Gen 2"
      },
      {
        "date": "2021-05-01T04:29:00.000Z",
        "voteCount": 2,
        "content": "Azure data lake storage gen2"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17980-exam-dp-201-topic-1-question-27-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.<br>You need to configure data access so that users see only the files in their respective department folder.<br>Solution: From the storage account, you enable a hierarchical namespace, and you use RBAC.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Disable the hierarchical namespace. And instead of RBAC use access control lists (ACLs).<br>Note: Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.<br>Blob container ACLs does not support the hierarchical namespace, so it must be disabled.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-08T02:15:00.000Z",
        "voteCount": 30,
        "content": "RBAC -&gt; Container level.\nACL -&gt; Each file and directory in your account.\n*NO*"
      },
      {
        "date": "2020-06-20T23:23:00.000Z",
        "voteCount": 9,
        "content": "'No' is correct. When you set ACL, if RBAC is enabled on that container, it takes precedence over ACL. So, RBAC should be disabled when using ACL"
      },
      {
        "date": "2021-05-01T04:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: No\nAzure RBAC : Storage accounts, containers. Cross resource Azure role assignments at subscription or resource group level."
      },
      {
        "date": "2021-04-07T00:46:00.000Z",
        "voteCount": 1,
        "content": "Answer: No\nAzure RBAC\t: Storage accounts, containers. Cross resource Azure role assignments at subscription or resource group level.\n\nACL\t: Directory, file"
      },
      {
        "date": "2021-01-05T11:27:00.000Z",
        "voteCount": 1,
        "content": "Data lake gen2 with hierarchical namespace support ACLS . Currently we can not set it up from storage explorer and portal.\n\nSupport for setting access control lists (ACLs) recursively\nThe ability to apply ACL changes recursively from parent directory to child items is generally available. In the current release of this capability, you can apply ACL changes by using PowerShell, Azure CLI, and the .NET, Java, and Python SDK. Support is not yet available for the Azure portal, or Azure Storage Explorer."
      },
      {
        "date": "2020-12-08T07:09:00.000Z",
        "voteCount": 1,
        "content": "Answer given is correct\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-namespace\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2020-12-02T13:13:00.000Z",
        "voteCount": 5,
        "content": "Answer: No\nThe only correct case: ACL &amp; HNS enabled.\n\n\nAzure RBAC and ACL both require the user (or application) to have an identity in Azure AD. Azure RBAC lets you grant \"coarse-grain\" access to storage account data, such as read or write access to all of the data in a storage account, while ACLs let you grant \"fine-grained\" access, such as write access to a specific directory or file.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model"
      },
      {
        "date": "2020-09-13T17:53:00.000Z",
        "voteCount": 1,
        "content": "Ans: NO. \nGeneral-purpose V2 --&gt;Blob container ACL- Not yet supported\nYou can set ACLs on the root folder of the container but not the container itself.\n\nCan't use ACL in data lake. (can't use in HNS enabled storage account)\nRef: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-supported-blob-storage-features"
      },
      {
        "date": "2020-09-13T18:03:00.000Z",
        "voteCount": 1,
        "content": "Sorry, please ignore the first one.\n\nFor Gen2 - can use ACL with HNS\nref: https://docs.microsoft.com/en-us/azure/storage/blobs/recursive-access-control-lists?tabs=azure-powershell"
      },
      {
        "date": "2020-08-11T15:13:00.000Z",
        "voteCount": 1,
        "content": "No. We need ACL."
      },
      {
        "date": "2020-07-10T01:38:00.000Z",
        "voteCount": 4,
        "content": "HNS should not be disabled. \"Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\" (https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control)"
      },
      {
        "date": "2020-06-20T23:25:00.000Z",
        "voteCount": 1,
        "content": "ACLs are granular and only evaluated when RBAC  if permissions aren't granted with RBAC."
      },
      {
        "date": "2020-05-08T23:03:00.000Z",
        "voteCount": 4,
        "content": "answer should be yes. In RBAC, minimum level of scope to implement security is at container level. Folder level auth is not possible. It needs ACL for that. No reason to disable HNS (data lake ) for that, we can use POSIX permissions provided by data lake to implement folder level permissions."
      },
      {
        "date": "2020-05-21T05:11:00.000Z",
        "voteCount": 3,
        "content": "Isn't the solution saying that RBAC is wrong? If at folder level Auth, RBAC is not possible, then No is correct.  Thoughts?"
      },
      {
        "date": "2020-06-10T14:23:00.000Z",
        "voteCount": 1,
        "content": "Yes that makes sense to me"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47832-exam-dp-201-topic-1-question-28-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.<br>You need to configure data access so that users see only the files in their respective department folder.<br>Solution: From the storage account, you disable a hierarchical namespace, and you use RBAC (role-based access control).<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead of RBAC use access control lists (ACLs).<br>Note: Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.<br>Blob container ACLs does not support the hierarchical namespace, so it must be disabled.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-25T22:30:00.000Z",
        "voteCount": 3,
        "content": "In ADLS, once the hierarchal name space is enabled. It can't be disabled."
      },
      {
        "date": "2021-09-23T12:50:00.000Z",
        "voteCount": 1,
        "content": "The answer is NO but the explanation should be updated. ADLS support ACLs and hierarchy cannot be disabled on Azure Blob Storage once it was enabled"
      },
      {
        "date": "2021-05-01T04:32:00.000Z",
        "voteCount": 3,
        "content": "Answer: No\nAzure RBAC : Storage accounts, containers. Cross resource Azure role assignments at subscription or resource group level."
      },
      {
        "date": "2021-03-20T14:50:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17750-exam-dp-201-topic-1-question-29-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.<br>You need to configure data access so that users see only the files in their respective department folder.<br>Solution: From the storage account, you disable a hierarchical namespace, and you use access control lists (ACLs).<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.<br>Blob container ACLs does not support the hierarchical namespace, so it must be disabled.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues https://docs.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-27T00:57:00.000Z",
        "voteCount": 37,
        "content": "sometimes u guys commenting confuse people"
      },
      {
        "date": "2020-04-01T03:26:00.000Z",
        "voteCount": 35,
        "content": "The question is unclear in this instance, as it doesn't specify whether the ADLS is v1 or v2.  For v1, Hierarchical namespaces must be off, for v2 they need to be on: \n\n\"Do I have to enable support for ACLs?\nNo. Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\"\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2020-04-06T06:54:00.000Z",
        "voteCount": 6,
        "content": "You are correct. I believe this is an old question, before Gen2 was available. In current exams, they ought to specify which Gen they are referring to."
      },
      {
        "date": "2020-10-23T05:51:00.000Z",
        "voteCount": 7,
        "content": "I think Gen 1 is not covered in the exams any more. So having the assumption that the question talks about Gen 2, the answer here is No."
      },
      {
        "date": "2021-08-12T04:04:00.000Z",
        "voteCount": 1,
        "content": "Solution: From the storage account, you disable a hierarchical namespace, and you use access control lists (ACLs).\n\nif disable hierarchical namespace , then the case has to be NO"
      },
      {
        "date": "2021-06-28T08:38:00.000Z",
        "voteCount": 1,
        "content": "enable hierarchical namespace, then access control lists"
      },
      {
        "date": "2021-06-23T05:19:00.000Z",
        "voteCount": 1,
        "content": "Enable HNS and ACL -- This is 100% correct"
      },
      {
        "date": "2021-05-27T20:47:00.000Z",
        "voteCount": 1,
        "content": "One the storage account is created. We can't enable or disable Namespace. The storage account must be re-created. I don't understand why the answer is Yes. It should be no in my opinion."
      },
      {
        "date": "2021-05-20T20:10:00.000Z",
        "voteCount": 1,
        "content": "From the question standpoint, it is pertaining to ADLS Gen 2 in which is it requires to enable the \"hierarchical namespace\" to utilize the functionality of Data Lake then we could configure the ACL in the folder level. Therefore, the answer is NO."
      },
      {
        "date": "2021-04-27T00:59:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer should be \"YES\":\nThe requirement is that data is organized into folders (hence, you have to enable hierarchical namespace\") and the users should only see their respective folders. The only way to give users fine-grained access to folders in ADLS Gen2 is to use Access Control Lists. If this is not used you will have to use RBAC and this can only give access to ALL of the data in a storage account or ALL of the data in the container, which will not fulfill the requirement. \n\nIt is also unlikely that Shared Access Signatures (SAS) should be used. The reason is that this is internal and you want to have a concept of who actually access what (and they likely have users set up in AAD). SAS is more often used in the context of applications than users, and therefore this is not the right answer either. Hence, hierarchical namespace and ACL should be used and the answer to this question is \"YES\"."
      },
      {
        "date": "2021-05-19T17:09:00.000Z",
        "voteCount": 1,
        "content": "what about this ?\nDo I have to enable support for ACLs?\nNo. Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\n\nIf HNS is turned OFF, the Azure Azure RBAC authorization rules still apply.\nin the below link?\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2021-01-31T09:52:00.000Z",
        "voteCount": 4,
        "content": "Refer: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nAccess control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\n\nIf HNS is turned OFF, the Azure Azure RBAC authorization rules still apply."
      },
      {
        "date": "2021-05-19T17:10:00.000Z",
        "voteCount": 1,
        "content": "that is exactly what I found \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2021-01-24T00:40:00.000Z",
        "voteCount": 1,
        "content": "isn't hierarchical namespace a fundamental property of data lake storage that separates it from blob storage type? why are they saying disable HNS then?"
      },
      {
        "date": "2021-06-01T19:57:00.000Z",
        "voteCount": 1,
        "content": "in ADLS Gen 1, there is no such feature that could disable the HNS while in Gen 2 this is possible."
      },
      {
        "date": "2021-01-18T04:45:00.000Z",
        "voteCount": 1,
        "content": "The Question n is out dated, it refresh to gen1. In gen2 there is no need to Disable Hierarchical Namespace"
      },
      {
        "date": "2021-01-11T00:57:00.000Z",
        "voteCount": 3,
        "content": "In storage V2, you can only create ACL's on a container with hierarchical namespace enabled. You cannot disable hierarchical namespace and have an ACL at the same time. Hence, the goal is not met. \nTest this yourself in Azure. Create two storage accounts: one with hierarchical namespace disabled (the \"blob account\"), and one with it enabled (the \"data lake account\"). Create a container in each. Install Azure Data Explorer on your local machine, then follow the instructions on this page: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-explorer#managing-access\nYou will see that ACL's are an option on the data lake container, but not on the blob container. Hence, disabling the hierarchical namespace makes it impossible to have an ACL on the containers in that account. The configuration as given in the question is therefore not meeting the goal."
      },
      {
        "date": "2021-05-19T17:07:00.000Z",
        "voteCount": 1,
        "content": "Do I have to enable support for ACLs?\nNo. Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\n\nIf HNS is turned OFF, the Azure Azure RBAC authorization rules still apply.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2020-12-10T14:19:00.000Z",
        "voteCount": 1,
        "content": "It clearly says the data is arranged into folders by department. If you don't have HNS you don't have the folders."
      },
      {
        "date": "2020-12-08T07:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is No; enable not disable the namespace"
      },
      {
        "date": "2020-09-13T18:02:00.000Z",
        "voteCount": 1,
        "content": "New update: https://docs.microsoft.com/en-us/azure/storage/blobs/recursive-access-control-lists?tabs=azure-powershell"
      },
      {
        "date": "2020-08-29T11:02:00.000Z",
        "voteCount": 1,
        "content": "Question clearly states Azure Data Lake Storage. Why he is talking about blob?"
      },
      {
        "date": "2020-08-10T02:24:00.000Z",
        "voteCount": 14,
        "content": "Enable heirarchial namespace and use ACL's \nThis is the one of the option i got in recent exam."
      },
      {
        "date": "2020-08-27T05:28:00.000Z",
        "voteCount": 1,
        "content": "whats the answer"
      },
      {
        "date": "2020-10-08T04:14:00.000Z",
        "voteCount": 4,
        "content": "This is what I found @ MS Docs:\n'Access control via ACLs is enabled for a storage account as long as the Hierarchical Namespace (HNS) feature is turned ON.\n\nIf HNS is turned OFF, the Azure RBAC authorization rules still apply.'\n\nRef: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16623-exam-dp-201-topic-1-question-30-discussion/",
    "body": "You plan to store 100 GB of data used by a line-of-business (LOB) app.<br>You need to recommend a data storage solution for the data. The solution must meet the following requirements:<br>\u2711 Minimize storage costs.<br>\u2711 Natively support relational queries.<br>\u2711 Provide a recovery time objective (RTO) of less than one minute.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage"
    ],
    "answer": "D",
    "answerDescription": "Incorrect Answers:<br>A: Azure Cosmos DB would require an SQL API.",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-17T18:57:00.000Z",
        "voteCount": 90,
        "content": "Should it not be SQL Database?\nrelational queries are supported in SQL DB and SQL DWH, however if cost becomes a factor, it should be SQL DB."
      },
      {
        "date": "2020-12-19T01:21:00.000Z",
        "voteCount": 23,
        "content": "Yes, answer is B. Azure SQL Database.\n\nCosmos DB - expensive\nSQL DW - expensive\nBlob Storage - cheap, but doesn't support SQL relational queries"
      },
      {
        "date": "2020-03-27T21:18:00.000Z",
        "voteCount": 36,
        "content": "The answer should be B.  Azure SQL Database manual database failover can be achieved in 30s.   https://docs.microsoft.com/en-us/azure/sql-database/sql-database-business-continuity"
      },
      {
        "date": "2020-05-13T16:06:00.000Z",
        "voteCount": 2,
        "content": "Yes, and it can be stored Point in time recovery with fastest. So Ans should be SQL DB"
      },
      {
        "date": "2022-06-26T23:37:00.000Z",
        "voteCount": 1,
        "content": "Option D. Azure Blob Storage    is correct answer 1000 percent sure.   Not B."
      },
      {
        "date": "2021-09-16T09:37:00.000Z",
        "voteCount": 1,
        "content": "I also believe it is B - Azure SQL Database.  FYI: SkillCertPro has the wrong answer!  Very confusing.  :-("
      },
      {
        "date": "2021-06-19T22:49:00.000Z",
        "voteCount": 2,
        "content": "should be Azure SQL DB"
      },
      {
        "date": "2021-05-21T00:53:00.000Z",
        "voteCount": 1,
        "content": "Relational queries support is only provided by a Azure SQL Database"
      },
      {
        "date": "2021-05-20T20:12:00.000Z",
        "voteCount": 1,
        "content": "Definitely B is the answer"
      },
      {
        "date": "2021-04-28T13:18:00.000Z",
        "voteCount": 2,
        "content": "B. Azure SQL Database"
      },
      {
        "date": "2021-04-09T11:18:00.000Z",
        "voteCount": 2,
        "content": "Blob storage is correct.\n-- minimum cost\n-- support SQL Relational queries by creating an external table\n-- hot  tier to recover fast"
      },
      {
        "date": "2021-06-02T00:27:00.000Z",
        "voteCount": 3,
        "content": "Natively support relational queries. hence Azure SQL DB"
      },
      {
        "date": "2021-03-23T03:00:00.000Z",
        "voteCount": 1,
        "content": "The Query Blob Contents API applies a simple Structured Query Language (SQL) statement on a blob's contents and returns only the queried subset of the data.\nso BLOB is correct"
      },
      {
        "date": "2020-12-08T07:14:00.000Z",
        "voteCount": 2,
        "content": "https://azure.microsoft.com/en-us/updates/azure-sql-db-published-first-in-industry-business-continuity-sla-for-a-relational-database-service/:\n\"100% SLA for a 30 second recovery time objective (RTO)\""
      },
      {
        "date": "2020-11-04T04:16:00.000Z",
        "voteCount": 5,
        "content": "Azure SQL DB .... 100%"
      },
      {
        "date": "2020-11-03T14:44:00.000Z",
        "voteCount": 4,
        "content": "Mark it for myself, SQL DB"
      },
      {
        "date": "2020-08-19T19:46:00.000Z",
        "voteCount": 3,
        "content": "It should be sql database instead of blob"
      },
      {
        "date": "2020-08-08T02:25:00.000Z",
        "voteCount": 10,
        "content": "Azure SQL Database\nDataStorage : Around 1TB (Max).\nRelational queries : Native support.\nRTO (Manual failover) : 30seconds."
      },
      {
        "date": "2020-07-27T07:42:00.000Z",
        "voteCount": 1,
        "content": "B does not require sql api support"
      },
      {
        "date": "2020-07-19T05:01:00.000Z",
        "voteCount": 2,
        "content": "The answer should be B.\n\nAzure SQL Database Business Critical tier configured with geo-replication has a guarantee of Recovery time objective (RTO) of 30 sec for 100% of deployed hours.\n\nhttps://azure.microsoft.com/en-us/support/legal/sla/sql-database/v1_4/#:~:text=of%20deployed%20hours.-,Azure%20SQL%20Database%20Business%20Critical%20tier%20configured%20with%20geo%2Dreplication,for%20100%25%20of%20deployed%20hours."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50318-exam-dp-201-topic-1-question-31-discussion/",
    "body": "HOTSPOT -<br>You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03774/0007700001.png\" class=\"in-exam-image\"><br>All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB.<br>Which type of table should you use for each table? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0007800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0007900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Replicated -<br>Replicated tables are ideal for small star-schema dimension tables, because the fact table is often distributed on a column that is not compatible with the connected dimension tables. If this case applies to your schema, consider changing small dimension tables currently implemented as round-robin to replicated.<br><br>Box 2: Replicated -<br><br>Box 3: Replicated -<br><br>Box 4: Hash-distributed -<br>For Fact tables use hash-distribution with clustered columnstore index. Performance improves when two hash tables are joined on the same distribution column.<br>Reference:<br>https://azure.microsoft.com/en-us/updates/reduce-data-movement-and-make-your-queries-more-efficient-with-the-general-availability-of-replicated-tables/ https://azure.microsoft.com/en-us/blog/replicated-tables-now-generally-available-in-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-17T08:10:00.000Z",
        "voteCount": 13,
        "content": "correct"
      },
      {
        "date": "2021-05-17T12:30:00.000Z",
        "voteCount": 12,
        "content": "Small Dimension Tables --&gt; Replicated\nLarge Fact Tables --&gt; Hash Distributed"
      },
      {
        "date": "2021-06-18T05:47:00.000Z",
        "voteCount": 1,
        "content": "all the dimension tables are replicated since they are below 2G. \nThe fact table should be round-robin in my view"
      },
      {
        "date": "2021-05-30T18:15:00.000Z",
        "voteCount": 1,
        "content": "In case if the dimension tables are larger than 2GB, should it be \"Round-Robin\" or still \"Replicated\" is a good option"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47833-exam-dp-201-topic-1-question-32-discussion/",
    "body": "You are designing a data storage solution for a database that is expected to grow to 50 TB. The usage pattern is singleton inserts, singleton updates, and reporting.<br>Which storage solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database elastic pools",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB that uses the Gremlin API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Hyperscale"
    ],
    "answer": "D",
    "answerDescription": "A Hyperscale database is an Azure SQL database in the Hyperscale service tier that is backed by the Hyperscale scale-out storage technology. A Hyperscale database supports up to 100 TB of data and provides high throughput and performance, as well as rapid scaling to adapt to the workload requirements. Scaling is transparent to the application \u05d2\u20ac\" connectivity, query processing, etc. work like any other Azure SQL database.<br>Incorrect Answers:<br>A: SQL Database elastic pools are a simple, cost-effective solution for managing and scaling multiple databases that have varying and unpredictable usage demands. The databases in an elastic pool are on a single Azure SQL Database server and share a set number of resources at a set price. Elastic pools in Azure<br>SQL Database enable SaaS developers to optimize the price performance for a group of databases within a prescribed budget while delivering performance elasticity for each database.<br>B: Rather than SQL Data Warehouse, consider other options for operational (OLTP) workloads that have large numbers of singleton selects.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-service-tier-hyperscale-faq",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-20T14:58:00.000Z",
        "voteCount": 9,
        "content": "correct answer"
      },
      {
        "date": "2021-10-17T07:59:00.000Z",
        "voteCount": 1,
        "content": "synapse support singleton operations..."
      },
      {
        "date": "2021-07-13T08:51:00.000Z",
        "voteCount": 1,
        "content": "Singlenton operations = transactional operations, so only relational databases supports it. Hyperscale supports up to 100TB."
      },
      {
        "date": "2021-05-22T03:23:00.000Z",
        "voteCount": 1,
        "content": "can anyone tell what 'singleton inserts, singleton updates' mean? I've been looking on the internet but no solid definition"
      },
      {
        "date": "2021-05-24T08:41:00.000Z",
        "voteCount": 1,
        "content": "Go through the ACID concept"
      },
      {
        "date": "2021-05-24T10:49:00.000Z",
        "voteCount": 1,
        "content": "Would take that to mean single row inserts, or updates"
      },
      {
        "date": "2021-05-21T01:38:00.000Z",
        "voteCount": 1,
        "content": "azure synapse is better for OLAP purposes while for OLTP, use of Azure SQL is the best choice. as the requirement states it requires to perform insert and update."
      },
      {
        "date": "2021-04-05T14:07:00.000Z",
        "voteCount": 1,
        "content": "But isn't Datawarehouse (Synapse Analytics) a right choice considering that we have table geometries (hash distribution) to efficiently perform inserts updates and deletes?"
      },
      {
        "date": "2021-06-21T05:13:00.000Z",
        "voteCount": 1,
        "content": "No DWH is not a place for singleton updates"
      },
      {
        "date": "2021-04-09T04:35:00.000Z",
        "voteCount": 10,
        "content": "Keyword here is 100TB based on the support link."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18498-exam-dp-201-topic-1-question-33-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution that will use Azure Table storage. The solution will log records in the following entity.<br><img src=\"/assets/media/exam-media/03774/0008100001.png\" class=\"in-exam-image\"><br>You are evaluating which partition key to use based on the following two scenarios:<br>\u2711 Scenario1: Minimize hotspots under heavy write workloads.<br>\u2711 Scenario2: Ensure that date lookups are as efficient as possible for read workloads.<br>Which partition key should you use for each scenario? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0008200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0008300001.png\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-22T11:54:00.000Z",
        "voteCount": 78,
        "content": "Scenerio1: DepartmentName+EmployeeID"
      },
      {
        "date": "2020-05-18T09:49:00.000Z",
        "voteCount": 1,
        "content": "Even if 1-2 departments have more employees than other departments, practically not all employees will not sign-up for events all at a time."
      },
      {
        "date": "2020-05-17T09:07:00.000Z",
        "voteCount": 20,
        "content": "thats the answer as per\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-design-guide#solution-11"
      },
      {
        "date": "2020-08-09T05:12:00.000Z",
        "voteCount": 23,
        "content": "Scenario 1: Department+EmployeeID = avoids hotspots on inserts which happen at the same time\nScenario 2: Year+month+day+hour+EventID = date is included as a string for date lookups\n\nWrong\nDatetime: The partition key value (For example: \"Andrew\"). The partition key value can be of string or numeric types.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-design-guide#solution-11"
      },
      {
        "date": "2020-09-29T14:15:00.000Z",
        "voteCount": 43,
        "content": "Correct final answers.....!!\n\n1: Department+EmployeeID = avoids hotspots on inserts which happen at the same time\n2: Year+month+day+hour+EventID = date is included as a string for date lookups"
      },
      {
        "date": "2021-03-24T06:47:00.000Z",
        "voteCount": 2,
        "content": "I agree with the given answer.\n\"You could also partition your data by a Date or DateTime attribute (or some part of).\" (https://trycatch.me/data-partitioning-strategy-in-cosmosdb/), so datetime field can be used as partition key."
      },
      {
        "date": "2021-03-24T06:44:00.000Z",
        "voteCount": 2,
        "content": "Not sure about DepartmentName+EmployeeID: the department name could change."
      },
      {
        "date": "2020-06-25T06:48:00.000Z",
        "voteCount": 2,
        "content": "For Scenario 2 : is it Year-Month-Day-Hour-EventID ??"
      },
      {
        "date": "2020-06-21T01:18:00.000Z",
        "voteCount": 2,
        "content": "The timestamp is of milliseconds precision, in that case, it won't lead to hot partitions."
      },
      {
        "date": "2020-08-11T15:51:00.000Z",
        "voteCount": 4,
        "content": "Can\u2019t set date time type as partition key. \nEither string or numeric type."
      },
      {
        "date": "2020-05-27T05:55:00.000Z",
        "voteCount": 1,
        "content": "Surname cannot be a partition key because ~750 records have a null value."
      },
      {
        "date": "2020-05-08T23:44:00.000Z",
        "voteCount": 1,
        "content": "choosing timestamp would create multiple partitions and affects insert operations. composite key could be the right choice here"
      },
      {
        "date": "2020-04-24T07:08:00.000Z",
        "voteCount": 7,
        "content": "DepartmentName+EmployeeID could still result in hot partitions as there might be departments with many more employees than others. I'd say both scenarios would have a combination of \"Year+month+day+hour_EventID\" to suffice the requirements, and then a \"rowkey\" would be used to distinguish between the two.\n\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage#r"
      },
      {
        "date": "2020-05-08T23:31:00.000Z",
        "voteCount": 1,
        "content": "the scenario is for write heavy workload, on a certain hour there could be many events causing hot spots."
      },
      {
        "date": "2020-05-16T23:09:00.000Z",
        "voteCount": 2,
        "content": "for each event it would be return in a separate partition since we are adding event id to the partition key. that would introduce write latency. dept + empid makes logical.  can be the answer."
      },
      {
        "date": "2020-04-15T11:37:00.000Z",
        "voteCount": 6,
        "content": "Timestamp would actually be the worst possible partition option for hotspots.  Will result in automatic range partitioning, causing all writes to go to a single partition:  https://docs.microsoft.com/en-us/rest/api/storageservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage"
      },
      {
        "date": "2020-04-24T03:41:00.000Z",
        "voteCount": 2,
        "content": "Besides, you have to use string values for PartitionKey, and \"EventTimestamp\" is a datetime type. I don't think you could use it at all unless you convert it to string."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17751-exam-dp-201-topic-1-question-34-discussion/",
    "body": "DRAG DROP -<br>You have data on the 75,000 employees of your company. The data contains the properties shown in the following table.<br><img src=\"/assets/media/exam-media/03774/0008400001.png\" class=\"in-exam-image\"><br>You need to store the employee data in an Azure Cosmos DB container. Most queries on the data will filter by the Current Department and the Employee<br>Surname properties.<br>Which partition key and item ID should you use for the container? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0008400002.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0008500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Partition key: Current Department<br><br>Item ID: Employee ID -<br>Reference:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/designing-a-scalable-partitioning-strategy-for-azure-table-storage",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-13T12:33:00.000Z",
        "voteCount": 80,
        "content": "I think the partition key should be Department rather than Surname. The reason for this is the read latency. As the question stated, \"most of query would filter by Current Department or Employer Surname\". Having Surname as partition key, you would have 40,000 partition (40,000 unique value), and when you filter your query by Department, you query will need to go through 40,000 partition which would be real bad on performance. Other another hand, having Department as partition key, you would have 25 partition, and to filter surname query, it would be much faster compare to query on 40,000 partition."
      },
      {
        "date": "2020-04-21T23:51:00.000Z",
        "voteCount": 1,
        "content": "Yepp, i agree too. 40k partitions can't be good."
      },
      {
        "date": "2020-12-05T06:58:00.000Z",
        "voteCount": 2,
        "content": "Current department suggests it's something can change, therefore it can't be parition key. Reasoning that we shouldn't use Surname because it will result in many logical partitions is completely wrong. It's even clearly stated in docs that even item id (with only unique) values is a valid option (however here Surname is more appropriate as we use it a predicate in queries)."
      },
      {
        "date": "2020-12-10T13:15:00.000Z",
        "voteCount": 4,
        "content": "Wrong, Department can change thus it cant be a partitioning key, moreover your argument is that it would be better if partitioning key would be Department and filtering on Surname because you would need to access only 25 partitions. Yes 25 partition of thousand of values! It is bad either way... but the better one is surname."
      },
      {
        "date": "2020-05-16T10:49:00.000Z",
        "voteCount": 57,
        "content": "From https://docs.microsoft.com/en-gb/azure/cosmos-db/partitioning-overview:\n\"For all containers, your partition key should:\nBe a property that has a value which does not change. If a property is your partition key, you can't update that property's value.\nHave a high cardinality. In other words, the property should have a wide range of possible values.\nSpread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions.\"\nFirstly, \"Current Department\" is something that could change. Secondly, \"25\" is not high cardinality, and does not guarantee even distribution of data. E.g. if that was a huge IT company, 50k could be in the Engineering department, 50 in HHRR, 50 in MKT, etc.\nSo I think it should be \"Surname\" and EmployeeID."
      },
      {
        "date": "2021-11-30T12:11:00.000Z",
        "voteCount": 1,
        "content": "The 'Sales' department will not change it's name.  An employee may transfer from 'Sales' to 'Engineering' but that causes no issue to the partitioning."
      },
      {
        "date": "2021-04-05T14:23:00.000Z",
        "voteCount": 1,
        "content": "agree 100%"
      },
      {
        "date": "2021-06-30T06:52:00.000Z",
        "voteCount": 1,
        "content": "No. It can't be as \"Surname\" as its values are populated only 99%. Will empty value in Partition key works? I think we need a column that is 100% populated. it can be EmpoyeID or the current department. \nSo, I am thinking to go with Key: Employer ID &amp; Item Id: current department"
      },
      {
        "date": "2020-04-02T06:25:00.000Z",
        "voteCount": 31,
        "content": "i think the answer should be partition by surname ( as it has more unique values than department) and employeeId as itemid since it's unique."
      },
      {
        "date": "2023-06-20T19:36:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is correct. if you see the data populated column, for the surname it's 99%. A partition key column should not have null values. for department it's 100%, hence department is the best choice here"
      },
      {
        "date": "2021-10-21T03:09:00.000Z",
        "voteCount": 1,
        "content": "Isn't employee ID a good candidate for partitioning ?"
      },
      {
        "date": "2021-09-24T08:19:00.000Z",
        "voteCount": 1,
        "content": "The anwser is wrong ! look at this  because the Department could change. \nhttps://docs.microsoft.com/en-gb/azure/cosmos-db/partitioning-overview#choose-partitionkey\nThe answer would be good for a large containers. But in this use case we have a small one. Partition strategy depends on the container size. Since we do not have an Read-Heavy container. We shoud use a property that does not change. Partion key = EmployeeID . (both Department and Surnames can change).\n\" For large read-heavy containers, however, you might want to choose a partition key that appears frequently as a filter in your queries. Queries can be efficiently routed to only the relevant physical partitions by including the partition key in the filter predicate.\nIf most of your workload's requests are queries and most of your queries have an equality filter on the same property, this property can be a good partition key choice.\""
      },
      {
        "date": "2021-08-09T10:24:00.000Z",
        "voteCount": 1,
        "content": "The partition key should be employee_id. From the documentation (https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview): \n\nFor all containers, your partition key should:\n\n    Be a property that has a value which does not change. If a property is your partition key, you can't update that property's value.\n\n    Have a high cardinality. In other words, the property should have a wide range of possible values.\n\n    Spread request unit (RU) consumption and data storage evenly across all logical partitions. This ensures even RU consumption and storage distribution across your physical partitions.\ncurrentDepartment has a low cardinality and can change. Lastname can change (people get married) and 1% has null lastname, which creates one large partition and thus uneven distribution. The same documentation states: \"For small read-heavy containers or write-heavy containers of any size, the item ID is naturally a great choice for the partition key.\". This container certainly qualifies as small, it's just 75.000 employee records."
      },
      {
        "date": "2021-05-05T03:36:00.000Z",
        "voteCount": 3,
        "content": "lot of confusion here. what is the correct answer?"
      },
      {
        "date": "2021-05-17T06:45:00.000Z",
        "voteCount": 6,
        "content": "pk: Employee Surname\nid: Employee Id"
      },
      {
        "date": "2021-02-27T03:14:00.000Z",
        "voteCount": 5,
        "content": "I agree with the given solution partion key - current dept\nitem id - emp id"
      },
      {
        "date": "2020-12-09T01:20:00.000Z",
        "voteCount": 2,
        "content": "The answer given is correct.\nPut aside all the theory and concepts about partitioning and just think about it:\nA company has different departments and each department has its own employees. Between name/surname and ID, ID is definitely the better identifier."
      },
      {
        "date": "2020-12-07T17:27:00.000Z",
        "voteCount": 9,
        "content": "partition key should be current department(populated 100%) as surname is only 99% populated. we cannot have  a partition key as NULL/ not populated."
      },
      {
        "date": "2021-06-01T00:08:00.000Z",
        "voteCount": 3,
        "content": "Finally someone said it"
      },
      {
        "date": "2021-07-06T05:15:00.000Z",
        "voteCount": 2,
        "content": "But also department is a field that can chagne quite easily, which is something that partitions cannot do. So ultimately this question sucks, but if at gunpoint I had to take one of them, I'd go with Surname."
      },
      {
        "date": "2020-10-29T07:09:00.000Z",
        "voteCount": 1,
        "content": "I suspect it's surname rather than department.  Firstly there are simply too few variants, its also \"current department\" so might is likely to change.  Surname is similarly bad on the changeable nature (assume a woman getting married e.g.) but assuming all else it's better than department."
      },
      {
        "date": "2020-09-14T11:07:00.000Z",
        "voteCount": 3,
        "content": "I agree with the suggested answer, I would also argue that the distribution of names will be highly uneven and would result in partitions of very different sizes, including 40,000 with one unique entry. So Current Department by elimination really."
      },
      {
        "date": "2020-08-27T12:13:00.000Z",
        "voteCount": 2,
        "content": "'Current' department suggests that it changes. Surnames change all the time. As a result neither of them are good choice for partition keys. I believe that leaves Employee ID"
      },
      {
        "date": "2020-08-05T14:34:00.000Z",
        "voteCount": 8,
        "content": "From the docs:\n\nFor all containers, your partition key should:\nBe a property that has a value which does not change. If a property is your partition key, you can't update that property's value.\n\nSo current dept can\u2019t be partition key. So obviously it\u2019s surname.\n\nEmployee ID should be item ID"
      },
      {
        "date": "2020-08-05T14:35:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview"
      },
      {
        "date": "2020-07-11T01:31:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is correct as\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/partitioning-overview#choose-partitionkey\nIf your container could grow to more than a few physical partitions, then you should make sure you pick a partition key that minimizes cross-partition queries. Your container will require more than a few physical partitions when either of the following are true:\n\t\u2022 Your container will have over 30,000 RU's provisioned\nYour container will store over 100 GB of data\n\nsurname has 40,000 values, which \" more than a few physical partitions\", we should pick a partition key that minimizes cross-partition queries and used in filter. which is \"Current Department\""
      },
      {
        "date": "2021-03-16T21:19:00.000Z",
        "voteCount": 2,
        "content": "Your Partition key should be a value that does not change cos you would not be able to change it. More so nothing in this question suggests the size of the database to be so large or would have over 30000 RUs provisioned. Yes, the nulls in the surname and the fact that surname could even change is a concern, but Surname is very unlikely to change compared to Current Department. Current tells us it is even very volatile.\n\nI would rather have the Surname as Partitioning key.\nThanks for raising this point though, it is worth considering too"
      },
      {
        "date": "2020-05-27T05:56:00.000Z",
        "voteCount": 5,
        "content": "Surname cannot be a partition key because ~750 records have a null value."
      },
      {
        "date": "2020-06-10T15:38:00.000Z",
        "voteCount": 1,
        "content": "That's not the worst thing in the world https://sqlstudies.com/2017/05/03/partitioning-on-a-nullable-column/"
      },
      {
        "date": "2020-05-04T03:52:00.000Z",
        "voteCount": 9,
        "content": "I think the answer for Partition Key should be Employee Surname. It has a wider range and more unique values, see:\nhttps://docs.microsoft.com/nl-nl/azure/cosmos-db/partitioning-overview#choose-partitionkey"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46668-exam-dp-201-topic-1-question-35-discussion/",
    "body": "DRAG DROP -<br>You need to design a data architecture to bring together all your data at any scale and provide insights into all your users through the use of analytical dashboards, operational reports, and advanced analytics.<br>How should you complete the architecture? To answer, drag the appropriate Azure services to the correct locations in the architecture. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0008600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0008600002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Ingest: Azure Data Factory -<br><br>Store: Azure Blob storage -<br>Model &amp; Serve: Azure Synapse Analytics<br>Load data into Azure Synapse Analytics.<br>Prep &amp; Train: Azure Databricks.<br>Extract data from Azure Blob storage.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-12T03:36:00.000Z",
        "voteCount": 14,
        "content": "Correct.\nhttps://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/advanced-analytics-on-big-data"
      },
      {
        "date": "2021-05-21T02:04:00.000Z",
        "voteCount": 2,
        "content": "The solution would be different if the link provided is the basis as Azure Synapse nowadays could perform the ADF (ingest) , Databricks (Prep &amp; Train) and Model &amp; Serve. Its like an all-in-one package."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48003-exam-dp-201-topic-1-question-36-discussion/",
    "body": "HOTSPOT -<br>You are designing an enterprise data warehouse in Azure Synapse Analytics that will store website traffic analytic in a star schema.<br>You plan to have a fact table for website visits. The table will be approximately 5 GB.<br>You need to recommend which distribution type and index type to use for the table. The solution must provide the fastest query performance.<br>What should you recommend? To answer, select the appropriate options in the answer area<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0008800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0008900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-index",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-18T01:38:00.000Z",
        "voteCount": 8,
        "content": "The answer is straightforward and correct. Even, there is no need to put a comment here."
      },
      {
        "date": "2021-04-05T14:39:00.000Z",
        "voteCount": 1,
        "content": "How should we decide if it is clustered or a non clustered index? Usually if there are over a million rows (or more than 60 million rows) we use  clustered index, but these aren't mentioned in question."
      },
      {
        "date": "2021-04-26T04:50:00.000Z",
        "voteCount": 8,
        "content": "Clustered indexes and non-clustered indexes only outperform clustered columnstore indexes when a single row needs to be quickly retrieved with extreme speed. So, for highly selective filters this is the right choice. Since this is a star schema where filters may vary, clustered columnstore indexes are the better choice as this generally provides the best overall query performance (and is best for large tables)."
      },
      {
        "date": "2021-06-09T04:36:00.000Z",
        "voteCount": 1,
        "content": "The question also mentions analytics implying that it is not intended for \u201csingle row\u201d queries. So, I agree with this reasoning."
      },
      {
        "date": "2021-03-23T03:21:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't be \"Round Robin\" in the first box? Isn't it more efficient compared to \"Hash\" since avoids computing the partitions?"
      },
      {
        "date": "2021-04-17T08:51:00.000Z",
        "voteCount": 5,
        "content": "no, since \"The solution must provide the fastest query performance.\""
      },
      {
        "date": "2021-04-02T14:27:00.000Z",
        "voteCount": 5,
        "content": "Hash for performance requirement"
      },
      {
        "date": "2021-06-28T08:57:00.000Z",
        "voteCount": 1,
        "content": "loading time: round robin\nquery time: hash"
      },
      {
        "date": "2021-05-21T02:06:00.000Z",
        "voteCount": 1,
        "content": "round-robin is use in staging tables"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39239-exam-dp-201-topic-1-question-37-discussion/",
    "body": "You plan to deploy a reporting database to Azure. The database will contain 30 GB of data. The amount of data will increase by 300 MB each year.<br>Rarely will the database be accessed during the second and third weeks of each month. During the first and fourth week of each month, new data will be loaded each night.<br>You need to recommend a solution for the planned database. The solution must meet the following requirements:<br>\u2711 Minimize costs.<br>\u2711 Minimize administrative effort.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure HDInsight cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Hyperscale",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Business Critical",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database serverless"
    ],
    "answer": "D",
    "answerDescription": "Serverless is a compute tier for single Azure SQL Databases that automatically scales compute based on workload demand and bills for the amount of compute used per second. The serverless compute tier also automatically pauses databases during inactive periods when only storage is billed and automatically resumes databases when activity returns.<br>Incorrect Answers:<br>A: Azure HDInsight is a managed Apache Hadoop service that lets you run Apache Spark, Apache Hive, Apache Kafka, Apache HBase, and more in the cloud.<br>B, C: Azure SQL Database Hyperscale and Azure SQL Database Business Critical are based on SQL Server database engine architecture that is adjusted for the cloud environment in order to ensure 99.99% availability even in the cases of infrastructure failures.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview https://docs.microsoft.com/en-us/azure/hdinsight/ https://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-08T08:08:00.000Z",
        "voteCount": 5,
        "content": "A, B and C are wrong for sure"
      },
      {
        "date": "2020-12-21T16:41:00.000Z",
        "voteCount": 1,
        "content": "SI, es SI"
      },
      {
        "date": "2021-05-22T02:32:00.000Z",
        "voteCount": 3,
        "content": "Keyword \"Minimize administrative effort\" ---&gt; Serverless"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51187-exam-dp-201-topic-1-question-38-discussion/",
    "body": "You are designing a solution for the ad hoc analysis of data in Azure Databricks notebooks. The data will be stored in Azure Blob storage.<br>You need to ensure that Blob storage will support the recovery of the data if the data is overwritten accidentally.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable soft delete.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a resource lock.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable diagnostics logging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse read-access geo-redundant storage (RA-GRS)."
    ],
    "answer": "A",
    "answerDescription": "Soft delete protects blob data from being accidentally or erroneously modified or deleted. When soft delete is enabled for a storage account, blobs, blob versions<br>(preview), and snapshots in that storage account may be recovered after they are deleted, within a retention period that you specify.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/soft-delete-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-29T22:10:00.000Z",
        "voteCount": 9,
        "content": "A is the correct answer because when you enable blob soft delete for a storage account, you specify a retention period for deleted objects of between 1 and 365 days. The retention period indicates how long the data remains available after it is deleted or overwritten."
      },
      {
        "date": "2021-05-24T06:59:00.000Z",
        "voteCount": 1,
        "content": "A. soft delete is the correct answer. It will allow you to recover quickly."
      },
      {
        "date": "2021-05-01T04:48:00.000Z",
        "voteCount": 2,
        "content": "Enable soft delete"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56082-exam-dp-201-topic-1-question-39-discussion/",
    "body": "You are planning a solution that combines log data from multiple systems. The log data will be downloaded from an API and stored in a data store.<br>You plan to keep a copy of the raw data as well as some transformed versions of the data. You expect that there will be at least 2 TB of log files. The data will be used by data scientists and applications.<br>You need to recommend a solution to store the data in Azure. The solution must minimize costs.<br>What storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB"
    ],
    "answer": "A",
    "answerDescription": "To land the data in Azure storage, you can move it to Azure Blob storage or Azure Data Lake Store Gen2. In either location, the data should be stored in text files.<br>PolyBase and the COPY statement can load from either location.<br>Incorrect Answers:<br>B: Azure Synapse Analytics, uses distributed query processing architecture that takes advantage of the scalability and flexibility of compute and storage resources. Use Azure Synapse Analytics transform and move the data.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-elt-data-loading",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-30T05:01:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2021-06-25T19:21:00.000Z",
        "voteCount": 3,
        "content": "Agree with the answer provided."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47163-exam-dp-201-topic-1-question-40-discussion/",
    "body": "You are designing a serving layer for data. The design must meet the following requirements:<br>\u2711 Authenticate users by using Azure Active Directory (Azure AD).<br>\u2711 Serve as a hot path for data.<br>\u2711 Support query scale out.<br>\u2711 Support SQL queries.<br>What should you include in the design?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics"
    ],
    "answer": "B",
    "answerDescription": "Do you need serving storage that can serve as a hot path for your data? If yes, narrow your options to those that are optimized for a speed serving layer. This would be Cosmos DB among the options given in this question.<br>Note: Analytical data stores that support querying of both hot-path and cold-path data are collectively referred to as the serving layer, or data serving storage.<br>There are several options for data serving storage in Azure, depending on your needs:<br>\u2711 Azure Synapse Analytics<br>\u2711 Azure Cosmos DB<br>\u2711 Azure Data Explorer<br><br>Azure SQL Database -<br><img src=\"/assets/media/exam-media/03774/0009200008.png\" class=\"in-exam-image\"><br>\u2711 SQL Server in Azure VM<br>\u2711 HBase/Phoenix on HDInsight<br>\u2711 Hive LLAP on HDInsight<br>\u2711 Azure Analysis Services<br>Incorrect Answers:<br>A, C: Azure Data Lake Storage &amp; Azure Blob storage are not data serving storage in Azure.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/analytical-data-stores",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-08T02:18:00.000Z",
        "voteCount": 19,
        "content": "scaleout+hotspot = cosmosdb\nhotspot = azuresqldb\nscaleout = azure synapse"
      },
      {
        "date": "2021-05-10T09:01:00.000Z",
        "voteCount": 1,
        "content": "Cosmos db is correct one because ( speed servicing not possible in azure Synapse)https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/analytical-data-stores"
      },
      {
        "date": "2021-05-10T09:06:00.000Z",
        "voteCount": 1,
        "content": "Do you need serving storage that can serve as a hot path for your data? If yes, narrow your options to those that are optimized for a speed serving layer  --&gt; in this way speed serving layer supports by cosmos db and not by synapse"
      },
      {
        "date": "2021-03-15T03:29:00.000Z",
        "voteCount": 2,
        "content": "I think that Azure Synapse Analytics could also be an answer for this question."
      },
      {
        "date": "2021-03-21T02:54:00.000Z",
        "voteCount": 11,
        "content": "Synapse is note optimized for speed serving layer."
      },
      {
        "date": "2021-05-23T23:06:00.000Z",
        "voteCount": 1,
        "content": "Agree on this"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51391-exam-dp-201-topic-1-question-41-discussion/",
    "body": "You are designing a storage solution for streaming data that is processed by Azure Databricks. The solution must meet the following requirements:<br>\u2711 The data schema must be fluid.<br>\u2711 The source data must have a high throughput.<br>\u2711 The data must be available in multiple Azure regions as quickly as possible.<br>What should you include in the solution to meet the requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage"
    ],
    "answer": "A",
    "answerDescription": "Azure Cosmos DB is Microsoft's globally distributed, multi-model database. Azure Cosmos DB enables you to elastically and independently scale throughput and storage across any number of Azure's geographic regions. It offers throughput, latency, availability, and consistency guarantees with comprehensive service level agreements (SLAs).<br>You can read data from and write data to Azure Cosmos DB using Databricks.<br>Note on fluid schema:<br>If you are managing data whose structures are constantly changing at a high rate, particularly if transactions can come from external sources where it is difficult to enforce conformity across the database, you may want to consider a more schema-agnostic approach using a managed NoSQL database service like Azure<br>Cosmos DB.<br>Reference:<br>https://docs.databricks.com/data/data-sources/azure/cosmosdb-connector.html https://docs.microsoft.com/en-us/azure/cosmos-db/relational-nosql",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-13T18:15:00.000Z",
        "voteCount": 2,
        "content": "I guess anywhere a question says 'multi region', the answer has to be cosmosdb :)"
      },
      {
        "date": "2021-05-01T04:51:00.000Z",
        "voteCount": 4,
        "content": "Cosmos DB 100%"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46594-exam-dp-201-topic-1-question-42-discussion/",
    "body": "You are designing a log storage solution that will use Azure Blob storage containers.<br>CSV log files will be generated by a multi-tenant application. The log files will be generated for each customer at five-minute intervals. There will be more than<br>5,000 customers. Typically, the customers will query data generated on the day the data was created.<br>You need to recommend a naming convention for the virtual directories and files. The solution must minimize the time it takes for the customers to query the log files.<br>What naming convention should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t{year}/{month}/{day}/{hour}/{minute}/{CustomerID}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t{year}/{month}/{day}/{CustomerID}/{hour}/{minute}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t{minute}/{hour}/{day}/{month}/{year}/{CustomeriD}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t{CustomerID}/{year}/{month}/{day}/{hour}/{minute}.csv"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-15T03:42:00.000Z",
        "voteCount": 42,
        "content": "In my opinion, option \"D\" would be the right one."
      },
      {
        "date": "2021-05-23T23:58:00.000Z",
        "voteCount": 4,
        "content": "Referencing the link that was provided in the solution, it was stated in the blob path that it started using the \"profile name\" then proceed with the datetime stamp. It make sense that 'D' is the appropriate answer in this question."
      },
      {
        "date": "2021-04-10T05:57:00.000Z",
        "voteCount": 11,
        "content": "I think B is correct. We want to minimize the time it takes for customers to query log files. 'Typically, the customers will query data generated on the day the data was created'. So it makes sense to include the path for a particular day i.e {Year}/{Month}/{Day} close to the start. Once we have reached a particular day then we will want to filter for a particular Customer so  {Year}/{Month}/{Day}/{CustomerID}. Then we will want to aggregate down to hour and minute. The only other viable option will be D. The reason I think {CustomerID} should NOT be at the beginning of the path is in the case a Customer wants to query data related to multiple CustomerIDs on the same day."
      },
      {
        "date": "2021-09-24T12:14:00.000Z",
        "voteCount": 3,
        "content": "I think the key word is \"Multi-tenant\". It appears to me that the logs for a single customer need to be under its own branch. D is the right answer"
      },
      {
        "date": "2021-08-29T21:10:00.000Z",
        "voteCount": 1,
        "content": "what is correct answer i'm confused between B and D?"
      },
      {
        "date": "2021-06-26T10:03:00.000Z",
        "voteCount": 1,
        "content": "Why now A be the correct answer? On the link - https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs, it's mentioned:\n\nThe name of the blob follows the following naming convention:\n\nresourceId=/SUBSCRIPTIONS/{Subscription Id}/RESOURCEGROUPS/{Resource Group Name}/PROVIDERS/MICROSOFT.CDN/PROFILES/{Profile Name}/ENDPOINTS/{Endpoint Name}/ y={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json\n\ny={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json"
      },
      {
        "date": "2021-06-12T01:25:00.000Z",
        "voteCount": 4,
        "content": "Since it is stated that this is a multi-tenant application, customers would not (and probably should not be able to) query data of other customers. This makes D the right answer. \nMoreover, while it said that typically the queries are done on the same day the data is created, this does not exclude the possibility of making queries that range across multiple days or months. With solution B this becomes unpleasant, since you cannot just query year/month since that will return data of all customers for that month. With solution D all queries are easier, since customerID/year/month returns immediately all the data for that customer of that month. \nBasically, while it is true that both B and D allow for rapid quering of data for a single customer for a single day, B is worse for all queries that want data of more than 1 day."
      },
      {
        "date": "2021-06-19T05:27:00.000Z",
        "voteCount": 1,
        "content": "\"this does not exclude the possibility of making queries \" that is additional assumption made the person who is supposed to answer it."
      },
      {
        "date": "2021-06-09T05:17:00.000Z",
        "voteCount": 1,
        "content": "All of these options are poor in my opinion and therefore hard to choose a \u201cbest\u201d option. If it were me, I\u2019d go with this: {CustomerID}/{year}/{month}/{day}/{CustomerID}_{year}{month}{day}{hour}{minute}.csv. This allows a customer to go directly to their folder and drill down quickly to the day they need. It also has the added benefit of the files being named intelligently and not just a \u201csingle bit of info\u201d.csv. It also allows for easier maintenance down the road when customers leave by allowing you to easily archive or delete their data simply by archiving or deleting their folder. All that being said, I would go with D because I don\u2019t think it is any slower for a customer to search for their data following that path than any of the others and in fact probably quicker. Also, it would provide easier maintenance down the road."
      },
      {
        "date": "2021-06-06T20:49:00.000Z",
        "voteCount": 3,
        "content": "I think, Answer B is correct. This is how you would like to restrict the access. question says, customer will access log information on the same day. So if you organize containers on year - month -day -customer - hour - time way, every customer has to come to day folder of that year and month and go to his container to get logs for the day. \nIf you organize container based on customer - year - month -day - hour - time, every customer has to traverse the long search path to get to day to get the logs. With option B, searching path would be optimum considering requirement"
      },
      {
        "date": "2021-06-09T05:00:00.000Z",
        "voteCount": 1,
        "content": "This logic is flawed because the customer still has to traverse a long search path when they drill down into the folder structure. You either traverse it to begin with or later in the drill down."
      },
      {
        "date": "2021-05-12T13:44:00.000Z",
        "voteCount": 4,
        "content": "I think answer is A"
      },
      {
        "date": "2021-04-27T00:26:00.000Z",
        "voteCount": 4,
        "content": "I am certain that B is wrong. Why should Customer ID be put randomly in between the data formats? \n\nI think D is the right answer and the reason is that each \"/\" takes you to a new directory (folder). As a hierarchy it would make the most sense to have a folder per customer, and then sort by date/time. Source: \"Blob Path Format\" Section here: https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs#blob-path-format"
      },
      {
        "date": "2021-05-21T17:31:00.000Z",
        "voteCount": 4,
        "content": "By the looks of the question overall your argument holds good  however if you read the question carefully it says ... \n1. customers will query data generated on the day the data was created --&gt; means it should start with a year to day granularity then \n2. log files will be generated for each customer at five-minute intervals --&gt; Now you are left with 2 options either organize by customer ID / hr/min or hr/min customer ID , given the case and nothing is explicility mentioned it is safe to assume that queries will be more customer centric and then within customer at a point in time and hence answer A happens to be logically more correct in the context of question !\n{year}/{month}/{day}/{CustomerID}/{hour}/{minute}.csv"
      },
      {
        "date": "2021-04-09T10:22:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct. D is wrong because you duplicate year and month folders. It is also worse option because consumers query data of the day so, when you set the name, you already have all the data you are interested in."
      },
      {
        "date": "2021-04-07T21:32:00.000Z",
        "voteCount": 3,
        "content": "The name of the blob follows the following naming convention:\n\nresourceId=/SUBSCRIPTIONS/{Subscription Id}/RESOURCEGROUPS/{Resource Group Name}/PROVIDERS/MICROSOFT.CDN/PROFILES/{Profile Name}/ENDPOINTS/{Endpoint Name}/ y={Year}/m={Month}/d={Day}/h={Hour}/m={Minutes}/PT1H.json\n\nso it should actually be answer a"
      },
      {
        "date": "2021-03-23T05:24:00.000Z",
        "voteCount": 1,
        "content": "confuse between A and B after reviewing https://docs.microsoft.com/en-us/azure/cdn/cdn-azure-diagnostic-logs feels like why we avoid A here."
      },
      {
        "date": "2021-03-13T17:23:00.000Z",
        "voteCount": 3,
        "content": "Typically, the customers will query data generated on the day the data was created.\nThis line clears query will be specific to date not customer. Or else D would be correct answer"
      },
      {
        "date": "2021-04-09T17:54:00.000Z",
        "voteCount": 1,
        "content": "agree, in this case B is more suitable"
      },
      {
        "date": "2021-03-11T15:42:00.000Z",
        "voteCount": 4,
        "content": "still not clear as query should be optimized for customers - they won't request not their data."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50352-exam-dp-201-topic-1-question-43-discussion/",
    "body": "You are designing an Azure Cosmos DB database that will contain news articles.<br>The articles will have the following properties: Category, Created Datetime, Publish Datetime, Author, Headline, Body Text, and Publish<br>Status. Multiple articles will be published in each category daily, but no two stories in a category will be published simultaneously.<br>Headlines may be updated over time. Publish Status will have the following values: draft, published, updated, and removed. Most articles will remain in the published or updated status. Publish Datetime will be populated only when Publish Status is set to published.<br>You will serve the latest articles to websites for users to consume.<br>You need to recommend a partition key for the database container. The solution must ensure that the articles are served to the websites as quickly as possible.<br>Which partition key should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish Status",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCategory + Created Datetime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHeadline",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish Date + random suffix"
    ],
    "answer": "B",
    "answerDescription": "You can form a partition key by concatenating multiple property values into a single artificial partitionKey property. These keys are referred to as synthetic keys.<br>Incorrect Answers:<br>D: Publish Datetime will be populated only when Publish Status is set to published.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/synthetic-partition-keys",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-18T01:47:00.000Z",
        "voteCount": 6,
        "content": "and the publish status and headline will change"
      },
      {
        "date": "2021-08-09T10:58:00.000Z",
        "voteCount": 2,
        "content": "And publish date as well, from null to some value. Changing partition keys is not allowed, so only possible answer is B"
      },
      {
        "date": "2021-05-24T00:01:00.000Z",
        "voteCount": 3,
        "content": "the propose solution is correct. Publish datetime is not an option here as the partition key should be in string or integer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49915-exam-dp-201-topic-1-question-44-discussion/",
    "body": "You are designing a product catalog for a customer. The product data will be stored in Azure Cosmos DB. The product properties will be different for each product and additional properties will be added to products as needed.<br>Which Cosmos DB API should you use to provision the database?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCassandra API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCore (SQL) API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGremlin API"
    ],
    "answer": "A",
    "answerDescription": "Cassandrsa is a type of NoSQL database.<br>NoSQL database (sometimes called as Not Only SQL) is a database that provides a mechanism to store and retrieve data other than the tabular relations used in relational databases.<br>Incorrect Answers:<br>B: Core (SQL) API  is a relational database which does not fit this scenario.<br>C: Gremlin is the graph traversal language of Apache TinkerPop. Gremlin is a functional, data-flow language that enables users to succinctly express complex traversals on (or queries of) their application's property graph.<br>Reference:<br>https://www.tutorialspoint.com/cassandra/cassandra_introduction.htm",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-13T21:08:00.000Z",
        "voteCount": 33,
        "content": "Ans: Core (SQL) API\nref: \nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog"
      },
      {
        "date": "2021-04-11T10:56:00.000Z",
        "voteCount": 9,
        "content": "As Microsoft recommend, when you create a cosmos dB from scratch and there isn\u2019t any previous work that you could reuse, you should use sql api , unless you need relationships between data, in which case you should use gremnlin."
      },
      {
        "date": "2021-04-13T21:10:00.000Z",
        "voteCount": 1,
        "content": "you are right"
      },
      {
        "date": "2021-05-09T13:59:00.000Z",
        "voteCount": 5,
        "content": "are you sure? \"he product properties will be different for each product and additional properties will be added to products as needed\" indicates NoSQL (Cassandra)"
      },
      {
        "date": "2021-12-29T19:00:00.000Z",
        "voteCount": 1,
        "content": "\"You've decided to look at how the new project is going to store the catalog for your customer facing e-commerce site. The sales team is likely to need support for adding new product categories quickly. The team had issues in the past as the old system that was using a relational database was too structured. Any necessary changes to add properties to products required downtime to update the table schemas, queries, and databases.\"\n\n\"Supporting new product categories is an important requirement for your project, and the Core (SQL) schema is flexible and requires a schemaless data store.\"\n\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog"
      },
      {
        "date": "2021-06-19T08:01:00.000Z",
        "voteCount": 2,
        "content": "\"Cassandra\tThis API isn't a good choice in this particular scenario, because the schema is unknown and will change over time.\"\nhttps://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog"
      },
      {
        "date": "2021-05-24T00:05:00.000Z",
        "voteCount": 1,
        "content": "By all means it is \"SQL API\" \n\nReference: https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/"
      },
      {
        "date": "2021-05-22T04:04:00.000Z",
        "voteCount": 3,
        "content": "Answer is Core (SQL) API. Core (SQL) API is a document database, which is also NoSQL database. It has the name SQL because you can use SQL language to query it, not because it is relational DB"
      },
      {
        "date": "2021-05-13T20:18:00.000Z",
        "voteCount": 2,
        "content": "A. Cassandra API is correct. the product catalog was an example for cassandra api in the MS Learning Path for this exam."
      },
      {
        "date": "2021-06-10T13:40:00.000Z",
        "voteCount": 2,
        "content": "are you sure about that? https://docs.microsoft.com/en-us/learn/modules/choose-api-for-cosmos-db/4-use-the-core-sql-api-to-store-a-product-catalog\n\n\"Cassandra - This API isn't a good choice in this particular scenario, because the schema is unknown and will change over time.\""
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55544-exam-dp-201-topic-1-question-45-discussion/",
    "body": "You work for a finance company.<br>You need to design a business network analysis solution that meets the following requirements:<br>\u2711 Analyzes the flow of transactions between the Azure environments of the company's various partner organizations<br>\u2711 Supports Gremlin (graph) queries<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2"
    ],
    "answer": "A",
    "answerDescription": "Gremlin is one of the most popular query languages for exploring and analyzing data modeled as property graphs. There are many graph-database vendors out there that support Gremlin as their query language, in particular Azure Cosmos DB which is one of the world's first self-managed, geo-distributed, multi-master capable graph databases.<br>Azure Synapse Link for Azure Cosmos DB is a cloud native hybrid transactional and analytical processing (HTAP) capability that enables you to run near real-time analytics over operational data. Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics.<br>Reference:<br>https://jayanta-mondal.medium.com/analyzing-and-improving-the-performance-azure-cosmos-db-gremlin-queries-7f68bbbac2c https://docs.microsoft.com/en-us/azure/cosmos-db/synapse-link-use-cases",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-17T15:47:00.000Z",
        "voteCount": 5,
        "content": "Gremlin API is supported by Cosmos DB only."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48721-exam-dp-201-topic-1-question-46-discussion/",
    "body": "HOTSPOT -<br>You are evaluating the use of an Azure Cosmos DB account for a new database.<br>The proposed account will be configured as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03774/0009700001.jpg\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0009800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0009800002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: vertices and edges -<br>Gremlin API is selected.<br>You can use the Gremlin language to create graph entities (vertices and edges), modify properties within those entities, perform queries and traversals, and delete entities.<br><br>Box 2: US East -<br>The (US) West US is selected as the primary location and geo- redundancy is enabled.<br>The secondary location for West US is East US.<br>Note: When a storage account is created, the customer chooses the primary location for their storage account. However, the secondary location for the storage account is fixed and customers do not have the ability to change this. The following table shows the current primary and secondary location pairings:<br><img src=\"/assets/media/exam-media/03774/0010000001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/gremlin-support https://technet2.github.io/Wiki/blogs/windowsazurestorage/windows-azure-storage-redundancy-options-and-read-access-geo-redundant-storage.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-01T22:04:00.000Z",
        "voteCount": 2,
        "content": "US West is selected in the picture - hence 2nd answer is US West."
      },
      {
        "date": "2021-04-09T18:06:00.000Z",
        "voteCount": 27,
        "content": "Wrong, as explained The (US) West US is selected as the primary location and geo-redundancy is enabled. The secondary location for West US is East US. So East US is the correct answer."
      },
      {
        "date": "2021-06-01T06:01:00.000Z",
        "voteCount": 1,
        "content": "Thats right they fall under the paired regions, hence it is US East."
      },
      {
        "date": "2021-06-08T23:46:00.000Z",
        "voteCount": 1,
        "content": "i agree with the explanation, some of the regions do have corresponding default secondary location whenever the geo-redundancy is enable and if there are no specified location in it."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48917-exam-dp-201-topic-1-question-47-discussion/",
    "body": "You are designing a streaming solution that must meet the following requirements:<br>\u2711 Accept input data from an Azure IoT hub.<br>\u2711 Write aggregated data to Azure Cosmos DB.<br>\u2711 Calculate minimum, maximum, and average sensor readings every five minutes.<br>\u2711 Define calculations by using a SQL query.<br>\u2711 Deploy to multiple environments by using Azure Resource Manager templates.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Spark Streaming",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics"
    ],
    "answer": "C",
    "answerDescription": "Cosmos DB is ideally suited for IoT solutions. Cosmos DB can ingest device telemetry data at high rates.<br><br>Architecture -<br><img src=\"/assets/media/exam-media/03774/0010100001.jpg\" class=\"in-exam-image\"><br><br>Data flow -<br>1. Events generated from IoT devices are sent to the analyze and transform layer through Azure IoT Hub as a stream of messages. Azure IoT Hub stores streams of data in partitions for a configurable amount of time.<br>2. Azure Databricks, running Apache Spark Streaming, picks up the messages in real time from IoT Hub, processes the data based on the business logic and sends the data to Serving layer for storage. Spark Streaming can provide real time analytics such as calculating moving averages, min and max values over time periods.<br>3. Device messages are stored in Cosmos DB as JSON documents.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-08T03:00:00.000Z",
        "voteCount": 32,
        "content": "ARM template can only be used to create databricks workspace but not for the application(notebooks),cluster etc. whereas it can be used for stream analytics. Hence Stream Analytics should be the answer here"
      },
      {
        "date": "2021-04-04T06:57:00.000Z",
        "voteCount": 13,
        "content": "Since the question clearly specifies to define calculations by using a SQL query as well as the points that  minimum, maximum, and average sensor readings are to be calculated every five minutes\nusing Windows functions within the Azure stream analytics would be a straight and preffered option. \nOne can always use Azure Databricks , however for minimal code using SQL and windows functions , the best possible solution ideally should be Azure Stream Analytics."
      },
      {
        "date": "2021-08-15T04:27:00.000Z",
        "voteCount": 1,
        "content": "Azure Databricks, running Apache Spark Streaming, picks up the messages in real time from IoT Hub, processes the data based on the business logic and sends the data to Serving layer for storage."
      },
      {
        "date": "2021-06-27T00:08:00.000Z",
        "voteCount": 1,
        "content": "In the link given there is an alternatives section which states for streaming Stream Analytics could be used as an alternative. So that is another plus to the argument that Stream Analytics should be the answer."
      },
      {
        "date": "2021-06-19T11:38:00.000Z",
        "voteCount": 1,
        "content": "The architectural diagram provided as part of the solution clearly shows that it needs to be databricks although Stream analytics makes more sense. Solution provided is correct - it is databricks"
      },
      {
        "date": "2021-06-27T00:00:00.000Z",
        "voteCount": 2,
        "content": "in the same page alternatives are provided and one is stream analytics. ARM template deploy of jobs are possible there. Where as DBR notebooks cannot be deployed through arm templates"
      },
      {
        "date": "2021-05-24T00:13:00.000Z",
        "voteCount": 4,
        "content": "D. Azure Stream Analytics is the appropriate solution for the requirements"
      },
      {
        "date": "2021-04-04T10:47:00.000Z",
        "voteCount": 1,
        "content": "Azure functions is also present in the architecture. Why incorrect answer then?"
      },
      {
        "date": "2021-04-03T04:14:00.000Z",
        "voteCount": 3,
        "content": "Why not D. Azure Stream analytics ?"
      },
      {
        "date": "2021-04-06T00:43:00.000Z",
        "voteCount": 2,
        "content": "I think because asa jobs queries are not exactly sql. If that si not te case"
      },
      {
        "date": "2021-04-06T00:45:00.000Z",
        "voteCount": 3,
        "content": "I would also choose asa (Azure stream analytics)."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49018-exam-dp-201-topic-1-question-48-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to store delimited text files in an Azure Data Lake Storage account that will be organized into department folders.<br>You need to configure data access so that users see only the files in their respective department folder.<br>Solution: From the storage account, you enable a hierarchical namespace, and you use access control lists (ACLs).<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Azure Data Lake Storage implements an access control model that derives from HDFS, which in turn derives from the POSIX access control model.<br>Blob container ACLs does not support the hierarchical namespace, so it must be disabled.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-known-issues",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-04T01:06:00.000Z",
        "voteCount": 12,
        "content": "Hierarchical namespace, must be enabled, so No.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-acl-dotnet"
      },
      {
        "date": "2021-06-06T02:39:00.000Z",
        "voteCount": 8,
        "content": "Question has \"Enable\",please check"
      },
      {
        "date": "2021-06-21T15:54:00.000Z",
        "voteCount": 7,
        "content": "It says enable, please check the question. Answer is \"Yes\""
      },
      {
        "date": "2021-05-24T00:30:00.000Z",
        "voteCount": 6,
        "content": "The link provided already stated that it requires to  \"ENABLE\" the Hierarchical namespace which is included in the question. The appropriate answer is \"Yes\". Also this configuration is only available in Azure Data Lake Storage Gen 2 (azure storage blob)"
      },
      {
        "date": "2021-05-02T08:04:00.000Z",
        "voteCount": 7,
        "content": "The question saying \"you enable a hierarchical namespace\" though. Did the question change?\nSolution: From the storage account, you enable a hierarchical namespace, and you use access control lists (ACLs)."
      },
      {
        "date": "2021-05-04T10:28:00.000Z",
        "voteCount": 5,
        "content": "Yes you are right question has change"
      },
      {
        "date": "2021-05-17T10:12:00.000Z",
        "voteCount": 7,
        "content": "So, the answer is Yes, right?"
      },
      {
        "date": "2021-06-03T18:04:00.000Z",
        "voteCount": 5,
        "content": "right, the answer is YES"
      },
      {
        "date": "2021-05-17T10:17:00.000Z",
        "voteCount": 9,
        "content": "so, YES is the answer"
      },
      {
        "date": "2021-08-12T21:24:00.000Z",
        "voteCount": 1,
        "content": "This should be Yes as solution Fit the problem statement , \nEnable Hierarchical namespace + ACL"
      },
      {
        "date": "2021-06-27T00:14:00.000Z",
        "voteCount": 1,
        "content": "you enable a hierarchical namespace = then the storage account becomes Gen2\nEnable ACL: Gen2 automatically has ACL\nso the answer is Yes"
      },
      {
        "date": "2021-06-27T00:16:00.000Z",
        "voteCount": 1,
        "content": "sorry ignore this, wrong answer. I cannot delete it. ACL is there in Gen1."
      },
      {
        "date": "2021-04-28T13:17:00.000Z",
        "voteCount": 1,
        "content": "B. No The answer"
      },
      {
        "date": "2021-04-26T02:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is \"NO\"\nHierarchical namespace must be enabled to have a folder structure and actually be an ADLS account (or else it is regular blob)\nHowever, it is correct to use ACLs, as this is the only mechanism to give \"finer grain\" level of access to directories and files. (except Shared Access Signature, but this would make more sense to use for external users for e.g. a limited amount of time) \nSource: https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model"
      },
      {
        "date": "2021-06-06T02:39:00.000Z",
        "voteCount": 1,
        "content": "Question has \"Enable\",please check"
      },
      {
        "date": "2021-04-05T13:56:00.000Z",
        "voteCount": 3,
        "content": "Also, note below from the Azure Documentation. In order to create ADLS account you have to enable Hierarchical option, else it is not ADLS. Hence correct ANSWER is \"NO\"\n\"You'll create a Data Lake Storage Gen2 account the same way you create an Azure Blob store, but with one setting difference. In Advanced, in the Data Lake Storage Gen2 (preview) section, next to Hierarchical namespace, select Enabled.\""
      },
      {
        "date": "2021-06-06T02:39:00.000Z",
        "voteCount": 1,
        "content": "Question has \"Enable\",please check"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53334-exam-dp-201-topic-1-question-49-discussion/",
    "body": "You need to design a solution to support the storage of datasets. The solution must meet the following requirements:<br>\u2711 Send email alerts when new datasets are added.<br>\u2711 Control access to collections of datasets by using Azure Active Directory groups.<br>Support the storage of Microsoft Excel, Comma Separated Values (CSV), and zip files.<br><img src=\"/assets/media/exam-media/03774/0010200003.png\" class=\"in-exam-image\"><br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage<br>Design data processing solutions",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-26T19:40:00.000Z",
        "voteCount": 5,
        "content": "Agreed, the answer is correct."
      },
      {
        "date": "2022-03-26T17:14:00.000Z",
        "voteCount": 1,
        "content": "The answer here should be CosmosDB. Access to datasets needs to be controlled. Storage account only provides container level access security at the most granular level which means all datasets will be available for anyone with access to storage account container and the respective blobs. That does not meet the access security requirement."
      },
      {
        "date": "2021-08-12T21:26:00.000Z",
        "voteCount": 1,
        "content": "ACD are definitely not so B should be the answer"
      },
      {
        "date": "2021-05-22T04:30:00.000Z",
        "voteCount": 4,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "1"
  }
]