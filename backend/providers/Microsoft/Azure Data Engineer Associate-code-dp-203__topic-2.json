[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62034-exam-dp-203-topic-2-question-1-discussion/",
    "body": "HOTSPOT -<br>You plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location.<br>You need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used.<br>What should you include in the Stream Analytics job? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0014700001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0014800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Input type: Stream -<br>You can process real-time IoT data streams with Azure Stream Analytics.<br><br>Function: Geospatial -<br>With built-in geospatial functions, you can use Azure Stream Analytics to build applications for scenarios such as fleet management, ride sharing, connected cars, and asset tracking.<br>Note: In a real-world scenario, you could have hundreds of these sensors generating events as a stream. Ideally, a gateway device would run code to push these events to Azure Event Hubs or Azure IoT Hubs.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-14T08:40:00.000Z",
        "voteCount": 51,
        "content": "Correct solution!"
      },
      {
        "date": "2024-05-17T13:40:00.000Z",
        "voteCount": 1,
        "content": "Agree!"
      },
      {
        "date": "2023-01-13T12:17:00.000Z",
        "voteCount": 18,
        "content": "Answers provided are correct!\nThe input type for the Stream Analytics job should be Stream, as it will be processing real-time data from devices.\nThe function to include in the Stream Analytics job should be Geospatial, which allows you to perform calculations on geographic data and make spatial queries, such as determining the distance between two points. This is necessary to determine if a device has traveled more than 200 meters away from a designated location."
      },
      {
        "date": "2024-03-26T01:35:00.000Z",
        "voteCount": 1,
        "content": "Very clear ,thank you for the explanation !"
      },
      {
        "date": "2024-04-24T19:23:00.000Z",
        "voteCount": 1,
        "content": "Stream it is\nBut streaming must use windowing function, I think"
      },
      {
        "date": "2023-09-04T03:32:00.000Z",
        "voteCount": 1,
        "content": "1-stream\n2-geospatial"
      },
      {
        "date": "2023-08-07T06:23:00.000Z",
        "voteCount": 2,
        "content": "1-stream\n2-geospatial"
      },
      {
        "date": "2023-07-29T17:38:00.000Z",
        "voteCount": 1,
        "content": "this is microsoft link for reference:\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios"
      },
      {
        "date": "2023-05-27T10:12:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2023-05-27T05:21:00.000Z",
        "voteCount": 2,
        "content": "Stream &amp; Geospatial is Correct"
      },
      {
        "date": "2023-05-17T10:42:00.000Z",
        "voteCount": 2,
        "content": "Stream &amp; Geospatial"
      },
      {
        "date": "2023-01-03T15:57:00.000Z",
        "voteCount": 1,
        "content": "I doubt that given solution is correct.\nNo reason to stream the designated location, that is used for lookup. Think of it as a dimension table.\n\nInput type: Reference \nFunction: Geospatial"
      },
      {
        "date": "2022-12-17T07:17:00.000Z",
        "voteCount": 1,
        "content": "Input Type: Stream\nFunction: Geospatial"
      },
      {
        "date": "2022-11-17T13:17:00.000Z",
        "voteCount": 4,
        "content": "Good application and need to learn more about it!"
      },
      {
        "date": "2022-07-30T00:25:00.000Z",
        "voteCount": 4,
        "content": "solution is correct"
      },
      {
        "date": "2022-05-17T09:43:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-04-11T00:34:00.000Z",
        "voteCount": 2,
        "content": "Correct!"
      },
      {
        "date": "2022-01-28T04:43:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60832-exam-dp-203-topic-2-question-2-discussion/",
    "body": "A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream<br>Analytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).<br>You need to optimize performance for the Azure Stream Analytics job.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement event ordering.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Azure Stream Analytics user-defined functions (UDF).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement query parallelization by partitioning the data output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale the SU count for the job up.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale the SU count for the job down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement query parallelization by partitioning the data input.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CF",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "DF",
        "count": 25,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "BF",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-01T02:46:00.000Z",
        "voteCount": 70,
        "content": "Partition input and output.\nREF: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"
      },
      {
        "date": "2021-09-18T17:44:00.000Z",
        "voteCount": 14,
        "content": "Agree. And partitioning Input and output with same number of partitions gives the best performance optimization.."
      },
      {
        "date": "2021-09-21T04:27:00.000Z",
        "voteCount": 15,
        "content": "No event consumer was mentioned. Therefore, partitioning output is not relevant. Answer is correct"
      },
      {
        "date": "2022-05-10T08:17:00.000Z",
        "voteCount": 2,
        "content": "The stream analytics job is the consumer."
      },
      {
        "date": "2021-11-15T11:57:00.000Z",
        "voteCount": 4,
        "content": "Stream analytics ALWAYS has at least one output. There is no need to mention that. So correct answer is input and output"
      },
      {
        "date": "2024-07-13T09:44:00.000Z",
        "voteCount": 1,
        "content": "Partitioning on both input and output can help, but we don't know if the output is a service that doesn't support partitioning like Power BI. Scaling up will always assign more resources at least."
      },
      {
        "date": "2024-07-08T08:09:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT 4o\nC. Implement query parallelization by partitioning the data output:\nOutput Partitioning: By partitioning the data output, you can ensure that the processing load is distributed evenly across multiple nodes, which can significantly improve performance by reducing bottlenecks in data writing.\nF. Implement query parallelization by partitioning the data input:\nInput Partitioning: Partitioning the data input allows the Stream Analytics job to process different partitions in parallel, leading to better utilization of the available streaming units and improved throughput."
      },
      {
        "date": "2024-05-01T07:38:00.000Z",
        "voteCount": 1,
        "content": "C and F &gt; same partitions &gt; embarrassingly parallel processing"
      },
      {
        "date": "2024-04-24T19:32:00.000Z",
        "voteCount": 1,
        "content": "It says optimize performance, does not say that it is bad so adding SU may be unneccesary cost increase. Parallelization and embarrassingly parallel job is correct"
      },
      {
        "date": "2024-04-01T23:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is D &amp; F"
      },
      {
        "date": "2024-03-05T23:20:00.000Z",
        "voteCount": 1,
        "content": "D. Scale the SU count for the job up: Increasing the number of Streaming Units (SUs) can improve the performance of the Stream Analytics job by providing more processing power to handle the incoming data stream.\n\nC. Implement query parallelization by partitioning the data output: Partitioning the data output can help distribute the processing load across multiple partitions, allowing for parallel execution of queries and enhancing performance."
      },
      {
        "date": "2024-02-02T04:01:00.000Z",
        "voteCount": 1,
        "content": "C and D"
      },
      {
        "date": "2024-01-28T02:57:00.000Z",
        "voteCount": 1,
        "content": "C. Implement query parallelization by partitioning the data output:\n\"Partitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it's highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.\"\n\nD. Scale the SU count for the job up:\n\"The total number of streaming units that can be used by a Stream Analytics job depends on the number of steps in the query defined for the job and the number of partitions for each step... All non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step.\""
      },
      {
        "date": "2024-01-16T11:05:00.000Z",
        "voteCount": 2,
        "content": "As there is no indication of any query parallelization currently, we have to choose to parallelize for both input and output as the first/correct answers."
      },
      {
        "date": "2023-12-30T07:28:00.000Z",
        "voteCount": 2,
        "content": "Partitioning lets you divide data into subsets based on a partition key. If your input (for example Event Hubs) is partitioned by a key, it's highly recommended to specify this partition key when adding input to your Stream Analytics job. Scaling a Stream Analytics job takes advantage of partitions in the input and output. A Stream Analytics job can consume and write different partitions in parallel, which increases throughput.\nRef: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"
      },
      {
        "date": "2023-12-27T07:33:00.000Z",
        "voteCount": 2,
        "content": "An embarrassingly parallel job allows the highest degree of parallelization. Looking at how the max number of stream units is calculated, it would not be useful to scale them up if you keep a bottleneck at the output. Unsure what a good reference value would be for the number of SUs, but 120 does not seem very low to me."
      },
      {
        "date": "2023-12-13T02:06:00.000Z",
        "voteCount": 1,
        "content": "Scale the SU count for the job up - (ChatGPT) This will not necessarily improve the performance of your job, unless your query is CPU-bound or memory-bound. Scaling up the SU count will increase the amount of resources available for your job, but it will also increase the cost. You should first try to optimize your query by using parallelization and repartitioning techniques, and then scale up the SU count only if needed1"
      },
      {
        "date": "2023-12-09T12:35:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt say DF :\nThe question in the image relates to optimizing the performance of an Azure Stream Analytics job. The correct actions would typically involve scaling the Streaming Units (SUs) appropriately based on the throughput needs and implementing query parallelization. In this context:\n\n- Scaling up the SU count (option D) would improve performance if the current SU allocation is insufficient.\n- Implementing query parallelization by partitioning the data input (option F) could also optimize performance as it would allow the job to process multiple data partitions concurrently."
      },
      {
        "date": "2023-10-08T11:21:00.000Z",
        "voteCount": 3,
        "content": "I would say DF is correct. Despite C being a correct option to optimize performance, we have no information about the output. If the output is Power BI, it does not support partition. Therefore we cannot state output partition without more information. Therefore best option will be SU"
      },
      {
        "date": "2023-09-30T01:30:00.000Z",
        "voteCount": 3,
        "content": "the question is about  actions should you perform, in case of power bi output , we cannot partition the stream analytics output.SO D &amp; F"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61523-exam-dp-203-topic-2-question-3-discussion/",
    "body": "You need to trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container.<br>Which resource provider should you enable?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft.Sql",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft.Automation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft.EventGrid\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft.EventHub"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-06T09:56:00.000Z",
        "voteCount": 33,
        "content": "Correct. C\nAzure Event Grids \u2013 Event-driven publish-subscribe model (think reactive programming)\nAzure Event Hubs \u2013 Multiple source big data streaming pipeline (think telemetry data)\nIn this case its more suitable vs Event Hubs."
      },
      {
        "date": "2021-09-25T01:19:00.000Z",
        "voteCount": 12,
        "content": "Correct\nhttps://docs.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory"
      },
      {
        "date": "2024-06-26T12:35:00.000Z",
        "voteCount": 1,
        "content": "Data Factory and Synapse pipelines natively integrate with Azure Event Grid, which lets you trigger pipelines on such events.\n\nthe arrival or deletion of a file in Azure Blob Storage account"
      },
      {
        "date": "2023-12-23T07:41:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2023-09-04T03:36:00.000Z",
        "voteCount": 1,
        "content": "custom trigger use  Event-grid"
      },
      {
        "date": "2023-08-26T00:41:00.000Z",
        "voteCount": 1,
        "content": "Correct. C"
      },
      {
        "date": "2023-08-26T00:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-05-27T05:23:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-03-06T21:51:00.000Z",
        "voteCount": 2,
        "content": "To trigger an Azure Data Factory pipeline when a file arrives in an Azure Data Lake Storage Gen2 container, you should enable the Microsoft.EventGrid resource provider.\n\nMicrosoft.EventGrid is an event-based publish/subscribe service that allows you to easily route events between different Azure services. By subscribing to the blob-created event in an Azure Data Lake Storage Gen2 container, you can trigger an Azure Data Factory pipeline whenever a file arrives in the container.\n\nTherefore, the correct answer is C. Microsoft.EventGrid."
      },
      {
        "date": "2022-12-17T07:29:00.000Z",
        "voteCount": 2,
        "content": "Usually while triggering an event using ADF, there is event-based trigger.\n\nApart from that, ADF is well integrated with Azure Event Grid, which lets us trigger pipelines on an event."
      },
      {
        "date": "2022-11-11T02:38:00.000Z",
        "voteCount": 1,
        "content": "Why it's not Microsoft.Automation?"
      },
      {
        "date": "2022-11-17T13:30:00.000Z",
        "voteCount": 1,
        "content": "Would that be part of the Power Platform instead?"
      },
      {
        "date": "2022-07-30T02:07:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-28T05:03:00.000Z",
        "voteCount": 2,
        "content": "Correct."
      },
      {
        "date": "2022-01-18T01:46:00.000Z",
        "voteCount": 1,
        "content": "But EventHub does not support ADLS, only Blob storage"
      },
      {
        "date": "2022-01-18T01:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/event-grid/overview"
      },
      {
        "date": "2022-01-10T08:17:00.000Z",
        "voteCount": 1,
        "content": "C. is correct.\nYou need storage event trigger (for this Microsoft.EventGrid service needs to be enabled)."
      },
      {
        "date": "2021-11-04T20:19:00.000Z",
        "voteCount": 4,
        "content": "Why not eventhub?"
      },
      {
        "date": "2021-09-04T07:05:00.000Z",
        "voteCount": 4,
        "content": "Absolutely correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61345-exam-dp-203-topic-2-question-4-discussion/",
    "body": "You plan to perform batch processing in Azure Databricks once daily.<br>Which type of Databricks cluster should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHigh Concurrency",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tautomated\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tinteractive"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T08:59:00.000Z",
        "voteCount": 16,
        "content": "Correct!"
      },
      {
        "date": "2023-09-04T03:39:00.000Z",
        "voteCount": 1,
        "content": "Automatic/Jobs - best for jobs and automated batch processing."
      },
      {
        "date": "2023-05-27T05:24:00.000Z",
        "voteCount": 2,
        "content": "Given Answer and explanation is correct"
      },
      {
        "date": "2023-01-25T11:11:00.000Z",
        "voteCount": 4,
        "content": "Databricks makes a distinction between all-purpose clusters and job clusters. You use all-purpose clusters to analyze data collaboratively using interactive notebooks. You use job clusters to run fast and robust automated jobs.\n\nYou can create an all-purpose cluster using the UI, CLI, or REST API. You can manually terminate and restart an all-purpose cluster. Multiple users can share such clusters to do collaborative interactive analysis.\n\nThe Databricks job scheduler creates a job cluster when you run a job on a new job cluster and terminates the cluster when the job is complete. You cannot restart a job cluster."
      },
      {
        "date": "2022-09-24T01:08:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/clusters/configure\nas per above link: Datablocks has 3 modes of cluster"
      },
      {
        "date": "2022-07-30T02:12:00.000Z",
        "voteCount": 1,
        "content": "right answer"
      },
      {
        "date": "2022-06-05T23:29:00.000Z",
        "voteCount": 2,
        "content": "Its correct answer"
      },
      {
        "date": "2022-05-04T20:46:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-29T04:04:00.000Z",
        "voteCount": 1,
        "content": "correct."
      },
      {
        "date": "2021-09-01T21:54:00.000Z",
        "voteCount": 2,
        "content": "What is automated cluster ?"
      },
      {
        "date": "2022-01-10T08:19:00.000Z",
        "voteCount": 2,
        "content": "Job cluster"
      },
      {
        "date": "2021-09-04T09:52:00.000Z",
        "voteCount": 19,
        "content": "There are 2 types of databricks clusters:\n1) Standard/Interactive - best for querying and processing data by users.\n2) Automatic/Jobs - best for jobs and automated batch processing."
      },
      {
        "date": "2022-07-20T14:33:00.000Z",
        "voteCount": 2,
        "content": "hi, may I ask where I can find these words?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61527-exam-dp-203-topic-2-question-5-discussion/",
    "body": "HOTSPOT -<br>You are processing streaming data from vehicles that pass through a toll booth.<br>You need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10-minute window.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0015200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0015300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: MAX -<br>The first step on the query finds the maximum time stamp in 10-minute windows, that is the time stamp of the last event for that window. The second step joins the results of the first query with the original stream to find the event that match the last time stamps in each window.<br>Query:<br><br>WITH LastInWindow AS -<br>(<br><br>SELECT -<br><br>MAX(Time) AS LastEventTime -<br><br>FROM -<br><br>Input TIMESTAMP BY Time -<br><br>GROUP BY -<br>TumblingWindow(minute, 10)<br>)<br><br>SELECT -<br>Input.License_plate,<br>Input.Make,<br><br>Input.Time -<br><br>FROM -<br><br>Input TIMESTAMP BY Time -<br><br>INNER JOIN LastInWindow -<br>ON DATEDIFF(minute, Input, LastInWindow) BETWEEN 0 AND 10<br>AND Input.Time = LastInWindow.LastEventTime<br><br>Box 2: TumblingWindow -<br>Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.<br><br>Box 3: DATEDIFF -<br>DATEDIFF is a date-specific function that compares and returns the time difference between two DateTime fields, for more information, refer to date functions.<br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-26T17:40:00.000Z",
        "voteCount": 30,
        "content": "correct"
      },
      {
        "date": "2021-12-19T00:17:00.000Z",
        "voteCount": 1,
        "content": "Why not Hopping Window??"
      },
      {
        "date": "2022-01-11T05:50:00.000Z",
        "voteCount": 11,
        "content": "Because a hopping window can overlap, and we need the data from 10 minute time frames that DON'T overlap"
      },
      {
        "date": "2023-06-28T05:52:00.000Z",
        "voteCount": 5,
        "content": "it needs 3 parameters in input."
      },
      {
        "date": "2023-01-30T11:19:00.000Z",
        "voteCount": 19,
        "content": "answer is here https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window"
      },
      {
        "date": "2023-07-07T22:34:00.000Z",
        "voteCount": 1,
        "content": "You are great!"
      },
      {
        "date": "2023-12-19T03:04:00.000Z",
        "voteCount": 2,
        "content": "The referenced link shows the provided answer is correct"
      },
      {
        "date": "2023-12-29T10:09:00.000Z",
        "voteCount": 1,
        "content": "I agree with the person who said this says to define the last event in that HOUR for the 10 minute window increments.  So it should only return one value. This will return more than one value, as it returns the last values in EACH 10 minute window, not the maximum timestamp for a defined HOUR."
      },
      {
        "date": "2023-12-14T05:28:00.000Z",
        "voteCount": 3,
        "content": "error in code: \nwrong:   DATEDIFF( minut, Input, LastInWindow)\ncorrect: DATEDIFF( minut, Input, LastEventTime)"
      },
      {
        "date": "2024-03-06T07:41:00.000Z",
        "voteCount": 2,
        "content": "Not according to Microsoft: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window\nSpecial thanks to @GodfreyMbizo"
      },
      {
        "date": "2023-10-04T11:04:00.000Z",
        "voteCount": 1,
        "content": "DATEDIFF used in the SELECT statement uses the general syntax where a datetime column or expression is passed in as the second and third parameter. However, when the DATEDIFF function is used inside the JOIN condition, the input_source name or its alias is used. Internally the timestamp associated for each event in that source is picked."
      },
      {
        "date": "2023-09-05T03:37:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-11T05:59:00.000Z",
        "voteCount": 3,
        "content": "max / tumblingwindow/datediff"
      },
      {
        "date": "2023-01-29T09:46:00.000Z",
        "voteCount": 6,
        "content": "May be a dumb question but why the datediff condition when the other condition is exactly matching on timestamp. Is it not unnecessary?"
      },
      {
        "date": "2023-12-16T02:57:00.000Z",
        "voteCount": 1,
        "content": "The reason for using DATEDIFF in this context is to create a condition for joining the two streams based on a time window. If you directly join on Input.Time = LastInWindow.LastEventTime, you are essentially checking for an exact match of timestamps. This might not capture events that are close to each other in time but are not exactly equal.\n\nBy using DATEDIFF, you allow for a time range (0 to 10 minutes) within which events will be considered as part of the same window. This ensures that events occurring slightly before or after the last event in the window are included in the result."
      },
      {
        "date": "2023-12-31T10:28:00.000Z",
        "voteCount": 1,
        "content": "it's AND Input.Time = LastInWindow.LastEventTime...i'm thinking this condition would invalidate anything that's only close in time, but not an exact match"
      },
      {
        "date": "2023-04-21T03:33:00.000Z",
        "voteCount": 2,
        "content": "exactly my thought too; we already have the unique timestamp per 10 min windows, so why simply not match the car's (event) timestamp with the max? what is the value added of the datediff"
      },
      {
        "date": "2022-11-29T18:03:00.000Z",
        "voteCount": 2,
        "content": "Are \"Input\" and \"LastInWindow\"  DateTime fields, to be comapred with datediff???"
      },
      {
        "date": "2022-11-30T04:54:00.000Z",
        "voteCount": 4,
        "content": "I have the answer here:\nhttps://learn.microsoft.com/en-us/stream-analytics-query/join-azure-stream-analytics"
      },
      {
        "date": "2022-09-05T20:04:00.000Z",
        "voteCount": 1,
        "content": "CORRECT"
      },
      {
        "date": "2022-08-14T22:40:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-08-02T14:01:00.000Z",
        "voteCount": 3,
        "content": "The full example with the answer is here: \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#return-the-last-event-in-a-window"
      },
      {
        "date": "2022-07-19T11:35:00.000Z",
        "voteCount": 1,
        "content": "Correct! \nThe keyword \"each\" in \"last vehicle passed during each 10-minute window\" pretty much makes it Clear!"
      },
      {
        "date": "2022-06-28T18:17:00.000Z",
        "voteCount": 3,
        "content": "I understand the first two, but why datediff? They ask for the hour that the last vehicle went through, shouldn't that be datepart?"
      },
      {
        "date": "2022-11-17T01:20:00.000Z",
        "voteCount": 2,
        "content": "In order to match the correct inputs with the last event in window, the difference between both times should not exceed 10 minutes."
      },
      {
        "date": "2022-01-29T04:17:00.000Z",
        "voteCount": 1,
        "content": "correct."
      },
      {
        "date": "2022-01-28T09:22:00.000Z",
        "voteCount": 4,
        "content": "HoppingWindow has a minimum of three arguments whereas TumblingWindow only takes two so considering the solution only has two arguments it has to be Tumbling\n\nhttps://docs.microsoft.com/en-us/stream-analytics-query/hopping-window-azure-stream-analytics\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics"
      },
      {
        "date": "2022-01-02T16:37:00.000Z",
        "voteCount": 2,
        "content": "Answer is 100% correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60393-exam-dp-203-topic-2-question-6-discussion/",
    "body": "You have an Azure Data Factory instance that contains two pipelines named Pipeline1 and Pipeline2.<br>Pipeline1 has the activities shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0015500001.jpg\" class=\"in-exam-image\"><br>Pipeline2 has the activities shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0015500002.jpg\" class=\"in-exam-image\"><br>You execute Pipeline2, and Stored procedure1 in Pipeline1 fails.<br>What is the status of the pipeline runs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipeline1 and Pipeline2 succeeded.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipeline1 and Pipeline2 failed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipeline1 succeeded and Pipeline2 failed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPipeline1 failed and Pipeline2 succeeded."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-08T05:49:00.000Z",
        "voteCount": 49,
        "content": "Correct answer is A. The trick is the fact that pipeline 1 only has a Failure dependency between de activity's. In this situation this results in a Succeeded pipeline if the Stored procedure failed. \n\nIf also the success connection was linked to a follow up activity, and the SP would fail, the pipeline would be indeed marked as failed.\n\nSo A."
      },
      {
        "date": "2022-02-13T12:30:00.000Z",
        "voteCount": 1,
        "content": "well explained! A is right"
      },
      {
        "date": "2023-05-28T01:03:00.000Z",
        "voteCount": 5,
        "content": "correct ,Execute pipeline explained here https://youtu.be/Jkz1dtLrBE4"
      },
      {
        "date": "2021-08-24T13:31:00.000Z",
        "voteCount": 28,
        "content": "Pipeline 2 executes Pipeline 1 if success set variable. Since Pipeline 1 exists it's a success\nPipeline 1 Stored procedure fails. If fails set variable. Since the expected outcome is fail the job runs successfully and sets variable1. \n\nAt least that's how I understand it"
      },
      {
        "date": "2024-07-13T09:53:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-02-08T20:45:00.000Z",
        "voteCount": 1,
        "content": "I chose  D\nThere are two paths defined in the question. \nFor pipeline1, upon failure \nFor pipeline2; Upon success. \nNow ,for pipeline1, to respond to the run of pipeline 2 simply showed  that they  were connected. \n\nThis is a case of  Upon if else block condition.\n\n\nWhen previous activity (procedure)failed : node Upon Success is skipped and its parent node failed; overall pipeline1 failed\n\nWhen previous activity succeeds: node Upon Success succeeded  and node Upon Failure is skipped (and its parent node succeeds); overall pipeline succeeds.\nHence ,pipeline 2 succeeds \n\nThe pipeline2  can be visualized as thus\nP2-&gt;P1"
      },
      {
        "date": "2023-09-05T03:39:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A."
      },
      {
        "date": "2023-08-16T20:23:00.000Z",
        "voteCount": 2,
        "content": "How do we know which is the default behaviour? Its not mentioned anywhere wether there is a success dependency or failure dependency"
      },
      {
        "date": "2023-08-07T06:39:00.000Z",
        "voteCount": 1,
        "content": "OPTION \"A\""
      },
      {
        "date": "2023-05-24T20:36:00.000Z",
        "voteCount": 2,
        "content": "Correct!!!"
      },
      {
        "date": "2023-05-11T05:59:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer"
      },
      {
        "date": "2022-12-02T04:35:00.000Z",
        "voteCount": 1,
        "content": "So the default behaviour is Failed dependency ? If so the answer is A. But it doesn't say this anywhere in the question."
      },
      {
        "date": "2022-09-25T23:30:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct.\nThis article helped: https://www.sqlshack.com/dependencies-in-azure-data-factory/"
      },
      {
        "date": "2022-07-30T02:49:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-03-22T12:47:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-03-16T12:11:00.000Z",
        "voteCount": 5,
        "content": "A correct:\nPipeline 1 is in try catch sentence --&gt; Success\nPipeline 2 --&gt; Success\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#try-catch-block"
      },
      {
        "date": "2023-09-06T01:12:00.000Z",
        "voteCount": 1,
        "content": "Thanks"
      },
      {
        "date": "2022-01-29T04:20:00.000Z",
        "voteCount": 3,
        "content": "A correct. I agree with SaferSephy's comments below."
      },
      {
        "date": "2022-01-16T23:33:00.000Z",
        "voteCount": 2,
        "content": "A is correct. Pipeline 1 is connected to Set variable to Failure node/event. Its like handling exceptions/errors in programming language. Without Failure node, it would be treated as failed."
      },
      {
        "date": "2022-01-02T11:37:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62041-exam-dp-203-topic-2-question-7-discussion/",
    "body": "HOTSPOT -<br>A company plans to use Platform-as-a-Service (PaaS) to create the new data pipeline process. The process must meet the following requirements:<br>Ingest:<br>\u2711 Access multiple data sources.<br>\u2711 Provide the ability to orchestrate workflow.<br>\u2711 Provide the capability to run SQL Server Integration Services packages.<br>Store:<br>\u2711 Optimize storage for big data workloads.<br>\u2711 Provide encryption of data at rest.<br>\u2711 Operate with no size limits.<br>Prepare and Train:<br>\u2711 Provide a fully-managed and interactive workspace for exploration and visualization.<br>\u2711 Provide the ability to program in R, SQL, Python, Scala, and Java.<br>Provide seamless user authentication with Azure Active Directory.<br><img src=\"/assets/media/exam-media/04259/0015600010.png\" class=\"in-exam-image\"><br>Model &amp; Serve:<br>\u2711 Implement native columnar storage.<br>\u2711 Support for the SQL language<br>\u2711 Provide support for structured streaming.<br>You need to build the data integration pipeline.<br>Which technologies should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0015800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0015900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Ingest: Azure Data Factory -<br>Azure Data Factory pipelines can execute SSIS packages.<br>In Azure, the following services and tools will meet the core requirements for pipeline orchestration, control flow, and data movement: Azure Data Factory, Oozie on HDInsight, and SQL Server Integration Services (SSIS).<br><br>Store: Data Lake Storage -<br>Data Lake Storage Gen1 provides unlimited storage.<br>Note: Data at rest includes information that resides in persistent storage on physical media, in any digital format. Microsoft Azure offers a variety of data storage solutions to meet different needs, including file, disk, blob, and table storage. Microsoft also provides encryption to protect Azure SQL Database, Azure Cosmos<br>DB, and Azure Data Lake.<br>Prepare and Train: Azure Databricks<br>Azure Databricks provides enterprise-grade Azure security, including Azure Active Directory integration.<br>With Azure Databricks, you can set up your Apache Spark environment in minutes, autoscale and collaborate on shared projects in an interactive workspace.<br>Azure Databricks supports Python, Scala, R, Java and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch and scikit-learn.<br>Model and Serve: Azure Synapse Analytics<br>Azure Synapse Analytics/ SQL Data Warehouse stores data into relational tables with columnar storage.<br>Azure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse. Access SQL Data<br>Warehouse from Azure Databricks using the SQL Data Warehouse connector.<br>Note: As of November 2019, Azure SQL Data Warehouse is now Azure Synapse Analytics.<br>Reference:<br>https://docs.microsoft.com/bs-latn-ba/azure/architecture/data-guide/technology-choices/pipeline-orchestration-data-movement https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-15T19:32:00.000Z",
        "voteCount": 73,
        "content": "Is this an ad?"
      },
      {
        "date": "2023-10-30T05:19:00.000Z",
        "voteCount": 7,
        "content": "You mean like all the Azure certification exams?"
      },
      {
        "date": "2023-06-30T04:03:00.000Z",
        "voteCount": 5,
        "content": "sounds like one, replace databricks with Azure Fabric and then it will be for sure"
      },
      {
        "date": "2021-09-14T09:05:00.000Z",
        "voteCount": 49,
        "content": "Correct solution!"
      },
      {
        "date": "2024-08-28T02:04:00.000Z",
        "voteCount": 1,
        "content": "\"I don't believe the storage solution is Azure Data Lake. Given that the question mentions PaaS, the Azure Data Encryption at Rest doc from Microsoft states: 'Platform as a Service (PaaS) customer's data typically resides in a storage service such as Blob Storage but may also be cached or stored in the application execution environment, such as a virtual machine.' To see the encryption at rest options available to you, examine the Data encryption models: supporting services table for the storage and application platforms that you use. So I believe the options are: Data Factory, Blob Storage, Databricks, and Synapse.\"\n\nhttps://learn.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest#encryption-at-rest-for-paas-customers"
      },
      {
        "date": "2024-04-18T13:52:00.000Z",
        "voteCount": 1,
        "content": "All correct, congrats!"
      },
      {
        "date": "2023-12-21T14:29:00.000Z",
        "voteCount": 5,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-09-05T03:43:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-09-05T20:07:00.000Z",
        "voteCount": 5,
        "content": "Given answer is correct !"
      },
      {
        "date": "2022-07-30T03:08:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-03-22T12:48:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-03-15T09:14:00.000Z",
        "voteCount": 3,
        "content": "for the store, couldn't we use also Azure Blob Storage? It supports all the three requisites"
      },
      {
        "date": "2022-04-11T00:51:00.000Z",
        "voteCount": 4,
        "content": "Because ADLS Gen2 support Big Data Workload better"
      },
      {
        "date": "2024-01-08T18:17:00.000Z",
        "voteCount": 1,
        "content": "ADLSG2 supports big data and large data storage as compared with blob"
      },
      {
        "date": "2022-02-20T00:36:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-29T04:23:00.000Z",
        "voteCount": 1,
        "content": "Correct solution."
      },
      {
        "date": "2022-01-08T20:40:00.000Z",
        "voteCount": 1,
        "content": "Correct Solution"
      },
      {
        "date": "2022-01-02T06:21:00.000Z",
        "voteCount": 1,
        "content": "for model and server, HDI has all of this. Why DataBricks?"
      },
      {
        "date": "2022-02-21T09:56:00.000Z",
        "voteCount": 3,
        "content": "Support for SQL"
      },
      {
        "date": "2022-02-21T09:58:00.000Z",
        "voteCount": 3,
        "content": "Also seamless integration with AAD"
      },
      {
        "date": "2022-09-18T04:18:00.000Z",
        "voteCount": 3,
        "content": "JAVA only supported in databricks"
      },
      {
        "date": "2021-12-17T11:24:00.000Z",
        "voteCount": 5,
        "content": "Would be best if people including answers that go against the popular responses provide some reference instead of blinding saying false"
      },
      {
        "date": "2021-11-11T00:29:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct.\nAzure Databricks supports java: https://azure.microsoft.com/en-us/services/databricks/#overview"
      },
      {
        "date": "2021-11-02T02:48:00.000Z",
        "voteCount": 5,
        "content": "Databricks doesn't support Java so in the Prep and Train should be HDInsight Apache Spark Cluster"
      },
      {
        "date": "2021-11-11T01:13:00.000Z",
        "voteCount": 11,
        "content": "Azure Databricks supports Python, Scala, R, Java, and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch, and scikit-learn."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60484-exam-dp-203-topic-2-question-8-discussion/",
    "body": "DRAG DROP -<br>You have the following table named Employees.<br><img src=\"/assets/media/exam-media/04259/0016100001.png\" class=\"in-exam-image\"><br>You need to calculate the employee_type value based on the hire_date value.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0016100002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0016200001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: CASE -<br>CASE evaluates a list of conditions and returns one of multiple possible result expressions.<br>CASE can be used in any statement or clause that allows a valid expression. For example, you can use CASE in statements such as SELECT, UPDATE,<br>DELETE and SET, and in clauses such as select_list, IN, WHERE, ORDER BY, and HAVING.<br>Syntax: Simple CASE expression:<br><br>CASE input_expression -<br>WHEN when_expression THEN result_expression [ ...n ]<br>[ ELSE else_result_expression ]<br><br>END -<br><br>Box 2: ELSE -<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/language-elements/case-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-29T05:20:00.000Z",
        "voteCount": 43,
        "content": "Correct"
      },
      {
        "date": "2021-08-24T03:33:00.000Z",
        "voteCount": 9,
        "content": "The answer is correct. But, is this in the scope of this exam?"
      },
      {
        "date": "2022-01-11T20:57:00.000Z",
        "voteCount": 1,
        "content": "it seems"
      },
      {
        "date": "2021-10-28T08:53:00.000Z",
        "voteCount": 12,
        "content": "Got this question yesterday so yes."
      },
      {
        "date": "2021-08-28T19:47:00.000Z",
        "voteCount": 11,
        "content": "make sense to me , data engineer should be able to write Queries"
      },
      {
        "date": "2024-07-24T12:21:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-05T03:43:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-05-27T05:38:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-30T11:29:00.000Z",
        "voteCount": 1,
        "content": "they cant bring hard questions only for exam to balance"
      },
      {
        "date": "2023-01-22T04:00:00.000Z",
        "voteCount": 1,
        "content": "yes the answer is correct but not sure if they will be included the DP-203 exam"
      },
      {
        "date": "2022-11-08T14:05:00.000Z",
        "voteCount": 6,
        "content": "i wish all the questions in the exam were like that one :)"
      },
      {
        "date": "2022-07-30T04:03:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-04-11T00:53:00.000Z",
        "voteCount": 2,
        "content": "the answer is correct\nCASE ...\nWHEN ... THEN...\nELSE ..."
      },
      {
        "date": "2022-01-29T04:25:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61795-exam-dp-203-topic-2-question-9-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Synapse Analytics workspace named WS1.<br>You have an Azure Data Lake Storage Gen2 container that contains JSON-formatted files in the following format.<br><img src=\"/assets/media/exam-media/04259/0016400001.png\" class=\"in-exam-image\"><br>You need to use the serverless SQL pool in WS1 to read the files.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0016500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0016600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: openrowset -<br>The easiest way to see to the content of your CSV file is to provide file URL to OPENROWSET function, specify csv FORMAT.<br>Example:<br>SELECT *<br>FROM OPENROWSET(<br>BULK 'csv/population/population.csv',<br>DATA_SOURCE = 'SqlOnDemandDemo',<br>FORMAT = 'CSV', PARSER_VERSION = '2.0',<br>FIELDTERMINATOR =',',<br>ROWTERMINATOR = '\\n'<br><br>Box 2: openjson -<br>You can access your JSON files from the Azure File Storage share by using the mapped drive, as shown in the following example:<br><br>SELECT book.* FROM -<br>OPENROWSET(BULK N't:\\books\\books.json', SINGLE_CLOB) AS json<br>CROSS APPLY OPENJSON(BulkColumn)<br>WITH( id nvarchar(100), name nvarchar(100), price float,<br>pages_i int, author nvarchar(100)) AS book<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file https://docs.microsoft.com/en-us/sql/relational-databases/json/import-json-documents-into-sql-server",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-11T02:12:00.000Z",
        "voteCount": 46,
        "content": "Answer is correct\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files"
      },
      {
        "date": "2021-09-22T19:17:00.000Z",
        "voteCount": 2,
        "content": "answer is correct as per this link"
      },
      {
        "date": "2021-09-10T05:39:00.000Z",
        "voteCount": 11,
        "content": "The question and answer seem out of place, there was no mention of the CSV and the query in the answer doesn't match up with openjson at all"
      },
      {
        "date": "2021-11-02T21:38:00.000Z",
        "voteCount": 1,
        "content": "agree with you, very misleading"
      },
      {
        "date": "2022-01-17T00:21:00.000Z",
        "voteCount": 1,
        "content": "Look at the WITH statement, the csv column can contain json data."
      },
      {
        "date": "2023-06-24T17:03:00.000Z",
        "voteCount": 3,
        "content": "The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote."
      },
      {
        "date": "2021-11-21T04:32:00.000Z",
        "voteCount": 12,
        "content": "Actually, the csv format is specified if you're using OPENROWSET to read json files in Synapse. The OPENJSON is required if you want to parse data from every array in the document. See the OPENJSON example in this link:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#query-json-files-using-openjson"
      },
      {
        "date": "2021-12-08T07:48:00.000Z",
        "voteCount": 5,
        "content": "Thanks, you're right:\n\"The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote.\""
      },
      {
        "date": "2024-07-14T05:14:00.000Z",
        "voteCount": 1,
        "content": "To complete the Transact-SQL statement for reading JSON-formatted files using the serverless SQL pool in WS1, you should use OPENROWSET to access the data and OPENJSON to parse the JSON content. Here is the correct completion of the statement:"
      },
      {
        "date": "2024-05-28T15:15:00.000Z",
        "voteCount": 1,
        "content": "This took me about 10 hours to understand this query"
      },
      {
        "date": "2024-10-05T16:02:00.000Z",
        "voteCount": 1,
        "content": "Me too, dammit!"
      },
      {
        "date": "2024-01-16T07:24:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#query-json-files-using-openjson"
      },
      {
        "date": "2023-09-05T03:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2023-05-11T06:01:00.000Z",
        "voteCount": 1,
        "content": "openrowset / openjson"
      },
      {
        "date": "2022-10-21T12:08:00.000Z",
        "voteCount": 1,
        "content": "does openjson do the same thing as jsoncontent ? \nI tried running a query on a json file and the auto filled code used jsoncontent instead of openjson"
      },
      {
        "date": "2022-07-30T04:09:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-03-22T12:52:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-29T04:37:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62528-exam-dp-203-topic-2-question-10-discussion/",
    "body": "DRAG DROP -<br>You have an Apache Spark DataFrame named temperatures. A sample of the data is shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0016700001.png\" class=\"in-exam-image\"><br>You need to produce the following table by using a Spark SQL query.<br><img src=\"/assets/media/exam-media/04259/0016800001.png\" class=\"in-exam-image\"><br>How should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all.<br>You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0016800002.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0016900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: PIVOT -<br>PIVOT rotates a table-valued expression by turning the unique values from one column in the expression into multiple columns in the output. And PIVOT runs aggregations where they're required on any remaining column values that are wanted in the final output.<br>Incorrect Answers:<br>UNPIVOT carries out the opposite operation to PIVOT by rotating columns of a table-valued expression into column values.<br><br>Box 2: CAST -<br>If you want to convert an integer value to a DECIMAL data type in SQL Server use the CAST() function.<br>Example:<br><br>SELECT -<br>CAST(12 AS DECIMAL(7,2) ) AS decimal_value;<br>Here is the result:<br>decimal_value<br>12.00<br>Reference:<br>https://learnsql.com/cookbook/how-to-convert-an-integer-to-a-decimal-in-sql-server/ https://docs.microsoft.com/en-us/sql/t-sql/queries/from-using-pivot-and-unpivot",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-26T12:14:00.000Z",
        "voteCount": 46,
        "content": "correct answer, pivot and cast"
      },
      {
        "date": "2021-09-21T21:42:00.000Z",
        "voteCount": 5,
        "content": "correct. cast not convert"
      },
      {
        "date": "2024-03-28T14:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-12-01T10:48:00.000Z",
        "voteCount": 1,
        "content": "Pivot and Cast are correct.\nThere is an issue with the problem though.\nIt should be CAST(avg(temp)...\nand not\nAvg (Cast(temp)..."
      },
      {
        "date": "2023-12-28T04:22:00.000Z",
        "voteCount": 2,
        "content": "No this is not correct, if you CAST(AVG(temp)) then you will first get AVG(temp) as an int. Casting it then results in the decimal value being 0 (like 2.0, 3.0...).\nTherefore, we have to AVG(CAST(temp))."
      },
      {
        "date": "2023-10-08T12:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2023-09-05T03:44:00.000Z",
        "voteCount": 1,
        "content": "pivot and cast"
      },
      {
        "date": "2023-05-27T05:40:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-07-19T06:44:00.000Z",
        "voteCount": 2,
        "content": "correct explanation"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61879-exam-dp-203-topic-2-question-11-discussion/",
    "body": "You have an Azure Data Factory that contains 10 pipelines.<br>You need to label each pipeline with its main purpose of either ingest, transform, or load. The labels must be available for grouping and filtering when using the monitoring experience in Data Factory.<br>What should you add to each pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta resource tag",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta correlation ID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta run group ID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan annotation\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-12T00:27:00.000Z",
        "voteCount": 22,
        "content": "Annotation"
      },
      {
        "date": "2022-01-13T20:15:00.000Z",
        "voteCount": 3,
        "content": "Cause ADF pipelines are not first class resources"
      },
      {
        "date": "2022-03-16T10:27:00.000Z",
        "voteCount": 12,
        "content": "What is the difference between resource tags and annotations?"
      },
      {
        "date": "2023-09-05T03:46:00.000Z",
        "voteCount": 1,
        "content": "D -Annotation"
      },
      {
        "date": "2023-08-07T06:57:00.000Z",
        "voteCount": 1,
        "content": "OPTION -D"
      },
      {
        "date": "2023-05-27T05:40:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-05-24T20:41:00.000Z",
        "voteCount": 1,
        "content": "D is correct!!"
      },
      {
        "date": "2023-03-07T00:52:00.000Z",
        "voteCount": 2,
        "content": "To label each pipeline with its main purpose of either ingest, transform, or load and make the labels available for grouping and filtering when using the monitoring experience in Data Factory, you should add an annotation to each pipeline.\n\nTherefore, the correct answer is D. an annotation.\n\nAnnotations are key-value pairs that you can add to pipelines, datasets, and activities to help you organize and categorize them. They can be used for a variety of purposes, including labeling pipelines with their main purpose of either ingest, transform, or load. Annotations can also be used for filtering, grouping, and searching for resources in the Data Factory monitoring experience."
      },
      {
        "date": "2023-01-22T04:07:00.000Z",
        "voteCount": 3,
        "content": "D -Annotation"
      },
      {
        "date": "2022-12-17T08:09:00.000Z",
        "voteCount": 4,
        "content": "ADF annotations are tags that you can add to your Azure Data Factory components to identify them.\n\nA tag allows you to classify or group different objects in order to easily monitor them after an execution. You can create multiple Azure Data Factory annotations."
      },
      {
        "date": "2022-10-07T02:25:00.000Z",
        "voteCount": 2,
        "content": "hjghfgh"
      },
      {
        "date": "2022-10-07T02:26:00.000Z",
        "voteCount": 4,
        "content": "KIndly delete above comment. Answer is D."
      },
      {
        "date": "2022-07-30T10:47:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-02-20T00:42:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2022-01-29T06:02:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-17T08:37:00.000Z",
        "voteCount": 1,
        "content": "Anotacion"
      },
      {
        "date": "2021-10-06T09:27:00.000Z",
        "voteCount": 2,
        "content": "yes correct, annotation provides label functionality than show in pipeline monitoring."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60391-exam-dp-203-topic-2-question-12-discussion/",
    "body": "HOTSPOT -<br>The following code segment is used to create an Azure Databricks cluster.<br><img src=\"/assets/media/exam-media/04259/0017100001.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0017200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0017200002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Yes -<br>A cluster mode of 'High Concurrency' is selected, unlike all the others which are 'Standard'. This results in a worker type of Standard_DS13_v2.<br><br>Box 2: No -<br>When you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.<br><br>Box 3: Yes -<br>Delta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.<br>Reference:<br>https://adatis.co.uk/databricks-cluster-sizing/<br>https://docs.microsoft.com/en-us/azure/databricks/jobs<br>https://docs.databricks.com/administration-guide/capacity-planning/cmbp.html https://docs.databricks.com/delta/index.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-15T03:52:00.000Z",
        "voteCount": 56,
        "content": "FROM DP-201, thanks to  rmk4ever ::\n\n1. Yes\nA cluster mode of \u2018High Concurrency\u2019 is selected, unlike all the others which are \u2018Standard\u2019. This results in a worker type of Standard_DS13_v2.\nref: https://adatis.co.uk/databricks-cluster-sizing/\n\n2. NO\nrecommended: New Job Cluster.\nWhen you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.\nref: https://docs.microsoft.com/en-us/azure/databricks/jobs\nScheduled batch workload- Launch new cluster via job\nref: https://docs.databricks.com/administration-guide/capacity-planning/cmbp.html#plan-capacity-and-control-cost\n\n3.YES\nDelta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.\nref: https://docs.databricks.com/delta/index.html"
      },
      {
        "date": "2022-04-17T11:54:00.000Z",
        "voteCount": 3,
        "content": "agree on this one"
      },
      {
        "date": "2023-07-28T03:21:00.000Z",
        "voteCount": 2,
        "content": "For 1, where do you see high concurrency?"
      },
      {
        "date": "2023-07-28T03:21:00.000Z",
        "voteCount": 1,
        "content": "I hadn't read the other comments yet, apparently it's in 'serverless' :)"
      },
      {
        "date": "2022-01-02T13:14:00.000Z",
        "voteCount": 39,
        "content": "Answer is Correct.\nBox 1: Yes\n\"spark.databricks.cluster.profile\": \"serverless\" means that the cluster is a High Concurrency Cluster, which support multi-users. \nBox 2: No\nScheduled jobs should run in standard cluster. High Concurrency clusters are intended for multi-users and won\u2019t benefit a cluster running a single job.\nBox 3:Yes"
      },
      {
        "date": "2024-03-19T09:49:00.000Z",
        "voteCount": 1,
        "content": "N, N, Y. 1 : N because that's for single user, shared not support R"
      },
      {
        "date": "2023-09-05T03:48:00.000Z",
        "voteCount": 1,
        "content": "the answer is Yes, No, Yes."
      },
      {
        "date": "2023-08-10T22:37:00.000Z",
        "voteCount": 1,
        "content": "the naming of the clusters are changed in recent UI:\nhttps://docs.databricks.com/en/archive/compute/cluster-ui-preview.html"
      },
      {
        "date": "2023-05-11T06:01:00.000Z",
        "voteCount": 3,
        "content": "yes / no / yes"
      },
      {
        "date": "2022-11-08T14:28:00.000Z",
        "voteCount": 3,
        "content": "i guess this question won't be relevant anymore, since cluster creation UI has changed"
      },
      {
        "date": "2023-04-20T23:44:00.000Z",
        "voteCount": 3,
        "content": "They still use this question, I had this one on my exam this week"
      },
      {
        "date": "2022-07-30T16:46:00.000Z",
        "voteCount": 4,
        "content": "1. should be 'No'. Its a standard cluster and it also has scala which is not supported on High Concurrence cluster."
      },
      {
        "date": "2022-07-30T10:58:00.000Z",
        "voteCount": 2,
        "content": "Yes, No, Yes"
      },
      {
        "date": "2022-01-29T06:10:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer. I agree with Canary_2021"
      },
      {
        "date": "2021-12-08T18:07:00.000Z",
        "voteCount": 3,
        "content": "I would say the answer is Yes, No, Yes. Delta lake was supported starting from Azure Databricks Runtime 6.0 with Scala 2.11.12.  https://docs.microsoft.com/en-us/azure/databricks/release-notes/runtime/6.0#system-environment"
      },
      {
        "date": "2021-10-12T07:27:00.000Z",
        "voteCount": 3,
        "content": "what is the answer lol"
      },
      {
        "date": "2021-10-06T09:43:00.000Z",
        "voteCount": 1,
        "content": "the same question is in DP-201 with the same answer. https://www.examtopics.com/discussions/microsoft/view/16875-exam-dp-201-topic-2-question-11-discussion/"
      },
      {
        "date": "2021-09-25T04:39:00.000Z",
        "voteCount": 1,
        "content": "IMO NO, YES, YES"
      },
      {
        "date": "2021-09-25T04:44:00.000Z",
        "voteCount": 3,
        "content": "Sorry, it should be NO,NO,YES. \nFor Box 2, the cheapest way is creating the cluster when it's time to execute the job and terminate immediately after the task completes. This is called New Job Clusters .\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs"
      },
      {
        "date": "2021-09-10T02:56:00.000Z",
        "voteCount": 2,
        "content": "what is correct answer here please?"
      },
      {
        "date": "2021-09-11T16:30:00.000Z",
        "voteCount": 9,
        "content": "Yes No No"
      },
      {
        "date": "2021-09-09T20:56:00.000Z",
        "voteCount": 2,
        "content": "High Concurrency clusters are intended for use by multiple users. hence correct answer"
      },
      {
        "date": "2021-09-01T03:15:00.000Z",
        "voteCount": 3,
        "content": "NO, NO, YES"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60470-exam-dp-203-topic-2-question-13-discussion/",
    "body": "You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs.<br>You need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-18T16:23:00.000Z",
        "voteCount": 87,
        "content": "My answer will be B\nStream Analytics supports \"extending SQL language with JavaScript and C# user-defined functions (UDFs)\". There is no mention of Python support; hence Stream Analytics is not correct.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction\n\nAzure Databricks supports near real-time data from Azure Event Hubs. And includes support for R, SQL, Python, Scala, and Java. So I will go for option B."
      },
      {
        "date": "2022-02-07T23:26:00.000Z",
        "voteCount": 2,
        "content": "But Python runs on Event Hubs why the other service does should support Python too?"
      },
      {
        "date": "2022-06-15T06:12:00.000Z",
        "voteCount": 2,
        "content": "It's mentioned that \"python runs on real time data from event hubs not on event hubs\". Also event hub is to gather that data and after that it is analyzed by either databricks stream analytics. And since stream analytics doesn't support python so the answer is databricks"
      },
      {
        "date": "2023-06-09T12:42:00.000Z",
        "voteCount": 2,
        "content": "therefore i agree wih ASA"
      },
      {
        "date": "2023-06-09T12:44:00.000Z",
        "voteCount": 1,
        "content": "python can run Event Hubs libraries real time, it doesn't have to be supported by ASA, it just needs to send data to analytics service"
      },
      {
        "date": "2023-12-19T04:11:00.000Z",
        "voteCount": 1,
        "content": "@RoyP654, the question asks which service to perform the statistical analysis (e.g. execute the python) suggesting that the python has not/will not be ran in events hubs"
      },
      {
        "date": "2022-02-07T23:29:00.000Z",
        "voteCount": 16,
        "content": "I'm sure it's Stream Analytics cause Event Hubs already supports Python (https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-python-get-started-send). We don't need the other service to support it. We just need to lower costs. Hence ASA is the correct solution"
      },
      {
        "date": "2023-06-09T12:41:00.000Z",
        "voteCount": 1,
        "content": "the question does not ask which service can run Python, it's asking where to send the data for analytics since Python can run with Event Hubs libraries"
      },
      {
        "date": "2024-09-10T00:16:00.000Z",
        "voteCount": 1,
        "content": "Should be B.\nSee the link below. Under the heading \"When to use other technologies\", it mentions \"Azure Stream Analytics supports user-defined functions (UDF) or user-defined aggregates (UDA) in JavaScript for cloud jobs and C# for IoT Edge jobs. C# user-defined deserializers are also supported. If you want to implement a deserializer, a UDF, or a UDA in other languages, such as Java or Python, you can use Spark Structured Streaming. You can also run the Event Hubs EventProcessorHost on your own virtual machines to do arbitrary streaming processing.\"\nAs the question mentions user-defined-function (UDF) in Python, ASA seems not support UDF in Python. Should use Spark Structured Streaming, which in this case here is Azure Databricks.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/streaming-technologies"
      },
      {
        "date": "2024-07-14T05:55:00.000Z",
        "voteCount": 1,
        "content": "C is wrong, stream analytics is a SQL based analytics"
      },
      {
        "date": "2024-07-08T08:39:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nAzure Databricks is well-suited for real-time data processing and analytics. It provides a collaborative environment for working with Apache Spark, which is ideal for performing complex statistical analyses and machine learning tasks in real-time."
      },
      {
        "date": "2024-05-29T07:19:00.000Z",
        "voteCount": 1,
        "content": "It says we are using python after it is sent to event hubs: \"custom proprietary Python functions on near real-time data FROM Azure Event Hubs\". Yes, we can send events to Event Hub with python but it says that we are running statistical analysis AFTER we send it to Event Hub. Therefore, my answer is Databricks"
      },
      {
        "date": "2024-05-29T04:51:00.000Z",
        "voteCount": 1,
        "content": "Azure Stream Analytics: Azure Stream Analytics is designed for real-time data processing and can directly ingest data from Azure Event Hubs. However, it has limited support for custom Python functions. It is more suitable for simple real-time analytics and transformations rather than complex statistical analysis with custom code.\ncorrect answer: Azure Databricks, we have custom python function"
      },
      {
        "date": "2024-04-24T20:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-03-27T00:56:00.000Z",
        "voteCount": 1,
        "content": "Azure Databricks provides a fast and scalable Apache Spark-based analytics platform that supports Python, among other programming languages. It allows you to perform near real-time data processing and analysis efficiently, making it ideal for scenarios where low latency is a priority. Additionally, it offers seamless integration with Azure Event Hubs, enabling you to ingest data in real-time and apply custom Python functions for statistical analysis."
      },
      {
        "date": "2024-03-05T23:40:00.000Z",
        "voteCount": 1,
        "content": "B. Azure Databricks\n\nAzure Databricks provides a fully managed Apache Spark-based analytics platform that is well-suited for processing and analyzing real-time streaming data. It offers native integration with Azure Event Hubs, allowing you to ingest data in real-time and apply custom Python functions for statistical analysis with minimal latency. Additionally, Databricks provides scalable compute resources, optimized processing capabilities, and support for various programming languages, making it an ideal choice for near real-time data analysis scenarios."
      },
      {
        "date": "2024-02-10T13:26:00.000Z",
        "voteCount": 1,
        "content": "C is correct.\nAt near realtime  ,the window functions in azure stream analytics  can be employed in compute some  statical values (e.g count,maximum,min,avg. etc) of the data streaming from the even hub."
      },
      {
        "date": "2024-02-04T05:55:00.000Z",
        "voteCount": 1,
        "content": "Corrected!!!\n\n FROM Azure Event Hubs, not ON\n\nAzure Databricks"
      },
      {
        "date": "2024-02-04T05:54:00.000Z",
        "voteCount": 1,
        "content": "FROM Azure Event Hubs, not ON"
      },
      {
        "date": "2024-01-28T06:03:00.000Z",
        "voteCount": 3,
        "content": "chatGPT explains - \nAzure Stream Analytics is designed for real-time data stream processing and analytics. It can ingest data from various sources, including Azure Event Hubs, and allows you to run near real-time analytics using a SQL-like language. With Stream Analytics, you can easily apply custom Python functions using user-defined functions (UDFs) and achieve low-latency processing.\n\nAzure Synapse Analytics and Azure Databricks are powerful analytics services, but they are more suitable for complex analytics and big data processing rather than near real-time, low-latency scenarios.\n\nAzure SQL Database is a relational database service and is not specifically designed for real-time stream processing.\n\nTherefore, in this case, Azure Stream Analytics is the recommended choice for minimizing latency in statistical analysis on near real-time data from Azure Event Hubs."
      },
      {
        "date": "2024-01-16T11:20:00.000Z",
        "voteCount": 2,
        "content": "Simply, they always want the Stream Analytics answer.  It's the most straightforward."
      },
      {
        "date": "2024-01-06T12:01:00.000Z",
        "voteCount": 1,
        "content": "Azure Databricks"
      },
      {
        "date": "2023-09-07T22:41:00.000Z",
        "voteCount": 3,
        "content": "From ChatGPT, To minimize latency for statistical analysis on near real-time data from Azure Event Hubs, I recommend using Azure Stream Analytics (Option C). Azure Stream Analytics is designed for real-time data processing and can ingest and analyze data from Event Hubs with low latency, making it a suitable choice for this scenario."
      },
      {
        "date": "2023-12-19T08:07:00.000Z",
        "voteCount": 1,
        "content": "Also from ChatGPT (GPT4) lol:\n\nFor processing near real-time data with custom proprietary Python functions and minimizing latency, the best service would be:\n\nB. Azure Databricks\n\nHere\u2019s why:\n\nAzure Databricks is an Apache Spark-based analytics service that integrates smoothly with Azure services such as Azure Event Hubs. It supports real-time streaming data processing and can execute custom Python code, which is necessary for your custom statistical analysis functions. Databricks is designed to handle large-scale data processing and analytics with low latency, making it suitable for near real-time scenarios.\nThe other services have their uses but may not be the optimal choice for this particular scenario"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67477-exam-dp-203-topic-2-question-14-discussion/",
    "body": "HOTSPOT -<br>You have an enterprise data warehouse in Azure Synapse Analytics that contains a table named FactOnlineSales. The table contains data from the start of 2009 to the end of 2012.<br>You need to improve the performance of queries against FactOnlineSales by using table partitions. The solution must meet the following requirements:<br>\u2711 Create four partitions based on the order date.<br>\u2711 Ensure that each partition contains all the orders placed during a given calendar year.<br>How should you complete the T-SQL command? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0017500001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0017600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Range Left or Right, both are creating similar partition but there is difference in comparison<br>For example: in this scenario, when you use LEFT and 20100101,20110101,20120101<br>Partition will be, datecol&lt;=20100101, datecol&gt;20100101 and datecol&lt;=20110101, datecol&gt;20110101 and datecol&lt;=20120101, datecol&gt;20120101<br>But if you use range RIGHT and 20100101,20110101,20120101<br>Partition will be, datecol&lt;20100101, datecol&gt;=20100101 and datecol&lt;20110101, datecol&gt;=20110101 and datecol&lt;20120101, datecol&gt;=20120101<br>In this example, Range RIGHT will be suitable for calendar comparison Jan 1st to Dec 31st<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql?view=sql-server-ver15",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-19T19:05:00.000Z",
        "voteCount": 27,
        "content": "Answer is correct.\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql?view=sql-server-ver15"
      },
      {
        "date": "2021-12-11T22:57:00.000Z",
        "voteCount": 17,
        "content": "I think the box 2 should be 20090101,2010101,20110101,20120101 since the question asked about 4 partitions. \nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql?view=sql-server-ver15#c-creating-a-range-right-partition-function-on-a-datetime-column"
      },
      {
        "date": "2021-12-15T18:20:00.000Z",
        "voteCount": 30,
        "content": "No! That's wrong! Number of partitions created = Number of partition boundaries specified + 1."
      },
      {
        "date": "2021-12-18T14:43:00.000Z",
        "voteCount": 6,
        "content": "Choosing box 2 with range right would create five partitions. The first partition would be &lt;20090101. So the provided answer is correct"
      },
      {
        "date": "2024-07-26T07:36:00.000Z",
        "voteCount": 1,
        "content": "Incorrect you read the requirement: -Ensure that each partition contains all the orders placed during a given calendar year.- now as you said [time&lt;2009-01-01] goes to the first partition and for sure in the second partition we will have only 2009"
      },
      {
        "date": "2024-07-26T07:32:00.000Z",
        "voteCount": 1,
        "content": "The given answer does not fully meet the requirement:\n\n\"\"Ensure\"\" that each partition contains all the orders placed during a given calendar year.\n\nUsing the values 2010101, 20110101, 20120101 means that all dates before 2010101 will go to the first partition. If, by any chance, a new date from the year 2008 is inserted into the table, it will go to the first partition together with 2009. Therefore, I believe 20090101, 2010101, 20110101, 20120101 is a better answer."
      },
      {
        "date": "2024-04-11T11:55:00.000Z",
        "voteCount": 1,
        "content": "RIGHT - [time&lt;2010-01-01] [2010-01-01&lt;=time&lt;2011-01-01] [2011-01-01&lt;=time&lt;2012-01-01] [time&lt;=2012-01-01]\nLEFT -  [time&lt;=2010-01-01] [2010-01-01&lt;time&lt;=2011-01-01] [2011-01-01&lt;time&lt;=2012-01-01] [time&lt;2012-01-01]"
      },
      {
        "date": "2024-02-10T15:46:00.000Z",
        "voteCount": 1,
        "content": "I stand with the chosen  answer.\nFor me it  is 100% correct \nIf you chose LEFT partition with 20090101,2010101,20110101,\nthen be aware that the fourth partition will not include  1st January 2012  which means the partition doesn't actually supports all the 12 calendar months is a particular year."
      },
      {
        "date": "2023-09-05T06:09:00.000Z",
        "voteCount": 2,
        "content": "RIGHT :\nt&lt;20100101, 20100101&lt;=t&lt;20110101, 20110101&lt;=t&lt;20120101\n20120101&lt;=t\nLEFT\nt&lt;=20100101, 20100101&lt;t&lt;=20110101,20110101&lt;t&lt;=20120101,\nt&gt;20120101"
      },
      {
        "date": "2022-11-23T08:53:00.000Z",
        "voteCount": 4,
        "content": "Answer is \n1.right\n2.  Last option becoz there they mentioned 4 partitions ( I'm sure that it is guarenteed)"
      },
      {
        "date": "2022-11-23T09:01:00.000Z",
        "voteCount": 1,
        "content": "The reason we are using right that is here the values are not null"
      },
      {
        "date": "2022-10-28T22:59:00.000Z",
        "voteCount": 9,
        "content": "IF use [RIGHT], it means : [time&lt;20100101],  [20100101&lt;=time&lt;20110101] and so on\nIF use [LEFT], it means: [time&lt;=20100101], [20100101&lt;time  &lt;= 20110101] and so on\nSee if you choose [LEFT] then will NOT include the 0101 value of current calendar year into the query, so GO with [RIGHT]"
      },
      {
        "date": "2022-07-30T23:18:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct.. given1st boundary value will be included in the 2nd partition (since right) so 1st partition will end at 20091231 and 2nd will start at 20100101 and end at 20101231 and so on.."
      },
      {
        "date": "2022-07-30T11:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2022-07-08T12:37:00.000Z",
        "voteCount": 2,
        "content": "How to remember LEFT / RIGHT concept, it's confusing, pls help"
      },
      {
        "date": "2022-08-26T03:59:00.000Z",
        "voteCount": 7,
        "content": "I do that by initials of L(eft) means starting as \"Less than equal to first boundary value, there onwards greater than \"   and for right, other way around."
      },
      {
        "date": "2022-01-29T06:32:00.000Z",
        "voteCount": 1,
        "content": "correct Answer."
      },
      {
        "date": "2022-01-18T21:15:00.000Z",
        "voteCount": 4,
        "content": "where does 2009 year stored? i think the 1st choice should be LEFT so th"
      },
      {
        "date": "2022-10-01T03:02:00.000Z",
        "voteCount": 3,
        "content": "the answer is correct, check the requirements:\n\u2711 Create four partitions based on the order date.\n\u2711 Ensure that each partition contains all the orders placed during a given calendar year.\nboth right an d left results in 4 partitions, but left will have mixed values from 2 years, check the clarification in the answer.\nso right will result in 4 partions each partition contains all the orders placed during a given calendar year."
      },
      {
        "date": "2022-01-03T06:35:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-12-09T16:56:00.000Z",
        "voteCount": 2,
        "content": "Respuesta correcta. RIGTH,  [3 VALORES]."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61856-exam-dp-203-topic-2-question-15-discussion/",
    "body": "You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool.<br>You have a table that was created by using the following Transact-SQL statement.<br><img src=\"/assets/media/exam-media/04259/0017700001.png\" class=\"in-exam-image\"><br>Which two columns should you add to the table? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveStartDate] [datetime] NOT NULL,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[CurrentProductCategory] [nvarchar] (100) NOT NULL,\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveEndDate] [datetime] NULL,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[ProductCategory] [nvarchar] (100) NOT NULL,",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[OriginalProductCategory] [nvarchar] (100) NOT NULL,\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-11T05:34:00.000Z",
        "voteCount": 27,
        "content": "Correct answer"
      },
      {
        "date": "2022-01-04T00:16:00.000Z",
        "voteCount": 14,
        "content": "Why can't the name of the current ProductCategory be just \"ProductCategory\"? I would say that D and E could be also correct."
      },
      {
        "date": "2022-07-25T12:37:00.000Z",
        "voteCount": 9,
        "content": "because if you look at Type 3 examples, usually there are \"original\" and \"current\"."
      },
      {
        "date": "2024-01-16T11:23:00.000Z",
        "voteCount": 1,
        "content": "This is correct. The ProductCategory may represent a current product category or an original one, but we don't know. Wording on this question is a little off."
      },
      {
        "date": "2023-09-06T06:16:00.000Z",
        "voteCount": 1,
        "content": "BE is Correct"
      },
      {
        "date": "2023-05-27T05:50:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-03-07T01:22:00.000Z",
        "voteCount": 1,
        "content": "To implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool and add the required columns to the existing table, you should add the following columns:\n\nA. [EffectiveStartDate] [datetime] NOT NULL, to track the start date of the current product category value.\nD. [ProductCategory] [nvarchar] (100) NOT NULL, to store the current product category value.\n\nExplanation:\n\nA Type 3 SCD tracks both the current and previous values of a column. For the product category data, you need to store the current product category value, as well as the previous/original value. To achieve this, you need to add the following columns to the existing table:\n\nA. [EffectiveStartDate] [datetime] NOT NULL, to track the start date of the current product category value. This column will store the date when the current product category value became effective.\nD. [ProductCategory] [nvarchar] (100) NOT NULL, to store the current product category value. This column will contain the most recent value for the product category."
      },
      {
        "date": "2023-01-09T15:02:00.000Z",
        "voteCount": 1,
        "content": "BE is how you should answer, but in reality DE is better.  The end user / analyst shouldn't have to remember \"oh, this is a Type 3 SCD field so I need to look under 'C' for CurrentProductCategory instead of 'P' for ProductCategory.\""
      },
      {
        "date": "2022-12-22T06:33:00.000Z",
        "voteCount": 1,
        "content": "Microsoft documentation is pretty confusing (https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types) pictures have no sense.\n\nI find at this link a good explanation:\nhttps://medium.com/geekculture/6-different-types-of-slowly-changing-dimensions-and-how-to-apply-them-b152ef908d4e"
      },
      {
        "date": "2022-11-22T17:42:00.000Z",
        "voteCount": 5,
        "content": "This is the same question as Question #59Topic 1"
      },
      {
        "date": "2022-09-05T20:25:00.000Z",
        "voteCount": 1,
        "content": "Given Answer is correct !!"
      },
      {
        "date": "2022-07-30T11:29:00.000Z",
        "voteCount": 1,
        "content": "BE is correct"
      },
      {
        "date": "2022-07-20T10:16:00.000Z",
        "voteCount": 1,
        "content": "Right Answer!"
      },
      {
        "date": "2022-06-30T04:46:00.000Z",
        "voteCount": 1,
        "content": "the explanation is confusing as email is more likely to be a type2 change, it is also confusing as we need to add the \"original\" category (it should already be there as ProductCategory right?). So my first guess was only to add ProductCategory, but that would not be a scd change, only an initialization of a variable that could be subject to a scd-3 change. SO I guess the only viable option is adding \"original\" and \"new\" categories. I do not like scd3 type changes anyway, maybe there should be a special scd type2 historized scdtype3 dimension ? Okay, too much, lets stick with this answer BE (It says that the change is also metioning the Previous ProductCategory key wasn't there yet, but should have been)"
      },
      {
        "date": "2022-04-17T12:02:00.000Z",
        "voteCount": 1,
        "content": "BE  is correct"
      },
      {
        "date": "2022-03-22T13:08:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-03T06:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-11-26T10:49:00.000Z",
        "voteCount": 4,
        "content": "B and E"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60551-exam-dp-203-topic-2-question-16-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure Stream Analytics solution that will analyze Twitter data.<br>You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.<br>Solution: You use a hopping window that uses a hop size of 10 seconds and a window size of 10 seconds.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 59,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-24T15:18:00.000Z",
        "voteCount": 125,
        "content": "The answer should be \"Yes\". Hopping window with hop size equals window size should be the same as Tumbling window."
      },
      {
        "date": "2023-04-18T01:08:00.000Z",
        "voteCount": 12,
        "content": "A Tumbling Window would be correct. But as stated in the following, a hopping window can be the same as a tumbling window: \"To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.\"\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions#hopping-window"
      },
      {
        "date": "2021-10-12T06:19:00.000Z",
        "voteCount": 22,
        "content": "ensure that each tweet is counted only once"
      },
      {
        "date": "2022-07-15T12:05:00.000Z",
        "voteCount": 8,
        "content": "Correct Ans: B. tumbling window\nRead the question carefully - \"The solution must ensure that each tweet is counted only once.\"\nBy defination, hopping window is not non-overlapping"
      },
      {
        "date": "2022-07-31T04:45:00.000Z",
        "voteCount": 9,
        "content": "a hopping window with equal window size to the hopping is effectively non-overlapping. If the size of the hop was smaller, sure."
      },
      {
        "date": "2021-10-14T10:08:00.000Z",
        "voteCount": 19,
        "content": "Unlike tumbling windows, hopping windows model scheduled overlapping windows. A hopping window specification consist of three parameters: the timeunit, the windowsize (how long each window lasts) and the hopsize (by how much each window moves forward relative to the previous one). Additionally, offsetsize may be used as an optional fourth parameter. Note that a tumbling window is simply a hopping window whose \u2018hop\u2019 is equal to its \u2018size\u2019."
      },
      {
        "date": "2024-07-13T11:18:00.000Z",
        "voteCount": 1,
        "content": "Overlapping only applies if the window size is larger than the hop size. Hopping is the same as tumbling with an equal hop to window size."
      },
      {
        "date": "2024-06-18T11:09:00.000Z",
        "voteCount": 1,
        "content": "The answer is B:No. Tumbling window should be used. With hopping window there is an overlap and an event can be counted more than once."
      },
      {
        "date": "2024-04-24T20:19:00.000Z",
        "voteCount": 1,
        "content": "Yes is correct"
      },
      {
        "date": "2024-04-16T11:32:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\nA tumbling window is the same as an hopping window with the window size equal to the hop size."
      },
      {
        "date": "2024-03-05T23:48:00.000Z",
        "voteCount": 2,
        "content": "B. No\n\nUsing a hopping window with a hop size of 10 seconds and a window size of 10 seconds wouldn't ensure that each tweet is counted only once. A hopping window with these parameters would result in overlapping windows, which means that a tweet might fall into multiple windows and could potentially be counted multiple times.\n\nTo ensure that each tweet is counted only once within a specific 10-second window, you should use a tumbling window with a size of 10 seconds. Tumbling windows are non-overlapping and fixed-size, ensuring that each tweet is counted within a single, distinct window."
      },
      {
        "date": "2024-02-10T17:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is Yes.\nThe solution   actually  defined Tumbling  window using hopping window.\nThe condition is ; if the hopping size equals window  size,then the  window is also Tumbling."
      },
      {
        "date": "2024-02-04T06:25:00.000Z",
        "voteCount": 2,
        "content": "To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size."
      },
      {
        "date": "2024-02-03T02:22:00.000Z",
        "voteCount": 2,
        "content": "If you specify a hopping window in Azure Stream Analytics with a duration of 10 seconds and a hop size of 10 seconds (for example, HoppingWindow(second, 10, 10)), it will indeed behave like a tumbling window. In this case, the hop size is equal to the window size, which means that there is no overlap between the windows.\n\nEach hopping window will move forward by 10 seconds, which is the same as its duration. Therefore, every event will fall into exactly one window, ensuring that each tweet is counted only once per 10-second window. This setup meets the requirement of counting tweets in each 10-second window without double-counting any tweet."
      },
      {
        "date": "2024-01-16T11:25:00.000Z",
        "voteCount": 2,
        "content": "While a tumbling window would be more straightforward, a Hopping window with 10/10 parameters will produce the same result, as it is set not to overlap."
      },
      {
        "date": "2024-01-13T03:26:00.000Z",
        "voteCount": 1,
        "content": "risposta B corretta"
      },
      {
        "date": "2024-01-01T06:17:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B"
      },
      {
        "date": "2023-12-26T08:38:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nThe proposed solution meets the goal. In Azure Stream Analytics, a hopping window generates output every hop size interval, and it aggregates events for the window size period. If both the hop size and window size are set to 10 seconds, the system will count the tweets in each 10-second window, ensuring each tweet is counted only once. This is because the window \u201chops\u201d forward by the specified hop size (10 seconds in this case) and does not overlap with the next window. Therefore, each tweet will fall into exactly one window and will be counted once. This makes the hopping window suitable for this scenario."
      },
      {
        "date": "2023-12-21T14:28:00.000Z",
        "voteCount": 2,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-12-09T15:03:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt : No\nThe answer is **B. No**, this does not meet the goal. Using a hopping window with both the hop size and the window size set to 10 seconds would result in each tweet being counted multiple times as each tweet could appear in several windows. To ensure each tweet is counted only once, the hop size should be equal to the window size or a different type of windowing, such as tumbling windows, should be used."
      },
      {
        "date": "2023-10-30T06:38:00.000Z",
        "voteCount": 1,
        "content": "Yes, the solution described using a hopping window with a hop size of 10 seconds and a window size of 10 seconds would meet the goal of counting tweets in each 10-second window. \n\nIn Azure Stream Analytics, a hopping window moves forward in time at regular intervals (the hop size) and collects data within the specified window size. In this case, with a window size of 10 seconds and a hop size of 10 seconds, you ensure that the system counts tweets"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55003-exam-dp-203-topic-2-question-17-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure Stream Analytics solution that will analyze Twitter data.<br>You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.<br>Solution: You use a hopping window that uses a hop size of 5 seconds and a window size 10 seconds.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-09T12:10:00.000Z",
        "voteCount": 20,
        "content": "answer is correct"
      },
      {
        "date": "2022-10-01T03:10:00.000Z",
        "voteCount": 5,
        "content": "if the hop size is equivalent to the window size then it can be true, but because the hop size is smaller, then each tweet can be count more than one and the windows will overlap with each others."
      },
      {
        "date": "2024-01-18T09:35:00.000Z",
        "voteCount": 1,
        "content": "Got this question on my exam on january 17, answer B is correct."
      },
      {
        "date": "2024-01-16T11:25:00.000Z",
        "voteCount": 1,
        "content": "answer is correct."
      },
      {
        "date": "2023-12-21T14:29:00.000Z",
        "voteCount": 1,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-09-12T07:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-06T06:27:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-05-27T05:57:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-22T04:27:00.000Z",
        "voteCount": 2,
        "content": "definitely No. as the twits will be counted more than once in this solution"
      },
      {
        "date": "2022-07-31T02:11:00.000Z",
        "voteCount": 1,
        "content": "CORRECR"
      },
      {
        "date": "2022-07-29T05:01:00.000Z",
        "voteCount": 1,
        "content": "No\n\nIf solution is: You use a hopping window that uses a hop size of 10 seconds and a window size 10 seconds, it is correct."
      },
      {
        "date": "2022-07-26T00:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is B. The solution must ensure that each tweet is counted only once. Therefore, it means no overlapping. Hence, the better solution is tumbling-window. The answer is B"
      },
      {
        "date": "2022-06-29T06:18:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2022-04-17T12:06:00.000Z",
        "voteCount": 1,
        "content": "hop size must be the same as window size... so B is correct"
      },
      {
        "date": "2022-03-08T12:44:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer."
      },
      {
        "date": "2022-02-20T13:38:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B"
      },
      {
        "date": "2022-02-11T21:15:00.000Z",
        "voteCount": 1,
        "content": "correct B"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54890-exam-dp-203-topic-2-question-18-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics job to identify how much time a user spends interacting with a feature on a webpage.<br>The job receives events based on user actions on the webpage. Each row of data represents an event. Each event has a type of either 'start' or 'end'.<br>You need to calculate the duration between start and end events.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0018000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0018100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DATEDIFF -<br>DATEDIFF function returns the count (as a signed integer value) of the specified datepart boundaries crossed between the specified startdate and enddate.<br>Syntax: DATEDIFF ( datepart , startdate, enddate )<br><br>Box 2: LAST -<br>The LAST function can be used to retrieve the last event within a specific condition. In this example, the condition is an event of type Start, partitioning the search by PARTITION BY user and feature. This way, every user and feature is treated independently when searching for the Start event. LIMIT DURATION limits the search back in time to 1 hour between the End and Start events.<br>Example:<br><br>SELECT -<br>[user],<br>feature,<br>DATEDIFF(<br>second,<br>LAST(Time) OVER (PARTITION BY [user], feature LIMIT DURATION(hour,<br>1) WHEN Event = 'start'),<br><br>Time) as duration -<br><br>FROM input TIMESTAMP BY Time -<br><br>WHERE -<br>Event = 'end'<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-08T06:42:00.000Z",
        "voteCount": 44,
        "content": "correct"
      },
      {
        "date": "2021-10-23T15:41:00.000Z",
        "voteCount": 26,
        "content": "Took the exam today. This question came out.\nAns: DateDiff, Last"
      },
      {
        "date": "2022-01-16T06:23:00.000Z",
        "voteCount": 2,
        "content": "how do you know?"
      },
      {
        "date": "2022-01-26T07:11:00.000Z",
        "voteCount": 13,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#detect-the-duration-between-events"
      },
      {
        "date": "2024-01-29T00:16:00.000Z",
        "voteCount": 2,
        "content": "Answer should be correct, because the link is there, but what a freak SQL is it? The original T-SQL is LAST_VALUE(COLUMN) OVER(ORDER BY ...)! If I search the internet for this solution, the only page where it exists is the link in the answer!"
      },
      {
        "date": "2023-09-06T06:27:00.000Z",
        "voteCount": 1,
        "content": "Ans: DateDiff, Last"
      },
      {
        "date": "2023-07-28T04:01:00.000Z",
        "voteCount": 1,
        "content": "Why is the answer to the second blank LAST() instead of TOPONE(), when this is about the startdate?"
      },
      {
        "date": "2023-08-11T02:53:00.000Z",
        "voteCount": 4,
        "content": "LAST gives the most recent event it contradicts the name.. ISFIRST gives the oldest event... TOP ONE is a aggregate function which requires ORDER BY mandatory more like a ranking function here no order by so eliminated"
      },
      {
        "date": "2022-07-31T04:42:00.000Z",
        "voteCount": 2,
        "content": "answer is correct"
      },
      {
        "date": "2022-07-21T13:53:00.000Z",
        "voteCount": 1,
        "content": "When Event ='Start' should not be there in this question"
      },
      {
        "date": "2022-01-29T06:47:00.000Z",
        "voteCount": 1,
        "content": "correct answer."
      },
      {
        "date": "2021-09-28T02:23:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct"
      },
      {
        "date": "2021-07-27T09:33:00.000Z",
        "voteCount": 5,
        "content": "This is Stream Analytics Query Language, a little different than tsql \nhttps://docs.microsoft.com/en-us/stream-analytics-query/last-azure-stream-analytics"
      },
      {
        "date": "2021-08-14T07:15:00.000Z",
        "voteCount": 1,
        "content": "so is the answer DATEDIFF+LAST incorrect then?"
      },
      {
        "date": "2021-08-26T02:20:00.000Z",
        "voteCount": 4,
        "content": "DATEDIFF and LAST are correct, please refer to: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns (\"Detect the duration between events\")"
      },
      {
        "date": "2021-07-03T03:43:00.000Z",
        "voteCount": 9,
        "content": "The query has no sense, at least if it is T-SQL. Look: each row is end event or start event. How window function (Last() over partition) can get start event if there is where condition that filter out end event only???"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55434-exam-dp-203-topic-2-question-19-discussion/",
    "body": "You are creating an Azure Data Factory data flow that will ingest data from a CSV file, cast columns to specified types of data, and insert the data into a table in an<br>Azure Synapse Analytic dedicated SQL pool. The CSV file contains three columns named username, comment, and date.<br>The data flow already contains the following:<br>\u2711 A source transformation.<br>\u2711 A Derived Column transformation to set the appropriate types of data.<br>\u2711 A sink transformation to land the data in the pool.<br>You need to ensure that the data flow meets the following requirements:<br>\u2711 All valid rows must be written to the destination table.<br>\u2711 Truncation errors in the comment column must be avoided proactively.<br>\u2711 Any rows containing comment values that will cause truncation errors upon insert must be written to a file in blob storage.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTo the data flow, add a sink transformation to write the rows to a file in blob storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTo the data flow, add a Conditional Split transformation to separate the rows that will cause truncation errors.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTo the data flow, add a filter transformation to filter out rows that will cause truncation errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a select transformation to select only the rows that will cause truncation errors."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-16T05:15:00.000Z",
        "voteCount": 34,
        "content": "correct"
      },
      {
        "date": "2021-12-20T12:49:00.000Z",
        "voteCount": 6,
        "content": "Conditional split with a sink transformation is the correct answer https://docs.microsoft.com/en-us/azure/data-factory/how-to-data-flow-error-rows?WT.mc_id=esi_studyguide_content_wwl#how-to-design-around-this-condition"
      },
      {
        "date": "2023-09-06T06:28:00.000Z",
        "voteCount": 1,
        "content": "CORRECT"
      },
      {
        "date": "2022-09-05T20:32:00.000Z",
        "voteCount": 3,
        "content": "CORRECT"
      },
      {
        "date": "2022-07-31T06:15:00.000Z",
        "voteCount": 2,
        "content": "A&amp;B are correct"
      },
      {
        "date": "2022-03-22T13:20:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-30T18:17:00.000Z",
        "voteCount": 2,
        "content": "Correct A and B"
      },
      {
        "date": "2022-01-29T06:58:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2021-12-11T23:51:00.000Z",
        "voteCount": 1,
        "content": "I agree with the answer.\nhttps://www.microsoft.com/en-us/videoplayer/embed/RE4uOHj"
      },
      {
        "date": "2021-12-07T07:11:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-08-26T02:55:00.000Z",
        "voteCount": 1,
        "content": "Why not C ?"
      },
      {
        "date": "2021-09-09T13:23:00.000Z",
        "voteCount": 3,
        "content": "You are right ! The job could be done by C only, BUT ! it has been asked for two(2) actions. No choice to add a SPLIT then another sink where to drop the bad records"
      },
      {
        "date": "2021-10-29T03:59:00.000Z",
        "voteCount": 4,
        "content": "But you need to write to blob right ? How can only C help with that?"
      },
      {
        "date": "2021-08-10T12:30:00.000Z",
        "voteCount": 1,
        "content": "Bad rows go to 'folder out' and the good rows to the junk table? How come?"
      },
      {
        "date": "2023-12-06T00:49:00.000Z",
        "voteCount": 1,
        "content": "The third point \"A sink transformation to land the data in the pool\" is for the good rows.  You choose the conditional split to ensure that any bad rows are put somewhere else before they break the logic of the data flow.f"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54230-exam-dp-203-topic-2-question-20-discussion/",
    "body": "DRAG DROP -<br>You need to create an Azure Data Factory pipeline to process data for the following three departments at your company: Ecommerce, retail, and wholesale. The solution must ensure that data can also be processed for the entire company.<br>How should you complete the Data Factory data flow script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0018400002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0018500001.png\" class=\"in-exam-image\">",
    "answerDescription": "The conditional split transformation routes data rows to different streams based on matching conditions. The conditional split transformation is similar to a CASE decision structure in a programming language. The transformation evaluates expressions, and based on the results, directs the data row to the specified stream.<br>Box 1: dept=='ecommerce', dept=='retail', dept=='wholesale'<br>First we put the condition. The order must match the stream labeling we define in Box 3.<br>Syntax:<br>&lt;incomingStream&gt;<br>split(<br>&lt;conditionalExpression1&gt;<br>&lt;conditionalExpression2&gt;<br>...<br>disjoint: {true | false}<br>) ~&gt; &lt;splitTx&gt;@(stream1, stream2, ..., &lt;defaultStream&gt;)<br><br>Box 2: discount : false -<br>disjoint is false because the data goes to the first matching condition. All remaining rows matching the third condition go to output stream all.<br>Box 3: ecommerce, retail, wholesale, all<br><br>Label the streams -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-02T08:36:00.000Z",
        "voteCount": 85,
        "content": "I think \"disjoint\" should be True, so that data can be sent to all matching conditions. In this way the \"all\" output can get the data from every department, which ensures that \"data can also be processed by the entire company\"."
      },
      {
        "date": "2021-10-25T22:18:00.000Z",
        "voteCount": 3,
        "content": "agree with you, disjount = true"
      },
      {
        "date": "2023-07-03T05:30:00.000Z",
        "voteCount": 3,
        "content": "Disagree, all is like an else"
      },
      {
        "date": "2022-09-10T06:04:00.000Z",
        "voteCount": 6,
        "content": "All is not defined in split so it has to be false. Refer \nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split#:~:text=CleanData%0A%20%20%20%20split(%0A%20%20%20%20%20%20%20%20year%20%3C%201960%2C%0A%09%20%20%20%20year%20%3E%201980%2C%0A%09%20%20%20%20disjoint%3A%20false%0A%20%20%20%20)%20~%3E%20SplitByYear%40(moviesBefore1960%2C%20moviesAfter1980%2C%20AllOtherMovies)"
      },
      {
        "date": "2023-08-27T23:23:00.000Z",
        "voteCount": 2,
        "content": "disjoint is false because the data goes to the first matching condition rather than all matching conditions."
      },
      {
        "date": "2021-10-11T03:12:00.000Z",
        "voteCount": 4,
        "content": "yes it's True :Disjoint=True"
      },
      {
        "date": "2021-06-03T11:50:00.000Z",
        "voteCount": 47,
        "content": "As per the link provided in the explanation disjoint:false looks correct. I believe you must go through the link https://docs.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split and choose you answer for disjoint wisely . I will go with \"False\""
      },
      {
        "date": "2022-02-02T22:00:00.000Z",
        "voteCount": 4,
        "content": "you also need to read question to understand requirement. I will choose disjoint: true"
      },
      {
        "date": "2023-06-21T14:24:00.000Z",
        "voteCount": 2,
        "content": "From the link you've posted:\ndisjoint is false because the data goes to the first matching condition rather than all matching conditions. \nSo the correct answer is True, considering we have to \"duplicate\" records for the ALL category."
      },
      {
        "date": "2024-07-26T15:09:00.000Z",
        "voteCount": 1,
        "content": "Guys, Guys, Guys.....\n\nThe clue is in the options given. \n\nIf default: true was the right answer, then options A and F would be the same. Either of them could have fulfilled the criteria.\n\nIf only one of them is right (and that is what we expect) then the order matters. And order only matters when default: false.\n\nThis also means that 'all' is slightly misleading. Refer back to the question: it does not imply all of the data needs to be available, just that the 'entire company can process'. Which is still okay if the 'all' had everything but ecommerce, retail and wholesale.\n\nFinal point: If default: true was the right answer, options B and C would be the same. Either of the them could have worked.\n\nConclusion: default: false."
      },
      {
        "date": "2024-07-26T15:16:00.000Z",
        "voteCount": 1,
        "content": "typo: replace default by disjoint"
      },
      {
        "date": "2024-07-14T17:09:00.000Z",
        "voteCount": 1,
        "content": "disjoint: true: If set to true, the row will be sent to all matching conditions. This means that a single row can appear in multiple output streams if it matches multiple conditions.\ndisjoint: false: If set to false, the row will be sent to the first matching condition only. Once a row matches a condition, it will not be evaluated against subsequent conditions."
      },
      {
        "date": "2023-12-17T03:09:00.000Z",
        "voteCount": 1,
        "content": "Guys Disjoint is True 110% and I will  tell you why.\n\ndisjoint: false means that rows will be directed to the first branch whose condition is satisfied, and subsequent conditions are ignored.\nThis might not fulfill the requirement because you want to process data for multiple departments, and with disjoint: false, a row would go to the first department branch it satisfies, ignoring the other departments.\n\nDisjoint TRUE is more appropriate because it fulfills the requirement of processing data for individual departments (Ecommerce, retail, and wholesale) while also handling data for the entire company. Because all rows will match 2 conditions: \n1st conditon. They will have one of the three depts\n2nd Condition. They will match the all condition\n\nThat's why it MUST BE TRUE."
      },
      {
        "date": "2023-08-27T23:34:00.000Z",
        "voteCount": 2,
        "content": "False is correct"
      },
      {
        "date": "2023-06-21T20:53:00.000Z",
        "voteCount": 1,
        "content": "I think the disjoint should be 'False'\nBy setting \"disjoint true\" for activities in a pipeline, you are essentially indicating that these activities are independent and can be executed concurrently. This can help improve the overall performance and efficiency of the pipeline by allowing for parallel execution of activities that do not have any interdependencies."
      },
      {
        "date": "2023-05-24T07:32:00.000Z",
        "voteCount": 2,
        "content": "CleanData split(dept==\u2018ecommerce\u2019, dept==\u2018retail\u2019, dept==\u2018wholesale\u2019) ~&gt; SplitByDept@(disjoint: false)\n\nThis will split the data by department and allow for processing of data for the entire company as well as for individual departments."
      },
      {
        "date": "2023-05-24T07:34:00.000Z",
        "voteCount": 2,
        "content": "The disjoint option in a split transformation determines whether the output streams are mutually exclusive or not. If disjoint is set to true, then each row of data can only be sent to one output stream. If disjoint is set to false, then a single row of data can be sent to multiple output streams.\n\nIn this case, setting disjoint to false allows for data to be processed for the entire company as well as for individual departments. This means that a single row of data can be sent to multiple output streams, allowing for processing at both the department and company level."
      },
      {
        "date": "2023-03-16T06:53:00.000Z",
        "voteCount": 4,
        "content": "disjoin = true if you want all , if disjoint = false, nothing in ALL split"
      },
      {
        "date": "2023-02-03T08:51:00.000Z",
        "voteCount": 1,
        "content": "Disjoint=False"
      },
      {
        "date": "2023-01-22T09:56:00.000Z",
        "voteCount": 4,
        "content": "disjoint=false \n\nThe below example is a conditional split transformation named SplitByYear that takes in incoming stream CleanData. This transformation has two split conditions year &lt; 1960 and year &gt; 1980. disjoint is false because the data goes to the first matching condition rather than all matching conditions. Every row matching the first condition goes to output stream moviesBefore1960. All remaining rows matching the second condition go to output stream moviesAFter1980. All other rows flow through the default stream AllOtherMovies.\nfrom https://learn.microsoft.com/en-us/azure/data-factory/data-flow-conditional-split"
      },
      {
        "date": "2022-12-29T08:03:00.000Z",
        "voteCount": 2,
        "content": "Given answer correct"
      },
      {
        "date": "2022-11-23T09:24:00.000Z",
        "voteCount": 2,
        "content": "The given answer is 100000% crct, don't confuse with others"
      },
      {
        "date": "2022-09-08T09:37:00.000Z",
        "voteCount": 6,
        "content": "Given answer is 100% correct"
      },
      {
        "date": "2022-06-11T08:15:00.000Z",
        "voteCount": 2,
        "content": "Everyone is discussing about disjoint. But if disjoint is true then there is no ordering required of ecommerce,retail,wholesale, all .so we can fill 1st option with 2 or 3 and 3rd option with 1 or 6."
      },
      {
        "date": "2022-05-09T14:31:00.000Z",
        "voteCount": 1,
        "content": "I think it should be disjoint is True based on microsofts example. it states that when disjoint is false each row will only go to the first matching condition. However in the example I believe each row will go to its matching department plus an aggregate stream that takes in every value regardless. Hence disjoint should be true"
      },
      {
        "date": "2022-04-29T04:55:00.000Z",
        "voteCount": 2,
        "content": "Definetely Disjoint=Trues as per Microsoft doc"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52573-exam-dp-203-topic-2-question-21-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.<br>You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.<br>You create the following components:<br>\u2711 A destination table in Azure Synapse<br>\u2711 An Azure Blob storage container<br>\u2711 A service principal<br>Which five actions should you perform in sequence next in is Databricks notebook? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0018600004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0018700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Mount the Data Lake Storage onto DBFS<br>Begin with creating a file system in the Azure Data Lake Storage Gen2 account.<br>Step 2: Read the file into a data frame.<br>You can load the json files as a data frame in Azure Databricks.<br>Step 3: Perform transformations on the data frame.<br>Step 4: Specify a temporary folder to stage the data<br>Specify a temporary folder to use while moving data between Azure Databricks and Azure Synapse.<br>Step 5: Write the results to a table in Azure Synapse.<br>You upload the transformed data frame into Azure Synapse. You use the Azure Synapse connector for Azure Databricks to directly upload a dataframe as a table in a Azure Synapse.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-12T17:54:00.000Z",
        "voteCount": 171,
        "content": "I think the correct order is:\n1) mount onto DBFS\n2) read into data frame\n3) transform data frame\n4) specify temporary folder\n5) write to table in SQL data warehouse\n\nAbout temporary folder, there is a note explain this:\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse#load-data-into-azure-synapse\n\nDiscussions about this question:\nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/"
      },
      {
        "date": "2024-04-15T12:17:00.000Z",
        "voteCount": 2,
        "content": "the given answer is correct!"
      },
      {
        "date": "2021-05-14T09:29:00.000Z",
        "voteCount": 2,
        "content": "Hi sagga! Thank you. I do agree...."
      },
      {
        "date": "2021-06-22T18:05:00.000Z",
        "voteCount": 3,
        "content": "fix solution on site"
      },
      {
        "date": "2021-06-28T16:00:00.000Z",
        "voteCount": 41,
        "content": "Today I received this question in my exam. Only appeared the 5 options of this response. I only had to order, not choice. This solutions is the correct. Thanks sagga."
      },
      {
        "date": "2021-11-18T10:02:00.000Z",
        "voteCount": 4,
        "content": "Correct also received the only five options.\nAlso see: \nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/"
      },
      {
        "date": "2021-09-10T06:18:00.000Z",
        "voteCount": 2,
        "content": "I agree, although, why do we need a temporary folder? We already have storage blob as temporary storage?"
      },
      {
        "date": "2022-06-18T20:58:00.000Z",
        "voteCount": 2,
        "content": "Databricks uses polybase to write to Synapse and thus we need to stage the file is required."
      },
      {
        "date": "2021-06-08T06:51:00.000Z",
        "voteCount": 22,
        "content": "1) mount the data onto DBFS\n2) Read the file into a data frame\n3) Perform transformations on the file\n4) Specify a temporary folder to stage the data\n5) Write the results to a table in Azure synapse"
      },
      {
        "date": "2022-11-22T08:00:00.000Z",
        "voteCount": 7,
        "content": "transformations on dataframe, not on the file."
      },
      {
        "date": "2023-12-09T15:55:00.000Z",
        "voteCount": 1,
        "content": "Answer are correct, chatgpt say :\nTo accomplish the task in an Azure Databricks notebook, the logical sequence of actions would be:\n\n1. **Mount the Data Lake Storage onto DBFS**: This allows access to the JSON file stored in Azure Data Lake Storage using the Databricks File System.\n\n2. **Read the file into a data frame**: Use Spark to read the JSON file into a DataFrame for processing.\n\n3. **Perform transformations on the data frame**: Apply transformations to concatenate the FirstName and LastName fields to create a new column.\n\n4. **Specify a temporary folder to stage the data**: Before writing the data to Azure Synapse, it is a common practice to stage it in a temporary folder.\n\n5. **Write the results to a table in Azure Synapse**: Finally, write the transformed DataFrame to the destination table in Azure Synapse Analytics. \n\nThese steps would ensure the JSON file data is properly transformed and loaded into Azure Synapse Analytics for further use."
      },
      {
        "date": "2023-11-23T07:13:00.000Z",
        "voteCount": 1,
        "content": "Just remember the initials first: M.R.P.S.W then go to the details."
      },
      {
        "date": "2023-05-24T07:46:00.000Z",
        "voteCount": 3,
        "content": "1. Mount the data lake storage onto DBFS.\n2. Read the file into a data frame.\n3. Perform transformations on the data frame.\n4. Specify a temporary folder to stage the data.\n5. Write the results to a table in Azure Synapse.\nThis will allow you to read the data from the JSON file into a data frame, perform the necessary transformations to concatenate the FirstName and LastName values, and then write the results to a table in Azure Synapse."
      },
      {
        "date": "2022-07-31T10:03:00.000Z",
        "voteCount": 1,
        "content": "answer is correct, explained by the reference link in the given solution"
      },
      {
        "date": "2022-05-05T06:18:00.000Z",
        "voteCount": 3,
        "content": "I don not see the reason why \"specify temporary folder\" can not be both before or after the \"read and transformation phase\""
      },
      {
        "date": "2022-06-19T14:26:00.000Z",
        "voteCount": 1,
        "content": "I want to know the reason too!"
      },
      {
        "date": "2022-04-17T12:29:00.000Z",
        "voteCount": 1,
        "content": "given answer is correct, after reading and rereading  stand with the given answer"
      },
      {
        "date": "2022-01-09T21:24:00.000Z",
        "voteCount": 3,
        "content": "I think the correct order is:\n1) mount onto DBFS\n2) read into data frame\n3) transform data frame\n4) specify temporary folder\n5) write to table in SQL data warehouse"
      },
      {
        "date": "2021-12-20T07:28:00.000Z",
        "voteCount": 2,
        "content": "Here is my answer.\n1) Create a service principal  - Not sure why this step is not a choice in this question. I don't thing need to mount onto DBFS, but you do need to assign permission to allow databricks  talk with Data Lake and read file.\n2) Read the file into data frame\n3) Perform transformations on the data frame \n   Data have been read into data from, so should transform data from data frame, not data file.\n4) Specify temporary folder to stage the data\n5) Write the results to a table in Azure Synapse\n\nI reviewed this online document. No any place mentioned that the data frame needs to be dropped. \nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse"
      },
      {
        "date": "2022-03-08T20:41:00.000Z",
        "voteCount": 1,
        "content": "you do not need to create a service principal, this is already exist"
      },
      {
        "date": "2021-12-15T05:09:00.000Z",
        "voteCount": 1,
        "content": "There Is A Contradiction Between Answers On The Drag &amp; Drop And The Answers In The Steps Listing, And I Think The Correct Ones Are That In The Listing And Not The Drag &amp; Drop."
      },
      {
        "date": "2021-12-14T08:20:00.000Z",
        "voteCount": 1,
        "content": "1) Mount the data onto DBFS\n2) Read the file into a data frame\n3) Perform transformations \n4) Specify a temporary folder to stage the data\n5) Write the results to a table in Azure synapse"
      },
      {
        "date": "2021-08-23T18:52:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct, after read the link provided carefully several times. There's already a service principal. With that, it's no need to mount. You do need to drop the dataframe as the last step."
      },
      {
        "date": "2021-09-10T06:17:00.000Z",
        "voteCount": 4,
        "content": "Service Principal has nothing to do with DataBricks."
      },
      {
        "date": "2022-05-09T14:44:00.000Z",
        "voteCount": 1,
        "content": "Actually you can assign a service principal to any data bricks account and use OAuth to connect with its tenant id app secret, and app id. You can then mount the data lake to databricks."
      },
      {
        "date": "2021-05-13T04:45:00.000Z",
        "voteCount": 5,
        "content": "Correct solution: \nRead the file into a data frame\nPerform transformations on the file\nSpecify a temporary folder to stage the data\nWrite the results to a table in Azure synapse\nDrop the data frame"
      },
      {
        "date": "2021-06-01T01:13:00.000Z",
        "voteCount": 3,
        "content": "you should not perform transformation on the file.\nYou need not to drop the dataframe. \nsagga options are correct"
      },
      {
        "date": "2021-05-17T20:15:00.000Z",
        "voteCount": 6,
        "content": "I believe you perform transformation on the data frame and not on the file"
      },
      {
        "date": "2022-03-23T15:48:00.000Z",
        "voteCount": 3,
        "content": "Step 1: Read the file into a data frame.\nYou can load the json files as a data frame in Azure Databricks.\nStep 2: Perform transformations on the data frame.\nStep 3:Specify a temporary folder to stage the data\nSpecify a temporary folder to use while moving data between Azure Databricks and Azure SQL Data Warehouse.\nStep 4: Write the results to a table in Azure synapse\nStep 5: Drop the data frame - Clean up resources. \n\nhttps://www.examtopics.com/discussions/microsoft/view/11653-exam-dp-200-topic-2-question-30-discussion/"
      },
      {
        "date": "2023-09-06T06:37:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55302-exam-dp-203-topic-2-question-22-discussion/",
    "body": "HOTSPOT -<br>You build an Azure Data Factory pipeline to move data from an Azure Data Lake Storage Gen2 container to a database in an Azure Synapse Analytics dedicated<br>SQL pool.<br>Data in the container is stored in the following folder structure.<br>/in/{YYYY}/{MM}/{DD}/{HH}/{mm}<br>The earliest folder is /in/2021/01/01/00/00. The latest folder is /in/2021/01/15/01/45.<br>You need to configure a pipeline trigger to meet the following requirements:<br>\u2711 Existing data must be loaded.<br>\u2711 Data must be loaded every 30 minutes.<br>\u2711 Late-arriving data of up to two minutes must be included in the load for the time at which the data should have arrived.<br>How should you configure the pipeline trigger? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0018800004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0018900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Tumbling window -<br>To be able to use the Delay parameter we select Tumbling window.<br>Box 2:<br>Recurrence: 30 minutes, not 32 minutes<br>Delay: 2 minutes.<br>The amount of time to delay the start of data processing for the window. The pipeline run is started after the expected execution time plus the amount of delay.<br>The delay defines how long the trigger waits past the due time before triggering a new run. The delay doesn't alter the window startTime.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-14T02:54:00.000Z",
        "voteCount": 51,
        "content": "ANswers are correct"
      },
      {
        "date": "2023-12-21T14:27:00.000Z",
        "voteCount": 7,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2024-03-29T08:53:00.000Z",
        "voteCount": 1,
        "content": "Both correct"
      },
      {
        "date": "2023-09-06T06:48:00.000Z",
        "voteCount": 1,
        "content": "Answers are correct"
      },
      {
        "date": "2022-08-14T23:49:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2022-06-16T22:44:00.000Z",
        "voteCount": 2,
        "content": "Answers are correct"
      },
      {
        "date": "2022-04-09T16:15:00.000Z",
        "voteCount": 4,
        "content": "Correct. https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"
      },
      {
        "date": "2021-10-24T16:14:00.000Z",
        "voteCount": 2,
        "content": "Why can't we use an event-based trigger here?"
      },
      {
        "date": "2021-10-26T00:08:00.000Z",
        "voteCount": 10,
        "content": "because we also wanna do backfill with past data. Technically, the event-based trigger will also allows ADF to find all the old files from the source which ADF hasn't processed yet (and we could add a datetime filter when loading the data) but ADF is gonna choke on so many past events from experience. With tumbling windows,  the trigger will kick off for each 30 minutes slices of the time span, emulating batch loads. be very careful when doing backfill with a tumbling window, by default, ADF will start 50 concurrent pipelines, it can be pricey, change the settings in advanced panel of the trigger creation form."
      },
      {
        "date": "2021-06-30T03:06:00.000Z",
        "voteCount": 2,
        "content": "not schedule ?"
      },
      {
        "date": "2021-07-05T02:33:00.000Z",
        "voteCount": 7,
        "content": "As the solution says, you cannot use the Delay with Schedule."
      },
      {
        "date": "2021-06-27T06:47:00.000Z",
        "voteCount": 1,
        "content": "why not schedule trigger?"
      },
      {
        "date": "2021-09-07T18:21:00.000Z",
        "voteCount": 8,
        "content": "Schedule trigger would not work because backfill is only possible with Tumbling window trigger. In this case, we need to use trigger for old data."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54741-exam-dp-203-topic-2-question-23-discussion/",
    "body": "HOTSPOT -<br>You are designing a near real-time dashboard solution that will visualize streaming data from remote sensors that connect to the internet. The streaming data must be aggregated to show the average value of each 10-second interval. The data will be discarded after being displayed in the dashboard.<br>The solution will use Azure Stream Analytics and must meet the following requirements:<br>\u2711 Minimize latency from an Azure Event hub to the dashboard.<br>\u2711 Minimize the required storage.<br>\u2711 Minimize development effort.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0019000004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0019100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-06T10:46:00.000Z",
        "voteCount": 69,
        "content": "Answer is correct"
      },
      {
        "date": "2021-06-07T01:41:00.000Z",
        "voteCount": 168,
        "content": "Agreed. So easy that even ExamTopics got it right."
      },
      {
        "date": "2021-10-26T22:05:00.000Z",
        "voteCount": 5,
        "content": "lol :)"
      },
      {
        "date": "2021-10-09T17:30:00.000Z",
        "voteCount": 14,
        "content": "The best comment ever :)"
      },
      {
        "date": "2022-11-28T14:31:00.000Z",
        "voteCount": 4,
        "content": "super like for this comment"
      },
      {
        "date": "2023-12-21T14:27:00.000Z",
        "voteCount": 6,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2024-03-29T09:19:00.000Z",
        "voteCount": 1,
        "content": "I really hope this question comes up on the exam"
      },
      {
        "date": "2023-09-06T06:49:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-12-29T08:15:00.000Z",
        "voteCount": 2,
        "content": "We agee on this"
      },
      {
        "date": "2022-07-31T10:29:00.000Z",
        "voteCount": 2,
        "content": "Correct solution"
      },
      {
        "date": "2022-05-11T23:11:00.000Z",
        "voteCount": 4,
        "content": "Correct. Question so easy I wonder if it was really in the exam."
      },
      {
        "date": "2022-02-20T01:03:00.000Z",
        "voteCount": 2,
        "content": "COrreECT"
      },
      {
        "date": "2022-02-02T22:47:00.000Z",
        "voteCount": 2,
        "content": "correct docs link is https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-real-time-fraud-detection"
      },
      {
        "date": "2021-07-12T00:34:00.000Z",
        "voteCount": 2,
        "content": "Right Answer. \nAnswer to 3rd drop down is already in the question."
      },
      {
        "date": "2022-04-18T01:07:00.000Z",
        "voteCount": 1,
        "content": "also the 1st is in the question"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55087-exam-dp-203-topic-2-question-24-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Stream Analytics job that is a Stream Analytics project solution in Microsoft Visual Studio. The job accepts data generated by IoT devices in the JSON format.<br>You need to modify the job to accept data generated by the IoT devices in the Protobuf format.<br>Which three actions should you perform from Visual Studio on sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0019200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0019200002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.<br><br>Create a custom deserializer -<br>1. Open Visual Studio and select File &gt; New &gt; Project. Search for Stream Analytics and select Azure Stream Analytics Custom Deserializer Project (.NET). Give the project a name, like Protobuf Deserializer.<br><img src=\"/assets/media/exam-media/04259/0019300001.jpg\" class=\"in-exam-image\"><br>2. In Solution Explorer, right-click your Protobuf Deserializer project and select Manage NuGet Packages from the menu. Then install the<br>Microsoft.Azure.StreamAnalytics and Google.Protobuf NuGet packages.<br>3. Add the MessageBodyProto class and the MessageBodyDeserializer class to your project.<br>4. Build the Protobuf Deserializer project.<br>Step 2: Add .NET deserializer code for Protobuf to the custom deserializer project<br>Azure Stream Analytics has built-in support for three data formats: JSON, CSV, and Avro. With custom .NET deserializers, you can read data from other formats such as Protocol Buffer, Bond and other user defined formats for both cloud and edge jobs.<br>Step 3: Add an Azure Stream Analytics Application project to the solution<br>Add an Azure Stream Analytics project<br>1. In Solution Explorer, right-click the Protobuf Deserializer solution and select Add &gt; New Project. Under Azure Stream Analytics &gt; Stream Analytics, choose<br>Azure Stream Analytics Application. Name it ProtobufCloudDeserializer and select OK.<br>2. Right-click References under the ProtobufCloudDeserializer Azure Stream Analytics project. Under Projects, add Protobuf Deserializer. It should be automatically populated for you.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-08T04:00:00.000Z",
        "voteCount": 65,
        "content": "The third one is wrong because the stream analytics application already exist in the project. \nThe goal is to modify the current stream analytics application in order to read protobuff data. \nI think the right answer is the first one in the list (update input.json file and reference dll)"
      },
      {
        "date": "2021-09-10T12:49:00.000Z",
        "voteCount": 6,
        "content": "Absolutely: https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer"
      },
      {
        "date": "2022-03-15T00:47:00.000Z",
        "voteCount": 6,
        "content": "this is a tricky question.. technically document here https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer describes it as follows:\n1. Add custom deserializer project\n2. Add the  MessageBodyProto class and the MessageBodyDeserializer class to your project (it's actually merged with point 1)\n3. add an Azure Stream Analytics project\n4. Configure a Stream Analytics job (including changing input.json)\n\nTechnically, the question asks for 3 steps so we might to either skip the \"add code\" as a separate step, and include point 4 as the last step (i.e. 1,3,4 above) or we stop at 3 and then the answer is as listed.. As usual, the description of the question is confusing."
      },
      {
        "date": "2021-10-18T00:03:00.000Z",
        "voteCount": 62,
        "content": "1 Add an Azure Stream Analytics Customer Deserializer Project(.net) project to the Solution \n2 Add .net deseriliaizer Code to ProtoBuf to customer deserializer project\n3. Change the event Serialization format to protobuf in the input.json File of the job and reference the DLL."
      },
      {
        "date": "2024-05-17T14:16:00.000Z",
        "voteCount": 7,
        "content": "I bet even the Microsoft arquitect that turned goose farmer has nightmares with this question"
      },
      {
        "date": "2023-12-09T16:02:00.000Z",
        "voteCount": 4,
        "content": "Chatgpt :\nTo modify an Azure Stream Analytics job in Visual Studio to accept data in Protobuf format from IoT devices, you would typically need to:\n\n1. **Add an Azure Stream Analytics Custom Deserializer Project (.NET project) to the solution**: This sets up a project that can include the custom deserialization logic.\n\n2. **Add .NET deserializer code for Protobuf to the custom deserializer project**: Here, you would implement the Protobuf deserialization logic within the project you added in the previous step.\n\n3. **Change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL**: Finally, you need to update the job configuration to use the custom deserializer by changing the serialization format and pointing it to the compiled DLL from your custom deserializer project. \n\nThese actions will enable the Azure Stream Analytics job to deserialize and process data in Protobuf format instead of JSON."
      },
      {
        "date": "2023-12-04T20:39:00.000Z",
        "voteCount": 1,
        "content": "1- Add Azure Stream Analyticst Custon Deserializer Project (.NET) project to the Solution;\n2 - Add an Azure Stream Analytics Application project to the solution;\n3 - Change... \"Iput.json\" is necessary your modification;"
      },
      {
        "date": "2023-09-06T06:51:00.000Z",
        "voteCount": 1,
        "content": "1. Add Azure Stream Analytics Custom Deserializer Project (.NET) 2. Add Azure Stream Analytics Application 3. Configure a Stream Analytics job in Input.json"
      },
      {
        "date": "2023-05-24T22:45:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is:\nAdd an Azure Stream Analytics Customer Deserializer Project(.net) project to the Solution\nAdd .net deserializer Code to ProtoBuf to the customer deserializer project\nChange the event Serialization format to protobuf in the input.json File of the job and reference the DLL"
      },
      {
        "date": "2023-05-24T07:57:00.000Z",
        "voteCount": 2,
        "content": "1. Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\n2. Add .NET deserializer code for Protobuf to the custom deserializer project.\n3. Change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL.\nThis will allow you to create a custom deserializer project and add .NET deserializer code for Protobuf to it. Then, you can change the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL containing the custom deserializer code."
      },
      {
        "date": "2023-04-24T05:40:00.000Z",
        "voteCount": 3,
        "content": "Chat GPT: B-C-A"
      },
      {
        "date": "2023-03-27T19:57:00.000Z",
        "voteCount": 6,
        "content": "guessing seem not for DP203, anyone agree?"
      },
      {
        "date": "2024-01-16T07:44:00.000Z",
        "voteCount": 2,
        "content": "It seems that not anymore, I think that as of january 16 of 2024, there is nothing related to .NET and ProtoBuf in the learning path of DP-203"
      },
      {
        "date": "2023-03-07T01:53:00.000Z",
        "voteCount": 1,
        "content": "To modify the Azure Stream Analytics job to accept data generated by the IoT devices in the Protobuf format, follow these steps in sequence:\n\nAdd an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.\nAdd .NET deserializer code for Protobuf to the custom deserializer project.\nChange the Event Serialization Format to Protobuf in the input.json file of the job and reference the DLL."
      },
      {
        "date": "2022-10-21T01:10:00.000Z",
        "voteCount": 3,
        "content": "According to the documentation: \n1- Create a custom deserializer for protocol buffer.\n2- Add an Azure Stream Analytics project\n3- Configure a Stream Analytics job ( in here you specify reference the dll...) ==&gt; Change the event Serialization format to protobuf in the input.json File of the job and reference the DLL.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer"
      },
      {
        "date": "2023-12-04T20:39:00.000Z",
        "voteCount": 1,
        "content": "I Agree;\n\n1- Add Azure Stream Analyticst Custon Deserializer Project (.NET) project to the Solution;\n2 - Add an Azure Stream Analytics Application project to the solution;\n3 - Change... \"Iput.json\" is necessary your modification;"
      },
      {
        "date": "2022-10-21T00:40:00.000Z",
        "voteCount": 1,
        "content": "As stated in the documentation : \n1- Create a custom deserializer project\n2- Add an azure stream alaytics project\n3- Configure a stream analytics job, ( in this configuration, the dll is referenced) ==&gt; update input.json file and reference dll\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer"
      },
      {
        "date": "2022-10-05T23:29:00.000Z",
        "voteCount": 1,
        "content": "I am still practicing and have not seen the answer yet. Have put my answer as 3-2-1. Will see the right answer in the end."
      },
      {
        "date": "2022-07-25T22:26:00.000Z",
        "voteCount": 7,
        "content": "As described in https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer:\n1. Add Azure Stream Analytics Custom Deserializer Project (.NET)\n2. Add  Azure Stream Analytics Application\n3. Configure a Stream Analytics job in Input.json"
      },
      {
        "date": "2023-06-23T07:04:00.000Z",
        "voteCount": 1,
        "content": "Yes, I think your answer is correct following this link : https://learn.microsoft.com/en-us/azure/stream-analytics/custom-deserializer"
      },
      {
        "date": "2021-11-04T08:21:00.000Z",
        "voteCount": 16,
        "content": "Has this question come up in the DP-203 exam?"
      },
      {
        "date": "2021-08-23T19:44:00.000Z",
        "voteCount": 4,
        "content": "Third one should be the first action listed: Change file format in input.json"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54531-exam-dp-203-topic-2-question-25-discussion/",
    "body": "You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region.<br>You need to copy blob data from the storage account to the data warehouse by using Azure Data Factory. The solution must meet the following requirements:<br>\u2711 Ensure that the data remains in the UK South region at all times.<br>\u2711 Minimize administrative effort.<br>Which type of integration runtime should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure integration runtime\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure-SSIS integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelf-hosted integration runtime"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-07-08T04:03:00.000Z",
        "voteCount": 44,
        "content": "A is the right answer (don't use autoresolve region)"
      },
      {
        "date": "2022-11-23T12:11:00.000Z",
        "voteCount": 1,
        "content": "Here I found in the docs https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime.\nYou can set the location region of an Azure IR, in which case the activity execution or dispatch will happen in the selected region\n\nSelf-hosted integration runtime can achieve the same goal with higher administrative effort"
      },
      {
        "date": "2021-06-21T19:10:00.000Z",
        "voteCount": 14,
        "content": "Should not this be option A??\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime\n\n\"If you have strict data compliance requirements and need ensure that data do not leave a certain geography, you can explicitly create an Azure IR in a certain region and point the Linked Service to this IR using ConnectVia property. For example, if you want to copy data from Blob in UK South to Azure Synapse Analytics in UK South and want to ensure data do not leave UK, create an Azure IR in UK South and link both Linked Services to this IR.\""
      },
      {
        "date": "2021-06-28T01:48:00.000Z",
        "voteCount": 2,
        "content": "Yes it's option A"
      },
      {
        "date": "2023-05-24T22:53:00.000Z",
        "voteCount": 2,
        "content": "Yes, it is Azure integration runtime - option A"
      },
      {
        "date": "2023-09-06T06:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-03-07T01:55:00.000Z",
        "voteCount": 3,
        "content": "To ensure that the data remains in the UK South region and minimize administrative effort while copying blob data from the storage account to the data warehouse by using Azure Data Factory, you should use the Azure integration runtime."
      },
      {
        "date": "2022-07-31T10:37:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-06-30T05:37:00.000Z",
        "voteCount": 2,
        "content": "I think the first requirement isn't adding much to the equation, so it is primarily focussed on administration which is lowest with A"
      },
      {
        "date": "2022-06-16T22:54:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"
      },
      {
        "date": "2022-01-03T06:09:00.000Z",
        "voteCount": 4,
        "content": "Why would I want to move data to a local host then back to cloud? That sounds a bit unwise, eh?"
      },
      {
        "date": "2021-12-07T07:17:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-08-27T10:38:00.000Z",
        "voteCount": 2,
        "content": "A it is."
      },
      {
        "date": "2021-06-21T09:54:00.000Z",
        "voteCount": 2,
        "content": "Correct answer."
      },
      {
        "date": "2021-06-16T11:38:00.000Z",
        "voteCount": 1,
        "content": "fully agree"
      },
      {
        "date": "2021-06-04T08:03:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54533-exam-dp-203-topic-2-question-26-discussion/",
    "body": "HOTSPOT -<br>You have an Azure SQL database named Database1 and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0019500001.png\" class=\"in-exam-image\"><br>You need to implement Azure Stream Analytics to calculate the average fare per mile by driver.<br>How should you configure the Stream Analytics input for each source? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0019600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0019700001.png\" class=\"in-exam-image\">",
    "answerDescription": "HubA: Stream -<br><br>HubB: Stream -<br><br>Database1: Reference -<br>Reference data (also known as a lookup table) is a finite data set that is static or slowly changing in nature, used to perform a lookup or to augment your data streams. For example, in an IoT scenario, you could store metadata about sensors (which don't change often) in reference data and join it with real time IoT data streams. Azure Stream Analytics loads reference data in memory to achieve low latency stream processing<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-04T08:04:00.000Z",
        "voteCount": 42,
        "content": "Answer is correct"
      },
      {
        "date": "2022-01-14T03:08:00.000Z",
        "voteCount": 16,
        "content": "Crap question. With that data, how are you supposed to link the stream data with the reference data."
      },
      {
        "date": "2024-05-30T09:42:00.000Z",
        "voteCount": 6,
        "content": "Why are some of these questions very simple and others are insanely hard?"
      },
      {
        "date": "2023-09-06T06:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-04-05T04:10:00.000Z",
        "voteCount": 3,
        "content": "Stream, STream, Reference"
      },
      {
        "date": "2022-10-21T01:25:00.000Z",
        "voteCount": 6,
        "content": "Correct\nData stream input : is an unbounded sequence of events over time. \nReference Data input : Reference data is either completely static or changes slowly\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs"
      },
      {
        "date": "2022-07-31T10:40:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-22T23:08:00.000Z",
        "voteCount": 3,
        "content": "why hubA, that contains driver's name, should be processed as stream?? it's a dimension type and should be processed as reference in my opinion"
      },
      {
        "date": "2022-10-31T10:45:00.000Z",
        "voteCount": 4,
        "content": "HubA doesn't contain driver's name. It is Database1 that contains driver's name."
      },
      {
        "date": "2021-10-27T19:32:00.000Z",
        "voteCount": 2,
        "content": "Correct: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs"
      },
      {
        "date": "2021-09-10T06:52:00.000Z",
        "voteCount": 4,
        "content": "I doubt if these questions are really what they asking in the real exam."
      },
      {
        "date": "2021-10-09T17:39:00.000Z",
        "voteCount": 3,
        "content": "I have tried some pretty easy question like this one before in prev exams"
      },
      {
        "date": "2023-10-05T02:46:00.000Z",
        "voteCount": 1,
        "content": "I mean they can't just ask hard questions where you need 5min to think about, no one would pass that. Also they want the certificate to be worth something but also have many people passing the exam and go for Azure instead of AWS, GCP."
      },
      {
        "date": "2021-10-09T17:38:00.000Z",
        "voteCount": 5,
        "content": "it could be real, he is asking  if you can realize the main difference between the real-time data vs the reference data and so you can choose the best service for each one"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55088-exam-dp-203-topic-2-question-27-discussion/",
    "body": "You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub.<br>You need to define a query in the Stream Analytics job. The query must meet the following requirements:<br>\u2711 Count the number of clicks within each 10-second window based on the country of a visitor.<br>\u2711 Ensure that each click is NOT counted more than once.<br>How should you define the Query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SlidingWindow(second, 10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, HoppingWindow(second, 10, 2)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SessionWindow(second, 5, 10)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-21T09:56:00.000Z",
        "voteCount": 25,
        "content": "Correct answer."
      },
      {
        "date": "2021-09-10T06:57:00.000Z",
        "voteCount": 5,
        "content": "keyword : do not overlap"
      },
      {
        "date": "2022-05-24T05:13:00.000Z",
        "voteCount": 1,
        "content": "Not really, the other Count option doesn't overlap. But it does skip."
      },
      {
        "date": "2023-09-06T07:02:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-26T08:47:00.000Z",
        "voteCount": 2,
        "content": "Yes B is correct!!!"
      },
      {
        "date": "2023-04-07T02:36:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-10-19T13:07:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-10-01T04:07:00.000Z",
        "voteCount": 4,
        "content": "A,C are excluded easily by the AVG function, the D also excluded by the session size that is less the window size and this will result in overlapped windows.\nB is left alone Hahaha just want to describe a different method to answer exam questions."
      },
      {
        "date": "2022-07-31T10:44:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2021-12-07T07:19:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-10-28T08:47:00.000Z",
        "voteCount": 2,
        "content": "Correct: https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics"
      },
      {
        "date": "2021-06-16T11:44:00.000Z",
        "voteCount": 2,
        "content": "Correct, Tumbling Window is needed to use periodic time intervals"
      },
      {
        "date": "2021-06-11T02:46:00.000Z",
        "voteCount": 2,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68346-exam-dp-203-topic-2-question-28-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Analytics query that will receive input data from Azure IoT Hub and write the results to Azure Blob storage.<br>You need to calculate the difference in the number of readings per sensor per hour.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0019900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0019900002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: LAG -<br>The LAG analytic operator allows one to look up a \u05d2\u20acprevious\u05d2\u20ac event in an event stream, within certain constraints. It is very useful for computing the rate of growth of a variable, detecting when a variable crosses a threshold, or when a condition starts or stops being true.<br><br>Box 2: LIMIT DURATION -<br>Example: Compute the rate of growth, per sensor:<br>SELECT sensorId,<br>growth = reading -<br>LAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1))<br><br>FROM input -<br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-18T06:30:00.000Z",
        "voteCount": 18,
        "content": "The answer is correct"
      },
      {
        "date": "2023-01-10T09:31:00.000Z",
        "voteCount": 9,
        "content": "The question and answer do not match.  The question asks for \"the difference in THE NUMBER OF READINGS per sensor per hour\".  The answer given is to compute the difference between the current sensor reading and the sensor reading from an hour ago."
      },
      {
        "date": "2024-02-04T09:23:00.000Z",
        "voteCount": 6,
        "content": "Correct.\n\nChoosing the Right Function:\n\nWhen you need to compare a value with its future occurrences: Use LEAD.\nWhen you need to analyze dependencies or trends based on past values: Use LAG.\nWhen you want to identify the maximum or minimum for a group based on a specific order: Use LAST."
      },
      {
        "date": "2023-12-09T16:05:00.000Z",
        "voteCount": 1,
        "content": "Correct, cgatpgt :\nTo calculate the difference in the number of readings per sensor per hour using an Azure Stream Analytics query, you would use the LAG function to access the previous value and then calculate the difference. Here is how you would complete the query:\n\n- Use `LAG` to get the previous reading.\n- Use `LIMIT DURATION` to set the window of time for comparison, which in this case is per hour.\n\nThe completed query would look something like this:\n\n```sql\nSELECT sensorId, \n       reading - LAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1)) AS growth \nFROM input\n```\n\nThis query assumes `reading` is the column holding the sensor data and `sensorId` is the column to partition the data by each sensor. The `LAG` function gets the last reading for the same sensor from the previous hour, and then you subtract this value from the current reading to find the growth."
      },
      {
        "date": "2023-09-06T07:04:00.000Z",
        "voteCount": 1,
        "content": "should be correct"
      },
      {
        "date": "2023-05-20T13:55:00.000Z",
        "voteCount": 4,
        "content": "LAG is the correct answer. Refer the below link. It mentions this example\nhttps://learn.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics"
      },
      {
        "date": "2022-07-31T10:49:00.000Z",
        "voteCount": 4,
        "content": "right answer"
      },
      {
        "date": "2022-04-29T05:06:00.000Z",
        "voteCount": 1,
        "content": "what about LAST?"
      },
      {
        "date": "2022-05-13T01:44:00.000Z",
        "voteCount": 4,
        "content": "LAST is possible, however the code sample in this question does not include any WHEN syntax, so that will rule out LAST"
      },
      {
        "date": "2023-07-19T09:48:00.000Z",
        "voteCount": 2,
        "content": "When clause is optional \nhttps://learn.microsoft.com/en-us/stream-analytics-query/last-azure-stream-analytics\nStill cannot understand the difference between LAG and LAST :("
      },
      {
        "date": "2023-08-11T03:24:00.000Z",
        "voteCount": 3,
        "content": "LAST is the most recent event - literally opposite to the name.. that is why they take LAG which gives the previous event"
      },
      {
        "date": "2021-12-20T15:46:00.000Z",
        "voteCount": 5,
        "content": "Answers as revealed are for computing the rate of growth per sensor. https://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics#examples"
      },
      {
        "date": "2021-12-25T06:18:00.000Z",
        "voteCount": 6,
        "content": "yep, but the question here is unclear"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54902-exam-dp-203-topic-2-question-29-discussion/",
    "body": "You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container.<br>Which type of trigger should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton-demand",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tevent\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-08T09:33:00.000Z",
        "voteCount": 37,
        "content": "correct"
      },
      {
        "date": "2022-01-21T22:08:00.000Z",
        "voteCount": 2,
        "content": "hundered percent :)"
      },
      {
        "date": "2024-05-30T10:26:00.000Z",
        "voteCount": 2,
        "content": "This is definitely the easiest question I have seen on this entire website so far"
      },
      {
        "date": "2024-02-11T14:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct. \nIt is a storage Event trigger\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"
      },
      {
        "date": "2023-12-21T14:27:00.000Z",
        "voteCount": 3,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-10-01T03:54:00.000Z",
        "voteCount": 1,
        "content": "Event-driven"
      },
      {
        "date": "2023-09-06T07:05:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-08T02:02:00.000Z",
        "voteCount": 1,
        "content": "Option D EVENT"
      },
      {
        "date": "2023-01-30T13:13:00.000Z",
        "voteCount": 2,
        "content": "Event grid"
      },
      {
        "date": "2022-07-31T10:50:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-07-09T05:30:00.000Z",
        "voteCount": 4,
        "content": "Correct. One question related to Event-driven architecture (EDA) is must in exam."
      },
      {
        "date": "2022-04-06T09:19:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-12-07T07:20:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2021-10-28T20:30:00.000Z",
        "voteCount": 1,
        "content": "very complex... lol"
      },
      {
        "date": "2021-06-16T11:57:00.000Z",
        "voteCount": 2,
        "content": "Fully agree"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55089-exam-dp-203-topic-2-question-30-discussion/",
    "body": "You have two Azure Data Factory instances named ADFdev and ADFprod. ADFdev connects to an Azure DevOps Git repository.<br>You publish changes from the main branch of the Git repository to ADFdev.<br>You need to deploy the artifacts from ADFdev to ADFprod.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom ADFdev, modify the Git configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom ADFdev, create a linked service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Azure DevOps, create a release pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Azure DevOps, update the main branch."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-16T12:04:00.000Z",
        "voteCount": 33,
        "content": "Correct!"
      },
      {
        "date": "2021-06-11T02:47:00.000Z",
        "voteCount": 9,
        "content": "Answer in Correct!"
      },
      {
        "date": "2024-03-18T11:25:00.000Z",
        "voteCount": 1,
        "content": "Correct. https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-automate-azure-pipelines"
      },
      {
        "date": "2024-01-18T09:37:00.000Z",
        "voteCount": 2,
        "content": "Got this question on my exam on january 17, answer C is correct."
      },
      {
        "date": "2023-09-06T07:06:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-10-01T05:28:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2022-07-31T10:51:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-17T12:51:00.000Z",
        "voteCount": 2,
        "content": "answer is correct"
      },
      {
        "date": "2022-01-04T02:08:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2021-12-07T07:25:00.000Z",
        "voteCount": 2,
        "content": "pipeline is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55510-exam-dp-203-topic-2-question-31-discussion/",
    "body": "You are developing a solution that will stream to Azure Stream Analytics. The solution will have both streaming data and reference data.<br>Which input type should you use for the reference data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure IoT Hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-27T07:13:00.000Z",
        "voteCount": 33,
        "content": "correct, blob storage or azure sql database"
      },
      {
        "date": "2023-05-24T01:00:00.000Z",
        "voteCount": 1,
        "content": "Correct https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#reference-data-input"
      },
      {
        "date": "2021-06-17T07:17:00.000Z",
        "voteCount": 6,
        "content": "This is correct."
      },
      {
        "date": "2024-07-14T17:41:00.000Z",
        "voteCount": 1,
        "content": "B is the best"
      },
      {
        "date": "2023-09-06T07:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-07-31T10:53:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-03-22T13:48:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2021-12-07T07:28:00.000Z",
        "voteCount": 3,
        "content": "for reference data blob storage is correct (or also azure sql database)"
      },
      {
        "date": "2021-10-26T13:17:00.000Z",
        "voteCount": 1,
        "content": "Why not connecting directly Event Hub? Answer D?"
      },
      {
        "date": "2022-05-11T23:44:00.000Z",
        "voteCount": 2,
        "content": "Because the question is about reference data, not streaming data."
      },
      {
        "date": "2021-10-30T07:11:00.000Z",
        "voteCount": 3,
        "content": "Since it is specified that Stream analytics is used , no need for event hub. It will just add cost. Correct me if I am wrong."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74092-exam-dp-203-topic-2-question-32-discussion/",
    "body": "You are designing an Azure Stream Analytics job to process incoming events from sensors in retail environments.<br>You need to process the events to produce a running average of shopper counts during the previous 15 minutes, calculated at five-minute intervals.<br>Which type of window should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsnapshot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thopping\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsliding"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-22T00:03:00.000Z",
        "voteCount": 16,
        "content": "Correct answer."
      },
      {
        "date": "2023-05-18T01:39:00.000Z",
        "voteCount": 7,
        "content": "Answer D\nThe running average is considered. So, it should be sliding window"
      },
      {
        "date": "2023-12-28T02:47:00.000Z",
        "voteCount": 1,
        "content": "For the scenario of producing a running average of shopper counts during the previous 15 minutes, calculated at five-minute intervals, you should use a:\n\nC. Hopping Window"
      },
      {
        "date": "2023-09-06T07:13:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-22T00:02:00.000Z",
        "voteCount": 3,
        "content": "It is Hopping Window, see the picture and the green note:\nhttps://learn.microsoft.com/en-us/stream-analytics-query/hopping-window-azure-stream-analytics"
      },
      {
        "date": "2023-05-25T11:27:00.000Z",
        "voteCount": 3,
        "content": "Chat GPT says option D. sliding window"
      },
      {
        "date": "2023-05-24T08:19:00.000Z",
        "voteCount": 2,
        "content": "A hopping window outputs events at a regular time interval (the hop size) and can be used to produce overlapping windows."
      },
      {
        "date": "2022-07-31T10:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-07-09T06:11:00.000Z",
        "voteCount": 2,
        "content": "Correct. hoppingWindow(minute, 15, 5)"
      },
      {
        "date": "2022-07-07T07:17:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-06-13T11:49:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer."
      },
      {
        "date": "2022-05-17T09:19:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52650-exam-dp-203-topic-2-question-33-discussion/",
    "body": "HOTSPOT -<br>You are designing a monitoring solution for a fleet of 500 vehicles. Each vehicle has a GPS tracking device that sends data to an Azure event hub once per minute.<br>You have a CSV file in an Azure Data Lake Storage Gen2 container. The file maintains the expected geographical area in which each vehicle should be.<br>You need to ensure that when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds. The solution must minimize cost.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0020400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0020500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Stream Analytics -<br><br>Box 2: Hopping -<br>Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap and be emitted more often than the window size. Events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.<br><br>Box 3: Point within polygon -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-03T11:20:00.000Z",
        "voteCount": 89,
        "content": "You do not need a Window function. You just process the data and perform the geospatial check as it arrives. See the same example here:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios"
      },
      {
        "date": "2021-06-07T03:02:00.000Z",
        "voteCount": 7,
        "content": "That's what I thought, there's no reporting over time periods. It's just a case of when this happens, ping it off."
      },
      {
        "date": "2021-12-29T08:38:00.000Z",
        "voteCount": 8,
        "content": "It works, But not the most const effective."
      },
      {
        "date": "2023-10-05T03:52:00.000Z",
        "voteCount": 1,
        "content": "The constraint is: \"a message is added to another event hub for processing within 30 seconds\".\nThe example you provided specifies that: \"Devices can emit their ID and location every minute through a stream called DeviceStreamInput\" so does DeviceStreamInput only emit every minute or can you adapt it? Else you would need a window."
      },
      {
        "date": "2021-06-16T00:50:00.000Z",
        "voteCount": 85,
        "content": "1. Azure Stream Analytics\n2. No Window\n3. Point within Polygon"
      },
      {
        "date": "2024-06-22T08:44:00.000Z",
        "voteCount": 3,
        "content": "Got this question in exam 22/06/2024. but only 3 options in Window without tumbling"
      },
      {
        "date": "2024-05-31T07:25:00.000Z",
        "voteCount": 1,
        "content": "I see that people are mentioning tumbling window here for this question. Would the query look something like this?\n\nSELECT Collect() as allEvents \nINTO Output\nFROM DeviceStreamInput \nJOIN GeoFenceReference\nON ST_WITHIN(DeviceStreamInput.GeoPosition, SiteReferenceInput.Geofence) = 0\nWHERE DeviceStreamInput.DeviceID = GeoFenceReference.DeviceID\nGROUP BY TumblingWindow(second, 30)"
      },
      {
        "date": "2023-12-16T04:43:00.000Z",
        "voteCount": 1,
        "content": "1. Azure Stream Analytics\n2. Sliding (change on event) or No window  for me\n3. Point within Polygon"
      },
      {
        "date": "2023-12-09T16:13:00.000Z",
        "voteCount": 2,
        "content": "Wrong, chatgpt :\n- **Service**: Azure Stream Analytics \u2013 This service is ideal for processing real-time streaming data from IoT devices.\n- **Window**: No window \u2013 As you're processing each GPS event as it arrives, you don't need to aggregate over a time window but process each event individually to check if it's within the expected area.\n- **Analysis type**: Point within polygon \u2013 This analysis type is used for geospatial analytics, where you're checking if a point (the GPS position) is within a predefined polygon (the expected geographical area for the vehicle)."
      },
      {
        "date": "2023-08-13T19:41:00.000Z",
        "voteCount": 3,
        "content": "It is clear tumbling window guys. \nrequirements:- \n1) new data created in every 1 min\n2) if any out of range problem is there then need to send with in 30 sec.\nso window size is 1 min, we will have 1 min of data., so we will not miss data.\nhop size will be 30 sec, so we can process the data in 30 sec and report the problem if any."
      },
      {
        "date": "2023-05-25T11:42:00.000Z",
        "voteCount": 8,
        "content": "Correct answers are:\n1. Azure Stream analytics\n2. Tumbling window\n3. Point within Polygon"
      },
      {
        "date": "2023-01-07T19:24:00.000Z",
        "voteCount": 9,
        "content": "It sounds like you want to use Azure Stream Analytics for this task. Stream Analytics is a real-time analytics service that allows you to analyze and process high volumes of streaming data from various sources, such as Azure Event Hubs.\n\nFor the window, you should use a Tumbling window. A tumbling window is a fixed-sized, non-overlapping window of data. It is well-suited for this scenario because you want to process the data once per minute, and a tumbling window with a size of 1 minute would allow you to do this.\n\nFor the analysis type, you should use Point within polygon. This analysis type allows you to determine whether a GPS position falls within a specific geographical area. You can use the CSV file in the Azure Data Lake Storage Gen2 container to define the expected geographical areas for each vehicle."
      },
      {
        "date": "2022-12-18T15:41:00.000Z",
        "voteCount": 9,
        "content": "Choosing no window is totally wrong. No window means you have to process a msg everytime a event happened. It's costly. But a session window can be trigged when there is a event and can combine all the events in maxduration size(here is 30s) to add a packet of events at one time. And other types of windows are not suitable for the situation here."
      },
      {
        "date": "2022-10-31T00:55:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is correct, should be hopping window with window size is 30 seconds and hope size is 1 minute."
      },
      {
        "date": "2023-07-31T06:01:00.000Z",
        "voteCount": 3,
        "content": "Why exactly? That means that every point is going to fall inside two events, so you'll be pinged two times."
      },
      {
        "date": "2022-08-01T08:38:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct, agree to the point made by  VyshakhUnnikrishnan"
      },
      {
        "date": "2022-07-29T05:38:00.000Z",
        "voteCount": 3,
        "content": "when a GPS position is outside the expected area, a message is added to another event hub for processing within 30 seconds.  - this has to be taken in to account \nNo Window"
      },
      {
        "date": "2022-07-27T13:56:00.000Z",
        "voteCount": 2,
        "content": "1. Azure Stream Analytics\n2. No Window\n3. Point within Polygon"
      },
      {
        "date": "2022-05-17T09:20:00.000Z",
        "voteCount": 2,
        "content": "i thing it is correct"
      },
      {
        "date": "2022-05-12T10:53:00.000Z",
        "voteCount": 3,
        "content": "I am not sure about tumbling window here. I think it would work but what would be the requirement to send an event out every thirty seconds with no data i.e no vehicle has traveled outside the geographic area. In my opinion, a no window could send data to another event hub as needed."
      },
      {
        "date": "2022-03-25T18:30:00.000Z",
        "voteCount": 12,
        "content": "It's Tumbling.\nNo Window: it should run 500 times per minute (500 messages are sent per minute) -&gt; costly.\nTumbling Window: if we configure 15s -&gt; it runs 4 times per minute -&gt; much better. \nHopping Window: Some messages are processed twice (don't care allowing duplication or not) -&gt; costly."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53677-exam-dp-203-topic-2-question-34-discussion/",
    "body": "You are designing an Azure Databricks table. The table will ingest an average of 20 million streaming events per day.<br>You need to persist the events in the table for use in incremental load pipeline jobs in Azure Databricks. The solution must minimize storage costs and incremental load times.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition by DateTime fields.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSink to Azure Queue storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude a watermark column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a JSON format for physical data storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-05-27T12:12:00.000Z",
        "voteCount": 29,
        "content": "The ABS-AQS source is deprecated. For new streams, we recommend using Auto Loader instead."
      },
      {
        "date": "2021-09-01T01:56:00.000Z",
        "voteCount": 15,
        "content": "Why not partition by date? What does the auto loader have to do with streaming jobs?"
      },
      {
        "date": "2024-09-08T16:28:00.000Z",
        "voteCount": 2,
        "content": "Partitioning by DateTime fields helps in organizing the data efficiently, which can significantly reduce the time required for incremental loads. It allows you to quickly access and process only the relevant partitions, rather than scanning the entire dataset1.\nIncluding a watermark column (Option C) is also important for managing late-arriving data and ensuring that only the most recent data is processed. However, it doesn\u2019t directly address storage costs or incremental load times2.\nSinking to Azure Queue storage (Option B) is not suitable for this scenario as it is more appropriate for message queuing rather than persistent storage for large volumes of data3.\nUsing a JSON format for physical data storage (Option D) is not recommended because JSON is not optimized for storage efficiency or query performance. Instead, using a columnar storage format like Parquet or Delta Lake would be more efficient4."
      },
      {
        "date": "2024-04-28T06:09:00.000Z",
        "voteCount": 1,
        "content": "A watermark column is essential for implementing event time-based processing in streaming data scenarios. It helps track the progress of event ingestion and ensures that only the latest data is processed, thereby enabling efficient incremental loading."
      },
      {
        "date": "2024-03-29T10:10:00.000Z",
        "voteCount": 2,
        "content": "Watermark column could reduce the storage"
      },
      {
        "date": "2024-02-04T10:10:00.000Z",
        "voteCount": 2,
        "content": "Well, Azure Queue Storage is a service for storing large numbers of messages. You access messages from anywhere in the world via authenticated calls using HTTP or HTTPS. A queue message can be up to 64 KB in size. A queue may contain millions of messages, up to the total capacity limit of a storage account. Queues are commonly used to create a backlog of work to process asynchronously, like in the Web-Queue-Worker architectural style.\n\nI believe the correct answer is A."
      },
      {
        "date": "2024-01-30T19:23:00.000Z",
        "voteCount": 1,
        "content": "Incremental key and time stamp matching watermark behaviour\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview"
      },
      {
        "date": "2024-02-22T00:57:00.000Z",
        "voteCount": 2,
        "content": "A. more likely a better answer.\n\nA. Partition by DateTime fields:\n\nReason: Partitioning the table by date-related fields (e.g., year, month, day) allows efficient filtering during incremental load jobs. Queries can easily scan only relevant partitions for new data, significantly reducing processing time and associated costs.\nC. Include a watermark column:\n\nReason: A watermark column helps track the progress of data processing, allowing incremental jobs to focus on data newer than the last processed watermark. This ensures efficient updates without reprocessing already loaded data."
      },
      {
        "date": "2023-12-28T02:56:00.000Z",
        "voteCount": 1,
        "content": "chat gpt \no design an efficient Azure Databricks table for ingesting an average of 20 million streaming events per day while minimizing storage costs and incremental load times, you should consider the following:\n\nA. Partition by DateTime fields.\n\nExplanation:\n\nPartitioning by DateTime fields is a common practice for time-series data in Azure"
      },
      {
        "date": "2023-09-06T07:18:00.000Z",
        "voteCount": 2,
        "content": "should be B"
      },
      {
        "date": "2023-08-08T02:07:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2023-06-26T14:43:00.000Z",
        "voteCount": 1,
        "content": "Sinking to Azure Queue storage is not necessary for persisting the events in the Azure Databricks table. Azure Queue storage is typically used for decoupling and asynchronous messaging scenarios and may not directly contribute to minimizing storage costs or incremental load times for the Databricks table."
      },
      {
        "date": "2023-06-22T00:39:00.000Z",
        "voteCount": 3,
        "content": "Probably it is B:\nPartition by date&amp;time is not the best, immagine events with each single partition because of (day, hour, minute, second) =&gt; the requirement is clear, minimiuze the space, etc..\nYou use Watermark when you need to reduce the amount of state data to improve latency during a long-running steaming operation.\nJSON&nbsp;I would exclude because how it is formulated.\nMy answer is B, even if it's deprecated, it's clear that this question is an old one, but looking at the commnents, we can still get in the exam."
      },
      {
        "date": "2023-05-07T02:36:00.000Z",
        "voteCount": 2,
        "content": "A. Partition by DateTime fields: Partitioning the table on frequently used columns such as DateTime fields can improve query performance and reduce incremental load times. Partitioning by DateTime can help to reduce the amount of data scanned during query execution and facilitate incremental loading."
      },
      {
        "date": "2023-03-27T23:14:00.000Z",
        "voteCount": 2,
        "content": "is the question outdated?"
      },
      {
        "date": "2023-02-12T06:14:00.000Z",
        "voteCount": 1,
        "content": "Im sure it is A. Partition by DateTime!!"
      },
      {
        "date": "2022-12-08T22:18:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/structured-streaming/aqs"
      },
      {
        "date": "2022-11-20T13:56:00.000Z",
        "voteCount": 4,
        "content": "question is deprecated, AutoLoader is the way to do the incremental loads"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52539-exam-dp-203-topic-2-question-35-discussion/",
    "body": "HOTSPOT -<br>You have a self-hosted integration runtime in Azure Data Factory.<br>The current status of the integration runtime has the following configurations:<br>\u2711 Status: Running<br>\u2711 Type: Self-Hosted<br>\u2711 Version: 4.4.7292.1<br>\u2711 Running / Registered Node(s): 1/1<br>\u2711 High Availability Enabled: False<br>\u2711 Linked Count: 0<br>\u2711 Queue Length: 0<br>\u2711 Average Queue Duration. 0.00s<br>The integration runtime has the following node details:<br>\u2711 Name: X-M<br>\u2711 Status: Running<br>\u2711 Version: 4.4.7292.1<br>\u2711 Available Memory: 7697MB<br>\u2711 CPU Utilization: 6%<br>\u2711 Network (In/Out): 1.21KBps/0.83KBps<br>\u2711 Concurrent Jobs (Running/Limit): 2/14<br>\u2711 Role: Dispatcher/Worker<br>\u2711 Credential Status: In Sync<br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0020800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0020900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: fail until the node comes back online<br>We see: High Availability Enabled: False<br>Note: Higher availability of the self-hosted integration runtime so that it's no longer the single point of failure in your big data solution or cloud data integration with<br>Data Factory.<br><br>Box 2: lowered -<br>We see:<br>Concurrent Jobs (Running/Limit): 2/14<br>CPU Utilization: 6%<br>Note: When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-24T20:44:00.000Z",
        "voteCount": 34,
        "content": "1/14 = 0.07\n6% = 0.06\n\nshould be lowered."
      },
      {
        "date": "2022-05-26T12:33:00.000Z",
        "voteCount": 4,
        "content": "The question mentions 2/14 which is 0.14, therefore it can be increased."
      },
      {
        "date": "2023-07-31T06:23:00.000Z",
        "voteCount": 2,
        "content": "Why is this the calculation you make? I see 6% utilization, so 94% to go, so the amount can be raised."
      },
      {
        "date": "2022-01-18T04:58:00.000Z",
        "voteCount": 10,
        "content": "0.06/2 = 0.03\n0.03 * 14 = 0.42 = maximally 42% of cpu for all jobs\nisn't this better?"
      },
      {
        "date": "2021-06-17T08:00:00.000Z",
        "voteCount": 24,
        "content": "\"We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\"\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime"
      },
      {
        "date": "2024-02-11T18:32:00.000Z",
        "voteCount": 1,
        "content": "In essence, the 6% of the CPU depicts a low resource usage, therefore the concurrent jobs limit should be increased."
      },
      {
        "date": "2024-02-11T18:30:00.000Z",
        "voteCount": 1,
        "content": "In essence, the 6% of the CPU depicts a low resource usage, therefore the concurrent jobs limit."
      },
      {
        "date": "2024-07-02T09:47:00.000Z",
        "voteCount": 2,
        "content": "The answer provided by edba is explaining the question as well as the answer very well. The second question is asking what should we do with the LIMIT of the concurrent jobs which is currently set to 14. Since only 2 jobs are running, the LIMIT of concurrent jobs can be reduced from 14, so the question is asking about what should be do with the limit of concurrent jobs and not about what should we do with the actual concurrent jobs that can be run.\n\nCheck the below link\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#scale-considerations"
      },
      {
        "date": "2024-02-03T05:50:00.000Z",
        "voteCount": 3,
        "content": "So the math is simple we have 2 concurrent processes running causing 6% CPU utilization. The concurrent process limit is 14 so if we multiply both sides (2 process = 6%) we get (14process = 42%) which is under utilized. We should raise concurrent process limit to 26 which gives us cpu utilization of 76%. \nALL THESE CALCULATIONS ARE DONE CONSIDERING EACH CONCURRENT PROCESS CAUSE SAME  CPU UTILIZATION WHICH IS 3%."
      },
      {
        "date": "2024-03-27T19:38:00.000Z",
        "voteCount": 1,
        "content": "100% / 3% = 33.33 SHOULD BE THE NEW LIMIT -&gt; RAISED"
      },
      {
        "date": "2024-01-30T19:37:00.000Z",
        "voteCount": 1,
        "content": "Agreed on the answer for the first part but not the second.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\n\"We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\"\nI think the answer should be raised"
      },
      {
        "date": "2024-01-18T09:39:00.000Z",
        "voteCount": 2,
        "content": "Got this question on my exam on january 17, I wasn't sure but I put the same answers as the dump"
      },
      {
        "date": "2023-12-09T16:38:00.000Z",
        "voteCount": 3,
        "content": "Chatgpt :\nGiven that high availability is not enabled for the self-hosted integration runtime, the correct answer for the first statement is:\n\n- **Fail until the node comes back online.**\n\nFor the second statement regarding the number of concurrent jobs, considering that the CPU utilization is quite low at 6%, and there is a significant difference between the number of running jobs (2) and the limit (14), the correct answer should be:\n\n- **Left as is.**\n\nThere is no indication from the given data that the concurrent jobs limit needs to be adjusted, as the system is currently underutilized."
      },
      {
        "date": "2023-10-30T12:12:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT says: *it should be raised* because there's currently a very low CPU utilization (only 6%) and two concurrent jobs running out of a limit of 14. The fact that the CPU utilization is quite low suggests that your integration runtime has available processing capacity."
      },
      {
        "date": "2023-10-05T04:32:00.000Z",
        "voteCount": 1,
        "content": "I don't know why there is this one standard bs answer with lowered everywhere.\nSo only 2 concurrent jobs are running out of 14 possible and CPU usage is at 6%, does it not make sense to raise the concurrent jobs to be even 14/14 and still have only 42% CPU usage. Or is this question aiming at something else?"
      },
      {
        "date": "2023-09-08T04:06:00.000Z",
        "voteCount": 2,
        "content": "Left as it is"
      },
      {
        "date": "2023-09-06T07:32:00.000Z",
        "voteCount": 1,
        "content": "be lowered"
      },
      {
        "date": "2023-06-23T20:30:00.000Z",
        "voteCount": 4,
        "content": "Based on the information provided, the CPU Utilization is 6% and the Concurrent Jobs (Running/Limit) is 2/14. This indicates that the integration runtime is utilizing only 6% of the available CPU capacity and currently running 2 out of a maximum limit of 14 concurrent jobs.\n\nGiven this information, the appropriate answer choice for the completion statement would be:\n\nConcurrent Job should be scaled up\n\nSince the current CPU utilization is relatively low at 6% and there is still capacity available for running additional jobs, scaling up the concurrent job limit would allow for more jobs to run simultaneously and make better use of the available resources."
      },
      {
        "date": "2023-06-22T01:18:00.000Z",
        "voteCount": 2,
        "content": "We are talking about max number of job running in parallel! \nIf you have available resource of course it is recommanded to raise up the current limit to afford future load. \nAlso Microsoft recommned that:\nhttps://learn.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\nWe recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node. I think this is the case, also the question doesn't tell you it's mandatory, what should! So I think we should follow recommandation and raise up the limit."
      },
      {
        "date": "2023-05-23T10:42:00.000Z",
        "voteCount": 3,
        "content": "when the explanation is \"scale up by increasing the number\" then why the answer is \"Lowered\"???"
      },
      {
        "date": "2023-01-14T05:36:00.000Z",
        "voteCount": 3,
        "content": "I would leave it as it is.\nSee:\nhttps://learn.microsoft.com/en-us/azure/data-factory/monitor-integration-runtime\n\n\"The default value of the concurrent jobs limit is set based on the machine size. The factors used to calculate this value depend on the amount of RAM and the number of CPU cores of the machine. So the more cores and the more memory, the higher the default limit of concurrent jobs.\n\nYou scale out by increasing the number of nodes. When you increase the number of nodes, the concurrent jobs limit is the sum of the concurrent job limit values of all the available nodes. For example, if one node lets you run a maximum of twelve concurrent jobs, then adding three more similar nodes lets you run a maximum of 48 concurrent jobs (that is, 4 x 12). We recommend that you increase the concurrent jobs limit only when you see low resource usage with the default values on each node.\""
      },
      {
        "date": "2022-12-23T06:52:00.000Z",
        "voteCount": 1,
        "content": "the concurrent jobs limit is the sum of the concurrent job limit values of all the available nodes"
      },
      {
        "date": "2022-11-14T04:37:00.000Z",
        "voteCount": 3,
        "content": "Here is my thinking of this. High availability is False, so no scaling. The 2nd Q is what should be done with the number of concurent jobs, not scaling up CPU. Since there are only 2 running jobs of possible 14 and CPU itilization is only 6% the number of concurent jobs should be Increased. If left as is we are overspending, if decreased we are still overspending even more since CPU utilization will be lovered too."
      },
      {
        "date": "2022-12-01T01:23:00.000Z",
        "voteCount": 1,
        "content": "When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run.\nhttps://learn.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime?tabs=data-factory#scale-up"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54675-exam-dp-203-topic-2-question-36-discussion/",
    "body": "You have an Azure Databricks workspace named workspace1 in the Standard pricing tier.<br>You need to configure workspace1 to support autoscaling all-purpose clusters. The solution must meet the following requirements:<br>\u2711 Automatically scale down workers when the cluster is underutilized for three minutes.<br>\u2711 Minimize the time it takes to scale to the maximum number of workers.<br>\u2711 Minimize costs.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable container services for workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade workspace1 to the Premium pricing tier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Cluster Mode to High Concurrency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster policy in workspace1."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-27T10:19:00.000Z",
        "voteCount": 19,
        "content": "B is the correct answer.\n\nAutomated (job) clusters always use optimized autoscaling. The type of autoscaling performed on all-purpose clusters depends on the workspace configuration.\n\nStandard autoscaling is used by all-purpose clusters in workspaces in the Standard pricing tier. Optimized autoscaling is used by all-purpose clusters in the Azure Databricks Premium Plan.\nhttps://docs.databricks.com/clusters/cluster-config-best-practices.html"
      },
      {
        "date": "2022-01-05T01:48:00.000Z",
        "voteCount": 7,
        "content": "We definitely need \"Optimized Autoscaling\" (not Standard Autoscaling) which is only part of Premium Plan. \n\nReason: We need to scale down after 3 min underutilization and Standard Autoscaling only allows scaling down after at least 10 minutes.\n\nStandard autoscaling: \"Scales down only when the cluster is completely idle and it has been underutilized for the last 10 minutes.\"\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2024-02-11T18:54:00.000Z",
        "voteCount": 2,
        "content": "B and C are both required to do the job .But .the FIRST thing to do before creating  the AUTO-SCALING CLUSTER POLICY s to migrate or upgrade to premium tier where AUTO-SCALING is supported or enabled.This explains why B is the valid answer .\nIn conclusion C is part of the process ,however not the first thing to do"
      },
      {
        "date": "2023-09-06T07:33:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-08T02:16:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-06-22T02:17:00.000Z",
        "voteCount": 1,
        "content": "I've finally found a valid answer:\nhttps://learn.microsoft.com/en-us/azure/databricks/administration-guide/clusters/policies"
      },
      {
        "date": "2023-06-28T08:03:00.000Z",
        "voteCount": 3,
        "content": "False, Cluster policies require the Premium plan.&nbsp;:) So B is the correct answer."
      },
      {
        "date": "2023-04-25T01:25:00.000Z",
        "voteCount": 5,
        "content": "B doesn't minimize the costs. To support autoscaling all-purpose clusters in Azure Databricks, you need to create a cluster policy that specifies the auto-scaling settings. The cluster policy allows you to specify when to add or remove workers based on the workload on the cluster.\n\nFor this scenario, the cluster policy should be configured to automatically scale down workers when the cluster is underutilized for three minutes. This will help to minimize costs by reducing the number of idle workers. The policy should also be configured to scale to the maximum number of workers quickly to minimize the time it takes to process workloads.\n\nEnabling container services for workspace1 (option A) is not necessary for autoscaling all-purpose clusters. Upgrading workspace1 to the Premium pricing tier (option B) may not be necessary and may not be cost-effective depending on your specific requirements. Setting Cluster Mode to High Concurrency (option C) is not related to autoscaling all-purpose clusters."
      },
      {
        "date": "2023-06-18T19:12:00.000Z",
        "voteCount": 3,
        "content": "Cluster policies are available only in the Premium pricing tier of Azure Data bricks, and not in the Standard pricing tier."
      },
      {
        "date": "2022-08-01T08:51:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-17T09:21:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-17T13:53:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-01-14T04:14:00.000Z",
        "voteCount": 6,
        "content": "Not sure if this is a valid question anymore. This link shows that the standard pricing tier supports optimised autoscaling. \n\nhttps://databricks.com/product/azure-pricing"
      },
      {
        "date": "2022-10-01T23:55:00.000Z",
        "voteCount": 1,
        "content": "the autoscaling is under the premuim plan not the standrd one and this is clear in the link you shared."
      },
      {
        "date": "2022-11-20T02:18:00.000Z",
        "voteCount": 1,
        "content": "as Jaws1990 says it is available on both on the link.  I has green for both types"
      },
      {
        "date": "2022-11-20T14:48:00.000Z",
        "voteCount": 1,
        "content": "no difference anymore between Standard and Premium, indeed"
      },
      {
        "date": "2024-04-11T06:12:00.000Z",
        "voteCount": 1,
        "content": "https://www.databricks.com/product/pricing/platform-addons\nCluster Policy obliga available on Premium"
      },
      {
        "date": "2021-12-22T09:36:00.000Z",
        "voteCount": 3,
        "content": "They need to use Optimized autoscaling for adapting requirements.\n- Optimized autoscaling is used by all-purpose clusters in the Azure Databricks Premium Plan.\n- On job clusters, scales down if the cluster is underutilized over the last 40 seconds.\n- On all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds.\nreference:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-12-21T14:51:00.000Z",
        "voteCount": 1,
        "content": "1. Both standard and premium pricing tire support Autopilot Cluster. Autopilot support Autoscaling and Terminate after X minutes of inactivity.\n2. 'Cluster Policies' is only supported by premium pricing tire. Control cost by limiting per cluster maximum cost.\n3. standard pricing tire is cheaper than premium pricing tire. \nBase on these 3 items, I don't figure out why it has to upgrade to Premium pricing tier."
      },
      {
        "date": "2021-12-21T15:04:00.000Z",
        "voteCount": 1,
        "content": "A .Enable Databricks Container Service only when you need to use customer containers, so it is not a correct answer.\n\nI vote C to be the correct Answer."
      },
      {
        "date": "2021-12-07T12:41:00.000Z",
        "voteCount": 6,
        "content": "Answer B is correct. One has to check on the documentation. There are two autoscaling solutions:\nstandard autoscaling (Standard Tier) and optimized autoscaling (Premimum Tier).\n\nSince there is a requirement of downscaling after three minutes of underutilization, only optimized autoscaling can offer such a solution.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#optimized-autoscaling\n\nOn all-purpose clusters, scales down if the cluster is underutilized over the last 150 seconds."
      },
      {
        "date": "2021-10-30T06:35:00.000Z",
        "voteCount": 5,
        "content": "Why is the answer not D? Autoscaling is available in the Standard pricing tier. Since \"costs\" is also a factor in this question, why upgrade to premium?"
      },
      {
        "date": "2021-08-14T09:30:00.000Z",
        "voteCount": 2,
        "content": "Is this correct?"
      },
      {
        "date": "2021-09-24T13:58:00.000Z",
        "voteCount": 1,
        "content": "Not sure, what about the cost factor and premium doesn\u2019t minimise cost."
      },
      {
        "date": "2021-06-05T18:57:00.000Z",
        "voteCount": 3,
        "content": "Concurent Jobs should be raised - There is less cpu utilization"
      },
      {
        "date": "2021-06-05T18:58:00.000Z",
        "voteCount": 10,
        "content": "please ignore this, it was meant for the question before"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54801-exam-dp-203-topic-2-question-37-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure Stream Analytics solution that will analyze Twitter data.<br>You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.<br>Solution: You use a tumbling window, and you set the window size to 10 seconds.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-07T05:50:00.000Z",
        "voteCount": 37,
        "content": "correct answer"
      },
      {
        "date": "2024-07-07T01:03:00.000Z",
        "voteCount": 1,
        "content": "Tumbling Window\nHopping Window\nSliding Window\nSession Window"
      },
      {
        "date": "2024-01-18T09:41:00.000Z",
        "voteCount": 1,
        "content": "Got this question on my exam on january 17, answer A is correct."
      },
      {
        "date": "2023-12-21T14:26:00.000Z",
        "voteCount": 1,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-09-06T07:34:00.000Z",
        "voteCount": 1,
        "content": "correct \nhttps://learn.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics"
      },
      {
        "date": "2022-08-01T11:08:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-07-10T11:03:00.000Z",
        "voteCount": 2,
        "content": "this question appears at topic 2 question 18 and it said the correct answer was hopping window with 10'' window... so, what's the right correct answer?"
      },
      {
        "date": "2022-10-11T14:17:00.000Z",
        "voteCount": 5,
        "content": "Both are correct. A Hopping window with hop-size = window-size is identical to a Tumbling window."
      },
      {
        "date": "2022-04-08T01:53:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-02-22T09:47:00.000Z",
        "voteCount": 1,
        "content": "correct \"D cholo"
      },
      {
        "date": "2022-01-26T21:45:00.000Z",
        "voteCount": 1,
        "content": "quite trivial, yes - correct answer: https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics#:~:text=Tumbling%20windows%20are%20a%20series,into%2010%2Dsecond%20tumbling%20windows."
      },
      {
        "date": "2022-01-05T08:46:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-01-05T01:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-12-21T19:42:00.000Z",
        "voteCount": 1,
        "content": "A is Correct Answer"
      },
      {
        "date": "2021-12-07T07:35:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-11-15T02:16:00.000Z",
        "voteCount": 1,
        "content": "Correct answer !"
      },
      {
        "date": "2021-10-09T04:00:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-10-01T02:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55002-exam-dp-203-topic-2-question-38-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are designing an Azure Stream Analytics solution that will analyze Twitter data.<br>You need to count the tweets in each 10-second window. The solution must ensure that each tweet is counted only once.<br>Solution: You use a session window that uses a timeout size of 10 seconds.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-09T12:10:00.000Z",
        "voteCount": 22,
        "content": "answer correct"
      },
      {
        "date": "2021-08-29T05:05:00.000Z",
        "voteCount": 13,
        "content": "False as we need to count tweets in EACH 10 sec. Session windows can have gaps if there is no event happening during the window size"
      },
      {
        "date": "2024-04-23T14:36:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-18T09:41:00.000Z",
        "voteCount": 2,
        "content": "Got this question on my exam on january 17, answer B is correct."
      },
      {
        "date": "2023-09-06T09:25:00.000Z",
        "voteCount": 2,
        "content": "A session window is designed to group events together that occur within a certain time frame, but it doesn't guarantee that each event will be counted only once."
      },
      {
        "date": "2023-09-06T07:35:00.000Z",
        "voteCount": 1,
        "content": "NO is the answer"
      },
      {
        "date": "2023-05-11T06:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer"
      },
      {
        "date": "2022-12-29T05:27:00.000Z",
        "voteCount": 1,
        "content": "NO is correct"
      },
      {
        "date": "2022-08-01T11:09:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-04-08T01:54:00.000Z",
        "voteCount": 4,
        "content": "session window wouldn't count periods with no tweets"
      },
      {
        "date": "2022-01-10T01:08:00.000Z",
        "voteCount": 1,
        "content": "This should be yes as the max duration of window is 10 secs and timeout size is also 10 sec . So this means irrespective of any events comes or not timeout is happening or not window size will be remain as 10 sec."
      },
      {
        "date": "2022-01-05T08:46:00.000Z",
        "voteCount": 1,
        "content": "B - it has to be tumbling window"
      },
      {
        "date": "2021-12-07T07:36:00.000Z",
        "voteCount": 1,
        "content": "correct: no"
      },
      {
        "date": "2021-07-01T15:14:00.000Z",
        "voteCount": 3,
        "content": "I think you can use a session window with 10 sec timeout... is like tumbling window with 10 second window size."
      },
      {
        "date": "2021-07-14T17:22:00.000Z",
        "voteCount": 1,
        "content": "Agree, cause it doesn't overlap any event, just group then in a given time that we can define;"
      },
      {
        "date": "2021-07-22T05:56:00.000Z",
        "voteCount": 4,
        "content": "I Disagree. The session could be extended if the maximum duration is set longer than the timeout."
      },
      {
        "date": "2021-07-22T02:37:00.000Z",
        "voteCount": 15,
        "content": "The important thing to remember in a session window is the maximum duration. So theoretically a 10 second timout can still result in a window of 20 minutes for example (if every 9 seconds a new event comes in and the window never \"closes\"). If the maximum duration would be 10 seconds, I would agree. But as the question is worded right now, the answer is NO.\n\nhttps://docs.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67583-exam-dp-203-topic-2-question-39-discussion/",
    "body": "You use Azure Stream Analytics to receive data from Azure Event Hubs and to output the data to an Azure Blob Storage account.<br>You need to output the count of records received from the last five minutes every minute.<br>Which windowing function should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTumbling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSliding",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHopping\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T11:01:00.000Z",
        "voteCount": 15,
        "content": "Respuesta Correcta: Hopping"
      },
      {
        "date": "2022-08-01T11:11:00.000Z",
        "voteCount": 5,
        "content": "Hopping is right"
      },
      {
        "date": "2024-09-22T00:07:00.000Z",
        "voteCount": 1,
        "content": "The secret here is \"last/previous five minutes every minute\".\n\nIf they were 5x5 or \"non repeated counts\" or no-overlap, it could be both Hop and Tumbling, being Tumbling the most appropriate."
      },
      {
        "date": "2024-03-29T10:28:00.000Z",
        "voteCount": 1,
        "content": "And why not sliding?"
      },
      {
        "date": "2023-09-14T02:46:00.000Z",
        "voteCount": 1,
        "content": "Updated comment: Hopping window since the we are 'hopping' every minute to count the number of records received in the last 5 minutes, 3 arguments: hooping(minute, 5, 1)"
      },
      {
        "date": "2023-09-14T02:44:00.000Z",
        "voteCount": 1,
        "content": "Hopping window since the we are 'hopping' every minute to count the number of records received in the last 5 minutes, 2 arguments: hopping(minute, 1)"
      },
      {
        "date": "2023-09-06T07:36:00.000Z",
        "voteCount": 1,
        "content": "Hopping"
      },
      {
        "date": "2023-08-08T02:21:00.000Z",
        "voteCount": 2,
        "content": "D is correct answer"
      },
      {
        "date": "2023-04-25T02:00:00.000Z",
        "voteCount": 3,
        "content": "A hopping window would not be the best option for this scenario because it does not allow you to set a sliding interval that is less than the window size.\n\nIn a hopping window, the window size is fixed, and the window \"hops\" forward by a specified number of intervals. For example, if you set a hopping window size of five minutes and a hop size of one minute, then the first window would include data from the first five minutes, the second window would include data from the second through sixth minutes, the third window would include data from the third through seventh minutes, and so on.\n\nIn this scenario, if you set the hopping window size to five minutes, you would only output the count of records every five minutes, which does not meet the requirement of outputting the count of records every minute. Therefore, a sliding window would be a better choice as it allows you to output data at smaller sliding intervals, which is required in this scenario."
      },
      {
        "date": "2022-06-16T23:22:00.000Z",
        "voteCount": 2,
        "content": "Why shouldn't it be sliding?"
      },
      {
        "date": "2022-05-11T00:53:00.000Z",
        "voteCount": 1,
        "content": "Why is sliding not correct?"
      },
      {
        "date": "2022-06-19T15:04:00.000Z",
        "voteCount": 1,
        "content": "I want to know too"
      },
      {
        "date": "2022-10-29T18:34:00.000Z",
        "voteCount": 2,
        "content": "I was thinking sliding as well but a sliding window wouldn't have advanced or returned a result if there was no data e.g. the count was zero.  Hopping advances when there is no input."
      },
      {
        "date": "2022-03-23T11:58:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-03-11T11:47:00.000Z",
        "voteCount": 1,
        "content": "Hopping is correct!"
      },
      {
        "date": "2022-02-11T19:40:00.000Z",
        "voteCount": 1,
        "content": "hopping window"
      },
      {
        "date": "2022-01-05T08:47:00.000Z",
        "voteCount": 4,
        "content": "Hopping window is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68627-exam-dp-203-topic-2-question-40-discussion/",
    "body": "HOTSPOT -<br>You configure version control for an Azure Data Factory instance as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0021500001.jpg\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0021600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0021600002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: adf_publish -<br>The Publish branch is the branch in your repository where publishing related ARM templates are stored and updated. By default, it's adf_publish.<br>Box 2: / dwh_batchetl/adf_publish/contososales<br>Note: RepositoryName (here dwh_batchetl): Your Azure Repos code repository name. Azure Repos projects contain Git repositories to manage your source code as your project grows. You can create a new repository or use an existing repository that's already in your project.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/source-control",
    "votes": [],
    "comments": [
      {
        "date": "2022-06-14T05:47:00.000Z",
        "voteCount": 25,
        "content": "Correct Answer i test it in devops"
      },
      {
        "date": "2022-02-10T14:59:00.000Z",
        "voteCount": 11,
        "content": "The assets are in the main branch which is the collaboration branch. \nThe template is repository/adf_publish/datafactoryname"
      },
      {
        "date": "2022-03-22T06:00:00.000Z",
        "voteCount": 3,
        "content": "could you please paste the source of that? I can't find it"
      },
      {
        "date": "2024-02-11T19:28:00.000Z",
        "voteCount": 1,
        "content": "This dump is so informative. Please can someone subscribe for me . My exam is this week Saturday (17/02/ 2024) .I will pay back second week of April.\nI am truly financially challenged now. \nI want to go through all the questions here.before my exam but the Catcha won't allow me"
      },
      {
        "date": "2023-09-06T07:37:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-05-11T06:04:00.000Z",
        "voteCount": 3,
        "content": "Adf_publish \n/Dwh_batchetl/adf_publish/contososales"
      },
      {
        "date": "2022-08-01T11:23:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-07-12T08:43:00.000Z",
        "voteCount": 1,
        "content": "Can someone please refer to below link which includes video as well to answer this question?\n&gt;&gt;&gt; https://docs.microsoft.com/en-us/azure/data-factory/source-control"
      },
      {
        "date": "2022-04-08T06:57:00.000Z",
        "voteCount": 2,
        "content": "I'm not sure, but I think we don't see the complete picture, so we cannot answer the second question for sure. See: https://docs.microsoft.com/en-us/azure/data-factory/source-control#github-settings"
      },
      {
        "date": "2022-02-12T07:58:00.000Z",
        "voteCount": 3,
        "content": "Can someone confirm the correct answer? Is it:\n1. adf_publish\n2. /.\nPlease let me know"
      },
      {
        "date": "2022-02-09T09:13:00.000Z",
        "voteCount": 2,
        "content": "The answers are correct."
      },
      {
        "date": "2022-01-13T09:18:00.000Z",
        "voteCount": 2,
        "content": "Second answer should be contososales"
      },
      {
        "date": "2022-01-04T15:33:00.000Z",
        "voteCount": 6,
        "content": "I dont think we ever refer to locations as REPO/BRANCH/PATH in devops. For me it is / as we assume the branch could be any and still location would be /"
      },
      {
        "date": "2022-01-16T10:23:00.000Z",
        "voteCount": 3,
        "content": "I would say so as well"
      },
      {
        "date": "2022-06-25T08:51:00.000Z",
        "voteCount": 2,
        "content": "You are thinking in an agnostic way and this is Azure DevOps"
      },
      {
        "date": "2022-01-26T21:53:00.000Z",
        "voteCount": 4,
        "content": "Yeah, totally agree. Nobody uses this notation"
      },
      {
        "date": "2022-01-04T10:07:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-12-26T22:45:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68090-exam-dp-203-topic-2-question-41-discussion/",
    "body": "HOTSPOT -<br>You are designing an Azure Stream Analytics solution that receives instant messaging data from an Azure Event Hub.<br>You need to ensure that the output from the Stream Analytics job counts the number of messages per time zone every 15 seconds.<br>How should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0021700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0021800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: timestamp by -<br><br>Box 2: TUMBLINGWINDOW -<br>Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.<br><img src=\"/assets/media/exam-media/04259/0021900001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-18T07:21:00.000Z",
        "voteCount": 28,
        "content": "The answers are correct"
      },
      {
        "date": "2023-03-25T07:42:00.000Z",
        "voteCount": 6,
        "content": "reason for \"TIMESTAMPBY\":\nhttps://learn.microsoft.com/en-us/stream-analytics-query/timestamp-by-azure-stream-analytics"
      },
      {
        "date": "2023-07-31T06:39:00.000Z",
        "voteCount": 2,
        "content": "But a TIMESTAMP BY would need an OVER clause if I see it correctly, and there is none here."
      },
      {
        "date": "2023-11-22T15:30:00.000Z",
        "voteCount": 1,
        "content": "As per the syntax, OVER is optional. \nhttps://learn.microsoft.com/en-us/stream-analytics-query/timestamp-by-azure-stream-analytics"
      },
      {
        "date": "2023-09-06T07:39:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-03-15T06:13:00.000Z",
        "voteCount": 1,
        "content": "which one is correct? many question left confusing depending on discussion.\nand I am not trusting the answer as many answers are wrong."
      },
      {
        "date": "2023-08-17T07:12:00.000Z",
        "voteCount": 1,
        "content": "you need to use the answers from ET and discussions to find the best answer yourself just like everyone on here."
      },
      {
        "date": "2022-08-01T11:27:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      },
      {
        "date": "2022-01-14T09:33:00.000Z",
        "voteCount": 3,
        "content": "I think it's system.timestamp()"
      },
      {
        "date": "2022-05-29T15:54:00.000Z",
        "voteCount": 1,
        "content": "From examples, I can only see system.timestamp() used after SELECT, not FROM."
      },
      {
        "date": "2021-12-16T15:47:00.000Z",
        "voteCount": 2,
        "content": "It only says about window size, not sure why tumbling window not hopping."
      },
      {
        "date": "2021-12-17T02:53:00.000Z",
        "voteCount": 22,
        "content": "Syntax would not be correct since hopping window expects three parameters.\nhttps://docs.microsoft.com/en-us/stream-analytics-query/hopping-window-azure-stream-analytics\n\nTumbling window is the correct answer"
      },
      {
        "date": "2021-12-15T19:59:00.000Z",
        "voteCount": 2,
        "content": "Correcto"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67825-exam-dp-203-topic-2-question-42-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Factory instance named ADF1 and two Azure Synapse Analytics workspaces named WS1 and WS2.<br>ADF1 contains the following pipelines:<br>\u2711 P1: Uses a copy activity to copy data from a nonpartitioned table in a dedicated SQL pool of WS1 to an Azure Data Lake Storage Gen2 account<br>\u2711 P2: Uses a copy activity to copy data from text-delimited files in an Azure Data Lake Storage Gen2 account to a nonpartitioned table in a dedicated SQL pool of WS2<br>You need to configure P1 and P2 to maximize parallelism and performance.<br>Which dataset settings should you configure for the copy activity if each pipeline? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0022000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0022100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Set the Copy method to PolyBase<br>While SQL pool supports many loading methods including non-Polybase options such as BCP and SQL BulkCopy API, the fastest and most scalable way to load data is through PolyBase. PolyBase is a technology that accesses external data stored in Azure Blob storage or Azure Data Lake Store via the T-SQL language.<br>Box 2: Set the Copy method to Bulk insert<br>Polybase not possible for text files. Have to use Bulk insert.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/load-data-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-21T14:55:00.000Z",
        "voteCount": 96,
        "content": "how to use PolyBase when copy data from Synapse to file ? I don't have idea.\nMoreover PolyBase option is available only when the target is Synapse\n\nit should be\nP1:  Set the partition option to \"Dynamic range \"\nP2: PolyBase \n\nregarding to P1\nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#parallel-copy-from-synapse-analytics\nScenario: \"Full load from large table, without physical partitions..\" -&gt; \nSuggested settings: Partition options: Dynamic range partition."
      },
      {
        "date": "2023-08-08T06:49:00.000Z",
        "voteCount": 1,
        "content": "It should be: \nP1: PolyBase\nP2: PolyBase\n\n\"PolyBase is the best choice when you are loading or exporting large volumes of data, or you need faster performance.\"\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool\n\nRegarding \"dynamic range partitions\": \n\" As repartitioning data takes time, Use [sic] current partitioning is recommended in most scenarios.\" -&gt; dynamic partitioning is NOT selected\nRef: https://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-performance"
      },
      {
        "date": "2024-02-20T10:30:00.000Z",
        "voteCount": 2,
        "content": "P1: Set the partition to \"Dynamic range\"\nP2: Polybase\n\nthe reason for P1 is that Polybase is a technology on Azure Synapse Analytics that can read from external sources but can't insert data there, so it is able to read from Data Lake Storage, but won't be able to write there.\n\non the second case, since we want to write to a SQL Pool, it will work, that's why P2 is Polybase.\n\nBesides, Dynamic range partitioning is a technique to partition a non-partition table that allows to parallelize the reading of the source data, which makes it more faster."
      },
      {
        "date": "2021-12-22T08:23:00.000Z",
        "voteCount": 46,
        "content": "P1: Copy data from SQL to Data Lake. \n\u2022\tBulk insert and PolyBase are not a choice in Sink tab if target is Data Lake. So they are not correct. \n\u2022\tIsolation level can be setup if SQL database is the source. Repeatable Read means that locks are placed on all data that is used in a query. Don't think it maximize parallelism and performance.\n\u2022\tSet the Partition option to Dynamic range\nCan be setup if source is SQL in copy activity. And it maximizes parallelism and performance. So I select this option.\n\nP2: Copy data from Data Lake to SQL. It is for sure to select PolyBase."
      },
      {
        "date": "2024-07-07T01:16:00.000Z",
        "voteCount": 6,
        "content": "P1:\nSet the Partition option to Dynamic range: 171 votes\nSet the Copy method to PolyBase: 26 votes\nSet the Copy method to Bulk insert: 3 votes\nSet the Isolation level to Repeatable read: 0 votes\nP2:\nSet the Copy method to PolyBase: 95 votes\nSet the Copy method to Bulk insert: 8 votes\nSet the Isolation level to Repeatable read: 0 votes\nSet the Partition option to Dynamic range: 1 vote"
      },
      {
        "date": "2024-06-30T13:22:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT:\nTo maximize parallelism and performance in your Azure Data Factory pipelines, configure the dataset settings as follows:\n\nFor Pipeline P1 (copying from WS1 to Azure Data Lake Storage Gen2):\n\nCopy Method: Set to PolyBase. This method is optimized for large data loads from SQL pools to Azure Data Lake Storage.\nFor Pipeline P2 (copying from Azure Data Lake Storage Gen2 to WS2):\n\nCopy Method: Set to Bulk Insert. This is efficient for loading data into SQL pools from Azure Data Lake Storage, especially with text-delimited files.\nThese settings will help enhance performance by leveraging the most efficient data transfer methods for each scenario."
      },
      {
        "date": "2023-12-26T11:44:00.000Z",
        "voteCount": 1,
        "content": "For the Azure Data Factory pipelines P1 and P2, the dataset settings for the copy activity should be configured as follows:\n\nP1:\n\nb. Set the Copy method to PolyBase\nPolyBase is a technology that accesses data outside of the database via the T-SQL language. It\u2019s designed to leverage parallelism, which can lead to significant performance improvements when copying large amounts of data12.\n\nP2:\n\na. Set the Copy method to Bulk insert\nBulk insert is a process that can be used to import large amounts of data into a SQL Server table. It\u2019s a highly efficient way to push data into a table, especially when dealing with text-delimited files12.\n\nPlease note that the actual performance may vary depending on the specific requirements and the structure of your data12.\n\nLearn more\n\n\n1\n\nlearn.microsoft.com\n2\n\nlearn.microsoft.com\n3\n\nsocial.msdn.microsoft.com"
      },
      {
        "date": "2023-12-21T14:25:00.000Z",
        "voteCount": 2,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-12-14T10:32:00.000Z",
        "voteCount": 1,
        "content": "P1: Dynamic range according to \nhttps://techcommunity.microsoft.com/t5/fasttrack-for-azure/leverage-copy-data-parallelism-with-dynamic-partitions-in-adf/ba-p/3692133"
      },
      {
        "date": "2023-12-09T16:45:00.000Z",
        "voteCount": 3,
        "content": "Correct, chatgpt\nFor P1, where data is copied from a non-partitioned table in a SQL pool to Azure Data Lake Storage Gen2:\n- **Set the Copy method to PolyBase**: This is because PolyBase is designed to efficiently transfer large amounts of data to and from SQL-based data stores into Azure Data Lake Storage.\nFor P2, which copies data from text-delimited files in Azure Data Lake Storage Gen2 to a non-partitioned table in a SQL pool:\n- **Set the Copy method to Bulk insert**: Bulk insert is an efficient way to load data from files into SQL tables, especially when dealing with non-partitioned tables where PolyBase might not be applicable or the most optimal choice."
      },
      {
        "date": "2023-10-03T07:12:00.000Z",
        "voteCount": 1,
        "content": "P1 : set the partition option to dynamic range (see here : https://techcommunity.microsoft.com/t5/fasttrack-for-azure/leverage-copy-data-parallelism-with-dynamic-partitions-in-adf/ba-p/3692133)\n\nP2: Polybase give the best performance\nPolyBase loads data from UTF-8 and UTF-16 encoded delimited text files. PolyBase also loads from the Hadoop file formats RC File, ORC, and Parquet. PolyBase can also load data from Gzip and Snappy compressed files. PolyBase currently does not support extended ASCII, fixed-width format, and nested formats such as WinZip, JSON, and XML."
      },
      {
        "date": "2023-09-06T07:50:00.000Z",
        "voteCount": 1,
        "content": "P1) Set the partition option to dynamic range p2) set the copy method to PolyBase"
      },
      {
        "date": "2023-08-03T01:00:00.000Z",
        "voteCount": 4,
        "content": "There's a really interesting video regarding PolyBase/COPY INTO here:\n\nhttps://microsoft.github.io/PartnerResources/skilling/modern-analytics-academy/vignettes/polybase-vs-copy\n\nThis video indicates that PolyBase can actually be used to pull/push data from/to Azure Data Lake Storage to/from Azure Synapse Analytics Dedicated SQL Pool tables (via CTAS &amp; CETAS statements)."
      },
      {
        "date": "2023-07-11T10:05:00.000Z",
        "voteCount": 1,
        "content": "P1 dynamic range, link is pretty clear: https://learn.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#parallel-copy-from-azure-synapse-analytics"
      },
      {
        "date": "2023-04-25T06:38:00.000Z",
        "voteCount": 2,
        "content": "for P1, you should set the copy method to Polybase, and for P2, you should set the copy method to Bulk.\n\nThe reason is that Polybase is better suited for copying data between Azure Synapse Analytics and Azure Data Lake Storage Gen2, and can achieve better performance than Bulk copy in this scenario. On the other hand, Bulk copy is the fastest method for copying data from text-delimited files in Azure Data Lake Storage Gen2 to Azure Synapse Analytics.\n\nSetting the partition option to Dynamic range for both pipelines can help to maximize parallelism and performance by allowing the copy activity to split the data into multiple partitions based on the data range."
      },
      {
        "date": "2023-02-10T07:19:00.000Z",
        "voteCount": 12,
        "content": "I tried to create a copy activity in adf and these were results:\n  \nP1) Synapse to ADLS    --&gt; Source   Partition option: None/Dynamic range\n\t\t\t\t\t       Sink     Copy behavior: Add dynamic content/None/Flatten hierarchy/Merge files/Preserve hierarchy\n\t\t\t\t\t\t  \n\t\t\t\t\t\nP2) ADLS to Synapse    --&gt; Source Copy method: NA\n\t\t\t\t\t\t   Sink   Copy method: Copy command/PolyBase/Bulk insert/Upsert\t  \n\t\t\t\t\t\t   \n\nSo I think correct answers should be:\nP1) Set the partition option to dynamic range\np2) set the copy method to PolyBase"
      },
      {
        "date": "2023-01-18T04:31:00.000Z",
        "voteCount": 1,
        "content": "Both answer are polybase"
      },
      {
        "date": "2022-12-03T13:53:00.000Z",
        "voteCount": 2,
        "content": "I think you can put both as PolyBase. PolyBase is much faster and supports text delimited files as well now. \nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse?tabs=data-factory#use-polybase-to-load-data-into-azure-synapse-analytics"
      },
      {
        "date": "2022-08-03T08:56:00.000Z",
        "voteCount": 5,
        "content": "Agree with marcin1212 \nit should be\nP1: Set the partition option to \"Dynamic range \"\nP2: PolyBase"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67703-exam-dp-203-topic-2-question-43-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv.<br>You need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs.<br>How should you configure the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0022200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0022300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Incremental load -<br><br>Box 2: Tumbling window -<br>Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals. The following diagram illustrates a stream with a series of events and how they are mapped into 10-second tumbling windows.<br><img src=\"/assets/media/exam-media/04259/0022400001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-20T16:40:00.000Z",
        "voteCount": 32,
        "content": "1) Incremental Load\n2) Tumbling Window\n\nSeems like you could go with either Schedule trigger or Tumbling Window here. I would use the latter option, and pass the windowStart system variable to the pipeline as a parameter, allowing me to more easily navigate to the proper directory in the storage account."
      },
      {
        "date": "2024-01-29T23:43:00.000Z",
        "voteCount": 1,
        "content": "Yes, it is true. If the Data Factory contains more than 1 pipeline and I like to trigger it together, the schedule trigger is the only solution: \"Supports many-to-many relationships. Multiple triggers can kick off a single pipeline. A single trigger can kick off multiple pipelines.\"\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers"
      },
      {
        "date": "2024-02-20T10:36:00.000Z",
        "voteCount": 1,
        "content": "I believe it's only tumbling window, because if you were to choose fixed schedule, then you wouldn't know which files to load, the tumbling window allows you to know what happened in that window of time and know which files to load"
      },
      {
        "date": "2024-03-31T23:33:00.000Z",
        "voteCount": 1,
        "content": "interesting, do you have references to support such a behaviour? From MS documentation, I can't find that DataFactory tracks file creation inside a tumbling window. From docs, It seems like a matter of scheduling instead of what you're saying."
      },
      {
        "date": "2024-05-20T07:53:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell"
      },
      {
        "date": "2022-02-23T08:40:00.000Z",
        "voteCount": 17,
        "content": "Since, we are loading NEW data and not going back in time, it should be Schedule as we are scheduling it for every 1 hour in the future. It would've been Tumbling if we scheduled it for every 1 hour in the past."
      },
      {
        "date": "2023-10-30T12:45:00.000Z",
        "voteCount": 2,
        "content": "Besides, a scheduled trigger is a better option for this specific scenario than a tumbling window due to precision, efficiency and cost savings."
      },
      {
        "date": "2023-12-21T14:25:00.000Z",
        "voteCount": 4,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-12-09T16:51:00.000Z",
        "voteCount": 2,
        "content": "Correct, chatgpt :\nFor the scenario described, to load data from an Azure Storage account to an Azure Data Lake hourly and to minimize load times and costs, you would configure the Azure Data Factory solution as follows:\n\n- **Load methodology**: Incremental Load - Because you are loading new data every hour, and the goal is to minimize the load times and costs, you would incrementally load only the new data that has arrived since the last load.\n\n- **Trigger**: Tumbling window - This trigger is suitable for fixed-duration, repeating intervals in Azure Data Factory, which fits the requirement of loading data hourly. \n\nUsing a tumbling window trigger ensures that each window of time is processed once and only once, and by doing an incremental load, you are only processing the new data that has appeared since the last hour, rather than reprocessing all existing data."
      },
      {
        "date": "2023-10-11T23:46:00.000Z",
        "voteCount": 1,
        "content": "I think one thing very important here is that Tumbling window manages state between runs, that means that it will not be count twice."
      },
      {
        "date": "2023-06-22T05:16:00.000Z",
        "voteCount": 6,
        "content": "From Azure Data Factory Studio when you create a new trigger, you can choise TYPE in ('Schedule', 'Tumbling window', 'Storage events', 'Custom events'). \nWe should exclude \"Fixed Schedule\" becuase of 'fixed'! :) \nSo my final answer will be Incremental Load and Tumbling Window."
      },
      {
        "date": "2023-05-05T03:01:00.000Z",
        "voteCount": 6,
        "content": "Hi there"
      },
      {
        "date": "2023-04-25T07:24:00.000Z",
        "voteCount": 5,
        "content": "To minimize load times and costs for loading new data from the storage account to an Azure Data Lake once hourly, you should configure the solution to use incremental load and a trigger based on new files arriving.\n\nLoad methodology: With 200,000 new files generated daily, a full load every hour could be time-consuming and expensive. Incremental load is a better option in this scenario because it only loads new or changed data since the last successful execution of the pipeline, which can significantly reduce load times and costs.\nTrigger: A trigger based on new files arriving is the most efficient option because it only runs the pipeline when new files are detected in the storage account. This avoids unnecessary pipeline executions and reduces costs. A fixed schedule trigger runs the pipeline at fixed intervals, regardless of whether there is new data to process or not. A tumbling window trigger runs the pipeline at specified intervals, but still processes all data within the window, regardless of whether there is new data or not. Therefore, a new file trigger is the best option in this scenario."
      },
      {
        "date": "2024-02-20T10:35:00.000Z",
        "voteCount": 2,
        "content": "Wrong, the questions specifies that it has to run hourly, so it's tumbling window"
      },
      {
        "date": "2022-12-23T13:33:00.000Z",
        "voteCount": 6,
        "content": "A schedule for an activity creates a series of tumbling windows with in the pipeline start and end times \n\nI think is  \"Fixed schedule\" because \"Tumbling windows\" are more related to streams analytics questions according to MS doc.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/v1/data-factory-scheduling-and-execution"
      },
      {
        "date": "2022-08-03T09:00:00.000Z",
        "voteCount": 4,
        "content": "1) Incremental Load\n2) Tumbling Window"
      },
      {
        "date": "2022-05-21T01:11:00.000Z",
        "voteCount": 14,
        "content": "With Scheduled trigger executions can overlaps if the process does not finish within 1 hour, Tumbling window is better, with concurrency setting it can allow only one ongoing execution."
      },
      {
        "date": "2022-03-22T07:20:00.000Z",
        "voteCount": 1,
        "content": "both Tumbling Window and Schedule trigger will reach the goal. Which one is more cost effective?"
      },
      {
        "date": "2022-05-12T00:48:00.000Z",
        "voteCount": 2,
        "content": "I think because every hour you're only processing the past hour's data. With a tumbling window you can define which messages to process, whereas with a schedule trigger you'd have to implement that filter separately."
      },
      {
        "date": "2022-02-23T01:38:00.000Z",
        "voteCount": 1,
        "content": "why not schedule trigger?"
      },
      {
        "date": "2022-03-31T22:01:00.000Z",
        "voteCount": 1,
        "content": "for backfill purpose? just guessing."
      },
      {
        "date": "2021-12-17T11:36:00.000Z",
        "voteCount": 6,
        "content": "incremental, fixed schedule every hour."
      },
      {
        "date": "2021-12-17T11:43:00.000Z",
        "voteCount": 1,
        "content": "correct answer..tumbling window"
      },
      {
        "date": "2021-12-12T00:12:00.000Z",
        "voteCount": 2,
        "content": "As a input we are receiving csv files so why not trigger mechanism to the pipeline when file arrived."
      },
      {
        "date": "2021-12-13T10:45:00.000Z",
        "voteCount": 35,
        "content": "The question says, \"load new data from the storage account to the Azure Data Lake once hourly.\" This already indicates a tumbling window to run every hour.\n\nOn top of that, if you executed this as an event every time a file arrived, you'd have 200,000 ADF pipeline executions per day - one per file. If you ran the pipeline once per hour per day, you'd have just 24.\n\n1,000 ADF runs is $1. In this situation, 1 day is 24 runs when executed on a tumbling window. That's 2.4 cents. If we ran 200,000 pipelines, that'd be $200/day. This excludes other costs.\n\nhttps://azure.microsoft.com/en-us/pricing/details/data-factory/data-pipeline/"
      },
      {
        "date": "2022-01-18T07:30:00.000Z",
        "voteCount": 2,
        "content": "That's correct. Well explained"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67483-exam-dp-203-topic-2-question-44-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL.<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a Standard cluster for the data engineers, and a High Concurrency cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 46,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-05T03:36:00.000Z",
        "voteCount": 40,
        "content": "B is correct but the explanation is wrong.\n\u2711 A workload for data engineers who will use Python and SQL. --&gt;  high concurrency \n\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL. --&gt; standard\n\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R. --&gt; standard because high concurrency does not support Scala\n\nhttps://stackoverflow.com/questions/65869399/high-concurrency-clusters-in-databricks"
      },
      {
        "date": "2022-03-15T11:18:00.000Z",
        "voteCount": 2,
        "content": "or rather Scala does not support concurrent instances (but yes, it implies HC cluster will not support Scala)"
      },
      {
        "date": "2024-01-29T23:57:00.000Z",
        "voteCount": 1,
        "content": "You do not need to use High concurrency, because \"Scala code will be executed inside the Spark JVM (per machine) that is shared between all users\": \nhttps://learn.microsoft.com/en-us/answers/questions/924587/azure-databricks-scala-on-high-concurrency-cluster"
      },
      {
        "date": "2024-02-20T12:45:00.000Z",
        "voteCount": 2,
        "content": "B is correct, but there's an update, high concurrency now supports scala https://learn.microsoft.com/en-us/azure/databricks/compute/configure#--high-concurrency-clusters\n\n- **Data scientist**: Cada Data scientist tendr\u00e1 su propio cluster, lo que significa que con un **standard** con la configuraci\u00f3n de autoterminate y ya est\u00e1n bien.\n- **Data engineer**: los Data engineer deberian usar un **High Concurrency** cluster, porque son muchos usuarios.\n- **Jobs**: esta bien que los jobs usen **High concurrency** porque muchos jobs podrian estar ejecutandose en el cluster y ahora high concurrency si soporta scala"
      },
      {
        "date": "2023-12-14T13:21:00.000Z",
        "voteCount": 2,
        "content": "Standard is enough for all workloads. High concurrency (due to Scala )possible only for data engineers"
      },
      {
        "date": "2023-12-09T16:55:00.000Z",
        "voteCount": 1,
        "content": "Correct, chatgpt :\nFor the given scenario, where data engineers must share a cluster, data scientists need their own clusters with auto-termination, and a managed job cluster is required for running notebooks, the solution provided may not fully meet the goal. Here's why:\n\n- Data engineers should share a cluster, so creating a single Standard cluster for all data engineers would meet this requirement.\n- For data scientists, the solution suggests a Standard cluster for each, but it should specify that these clusters have auto-termination settings configured to minimize costs.\n- The High Concurrency cluster is suitable for running jobs because it allows multiple users to share the cluster and run jobs concurrently. However, it should be managed as per the enterprise team's standards.\n\nThe provided solution does not fully adhere to these standards, especially regarding the auto-termination requirement for data scientists' clusters. Thus, the answer would be:\n\nB. No, the solution does not meet the goal."
      },
      {
        "date": "2023-09-06T07:53:00.000Z",
        "voteCount": 2,
        "content": "high concurrency does not support Scala"
      },
      {
        "date": "2023-08-08T02:30:00.000Z",
        "voteCount": 1,
        "content": "Correct option is B-NO"
      },
      {
        "date": "2023-04-25T07:52:00.000Z",
        "voteCount": 1,
        "content": "A)Yes\nThe use of a shared Standard cluster for data engineers, a High Concurrency cluster for jobs, and individual Standard clusters for each data scientist that auto-terminates after 120 minutes of inactivity aligns with the specified standards and is a valid approach for creating a tiered Databricks workspace."
      },
      {
        "date": "2022-12-09T20:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/clusters/configure.html"
      },
      {
        "date": "2022-11-21T13:25:00.000Z",
        "voteCount": 2,
        "content": "high concurrency cluster is already a legacy cluster mode. question is not relevant anymore"
      },
      {
        "date": "2022-10-13T17:50:00.000Z",
        "voteCount": 2,
        "content": "Standard mode can be shared by multiple users and terminate automatically, on the other hand High do not terminate automatically and Scala workload is not supported."
      },
      {
        "date": "2022-09-21T06:55:00.000Z",
        "voteCount": 1,
        "content": "NO IS CORRECT ANSWER"
      },
      {
        "date": "2022-08-04T04:25:00.000Z",
        "voteCount": 1,
        "content": "correct , answer B, agree with lukeonline"
      },
      {
        "date": "2022-06-20T06:13:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2022-03-10T13:00:00.000Z",
        "voteCount": 3,
        "content": "As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --&gt; Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --&gt; Standard for Data Scientists &amp; High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --&gt; Jobs needs Standard"
      },
      {
        "date": "2021-12-22T05:32:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-12-09T18:01:00.000Z",
        "voteCount": 3,
        "content": "Respuesta correcta; Standar para Cientificos y jobs. Alta concurrencia para ingenieros de datos."
      },
      {
        "date": "2021-12-25T02:34:00.000Z",
        "voteCount": 3,
        "content": "Agree! - Correct answer; Standard for Scientists and jobs. High concurrency for data engineers."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67965-exam-dp-203-topic-2-question-45-discussion/",
    "body": "You have the following Azure Data Factory pipelines:<br>\u2711 Ingest Data from System1<br>\u2711 Ingest Data from System2<br>\u2711 Populate Dimensions<br>\u2711 Populate Facts<br>Ingest Data from System1 and Ingest Data from System2 have no dependencies. Populate Dimensions must execute after Ingest Data from System1 and Ingest<br>Data from System2. Populate Facts must execute after Populate Dimensions pipeline. All the pipelines must execute every eight hours.<br>What should you do to schedule the pipelines for execution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an event trigger to all four pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a schedule trigger to all four pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a patient pipeline that contains the four pipelines and use a schedule trigger.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a patient pipeline that contains the four pipelines and use an event trigger."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 71,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-20T19:29:00.000Z",
        "voteCount": 62,
        "content": "C is correct, but with poor wording. Should be 'parent pipeline' with a schedule trigger.\n\nThe parent pipeline has 4 execute pipeline activities. Ingest 1 and Ingest 2 have no dependencies. Dimension pipeline has two dependencies from 'on completion' outputs of both Ingest 1 and Ingest 2 pipelines. Fact pipeline has one 'on completion' dependency on the Dimension pipeline. Absolutely nothing to do with a tumbling window trigger"
      },
      {
        "date": "2022-07-09T08:59:00.000Z",
        "voteCount": 2,
        "content": "Big thanks onyerleft :)"
      },
      {
        "date": "2022-06-20T05:53:00.000Z",
        "voteCount": 3,
        "content": "Thank you. I was wondering about \"patient\" and related it on my poor english!"
      },
      {
        "date": "2022-01-05T03:40:00.000Z",
        "voteCount": 20,
        "content": "Lol, I searched in the internet for the \"patient pipeline\".... should have read the comments first :)"
      },
      {
        "date": "2023-10-05T06:42:00.000Z",
        "voteCount": 1,
        "content": "Also looked up \"patient pipeline\" and was confused xD"
      },
      {
        "date": "2024-07-07T01:54:00.000Z",
        "voteCount": 2,
        "content": "patient=&gt;parent"
      },
      {
        "date": "2023-10-31T07:57:00.000Z",
        "voteCount": 2,
        "content": "Was on my exam today (31.10.2023)."
      },
      {
        "date": "2023-09-07T05:55:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2023-06-18T20:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-17T23:55:00.000Z",
        "voteCount": 4,
        "content": "Its not patient pipeline, it should be parent pipeline. Since there are 3 types of triggers in ADF:\n1) Schedule Trigger - trigger a pipeline at a fixed hour/minute of the day.\n2) Tumbling Window Trigger - trigger a pipeline which usually works for real time data\n3) Event-based Trigger - trigger a pipeline incase of an event i.e. new file coming to blob/adls etc.\n\nSince the 4 pipelines must be triggered every 8 hrs, then it should be schedule trigger."
      },
      {
        "date": "2022-08-04T04:28:00.000Z",
        "voteCount": 1,
        "content": "right C"
      },
      {
        "date": "2022-01-03T10:41:00.000Z",
        "voteCount": 2,
        "content": "what the hk is a patient pipeline?"
      },
      {
        "date": "2022-01-28T05:11:00.000Z",
        "voteCount": 1,
        "content": "lol I think they mean \"parent\""
      },
      {
        "date": "2021-12-18T05:26:00.000Z",
        "voteCount": 3,
        "content": "It should be tumbling window since 2 dependent pipelines on run state. from given option only schedule event fits but its not correct."
      },
      {
        "date": "2022-11-23T20:39:00.000Z",
        "voteCount": 1,
        "content": "If those pipelines finish quickly, schedule trigger should be fine. If there is possibility that those pipelines may run for close to or more than 8 hours, definitely tumbling window should be used instead"
      },
      {
        "date": "2021-12-15T04:23:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't the answer be A/D?"
      },
      {
        "date": "2022-09-10T00:38:00.000Z",
        "voteCount": 1,
        "content": "So the question tries to trick you, they don't want to ask about individual pipeline configurations where you need to account for dependencies, they literally want to know how you will schedule the pipelines for execution. The additional information is there to confuse you and make you overthink, focus on the question. In this case, it is C."
      },
      {
        "date": "2021-12-14T10:51:00.000Z",
        "voteCount": 2,
        "content": "I think it should be Tumbling window"
      },
      {
        "date": "2021-12-17T16:52:00.000Z",
        "voteCount": 2,
        "content": "The question or answers do not mention Tumbling Window. What is the basis for the response? Any more context?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67485-exam-dp-203-topic-2-question-46-discussion/",
    "body": "DRAG DROP -<br>You are responsible for providing access to an Azure Data Lake Storage Gen2 account.<br>Your user account has contributor access to the storage account, and you have the application ID and access key.<br>You plan to use PolyBase to load data into an enterprise data warehouse in Azure Synapse Analytics.<br>You need to configure PolyBase to connect the data warehouse to storage account.<br>Which three components should you create in sequence? To answer, move the appropriate components from the list of components to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0022700001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0022700002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: an asymmetric key -<br>A master key should be created only once in a database.  The Database Master Key is a symmetric key used to protect the private keys of certificates and asymmetric keys in the database.<br>Step 2: a database scoped credential<br>Create a Database Scoped Credential. A Database Scoped Credential is a record that contains the authentication information required to connect an external resource. The master key needs to be created first before creating the database scoped credential.<br><br>Step 3: an external data source -<br>Create an External Data Source. External data sources are used to establish connectivity for data loading using Polybase.<br>Reference:<br>https://www.sqlservercentral.com/articles/access-external-data-from-azure-synapse-analytics-using-polybase",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-09T18:10:00.000Z",
        "voteCount": 207,
        "content": "1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format"
      },
      {
        "date": "2023-01-09T06:50:00.000Z",
        "voteCount": 2,
        "content": "agreed.  \nhttps://www.sqlshack.com/sql-server-polybase-external-tables-with-azure-blob-storage/"
      },
      {
        "date": "2024-07-02T21:01:00.000Z",
        "voteCount": 1,
        "content": "Thank you for the link, explains it very well."
      },
      {
        "date": "2022-07-25T23:46:00.000Z",
        "voteCount": 19,
        "content": "you need to connect to the DW, not to a specific file. Therefore :\n1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source"
      },
      {
        "date": "2024-04-24T23:35:00.000Z",
        "voteCount": 1,
        "content": "don't need encription ke"
      },
      {
        "date": "2023-03-28T03:35:00.000Z",
        "voteCount": 1,
        "content": "File format is not related to a specific file"
      },
      {
        "date": "2023-06-22T05:26:00.000Z",
        "voteCount": 5,
        "content": "It should be: \"Create a Master Key\" not 'DataBase&nbsp;Encryption Key' if you are trying to findout a predecessor step before \"create a Database Scoped Credential\"."
      },
      {
        "date": "2023-03-28T03:12:00.000Z",
        "voteCount": 2,
        "content": "Agree:\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-hadoop"
      },
      {
        "date": "2022-01-07T23:07:00.000Z",
        "voteCount": 56,
        "content": "According to the documentation, the first thing you are to create is \nCREATE MASTER KEY ENCRYPTION BY PASSWORD = 'S0me!nfo'; \nI don't think this is means an asymmetric key. It is simply a database encryption key. So I think the answer is \n\n1- Create a Database Encryption Key\n2 - Create a Database Scoped Credential\n3 - Create an External Data Source"
      },
      {
        "date": "2022-03-17T07:32:00.000Z",
        "voteCount": 4,
        "content": "Does the text not say you already have an access key? Should the correct answer not be \n1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format\n\nas alex mentions?"
      },
      {
        "date": "2022-05-27T20:42:00.000Z",
        "voteCount": 4,
        "content": "access key is for storage account so you still need a master/asymmetric key for the database."
      },
      {
        "date": "2022-05-28T17:59:00.000Z",
        "voteCount": 3,
        "content": "*sorry, not asymmetric"
      },
      {
        "date": "2022-03-16T09:57:00.000Z",
        "voteCount": 2,
        "content": "Btw yes even in the description it says that the master key is a symmetric key, not an asymmetric one. It"
      },
      {
        "date": "2022-03-15T22:23:00.000Z",
        "voteCount": 3,
        "content": "also, the question only mentions storage account in general not a file or folder, so I believe we don't need to go as far as creating file format anyway"
      },
      {
        "date": "2024-09-22T04:42:00.000Z",
        "voteCount": 1,
        "content": "Answer:\n1. DB scoped credential\n2. External data source\n3. External file format\n\nDocs:\n\nMASTER [symmetric] KEY w/ PASSWORD (seams optional: see link 2; is symmetric: see link 3)\n &gt; DATABASE SCOPED CREDENTIAL w/ IDENTITY and SECRET key\n  &gt; EXTERNAL DATA SOURCE w/ TYPE, LOCATION, CREDENTIAL\n   &gt; EXTERNAL FILE w/ TYPE and OPTIONS &gt; TERMINATOR\n    &gt; EXTERNAL TABLE w/ DATA_SOURCE and LOCATION\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-sql-server?view=sql-server-ver16#configure-a-sql-server-external-data-source\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16#remarks\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/open-master-key-transact-sql?view=sql-server-ver16#remarks"
      },
      {
        "date": "2024-07-14T20:47:00.000Z",
        "voteCount": 2,
        "content": "A database scoped credential: This is needed to authenticate and access the storage account.\nAn external data source: This defines the location of the data source using the database scoped credential.\nAn external file format: This specifies the format of the data files (e.g., CSV, Parquet) in the data lake."
      },
      {
        "date": "2024-07-07T04:14:00.000Z",
        "voteCount": 2,
        "content": "the correct order for the components is:\n\nA database scoped credential\nAn external data source\nAn external file format"
      },
      {
        "date": "2024-02-17T06:46:00.000Z",
        "voteCount": 3,
        "content": "Database Scoped Credential: This comes first as it holds the secure credentials needed for authentication.\nExternal Data Source: This relies on the credential to define the connection to the storage account.\nExternal File Format (Optional): This step further enhances performance and efficiency by informing PolyBase of the data format."
      },
      {
        "date": "2023-12-29T05:31:00.000Z",
        "voteCount": 1,
        "content": "Given answer appears correct. This section specifies we need the master key (which is an asymmetric key), then create the scoped credential. The scoped credential can finally be used to create an external data source.\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-database-scoped-credential-transact-sql?view=sql-server-ver16#c-creating-a-database-scoped-credential-for-polybase-connectivity-to-azure-data-lake-store"
      },
      {
        "date": "2023-12-26T12:12:00.000Z",
        "voteCount": 1,
        "content": "check this out, the answers provided are correct  https://learn.microsoft.com/en-us/sql/analytics-platform-system/polybase-configure-azure-blob-storage?view=aps-pdw-2016-au7"
      },
      {
        "date": "2024-03-01T12:45:00.000Z",
        "voteCount": 1,
        "content": "This links also mentions a 4th step: create external file format, that doesn't fit in the number of asked answers."
      },
      {
        "date": "2023-12-09T16:58:00.000Z",
        "voteCount": 5,
        "content": "Chatgpt :\nTo configure PolyBase to connect an Azure Synapse Analytics data warehouse to an Azure Data Lake Storage Gen2 account, you need to create:\n\n1. **A database scoped credential**: This stores the necessary authentication to access the data lake, such as the storage account access key.\n\n2. **An external data source**: This defines the location of the data in the storage account and uses the scoped credential for authentication.\n\n3. **An external file format**: This specifies the format of the data files (e.g., CSV, Parquet) in the data lake so that PolyBase knows how to parse the data.\n\nThese components should be created in the sequence listed above to ensure that PolyBase has the information it needs to authenticate, locate, and read the data from the storage account."
      },
      {
        "date": "2023-11-18T04:58:00.000Z",
        "voteCount": 2,
        "content": "Seems like answers are differing.  Do we get partial points if answer is partially correct ?"
      },
      {
        "date": "2023-10-08T10:27:00.000Z",
        "voteCount": 2,
        "content": "The database master key is a SYMMETRIC KEY (https://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16#remarks) therefore the answer should be:\n\n1- Create a Database Encryption Key (CREATE MASTER KEY ENCRYPTION BY PASSWORD)\n2 - Create a Database Scoped Credential (CREATE DATABASE SCOPED CREDENTIAL)\n3 - Create an External Data Source (CREATE EXTERNAL DATA SOURCE)\n(https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#1-create-database-scoped-credential)"
      },
      {
        "date": "2023-10-05T01:36:00.000Z",
        "voteCount": 1,
        "content": "CREATE MASTER KEY;\n\n-- SECRET: Provide your Azure storage account key.\nCREATE DATABASE SCOPED CREDENTIAL AzureStorageCredential\nWITH\n    IDENTITY = 'user',\n    SECRET = '&lt;azure_storage_account_key&gt;'\n;\nCREATE EXTERNAL DATA SOURCE AzureStorage\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'wasbs://&lt;blob_container_name&gt;@&lt;azure_storage_account_name&gt;.blob.core.windows.net',\n    CREDENTIAL = AzureStorageCredential\n);\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-load-from-azure-blob-storage-with-polybase"
      },
      {
        "date": "2023-09-07T05:55:00.000Z",
        "voteCount": 2,
        "content": "1.- A database scoped credential 2.- an External data sorce 3.- a external file format"
      },
      {
        "date": "2023-08-11T23:31:00.000Z",
        "voteCount": 2,
        "content": "1.A database scoped credential\n2.an External data source\n3 a external file format\n\nCreate master key is an symmetric key https://learn.microsoft.com/en-us/sql/t-sql/statements/create-master-key-transact-sql?view=sql-server-ver16\n\nDEK comes under the concept of azure SQL TDE and no way related to this question\n\nHence proved"
      },
      {
        "date": "2023-07-09T01:21:00.000Z",
        "voteCount": 2,
        "content": "Create external tables for Azure Data Lake Store\nFrom &lt;https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-data-lake-store&gt; \n\n1. Create database scoped credential\n2. Create external data source to reference Azure Data Lake Store (ADLS)\n3. Create external file format"
      },
      {
        "date": "2023-05-29T18:31:00.000Z",
        "voteCount": 1,
        "content": "1.- A database scoped credential\n2.- an External data sorce\n3.- a external file format"
      },
      {
        "date": "2023-04-25T08:37:00.000Z",
        "voteCount": 1,
        "content": "Create an external data source (C) that specifies the location of the data in the storage account.\nCreate an external file format (E) that describes the format of the data in the external data source.\nCreate a database scoped credential (A) that contains the credentials needed to access the storage account.\nNote that asymmetric keys and database encryption keys are not required for configuring PolyBase with Azure Data Lake Storage Gen2."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68629-exam-dp-203-topic-2-question-47-discussion/",
    "body": "You are monitoring an Azure Stream Analytics job by using metrics in Azure.<br>You discover that during the last 12 hours, the average watermark delay is consistently greater than the configured late arrival tolerance.<br>What is a possible cause of this behavior?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvents whose application timestamp is earlier than their arrival time by more than five minutes arrive as inputs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are errors in the input data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe late arrival policy causes events to be dropped.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe job lacks the resources to process the volume of incoming data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-26T23:51:00.000Z",
        "voteCount": 14,
        "content": "The link provided is the source of truth"
      },
      {
        "date": "2022-01-06T00:37:00.000Z",
        "voteCount": 7,
        "content": "D is correct"
      },
      {
        "date": "2023-12-21T14:25:00.000Z",
        "voteCount": 1,
        "content": "Got this question today on the exam"
      },
      {
        "date": "2023-09-07T05:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-08T02:32:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2022-08-04T07:50:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-03-17T10:51:00.000Z",
        "voteCount": 3,
        "content": "D is correct."
      },
      {
        "date": "2022-01-03T16:20:00.000Z",
        "voteCount": 3,
        "content": "Correct. D is the one that makes most sense."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67487-exam-dp-203-topic-2-question-48-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics job to retrieve game data.<br>You need to ensure that the job returns the highest scoring record for each five-minute time interval of each game.<br>How should you complete the Stream Analytics query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0022900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0023000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: TopOne OVER(PARTITION BY Game ORDER BY Score Desc)<br>TopOne returns the top-rank record, where rank defines the ranking position of the event in the window according to the specified ordering. Ordering/ranking is based on event columns and can be specified in ORDER BY clause.<br>Box 2: Hopping(minute,5)<br>Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap and be emitted more often than the window size. Events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.<br><img src=\"/assets/media/exam-media/04259/0023100001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/topone-azure-stream-analytics https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-09T18:19:00.000Z",
        "voteCount": 135,
        "content": "TopOne() / Tumbling"
      },
      {
        "date": "2023-01-26T09:11:00.000Z",
        "voteCount": 11,
        "content": "The Select built with the TopOne() option would return one row for each game. Still, it would not tell you the game (SELECT TOP_ONE()... FROM). On the other hand, the GAME,MAX() option clearly informs the Game."
      },
      {
        "date": "2023-01-27T02:41:00.000Z",
        "voteCount": 3,
        "content": "I think its TopOne() as \"TopOne() OVER(partition by Game order by Score Desc)\", it orders by descending of Score and by partition, and top one of each of them."
      },
      {
        "date": "2023-04-08T08:40:00.000Z",
        "voteCount": 3,
        "content": "The question was \"the highest scoring record of each game\", so that's what we need - one row for each game"
      },
      {
        "date": "2024-01-30T02:15:00.000Z",
        "voteCount": 2,
        "content": "In case of \"game, max()\" the \"group by\" cause should contain the \"game\", otherwise it returns an error"
      },
      {
        "date": "2024-01-30T02:17:00.000Z",
        "voteCount": 2,
        "content": "but in this case no definition for 5 minutes, so it is a bad idea"
      },
      {
        "date": "2021-12-10T01:25:00.000Z",
        "voteCount": 44,
        "content": "Syntax for Hopping window requires 3 arguments, seems this should be Tumbling Window which fulfils the exact same requirements."
      },
      {
        "date": "2022-01-28T21:57:00.000Z",
        "voteCount": 2,
        "content": "Yeah sure"
      },
      {
        "date": "2024-04-24T23:51:00.000Z",
        "voteCount": 2,
        "content": "topone().. is correct\nbut it is a tumbling window; hopping window has two number, window length and hop length"
      },
      {
        "date": "2024-04-16T00:40:00.000Z",
        "voteCount": 1,
        "content": "TopOne\nTumbling"
      },
      {
        "date": "2024-03-30T07:59:00.000Z",
        "voteCount": 1,
        "content": "Top One and Tumbling"
      },
      {
        "date": "2024-02-10T11:29:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/stream-analytics-query/topone-azure-stream-analytics\n\nTopone()\nTumbling"
      },
      {
        "date": "2024-02-16T02:37:00.000Z",
        "voteCount": 2,
        "content": "I believe Max score is simpler.\nNot making any sense to re-arrange it and then pick the top."
      },
      {
        "date": "2024-02-08T07:56:00.000Z",
        "voteCount": 2,
        "content": "got this question on my exam"
      },
      {
        "date": "2023-12-27T03:03:00.000Z",
        "voteCount": 3,
        "content": "SELECT\n    TopOne() OVER(PARTITION BY Game ORDER BY Score Desc) as HighestScore\nFROM input TIMESTAMP BY CreatedAt\nGROUP BY\n    Game, TumblingWindow(minute,5)\nHere\u2019s what this query does:\n\nTopOne() OVER(PARTITION BY Game ORDER BY Score Desc) selects the highest scoring record in each partition of games.\nTIMESTAMP BY CreatedAt specifies the timestamp of each event.\nGROUP BY Game, TumblingWindow(minute,5) groups the output by game and in five-minute intervals. The TumblingWindow function creates a series of fixed-sized, non-overlapping and contiguous time intervals.\nPlease note that this is a general guidance and the actual query might need to be adjusted based on the specific requirements and data schema of your game data."
      },
      {
        "date": "2023-12-09T17:01:00.000Z",
        "voteCount": 2,
        "content": "Wrong, chatgpt :\nFor the Azure Stream Analytics job that needs to return the highest scoring record for each five-minute interval of each game, the query should use the following options:\n\n1. **SELECT**: `TopOne() OVER(PARTITION BY Game ORDER BY Score Desc)` as HighestScore\n   - This function returns the highest score for each partitioned group (each game in this case), ordered by score in descending order, ensuring that the highest score is selected.\n\n2. **GROUP BY**: `TumblingWindow(minute, 5)`\n   - This window function groups the events into non-overlapping, continuous five-minute intervals, which is what's required to get the highest score in each five-minute time slice.\n\nThis configuration will ensure that you get the highest score for each game every five minutes."
      },
      {
        "date": "2023-12-01T18:18:00.000Z",
        "voteCount": 4,
        "content": "There really isn't a solution based on the options given.\nFirst off, Hopping() and Tumbling() don't exist, it's HoppingWindow and TumblingWindow\nSo, the group by only has \nGame\nWindows(TublingWindow(minute, 5, Hopping(minute, 5))\nBut, that last one isn't valid either in multiple ways.\nSo, only Group by Game is valid.\nSo, you can try it this way...\nSELECT TOPOne() OVER (PARTITION BY Game ORDER BY Score DESC)\n FROM input TIMESTAMP BY CreatedDt\nGROUP BY game\n\nBut, that does nothing with 5 minute window.\n\nSo, there is NO solution based on the data given.\n\nI would write it this way\nSELECT game, MAX(Score) as MaxScore\n FROM intput TIMESTAMP by CreatedAt\nGroup by Game, TumblingWindow(minute, 5)\n\nBut, those aren't options provided."
      },
      {
        "date": "2024-03-01T06:40:00.000Z",
        "voteCount": 2,
        "content": "it is accepted both syntax:\n\n{TUMBLINGWINDOW | TUMBLING} ( timeunit  , windowsize, [offsetsize] )  {HOPPINGWINDOW | HOPPING} ( timeunit  , windowsize , hopsize, [offsetsize] )\nhttps://learn.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics"
      },
      {
        "date": "2023-09-07T05:58:00.000Z",
        "voteCount": 6,
        "content": "Game(max) and tumbling window is the correct answer"
      },
      {
        "date": "2023-07-06T06:40:00.000Z",
        "voteCount": 7,
        "content": "\"Hopping(minute,5)\" - the syntax itself is wrong. Not sure who is preparing these answers???"
      },
      {
        "date": "2023-01-08T11:24:00.000Z",
        "voteCount": 9,
        "content": "minute time interval of each game, you can use the TumblingWindow function to define a five-minute tumbling window over the data, and then use the MAX function to select the highest scoring record within each window."
      },
      {
        "date": "2023-02-13T07:33:00.000Z",
        "voteCount": 10,
        "content": "Totally agree:\n- Game, max() - you need to have [Game] column to know, which game the max score refers to,\n- tumbling window - requires 2 arguments, hopping window could be used, but requires 3 arguments"
      },
      {
        "date": "2023-06-22T04:17:00.000Z",
        "voteCount": 4,
        "content": "Then you have to put \"game\" attribute in the group by as well, but there is only 1 option and it's without the window! So ripone / tumbling!"
      },
      {
        "date": "2023-06-22T04:18:00.000Z",
        "voteCount": 3,
        "content": "Topone"
      },
      {
        "date": "2022-12-22T20:50:00.000Z",
        "voteCount": 7,
        "content": "Game(max) and tumbling window is the correct answer"
      },
      {
        "date": "2023-01-12T01:19:00.000Z",
        "voteCount": 3,
        "content": "Correct. Here top score is being asked, instead of Rank"
      },
      {
        "date": "2022-12-12T08:06:00.000Z",
        "voteCount": 1,
        "content": "Tumbling window seems to be correct, in question there is no fixed time interval specified."
      },
      {
        "date": "2022-11-17T07:04:00.000Z",
        "voteCount": 2,
        "content": "TopOne() / Tumbling is correct answer"
      },
      {
        "date": "2022-09-18T05:22:00.000Z",
        "voteCount": 3,
        "content": "This is clearly a fu**-up, it's a tumbling Window. For sure! I wonder what would happen in the exam if you select Tumbling..."
      },
      {
        "date": "2022-10-08T02:54:00.000Z",
        "voteCount": 3,
        "content": "true, if tumbling is not the correct answer in the exam then we fu**-up really fu**-up hahaha"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68237-exam-dp-203-topic-2-question-49-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "R",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-21T03:57:00.000Z",
        "voteCount": 55,
        "content": "Correct answer: No - you cannot execute the R script using a stored procedure activity"
      },
      {
        "date": "2023-06-22T05:38:00.000Z",
        "voteCount": 1,
        "content": "I agree, I've found an articol where it's saying that you can run R script from a Custom .net activity, or better if you have BYOC HDInsight cluster that already has R Installed on it."
      },
      {
        "date": "2022-01-21T06:03:00.000Z",
        "voteCount": 15,
        "content": "I select A because you can use R script in sp_execute_external_script"
      },
      {
        "date": "2022-03-27T22:34:00.000Z",
        "voteCount": 6,
        "content": "i admire your thought, but context looks wanna discriminate the inavailability of R in SP not like that in Databricks."
      },
      {
        "date": "2023-04-26T00:56:00.000Z",
        "voteCount": 2,
        "content": "The answer is NO for other reasons than the SP.\nConcerning the SP: To execute an R script within a stored procedure in Synapse Analytics, you can use the sp_execute_external_script system stored procedure. This procedure can be used to execute R scripts, as well as scripts written in other languages such as Python."
      },
      {
        "date": "2024-03-31T22:52:00.000Z",
        "voteCount": 1,
        "content": "\"and then uses a stored procedure to execute the R script\"\nyou cannot use stored procedures to execute R scripts: it's just absurd. Stored procedures are SQL statements."
      },
      {
        "date": "2024-01-04T14:35:00.000Z",
        "voteCount": 5,
        "content": "**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**"
      },
      {
        "date": "2023-10-05T00:42:00.000Z",
        "voteCount": 2,
        "content": "While the most voted is 'No', the green bar indicates 'Yes'. My first inclination was 'No' as well, because it's not the easiest or most logical way to do it. Then again, there's many roads to Rome. Does it meet the goal, yes."
      },
      {
        "date": "2023-09-07T06:06:00.000Z",
        "voteCount": 1,
        "content": "B should be correct"
      },
      {
        "date": "2023-04-11T23:55:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer B: \nThe solution proposed does not meet the goal because it suggests executing the R script using a stored procedure in the data warehouse. Azure Synapse Analytics does not support executing R scripts directly within stored procedures. Instead, you should use Azure Data Factory to orchestrate the process, using an Azure Machine Learning activity to execute the R script for data transformation before loading the transformed data into Azure Synapse Analytics."
      },
      {
        "date": "2023-03-06T08:18:00.000Z",
        "voteCount": 3,
        "content": "Btw.. Is it worth to pay for accessing the rest of pages? Since the actual value is community discussion. And beyond this point, it's supposed to be less people."
      },
      {
        "date": "2023-03-25T04:57:00.000Z",
        "voteCount": 2,
        "content": "I have paid for further access and would say it is worth it. The community discussion continues"
      },
      {
        "date": "2023-02-13T21:11:00.000Z",
        "voteCount": 3,
        "content": "Cant we fix answers correctly in the portal, instead of relying on votes"
      },
      {
        "date": "2023-02-02T10:59:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-18T10:35:00.000Z",
        "voteCount": 2,
        "content": "Next page is asking for contributor access, anyone have credentials or how we can skip the payment"
      },
      {
        "date": "2024-04-24T23:59:00.000Z",
        "voteCount": 1,
        "content": "you can't skip the payment. It is not too much money for its worth"
      },
      {
        "date": "2023-01-29T04:40:00.000Z",
        "voteCount": 3,
        "content": "I think this is not possible we have to pay :("
      },
      {
        "date": "2023-01-12T01:09:00.000Z",
        "voteCount": 3,
        "content": "there is a staging zone in Azure Data Lake Storage. The very fact that A suggest copying into DWH staging zone makes it invalid so any other discussion is unnecessary. It is B"
      },
      {
        "date": "2023-01-08T11:25:00.000Z",
        "voteCount": 1,
        "content": "his solution does not meet the goal of the daily process you have described. While using an Azure Data Factory schedule trigger to execute a pipeline is a good approach for scheduling the process to run on a daily basis, the pipeline you have described does not include any steps to transform the data using an R script.\n\nTo meet the goal of the daily process, you will need to include a step in the pipeline to execute the R script that transforms the data. One way to do this would be to use an Azure Data Factory activity, such as an Execute R Script activity, to run the R script on the data as it is being copied from the staging zone to the staging table in the data warehouse. You can then use a stored procedure or another Data Factory activity, such as an SQL activity, to insert the transformed data into the final destination table in the data warehouse."
      },
      {
        "date": "2022-08-12T22:19:00.000Z",
        "voteCount": 2,
        "content": "Synapse doesn't support R at the moment\nhttps://docs.microsoft.com/en-us/answers/questions/222624/is-azure-synapse-analytics-supporting-r-language.html"
      },
      {
        "date": "2022-08-04T23:54:00.000Z",
        "voteCount": 3,
        "content": "should be B"
      },
      {
        "date": "2022-06-15T04:19:00.000Z",
        "voteCount": 5,
        "content": "As per problem, Azure Data Lake Storage account that contains a staging zone. From staging zone, transform the data and insert into Azure Synapse Analytics.\nBut the solution providing as copy data to a staging table in data warehouse.\nAs per problem, staging will be in Azure Data Lake Storage account, not in data warehouse.\nAnswer is 'B'"
      },
      {
        "date": "2022-03-18T13:16:00.000Z",
        "voteCount": 1,
        "content": "Sim, o script vai ser executado e carregado posteriormente a execu\u00e7\u00e3o pode ser chamada pela sp_exec_external_script"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52911-exam-dp-203-topic-2-question-50-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL.<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-05T03:28:00.000Z",
        "voteCount": 45,
        "content": "data scientist need scala so standard, jobs need scala so standard, so B but for different reasons"
      },
      {
        "date": "2022-01-21T17:06:00.000Z",
        "voteCount": 2,
        "content": "engineer has to share the cluster so high -concurrency is correct.  the answer should be A"
      },
      {
        "date": "2022-06-08T01:18:00.000Z",
        "voteCount": 8,
        "content": "gina8008 you are missing a point here that data scientists uses scala as per question and scala is not supported in high concurrency cluster. So the answer is no"
      },
      {
        "date": "2021-05-16T14:49:00.000Z",
        "voteCount": 17,
        "content": "Correct is A"
      },
      {
        "date": "2021-05-18T22:46:00.000Z",
        "voteCount": 5,
        "content": "Agree. Jobs cannot use a high-concurrency cluster because it does not support Scala."
      },
      {
        "date": "2022-06-08T01:20:00.000Z",
        "voteCount": 8,
        "content": "and what about the data scientists requirement? Read the question properly and dn't mislead people looking for answers. Scala is not supported in high concurrency and data scientists are using scala as per question so answer in No"
      },
      {
        "date": "2023-09-14T07:30:00.000Z",
        "voteCount": 1,
        "content": "Answer : B\n\"High Concurrency clusters can run workloads developed in SQL, Python, and R.\"\nhttps://learn.microsoft.com/en-us/azure/databricks/archive/compute/configure"
      },
      {
        "date": "2024-03-06T05:46:00.000Z",
        "voteCount": 3,
        "content": "This is an old link (taken from /archive/) and says:\n\"Standard mode clusters are now called No Isolation Shared access mode clusters.\nHigh Concurrency with Tables ACLs are now called Shared access mode clusters.\"\nNew link: https://learn.microsoft.com/en-us/azure/databricks/compute/configure"
      },
      {
        "date": "2023-09-07T06:05:00.000Z",
        "voteCount": 1,
        "content": "B should be correct"
      },
      {
        "date": "2023-08-08T02:35:00.000Z",
        "voteCount": 2,
        "content": "NO is correct answer"
      },
      {
        "date": "2022-03-10T12:56:00.000Z",
        "voteCount": 2,
        "content": "As per Link: https://docs.azuredatabricks.net/clusters/configure.html\n\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --&gt; Data Scientists\nHigh Concurrency clusters&nbsp;do not&nbsp;terminate automatically by default.\nA Standard cluster is recommended for a single user. --&gt; Standard for Data Scientists &amp; High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\n\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --&gt; Jobs needs Scala, hence: Standard"
      },
      {
        "date": "2021-12-13T18:57:00.000Z",
        "voteCount": 6,
        "content": "NO - as High concurrency  not support Scala"
      },
      {
        "date": "2021-12-07T07:36:00.000Z",
        "voteCount": 5,
        "content": "correct: no"
      },
      {
        "date": "2021-11-16T17:53:00.000Z",
        "voteCount": 2,
        "content": "Like djincheg said, Data scientists need scala so B.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-09-17T04:43:00.000Z",
        "voteCount": 15,
        "content": "-Data Engineers: Correct, they are working together, thay need High-Concurency cluster\n-Jobs: Correct, Standad Cluster since it supports SCALA \nHOWEVER:\n- Data Scientists need cluster who terminates after 120 minutes automatically: THAT MEANS ONLY STANDARD AND SINGLE NODE CLUSTERS CAN SUPPORT THAT. \n\nSince this is the holistic question, the answer is NO."
      },
      {
        "date": "2021-09-17T04:20:00.000Z",
        "voteCount": 2,
        "content": "All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity.\nThat means they need standard cluster, not high-concurency cluster. STANDARD cluster terminates automatically after 120 minutes:\n\"Standard and Single Node clusters terminate automatically after 120 minutes by default.\"\nIMO the answer is NO, since all 3 solutions have to be correct."
      },
      {
        "date": "2021-09-02T08:00:00.000Z",
        "voteCount": 4,
        "content": "It's correct that standard cluster is for job workload, but they assigned high concurrency cluster for data scientist, who want to use scala too, so it's false"
      },
      {
        "date": "2021-06-16T14:03:00.000Z",
        "voteCount": 2,
        "content": "Answer: A\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n-Cluster for Jobs should support scala - STANDARD\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-09-16T16:28:00.000Z",
        "voteCount": 1,
        "content": "Solution: You create a High Concurrency cluster for each data scientist\nDoes this meet the goal?\nA. Yes\n\nAnswer: A\n-Data scientist should have their own cluster and should terminate after 120 mins - STANDARD\n\nGENIUSSSSSSSSSS"
      },
      {
        "date": "2021-06-04T09:20:00.000Z",
        "voteCount": 2,
        "content": "A is the right answer because Standard cluster supports scala"
      },
      {
        "date": "2021-05-18T08:30:00.000Z",
        "voteCount": 6,
        "content": "I too agree on the comment by 111222333. As per the requirement \" A workload for jobs that will run notebooks that use Python, Scala, and SOL\". Scala is only supported by Standard"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68447-exam-dp-203-topic-2-question-51-discussion/",
    "body": "You are designing an Azure Databricks cluster that runs user-defined local processes.<br>You need to recommend a cluster configuration that meets the following requirements:<br>\u2711 Minimize query latency.<br>\u2711 Maximize the number of users that can run queries on the cluster at the same time.<br>\u2711 Reduce overall costs without compromising other requirements.<br>Which cluster type should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandard with Auto Termination",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHigh Concurrency with Autoscaling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHigh Concurrency with Auto Termination",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStandard with Autoscaling"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-22T19:17:00.000Z",
        "voteCount": 18,
        "content": "B is correct answer.\nHigh concurrency cluster cannot terminated, so C is wrong. \nStandard cluster cannot shared by multiple tasks, so A and D are wrong."
      },
      {
        "date": "2021-12-30T01:27:00.000Z",
        "voteCount": 17,
        "content": "\"High Concurrency clusters do not terminate automatically by default.\"\nbut u can change that default so your argument about C is incorrect..\nLink: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode"
      },
      {
        "date": "2023-04-08T08:51:00.000Z",
        "voteCount": 5,
        "content": "The cluster does auto-scaling by default. Auto-termination should be set up manually"
      },
      {
        "date": "2023-09-07T06:11:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-07-14T08:22:00.000Z",
        "voteCount": 4,
        "content": "Here is my take on this. Autoscaling addresses first requirement i-e latency and Auto terminate addresses third requirement i-e cost. Now we have to implement both of them manually, meaning none of them is by default. But in the question it says reduce cost without compromising other requirements. So if we go for auto-termination that means we are not autoscaling, compromising the first requirement i-e the latency requirement. therefore the given option is correct i-e B: High Concurrency with autoscaling"
      },
      {
        "date": "2023-05-14T14:46:00.000Z",
        "voteCount": 1,
        "content": "Agree to B"
      },
      {
        "date": "2022-08-05T00:10:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-05-06T01:27:00.000Z",
        "voteCount": 4,
        "content": "Just because auto termination is eligible for high concurrency clusters, doesn't mean we have to use it.\nA key requirement is to \"minimize query latency\", which makes autoscaling more favorable.\n\nRef: \"Workloads can run faster compared to a constant-sized under-provisioned cluster.\"\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-size-and-autoscaling"
      },
      {
        "date": "2022-04-09T01:50:00.000Z",
        "voteCount": 4,
        "content": "High Concurrency clusters can be configured with auto termination (I just checked). BUT: The questions says: reduce costs WITHOUT compromising the other requirements. So I would still go for autoscaling, since there is no answer option that offers both (autoscaling and auto termination)"
      },
      {
        "date": "2022-03-19T09:55:00.000Z",
        "voteCount": 4,
        "content": "Answer should be B as per below \nThe key benefits of High Concurrency clusters are that they provide fine-grained sharing for maximum resource utilization and minimum query latencies.Autoscaling clusters can reduce overall costs compared to a statically-sized cluster."
      },
      {
        "date": "2022-03-09T04:55:00.000Z",
        "voteCount": 3,
        "content": "i try it and it\u00b4s possible to create a cluster with auto termination."
      },
      {
        "date": "2022-02-18T23:06:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/databricks/clusters/configure#cluster-mode\n- High Concurrency clusters do not terminate automatically by default.\n- A Standard cluster is recommended for a single user."
      },
      {
        "date": "2022-02-14T14:47:00.000Z",
        "voteCount": 2,
        "content": "I would say C. High Concurrency with Auto Termination.\nAlthough the default is no auto terminate we can still overwrite that setting."
      },
      {
        "date": "2022-02-12T08:49:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer.\nHigh concurrency cluster cannot AUTO terminated"
      },
      {
        "date": "2022-01-04T14:15:00.000Z",
        "voteCount": 5,
        "content": "Auto terminate for high concurrency cluster is possible. But due to the 2nd point 'Maximize the number of users that can run queries on the cluster at the same time', I will go with option B. High Concurrency and Auto Scaling"
      },
      {
        "date": "2022-03-16T10:21:00.000Z",
        "voteCount": 3,
        "content": "Also to minimise query latency you don't want to have to wait for a cluster to spin up after it terminates"
      },
      {
        "date": "2022-01-01T22:37:00.000Z",
        "voteCount": 1,
        "content": "it's correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67969-exam-dp-203-topic-2-question-52-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Data Factory solution to process data received from Azure Event Hubs, and then ingested into an Azure Data Lake Storage Gen2 container.<br>The data will be ingested every five minutes from devices into JSON files. The files have the following naming pattern.<br>/{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{deviceID}_{YYYY}{MM}{DD}HH}{mm}.json<br>You need to prepare the data for batch data processing so that there is one dataset per hour per deviceType. The solution must minimize read times.<br>How should you configure the sink for the copy activity? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0023500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0023600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: @trigger().startTime -<br>startTime: A date-time value. For basic schedules, the value of the startTime property applies to the first occurrence. For complex schedules, the trigger starts no sooner than the specified startTime value.<br>Box 2: /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json<br>One dataset per hour per deviceType.<br><br>Box 3: Flatten hierarchy -<br>- FlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers https://docs.microsoft.com/en-us/azure/data-factory/connector-file-system",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-14T11:50:00.000Z",
        "voteCount": 102,
        "content": "The correct copy behavior is merge - not flatten hierarchy. \n\nThe question starts with a folder structure as the following:\n/{deviceType}/in/{YYYY}/{MM}/{DD}/{HH}/{deviceID}_{YYYY}{MM}{DD}HH}{mm}.json\n\nIt indicates there are multiple device ID JSON files per deviceType. Those need to be merged to get the target naming pattern - \"one file per device type per hour.\"\nThe target naming pattern is the following:\n/{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\n\nThe correct copy behavior is \"Merge\" because there are multiple files in the source folder that are merged into a single folder per device type per hour."
      },
      {
        "date": "2022-12-02T12:26:00.000Z",
        "voteCount": 3,
        "content": "Why not  /{deviceType}/out/{YYYY}/{MM}/{DD}/{HH}.json ?"
      },
      {
        "date": "2022-12-05T02:39:00.000Z",
        "voteCount": 6,
        "content": "It is not an option. It says /{deviceID}/out/{YYYY}/{MM}/{DD}/{HH}.json"
      },
      {
        "date": "2021-12-20T19:55:00.000Z",
        "voteCount": 90,
        "content": "1) @trigger().outputs.windowStartTime - this output is from a tumbling window trigger, and is required to identify the correct directory at the /{HH}/ level. Using windowStartTime will give the hour with complete data. The @trigger().startTime is for a schedule trigger, which corresponds to the hour for which data has not arrived yet. \n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json is the naming pattern to achieve an hourly dataset for each device type.\n3) Multiple files for each device type will exist on the source side, since the naming pattern starts with {deviceID}... so the files must be merged in the sink to create a single file per device type."
      },
      {
        "date": "2022-06-25T09:14:00.000Z",
        "voteCount": 2,
        "content": "but, the solution must minimize read times, I think is @trigger().startTime"
      },
      {
        "date": "2024-09-21T08:15:00.000Z",
        "voteCount": 1,
        "content": "1. @trigger().startTime\n- the window trigger requires a window, this does not necessarally is the case in the question\n- the oders are using commas. Could be @pipeline().TriggerTime, bit it uses a dot.\n\n2. /yyyy/mm/dd/hh_deviceType.json\n- deviceID would not aggregate all devices by type\n- /deviceType.json would not split by hour\n- hh.json would not split by device type\n\n3. Merge\n- Dynamic content requires reading the content\n- Flatten would not merge data into one file"
      },
      {
        "date": "2024-01-18T09:45:00.000Z",
        "voteCount": 10,
        "content": "Got this question on my exam on january 17, I answered these\n@trigger().StartTime\n /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json \nMerge files\nI passed :)"
      },
      {
        "date": "2024-02-01T15:52:00.000Z",
        "voteCount": 2,
        "content": "Merge is a better answer.\nFlatten hierarchy does not reduce the number of files in the directory.\n https://www.linkedin.com/pulse/copy-behaviour-activity-adf-lokesh-sharma/"
      },
      {
        "date": "2023-12-21T07:39:00.000Z",
        "voteCount": 2,
        "content": "The files must be MERGED &gt; each hour, \nso on @trigger().outputs.windowStartTime =&gt; start time of the window\n\nThe author made 2/3 errors on this Q,   grrr :)\n\n@trigger().outputs.windowStartTime: \nGives the start time of the current window n\n\n@trigger().StartTime: \nGives the start time of each trigger within that window n"
      },
      {
        "date": "2023-10-31T07:58:00.000Z",
        "voteCount": 6,
        "content": "Was on my exam today (31.10.2023)."
      },
      {
        "date": "2023-09-14T07:40:00.000Z",
        "voteCount": 1,
        "content": "It's @trigger().outputs.windowStartTime\nRef : https://learn.microsoft.com/en-us/azure/data-factory/control-flow-system-variables\nUnder Tumbling Window Trigger"
      },
      {
        "date": "2023-09-07T06:17:00.000Z",
        "voteCount": 1,
        "content": "1) @trigger().outputs.windowStartTime\n/{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\nMerge"
      },
      {
        "date": "2023-07-06T06:50:00.000Z",
        "voteCount": 2,
        "content": "what exactly you want to \"FLATTEN\"??? You need to Merge files. period."
      },
      {
        "date": "2023-05-29T20:01:00.000Z",
        "voteCount": 9,
        "content": "1) @trigger().outputs.windowStartTime\n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\n3) Merge"
      },
      {
        "date": "2022-10-28T01:40:00.000Z",
        "voteCount": 6,
        "content": "1. windowstarttime\n2. yyyy/mm/dd/hh_devicetype.json\n3. Merge"
      },
      {
        "date": "2022-08-05T03:54:00.000Z",
        "voteCount": 3,
        "content": "1) @trigger().outputs.windowStartTime\n2) /{YYYY}/{MM}/{DD}/{HH}_{deviceType}.json \n3) Merge\nagree with onyer"
      },
      {
        "date": "2022-02-22T07:19:00.000Z",
        "voteCount": 10,
        "content": "Of course it is a merge, can't believe the official provided answers are so wrong ... Who wrote that"
      },
      {
        "date": "2022-03-16T10:31:00.000Z",
        "voteCount": 4,
        "content": "I know it's almost as bad as Microsoft documentation about Azure.. That's why we see so much confusion over so many questions"
      },
      {
        "date": "2022-01-17T07:57:00.000Z",
        "voteCount": 1,
        "content": "Would you have to delay the tumbling processing by 60minutes to pick up data that hasn't arrived for that hour yet?"
      },
      {
        "date": "2022-01-04T18:30:00.000Z",
        "voteCount": 3,
        "content": "The batch job runs in Data Factory should use Tumbling window trigger, so system variable trigger().outputs.windowStartTime should be passed in  as the parameter."
      },
      {
        "date": "2021-12-18T06:50:00.000Z",
        "voteCount": 2,
        "content": "data is generated every 5 min but output needs every 1 hour/device it, it needs to merge files to achieve this."
      },
      {
        "date": "2021-12-18T05:45:00.000Z",
        "voteCount": 2,
        "content": "The answers are correct. Flatten Hierarchy. https://vmfocus.com/2019/01/09/using-azure-data-factory-to-copy-data-between-azure-file-shares-part-1/"
      },
      {
        "date": "2022-06-08T01:52:00.000Z",
        "voteCount": 4,
        "content": "think logically what flatten and merge means and what is asked in the question"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67521-exam-dp-203-topic-2-question-53-discussion/",
    "body": "DRAG DROP -<br>You are designing an Azure Data Lake Storage Gen2 structure for telemetry data from 25 million devices distributed across seven key geographical regions. Each minute, the devices will send a JSON payload of metrics to Azure Event Hubs.<br>You need to recommend a folder structure for the data. The solution must meet the following requirements:<br>\u2711 Data engineers from each region must be able to build their own pipelines for the data of their respective region only.<br>\u2711 The data must be processed at least once every 15 minutes for inclusion in Azure Synapse Analytics serverless SQL pools.<br>How should you recommend completing the structure? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0023700003.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0023800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: {raw/regionID}<br>Box 2: {YYYY}/{MM}/{DD}/{HH}/{mm}<br>Box 3: {deviceID}<br>Reference:<br>https://github.com/paolosalvatori/StreamAnalyticsAzureDataLakeStore/blob/master/README.md",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-14T11:40:00.000Z",
        "voteCount": 205,
        "content": "The correct answer is \n{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json\n\n{raw/regionID} is the first level because raw is the container name for the raw data. RegionID follows it for ease of managing security.\n\n{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json instead of {deviceID}/{YYYY}/{MM}/{DD}/{HH}/{mm}.json. The primary reason is that you want your namespace structure to have as few folders as high up and narrow those down as you get deeper into your structure.\n\nFor example, if you have 1 year worth of data and 25 million devices, using {YYYY}/{MM}/{DD}/{HH}/{mm}/ results in 2.1 million folders (1 year * 12 months * 30 days [estimate] * 24 hours * 60 minutes). If you start your folder structure with {deviceID}, you end up with 25 million folders - one for each device - before you even get to including the date in the hierarchy."
      },
      {
        "date": "2023-06-22T23:50:00.000Z",
        "voteCount": 8,
        "content": "In IoT workloads, there can be a great deal of data being ingested that spans across numerous products, devices, organizations, and customers. It's important to pre-plan the directory layout for organization, security, and efficient processing of the data for down-stream consumers. A general template to consider might be the following layout:\n\n{Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/\n\nFor example, landing telemetry for an airplane engine within the UK might look like the following structure:\n\nUK/Planes/BA1293/Engine1/2017/08/11/12/\nIn this example, by putting the date at the end of the directory structure, you can use ACLs to more easily secure regions and subject matters to specific users and groups. If you put the date structure at the beginning, it would be much more difficult to secure these regions and subject matters. For example, if you wanted to provide access only to UK data or certain planes, you'd need to apply a separate permission for numerous directories under every hour directory. This structure would also exponentially increase the number of directories as time went on."
      },
      {
        "date": "2024-02-13T17:41:00.000Z",
        "voteCount": 1,
        "content": "This explains it.I retire to best practice.Lol...Thanks"
      },
      {
        "date": "2023-06-22T23:51:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#monitor-telemetry"
      },
      {
        "date": "2022-01-30T22:47:00.000Z",
        "voteCount": 2,
        "content": "Brilliantly explained thank you. correct answer."
      },
      {
        "date": "2023-02-09T17:20:00.000Z",
        "voteCount": 1,
        "content": "The correct structure answer will have 561.600 folders per year."
      },
      {
        "date": "2021-12-20T19:58:00.000Z",
        "voteCount": 3,
        "content": "This is correct"
      },
      {
        "date": "2021-12-10T00:55:00.000Z",
        "voteCount": 15,
        "content": "raw/RegionId should be in the first box as raw is the name of your container. Furthermore, putting RegionId as one of the first foldernames allows easy partitioning and simpler RBAC for the Data Engineers."
      },
      {
        "date": "2021-12-14T07:47:00.000Z",
        "voteCount": 5,
        "content": "Yes I agree, raw/regionId --&gt; timestamp --&gt; deviceId.json"
      },
      {
        "date": "2023-06-22T23:30:00.000Z",
        "voteCount": 5,
        "content": "I'll follow best practice from Microsoft:\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#monitor-telemetry\n\nSo: /raw/regionid/deviceid/YYYY/MM/DD/HH\n(without minutes)."
      },
      {
        "date": "2023-05-29T20:04:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is\n{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json"
      },
      {
        "date": "2022-03-30T23:39:00.000Z",
        "voteCount": 4,
        "content": "I think that link will help us to find the correct answer:\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices\n\nThe given example for a directory structure is: *{Region}/{SubjectMatter(s)}/{yyyy}/{mm}/{dd}/{hh}/*"
      },
      {
        "date": "2022-03-22T08:01:00.000Z",
        "voteCount": 2,
        "content": "{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{mm}/{deviceID}.json"
      },
      {
        "date": "2022-02-22T11:00:00.000Z",
        "voteCount": 3,
        "content": "IMHO  {YYYY}/{MM}/{DD}/{HH}/{regionID/raw}/{deviceID}.json (given answer) is correct. Please pay attention that there is no minutes {mm} course it is not supported by Time format \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/blob-storage-azure-data-lake-gen2-output"
      },
      {
        "date": "2022-02-22T06:35:00.000Z",
        "voteCount": 3,
        "content": "{raw/regionID}/{YYYY}/{MM}/{DD}/{HH}/{deviceID}.json\n\nTime Format [optional]: if the time token is used in the prefix path, specify the time format in which your files are organized. Currently the only supported value is\nHH."
      },
      {
        "date": "2022-01-22T18:19:00.000Z",
        "voteCount": 1,
        "content": "Question 54: the correct answer of box 2 is {YYYY}/{MM}/{DD}/{HH}_{deviceType}.json\nOne dataset per hour per deviceType.\n\nSo looks like regionid and deviceid should be put after {YYYY}/{MM}/{DD}/{HH}/{mm} .\n\n{YYYY}/{MM}/{DD}/{HH}/{mm}/{raw/regionID}/{deviceID}.json"
      },
      {
        "date": "2022-01-22T18:32:00.000Z",
        "voteCount": 2,
        "content": "Still feel {raw/RegionID} / {YYYY/MM/DD/mm} /{DeviceID} is correct. Just have some questions after compare answers of question 54."
      },
      {
        "date": "2022-01-07T23:41:00.000Z",
        "voteCount": 2,
        "content": "The Question says : Each minute, the devices will send a JSON payload. That means the data is demarcated by region and by minutes.\n{raw/RegionID} / {YYYY/MM/DD/mm}   /{DeviceID}"
      },
      {
        "date": "2021-12-29T23:05:00.000Z",
        "voteCount": 2,
        "content": "/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}."
      },
      {
        "date": "2021-12-14T03:42:00.000Z",
        "voteCount": 4,
        "content": "raw/regionid - &gt; DeviceId -&gt; YYYY/MM/dd/HH-mm"
      },
      {
        "date": "2023-06-22T23:49:00.000Z",
        "voteCount": 2,
        "content": "without minute info."
      },
      {
        "date": "2021-12-14T00:48:00.000Z",
        "voteCount": 4,
        "content": "{raw/regionID}/{deviceID}/{YYYY}/{MM}/{DD}/{HH}{mm} imo."
      },
      {
        "date": "2023-06-22T23:48:00.000Z",
        "voteCount": 2,
        "content": "without minute in my opinion"
      },
      {
        "date": "2023-07-12T06:48:00.000Z",
        "voteCount": 1,
        "content": "IMO, with {mm}. \nOtherwise, every HH dir will have 25mil (device) * 60 (freq. of incoming files)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68068-exam-dp-203-topic-2-question-54-discussion/",
    "body": "HOTSPOT -<br>You are implementing an Azure Stream Analytics solution to process event data from devices.<br>The devices output events when there is a fault and emit a repeat of the event every five seconds until the fault is resolved. The devices output a heartbeat event every five seconds after a previous event if there are no faults present.<br>A sample of the events is shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0023900001.png\" class=\"in-exam-image\"><br>You need to calculate the uptime between the faults.<br>How should you complete the Stream Analytics SQL query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0024000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0024100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: WHERE EventType='HeartBeat'<br>Box 2: ,TumblingWindow(Second, 5)<br>Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.<br>The following diagram illustrates a stream with a series of events and how they are mapped into 10-second tumbling windows.<br><img src=\"/assets/media/exam-media/04259/0024200001.jpg\" class=\"in-exam-image\"><br>Incorrect Answers:<br>,SessionWindow.. : Session windows group events that arrive at similar times, filtering out periods of time where there is no data.<br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-15T12:02:00.000Z",
        "voteCount": 92,
        "content": "I think the right answers should be WHERE EventType='HeartBeat' and Session window. If we want to calculate the uptime between the faults, we must use session window for each device, we know that will be receiving events for each 5 seconds if there is no error, so when an error occurs (or if we reach the maximum size of the window) then a new event will not be received within the next 5 seconds and the window will close, calculating the uptime. However if We use Tumbling window, it\u00b4s not possible to calculate the uptime beyond 5 seconds"
      },
      {
        "date": "2022-02-24T19:09:00.000Z",
        "voteCount": 1,
        "content": "I concur!"
      },
      {
        "date": "2021-12-20T20:18:00.000Z",
        "voteCount": 3,
        "content": "Yes this sounds right"
      },
      {
        "date": "2022-06-23T19:34:00.000Z",
        "voteCount": 1,
        "content": "what happen if the event continues and the 50,000 second finishes? you cannot count that as a fault event"
      },
      {
        "date": "2022-06-23T19:59:00.000Z",
        "voteCount": 2,
        "content": "Sorry, you are right @Fer079!"
      },
      {
        "date": "2022-01-18T07:09:00.000Z",
        "voteCount": 8,
        "content": "How it can be WHERE EventType='HearBeat'? If we filter out all the faults how can you calculate the time between the faults? I would go for BC"
      },
      {
        "date": "2022-02-02T05:55:00.000Z",
        "voteCount": 4,
        "content": "when we are filtering out the faults then we expect to receive right events every 5 seconds, so if we don\u00b4t receive an event within 5 seconds then we know that there is a fault and the session windows will close (because it has been set to 5 seconds) calculating the time in this session window"
      },
      {
        "date": "2022-05-13T17:07:00.000Z",
        "voteCount": 1,
        "content": "It actually says when a fault occurs the device will repeat the same message every 5 seconds so there actually is never a time where you do not receive a message every 5 seconds but you may receive a message within 5 seconds in case of an error."
      },
      {
        "date": "2022-06-15T11:02:00.000Z",
        "voteCount": 1,
        "content": "yeah it will repeat the message but not with  event type as heartbeat. So the where condition works here"
      },
      {
        "date": "2023-04-26T00:41:00.000Z",
        "voteCount": 1,
        "content": "we are trying to calculate here the uptime i.e. when the device was up and running correctly, and exclude the failures;   the session window will group 'hearbeat'events which happened within 5 seconds from each other; if any failure happens, the next event with hearbeat will be happening after 6 seconds (e.g. hearbeat 1 sec, failure 2 sec, heartbeat 7 sec) -&gt; because they write in the question that it always is 5 seconds between events, unless failure happens (obviously a failure of a device is happening on no schedule, so it can happen as soon as you plug in your device to measure heartbeat);"
      },
      {
        "date": "2023-04-26T00:41:00.000Z",
        "voteCount": 1,
        "content": "but after failure its emitting also signal only after 5 seconds, even if failure persists; so the next event will be minimum after 6 seconds from the previous one; with windows session of timeout of 5 seconds, the windows will just timeout; and as soon as devices restarts a new session will be counted , until next failure; if no failure occurs, the session will last almost 14 hrs (max duration set to 50000 seconds i.e. around 14hrs); so imo condition on eventype is correct, and it should be grouped by sessionwindow; here is some useful docs on the concepts https://www.passio-consulting.com/post/azure-stream-analytics-windowing-functions#:~:text=The%20hopping%20window%20is%20similar,information%20in%20another%20time%20frame.\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns"
      },
      {
        "date": "2021-12-24T14:19:00.000Z",
        "voteCount": 23,
        "content": "My answer is:\nQuestion 1: B. Use LAG function as a filter to only filter out the events that switch from 'HeartBeat' to fault or witch from fault to 'HeartBeat'.\nQuestion 2: C. No matter if there is a fault, device always sends message every 5min. Calculate the uptime between the faults don't need any window here. Any duration &gt; 5s should between fault line and heartbeat line should be part of items that need to count into to calculate duration."
      },
      {
        "date": "2022-01-29T11:39:00.000Z",
        "voteCount": 8,
        "content": "You cannot use the LAG function here because the \"partition by\" by deviceId is not included here, so the change between the status could be between different devices. This LAG function is evaluated before the \"group by\" clause of the query.\nIf you see the Microsoft documentation:\nhttps://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics\nIt says clearly that \"LAG isn't affected by predicates in the WHERE clause, join conditions in the JOIN clause, or grouping expressions in the GROUP BY clause of the current query because it's evaluated before those clauses.\""
      },
      {
        "date": "2022-04-14T10:39:00.000Z",
        "voteCount": 2,
        "content": "LAG does not require the PARTITION BY this is optional.."
      },
      {
        "date": "2023-04-25T23:46:00.000Z",
        "voteCount": 1,
        "content": "you do not need partition by with LAG function; its an optional parameter; however in this scenario this is not the reason why we should not be using this function; with LAG we will receive in the query result only the \"transition\" events i.e. the device works correctly (eventype'heartbeat;) and then there is fault ('fault')-&gt; we would receive only the record with \"faul\" (as its different then previous line event i.e. heartbeat; by this one record we will not know how long the device was operational correctly, because we dont have these records anymore; we need to have 'startting ' record for correctly operating device with heartbeat event , and this for every singe \"re-start' after the fault;  LAG function would be good to calculate e.g. the increasing heartbeat by comparing the heartbeat of previous records with current one; but not in this user case;"
      },
      {
        "date": "2024-10-05T16:49:00.000Z",
        "voteCount": 1,
        "content": "SELECT DeviceID,\n    MIN(EventTime) AS StartTime,\n    MAX(EventTime) AS EndTime,\n    DATEDIFF(second, MIN(EventTime), MAX(EventTime)) AS duration_in_seconds\nFROM input TIMESTAMP BY EventTime\nWHERE EventType = 'HeartBeat'\nGROUP BY DeviceID, TumblingWindow(second, 5)\nHAVING DATEDIFF(second, MIN(EventTime), MAX(EventTime)) &gt; 5"
      },
      {
        "date": "2024-09-21T23:55:00.000Z",
        "voteCount": 1,
        "content": "1. You want \"between faults\" and between faults there are heartbeats. So EventType must be heartbeat.\n\n2. Usually I'd go with Session (check link), but this session timing is bugging me. Why 50k seconds for a window limit? I will probably have a global uptime, not uptimes between faults... So I'll go with tumbling.\n\nhttps://learn.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics"
      },
      {
        "date": "2024-08-24T14:19:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT - EventType = 'Heartbeat', Session Window\n\nTo calculate the uptime between faults in the Azure Stream Analytics SQL query, you should complete the query by using the SessionWindow function, which groups events based on a period of inactivity, allowing you to calculate durations of uptime between faults.\n\nHere's how to complete the query:\n\nFilter the HeartBeat events: You should filter only for HeartBeat events because these indicate the system is running without faults.\nUse SessionWindow: This function groups events into sessions based on gaps of inactivity. In this scenario, a session is defined by the HeartBeat events, which will end when a fault occurs.\nCalculate the duration: The DATEDIFF function is used to calculate the time difference between the start and end of each session, giving you the uptime between faults."
      },
      {
        "date": "2023-09-07T06:21:00.000Z",
        "voteCount": 2,
        "content": "WHERE EventType='HeartBeat' and Session window."
      },
      {
        "date": "2023-05-29T20:06:00.000Z",
        "voteCount": 4,
        "content": "1. Where EventType = 'HeartBeat'\n2. SessionWindow"
      },
      {
        "date": "2023-05-01T16:38:00.000Z",
        "voteCount": 1,
        "content": "1. Where EventType = 'HeartBeat'\n2. SessionWindow"
      },
      {
        "date": "2022-08-26T23:25:00.000Z",
        "voteCount": 1,
        "content": "The where clause must be EvenType != 'HearBeat' otherwise you're not counting the uptime between the fault"
      },
      {
        "date": "2022-08-26T23:30:00.000Z",
        "voteCount": 1,
        "content": "Sorry ignore it"
      },
      {
        "date": "2022-08-06T07:47:00.000Z",
        "voteCount": 1,
        "content": "Agree with Fer079 ,  EventType='HeartBeat' and Session window is correct"
      },
      {
        "date": "2022-06-25T21:44:00.000Z",
        "voteCount": 1,
        "content": "WHERE EventType='HeartBeat'  is definitely correct as you would need to filter out other events to calculate the uptime.\nIf you look at the example  in link https://docs.microsoft.com/en-us/stream-analytics-query/session-window-azure-stream-analytics \nit would be crystal clear that sessionwindow is the right answer and @iooj (a lot of thanks) has already tested it"
      },
      {
        "date": "2022-03-31T23:07:00.000Z",
        "voteCount": 2,
        "content": "tricky but instructive question."
      },
      {
        "date": "2022-01-21T00:47:00.000Z",
        "voteCount": 13,
        "content": "I created a Stream Analytics job and tested all combinations and here is my answer.\nWith a tumbling window, you will never be able to accumulate the correct interval. \nThe session is suitable here, but if the session closes earlier (by timeout) than the event occurs, then it will also fail to accumulate. So please note that in the timeout should be 6, not 5. A working version: EventType='HeartBeat' and SessionWindow(second, 6, 50000). But... \nP.S. In the data example on the screenshot, the difference is generally indicated in minutes, in this situation, none of the answers will work, you will need to change seconds to minutes."
      },
      {
        "date": "2022-05-23T03:45:00.000Z",
        "voteCount": 1,
        "content": "Thanks for testing it, but I think your conclusion is wrong.\nWe should calculate the difference (without any limitation). If you use SessionWindow with a timeout of 6 you limit this functionality. You get the right answer for the data in the table but what happens if you have a failure after &gt;6 seconds?\nI think Canary_2021 is right -&gt; B, C\n\nP.S. :-D didn't recognized it... but would say that this is a typo in the table."
      },
      {
        "date": "2022-01-17T09:06:00.000Z",
        "voteCount": 1,
        "content": "B and A?"
      },
      {
        "date": "2022-01-07T23:47:00.000Z",
        "voteCount": 5,
        "content": "The answer is BC. Every T_SQL Group by Query that needs to calculate max based on certain criteria should use the HAVING function to group that criteria"
      },
      {
        "date": "2021-12-21T06:27:00.000Z",
        "voteCount": 4,
        "content": "For me the  session window suits for the given scenario. Also no device ID has been considered in the given answer, which is essential for calculating the uptime period per device"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68281-exam-dp-203-topic-2-question-55-discussion/",
    "body": "You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.<br>Which switch should you use to switch between languages?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%&lt;language&gt;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t@&lt;Language &gt;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\\\[&lt;language &gt;]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\\\(&lt;language &gt;)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-22T06:15:00.000Z",
        "voteCount": 50,
        "content": "I wish you a DP203 as easy as this question folks"
      },
      {
        "date": "2022-01-29T02:41:00.000Z",
        "voteCount": 4,
        "content": "We all hope man"
      },
      {
        "date": "2022-11-01T04:59:00.000Z",
        "voteCount": 2,
        "content": "Username suits youe wis! :D"
      },
      {
        "date": "2022-08-06T07:48:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2022-01-17T09:07:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2021-12-21T18:45:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2021-12-19T02:00:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68558-exam-dp-203-topic-2-question-56-discussion/",
    "body": "You have an Azure Data Factory pipeline that performs an incremental load of source data to an Azure Data Lake Storage Gen2 account.<br>Data to be loaded is identified by a column named LastUpdatedDate in the source table.<br>You plan to execute the pipeline every four hours.<br>You need to ensure that the pipeline execution meets the following requirements:<br>\u2711 Automatically retries the execution when the pipeline run fails due to concurrency or throttling limits.<br>\u2711 Supports backfilling existing data in the table.<br>Which type of trigger should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tevent",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton-demand",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling window\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-24T19:00:00.000Z",
        "voteCount": 26,
        "content": "D is correct answer. \nhttps://www.sqlshack.com/how-to-schedule-azure-data-factory-pipeline-executions-using-triggers/\nAzure Data Factory pipeline executions using Triggers: \n\u2022\tSchedule Trigger: The schedule trigger is used to execute the Azure Data Factory pipelines on a wall-clock schedule.\n\u2022\tTumbling Window Trigger: Can be used to process history data. Also can define Delay, Max concurrency, retry policy etc.\n\u2022\tEvent-Based Triggers : The event-based trigger executes the pipelines in response to a blob-related event, such as creating or deleting a blob file, in an Azure Blob Storage"
      },
      {
        "date": "2023-06-18T18:19:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-05-31T22:58:00.000Z",
        "voteCount": 1,
        "content": "Correct. As soon as you see backfill, its tumbling."
      },
      {
        "date": "2022-08-06T08:16:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-07-25T10:39:00.000Z",
        "voteCount": 3,
        "content": "(D)\nTumbling window trigger\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#tumbling-window-trigger\n\nRetry capability:\nIs Supported. Failed pipeline runs have a default retry policy of 0, or a policy that's specified by the user in the trigger definition. Automatically retries when the pipeline runs fail due to concurrency/server/throttling limits (that is, status codes 400: User Error, 429: Too many requests, and 500: Internal Server error)."
      },
      {
        "date": "2022-01-24T23:27:00.000Z",
        "voteCount": 2,
        "content": "D is correct. Tumbling Window has more advance options for setting retry and concurrency policies which schedule doesnt have."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69388-exam-dp-203-topic-2-question-57-discussion/",
    "body": "You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account.<br>The data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/.<br>You need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts.<br>Which two configurations should you include in the design? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a file naming pattern for the destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the files in the destination before loading the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter by the last modified date of the source files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the source files after they are copied."
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-02-04T12:00:00.000Z",
        "voteCount": 15,
        "content": "AC is correct, there is no point about deletion in source and might be the case that the data should stay in source too."
      },
      {
        "date": "2022-04-28T22:03:00.000Z",
        "voteCount": 7,
        "content": "I think the C option has impact in data transfer, B are incorrect, D is irrelevant for the question, and A is a complement of the task"
      },
      {
        "date": "2024-02-13T21:35:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D \nThe question requested a solution that will reduce data transfer between the two systems .\nThe solution is thesame as how  can REUNDANCY or Multiple Copies of the data  be avoided during copying.\nExplanation \n1.). Deleting the source files after they are copied  will keep track of the where to start the next copy .\n2.)Filter by the last modified date of the source files; This also helps to keep track of where to resuming file movement from"
      },
      {
        "date": "2023-09-07T06:25:00.000Z",
        "voteCount": 2,
        "content": "AC is correct"
      },
      {
        "date": "2023-05-06T11:25:00.000Z",
        "voteCount": 5,
        "content": "A. Specify a file naming pattern for the destination:\nBy specifying a file naming pattern for the destination files in the Azure Data Lake Storage Gen2 account, you can ensure that the files are organized and stored in a structured manner. This can help with data management and subsequent processing.\n\nC. Filter by the last modified date of the source files:\nBy filtering the source files based on the last modified date, you can select only the files that have been modified on the current day. This reduces the amount of data transferred and improves the efficiency of the data load process."
      },
      {
        "date": "2022-08-06T11:32:00.000Z",
        "voteCount": 5,
        "content": "should be AC"
      },
      {
        "date": "2022-03-09T03:28:00.000Z",
        "voteCount": 3,
        "content": "I will go for AC\nWhy not D? Cause they are not mentionned some cost opitmisation"
      },
      {
        "date": "2022-02-22T18:13:00.000Z",
        "voteCount": 1,
        "content": "AD are correct ?"
      },
      {
        "date": "2022-02-03T13:17:00.000Z",
        "voteCount": 4,
        "content": "The requirement is to minimize the data transfer.\nIf we delete the files in source then there is no need to filter for daily load. So answer C,D is incorrect. Beside, there is no requirement to for minimizing the cost.\nTo my point of view, AC is correct because, even though filter by the modified date will take long time for lot of files, it won't impact the transfer."
      },
      {
        "date": "2022-01-24T23:39:00.000Z",
        "voteCount": 6,
        "content": "Normally we move the files after being processed, so it has to be D."
      },
      {
        "date": "2022-02-02T14:08:00.000Z",
        "voteCount": 2,
        "content": "is A,D correct"
      },
      {
        "date": "2022-01-08T20:04:00.000Z",
        "voteCount": 2,
        "content": "Shout it be A &amp;D as the requirement is to minimize the process time. Will option C take longer compared to D?"
      },
      {
        "date": "2022-02-18T04:31:00.000Z",
        "voteCount": 4,
        "content": "Minimizing the process time is not part of the question. \"Minimizing the data transfer\", whatever that is - either time or amount."
      },
      {
        "date": "2022-01-04T18:49:00.000Z",
        "voteCount": 3,
        "content": "Either C or D can realize daily incremental load. Not sure why need to setup both of them."
      },
      {
        "date": "2022-01-03T14:03:00.000Z",
        "voteCount": 2,
        "content": "should it be C, D?"
      },
      {
        "date": "2023-01-12T20:51:00.000Z",
        "voteCount": 4,
        "content": "YOU CAN'T GO WITHOUT A"
      },
      {
        "date": "2024-10-03T16:35:00.000Z",
        "voteCount": 1,
        "content": "why? \"Minimizing the data transfer\" has nothing to do with A."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68821-exam-dp-203-topic-2-question-58-discussion/",
    "body": "You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table.<br>Which output mode should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tupdate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcomplete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tappend\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-19T09:09:00.000Z",
        "voteCount": 8,
        "content": "Correct Answer."
      },
      {
        "date": "2024-02-07T06:46:00.000Z",
        "voteCount": 1,
        "content": "Append"
      },
      {
        "date": "2023-09-07T06:25:00.000Z",
        "voteCount": 1,
        "content": "Append"
      },
      {
        "date": "2023-06-18T19:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-12-29T13:03:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-10-20T15:27:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-08-06T11:33:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2022-06-20T08:52:00.000Z",
        "voteCount": 1,
        "content": "Append is correct"
      },
      {
        "date": "2021-12-28T13:10:00.000Z",
        "voteCount": 4,
        "content": "agree with append"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68835-exam-dp-203-topic-2-question-59-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.<br>You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.<br>You plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of<br>Table1.<br>You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.<br>Solution: In an Azure Synapse Analytics pipeline, you use a data flow that contains a Derived Column transformation.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-28T15:03:00.000Z",
        "voteCount": 23,
        "content": "\"Data flows are available both in Azure Data Factory and Azure Synapse Pipelines\"\n\"Use the derived column transformation to generate new columns in your data flow or to modify existing fields.\"\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"
      },
      {
        "date": "2022-01-19T09:08:00.000Z",
        "voteCount": 9,
        "content": "Derived Column cannot get  DateTime (created or lastmodified datetime) of the files. \nGet Metadata activity can retrieve the DateTime of the files.\nso answer should be B."
      },
      {
        "date": "2022-01-19T09:14:00.000Z",
        "voteCount": 10,
        "content": "If it is a real time process and pipeline is triggered to load data to table1 when file drop to container immediately, the created datetime of the file is similar as the pipeline process datetime. In this way Derived Column works. \nThe question is not clear."
      },
      {
        "date": "2023-01-23T08:30:00.000Z",
        "voteCount": 3,
        "content": "Cant we just use the current datetime when the data is loaded. It doesnt say that we need to get data from the files. Just datetime which is kind of confusing. I will say, use derived column"
      },
      {
        "date": "2023-08-28T21:47:00.000Z",
        "voteCount": 2,
        "content": "A. Use this transformation to add any new columns to existing data."
      },
      {
        "date": "2022-08-06T23:49:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-08-06T09:59:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-06-20T08:24:00.000Z",
        "voteCount": 1,
        "content": "What is the DateTime measuring?  The DML transaction time or a file property?\n\nIf the measurement gives respect to the DML transaction time, you can use this: https://docs.microsoft.com/en-us/azure/data-factory/data-flow-expressions-usage#currentTimestamp"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68559-exam-dp-203-topic-2-question-60-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.<br>You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.<br>You plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of<br>Table1.<br>You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.<br>Solution: You use a dedicated SQL pool to create an external table that has an additional DateTime column.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-24T19:26:00.000Z",
        "voteCount": 19,
        "content": "Answer should be B.\nAn external table is based on a source flat file structure. It seems to make no sense to add additional date time columns to such a table."
      },
      {
        "date": "2024-02-13T15:14:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-11-21T19:11:00.000Z",
        "voteCount": 1,
        "content": "Answer B. The answer is incomplete because 2 additional steps were missing. After the 1st step which is creating the external table in the dedicated SQL pool with the additional DateTime column, the 2nd step is to load data using for example PolyBase to load data from the files in container1 into the external table of your dedicated SQL pool. 3rd step is to transform and insert once the data is in the dedicated SQL pool, and then insert the transformed data into your actual Table1, including the additional DateTime column."
      },
      {
        "date": "2022-08-06T23:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-08-06T20:31:00.000Z",
        "voteCount": 3,
        "content": "From the words in the Solution part, it seems to use PolyBase to read external tables. PolyBase can't change the schemas of external tables(files). You can only transform the data after loading data in the staging directory. And then load the data into tables"
      },
      {
        "date": "2022-05-29T16:25:00.000Z",
        "voteCount": 2,
        "content": "serverless works for data lake\ndedicated doesn't"
      },
      {
        "date": "2022-01-29T11:58:00.000Z",
        "voteCount": 4,
        "content": "Its clearly mentioned \"You plan to insert data from the files in container1 into Table1\". External tables dont get the data inserted into themselves, but instead refer outside data."
      },
      {
        "date": "2022-01-08T18:13:00.000Z",
        "voteCount": 3,
        "content": "If using dedicated SQL pool, after creating an external table, need a further CTAS for adding derived columns."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69389-exam-dp-203-topic-2-question-61-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.<br>You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.<br>You plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of<br>Table1.<br>You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.<br>Solution: You use an Azure Synapse Analytics serverless SQL pool to create an external table that has an additional DateTime column.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-23T01:39:00.000Z",
        "voteCount": 22,
        "content": "You can't use serverless pool to create table in dedicate pool"
      },
      {
        "date": "2022-11-04T08:07:00.000Z",
        "voteCount": 7,
        "content": "Table1 is in dedicated  sql pool"
      },
      {
        "date": "2024-03-24T05:46:00.000Z",
        "voteCount": 2,
        "content": "The solution does not meet the goal. While an Azure Synapse Analytics serverless SQL pool can be used to create an external table, this alone will not ensure that the DateTime is stored as an additional column in Table1 when the source data files are loaded to container1. The DateTime would need to be included in the data files themselves or added during the transformation process before loading the data into Table1. Therefore, the proposed solution is incorrect."
      },
      {
        "date": "2024-02-13T15:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is Yes,"
      },
      {
        "date": "2024-02-11T01:58:00.000Z",
        "voteCount": 1,
        "content": "No. Because of the serverless pool."
      },
      {
        "date": "2023-09-07T06:29:00.000Z",
        "voteCount": 2,
        "content": "should be no"
      },
      {
        "date": "2023-09-10T06:21:00.000Z",
        "voteCount": 2,
        "content": "correct to yes"
      },
      {
        "date": "2023-08-12T03:20:00.000Z",
        "voteCount": 1,
        "content": "You can't use serverless pool to create table in dedicate pool"
      },
      {
        "date": "2023-08-11T06:57:00.000Z",
        "voteCount": 2,
        "content": "B since Table1 is in dedicated pool"
      },
      {
        "date": "2023-08-03T16:53:00.000Z",
        "voteCount": 2,
        "content": "You can use external tables to read external data using dedicated SQL pool or serverless SQL pool.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"
      },
      {
        "date": "2022-12-05T04:00:00.000Z",
        "voteCount": 3,
        "content": "Q:\"You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.\nYou plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of\nTable1.\nYou need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\"\nPark for a while Table1 and dedicated SQL pool, that is where the transformation will happen AFTER loading from container1 to Table1.\nHere is about loading data to ADLSG2 continer1 and adding a column which can be done with serverless SQL as an external table."
      },
      {
        "date": "2022-10-25T01:04:00.000Z",
        "voteCount": 1,
        "content": "if table 1 would be serverless, yes, now no"
      },
      {
        "date": "2022-09-16T01:35:00.000Z",
        "voteCount": 4,
        "content": "The job is to insert data from the files in container1 into Table1 (in the dedicated sql pool) and transform the data after that and we need to add a new additional column.\n\nExternal table are just references to the data, only metadata is really stored in the sql pool.\nHence anything including external table will be not a solution.\nIf you follow the different proposed solutions from previous questions, the most efficient solution is to use derived column transformation."
      },
      {
        "date": "2022-08-07T00:01:00.000Z",
        "voteCount": 1,
        "content": "yes, with serverless pool we can add a new column while creating an external table"
      },
      {
        "date": "2022-08-06T20:49:00.000Z",
        "voteCount": 1,
        "content": "The aim of the solution is to load data from Data Lake's files to dedicated SQL pool's tables. There are three ways: DF's Copy Activity, PolyBase and Bulk insert. It's not serverless SQL pool's business..."
      },
      {
        "date": "2022-06-24T15:42:00.000Z",
        "voteCount": 1,
        "content": "The answer should be yes as we can create an additional column using CETAS in a serverless SQL pool though it is not a complete solution but a step closer to the required result."
      },
      {
        "date": "2022-05-29T16:24:00.000Z",
        "voteCount": 1,
        "content": "Serverless pool works for data lake\nDedicated doesn't"
      },
      {
        "date": "2022-05-14T22:01:00.000Z",
        "voteCount": 3,
        "content": "Apparently when dealing with dedicated sql pools you can only create an external table by importing the data from source using ctas. However, when using serverless using cetas will actually export a new file to your data source as well as create an external table. With that being said I think the answer is A."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/73939-exam-dp-203-topic-2-question-62-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1.<br>You have files that are ingested and loaded into an Azure Data Lake Storage Gen2 container named container1.<br>You plan to insert data from the files in container1 into Table1 and transform the data. Each row of data in the files will produce one row in the serving layer of<br>Table1.<br>You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.<br>Solution: In an Azure Synapse Analytics pipeline, you use a Get Metadata activity that retrieves the DateTime of the files.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T09:10:00.000Z",
        "voteCount": 21,
        "content": "Is part of a posible solution, but it isn't sufficient to meet the goal, yo need to pass the \"Get metadata\"'s output as a parameter to the ingest process, processing each file inside a \"for\" loop, for example.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity"
      },
      {
        "date": "2022-06-02T00:18:00.000Z",
        "voteCount": 11,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity\npoints that Get Metadata activity can retrieve the corresponding Matadata type of: Created datetime of the file or folder."
      },
      {
        "date": "2024-06-29T21:12:00.000Z",
        "voteCount": 3,
        "content": "This is my take on the answer. The question clearly states that \"You need to ensure that when the source data files are loaded to container1, the DateTime is stored as an additional column in Table1.\" Here the focus should be on the term \"additional column\" which helps us understand that the datetime column does not already exist in the files in container1. \nNow what is Get Metadata activity? As per documentation link https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity \"You can use the Get Metadata activity to retrieve the metadata of any data in Azure Data Factory or a Synapse pipeline\". This will work only if the underlying data files contains the DateTime column which is not the case here. Hence to add an additional column to Table1 in Synapse, Get Metadata activity won't be of help and we have to derive the column through data flow transformation which is the result for Question 59."
      },
      {
        "date": "2024-02-13T15:25:00.000Z",
        "voteCount": 1,
        "content": "The Get Metadata activity in Azure Synapse Analytics pipeline can retrieve the metadata of any data, including the DateTime of the files. So Answer is Yes"
      },
      {
        "date": "2024-05-18T02:10:00.000Z",
        "voteCount": 1,
        "content": "yes but it won't map as a column"
      },
      {
        "date": "2023-09-07T06:31:00.000Z",
        "voteCount": 1,
        "content": "nonono"
      },
      {
        "date": "2023-08-12T03:24:00.000Z",
        "voteCount": 1,
        "content": "Does not meet the goal"
      },
      {
        "date": "2023-06-14T15:08:00.000Z",
        "voteCount": 3,
        "content": "According to ChatGPT:\n\nB. No\n\nThe proposed solution of using a Get Metadata activity in an Azure Synapse Analytics pipeline will retrieve the DateTime of the files, but it does not address the requirement of storing the DateTime as an additional column in Table1."
      },
      {
        "date": "2023-09-27T18:31:00.000Z",
        "voteCount": 2,
        "content": "chatGpt is not to be trusted at all."
      },
      {
        "date": "2024-03-27T07:55:00.000Z",
        "voteCount": 2,
        "content": "But in this case is right :)"
      },
      {
        "date": "2023-03-07T05:37:00.000Z",
        "voteCount": 5,
        "content": "No, using a Get Metadata activity in an Azure Synapse Analytics pipeline to retrieve the DateTime of the files does not meet the goal of storing the DateTime as an additional column in Table1. The Get Metadata activity retrieves metadata about the files, such as file size, file name, or last modified date, but it does not provide the file content needed to extract the DateTime value and store it as an additional column in Table1. To achieve the goal, you need to use a data flow in the pipeline that loads the data from container1, extracts the DateTime value, and transforms the data by adding the DateTime column to Table1."
      },
      {
        "date": "2022-11-22T01:53:00.000Z",
        "voteCount": 2,
        "content": "If DateTime is part of data in files in container1 than answer is A, but if it is not part of data in files but only Meta data of files then B. Wording in question is really strange but I think it is A because it says \"data from files in container1\""
      },
      {
        "date": "2022-08-07T00:16:00.000Z",
        "voteCount": 2,
        "content": "Its confusing,  if we need to insert the dateTime of insertion then answer should be No, but if we need to insert the datetime of file modified then answer should be yes.\n\nTo me looks like the question is about 1st case so the answer should be No"
      },
      {
        "date": "2023-01-12T21:12:00.000Z",
        "voteCount": 1,
        "content": "AGREED"
      },
      {
        "date": "2022-07-25T11:29:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
      },
      {
        "date": "2022-06-25T09:40:00.000Z",
        "voteCount": 1,
        "content": "I'm confusing more every time a read the solution, I don't know if it says that you have to do it in two setps, that changes everything"
      },
      {
        "date": "2022-06-14T06:33:00.000Z",
        "voteCount": 3,
        "content": "It seems rather odd that in the same two previous questions \"Use the derived column transformation to generate new columns in your data flow or to modify existing fields.\" was the answer. This is very confusing.\nReference:\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-derived-column"
      },
      {
        "date": "2022-04-20T12:10:00.000Z",
        "voteCount": 2,
        "content": "Get Metadata seems possible\nhttps://www.mssqltips.com/sqlservertip/6246/azure-data-factory-get-metadata-example/"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74332-exam-dp-203-topic-2-question-63-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 61,
        "isMostVoted": true
      },
      {
        "answer": "U",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T08:55:00.000Z",
        "voteCount": 21,
        "content": "I think is A. Yes.\nYou can execute R code in a notebook, and then call it from Data Factory.\nYou can check it at \"Databricks Notebook activity\" header:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data\nAnd also: \nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/sparkr/overview"
      },
      {
        "date": "2024-01-30T08:50:00.000Z",
        "voteCount": 1,
        "content": "No is the answer: It is all true, what you wrote down, but Synapse Analytics has got its own \"data factory\". Its name is \"pipeline\". I do not see a chance to set up to data factory the sick as DWH in Synapse."
      },
      {
        "date": "2024-01-30T09:24:00.000Z",
        "voteCount": 2,
        "content": "I tried it. I was wrong."
      },
      {
        "date": "2024-02-02T04:17:00.000Z",
        "voteCount": 7,
        "content": "Next time only comment when you are sure. People like you are making it more confusing for people who are trying to learn and prepare for the exam."
      },
      {
        "date": "2024-01-04T14:35:00.000Z",
        "voteCount": 6,
        "content": "**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**"
      },
      {
        "date": "2023-09-07T06:34:00.000Z",
        "voteCount": 1,
        "content": "yes can do it"
      },
      {
        "date": "2023-06-14T15:09:00.000Z",
        "voteCount": 3,
        "content": "A. Yes\n\nThe proposed solution meets the goal of designing a daily process to ingest incremental data from the staging zone, transform the data using an R script, and insert the transformed data into a data warehouse in Azure Synapse Analytics. The solution involves using an Azure Data Factory (ADF) schedule trigger to execute a pipeline that executes an Azure Databricks notebook and then inserts the data into the data warehouse."
      },
      {
        "date": "2023-03-07T05:38:00.000Z",
        "voteCount": 4,
        "content": "Yes, this solution meets the goal of ingesting incremental data from the staging zone, transforming the data by executing an R script, and inserting the transformed data into a data warehouse in Azure Synapse Analytics. By using an Azure Data Factory schedule trigger, you can schedule the pipeline to run on a daily basis. The pipeline can execute an Azure Databricks notebook, which can perform the transformation using R scripts, and then insert the transformed data into the data warehouse."
      },
      {
        "date": "2023-01-05T03:09:00.000Z",
        "voteCount": 4,
        "content": "yes, you can execute R script in notebook and call it via adf"
      },
      {
        "date": "2022-11-27T08:13:00.000Z",
        "voteCount": 4,
        "content": "the answer is YES. I already used this solution in a previous project."
      },
      {
        "date": "2022-10-28T01:51:00.000Z",
        "voteCount": 2,
        "content": "should be YES"
      },
      {
        "date": "2022-08-27T04:10:00.000Z",
        "voteCount": 4,
        "content": "We do sth like it in my company"
      },
      {
        "date": "2022-08-07T00:20:00.000Z",
        "voteCount": 5,
        "content": "answer should be A"
      },
      {
        "date": "2022-07-25T11:48:00.000Z",
        "voteCount": 3,
        "content": "A.\nR Language is supported in ADB.\nADB notebooks, can be called from ADF  pipeline(Use Notebook Activity) to link to the ADB notebook"
      },
      {
        "date": "2022-06-23T20:17:00.000Z",
        "voteCount": 2,
        "content": "I don't know guys, it's kind of tricky, in 2 next questions, it says \"inser the TRANSFORMED data\" and here it says jus \"DATA\".... what do you think?"
      },
      {
        "date": "2022-06-09T07:17:00.000Z",
        "voteCount": 3,
        "content": "Para mi es la respuesta A. En un pipeline de ADF puede tener una actividad de notebook para databricks, el cual permitir\u00e1 ejecutar el notebook una vez al d\u00eda a trav\u00e9s de un trigger."
      },
      {
        "date": "2022-05-14T08:44:00.000Z",
        "voteCount": 4,
        "content": "R in notebook and call via Data Factory"
      },
      {
        "date": "2022-05-12T01:54:00.000Z",
        "voteCount": 4,
        "content": "You can execute R code in a notebook."
      },
      {
        "date": "2022-05-14T00:43:00.000Z",
        "voteCount": 2,
        "content": "The correct answer should be No, based on the how it is worded and the following logic:\n\nIn Azure Data Factory a Databricks Activity can be used to execute a Databricks notebook. However, it cannot pass the data along to the next activity ( dbutils.notebook.exit(\"returnValue\") only passes a string). Given that the way this is worded it says \" execute a pipeline that executes an Azure Databricks notebook, and then inserts the data \" the \"then\" implies a next step which wont work as cant pass the data along. If the transformation  and insert both happened in the notebook only then it would work.\n\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data-databricks-notebook"
      },
      {
        "date": "2022-05-14T22:05:00.000Z",
        "voteCount": 2,
        "content": "Yea but you do not have to pass the data along in ADF. You can insert it into Synapse from the notebook."
      },
      {
        "date": "2022-05-17T06:36:00.000Z",
        "voteCount": 2,
        "content": "precisely my point, either both things ( R and Insert) should be in the one workbook OR you need two workbooks. The wording indicates 2 steps rather than all in one book: \"notebook\" being the first step and \u201cthen\u201d indicating another step."
      },
      {
        "date": "2022-05-17T06:44:00.000Z",
        "voteCount": 1,
        "content": "*2 notebooks =if possible to transfer info between them Imeant"
      },
      {
        "date": "2022-06-25T09:32:00.000Z",
        "voteCount": 2,
        "content": "It says what the process need, but not necessary in 2 steps"
      },
      {
        "date": "2022-05-02T03:47:00.000Z",
        "voteCount": 2,
        "content": "I agree that Yes"
      },
      {
        "date": "2022-04-24T15:32:00.000Z",
        "voteCount": 3,
        "content": "I think it should be Yes. i.e. A\nR Script is well supported by databricks notepad"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74334-exam-dp-203-topic-2-question-64-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T09:05:00.000Z",
        "voteCount": 8,
        "content": "Is correct.\nMapping Dataflows can't execute R code that is a requeriment, so not meet the goal."
      },
      {
        "date": "2024-01-04T14:36:00.000Z",
        "voteCount": 1,
        "content": "**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**"
      },
      {
        "date": "2023-09-07T06:35:00.000Z",
        "voteCount": 1,
        "content": "no is correct"
      },
      {
        "date": "2022-08-27T04:13:00.000Z",
        "voteCount": 4,
        "content": "There is no R in ADF dataflow"
      },
      {
        "date": "2022-08-07T00:25:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2022-06-21T12:17:00.000Z",
        "voteCount": 3,
        "content": "The answer is no."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74335-exam-dp-203-topic-2-question-65-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T09:12:00.000Z",
        "voteCount": 13,
        "content": "The correct answer is \"A. Yes\"\nYou can execute R code in a notebook, and then call it from Data Factory.\nYou can check it at \"Databricks Notebook activity\" header:\nhttps://docs.microsoft.com/en-US/azure/data-factory/transform-data\nAnd also:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/sparkr/overview"
      },
      {
        "date": "2023-04-30T08:21:00.000Z",
        "voteCount": 2,
        "content": "...but where is the ingest done?"
      },
      {
        "date": "2022-04-24T09:19:00.000Z",
        "voteCount": 12,
        "content": "I'm Sorry, in the statement there isn't mention to \"Data factory\", but you can use a Databrick's job also,  therefore the solution meet the goal.\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs#--run-a-job"
      },
      {
        "date": "2024-05-01T22:50:00.000Z",
        "voteCount": 1,
        "content": "well it is a trick question and I would hate to get it. \nIt says execute DBr job which can be only executed from ADF"
      },
      {
        "date": "2024-01-10T02:50:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nExplanation:\n\nScheduling an Azure Databricks job that executes an R notebook and then inserts the data into the data warehouse is a valid solution that meets the goal. Azure Databricks is a cloud-based platform that integrates with Apache Spark, providing a collaborative environment for big data analytics and machine learning. It supports multiple programming languages, including R."
      },
      {
        "date": "2024-01-04T14:36:00.000Z",
        "voteCount": 2,
        "content": "**VARIATIONS OF THIS QUESTION**\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. **NO**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse. **YES**\n\nSolution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes mapping data flow, and then inserts the data into the data warehouse. **NO**\n\nSolution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse. **YES**"
      },
      {
        "date": "2023-09-07T06:36:00.000Z",
        "voteCount": 1,
        "content": "Yes, this solution would meet the goal."
      },
      {
        "date": "2023-03-07T05:43:00.000Z",
        "voteCount": 3,
        "content": "Yes, this solution would meet the goal. An Azure Databricks job can be scheduled to run on a regular basis, such as daily, and can execute an R notebook that reads data from Azure Data Lake Storage, transforms the data using R code, and then writes the transformed data to the data warehouse in Azure Synapse Analytics."
      },
      {
        "date": "2023-01-05T03:15:00.000Z",
        "voteCount": 3,
        "content": "should be yes, you can schedule notebook directly from databricks"
      },
      {
        "date": "2022-12-27T17:01:00.000Z",
        "voteCount": 1,
        "content": "Has to be Yes"
      },
      {
        "date": "2022-12-03T14:55:00.000Z",
        "voteCount": 1,
        "content": "The Answer is A. You can only execute R notebook in Databricks and not in Data Factory. The key word here is Databricks."
      },
      {
        "date": "2022-10-15T05:06:00.000Z",
        "voteCount": 2,
        "content": "1.\textract data from Azure Data Lake Storage Gen2 into Azure Databricks, \n2.\trun transformations on the data in Azure Databricks, \n3.\tload the transformed data into Azure Synapse Analytics."
      },
      {
        "date": "2022-08-07T00:35:00.000Z",
        "voteCount": 1,
        "content": "yes, its possible"
      },
      {
        "date": "2022-05-31T15:50:00.000Z",
        "voteCount": 2,
        "content": "I go for A as well"
      },
      {
        "date": "2022-05-31T10:38:00.000Z",
        "voteCount": 2,
        "content": "You have an Azure subscription that includes the following resources:\n\nVNet1, a virtual network\n\nSubnet1, a subnet in VNet1\n\nWebApp1, a web app application service\n\nNSG1, a network security group\n\nYou create an application security group named ASG1.\n\nWhich resource can use ASG1?\n\nSelecione somente uma resposta.\n\nVNet1\n\nSubnet1\n\nWebApp1\n\nNSG1"
      },
      {
        "date": "2022-10-27T22:13:00.000Z",
        "voteCount": 1,
        "content": "the anwser is : VNet1"
      },
      {
        "date": "2022-05-31T02:35:00.000Z",
        "voteCount": 2,
        "content": "I go for A."
      },
      {
        "date": "2022-05-25T01:27:00.000Z",
        "voteCount": 2,
        "content": "I go for A.\nDatabrick should have an option to trigger the job on selected schedule, it doesn't need data factory to trigger."
      },
      {
        "date": "2022-05-06T01:00:00.000Z",
        "voteCount": 2,
        "content": "I would go for No. You can create a Spark Submit Job to run R Code but as shown in the second link, Databricks Utilities is not supoorted which would be necessary in my opinion to connect to Data Lake\n\nhttps://docs.microsoft.com/en-us/azure/databricks/jobs\n\nWhat do you think ?\nhttps://docs.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/examples#spark-submit-api-example-r"
      },
      {
        "date": "2022-06-24T20:02:00.000Z",
        "voteCount": 1,
        "content": "you made me doubt about it"
      },
      {
        "date": "2022-05-02T01:21:00.000Z",
        "voteCount": 2,
        "content": "The solution meet the goal"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74336-exam-dp-203-topic-2-question-66-discussion/",
    "body": "You plan to create an Azure Data Factory pipeline that will include a mapping data flow.<br>You have JSON data containing objects that have nested arrays.<br>You need to transform the JSON-formatted data into a tabular dataset. The dataset must have one row for each item in the arrays.<br>Which transformation method should you use in the mapping data flow?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tnew branch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunpivot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\talter row",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tflatten\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-24T15:52:00.000Z",
        "voteCount": 7,
        "content": "Correct"
      },
      {
        "date": "2022-04-24T09:22:00.000Z",
        "voteCount": 6,
        "content": "Is correct\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-flow-flatten"
      },
      {
        "date": "2024-07-26T15:02:00.000Z",
        "voteCount": 1,
        "content": "flatten for JSON"
      },
      {
        "date": "2023-09-07T06:37:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-08-07T00:37:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74446-exam-dp-203-topic-2-question-67-discussion/",
    "body": "You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and to output the data to an Azure Blob storage account.<br>You need to output the count of tweets during the last five minutes every five minutes. Each tweet must only be counted once.<br>Which windowing function should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Sliding window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Session window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Hopping window that has a one-minute hop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Tumbling window\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-21T12:56:00.000Z",
        "voteCount": 7,
        "content": "corrett. It would be corret also a hopping window with hop and size both to 5 seconds"
      },
      {
        "date": "2023-12-08T10:58:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-07T06:37:00.000Z",
        "voteCount": 1,
        "content": "repeated"
      },
      {
        "date": "2022-08-20T18:41:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-08-07T00:38:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-07T14:43:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-25T05:30:00.000Z",
        "voteCount": 3,
        "content": "Is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74226-exam-dp-203-topic-2-question-68-discussion/",
    "body": "You are planning a streaming data solution that will use Azure Databricks. The solution will stream sales transaction data from an online store. The solution has the following specifications:<br>The output data will contain items purchased, quantity, line total sales amount, and line total tax amount.<br><img src=\"/assets/media/exam-media/04259/0025200002.png\" class=\"in-exam-image\"><br>\u2711 Line total sales amount and line total tax amount will be aggregated in Databricks.<br>\u2711 Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.<br>You need to recommend an output mode for the dataset that will be processed by using Structured Streaming. The solution must minimize duplicate data.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComplete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-30T07:04:00.000Z",
        "voteCount": 18,
        "content": "I think Update is correct, because \" new rows will be added to adjust a sale\" , that means that in the course of a day you must update de daily import with the new sales, the group by process generates new amounts, keep in mind that when it say \"sales transactions will never be updated\" its about the online store, not the aggregated rows."
      },
      {
        "date": "2023-06-27T13:25:00.000Z",
        "voteCount": 7,
        "content": "Sales transactions will never be updated. Instead, new rows will be added to adjust a sale."
      },
      {
        "date": "2023-01-17T19:22:00.000Z",
        "voteCount": 16,
        "content": "Using chatgpt : Append"
      },
      {
        "date": "2024-07-15T07:10:00.000Z",
        "voteCount": 1,
        "content": "We need to append new entries only, the question describes updates that are not done on existing row entries, but by adding new rows of duplicate transactions"
      },
      {
        "date": "2024-06-18T13:36:00.000Z",
        "voteCount": 1,
        "content": "append..."
      },
      {
        "date": "2024-04-25T01:08:00.000Z",
        "voteCount": 3,
        "content": "Pay attention to the \"adjust\" word. That is like in double entry accounting. Lines are added positive or negative, then queries are used to produce final numeric value (aggregating).\nIt is C"
      },
      {
        "date": "2024-05-18T02:18:00.000Z",
        "voteCount": 1,
        "content": "Exacly"
      },
      {
        "date": "2024-04-20T02:15:00.000Z",
        "voteCount": 1,
        "content": "It's Update"
      },
      {
        "date": "2024-02-19T07:05:00.000Z",
        "voteCount": 2,
        "content": "I think 'A. Update' is correct.\nFrom what I understand, \"Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.\" means that the input stream will have new rows reflecting the corrected sales transaction. If we use \"append\" output mode we will have duplicates in the target table, corresponding to both the original transaction as well as the new corrected transaction.\nInstead, we can use the forEachBatch method and \"update\" output mode to merge each microBatch to the target table, updating old transactions if they match or inserting new ones if they don't. This would minimize duplicate data as well as allow for line sales amount and tax amount to be aggregated correctly in the target table."
      },
      {
        "date": "2024-02-19T07:09:00.000Z",
        "voteCount": 1,
        "content": "https://docs.databricks.com/en/structured-streaming/delta-lake.html#upsert-from-streaming-queries-using-foreachbatch"
      },
      {
        "date": "2024-02-11T02:12:00.000Z",
        "voteCount": 3,
        "content": "Append: This mode adds new rows to the output sink for each received event. It's perfect for your scenario where sales transactions are never updated but adjusted by adding new rows. This guarantees no duplicate data due to updates, minimizing duplicates.\nUpdate: This mode updates existing rows in the output sink if a matching key is found. Since you mentioned no updates occur, using this mode would lead to unnecessary operations and potential inconsistencies.\nComplete: This mode writes the entire dataset to the output sink for each micro-batch interval. This is unnecessary and inefficient for your scenario since only new rows need to be added, and it potentially duplicates data across micro-batches."
      },
      {
        "date": "2024-02-02T02:58:00.000Z",
        "voteCount": 1,
        "content": "Append. The update does not add the new rows"
      },
      {
        "date": "2024-01-01T17:10:00.000Z",
        "voteCount": 1,
        "content": "Append: you are only adding new rows and existing rows do not need to be updated"
      },
      {
        "date": "2023-12-28T10:40:00.000Z",
        "voteCount": 1,
        "content": "when you see new rows will be added to   APPEND is always the answers"
      },
      {
        "date": "2023-12-15T13:00:00.000Z",
        "voteCount": 1,
        "content": "(ChatGPT) The Append output mode is used when new rows are added to the result table. This mode is suitable for scenarios where the output table is a summary of the input data, and the input data is not updated"
      },
      {
        "date": "2023-09-26T04:36:00.000Z",
        "voteCount": 1,
        "content": "Sales transactions will NEVER be updated.\n\nAppend."
      },
      {
        "date": "2023-09-08T17:36:00.000Z",
        "voteCount": 1,
        "content": "Update"
      },
      {
        "date": "2023-09-07T07:58:00.000Z",
        "voteCount": 1,
        "content": "C. Append\nThis mode is used when you are always adding new records to the output data. Given that sales transactions will never be updated and new rows will be added to adjust a sale, this mode seems to be the most suitable. It will also help in minimizing duplicate data since it only adds new records and does not modify existing ones."
      },
      {
        "date": "2023-09-07T06:40:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-07T06:43:00.000Z",
        "voteCount": 2,
        "content": "ignore it"
      },
      {
        "date": "2023-08-19T10:58:00.000Z",
        "voteCount": 1,
        "content": "Append is the right choice . Update is for modifications and append is to add new rows"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74724-exam-dp-203-topic-2-question-69-discussion/",
    "body": "You have an enterprise data warehouse in Azure Synapse Analytics named DW1 on a server named Server1.<br>You need to determine the size of the transaction log file for each distribution of DW1.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn DW1, execute a query against the sys.database_files dynamic management view.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Azure Monitor in the Azure portal, execute a query against the logs of DW1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute a query against the logs of DW1 by using the Get-AzOperationalInsightsSearchResult PowerShell cmdlet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOn the master database, execute a query against the sys.dm_pdw_nodes_os_performance_counters dynamic management view.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-06-10T03:56:00.000Z",
        "voteCount": 22,
        "content": "The question asks for transaction log size on each distribution. The correct answer is D: Link below: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor\n-- Transaction log size\nSELECT\n  instance_name as distribution_db,\n  cntr_value*1.0/1048576 as log_file_size_used_GB,\n  pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'"
      },
      {
        "date": "2024-06-29T22:43:00.000Z",
        "voteCount": 1,
        "content": "This is correct. The explanation is given in the link provided above."
      },
      {
        "date": "2022-06-25T09:46:00.000Z",
        "voteCount": 5,
        "content": "but you don't need it from master, just DW1"
      },
      {
        "date": "2022-06-26T15:21:00.000Z",
        "voteCount": 11,
        "content": "D is totally correct. Link has this very clearly mentioned\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
      },
      {
        "date": "2024-09-20T07:45:00.000Z",
        "voteCount": 1,
        "content": "```\nSELECT\n  instance_name as distribution_db, pdw_node_id,\n  cntr_value*1.0/1048576 as log_file_size_used_GB\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE instance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n```\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size"
      },
      {
        "date": "2024-09-18T03:09:00.000Z",
        "voteCount": 1,
        "content": "ans-: D because the Reference document\n from Xam topic i checked"
      },
      {
        "date": "2024-07-08T12:26:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nThe sys.dm_pdw_nodes_os_performance_counters dynamic management view provides performance counter information for each node in your Synapse Analytics instance. This includes metrics related to the transaction log file."
      },
      {
        "date": "2024-06-30T16:00:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT:\nSELECT\n    name,\n    type_desc,\n    size * 8 / 1024 AS size_in_MB\nFROM\n    sys.database_files\nWHERE\n    type = 1;  -- Type 1 corresponds to log files"
      },
      {
        "date": "2024-04-17T14:42:00.000Z",
        "voteCount": 1,
        "content": "sys.dm_pdw_nodes_os_performance_counters DMV in Azure Synapse Analytics does not provide information about the size of the transaction log file for each distribution of the data warehouse; it provides CPU utilization, memory usage, disk I/O rates, and network traffic at the node level. \nTo obtain information about the size of transaction log files, we can use sys.dm_db_file_space_usage or sys.database_files."
      },
      {
        "date": "2023-12-13T13:53:00.000Z",
        "voteCount": 2,
        "content": "Table sys.dm_pdw_nodes_os_performance_counter contains information about current size of file log each distribution. \nYou can use sys.database_files to determine size of file log of DW1 (each distribiution the same)."
      },
      {
        "date": "2024-02-02T03:05:00.000Z",
        "voteCount": 1,
        "content": "Agreed with A."
      },
      {
        "date": "2023-09-07T06:43:00.000Z",
        "voteCount": 1,
        "content": "Should be D"
      },
      {
        "date": "2023-08-12T03:56:00.000Z",
        "voteCount": 1,
        "content": "A is wrong it applies for SQL server and non distributed non MPP database.. question clearly says per distribution and synapse"
      },
      {
        "date": "2023-07-06T07:30:00.000Z",
        "voteCount": 1,
        "content": "the question is about distribution, so D should be answer."
      },
      {
        "date": "2023-06-27T13:28:00.000Z",
        "voteCount": 2,
        "content": "-- Transaction log size\nSELECT\n  instance_name as distribution_db,\n  cntr_value*1.0/1048576 as log_file_size_used_GB,\n  pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size"
      },
      {
        "date": "2023-06-27T13:29:00.000Z",
        "voteCount": 1,
        "content": "This query returns the transaction log size on each distribution."
      },
      {
        "date": "2023-06-23T00:24:00.000Z",
        "voteCount": 2,
        "content": "Probably A and D are correct, but I would choise D, because it's clearly described as the question:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
      },
      {
        "date": "2023-06-14T15:19:00.000Z",
        "voteCount": 1,
        "content": "Monitor transaction log size\nThe following query returns the transaction log size on each distribution. If one of the log files is reaching 160 GB, you should consider scaling up your instance or limiting your transaction size.\n\n-- Transaction log size\nSELECT\n  instance_name as distribution_db,\n  cntr_value*1.0/1048576 as log_file_size_used_GB,\n  pdw_node_id\nFROM sys.dm_pdw_nodes_os_performance_counters\nWHERE\ninstance_name like 'Distribution_%'\nAND counter_name = 'Log File(s) Used Size (KB)'\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size"
      },
      {
        "date": "2023-05-26T11:28:00.000Z",
        "voteCount": 1,
        "content": "D. See this article https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor#monitor-transaction-log-size"
      },
      {
        "date": "2023-01-15T16:28:00.000Z",
        "voteCount": 2,
        "content": "According to the documentation:\n\"For information about the current log file size, its maximum size, and the autogrow option for the file, you can also use the size, max_size, and growth columns for that log file in sys.database_files.\"\nA seems enough, I am not sure it gives the results for each distribution but it seems so."
      },
      {
        "date": "2022-11-28T00:29:00.000Z",
        "voteCount": 1,
        "content": "i think \"D\""
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76198-exam-dp-203-topic-2-question-70-discussion/",
    "body": "You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:<br>\u2711 Send the output to Azure Synapse.<br>\u2711 Identify spikes and dips in time series data.<br>\u2711 Minimize development and configuration effort.<br>Which should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-23T09:32:00.000Z",
        "voteCount": 11,
        "content": "Obviously B, iOT is event hub of stream data so we need stream analytics for sure."
      },
      {
        "date": "2023-09-07T06:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-01T21:28:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-04-16T18:32:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-08-07T01:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-06-21T13:13:00.000Z",
        "voteCount": 2,
        "content": "i agree"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76457-exam-dp-203-topic-2-question-71-discussion/",
    "body": "A company uses Azure Stream Analytics to monitor devices.<br>The company plans to double the number of devices that are monitored.<br>You need to monitor a Stream Analytics job to ensure that there are enough processing resources to handle the additional load.<br>Which metric should you monitor?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEarly Input Events",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLate Input Events",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatermark delay\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInput Deserialization Errors"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T17:11:00.000Z",
        "voteCount": 7,
        "content": "C--&gt;it measures the amount of delay in the processing of the input events. If the watermark delay increases, it could indicate that the Stream Analytics job is not able to keep up with the incoming data and may not have enough processing resources to handle the additional load."
      },
      {
        "date": "2024-09-20T07:53:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-time-handling#watermark-delay-details"
      },
      {
        "date": "2023-09-07T06:46:00.000Z",
        "voteCount": 1,
        "content": "sure it"
      },
      {
        "date": "2022-12-27T06:40:00.000Z",
        "voteCount": 2,
        "content": "correct !"
      },
      {
        "date": "2022-08-07T02:56:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-07-09T12:51:00.000Z",
        "voteCount": 2,
        "content": "Watermark delay - correct"
      },
      {
        "date": "2022-05-31T03:07:00.000Z",
        "voteCount": 4,
        "content": "seems ok"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76974-exam-dp-203-topic-2-question-72-discussion/",
    "body": "HOTSPOT -<br>You are designing an enterprise data warehouse in Azure Synapse Analytics that will store website traffic analytics in a star schema.<br>You plan to have a fact table for website visits. The table will be approximately 5 GB.<br>You need to recommend which distribution type and index type to use for the table. The solution must provide the fastest query performance.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0025600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0025700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash -<br>Consider using a hash-distributed table when:<br>The table size on disk is more than 2 GB.<br>The table has frequent insert, update, and delete operations.<br><br>Box 2: Clustered columnstore -<br>Clustered columnstore tables offer both the highest level of data compression and the best overall query performance.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index",
    "votes": [],
    "comments": [
      {
        "date": "2022-08-07T03:01:00.000Z",
        "voteCount": 10,
        "content": "Hash and clustered columnstore..right"
      },
      {
        "date": "2024-02-16T19:46:00.000Z",
        "voteCount": 1,
        "content": "Distribution               \tIdeal data\t          Recommended size\nHash:   \tFact tables with join column\t   Large (hundreds MBs - TBs)\nRound-Robin:\tStaging tables, no join column\t             Smaller (up to hundreds of MBs)\nReplicated Distribution: Dimension tables, small references\t     Very small (up to 2 GB)"
      },
      {
        "date": "2023-06-01T21:32:00.000Z",
        "voteCount": 4,
        "content": "Correct\nhash and CCI"
      },
      {
        "date": "2022-06-21T02:39:00.000Z",
        "voteCount": 1,
        "content": "I'm not sure about the hash distribution since we don't have enough information on what columns we get. In any case I would choose Round Robin to just have a even distribution."
      },
      {
        "date": "2022-06-29T03:42:00.000Z",
        "voteCount": 9,
        "content": "I would go with Hash as the table is &gt;2gb and is a fact table..."
      },
      {
        "date": "2023-05-08T13:24:00.000Z",
        "voteCount": 1,
        "content": "Round Robin is used for Staging Table"
      },
      {
        "date": "2023-06-14T15:25:00.000Z",
        "voteCount": 4,
        "content": "Anything above 2GB you should go with Hash. Round Robin/Heap is for staging tables."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76468-exam-dp-203-topic-2-question-73-discussion/",
    "body": "You have an Azure Stream Analytics job.<br>You need to ensure that the job has enough streaming units provisioned.<br>You configure monitoring of the SU % Utilization metric.<br>Which two additional metrics should you monitor? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBacklogged Input Events\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWatermark Delay\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFunction Events",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOut of order Events",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLate Input Events"
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-31T15:56:00.000Z",
        "voteCount": 12,
        "content": "A and B are correct"
      },
      {
        "date": "2023-09-07T06:47:00.000Z",
        "voteCount": 1,
        "content": "A and B are correct"
      },
      {
        "date": "2023-06-01T21:33:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-04-16T18:37:00.000Z",
        "voteCount": 2,
        "content": "Correct Answers"
      },
      {
        "date": "2022-08-07T03:04:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/microsoft/view/73955-exam-dp-203-topic-2-question-74-discussion/",
    "body": "You have an activity in an Azure Data Factory pipeline. The activity calls a stored procedure in a data warehouse in Azure Synapse Analytics and runs daily.<br>You need to verify the duration of the activity when it ran last.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tactivity runs in Azure Monitor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tActivity log in Azure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe sys.dm_pdw_wait_stats data management view in Azure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Resource Manager template"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-08-07T03:07:00.000Z",
        "voteCount": 9,
        "content": "Monitor, in ADF we have monitor to check all activity runs"
      },
      {
        "date": "2023-09-07T06:47:00.000Z",
        "voteCount": 1,
        "content": "Monitor, in ADF we have monitor to check all activity runs"
      },
      {
        "date": "2023-05-01T03:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-27T06:43:00.000Z",
        "voteCount": 4,
        "content": "is correct answer"
      },
      {
        "date": "2022-12-20T17:49:00.000Z",
        "voteCount": 2,
        "content": "A IS CORRECT ANSWER"
      },
      {
        "date": "2022-07-26T04:25:00.000Z",
        "voteCount": 3,
        "content": "I'd go with A using this:\n https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor"
      },
      {
        "date": "2022-06-25T08:28:00.000Z",
        "voteCount": 4,
        "content": "Azure monitor is different from the Monitor option shown in the screenshot."
      },
      {
        "date": "2022-05-31T05:16:00.000Z",
        "voteCount": 2,
        "content": "answer is correct. screenshot shows azure data factory pipeline run"
      },
      {
        "date": "2022-05-31T05:17:00.000Z",
        "voteCount": 3,
        "content": "* under monitor section in ADF"
      },
      {
        "date": "2022-04-20T13:23:00.000Z",
        "voteCount": 2,
        "content": "based upon the screen shot, isn't that part of the azure synapse analytics (one of the icons from the left)?"
      },
      {
        "date": "2022-05-25T23:05:00.000Z",
        "voteCount": 1,
        "content": "Looks like Data Factory to me. If Data Factory was there I would have picked it."
      },
      {
        "date": "2022-05-23T06:49:00.000Z",
        "voteCount": 3,
        "content": "answer is correct, monitoring is under Monitor and Dashboard"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/microsoft/view/73956-exam-dp-203-topic-2-question-75-discussion/",
    "body": "You have an Azure Data Factory pipeline that is triggered hourly.<br>The pipeline has had 100% success for the past seven days.<br>The pipeline execution fails, and two retries that occur 15 minutes apart also fail. The third failure returns the following error.<br>ErrorCode=UserErrorFileNotFound,'Type=Microsoft.DataTransfer.Common.Shared.HybridDeliveryException,Message=ADLS Gen2 operation failed for: Operation returned an invalid status code 'NotFound'. Account: 'contosoproduksouth'. Filesystem: wwi. Path: 'BIKES/CARBON/year=2021/month=01/day=10/hour=06'. ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'. RequestId: '6d269b78-901f-001b-4924-e7a7bc000000'. TimeStamp: 'Sun, 10 Jan 2021 07:45:05<br>What is a possible cause of the error?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom 06:00 to 07:00 on January 10, 2021, there was no data in wwi/BIKES/CARBON.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom 06:00 to 07:00 on January 10, 2021, the file format of data in wwi/BIKES/CARBON was incorrect.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe pipeline was triggered too early."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 58,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-08T07:02:00.000Z",
        "voteCount": 32,
        "content": "The error message says a missing file, which matches with answer B: missing data from 06:00. The process had re-tried three times, 15 mins apart, which explains that the error was generated 07:45."
      },
      {
        "date": "2022-12-04T09:29:00.000Z",
        "voteCount": 3,
        "content": "i don't agree. the path is not created correctly and therefore the file is ' missing' . It is in the error message too."
      },
      {
        "date": "2022-12-19T03:29:00.000Z",
        "voteCount": 3,
        "content": "Answer A. The path in error message shows hour=06 whereas the hour of retry run is 07."
      },
      {
        "date": "2024-04-12T07:34:00.000Z",
        "voteCount": 2,
        "content": "Agree, noting that previous runs were succeeded which indicates it is not parameter issue"
      },
      {
        "date": "2022-06-22T12:23:00.000Z",
        "voteCount": 1,
        "content": "Thank you for the detail"
      },
      {
        "date": "2023-12-30T02:17:00.000Z",
        "voteCount": 4,
        "content": "To elaborate, the pipeline is triggered hourly, any processing that is done has to be applied to the data that was received in the last hour. In other words, at 07:00 the data received between 06:00 and 07:00 is processed. Accounting for 3*15m results in 07:45.\n\nMore elegantly however, the timestamps worked fine for a week which indicates that path creation is not a problem. Answer is B as you say."
      },
      {
        "date": "2023-03-09T08:33:00.000Z",
        "voteCount": 9,
        "content": "For 7 days, this job was succeeding.\nSo path rule seems to be right."
      },
      {
        "date": "2024-09-20T08:25:00.000Z",
        "voteCount": 1,
        "content": "The pipeline ran fine for 7 days?\n\nNo miscalculation, unless summer time was applied, which usually doesn't happen at 7 AM anywhere. Also, no bad params or it would never have run.\n\nIt's about data."
      },
      {
        "date": "2024-05-01T23:14:00.000Z",
        "voteCount": 2,
        "content": "when something runs smoothly and then all of the sudden it does not then it must be data\neither missing or incorrect\npresuming that the data is also loaded by consistent process it would not be wrong formatting but only missing data"
      },
      {
        "date": "2024-02-16T20:50:00.000Z",
        "voteCount": 1,
        "content": "I think A is the closest matching answer.\nIt's been unable to find the file, it can be a valid reason that someone has changed the folder, therefore the path is not found.\nIf it's a payload or data-related matter, I would expect a different error rather than not found."
      },
      {
        "date": "2024-02-11T04:02:00.000Z",
        "voteCount": 1,
        "content": "The process is scheduled to process the file on a daily basis every day at 7am. The last try at 7:45 return the error - the file is missing. Therefore, the data is missing data from 06:00. And this is the normal schedule. For 7 days, this job was succeeding."
      },
      {
        "date": "2024-02-07T09:07:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2023-08-29T23:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-24T04:03:00.000Z",
        "voteCount": 1,
        "content": "A. The parameter used to generate year=2021/month=01/day=10/hour=06 was incorrect.\n\nThe error message indicates that the specified path 'BIKES/CARBON/year=2021/month=01/day=10/hour=06' does not exist. This suggests that the parameters used to generate the path might be incorrect, resulting in an invalid or non-existent path. Double-checking the parameter values used to construct the path would be the most likely reason for this error."
      },
      {
        "date": "2023-08-19T11:09:00.000Z",
        "voteCount": 2,
        "content": "This option matches with the parameter 6th hour of Jan 10th . Option A might or might not be true but we have no data about what hour of data to be retrieved. Whereas, Option B clearly points to the time mentioned in the question and Path not found could be due to no data prsent"
      },
      {
        "date": "2023-03-07T06:03:00.000Z",
        "voteCount": 3,
        "content": "The error message states that the specified path does not exist. Therefore, a possible cause of the error could be that the data for the specified path, which is wwi/BIKES/CARBON/year=2021/month=01/day=10/hour=06, does not exist in the storage account. This could be due to missing data or incorrect path or container name. Option B is the most likely cause of the error as it suggests that there was no data in the specified path during the given time frame."
      },
      {
        "date": "2023-01-01T17:32:00.000Z",
        "voteCount": 2,
        "content": "Provided answer is correct : PATH NOT FOUND  . The right path must be BIKES/CARBON/2021/01/06/Filename.*\nFilesystem: wwi. Path: 'BIKES/CARBON/year=2021/month=01/day=10/hour=06'. ErrorCode: 'PathNotFound'. Message: 'The specified path does not exist.'."
      },
      {
        "date": "2023-01-14T14:35:00.000Z",
        "voteCount": 4,
        "content": "question says that it ran fine earlier. Parameter must have been set correctly. Answer is B"
      },
      {
        "date": "2022-11-02T11:02:00.000Z",
        "voteCount": 2,
        "content": "No file"
      },
      {
        "date": "2022-10-10T23:01:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-08-27T08:42:00.000Z",
        "voteCount": 3,
        "content": "B of course"
      },
      {
        "date": "2022-08-27T08:36:00.000Z",
        "voteCount": 5,
        "content": "The question states the pipeline runs hourly and in the timestamp of the error we can see that the time is 7:45 for the third run. So the initial run was at 7:00, but the folder it was looking at is hour=06, which is wrong, it should be hour = 07. So I agree with the option A"
      },
      {
        "date": "2022-09-06T21:39:00.000Z",
        "voteCount": 9,
        "content": "With run starting at 7.00 pointing to the hour=07 folder, you wouldn't have anything to work with. One hour delay needed here."
      },
      {
        "date": "2022-08-07T03:12:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/microsoft/view/76623-exam-dp-203-topic-2-question-76-discussion/",
    "body": "You have an Azure Synapse Analytics job that uses Scala.<br>You need to view the status of the job.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Synapse Studio, select the workspace. From Monitor, select SQL requests.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Azure Monitor, run a Kusto query against the AzureDiagnostics table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Synapse Studio, select the workspace. From Monitor, select Apache Sparks applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Azure Monitor, run a Kusto query against the SparkLoggingEvent_CL table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-07T14:04:00.000Z",
        "voteCount": 10,
        "content": "I think this is one is correct."
      },
      {
        "date": "2023-08-29T23:53:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-06-01T21:39:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-02-09T20:40:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is C. From Synapse Studio, select the workspace. From Monitor, select Apache Spark applications."
      },
      {
        "date": "2022-09-06T21:40:00.000Z",
        "voteCount": 4,
        "content": "Correct\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/monitoring/how-to-monitor-spark-applications"
      },
      {
        "date": "2022-08-07T03:15:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/microsoft/view/75204-exam-dp-203-topic-2-question-77-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Data Lake Storage Gen2 account that contains a JSON file for customers. The file contains two attributes named FirstName and LastName.<br>You need to copy the data from the JSON file to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.<br>You create the following components:<br>\u2711 A destination table in Azure Synapse<br>\u2711 An Azure Blob storage container<br>\u2711 A service principal<br>In which order should you perform the actions? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0026400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0026400002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Mount the Data Lake Storage onto DBFS<br>Begin with creating a file system in the Azure Data Lake Storage Gen2 account.<br>Step 2: Read the file into a data frame.<br>You can load the json files as a data frame in Azure Databricks.<br>Step 3: Perform transformations on the data frame.<br>Step 4: Specify a temporary folder to stage the data<br>Specify a temporary folder to use while moving data between Azure Databricks and Azure Synapse.<br>Step 5: Write the results to a table in Azure Synapse.<br>You upload the transformed data frame into Azure Synapse. You use the Azure Synapse connector for Azure Databricks to directly upload a dataframe as a table in a Azure Synapse.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2022-05-05T07:03:00.000Z",
        "voteCount": 19,
        "content": "Similar to another question in this dump. Seems correct!"
      },
      {
        "date": "2024-04-28T08:01:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-08-29T23:54:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-10-28T02:48:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2022-08-27T08:52:00.000Z",
        "voteCount": 1,
        "content": "\"Specify a temporary folder to stage the data\" must be before creating the DF : I am wrong ?"
      },
      {
        "date": "2023-01-09T22:45:00.000Z",
        "voteCount": 3,
        "content": "As mentioned earlier, the Azure Synapse connector uses Azure Blob storage as temporary storage to upload data between Azure Databricks and Azure Synapse\nso it means only before you loading data into ADLS, you need this temporary folder.\nhttps://learn.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse"
      },
      {
        "date": "2022-08-07T03:21:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-07T14:01:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-31T05:26:00.000Z",
        "voteCount": 3,
        "content": "answer is correct. Similar to a duplicated question in this question catalog."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80665-exam-dp-203-topic-2-question-78-discussion/",
    "body": "You have an Azure data factory named ADF1.<br>You currently publish all pipeline authoring changes directly to ADF1.<br>You need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the UX Authoring canvas for ADF1.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the UX Authoring canvas, select Set up code repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Git repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GitHub action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Data Factory trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the UX Authoring canvas, select Publish.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the UX Authoring canvas, run Publish All."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "AF",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-10T07:23:00.000Z",
        "voteCount": 18,
        "content": "They are asking to \"implement version control\".\nB Create Git repo\nA From the UX Set up code repository"
      },
      {
        "date": "2023-03-08T04:49:00.000Z",
        "voteCount": 7,
        "content": "To implement version control for changes made to pipeline artifacts in ADF1 while ensuring that version control can be applied to the resources currently defined in the UX Authoring canvas, you should perform the following two actions:\n\nA. From the UX Authoring canvas, select Set up code repository: This will allow you to configure ADF1 to integrate with a version control system such as Git, which will enable you to track changes made to pipeline artifacts over time.\n\nB. Create a Git repository: This will provide the version control system needed to track changes made to pipeline artifacts in ADF1.\n\nTherefore, options A and B are the correct answers.\n\nC, D, E, and F are not relevant to implementing version control for changes made to pipeline artifacts in ADF1."
      },
      {
        "date": "2024-09-20T05:23:00.000Z",
        "voteCount": 1,
        "content": "B - Repo must be created before setting it up\nA - Setup the repo in the Azure service\n(implementation ends here)"
      },
      {
        "date": "2024-05-27T10:13:00.000Z",
        "voteCount": 1,
        "content": "(B)create git repo and conect to it from ADF (A)"
      },
      {
        "date": "2023-08-30T00:01:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-06-01T21:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-05-10T07:00:00.000Z",
        "voteCount": 3,
        "content": "I think the answer should be AF as \"Set up code repository\" gives us the option of creating new repository if not already created so option B is redundant. More over we should not individually publish existing artifacts rather should go for \"Publish All\"."
      },
      {
        "date": "2023-05-09T22:09:00.000Z",
        "voteCount": 1,
        "content": "\"ensuring that version control can be applied to the resources currently defined in the UX Authoring canvas\"\nWhen creating a Git repository this option is ticked by default, so all available resources at the time of the creation are imported into Git, no need to publish which is what the last answers are trying to imply.\nImport existing resources to repository\tSpecifies whether to import existing data factory resources from the UX authoring canvas into a GitHub repository. Select the box to import your data factory resources into the associated Git repository in JSON format. This action exports each resource individually (that is, the linked services and datasets are exported into separate JSONs). When this box isn't selected, the existing resources aren't imported.\tSelected (default)"
      },
      {
        "date": "2023-02-09T20:43:00.000Z",
        "voteCount": 1,
        "content": "The correct answers are B. Create a Git repository and A. From the UX Authoring canvas, select Set up code repository."
      },
      {
        "date": "2023-01-14T17:45:00.000Z",
        "voteCount": 2,
        "content": "option A is correct because it allows you to set up a code repository to store and manage the changes made to pipeline artifacts in ADF1.\nOption B is correct because it allows you to create a Git repository, which is a version control system that stores the history of changes made to the pipeline artifacts. This allows you to easily roll back to a previous version or compare changes made over time."
      },
      {
        "date": "2022-11-29T10:10:00.000Z",
        "voteCount": 2,
        "content": "Since there is no mention of GitHub or DevOps the solution that works for both is A &amp; F"
      },
      {
        "date": "2022-11-02T11:34:00.000Z",
        "voteCount": 4,
        "content": "I did a setup of the version control for my test ADF instance in the following way:\n\nA. From the UX Authoring canvas, select Set up code repository.\nHere I configured a connection to the Azure DevOps organization, chose a project, and created a new repo.\n\nE. From the UX Authoring canvas, select Publish."
      },
      {
        "date": "2022-10-13T05:50:00.000Z",
        "voteCount": 3,
        "content": "A &amp; B: The documentation attached to this question states the first step is to set up a code repository from the UX and this question is around setting up version control, not saving your changes which is what F suggests"
      },
      {
        "date": "2022-09-14T01:00:00.000Z",
        "voteCount": 3,
        "content": "F A is the correct order. Save changes then set up code repos"
      },
      {
        "date": "2022-09-06T14:18:00.000Z",
        "voteCount": 5,
        "content": "Should be AF as we want to achieve the version control for the code changes."
      },
      {
        "date": "2022-09-06T09:41:00.000Z",
        "voteCount": 3,
        "content": "why not A and B ? I would set up the code repository after creating the git repo"
      },
      {
        "date": "2023-03-02T01:54:00.000Z",
        "voteCount": 1,
        "content": "in my opinion, A &amp; B resulted the same - repo created"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80719-exam-dp-203-topic-2-question-79-discussion/",
    "body": "DRAG DROP -<br>You have an Azure subscription that contains an Azure Synapse Analytics workspace named workspace1. Workspace1 connects to an Azure DevOps repository named repo1. Repo1 contains a collaboration branch named main and a development branch named branch1. Branch1 contains an Azure Synapse pipeline named pipeline1.<br>In workspace1, you complete testing of pipeline1.<br>You need to schedule pipeline1 to run daily at 6 AM.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0026600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0026700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-02-06T20:19:00.000Z",
        "voteCount": 47,
        "content": "If it's hard to remember, memorize it as CAMP."
      },
      {
        "date": "2023-05-09T22:15:00.000Z",
        "voteCount": 8,
        "content": "Correct. I've worked with this many times. It's the right order."
      },
      {
        "date": "2024-04-20T05:28:00.000Z",
        "voteCount": 1,
        "content": "Oh yes!"
      },
      {
        "date": "2023-08-30T00:03:00.000Z",
        "voteCount": 4,
        "content": "C-&gt;A-&gt;M-&gt;P"
      },
      {
        "date": "2023-01-09T23:06:00.000Z",
        "voteCount": 4,
        "content": "you should associate the trigger before merge the code into main, because schedule also is part of code. all code store in main, do not change it directly, that is the purpose of version control."
      },
      {
        "date": "2023-01-05T07:57:00.000Z",
        "voteCount": 5,
        "content": "why not this?\n1.merge th echanges from branch1 into main\n2.publish the contents of main\n3.create a schedule trigger\n4.associate the schedule trigger with pipeline1"
      },
      {
        "date": "2023-01-10T01:57:00.000Z",
        "voteCount": 4,
        "content": "sorry, I noticed that the question claims:\nNOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select."
      },
      {
        "date": "2023-02-13T11:14:00.000Z",
        "voteCount": 3,
        "content": "This order is also possible, but not recommended. As the trigger would not be visible in the repo, which can be misleading to anyone that is reviewing or auditing the solution."
      },
      {
        "date": "2022-11-02T11:47:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-10-31T10:33:00.000Z",
        "voteCount": 3,
        "content": "Should it not be merge, then publish, then create a schedule trigger, finally associate the schedule trigger with pipeline1?"
      },
      {
        "date": "2022-10-31T10:37:00.000Z",
        "voteCount": 2,
        "content": "Pls ignore, I agree with the suggested answer"
      },
      {
        "date": "2022-10-28T02:56:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-09-06T14:23:00.000Z",
        "voteCount": 2,
        "content": "Looks correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79639-exam-dp-203-topic-2-question-80-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage account named storage1. Storage1 requires secure transfers.<br>You need to create an external data source in Pool1 that will be used to read .orc files in storage1.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0026800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0026900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-02T19:25:00.000Z",
        "voteCount": 38,
        "content": "Answer: abfss and Hadoop\nHint: Storage1 requires secure transfers --&gt;  The default option is to use enable secure SSL connections when provisioning Azure Data Lake Storage Gen2. When this is enabled, you must use abfss when a secure TLS/SSL connection is selected.\n\nReference: https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated"
      },
      {
        "date": "2022-12-18T04:36:00.000Z",
        "voteCount": 7,
        "content": "abfss and Hadoop"
      },
      {
        "date": "2023-08-30T00:03:00.000Z",
        "voteCount": 2,
        "content": "Answer: abfss and Hadoop"
      },
      {
        "date": "2023-01-12T05:20:00.000Z",
        "voteCount": 5,
        "content": "abfss &amp; hadoop"
      },
      {
        "date": "2022-10-13T18:39:00.000Z",
        "voteCount": 6,
        "content": "abfss\nHadoop\nabfss endpoint when your account has secure transfer enabled"
      },
      {
        "date": "2022-09-22T10:55:00.000Z",
        "voteCount": 4,
        "content": "abfss\nHadoop"
      },
      {
        "date": "2022-09-06T14:26:00.000Z",
        "voteCount": 6,
        "content": "-abfss\n-hadoop"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/microsoft/view/78654-exam-dp-203-topic-2-question-81-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named SQLPool1.<br>SQLPool1 is currently paused.<br>You need to restore the current state of SQLPool1 to a new SQL pool.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a user-defined restore point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResume SQLPool1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new SQL pool."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-28T15:38:00.000Z",
        "voteCount": 18,
        "content": "You wont be able to create restore point when the SQL pool is paused. The the correct answer is Result SQL Pool. See below from Microsoft documentation.\n\nUser-defined restore points can also be created through Azure portal.\n\nSign in to your Azure portal account.\n\nNavigate to the dedicated SQL pool (formerly SQL DW) that you want to create a restore point for.\n\nSelect Overview from the left pane, select + New Restore Point. If the New Restore Point button isn't enabled, make sure that the dedicated SQL pool (formerly SQL DW) isn't paused.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-restore-points"
      },
      {
        "date": "2023-03-08T06:30:00.000Z",
        "voteCount": 6,
        "content": "Before restoring the state of SQLPool1 to a new SQL pool, you should resume SQLPool1. Therefore, the correct answer is:\n\nC. Resume SQLPool1."
      },
      {
        "date": "2024-01-18T19:22:00.000Z",
        "voteCount": 1,
        "content": "Dedicated SQL pool should be in an active state for restore point creation.\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/synapse-analytics/dedicated-sql/dsql-maint-backup-restore-guidance?source=recommendations"
      },
      {
        "date": "2023-12-11T07:41:00.000Z",
        "voteCount": 1,
        "content": "Correcto"
      },
      {
        "date": "2023-09-21T21:23:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C.\nNot restore points are created when pool is paused. And there default retention is 7 days.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore#automatic-restore-points"
      },
      {
        "date": "2023-08-30T00:04:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-06-23T01:40:00.000Z",
        "voteCount": 2,
        "content": "Resume."
      },
      {
        "date": "2023-03-03T08:36:00.000Z",
        "voteCount": 3,
        "content": "you cannot create user-defined restore points when the Azure Synapse Analytics dedicated SQL pool is currently paused. In order to create a user-defined restore point, the SQL pool must be running."
      },
      {
        "date": "2022-12-17T08:01:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-09-10T07:36:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2022-08-31T02:55:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/microsoft/view/83033-exam-dp-203-topic-2-question-82-discussion/",
    "body": "You are designing an Azure Synapse Analytics workspace.<br>You need to recommend a solution to provide double encryption of all the data at rest.<br>Which two components should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan X.509 certificate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan RSA key\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure virtual network that has a network security group (NSG)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Policy initiative",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure key vault that has purge protection enabled\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-28T02:30:00.000Z",
        "voteCount": 7,
        "content": "Answer is correct : BE"
      },
      {
        "date": "2024-02-13T13:58:00.000Z",
        "voteCount": 1,
        "content": "To provide double encryption of all the data at rest in an Azure Synapse Analytics workspace, you should include the following components in the recommendation:\n\nA. An X.509 certificate: X.509 certificates are commonly used for securing communication channels and can be used for encrypting data at rest in conjunction with other encryption mechanisms.\n\nB. An RSA key: RSA keys are widely used for encryption purposes, including encrypting data at rest. By using an RSA key, you can encrypt the data before storing it, providing an additional layer of security."
      },
      {
        "date": "2024-02-21T15:20:00.000Z",
        "voteCount": 1,
        "content": "the RSA key already does the double encryption, so you need a place to safeguard the key, which is the azure key vault that has purge protection enabled"
      },
      {
        "date": "2024-01-11T06:14:00.000Z",
        "voteCount": 3,
        "content": "I got this question today in exam 11-jan-2024"
      },
      {
        "date": "2023-12-16T03:08:00.000Z",
        "voteCount": 2,
        "content": "BE is correct \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption#workspace-encryption-configuration"
      },
      {
        "date": "2023-09-14T05:14:00.000Z",
        "voteCount": 1,
        "content": "BE is correct to provide double encryption at REST. RSA key and Azure key vault."
      },
      {
        "date": "2023-09-08T20:35:00.000Z",
        "voteCount": 1,
        "content": "BE is correct"
      },
      {
        "date": "2023-07-28T04:28:00.000Z",
        "voteCount": 1,
        "content": "Answer: BE\nExplanation:\nSynapse workspaces encryption uses existing keys or new keys generated in Azure Key Vault. A single key is\nused to encrypt all the data in a workspace.\nSynapse workspaces support RSA 2048 and 3072 byte-sized keys, and RSA-HSM keys.\nThe Key Vault itself needs to have purge protection enabled.\nReference:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption"
      },
      {
        "date": "2023-06-27T13:46:00.000Z",
        "voteCount": 1,
        "content": "A. Including an X.509 certificate in the solution can be used to provide encryption at rest for the data in Azure Synapse Analytics. X.509 certificates are widely used for securing data and communications.\n\nE. An Azure Key Vault with purge protection enabled can be utilized to securely store and manage encryption keys. By storing the encryption keys in Azure Key Vault, you can ensure that the keys are well protected and access to them is tightly controlled."
      },
      {
        "date": "2023-06-23T01:57:00.000Z",
        "voteCount": 1,
        "content": "Correct."
      },
      {
        "date": "2022-11-02T15:21:00.000Z",
        "voteCount": 4,
        "content": "Agree with the answer"
      },
      {
        "date": "2022-09-21T00:19:00.000Z",
        "voteCount": 3,
        "content": "Correct ans."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80722-exam-dp-203-topic-2-question-83-discussion/",
    "body": "You have an Azure Synapse Analytics serverless SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named storage1. The<br>AllowBlobPublicAccess property is disabled for storage1.<br>You need to create an external data source that can be used by Azure Active Directory (Azure AD) users to access storage from Pool1.<br>What should you create first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan external resource pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan external library",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdatabase scoped credentials\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta remote service binding"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-06T14:30:00.000Z",
        "voteCount": 5,
        "content": "Correct Answer !"
      },
      {
        "date": "2023-08-30T00:04:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-06-01T21:59:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-01-31T01:59:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      },
      {
        "date": "2023-01-31T01:59:00.000Z",
        "voteCount": 3,
        "content": "database scoped credentials first"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79113-exam-dp-203-topic-2-question-84-discussion/",
    "body": "You have an Azure Data Factory pipeline named Pipeline1. Pipeline1 contains a copy activity that sends data to an Azure Data Lake Storage Gen2 account.<br>Pipeline1 is executed by a schedule trigger.<br>You change the copy activity sink to a new storage account and merge the changes into the collaboration branch.<br>After Pipeline1 executes, you discover that data is NOT copied to the new storage account.<br>You need to ensure that the data is copied to the new storage account.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish from the collaboration branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the schedule trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the change feed of the new storage account."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-07T04:23:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-07-03T03:26:00.000Z",
        "voteCount": 1,
        "content": "The \"schedule trigger\" always runs in live mode, hence it will take the storage account name which is available in the live mode. Now on the other hand, when the changes in storage account name is done and merged to collaboration branch(by raising a PULL REQUEST) , it is still not yet available in ADF live mode which we need to do manually by publishing from the collaboration branch."
      },
      {
        "date": "2024-07-03T03:35:00.000Z",
        "voteCount": 1,
        "content": "So basically the data did not get copied to new storage account because in live mode the trigger still holds the old storage account name and most certainly the data got copied to the old storage account when the scheduled trigger ran. So we need to publish \"from\" Collaboration branch \"to\" ADF live mode so that the updated storage account name gets available in the ADF live mode for the scheduled trigger to pick the NEW storage account name as parameter when it triggers next time."
      },
      {
        "date": "2024-03-07T11:14:00.000Z",
        "voteCount": 1,
        "content": "I don't get it. It is saying: \n\"After Pipeline1 executes, you discover that data is NOT copied to the new storage account.\nYou need to ensure that the data is copied to the new storage account.\"\nSo this means, the pipeline does not work. How on earth would it make sense to publish a pipeline which is not working?"
      },
      {
        "date": "2024-03-10T03:14:00.000Z",
        "voteCount": 4,
        "content": "because you merged the pipeline to the collaboration branch, but since you didn't publish the changes, the pipeline didn't copy the data to the new storage account. Think of publish as deploying to live"
      },
      {
        "date": "2023-08-30T00:08:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-06-19T06:50:00.000Z",
        "voteCount": 2,
        "content": "Correct answer."
      },
      {
        "date": "2023-05-11T10:55:00.000Z",
        "voteCount": 1,
        "content": "I selected B, pull request"
      },
      {
        "date": "2023-06-23T03:10:00.000Z",
        "voteCount": 3,
        "content": "I guessed the same, but in adf publish step is manually and not automatic, so when the question says code is merged, you can automatically assume that pull request was done and also approved by reviewers. Therefore the correct answer for me it's A."
      },
      {
        "date": "2022-10-14T15:27:00.000Z",
        "voteCount": 2,
        "content": "I had heard \"publish to\", never heard of \"publish from\". So confused."
      },
      {
        "date": "2022-11-25T16:03:00.000Z",
        "voteCount": 2,
        "content": "probably it was meant to publish from collaboration branch to adf_publish branch"
      },
      {
        "date": "2022-09-01T11:32:00.000Z",
        "voteCount": 1,
        "content": "Why not B ?"
      },
      {
        "date": "2022-09-02T08:29:00.000Z",
        "voteCount": 12,
        "content": "Because the pull request is already implicit in the statement as it is said to be merged into the collaborating branch:\n\"You change the copy activity sink to a new storage account and MERGE the CHANGES INTO the COLLABORATION BRANCH.\""
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81557-exam-dp-203-topic-2-question-85-discussion/",
    "body": "You have an Azure Data Factory pipeline named pipeline1 that is invoked by a tumbling window trigger named Trigger1. Trigger1 has a recurrence of 60 minutes.<br>You need to ensure that pipeline1 will execute only if the previous execution completes successfully.<br>How should you configure the self-dependency for Trigger1?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toffset: \"-00:01:00\" size: \"00:01:00\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toffset: \"01:00:00\" size: \"-01:00:00\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toffset: \"01:00:00\" size: \"01:00:00\"",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\toffset: \"-01:00:00\" size: \"01:00:00\"\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-15T23:54:00.000Z",
        "voteCount": 7,
        "content": "Correct Answer: D \n Offset of \"-01:00:00\" indicates to start the next trigger instance only after the previous trigger instance completes, and size of \"01:00:00\" indicates to wait for 1 hour after the previous trigger instance completes before starting the next one."
      },
      {
        "date": "2022-09-10T08:39:00.000Z",
        "voteCount": 5,
        "content": "\"dependsOn\": [\n                {\n                    \"type\": \"SelfDependencyTumblingWindowTriggerReference\",\n                    \"size\": \"01:00:00\",\n                    \"offset\": \"-01:00:00\"\n                }"
      },
      {
        "date": "2023-08-30T00:12:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-06-15T15:57:00.000Z",
        "voteCount": 4,
        "content": "Offset: \"-01:00:00\"\nSize: \"01:00:00\"\n\nThis configuration ensures that the trigger waits for the completion of the previous execution before starting a new one. The offset of \"-01:00:00\" indicates that the trigger should start one hour before the current time, and the size of \"01:00:00\" indicates that the trigger should have a duration of one hour.\n\nTherefore, the correct option is:\n\nD. offset: \"-01:00:00\" size: \"01:00:00\""
      },
      {
        "date": "2022-12-31T06:15:00.000Z",
        "voteCount": 5,
        "content": "Answer: D\noffset\t\nOffset of the dependency trigger. Provide a value in time span format and both negative and positive offsets are allowed. This property is mandatory if the trigger is depending on itself and in all other cases it is optional. Self-dependency should always be a negative offset. If no value specified, the window is the same as the trigger itself.\n\nsize\t\nSize of the dependency tumbling window. Provide a positive timespan value. This property is optional.\nhttps://learn.microsoft.com/en-us/azure/data-factory/tumbling-window-trigger-dependency"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80744-exam-dp-203-topic-2-question-86-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics pipeline named Pipeline1 that contains a data flow activity named Dataflow1.<br>Pipeline1 retrieves files from an Azure Data Lake Storage Gen 2 account named storage1.<br>Dataflow1 uses the AutoResolveIntegrationRuntime integration runtime configured with a core count of 128.<br>You need to optimize the number of cores used by Dataflow1 to accommodate the size of the files in storage1.<br>What should you configure? To answer, select the appropriate options in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0027400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0027400002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: A Get Metadata activity -<br>Dynamically size data flow compute at runtime<br>The Core Count and Compute Type properties can be set dynamically to adjust to the size of your incoming source data at runtime. Use pipeline activities like<br>Lookup or Get Metadata in order to find the size of the source dataset data. Then, use Add Dynamic Content in the Data Flow activity properties.<br><br>Box 2: Dynamic content -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/control-flow-execute-data-flow-activity",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-10T08:53:00.000Z",
        "voteCount": 14,
        "content": "Correct :\nUse pipeline activities like Lookup or Get Metadata in order to find the size of the source dataset data. Then, use Add Dynamic Content in the Data Flow activity properties. You can choose small, medium, or large compute sizes. Optionally, pick \"Custom\" and configure the compute types and number of cores manually."
      },
      {
        "date": "2022-11-03T13:59:00.000Z",
        "voteCount": 6,
        "content": "Looks correct. Checked in the doc."
      },
      {
        "date": "2024-07-25T18:29:00.000Z",
        "voteCount": 2,
        "content": "Copilot\nSent by Copilot:\nTo optimize the number of cores used by Dataflow1 in your Azure Synapse Analytics pipeline, you should configure the core count dynamically based on the size of the incoming source data. Here are the steps to achieve this:\n\nUse a Lookup or Get Metadata activity in your pipeline to determine the size of the source dataset in storage1.\nAdd Dynamic Content to the Data Flow activity properties to set the core count based on the size of the data"
      },
      {
        "date": "2024-02-09T07:43:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-08-30T00:18:00.000Z",
        "voteCount": 1,
        "content": "Get Metadata &amp; Dynamic Content"
      },
      {
        "date": "2022-09-06T17:11:00.000Z",
        "voteCount": 3,
        "content": "Looks Correct !!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52912-exam-dp-203-topic-2-question-87-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL.<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-30T03:05:00.000Z",
        "voteCount": 91,
        "content": "- data engineers: high concurrency cluster\n- jobs: Standard cluster\n- data scientists: Standard cluster"
      },
      {
        "date": "2022-04-17T06:05:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2021-09-17T07:40:00.000Z",
        "voteCount": 1,
        "content": "Tell me one thing: is this answer 9jobs) based on the text:\n\"A Single Node cluster has no workers and runs Spark jobs on the driver node.\n\nIn contrast, a Standard cluster requires at least one Spark worker node in addition to the driver node to execute Spark jobs.\"?\nI dont understand the connection between worker noodes and the requirements given in the question about jobs workspace."
      },
      {
        "date": "2022-06-07T02:45:00.000Z",
        "voteCount": 1,
        "content": "single node cluster and standard cluster are different. In single node cluster you only have 1 node which act as driver and worker node while in standard cluster you can have separate driver and worker node and for jobs you can use standard or high concurrency cluster as well. So the requirements are satisfied here"
      },
      {
        "date": "2022-09-19T01:11:00.000Z",
        "voteCount": 2,
        "content": "Correct. Because jobs could be for Scala notebook, which is supported by Standard cluster mode"
      },
      {
        "date": "2023-05-05T02:05:00.000Z",
        "voteCount": 1,
        "content": "The issue is the jobs are going to be ran by multiple users i.e. engineers and scientists? So it needs to be hugi concurrency cluster?"
      },
      {
        "date": "2023-06-23T03:18:00.000Z",
        "voteCount": 2,
        "content": "If you enable high concurrency then all scale scripts doesn't works, so scientists will stop to work). Standard cluster is scalable, will support all jobs and users! ;-)"
      },
      {
        "date": "2021-08-30T17:29:00.000Z",
        "voteCount": 34,
        "content": "The answer must be A!"
      },
      {
        "date": "2024-07-16T12:03:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-01-01T01:17:00.000Z",
        "voteCount": 2,
        "content": "we need HC cluster for data engineer,data scientist,jobs"
      },
      {
        "date": "2023-08-30T00:26:00.000Z",
        "voteCount": 1,
        "content": "correct\n- data engineers: high concurrency cluster\n- jobs: Standard cluster\n- data scientists: Standard cluster"
      },
      {
        "date": "2023-06-23T03:27:00.000Z",
        "voteCount": 4,
        "content": "High concurrence doesn't support scala."
      },
      {
        "date": "2023-06-23T02:24:00.000Z",
        "voteCount": 1,
        "content": "True, correct."
      },
      {
        "date": "2023-03-03T08:48:00.000Z",
        "voteCount": 5,
        "content": "SCALA = STANDARD"
      },
      {
        "date": "2022-10-28T22:10:00.000Z",
        "voteCount": 3,
        "content": "data scientists and Job --&gt; Scala --&gt; Standard cluster ."
      },
      {
        "date": "2022-10-13T18:50:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-09-06T17:16:00.000Z",
        "voteCount": 1,
        "content": "We would need a Standard cluster for the jobs to support Scala. High-concurrecny cluster does not support Scala.\nHence, the Answer is A !"
      },
      {
        "date": "2022-07-30T00:20:00.000Z",
        "voteCount": 1,
        "content": "the answer should be No"
      },
      {
        "date": "2022-08-14T22:24:00.000Z",
        "voteCount": 3,
        "content": "sorry 'A' should be correct"
      },
      {
        "date": "2022-06-27T10:01:00.000Z",
        "voteCount": 2,
        "content": "The answer should be \"NO\" as per the given statement \"The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.\" since the Job cluster is standard it will not allow data scientists and engineers to collectively deploy their Notebooks in standard cluster as it requires High Concurrency Cluster"
      },
      {
        "date": "2023-09-13T23:37:00.000Z",
        "voteCount": 2,
        "content": "High Concurrency Cluster does not support Scala."
      },
      {
        "date": "2024-02-20T00:22:00.000Z",
        "voteCount": 2,
        "content": "The Shared access mode clusters aka the former High Concurrency cluster do now support Scala , they do not support R (https://learn.microsoft.com/en-us/azure/databricks/archive/compute/cluster-ui-preview)"
      },
      {
        "date": "2024-02-21T15:57:00.000Z",
        "voteCount": 1,
        "content": "Exactly! before, the cluster should've been Standard, because it wasn't able to support Scala, but now that it can, the best cluster is High concurrency from all the people executing jobs there."
      },
      {
        "date": "2022-03-26T21:00:00.000Z",
        "voteCount": 4,
        "content": "Standard clusters do not have fault tolerance. Both the data scientist and data engineers will be using the job cluster for processing their notebooks, so if a standard cluster is chosen and a fault occurs in the notebook of any one user, there is a chance that other notebooks might also fail. Due to this a high concurrency cluster is recommended for running jobs."
      },
      {
        "date": "2022-05-10T08:04:00.000Z",
        "voteCount": 1,
        "content": "It may not be a best practice, but the question asked is: does the solution meet the stated requirements, and it does.."
      },
      {
        "date": "2022-06-11T02:23:00.000Z",
        "voteCount": 2,
        "content": "Read the question properly. it states that each data scientist will have a standard cluster and a separate standard cluster for running jobs. So there is no question of fault due to other users. The answer is A"
      },
      {
        "date": "2022-03-10T13:02:00.000Z",
        "voteCount": 6,
        "content": "As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --&gt; Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --&gt; Standard for Data Scientists &amp; High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --&gt; Jobs needs Standard"
      },
      {
        "date": "2022-02-23T17:20:00.000Z",
        "voteCount": 2,
        "content": "Yes it seems to be!"
      },
      {
        "date": "2022-01-28T02:54:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53073-exam-dp-203-topic-2-question-88-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL.<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL.<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R.<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a High Concurrency cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-18T22:14:00.000Z",
        "voteCount": 46,
        "content": "High-concurrency clusters do not support Scala. So the answer is still 'No' but the reasoning is wrong. \nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-06-08T23:46:00.000Z",
        "voteCount": 3,
        "content": "I agree that High concurrency does not support Scala. But they specified using a Standard cluster for the jobs, which does support Scala. Why is the answer 'No'?"
      },
      {
        "date": "2021-06-17T22:05:00.000Z",
        "voteCount": 6,
        "content": "Because the High Concurrency cluster for each data scientist is not correct, it should be standard for a single user!"
      },
      {
        "date": "2024-02-21T15:59:00.000Z",
        "voteCount": 1,
        "content": "High concurrency supports scala now, so the answer should be A)"
      },
      {
        "date": "2021-06-17T01:40:00.000Z",
        "voteCount": 15,
        "content": "Answer should be NO, which\nData scientist: STANDARD as need to run scala\nJobs:  STANDARD as need to run scala\nData Engineers:  High-concurrency clusters as better resource sharing"
      },
      {
        "date": "2024-06-18T15:27:00.000Z",
        "voteCount": 4,
        "content": "Down the line 3 years back No must have been correct answer but now its not.\nScala is supported at High Concurrency cluster only thing it doesn't benefit with the concurrency due the JVM thingy.\nSo ideally speaking...\nData Engineers : High Concurrency - due to the fact of sharing \nData Scientist : Standard - because its better to leave them in their space\nJob Cluster : Standard is best, but High Concurrency is also good.\nSo answer should be Yes, because here they don't talk about cost, performance etc."
      },
      {
        "date": "2024-02-11T04:58:00.000Z",
        "voteCount": 1,
        "content": "There are potential cost and resource utilization concerns with the separate High Concurrency cluster for jobs."
      },
      {
        "date": "2023-10-22T18:56:00.000Z",
        "voteCount": 2,
        "content": "High concurrency doesn't support Scala"
      },
      {
        "date": "2023-08-30T00:28:00.000Z",
        "voteCount": 1,
        "content": "Answer is No."
      },
      {
        "date": "2023-06-23T03:30:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: false (no)"
      },
      {
        "date": "2022-11-30T03:02:00.000Z",
        "voteCount": 1,
        "content": "High-concurrency cluster does not support Scala."
      },
      {
        "date": "2022-11-14T09:32:00.000Z",
        "voteCount": 1,
        "content": "Jobs require Scala so the answer is B) No."
      },
      {
        "date": "2022-10-13T18:54:00.000Z",
        "voteCount": 1,
        "content": "Cluster for Jobs should support scala - STANDARD"
      },
      {
        "date": "2022-09-06T17:17:00.000Z",
        "voteCount": 1,
        "content": "We would need a Standard cluster for the jobs to support Scala. High-concurrency cluster does not support Scala.\nHence, Answer is NO"
      },
      {
        "date": "2022-09-03T08:55:00.000Z",
        "voteCount": 1,
        "content": "High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2022-08-14T22:27:00.000Z",
        "voteCount": 1,
        "content": "No is correct"
      },
      {
        "date": "2022-05-17T09:40:00.000Z",
        "voteCount": 1,
        "content": "High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala."
      },
      {
        "date": "2022-03-31T20:04:00.000Z",
        "voteCount": 2,
        "content": "1 and 2 are same questions but answers are different why?"
      },
      {
        "date": "2022-03-10T13:02:00.000Z",
        "voteCount": 2,
        "content": "As per Link: https://docs.azuredatabricks.net/clusters/configure.html\nStandard and Single Node clusters terminate automatically after 120 minutes by default. --&gt; Data Scientists\nHigh Concurrency clusters do not terminate automatically by default.\nA Standard cluster is recommended for a single user. --&gt; Standard for Data Scientists &amp; High Concurrency for Data Engineers\nStandard clusters can run workloads developed in any language: Python, SQL, R, and Scala.\nHigh Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala. --&gt; Jobs needs Standard"
      },
      {
        "date": "2022-01-03T05:51:00.000Z",
        "voteCount": 2,
        "content": "high concurrency does not support scala"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80747-exam-dp-203-topic-2-question-89-discussion/",
    "body": "You are designing a folder structure for the files in an Azure Data Lake Storage Gen2 account. The account has one container that contains three years of data.<br>You need to recommend a folder structure that meets the following requirements:<br>\u2711 Supports partition elimination for queries by Azure Synapse Analytics serverless SQL pools<br>\u2711 Supports fast data retrieval for data from the current month<br>\u2711 Simplifies data security management by department<br>Which folder structure should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\Department\\DataSource\\YYYY\\MM\\DataFile_YYYYMMDD.parquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\DataSource\\Department\\YYYYMM\\DataFile_YYYYMMDD.parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\DD\\MM\\YYYY\\Department\\DataSource\\DataFile_DDMMYY.parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\YYYY\\MM\\DD\\Department\\DataSource\\DataFile_YYYYMMDD.parquet"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-06T17:19:00.000Z",
        "voteCount": 9,
        "content": "Answer is Correct !"
      },
      {
        "date": "2022-11-03T14:08:00.000Z",
        "voteCount": 5,
        "content": "Of course A"
      },
      {
        "date": "2024-04-25T02:08:00.000Z",
        "voteCount": 1,
        "content": "Partition elimination &gt; date is the main here so it is D"
      },
      {
        "date": "2023-09-08T20:41:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-06-23T04:07:00.000Z",
        "voteCount": 3,
        "content": "The raw zone may be organised by source system, then entity. Here is an example folder structure, optimal for folder security:\n\\Raw\\DataSource\\Entity\\YYYY\\MM\\DD\\File.extension\nTypically each source system will be granted write permissions at the DataSource folder level with default ACLs (see section on ACLs below) specified. This will ensure permissions are inherited as new daily folders and files are created. \nWhilst many use time based partitioning there are a number of options which may provide more efficient access paths. -&gt;this justify the YYYYMM to get easily the current month. Correct answer for me is B."
      },
      {
        "date": "2023-12-06T14:33:00.000Z",
        "voteCount": 1,
        "content": "Except is contradicts the requirement \"Simplifies data security management by department\".  You need minimum (#DataSource) * (#Department) ACLs with option B\nWhereas with option A, you need just minimum #Department ACLs.\nAlso, Option A allows you to have additional datasource specific ACLs within a particular department, if that is so desired."
      },
      {
        "date": "2022-10-28T03:23:00.000Z",
        "voteCount": 4,
        "content": "A is right"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80750-exam-dp-203-topic-2-question-90-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 receives new data once every 24 hours.<br>You have the following function.<br><img src=\"/assets/media/exam-media/04259/0027800001.jpg\" class=\"in-exam-image\"><br>You have the following query.<br><img src=\"/assets/media/exam-media/04259/0027800002.jpg\" class=\"in-exam-image\"><br>The query is executed once every 15 minutes and the @parameter value is set to the current date.<br>You need to minimize the time it takes for the query to return results.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an index on the avg_f column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the avg_c column into a calculated column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an index on the sensorid column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable result set caching.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the table distribution to replicate."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-10T01:32:00.000Z",
        "voteCount": 14,
        "content": "B. Convert the avg_c column into a calculated column.\nD. Enable result set caching.\n\nExplanation:\nA calculated column is a column that uses an expression to calculate its value based on other columns in the same table. In this case, the udfFtoC function can be used to calculate the avg_c value based on the avg_temperature column, eliminating the need to call the UDF in the SELECT statement.\n\nEnabling result set caching can improve query performance by caching the result set of the query, so subsequent queries that use the same parameters can be retrieved from the cache instead of executing the query again.\n\nCreating an index on the avg_f column or the sensorid column is not useful because there are no join or filter conditions on these columns in the WHERE clause. Changing the table distribution to replicate is also not necessary because it does not affect the query performance in this scenario"
      },
      {
        "date": "2024-07-27T15:40:00.000Z",
        "voteCount": 1,
        "content": "No, Azure Synapse Analytics dedicated SQL pool does not support calculated (or computed) columns1. This means you cannot define columns in your table that automatically compute their values based on other columns in the same table.\n\nIf you need to include calculated values, you can handle the calculations in your ETL (Extract, Transform, Load) process before loading the data into the dedicated SQL pool. Alternatively, you can perform the calculations in your queries when retrieving data from the table.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview#unsupported-table-features"
      },
      {
        "date": "2024-07-24T03:37:00.000Z",
        "voteCount": 1,
        "content": "Copilot: (DC)\nConverting the avg_c column into a calculated column (Option B) might not be as effective for improving query performance in this scenario. Calculated columns can be useful for simplifying queries and ensuring consistent calculations, but they don\u2019t necessarily improve performance, especially if the calculation is complex or if the column is frequently queried.\nIn contrast, enabling result set caching (Option D) and creating an index on the sensorid column (Option C) directly target performance improvements by reducing query execution time and speeding up data retrieval"
      },
      {
        "date": "2024-02-20T22:26:00.000Z",
        "voteCount": 1,
        "content": "I don't think D is a solution , \"What's not cached .... Queries using user defined functions\" from https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching"
      },
      {
        "date": "2024-08-11T10:48:00.000Z",
        "voteCount": 1,
        "content": "notice that the sink table in dedicated SQL pool gets updated every 24 hours, but this query(due to some reasons) runs every 15 minutes; as a result, for 24 * 4 - 1 times, there is not new result to show. so, if the data is already in cache, db engine can access them faster."
      },
      {
        "date": "2024-01-01T01:39:00.000Z",
        "voteCount": 2,
        "content": "chatgpt \nTo minimize the time it takes for the query to return results, you should consider the following actions:\n\nA. Create an index on the avg_f column:\n\nCreating an index on the avg_f column can improve the query performance, especially if there are frequent searches or filtering based on this column.\nC. Create an index on the sensorid column:\n\nIf the sensorid column is frequently used in filtering or joins, creating an index on this column can improve the query performance."
      },
      {
        "date": "2023-09-27T02:31:00.000Z",
        "voteCount": 1,
        "content": "First the wording of the question is ridiculous. \"Query is executed once every 15 minutes\".\nSo what is it, \"Once\" or \"every 15 minutes\"?\nEither way, they are asking what to do to speed up the query.\nD Setting result caching\nE Replicated distribution"
      },
      {
        "date": "2023-08-30T00:56:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-30T00:39:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-30T00:58:00.000Z",
        "voteCount": 1,
        "content": "D &amp; E should be correct"
      },
      {
        "date": "2023-08-09T05:26:00.000Z",
        "voteCount": 3,
        "content": "Calculated columns exist in Power BI, not dedicated SQL pools. Computed columns are not supported in dedicated SQL pools.\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview"
      },
      {
        "date": "2023-04-24T10:30:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-01-18T09:09:00.000Z",
        "voteCount": 1,
        "content": "With that point by erhard being made (caching does work with queries using UDF), the most commonly voted D is wrong, so B and what now?\nReplicated cannot be right because it received date everyday and has aggregations so not a dim table and we have no clue about its size. \n by elimination that leaves us A and C\n\nIndexing is less useful with no joins but it does improve some performance being on where clause target. so I'd go with A and B."
      },
      {
        "date": "2023-01-18T09:30:00.000Z",
        "voteCount": 1,
        "content": "Creating an index on the avg_f column will improve the performance of the query, as it will allow the query to find the relevant data more quickly. Converting the avg_c column into a calculated column will allow the query to return the temperature in Celsius without the need to perform the calculation at runtime, which will also improve the performance of the query."
      },
      {
        "date": "2023-01-26T01:38:00.000Z",
        "voteCount": 1,
        "content": "After re-considering, I am unsure whether the indexing would help. That would only leave Replication as the viable option even though it is not viable design but the request is to  minimize query time and that is what it will do, so I guess final answer is\nBE"
      },
      {
        "date": "2024-02-20T22:24:00.000Z",
        "voteCount": 1,
        "content": "\"2 GB is not a hard limit. If the data is static and does not change, you can replicate larger tables.\" https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-guidance-for-replicated-tables"
      },
      {
        "date": "2022-11-21T08:46:00.000Z",
        "voteCount": 4,
        "content": "need to first chage UDF to a calculated column and then enable result set caching. agreed with the answer"
      },
      {
        "date": "2022-11-16T05:22:00.000Z",
        "voteCount": 4,
        "content": "Queries using user defined functions are not cached.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-result-set-caching"
      },
      {
        "date": "2022-11-11T08:57:00.000Z",
        "voteCount": 3,
        "content": "A,C not right since index don't help if join are not involved.\nD for sure help query performance.\n\nI don't get why B:\n\"A computed column is a virtual column whose value is calculated from other values in the table. By default, the expression\u2019s outputted value is not physically stored. Instead, SQL Server runs the expression when the column is queried and returns the value as part of the result set ... In many cases, non-persistent computed columns put too much burden on the processor, resulting in SLOWER QUERIES and unresponsive applications\"\n\nSince the only requirements is faster execution times for queries, i don't think calculated columns  will improve performance.\nSi second option for me would be D (replicate). Although it will cause more effort writing, because updates should be written to every partition, optimized writes aren't a requirement in the question."
      },
      {
        "date": "2022-10-28T03:33:00.000Z",
        "voteCount": 1,
        "content": "pool ingest data once per 24 hrs, while query happens every 15mins, caching result can definitely avoid the some duplicate calculation, I'll go with BD."
      },
      {
        "date": "2022-10-14T15:45:00.000Z",
        "voteCount": 2,
        "content": "I think should be DE. \nsince \"the query is executed once every 15 minutes and the @parameter value is set to the current date\", and the it receives new data once every 24 hours, it means the query result isn't change in one day even you run it every 15 mins. The data is static within a day. Replication could help the performance."
      },
      {
        "date": "2022-09-06T17:21:00.000Z",
        "voteCount": 4,
        "content": "Answer is Correct !"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95404-exam-dp-203-topic-2-question-91-discussion/",
    "body": "You need to design a solution that will process streaming data from an Azure Event Hub and output the data to Azure Data Lake Storage. The solution must ensure that analysts can interactively query the streaming data.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics and Azure Synapse notebooks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStructured Streaming in Azure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tevent triggers in Azure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Queue storage and read-access geo-redundant storage (RA-GRS)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-16T04:46:00.000Z",
        "voteCount": 16,
        "content": "B. Structured Streaming in Azure Databricks is the best option for this scenario as it allows for processing of streaming data and outputting it to Azure Data Lake Storage, while also providing the ability for analysts to interactively query the data using Databricks notebooks.\n\nAzure Stream Analytics and Azure Synapse notebooks (option A) can also process streaming data and output to Data Lake Storage, but they may not provide the same level of interactivity for analysts.\n\nEvent triggers in Azure Data Factory (option C) can help automate data movement between Event Hubs and Data Lake Storage, but they do not provide the necessary functionality for processing and querying streaming data.\n\nAzure Queue Storage and read-access geo-redundant storage (RA-GRS) (option D) are not relevant for this scenario as they do not provide capabilities for processing and querying streaming data."
      },
      {
        "date": "2024-07-27T15:47:00.000Z",
        "voteCount": 1,
        "content": "B. Structured Streaming in Azure Databricks\n\nHere\u2019s why:\n\nStructured Streaming in Azure Databricks:\n\nReal-time Processing: \nInteractive Querying: Databricks notebooks allow analysts to interactively query and visualize the streaming data, making it easy to gain insights in real-time.\n\nIntegration: It integrates seamlessly with Azure Event Hubs for data ingestion and Azure Data Lake Storage for data output.\n\nOther Options:\nA. Azure Stream Analytics and Azure Synapse notebooks: While Azure Stream Analytics is excellent for real-time processing and Azure Synapse notebooks for interactive querying, combining them might not be as seamless and efficient as using Databricks for both tasks."
      },
      {
        "date": "2024-02-20T22:36:00.000Z",
        "voteCount": 1,
        "content": "\"Interactive querying\" makes me think it is B"
      },
      {
        "date": "2023-12-24T14:06:00.000Z",
        "voteCount": 1,
        "content": "selected answer : B \nto visualize data in stream analytics you use SQL query in the Azure portal inside synapse , not a notebook , therefore the answer is B"
      },
      {
        "date": "2023-10-17T00:49:00.000Z",
        "voteCount": 4,
        "content": "A. Azure Stream Analytics and Azure Synapse notebooks:\nAzure Stream Analytics can be used to process the streaming data from Azure Event Hub and output the data to Azure Data Lake Storage. Azure Synapse notebooks provide interactive querying capabilities, and they can be integrated with the Azure Data Lake Storage to enable analysts to run their analytics on the stored data.\n\nB. Structured Streaming in Azure Databricks:\nStructured Streaming in Azure Databricks indeed supports streaming data and can write outputs to Azure Data Lake Storage. However, the question emphasizes \"interactively querying\" the streaming data, and while Databricks notebooks allow for interactive queries, Azure Synapse notebooks are better integrated with Microsoft's suite of data tools for broader analytics purposes."
      },
      {
        "date": "2023-12-19T15:22:00.000Z",
        "voteCount": 2,
        "content": "@Andrew_Chen Azure Databricks with Structured Streaming is preferred over Azure Stream Analytics and Azure Synapse Notebooks for real-time streaming data processing from Azure Event Hubs due to its native support for continuous processing, live querying, and seamless integration with Azure Data Lake Storage."
      },
      {
        "date": "2023-08-30T01:17:00.000Z",
        "voteCount": 1,
        "content": "should be correct"
      },
      {
        "date": "2023-06-23T04:51:00.000Z",
        "voteCount": 1,
        "content": "What streaming sources and sinks does Azure Databricks support?\nDatabricks recommends using Auto Loader to ingest supported file types from cloud object storage into Delta Lake. For ETL pipelines, Databricks recommends using Delta Live Tables (which uses Delta tables and Structured Streaming). You can also configure incremental ETL workloads by streaming to and from Delta Lake tables.\n\nIn addition to Delta Lake and Auto Loader, Structured Streaming can connect to messaging services such as Apache Kafka.\nhttps://learn.microsoft.com/en-us/azure/databricks/structured-streaming/\nI don't see data lake in the list, so probably the answer is A."
      },
      {
        "date": "2023-04-03T04:36:00.000Z",
        "voteCount": 3,
        "content": "I am in favour of B because of this piece of information I have encountered:\nhttps://www.databricks.com/spark/getting-started-with-apache-spark/streaming"
      },
      {
        "date": "2023-04-03T04:40:00.000Z",
        "voteCount": 1,
        "content": "On the other hand, there is this: https://learn.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics\nSo I believe both to be valid, Azure Stream Analytics seems to be more straightforward"
      },
      {
        "date": "2023-03-08T01:18:00.000Z",
        "voteCount": 1,
        "content": "An Azure Stream Analytics job consists of an input, query, and an output."
      },
      {
        "date": "2023-01-26T18:02:00.000Z",
        "voteCount": 2,
        "content": "\"The solution must ensure that analysts can interactively query the streaming data\"\nStreaming analysis can't query streaming data interactively"
      },
      {
        "date": "2023-01-18T09:35:00.000Z",
        "voteCount": 2,
        "content": "B. Structured Streaming in Azure Databricks is incorrect because while it allows you to process streaming data using Spark's structured streaming API, it is not designed to directly output the data to Azure Data Lake Storage. Instead, it typically outputs the data to storage systems like HDFS, S3, or Cosmos DB. Additionally, Databricks is a separate service that does not integrate with Azure Synapse for interactive querying. While it's possible to use Databricks to read the data from Data Lake Storage and use Spark to process the data and then write it back to Data Lake Storage, it will not be as efficient as using Azure Stream Analytics for this use case as it is specifically designed for streaming data processing and also has built-in connectors to various data storage and analytics services like Data Lake Storage"
      },
      {
        "date": "2023-01-26T09:06:00.000Z",
        "voteCount": 1,
        "content": "Although this might be true, after some pondering, the given solution A. Azure Stream Analytics and Azure Synapse notebooks requires a Synpase workspace which is not implied. \nSo I guess it would be databricks."
      },
      {
        "date": "2023-06-14T01:05:00.000Z",
        "voteCount": 2,
        "content": "It's implied. Solutions said Azure Stream Analytics and Azure Synapse Notebook, Azure Synapse notebook cannot be created without Azure Synapse Workspace."
      },
      {
        "date": "2023-01-15T03:34:00.000Z",
        "voteCount": 2,
        "content": "Why not Azure Stream Analytics and Azure Synapse Analytics?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95362-exam-dp-203-topic-2-question-92-discussion/",
    "body": "You are creating an Apache Spark job in Azure Databricks that will ingest JSON-formatted data.<br><br>You need to convert a nested JSON string into a DataFrame that will contain multiple rows.<br><br>Which Spark SQL function should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfilter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcoalesce",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\textract"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-15T15:19:00.000Z",
        "voteCount": 6,
        "content": "https://learn.microsoft.com/en-us/azure/databricks/kb/scala/flatten-nested-columns-dynamically"
      },
      {
        "date": "2024-01-31T06:27:00.000Z",
        "voteCount": 1,
        "content": "This example is for scala. SQL example like this: \nhttps://learn.microsoft.com/en-us/u-sql/statements-and-expressions/select/from/cross-apply/explode"
      },
      {
        "date": "2024-01-08T07:56:00.000Z",
        "voteCount": 2,
        "content": "The explode function in Spark SQL is used to transform an array or a map column into multiple rows, essentially \"exploding\" the nested structure"
      },
      {
        "date": "2023-09-05T04:02:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer!"
      },
      {
        "date": "2023-08-30T01:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-01-14T20:04:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95363-exam-dp-203-topic-2-question-93-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure subscription that contains an Azure Databricks workspace. The workspace contains a notebook named Notebook1.<br><br>In Notebook1, you create an Apache Spark DataFrame named df_sales that contains the following columns:<br><br>\u2022\tCustomer<br>\u2022\tSalesPerson<br>\u2022\tRegion<br>\u2022\tAmount<br><br>You need to identify the three top performing salespersons by amount for a region named HQ.<br><br>How should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image266.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image267.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-03-10T01:43:00.000Z",
        "voteCount": 14,
        "content": "df_sales.filter(col(\"Region\") == \"HQ\")\n        .groupBy(col('SalesPerson'))\n        .agg(sum('Amount').alias('TotalAmount'))\n        .orderBy(desc('TotalAmount'))\n        .limit(3)"
      },
      {
        "date": "2023-08-30T01:20:00.000Z",
        "voteCount": 5,
        "content": ".groupBy(col('SalesPerson')) and  .orderBy(desc('TotalAmount'))"
      },
      {
        "date": "2024-02-09T08:45:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct!"
      },
      {
        "date": "2023-01-26T22:45:00.000Z",
        "voteCount": 4,
        "content": "for the sequence, group by usually put before the order by operations"
      },
      {
        "date": "2023-01-14T20:09:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104991-exam-dp-203-topic-2-question-94-discussion/",
    "body": "You need to schedule an Azure Data Factory pipeline to execute when a new file arrives in an Azure Data Lake Storage Gen2 container.<br><br>Which type of trigger should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ton-demand",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstorage event\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T09:46:00.000Z",
        "voteCount": 9,
        "content": "Correct, D"
      },
      {
        "date": "2024-08-24T15:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-04-14T00:20:00.000Z",
        "voteCount": 1,
        "content": "Correct. So easy that even Exam Topics got it right!"
      },
      {
        "date": "2024-02-09T08:46:00.000Z",
        "voteCount": 1,
        "content": "It's ok"
      },
      {
        "date": "2023-08-30T01:21:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-05-09T08:38:00.000Z",
        "voteCount": 2,
        "content": "Correct, D"
      },
      {
        "date": "2023-04-03T16:39:00.000Z",
        "voteCount": 1,
        "content": "ans is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104992-exam-dp-203-topic-2-question-95-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a project in Azure DevOps that contains a repository named Repo1. Repo1 contains a branch named main.<br><br>You create a new Azure Synapse workspace named Workspace1.<br><br>You need to create data processing pipelines in Workspace1. The solution must meet the following requirements:<br><br>\u2022\tPipeline artifacts must be stored in Repo1<br>\u2022\tSource control must be provided for pipeline artifacts.<br>\u2022\tAll development must be performed in a feature branch.<br><br>Which four actions should you perform in sequence in Synapse Studio? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-203/image276.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image277.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-20T14:39:00.000Z",
        "voteCount": 25,
        "content": "Configure a code repo and select Repo1\nSet the main branch as the collaboration branch\nCreate a new brach\nCreate pipeline artifacts and save them in the new branch"
      },
      {
        "date": "2024-03-04T01:59:00.000Z",
        "voteCount": 2,
        "content": "In the context of Azure Synapse Studio, it's recommended not to use the 'main' branch as a collaboration branch. Here's why:\n\n- Deployment Branch: The 'main' branch serves as the source of truth for your production-ready pipelines. Changes made directly in 'main' could potentially introduce instability or untested features into the production environment.\n-Code Stability: Isolating development work in separate collaboration branches protects your 'main' branch from unintended changes or broken code during development.\n-Review Process: Collaboration branches allow for code reviews, testing, and approval processes before merging changes into the 'main' branch, ensuring higher code quality."
      },
      {
        "date": "2023-09-11T14:26:00.000Z",
        "voteCount": 1,
        "content": "Isn't the main branch by default the collaboration branch?"
      },
      {
        "date": "2024-03-08T00:20:00.000Z",
        "voteCount": 1,
        "content": "Your Azure Repos collaboration branch that is used for publishing. By default, it's main. \nhttps://learn.microsoft.com/en-us/azure/data-factory/source-control"
      },
      {
        "date": "2023-12-09T10:22:00.000Z",
        "voteCount": 3,
        "content": "This is wrong, given answer by exam topics is right"
      },
      {
        "date": "2023-05-16T04:28:00.000Z",
        "voteCount": 14,
        "content": "Shouldn't you merge the new branch into the main branch?"
      },
      {
        "date": "2023-06-13T15:19:00.000Z",
        "voteCount": 4,
        "content": "Agree, you create a feature branch from the collaboration branch, work on it, and after you finished you merge back to the collaboration branch (by default is main). Source: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control#version-control"
      },
      {
        "date": "2024-07-15T03:38:00.000Z",
        "voteCount": 1,
        "content": "Configure a code repository and select Repo1.\nSet the main branch as the collaboration branch.\nCreate a new branch.\nCreate pipeline artifacts and save them in the new branch."
      },
      {
        "date": "2024-06-07T09:13:00.000Z",
        "voteCount": 1,
        "content": "Read question first line again don't know why you all missed it, the top line says code repository Repo1 exists and has main branch therefore:\n\n\n1. Set the main branch as collaboration branch.\n2. Create a new branch.\n3. Create a pull request to merge the contents of main branch to new branch. (You do this step to make new branch up-to-date to main branch)\n4. Create pipeline artifacts in the new branch"
      },
      {
        "date": "2024-06-10T01:15:00.000Z",
        "voteCount": 1,
        "content": "Ignore you might have to configure repo in local before doing any operation"
      },
      {
        "date": "2024-03-17T08:31:00.000Z",
        "voteCount": 4,
        "content": "I think we could exclude two options, one is wrong and the other doesn\u00b4t make sense:\n- You do not create a PR to merge the contents of the \"main\" branch to \"feature\" branch, it\u00b4s from the \"feature\" into the \"main\" branch.\n- We don\u00b4t create the artifacts for the first time directly on the main branch and then fork to a new branch, you create directly on the new feature branch.\n\nThis leaves four available options:\n- Configure a code repository and select Repo1\n- Set the main branch as the collaboration branch (remember that repo was already created inside DevOps, a generic git project)\n- Create a new branch\n- Create pipelines artifacts and save in the new branch"
      },
      {
        "date": "2024-01-10T04:07:00.000Z",
        "voteCount": 3,
        "content": "From DOWN \nYOU CAN PUT THEM IN SEQUENCE DONT NEED TO REMEMBER THE STEPS"
      },
      {
        "date": "2024-03-02T06:12:00.000Z",
        "voteCount": 1,
        "content": "In the real exam the order may be randomised, speaking from personal experience with other fundamental exams. Memorise the answers instead"
      },
      {
        "date": "2023-08-30T01:43:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-30T01:46:00.000Z",
        "voteCount": 2,
        "content": "Configure a code repo and select Repo1\nSet the main branch as the collaboration branch\nCreate a new brach\n and PR"
      },
      {
        "date": "2023-09-08T20:49:00.000Z",
        "voteCount": 5,
        "content": "forgot it ,the given answer is right."
      },
      {
        "date": "2023-08-27T07:49:00.000Z",
        "voteCount": 3,
        "content": "Given solution is correct"
      },
      {
        "date": "2023-08-09T05:50:00.000Z",
        "voteCount": 1,
        "content": "\"Configure a code repository and select Repo2\" is not required as you already have a repo Repo1 with main as branch."
      },
      {
        "date": "2023-04-03T09:51:00.000Z",
        "voteCount": 6,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104993-exam-dp-203-topic-2-question-96-discussion/",
    "body": "You have an Azure subscription that contains an Azure SQL database named DB1 and a storage account named storage1. The storage1 account contains a file named File1.txt. File1.txt contains the names of selected tables in DB1.<br><br>You need to use an Azure Synapse pipeline to copy data from the selected tables in DB1 to the files in storage1. The solution must meet the following requirements:<br><br>\u2022\tThe Copy activity in the pipeline must be parameterized to use the data in File1.txt to identify the source and destination of the copy.<br>\u2022\tCopy activities must occur in parallel as often as possible.<br><br>Which two pipeline activities should you include in the pipeline? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet Metadata",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForEach\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIf Condition"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 31,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T09:56:00.000Z",
        "voteCount": 13,
        "content": "It's BC. Use the LookUp Activity to read the .txt file. ForEach to Loop though making sure Sequential is off (which off by default) for parallelization"
      },
      {
        "date": "2023-12-19T15:46:00.000Z",
        "voteCount": 1,
        "content": "This is correct,  Get Metadata doesn't directly retrieve the content of the file itself. If you want to obtain the content of a .txt file like File1.txt, specifically the table names contained within it, you'd typically use Lookup.\nCorrect Anwser: BC"
      },
      {
        "date": "2023-04-16T03:25:00.000Z",
        "voteCount": 6,
        "content": "Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure. The output can be a singleton value or an array of attributes, which can be consumed in a subsequent copy, transformation, or control flow activities like ForEach activity.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"
      },
      {
        "date": "2023-08-30T01:57:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-08-09T05:07:00.000Z",
        "voteCount": 2,
        "content": "BC - Lookup and ForEach.\nLookup - reads .txt file\nForEach - iteration through contents read in Lookup activity for COPY activity"
      },
      {
        "date": "2023-06-23T05:42:00.000Z",
        "voteCount": 2,
        "content": "Correct answers."
      },
      {
        "date": "2023-06-16T18:31:00.000Z",
        "voteCount": 2,
        "content": "B. Lookup: The Lookup activity can be used to read the contents of File1.txt from the storage account. It will retrieve the names of selected tables in DB1 as parameter values for the Copy activity.\n\nC. ForEach: The ForEach activity can be used to iterate over the retrieved table names from File1.txt. Inside the loop, you can configure the Copy activity with the source and destination information based on the current table name."
      },
      {
        "date": "2023-04-09T04:29:00.000Z",
        "voteCount": 4,
        "content": "Answer is B and C."
      },
      {
        "date": "2023-04-05T11:10:00.000Z",
        "voteCount": 4,
        "content": "' Get Metadata' cannot read the content of the file. Its Lookup and ForEach.\nRefer to link : https://learn.microsoft.com/en-us/azure/data-factory/control-flow-get-metadata-activity and https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105322-exam-dp-203-topic-2-question-97-discussion/",
    "body": "You have an Azure data factory that connects to a Microsoft Purview account. The data factory is registered in Microsoft Purview.<br><br>You update a Data Factory pipeline.<br><br>You need to ensure that the updated lineage is available in Microsoft Purview.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisconnect the Microsoft Purview account from the data factory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute an Azure DevOps build pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocate the related asset in the Microsoft Purview portal."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-05T11:39:00.000Z",
        "voteCount": 11,
        "content": "B. Execute the Pipeline is correct answer.\nRefer link : https://learn.microsoft.com/en-us/azure/data-factory/tutorial-push-lineage-to-purview and https://learn.microsoft.com/en-us/azure/data-factory/connect-data-factory-to-azure-purview"
      },
      {
        "date": "2023-05-10T11:18:00.000Z",
        "voteCount": 3,
        "content": "Correct.\n\"The lineage data will automatically be captured during the activities execution.\""
      },
      {
        "date": "2024-07-15T01:07:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-02-12T02:47:00.000Z",
        "voteCount": 1,
        "content": "From the official documentation: \"You can create pipelines, Copy activities and Dataflow activities in Data Factory. You don't need any additional configuration for lineage data capture. The lineage data will automatically be captured during the activities execution.\"\nThen, B is correct."
      },
      {
        "date": "2023-08-30T01:59:00.000Z",
        "voteCount": 1,
        "content": "executing the pipeline"
      },
      {
        "date": "2023-06-16T18:33:00.000Z",
        "voteCount": 2,
        "content": "By executing the pipeline, the Data Factory will generate the lineage information and propagate it to the connected Microsoft Purview account. This will update the lineage in Purview and reflect any changes made in the pipeline."
      },
      {
        "date": "2023-04-28T03:50:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/tutorial-push-lineage-to-purview"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/microsoft/view/107121-exam-dp-203-topic-2-question-98-discussion/",
    "body": "You have a Microsoft Purview account.<br><br>The Lineage view of a CSV file is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image278.png\"><br><br>How is the data for the lineage populated?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanually",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tby scanning data stores",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tby executing a Data Factory pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-23T02:29:00.000Z",
        "voteCount": 6,
        "content": "Answer is C\nFind reason here: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-push-lineage-to-purview#run-pipeline-and-push-lineage-data-to-microsoft-purview"
      },
      {
        "date": "2023-04-23T02:31:00.000Z",
        "voteCount": 18,
        "content": "The answer is also displayed on the top right corner of the image displayed."
      },
      {
        "date": "2024-02-12T02:51:00.000Z",
        "voteCount": 2,
        "content": "The answer is on the image --&gt; Data Factory Pipeline"
      },
      {
        "date": "2023-08-30T02:01:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105347-exam-dp-203-topic-2-question-99-discussion/",
    "body": "You have an Azure subscription that contains a Microsoft Purview account named MP1, an Azure data factory named DF1, and a storage account named storage1. MP1 is configured to scan storage1. DF1 is connected to MP1 and contains a dataset named DS1. DS1 references a file in storage1.<br><br>In DF1, you plan to create a pipeline that will process data from DS1.<br><br>You need to review the schema and lineage information in MP1 for the data referenced by DS1.<br><br>Which two features can you use to locate the information? Each correct answer presents a complete solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe search bar in the Microsoft Purview governance portal\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Storage browser of storage1 in the Azure portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe search bar in the Azure portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe search bar in Azure Data Factory Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-05T16:55:00.000Z",
        "voteCount": 16,
        "content": "From ChatGPT :\n\nA. the search bar in the Microsoft Purview governance portal\nD. the search bar in Azure Data Factory Studio\n\nTo review the schema and lineage information in MP1 for the data referenced by DS1, you can use the following two features:\n\nThe search bar in the Microsoft Purview governance portal: You can search for the file in storage1 that is referenced by DS1 in the search bar of the Purview governance portal. Once you locate the file, you can view the schema and lineage information for it.\n\nThe search bar in Azure Data Factory Studio: You can search for the dataset DS1 in the Azure Data Factory Studio search bar. Once you locate the dataset, you can view the schema and lineage information for the data it references in storage1, which can also be viewed in Purview."
      },
      {
        "date": "2024-01-10T04:13:00.000Z",
        "voteCount": 1,
        "content": "Same chatgpt\nA. the search bar in the Microsoft Purview governance portal\nB. the Storage browser of storage1 in the Azure portal\n\nExplanation:\n\nThe search bar in the Microsoft Purview governance portal (A): Microsoft Purview is designed for data discovery, classification, and lineage tracking. You can use the search bar in the Purview governance portal to search for and locate information about the schema and lineage of the data.\n\nThe Storage browser of storage1 in the Azure portal (B): While this won't directly provide schema and lineage information, you can use the Storage browser to explore the contents of your storage account. This can be useful for understanding the structure of the data stored in storage1."
      },
      {
        "date": "2024-02-17T19:44:00.000Z",
        "voteCount": 1,
        "content": "'The Storage browser.... won't directly provide schema and lineage information'\nSo the answer is AD"
      },
      {
        "date": "2023-04-23T19:33:00.000Z",
        "voteCount": 7,
        "content": "You need lineage info. Lineage is in Purview. Also, the lineage is all based off what the Data Factory pipeline is doing. I'd say A and D.\n\nhttps://learn.microsoft.com/en-us/azure/purview/how-to-search-catalog#searching-microsoft-purview-in-connected-services"
      },
      {
        "date": "2023-12-19T16:01:00.000Z",
        "voteCount": 3,
        "content": "I don't fully understand this discussion, I have been led to believe that\nAzure Data Factory Studio is primarily used to search for components within the data factory itself, such as datasets, pipelines, activities, linked services, etc. It is useful for finding and managing resources within the data factory BUT not for exploring external metadata or lineage information stored in Purview."
      },
      {
        "date": "2024-05-20T22:45:00.000Z",
        "voteCount": 1,
        "content": "If you integrate Purview wth Data Factory,  then you can search the Purview Data Catalog from within Data Factory"
      },
      {
        "date": "2023-08-30T02:04:00.000Z",
        "voteCount": 2,
        "content": "is correct"
      },
      {
        "date": "2023-06-05T03:01:00.000Z",
        "voteCount": 4,
        "content": "If the Data Factory resource is connected to a Purview account there will be a column in the monitoring view of the Pipeline with the lineage status. https://learn.microsoft.com/en-us/azure/data-factory/tutorial-push-lineage-to-purview#step-3-monitor-lineage-reporting-status"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104997-exam-dp-203-topic-2-question-100-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Blob storage account that contains a folder. The folder contains 120,000 files. Each file contains 62 columns.<br><br>Each day, 1,500 new files are added to the folder.<br><br>You plan to incrementally load five data columns from each new file into an Azure Synapse Analytics workspace.<br><br>You need to minimize how long it takes to perform the incremental loads.<br><br>What should you use to store the files and in which format? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image279.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image280.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-03T10:44:00.000Z",
        "voteCount": 49,
        "content": "Time partitioning is correct as the fastest way to load only new files, but requires that the timeslice information be part of the file or folder name (https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-overview)\n\nHowever, Parquet is the correct file format since it's a columnar format"
      },
      {
        "date": "2023-08-30T02:07:00.000Z",
        "voteCount": 8,
        "content": "Time partitioning  and parquet"
      },
      {
        "date": "2024-04-01T03:07:00.000Z",
        "voteCount": 1,
        "content": "Parquet is the answer to the second question. You nee to take only 5 colums out of 62: with CSV you'd have to explore the file row-wise sequentially... Parquet is more efficient, since seleciton proceeds column-wise."
      },
      {
        "date": "2024-04-01T03:09:00.000Z",
        "voteCount": 2,
        "content": "Just a notice. This answer should be proven by a benchmark. Currently I didn't find any benchmark comparing the performances of the two files. Logically speaking, I'd expect Parquet to be a lot more efficient, but it should have to be measured in practise."
      },
      {
        "date": "2023-06-16T18:40:00.000Z",
        "voteCount": 2,
        "content": "You need to minimize how long it takes to perform the incremental loads. With Parquet, which is a columnar format, it is way faster to select a few columns than csv."
      },
      {
        "date": "2023-05-29T01:37:00.000Z",
        "voteCount": 1,
        "content": "we can do incremental load just with deltatable for a parquet file which supported by datarbricks or synapse spark and here he didn't give details so I think it will be CSV"
      },
      {
        "date": "2023-05-24T12:56:00.000Z",
        "voteCount": 1,
        "content": "I think the requirement is to select specific columns, hence CSV?"
      },
      {
        "date": "2023-05-16T20:10:00.000Z",
        "voteCount": 5,
        "content": "it supposed to be Parquet instead of CSV"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108691-exam-dp-203-topic-2-question-101-discussion/",
    "body": "DRAG DROP<br> -<br><br>You are batch loading a table in an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to load data from a staging table to the target table. The solution must ensure that if an error occurs while loading the data to the target table, all the inserts in that batch are undone.<br><br>How should you complete the Transact-SQL code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image296.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image297.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-07T07:52:00.000Z",
        "voteCount": 30,
        "content": "Given answer is wrong. It should be BEGIN TRAN as SQL pool in Azure Synapse Analytics does not support distributed transaction.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-develop-transactions\n\n\"Limitations\n\nSQL pool does have a few other restrictions that relate to transactions.\n\nThey are as follows:\n\n    No distributed transactions\n    No nested transactions permitted\n    No save points allowed\n    No named transactions\n    No marked transactions\n    No support for DDL such as CREATE TABLE inside a user-defined transaction\n\"\nDistributed Transactions are only allowed in SQL Server and Azure SQL Managed Instance:\n\nhttps://learn.microsoft.com/de-de/sql/t-sql/language-elements/begin-distributed-transaction-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2023-06-01T13:32:00.000Z",
        "voteCount": 11,
        "content": "Its BEGIN TRAN\nthen ROLLBACK TRAN"
      },
      {
        "date": "2024-07-09T05:16:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o code:\nBEGIN TRANSACTION;\n\nBEGIN TRY\n    -- Insert data from staging table to target table\n    INSERT INTO target_table (column1, column2, ...)\n    SELECT column1, column2, ...\n    FROM staging_table;\n    \n    -- Commit the transaction if no error occurs\n    COMMIT TRANSACTION;\nEND TRY\nBEGIN CATCH\n    -- Rollback the transaction if an error occurs\n    ROLLBACK TRANSACTION;\n\n    -- Optionally, you can log the error or re-throw it\n    -- SELECT ERROR_MESSAGE() AS ErrorMessage;\n    -- THROW;\nEND CATCH;"
      },
      {
        "date": "2024-02-12T03:10:00.000Z",
        "voteCount": 1,
        "content": "BEGIN TRAN\nROLLBACK TRAN"
      },
      {
        "date": "2024-01-31T16:40:00.000Z",
        "voteCount": 1,
        "content": "BEGIN TRAN &amp; ROLLBACK TRAN"
      },
      {
        "date": "2023-12-27T15:36:00.000Z",
        "voteCount": 1,
        "content": "It should be BEGIN TRAN\nand ROLLBACK TRAN"
      },
      {
        "date": "2023-08-30T02:10:00.000Z",
        "voteCount": 1,
        "content": "BEGIN TRAN\nROLLBACK TRAN"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108724-exam-dp-203-topic-2-question-102-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have two Azure SQL databases named DB1 and DB2.<br><br>DB1 contains a table named Table1. Table1 contains a timestamp column named LastModifiedOn. LastModifiedOn contains the timestamp of the most recent update for each individual row.<br><br>DB2 contains a table named Watermark. Watermark contains a single timestamp column named WatermarkValue.<br><br>You plan to create an Azure Data Factory pipeline that will incrementally upload into Azure Blob Storage all the rows in Table1 for which the LastModifiedOn column contains a timestamp newer than the most recent value of the WatermarkValue column in Watermark.<br><br>You need to identify which activities to include in the pipeline. The solution must meet the following requirements:<br><br>\u2022\tMinimize the effort to author the pipeline.<br>\u2022\tEnsure that the number of data integration units allocated to the upload operation can be controlled.<br><br>What should you identify? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image298.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image299.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-01T07:34:00.000Z",
        "voteCount": 15,
        "content": "Correct. The example is here\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"
      },
      {
        "date": "2023-05-08T01:42:00.000Z",
        "voteCount": 11,
        "content": "Seems correct to me"
      },
      {
        "date": "2024-06-30T16:11:00.000Z",
        "voteCount": 1,
        "content": "Create a pipeline with the following workflow:\n\nThe pipeline in this solution has the following activities:\n\nCreate two Lookup activities. Use the first Lookup activity to retrieve the last watermark value. Use the second Lookup activity to retrieve the new watermark value. These watermark values are passed to the Copy activity.\nCreate a Copy activity that copies rows from the source data store with the value of the watermark column greater than the old watermark value and less than the new watermark value. Then, it copies the delta data from the source data store to Blob storage as a new file.\nCreate a StoredProcedure activity that updates the watermark value for the pipeline that runs next time."
      },
      {
        "date": "2024-05-04T04:38:00.000Z",
        "voteCount": 2,
        "content": "I found this question on my exam 30/04/2024, and I put \n- LOOKUP\n- DATAFLOW\n I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-07-02T03:16:00.000Z",
        "voteCount": 2,
        "content": "you will get higher score if you choose Lookup and Copy"
      },
      {
        "date": "2023-08-30T02:15:00.000Z",
        "voteCount": 1,
        "content": "lookup &amp; copy activity"
      },
      {
        "date": "2023-05-09T09:46:00.000Z",
        "voteCount": 3,
        "content": "Filter not lookup, because we have to \"Minimize the effort to author the pipeline\" and we have only the LastModifiedOn column as information, we are not sure for lookup."
      },
      {
        "date": "2023-06-23T06:14:00.000Z",
        "voteCount": 5,
        "content": "The Filter activity in Azure Data Factory is used to filter an array of objects from a previous activity's output (typically from a Lookup activity). It cannot directly query a database or compare a value from a database (watermark in this case) against data in another database."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108533-exam-dp-203-topic-2-question-103-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse serverless SQL pool.<br><br>You need to read JSON documents from a file by using the OPENROWSET function.<br><br>How should you complete the query? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image300.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image301.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-05T03:20:00.000Z",
        "voteCount": 32,
        "content": "Correct. It's weird but best way to open a json is as a csv and with 0x0b for fieldterminator and fieldquote.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files"
      },
      {
        "date": "2024-01-11T06:22:00.000Z",
        "voteCount": 6,
        "content": "I got this question today in exam 11-jan-2024"
      },
      {
        "date": "2024-06-23T14:37:00.000Z",
        "voteCount": 4,
        "content": "GPT would fail this exam and still explain the incorrect answers like a PRO"
      },
      {
        "date": "2024-02-12T03:24:00.000Z",
        "voteCount": 1,
        "content": "It's correct"
      },
      {
        "date": "2023-08-30T02:17:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108594-exam-dp-203-topic-2-question-104-discussion/",
    "body": "You use Azure Data Factory to create data pipelines.<br><br>You are evaluating whether to integrate Data Factory and GitHub for source and version control.<br><br>What are two advantages of the integration? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tadditional triggers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlower pipeline execution times",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe ability to save without publishing\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe ability to save pipelines that have validation issues\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-06T08:50:00.000Z",
        "voteCount": 24,
        "content": "C. the ability to save without publishing\nD. the ability to save pipelines that have validation issues\nWhen you integrate Data Factory and GitHub, you can save your pipelines to a GitHub repository without publishing them to Azure. This allows you to work on your pipelines in a development environment and then publish them to Azure when you are ready.\n\nYou can also save pipelines that have validation issues. This is because GitHub does not validate your pipelines when you save them. This allows you to work on your pipelines and fix the validation issues before you publish them to Azure."
      },
      {
        "date": "2023-05-09T12:39:00.000Z",
        "voteCount": 3,
        "content": "agree with you"
      },
      {
        "date": "2023-08-30T02:19:00.000Z",
        "voteCount": 1,
        "content": "agree with you"
      },
      {
        "date": "2023-09-26T02:37:00.000Z",
        "voteCount": 2,
        "content": "absolutely right"
      },
      {
        "date": "2024-05-02T01:07:00.000Z",
        "voteCount": 1,
        "content": "B definitely; 4 times faster\nLeaning towards C although D is not incorrect either"
      },
      {
        "date": "2024-06-18T17:09:00.000Z",
        "voteCount": 1,
        "content": "No brainer its C &amp; D."
      },
      {
        "date": "2024-03-02T06:40:00.000Z",
        "voteCount": 2,
        "content": "Answer is CD hands down, no contest,"
      },
      {
        "date": "2024-02-12T03:28:00.000Z",
        "voteCount": 1,
        "content": "CD with no doubts"
      },
      {
        "date": "2024-01-31T16:42:00.000Z",
        "voteCount": 1,
        "content": "Correct \nC. the ability to save without publishing\nD. the ability to save pipelines that have validation issues"
      },
      {
        "date": "2023-06-16T18:52:00.000Z",
        "voteCount": 3,
        "content": "C. The ability to save without publishing: Integrating Data Factory with GitHub allows you to save changes to your pipelines without immediately publishing them. This provides flexibility in terms of saving work-in-progress or experimental changes without impacting the production pipelines.\n\nD. The ability to save pipelines that have validation issues: With GitHub integration, you can save pipelines that have validation issues. This is useful when you want to save your work-in-progress changes or modifications to a pipeline, even if it doesn't currently pass validation. You can continue to work on resolving the validation issues without losing your progress."
      },
      {
        "date": "2023-05-10T11:59:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2023-05-09T12:02:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2023-05-05T17:16:00.000Z",
        "voteCount": 1,
        "content": "I think B and C"
      },
      {
        "date": "2024-02-12T03:28:00.000Z",
        "voteCount": 1,
        "content": "Why BC?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108583-exam-dp-203-topic-2-question-105-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Synapse Analytics workspace named Workspace1.<br><br>You perform the following changes:<br><br>\u2022\tImplement source control for Workspace1.<br>\u2022\tCreate a branch named Feature based on the collaboration branch.<br>\u2022\tSwitch to the Feature branch.<br>\u2022\tModify Workspace1.<br><br>You need to publish the changes to Azure Synapse.<br><br>From which branch should you perform each change? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point<br><br><img src=\"https://img.examtopics.com/dp-203/image302.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image303.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-09T12:40:00.000Z",
        "voteCount": 10,
        "content": "Correct! It's a easy one."
      },
      {
        "date": "2024-06-18T01:34:00.000Z",
        "voteCount": 1,
        "content": "Yup also there no branch named publish"
      },
      {
        "date": "2023-08-30T02:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-06-16T18:53:00.000Z",
        "voteCount": 3,
        "content": "Seems correct."
      },
      {
        "date": "2023-05-05T13:40:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111881-exam-dp-203-topic-2-question-106-discussion/",
    "body": "You have two Azure Blob Storage accounts named account1 and account2.<br><br>You plan to create an Azure Data Factory pipeline that will use scheduled intervals to replicate newly created or modified blobs from account1 to account2.<br><br>You need to recommend a solution to implement the pipeline. The solution must meet the following requirements:<br>\u2022\tEnsure that the pipeline only copies blobs that were created or modified since the most recent replication event.<br>\u2022\tMinimize the effort to create the pipeline.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the Copy Data tool and select Metadata-driven copy task.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline that contains a Data Flow activity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline that contains a flowlet.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the Copy Data tool and select Built-in copy task.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-11T02:30:00.000Z",
        "voteCount": 13,
        "content": "Just use Built-in copy task, according to: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool"
      },
      {
        "date": "2023-06-16T18:56:00.000Z",
        "voteCount": 6,
        "content": "\"[...] use the Copy Data tool to create a pipeline that incrementally copies new and changed files only, from Azure Blob storage to Azure Blob storage. It uses LastModifiedDate to determine which files to copy.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool"
      },
      {
        "date": "2024-04-26T07:20:00.000Z",
        "voteCount": 1,
        "content": "correct according to doc"
      },
      {
        "date": "2024-04-25T17:53:00.000Z",
        "voteCount": 1,
        "content": "vote for D"
      },
      {
        "date": "2024-04-06T01:48:00.000Z",
        "voteCount": 2,
        "content": "A. Run the Copy Data tool and select Metadata-driven copy task.\n\nExplanation:\n\nThe Metadata-driven copy task in Azure Data Factory allows you to dynamically define and manage data movement and transformation tasks based on metadata. This includes copying only blobs that were created or modified since the most recent replication event, which aligns with the requirements.\nThis solution minimizes the effort to create the pipeline, as you can configure the replication task based on metadata properties such as the creation or modification timestamp of the blobs.\nThe Built-in copy task also provides a solution for data movement but may require more manual effort to configure the incremental replication based on timestamps"
      },
      {
        "date": "2024-02-12T09:16:00.000Z",
        "voteCount": 1,
        "content": "The built-in copy task leads you to create a pipeline within five minutes to replicate data without learning about entities. The metadata driven copy task to ease your journey of creating parameterized pipelines and external control table in order to manage to copy large amounts of objects (for example, thousands of tables) at scale."
      },
      {
        "date": "2024-01-10T05:23:00.000Z",
        "voteCount": 2,
        "content": "Metadata-driven copy task (A): The Metadata-driven copy task in the Copy Data tool of Azure Data Factory allows you to replicate data based on changes in metadata. This includes the ability to only copy blobs that were created or modified since the last replication event.\nGiven the requirements to replicate blobs based on changes in metadata and to minimize effort, the Metadata-driven copy task is a suitable choice."
      },
      {
        "date": "2024-02-04T17:31:00.000Z",
        "voteCount": 1,
        "content": "I agree A is more appropriate."
      },
      {
        "date": "2024-02-20T02:50:00.000Z",
        "voteCount": 2,
        "content": "From the documentation https://learn.microsoft.com/en-us/azure/data-factory/copy-data-tool-metadata-driven it looks like the Metadata-driven copy task retreives metaData from control table in order to perform the copy task from multiple sources to multiple destinations. That is not the requirement here.\nThe built-in copy task checks one source folder and looks at the metaData of the files present there and copies only the files that were newly added or modified since the last run: https://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool\nSo the answer has to be - D. Run the Copy Data tool and select Built-in copy task."
      },
      {
        "date": "2023-12-06T18:06:00.000Z",
        "voteCount": 3,
        "content": "A is correct because of the requirement \"Minimize the effort to create the pipeline.\""
      },
      {
        "date": "2024-01-07T14:48:00.000Z",
        "voteCount": 1,
        "content": "metadriven is for copy huge amounts of objects (for example, thousands of tables) or load data from large variety of sources"
      },
      {
        "date": "2023-08-30T02:51:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-06-23T06:25:00.000Z",
        "voteCount": 2,
        "content": "Create a data factory.\nUse the Built-in Copy Data tool to create a pipeline.\nMonitor the pipeline and activity runs."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113092-exam-dp-203-topic-2-question-107-discussion/",
    "body": "You have an Azure Data Factory pipeline named pipeline1 that contains a data flow activity named activity1.<br><br>You need to run pipeline1.<br><br>Which runtime will be used to run activity1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Integration runtime\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelf-hosted integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSIS integration runtime"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-30T02:52:00.000Z",
        "voteCount": 5,
        "content": "correct"
      },
      {
        "date": "2024-04-25T17:57:00.000Z",
        "voteCount": 1,
        "content": "A is correct; look at the link:\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"
      },
      {
        "date": "2024-04-25T07:36:00.000Z",
        "voteCount": 3,
        "content": "question is incomplete."
      },
      {
        "date": "2024-04-08T09:51:00.000Z",
        "voteCount": 1,
        "content": "Why answer is A?"
      },
      {
        "date": "2024-01-11T06:25:00.000Z",
        "voteCount": 3,
        "content": "I got this question today in exam 11-jan-2024"
      },
      {
        "date": "2023-06-23T06:26:00.000Z",
        "voteCount": 2,
        "content": "Probably the correct answer."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/microsoft/view/112033-exam-dp-203-topic-2-question-108-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription that contains an Azure Synapse Analytics workspace named workspace1. Workspace1 contains a dedicated SQL pool named SQLPool1 and an Apache Spark pool named sparkpool1. Sparkpool1 contains a DataFrame named pyspark_df.<br><br>You need to write the contents of pyspark_df to a table in SQLPool1 by using a PySpark notebook.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image316.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image317.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-13T00:32:00.000Z",
        "voteCount": 8,
        "content": "Correct\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/synapse-spark-sql-pool-import-export?tabs=scala%2Cscala1%2Cscala2%2Cscala3%2Cscala4%2Cscala5"
      },
      {
        "date": "2024-02-24T00:26:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2023-08-30T03:00:00.000Z",
        "voteCount": 2,
        "content": "%%spark\n&amp;&amp;\ndf.write.synapsesql"
      },
      {
        "date": "2023-07-09T05:10:00.000Z",
        "voteCount": 2,
        "content": "Correct, also according to this link\nhttps://microsoftlearning.github.io/DP-203-Data-Engineer/Instructions/Labs/LAB_04_data_warehouse_using_apache_spark.html\n\n*******\n %%spark\n // Make sure the name of the dedcated SQL pool (SQLPool01 below) matches the name of your SQL pool.\n val df = spark.sqlContext.sql(\"select * from top_purchases\")\n df.write.synapsesql(\"SQLPool01.wwi.TopPurchases\", Constants.INTERNAL)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111855-exam-dp-203-topic-2-question-109-discussion/",
    "body": "You have an Azure data factory named ADF1 and an Azure Synapse Analytics workspace that contains a pipeline named SynPipeLine1. SynPipeLine1 includes a Notebook activity.<br><br>You create a pipeline in ADF1 named ADFPipeline1.<br><br>You need to invoke SynPipeLine1 from ADFPipeline1.<br><br>Which type of activity should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeb\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustom",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNotebook"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T09:46:00.000Z",
        "voteCount": 7,
        "content": "Web Activity \nhttps://learn.microsoft.com/en-us/azure/data-factory/solution-template-synapse-notebook"
      },
      {
        "date": "2023-06-23T06:35:00.000Z",
        "voteCount": 6,
        "content": "To invoke a Synapse pipeline from a Data Factory pipeline, you should use a Web activity."
      },
      {
        "date": "2024-01-12T01:05:00.000Z",
        "voteCount": 2,
        "content": "To invoke a Synapse Analytics pipeline from an Azure Data Factory pipeline, you should use the \"Web\" activity. This activity is specifically designed for making HTTP requests and can be used to trigger the execution of pipelines in other services, such as Azure Synapse Analytics."
      },
      {
        "date": "2024-01-03T15:19:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-08-30T03:05:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-06-27T18:45:00.000Z",
        "voteCount": 1,
        "content": "its a notebook activity \nhttps://learn.microsoft.com/en-us/azure/data-factory/solution-template-synapse-notebook"
      },
      {
        "date": "2023-06-27T14:35:00.000Z",
        "voteCount": 2,
        "content": "Web calls a Synapse pipeline with a notebook activity."
      },
      {
        "date": "2023-06-10T17:26:00.000Z",
        "voteCount": 4,
        "content": "Web Activity as per this article.\n\nhttps://fnuson.medium.com/invoke-synapse-notebook-spark-job-by-azure-data-factory-adf-fc19cef89bdd"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/microsoft/view/112077-exam-dp-203-topic-2-question-110-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure data factory that contains the linked service shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image318.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image319.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image320.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-13T14:35:00.000Z",
        "voteCount": 21,
        "content": "According to Microsoft, AutoResolveIntegrationRuntime will attempt to use the sink location to get an IR in the same region (or the closest available) to execute the Copy activity, not the source location. I would go with the region of data factory, since that is the default option when the sink's location is not detectable. Source: https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location"
      },
      {
        "date": "2023-11-02T02:40:00.000Z",
        "voteCount": 1,
        "content": "\"the sink's location is not detectable\" is any wording in the Q that confirms ?\n\nIf not, no confirmation about the undetectable sink source, the correct answer is \nthe selected from ET ( \"in the region of the source database\" ).\n\nr: https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location - 1st bullet"
      },
      {
        "date": "2023-09-11T01:42:00.000Z",
        "voteCount": 9,
        "content": "1. upon publishing changes to the service\n2.  in the region of data factory"
      },
      {
        "date": "2024-02-12T10:12:00.000Z",
        "voteCount": 2,
        "content": "- Upon publishing the changes.\n- In the region of the source database."
      },
      {
        "date": "2024-01-31T17:20:00.000Z",
        "voteCount": 5,
        "content": "1. upon saving the changes ( refer to the warning message in the screenshot given in the question )\n2. In the region of data factory. ( https://learn.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#azure-ir-location) \n\nAzure IR location\nYou can set the location region of an Azure IR, in which case the activity execution or dispatch will happen in the selected region.\n\nThe default is to auto-resolve the Azure IR in the public network. With this option:\n\nFor copy activity, a best effort is made to automatically detect your sink data store's location, then use the IR in either the same region, if available, or the closest one in the same geography, otherwise; if the sink data store's region is not detectable, the IR in the instance's region is used instead."
      },
      {
        "date": "2024-01-10T05:30:00.000Z",
        "voteCount": 1,
        "content": "Once you make a changes it needs to make a pull request then merge the changes in the main pipeline then you will be able to see the changes in this case merging into collaboration branch is the right answer"
      },
      {
        "date": "2024-01-10T05:33:00.000Z",
        "voteCount": 1,
        "content": "C. in the region of the source database\n\nExplanation:\n\nWhen a copy activity uses a linked service as the source in Azure Data Factory, the data movement will be performed in the region of the source database. The linked service contains the connection information needed to connect to the source data store (such as Azure Blob Storage, Azure SQL Database, etc.). Therefore, the location where the data movement is initiated corresponds to the region where the source data store is located."
      },
      {
        "date": "2023-09-21T23:14:00.000Z",
        "voteCount": 3,
        "content": "For Linked Services, changes are published immediately unless you use key vault, which means basically upon saving. \n\n\"Changes to Linked Services are published immediately, unless you use Key Vault. This can mean a branch change to a Linked Service could impact other branch tests. The reason for this is credential protection. While \"Live-Mode\" retrieves definitions from the back-end, \"Github\" or \"Devops\" mode construct the definitions from the repository. Putting credentials into repository code is a very bad idea. This is why, without Key Vault, credentials are stored and encrypted in Data Factory back end.\"\nhttps://learn.microsoft.com/en-us/answers/questions/568057/advanced-feature-branch-development"
      },
      {
        "date": "2024-08-27T03:34:00.000Z",
        "voteCount": 1,
        "content": "No. Changes are published immediately only from the collaboration branch. But the question says that we are working in a feature branch. \nThe answer should be \"when the changes are merged into the collaboration branch\""
      },
      {
        "date": "2023-08-30T03:22:00.000Z",
        "voteCount": 2,
        "content": "upon publishing changes to the service"
      },
      {
        "date": "2023-07-23T02:43:00.000Z",
        "voteCount": 8,
        "content": "the first one should be \"upon publishing changes to the service\" . See https://learn.microsoft.com/en-us/azure/data-factory/source-control"
      },
      {
        "date": "2023-07-09T02:59:00.000Z",
        "voteCount": 5,
        "content": "Shouldn't we choose the 'upon saving the changes' option in the first dropdown?\nLink: https://learn.microsoft.com/en-us/azure/data-factory/source-control#stale-publish-branch"
      },
      {
        "date": "2023-06-21T03:05:00.000Z",
        "voteCount": 4,
        "content": "I did not manage to find a clear answer to this one, but based on cross-reading a few articles, I think \"in the region of data factory\" should be the correct answer, and this article explains it a bit better than the others I found: https://asankap.wordpress.com/2021/10/26/why-you-shouldnt-use-auto-resolve-integration-runtime-in-azure-data-factory-or-synapse/"
      },
      {
        "date": "2023-06-20T00:03:00.000Z",
        "voteCount": 1,
        "content": "When using the AutoResolveIntegrationRuntime with a Copy activity in Azure Data Factory that uses a linked service as the source, the copy operation will be performed in the region of the source data store.\n\nThe AutoResolveIntegrationRuntime is a system-assigned integration runtime that automatically routes data movement and activity dispatch to the optimal region based on the location of the source and sink data stores. When using a linked service as the source, the service will attempt to detect the location of the source data store and use an Integration Runtime in the same region to perform the copy operation."
      },
      {
        "date": "2023-06-19T14:32:00.000Z",
        "voteCount": 1,
        "content": "For copy activity, a best effort is made to automatically detect your sink data store's location, then use the IR in either the same region, if available, or the closest one in the same geography, otherwise; if the sink data store's region is not detectable, the IR in the instance's region is used instead."
      },
      {
        "date": "2023-06-13T18:20:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/microsoft/view/112073-exam-dp-203-topic-2-question-111-discussion/",
    "body": "HOTSPOT<br> -<br><br>In Azure Data Factory, you have a schedule trigger that is scheduled in Pacific Time.<br><br>Pacific Time observes daylight saving time.<br><br>The trigger has the following JSON file.<br><br><img src=\"https://img.examtopics.com/dp-203/image321.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image322.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image323.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T10:04:00.000Z",
        "voteCount": 20,
        "content": "1. two times\n2. will automatically adjust \n\"For time zones that observe daylight saving, trigger time will auto-adjust for the twice a year change, if the recurrence is set to Days or above. To opt out of the daylight saving change, please select a time zone that does not observe daylight saving, for instance UTC.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory#azure-data-factory-and-synapse-portal-experience"
      },
      {
        "date": "2023-11-28T03:05:00.000Z",
        "voteCount": 5,
        "content": "Why  is it two times instead of one time, if it will automatically adjust?"
      },
      {
        "date": "2024-02-01T08:36:00.000Z",
        "voteCount": 1,
        "content": "at 3:00 and at 21:00"
      },
      {
        "date": "2024-02-01T08:42:00.000Z",
        "voteCount": 3,
        "content": "Just for fun in Hungary in 2024. Oct 27. Sunday at 3:00 you should change the clock to 2:00. It means 2 times there is 3:00 next to each other. :D"
      },
      {
        "date": "2024-09-08T20:49:00.000Z",
        "voteCount": 1,
        "content": "The trigger will execute [answer choice] on Sunday, March 3, 2024:\ntwo times\nThe trigger [answer choice] daylight saving time:\nwill automatically adjust for"
      },
      {
        "date": "2024-02-14T04:07:00.000Z",
        "voteCount": 1,
        "content": "bing copilot:\n\nThe trigger is set to execute at 3 AM and 9 PM Pacific Time on Sundays. However, on March 3, 2024, daylight saving time begins in the Pacific Time zone, moving the clock forward by one hour. So, the trigger will execute only one time on Sunday, March 3, 2024.\nThe trigger operates in the Pacific Standard Time zone, which observes daylight saving time. However, the JSON file does not specify any adjustments for daylight saving time. Therefore, the trigger is unaffected by daylight saving time."
      },
      {
        "date": "2024-02-12T10:40:00.000Z",
        "voteCount": 1,
        "content": "Two times.\nWill automatically adjust. \n\nor time zones that observe daylight saving, trigger time will auto-adjust for the twice a year change, if the recurrence is set to Days or above. To opt out of the daylight saving change, please select a time zone that does not observe daylight saving, for instance UTC\nDaylight saving adjustment only happens for trigger with recurrence set to Days or above. If the trigger is set to Hours or Minutes frequency, it will continue to fire at regular intervals."
      },
      {
        "date": "2024-01-31T17:25:00.000Z",
        "voteCount": 1,
        "content": "1. two times\n2. will automatically adjust"
      },
      {
        "date": "2023-08-04T09:02:00.000Z",
        "voteCount": 3,
        "content": "There's a catch here, as daylight savings actually starts on the SECOND Sunday of March, and March 3 2024 is before this date."
      },
      {
        "date": "2023-06-19T14:39:00.000Z",
        "voteCount": 3,
        "content": "\"[...] we are also adding support for Daylight Saving auto-adjustment: for time zones that observe Daylight Saving, auto change schedule trigger time twice a year (e.g. 8AM daily trigger will fire at 8AM, whether it's PST or PDT)\"\n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/time-zone-and-daylight-saving-support-for-schedule-trigger/ba-p/1840199"
      },
      {
        "date": "2023-06-17T00:06:00.000Z",
        "voteCount": 2,
        "content": "2nd answer should be : will require an adjustment for\n\nref to : https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory\nThe timeZone element specifies the time zone that the trigger is created in. This setting affects both startTime and endTime."
      },
      {
        "date": "2023-06-13T14:53:00.000Z",
        "voteCount": 4,
        "content": "Agree, as of 2020 ADF supports auto-adjustemt for Daylight Saving in Schedule Triggers for time zones that aren't UTC. Since here we are using Pacific time, answer seems correct. Source: https://techcommunity.microsoft.com/t5/azure-data-factory-blog/time-zone-and-daylight-saving-support-for-schedule-trigger/ba-p/1840199"
      },
      {
        "date": "2023-06-13T13:38:00.000Z",
        "voteCount": 2,
        "content": "Azure Data Factory only supports time zones UTC. I think should requiret the adjustment."
      },
      {
        "date": "2023-06-19T14:40:00.000Z",
        "voteCount": 2,
        "content": "Incorrect. As of 2020 you can create schedule triggers in your local time zone, without the need to convert timestamps to Coordinated Universal Time (UTC) first."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111857-exam-dp-203-topic-2-question-112-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to create a pipeline that will execute a stored procedure in the dedicated SQL pool and use the returned result set as the input for a downstream activity. The solution must minimize development effort.<br><br>Which type of activity should you use in the pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tU-SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStored Procedure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScript\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNotebook"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-19T10:19:00.000Z",
        "voteCount": 14,
        "content": "For me the correct answer is C.\nThe store procedure activity doesn't return any data. \nIn the description of the script activity is written that it can be used for : \"Run stored procedures. If the SQL statement invokes a stored procedure that returns results from a temporary table, use the WITH RESULT SETS option to define metadata for the result set. \"\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"
      },
      {
        "date": "2023-06-21T03:24:00.000Z",
        "voteCount": 2,
        "content": "I also think this one is correct. One of the things script activity can do is \"...Save the rowset returned from a query as activity output for downstream consumption.\" which is pretty much what is needed here. This is not viable with 'execute SP' activity as it doesn't cannot return any data."
      },
      {
        "date": "2023-06-29T00:07:00.000Z",
        "voteCount": 7,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script\n\nThe script may contain either a single SQL statement or multiple SQL statements that run sequentially. You can use the Script task for the following purposes:\n\nTruncate a table in preparation for inserting data.\nCreate, alter, and drop database objects such as tables and views.\nRe-create fact and dimension tables before loading data into them.\nRun stored procedures. If the SQL statement invokes a stored procedure that returns results from a temporary table, use the WITH RESULT SETS option to define metadata for the result set.\nSave the rowset returned from a query as activity output for downstream consumption."
      },
      {
        "date": "2024-07-28T11:28:00.000Z",
        "voteCount": 1,
        "content": "I will go with Stored Procedure.. The requirement is - The solution must minimize development effort. If we go with Script, we will have to write a script which involve development efforts."
      },
      {
        "date": "2024-07-17T03:42:00.000Z",
        "voteCount": 1,
        "content": "\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet. \"\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"
      },
      {
        "date": "2024-07-15T00:39:00.000Z",
        "voteCount": 2,
        "content": "C is wrong"
      },
      {
        "date": "2024-06-12T02:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is script 100%"
      },
      {
        "date": "2024-04-06T01:54:00.000Z",
        "voteCount": 2,
        "content": "B. Stored Procedure\n\nExplanation:\nTo execute a stored procedure in an Azure Synapse Analytics dedicated SQL pool and use the returned result set as input for a downstream activity, you should use the Stored Procedure activity in the pipeline.\n\nThe Stored Procedure activity allows you to execute a stored procedure within the dedicated SQL pool and retrieve the results, which can then be used as input for subsequent activities in the pipeline. This approach minimizes development effort as it directly integrates with the SQL pool and provides a seamless way to execute stored procedures as part of your data processing"
      },
      {
        "date": "2024-02-18T00:59:00.000Z",
        "voteCount": 1,
        "content": "Got the feeling the question itself is referring to this GUI: ttps://stackoverflow.com/questions/72642013/pipeline-for-stored-procedure-dedicated-sql-pool\n\nSo the answer may be A as store procedure"
      },
      {
        "date": "2024-01-31T17:31:00.000Z",
        "voteCount": 1,
        "content": "Ans is C. Script"
      },
      {
        "date": "2024-01-11T06:26:00.000Z",
        "voteCount": 1,
        "content": "I got this question today in exam 11-jan-2024"
      },
      {
        "date": "2024-03-09T06:35:00.000Z",
        "voteCount": 1,
        "content": "What did you put for your answer?"
      },
      {
        "date": "2024-01-02T03:48:00.000Z",
        "voteCount": 1,
        "content": "The key is 'Output Result set support' which the stored procedure activity does not have. Therefore we have to use a script which supports running stored procedures.\n\nhttps://techcommunity.microsoft.com/t5/azure-data-factory-blog/execute-sql-statements-using-the-new-script-activity-in-azure/ba-p/3239969"
      },
      {
        "date": "2023-08-30T03:31:00.000Z",
        "voteCount": 1,
        "content": "C is corret"
      },
      {
        "date": "2023-07-04T09:06:00.000Z",
        "voteCount": 1,
        "content": "B. Chat GPT says the given answer is correct. Stored Procedure is specifically designed to execute stored procedures within Azure Synapse Analytics and is the most suitable option for the scenario, minimizing development effort."
      },
      {
        "date": "2023-06-23T07:27:00.000Z",
        "voteCount": 1,
        "content": "The \"Script\" activity in Azure Data Factory is primarily used to run HDInsight scripts such as Hive, Pig, MapReduce, and Spark. These are typically used for big data processing tasks."
      },
      {
        "date": "2023-06-29T00:06:00.000Z",
        "voteCount": 5,
        "content": "False, finally I've found the link, it's C:\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"
      },
      {
        "date": "2023-06-19T14:42:00.000Z",
        "voteCount": 2,
        "content": "\"In Azure Synapse Analytics, you can use the SQL pool Stored Procedure Activity to invoke a stored procedure in a dedicated SQL pool.\"\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/sql-pool-stored-procedure-activity"
      },
      {
        "date": "2023-06-10T17:41:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/sql-pool-stored-procedure-activity"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/microsoft/view/112294-exam-dp-203-topic-2-question-113-discussion/",
    "body": "You have an Azure SQL database named DB1 and an Azure Data Factory data pipeline named pipeline1.<br><br>From Data Factory, you configure a linked service to DB1.<br><br>In DB1, you create a stored procedure named SP1. SP1 returns a single row of data that has four columns.<br><br>You need to add an activity to pipeline1 to execute SP1. The solution must ensure that the values in the columns are stored as pipeline variables.<br><br>Which two types of activities can you use to execute SP1? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScript\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStored Procedure"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-04T23:25:00.000Z",
        "voteCount": 7,
        "content": "https://learn.microsoft.com/en-us/answers/questions/925742/how-to-process-output-from-stored-procedure-in-azu\n\nSP Activity does not capture result.. use lookup instead of script"
      },
      {
        "date": "2023-07-28T11:27:00.000Z",
        "voteCount": 6,
        "content": "why not CD? \nA Lookup activity can be used to execute a query or stored procedure against a data source and retrieve a single row of data. The returned values can then be stored as pipeline variables and used in subsequent activities.\n\nA Stored Procedure activity can be used to directly execute a stored procedure against a data source. The returned values can be captured as output parameters and stored as pipeline variables for use in subsequent activities."
      },
      {
        "date": "2024-08-24T11:23:00.000Z",
        "voteCount": 1,
        "content": "correct answers are A and C\n\n\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\n\nIf you call a stored procedure with output parameters using stored procedure activity, following error occurs.\n\nExecution fail against sql server. Please contact SQL Server team if you need further support. Sql error number: 201. Error Message: Procedure or function 'sp_name' expects parameter '@output_param_name', which was not supplied.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"
      },
      {
        "date": "2024-04-26T08:16:00.000Z",
        "voteCount": 2,
        "content": "correct answer is C and D"
      },
      {
        "date": "2024-04-25T18:29:00.000Z",
        "voteCount": 1,
        "content": "AC\ncannot capture SP output params with sP activity"
      },
      {
        "date": "2024-02-12T10:55:00.000Z",
        "voteCount": 2,
        "content": "When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup activity and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"
      },
      {
        "date": "2024-01-31T17:47:00.000Z",
        "voteCount": 1,
        "content": "A. Script &amp; \nC. Lookup \nare the correct answers\nIncorrect\nD.Stored procedure activity doesn't return any values. \nB.Copy Activity is meant for copying data only"
      },
      {
        "date": "2023-09-08T18:06:00.000Z",
        "voteCount": 1,
        "content": "Script lookup"
      },
      {
        "date": "2023-09-03T10:18:00.000Z",
        "voteCount": 1,
        "content": "Here is an example of how we can use the Script activity to execute SP1:\n\nScript activity (name: \"ExecuteSP1Script\")\n{\n  ScriptSource = \"&lt;![CDATA[\n  var results = SqlCommand('EXEC SP1', connection);\n  var myVar = results[0];\n  ]]&gt;\"\n}\nIn this example, the ScriptSource property specifies the script that is used to execute SP1. The script first executes the SQL statement EXEC SP1. The script then stores the results of SP1 in the variable myVar.\n\nCorrect Answers are: A &amp; D"
      },
      {
        "date": "2023-08-30T03:46:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-07-04T23:24:00.000Z",
        "voteCount": 1,
        "content": "sorry, Answer - AC"
      },
      {
        "date": "2023-07-04T23:23:00.000Z",
        "voteCount": 3,
        "content": "Answer is wrong - https://learn.microsoft.com/en-us/answers/questions/925742/how-to-process-output-from-stored-procedure-in-azu\n\nAnswer is CD. SP act cannot emit result.."
      },
      {
        "date": "2023-06-25T22:59:00.000Z",
        "voteCount": 2,
        "content": "CD is the rite answer"
      },
      {
        "date": "2023-06-19T14:47:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"
      },
      {
        "date": "2023-06-15T23:38:00.000Z",
        "voteCount": 3,
        "content": "C. Lookup\nD. Stored Procedure\n\nExplanation:\n\nC. Lookup: The Lookup activity is used to retrieve a dataset from a data source and the output can be used in subsequent activities. It is often used to fetch a small amount of data to be used as parameters in other activities. In this case, it can be used to execute the stored procedure and capture the result into pipeline variables.\n\nD. Stored Procedure: The Stored Procedure activity is used specifically to execute stored procedures. You can capture the output of the stored procedure and assign it to pipeline variables. This activity is designed specifically for executing stored procedures, making it a direct option for this requirement.\n\nA. Script: There is no \"Script\" activity in Azure Data Factory.\n\nB. Copy: The Copy activity is primarily used for copying data from a source to a destination and is not suitable for executing a stored procedure and capturing its output into pipeline variables."
      },
      {
        "date": "2023-06-19T14:46:00.000Z",
        "voteCount": 4,
        "content": "There is Script activity in ADF.\n\n\"The script may contain either a single SQL statement or multiple SQL statements that run sequentially. You can use the Script task for the following purposes:\n[...]\nRun stored procedures. [...]\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-script"
      },
      {
        "date": "2023-06-15T12:21:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer is C and D."
      },
      {
        "date": "2023-06-19T14:48:00.000Z",
        "voteCount": 1,
        "content": "You use lookup to consume, not to get."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/microsoft/view/118994-exam-dp-203-topic-2-question-114-discussion/",
    "body": "You have an Azure data factory named ADF1.<br><br>You currently publish all pipeline authoring changes directly to ADF1.<br><br>You need to implement version control for the changes made to pipeline artifacts. The solution must ensure that you can apply version control to the resources currently defined in the Azure Data Factory Studio for ADF1.<br><br>Which two actions should you perform? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure Data Factory Studio, run Publish All.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Data Factory trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Git repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a GitHub action.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure Data Factory Studio, select Set up code repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure Data Factory Studio, select Publish."
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-24T19:38:00.000Z",
        "voteCount": 9,
        "content": "Answer is highly correct. I know this one for sure."
      },
      {
        "date": "2024-09-19T09:32:00.000Z",
        "voteCount": 1,
        "content": "One of those questions, let's try to see the catch.\n\n1. To set up or configure source code control in ADF, the source code repository must be created in advance.\n2. Once source code is enabled, pipelines will run from a publishing branch, from the repo.\n\nSo...\nC. You have to create a repo before setting it up in ADF. ADF does not create repos.\nE. You link ADF to a repo by hitting \"Set up\" from the main page. But from the Manager tab, the button's label is \"Configure\".\nF. Once git is set, it runs pipelines from the publishing branch it creates when you set git. If you don't Publish and try to run the pipeline, ADF yields an error. The question does not require you to run right away but if you don't Publish, it will never run."
      },
      {
        "date": "2024-02-12T09:35:00.000Z",
        "voteCount": 1,
        "content": "It's correct"
      },
      {
        "date": "2024-01-02T05:33:00.000Z",
        "voteCount": 3,
        "content": "Version control = always github"
      },
      {
        "date": "2024-09-19T09:35:00.000Z",
        "voteCount": 1,
        "content": "Can be Azure DevOps Git too. ;-)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/microsoft/view/118995-exam-dp-203-topic-2-question-115-discussion/",
    "body": "You have an Azure data factory named ADF1 that contains a pipeline named Pipeline1.<br><br>Pipeline1 must execute every 30 minutes with a 15-minute offset.<br><br>You need to create a trigger for Pipeline1. The trigger must meet the following requirements:<br><br>\u2022\tBackfill data from the beginning of the day to the current time.<br>\u2022\tIf Pipeline1 fails, ensure that the pipeline can re-execute within the same 30-minute period.<br>\u2022\tEnsure that only one concurrent pipeline execution can occur.<br>\u2022\tMinimize development and configuration effort.<br><br>Which type of trigger should you create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tevent-based",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling window\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-26T08:24:00.000Z",
        "voteCount": 1,
        "content": "almost always it's tumbling window"
      },
      {
        "date": "2023-12-30T07:22:00.000Z",
        "voteCount": 2,
        "content": "Tumbling window because of the backfill"
      },
      {
        "date": "2023-12-12T03:33:00.000Z",
        "voteCount": 1,
        "content": "data is expected to arrive at regular intervals, and you want to trigger a pipeline with a fixed window size.\n\nFor the specific requirements mentioned, especially the need to execute every 30 minutes with a 15-minute offset and backfill data from the beginning of the day to the current time, a schedule trigger is more suitable. The tumbling window trigger is generally used for scenarios where you want to process data in fixed windows based on its arrival time."
      },
      {
        "date": "2023-09-21T23:41:00.000Z",
        "voteCount": 3,
        "content": "Tumbling window is correct.\nBackfill scenario: https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell#execution-order-of-windows-in-a-backfill-scenario\noffset, concurrency, ... :\nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger?tabs=data-factory%2Cazure-powershell#tumbling-window-trigger-type-properties"
      },
      {
        "date": "2023-09-11T08:58:00.000Z",
        "voteCount": 2,
        "content": "schedule because it occurs every 30 minutes"
      },
      {
        "date": "2023-08-30T04:09:00.000Z",
        "voteCount": 2,
        "content": "seem to correct"
      },
      {
        "date": "2023-08-24T19:41:00.000Z",
        "voteCount": 4,
        "content": "It seems correct but the 15mins offset is throwing me off. Somebody please explain. Thanks"
      },
      {
        "date": "2023-10-23T12:28:00.000Z",
        "voteCount": 2,
        "content": "Ensure that only one concurrent pipeline execution can occur &lt;--- this suggests the tumbling window"
      },
      {
        "date": "2024-01-08T12:56:00.000Z",
        "voteCount": 1,
        "content": "offset refers to the delay of start of the trigger : A timespan value that must be negative in a self-dependency. If no value specified, the window is the same as the trigger itself."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125170-exam-dp-203-topic-2-question-116-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account named account1 and an Azure event hub named Hub1. Data is written to account1 by using Event Hubs Capture.<br><br>You plan to query account by using an Apache Spark pool in Azure Synapse Analytics.<br><br>You need to create a notebook and ingest the data from account1. The solution must meet the following requirements:<br><br>\u2022\tRetrieve multiple rows of records in their entirety.<br>\u2022\tMinimize query execution time.<br>\u2022\tMinimize data processing.<br><br>Which data format should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet -<br>O. Avro\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tORC",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-02T04:15:00.000Z",
        "voteCount": 5,
        "content": "Answer is B, here is why:\n\nAvro and Parquet are both binary compressed file formats which makes them preferred over CSV which stores the data as strings (much larger files).\n\nNow the difference between Parquet and Avro is the format, as Parquet is column based while Avro provides row based store. Since the requirement is to retrieve rows in their entirety, it is better to use Avro. Scenarios where we only retrieve a subset of columns for analysis would favour the use of Parquet."
      },
      {
        "date": "2024-03-25T00:21:00.000Z",
        "voteCount": 2,
        "content": "Avro, because its a row oriented storage offer slight advantage over Parquet which is columnar storage."
      },
      {
        "date": "2024-02-17T10:31:00.000Z",
        "voteCount": 1,
        "content": "AVRO, because parquet is better for columnar mode"
      },
      {
        "date": "2024-01-31T17:54:00.000Z",
        "voteCount": 3,
        "content": "AVRO ( just because we want to retrieve the data in its entirety )"
      },
      {
        "date": "2024-03-12T01:45:00.000Z",
        "voteCount": 2,
        "content": "Avro is a row-based format and can be a good choice when you need to read or write individual records in a stream. However, for analytical queries that often involve reading multiple rows at once, columnar formats like Parquet are typically more efficient."
      },
      {
        "date": "2024-01-20T15:27:00.000Z",
        "voteCount": 2,
        "content": "Parquet is the clear winner"
      },
      {
        "date": "2024-06-21T15:20:00.000Z",
        "voteCount": 1,
        "content": "Parquet will be performing always better considering the fact of read operation. Avro are good for write performance. But parquets are best for analytical and read operations"
      },
      {
        "date": "2023-11-02T03:45:00.000Z",
        "voteCount": 3,
        "content": "Parquet showed either similar or better results on every test [than Avro]. The query-performance differences on the larger datasets in Parquet\u2019s favor are partly due to the compression results; when querying the wide dataset, Spark had to read 3.5x less data for Parquet than Avro. Avro did not perform well when processing the entire dataset, as suspected.\"\n\nR: https://blog.cloudera.com/benchmarking-apache-parquet-the-allstate-experience/"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125855-exam-dp-203-topic-2-question-117-discussion/",
    "body": "You have an Azure Blob Storage account named blob1 and an Azure Data Factory pipeline named pipeline1.<br><br>You need to ensure that pipeline1 runs when a file is deleted from a container in blob1. The solution must minimize development effort.<br><br>Which type of trigger should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstorage event\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcustom event"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-12T08:30:00.000Z",
        "voteCount": 5,
        "content": "\"A Storage Event trigger in Azure Data Factory is designed to initiate a pipeline in response to an event happening in Azure Blob Storage, such as the deletion of a file\"\n\nSo B."
      },
      {
        "date": "2024-09-18T11:04:00.000Z",
        "voteCount": 1,
        "content": "Source: https://learn.microsoft.com/en-us/azure/data-factory/how-to-create-event-trigger?tabs=data-factory"
      },
      {
        "date": "2024-03-03T03:10:00.000Z",
        "voteCount": 1,
        "content": "I would go with storage event as well"
      },
      {
        "date": "2024-02-24T00:50:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125594-exam-dp-203-topic-2-question-118-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have Azure Data Factory configured with Azure Repos Git integration. The collaboration branch and the publish branch are set to the default values.<br><br>You have a pipeline named pipeline1.<br><br>You build a new version of pipeline1 in a branch named feature1.<br><br>From the Data Factory Studio, you select Publish.<br><br>The source code of which branch will be built, and which branch will contain the output of the Azure Resource Manager (ARM) template? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image348.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image349.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-28T09:38:00.000Z",
        "voteCount": 2,
        "content": "Feature1 &amp; adf_publish"
      },
      {
        "date": "2024-02-20T07:37:00.000Z",
        "voteCount": 3,
        "content": "The given answer is correct. The diagram in this documentation gives a clear picture of the CI/CD process: https://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery#cicd-lifecycle"
      },
      {
        "date": "2024-01-02T04:35:00.000Z",
        "voteCount": 3,
        "content": "Correct, publish branch contains only ADF related code in JSON format. All the source code can be found in the collaboration branch which is by default the main branch.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/source-control"
      },
      {
        "date": "2023-11-07T09:17:00.000Z",
        "voteCount": 2,
        "content": "Seems correct, as there was not a PR of the changes made in the feature branch. Thus, if you \"Publish\" the built will be based on what is in the collab branch, not in the feature branch."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130645-exam-dp-203-topic-2-question-119-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure subscription that contains an Azure data factory.<br><br>You are editing an Azure Data Factory activity JSON.<br><br>The script needs to copy a file from Azure Blob Storage to multiple destinations. The solution must ensure that the source and destination files have consistent folder paths.<br><br>How should you complete the script? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image363.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image364.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-08T13:05:00.000Z",
        "voteCount": 7,
        "content": "Correct, refer to https://learn.microsoft.com/en-us/azure/data-factory/control-flow-for-each-activity"
      },
      {
        "date": "2024-05-12T05:02:00.000Z",
        "voteCount": 2,
        "content": "ForEach - As multiple destination\nPreserve Hierachy - Consistent folder structure between Source and Target"
      },
      {
        "date": "2024-04-02T11:06:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130621-exam-dp-203-topic-2-question-120-discussion/",
    "body": "You are building a data flow in Azure Data Factory that upserts data into a table in an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to add a transformation to the data flow. The transformation must specify logic indicating when a row from the input data must be upserted into the sink.<br><br>Which type of transformation should you add to the data flow?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tjoin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\talter row\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsurrogate key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tselect"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T11:08:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-03-09T05:58:00.000Z",
        "voteCount": 1,
        "content": "Use the Alter Row transformation to set insert, delete, update, and upsert policies on rows. \nRef: https://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row"
      },
      {
        "date": "2024-01-08T11:58:00.000Z",
        "voteCount": 2,
        "content": "correct, chatGPT:\nFor upsert operations (insert or update), you typically need to determine whether a record already exists in the destination table based on some condition. In Azure Data Factory's data flow, you would use the \"Alter Row\" transformation for this purpose."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130620-exam-dp-203-topic-2-question-121-discussion/",
    "body": "You have an on-premises database named db1 and a set-hosted integration runtime.<br><br>You have an Azure subscription that contains an Azure Data Lake Storage account named dl1.<br><br>You need to develop four data pipeline projects that will use Microsoft Power Query to copy data from db1 to dl1. The solution must meet the following requirements:<br><br>\u2022\tAll pipelines must use the self-hosted integration runtime.<br>\u2022\tEach project must be stored in a separate Git repository.<br>\u2022\tDevelopment effort must be minimized.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Logic Apps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Power BI"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-14T19:51:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-12T03:33:00.000Z",
        "voteCount": 3,
        "content": "To develop data pipeline projects that use Microsoft Power Query and meet the specified requirements, you should use Azure Data Factory (option C). Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines that can move data between supported on-premises and cloud-based data stores.\n\nWith Azure Data Factory, you can use Power Query to transform and prepare your data before moving it to the destination. It supports the use of self-hosted integration runtimes for on-premises data movement."
      },
      {
        "date": "2024-01-08T11:58:00.000Z",
        "voteCount": 1,
        "content": "correct, chat gpt confirmed"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130463-exam-dp-203-topic-2-question-122-discussion/",
    "body": "You have the Azure Synapse Analytics pipeline shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image365.png\"><br><br>You need to add a set variable activity to the pipeline to ensure that after the pipeline\u2019s completion, the status of the pipeline is always successful.<br><br>What should you configure for the set variable activity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta skipped dependency on the Upon Failure activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta skipped dependency on the Upon Success activity\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta success dependency on the Business Activity That Fails activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta failure dependency on the Upon Failure activity"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T12:54:00.000Z",
        "voteCount": 3,
        "content": "In this approach, customer defines the business logic, and defines both the Upon Failure path, and Upon Success path, with a dummy Upon Skipped activity attached. This approach renders pipeline succeeds, if Upon Failure path succeeds."
      },
      {
        "date": "2024-03-29T08:40:00.000Z",
        "voteCount": 1,
        "content": "for sure the correct answer.\n\ndirectly from docs --&gt; https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling#do-if-skip-else-block"
      },
      {
        "date": "2024-01-13T07:28:00.000Z",
        "voteCount": 4,
        "content": "Correct.\nhttps://learn.microsoft.com/es-es/azure/data-factory/tutorial-pipeline-failure-error-handling"
      },
      {
        "date": "2024-01-11T14:57:00.000Z",
        "voteCount": 4,
        "content": "Answer B, tested in ADF"
      },
      {
        "date": "2024-01-06T07:01:00.000Z",
        "voteCount": 3,
        "content": "Should be B, creating an 'Do If Skip Else' statement by adding a skipped dependency to the success path means that the pipeline is always succeeded."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130706-exam-dp-203-topic-2-question-123-discussion/",
    "body": "You have an on-premises Linux server that contains a database named DB1.<br><br>You have an Azure subscription that contains an Azure data factory named ADF1 and an Azure Data Lake Storage account named ADLS1.<br><br>You need to create a pipeline in ADF1 that will copy data from DB1 to ADLS1.<br><br>Which type of integration runtime should you use to read the data from DB1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tself-hosted integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure-SQL Server Integration Services (SSIS)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-31T20:19:00.000Z",
        "voteCount": 1,
        "content": "It's kind of a toss up here, but the question does specify to \"create a Data Factory pipeline to copy data\".  ONLY if it requested to \"Create a pipeline with an Execute SSIS Package activity\" would it be better to choose SSIS runtime."
      },
      {
        "date": "2024-07-02T04:54:00.000Z",
        "voteCount": 3,
        "content": "ChatGPT confirmed self-hosted integration runtime"
      },
      {
        "date": "2024-06-24T10:19:00.000Z",
        "voteCount": 1,
        "content": "the self-hosted integration runtime cannot be installed directly on a Linux server, you can set it up on a Windows machine within your network to facilitate data movement between an on-premises Linux database and Azure services."
      },
      {
        "date": "2024-06-21T15:41:00.000Z",
        "voteCount": 1,
        "content": "A. Self Hosted integration run times will work but SHIR need to be installed on a windows machine in same network of Linux with supported DB driver. Its not easy but still very much possible."
      },
      {
        "date": "2024-06-21T15:46:00.000Z",
        "voteCount": 1,
        "content": "After thinking, I think it will be better to use SSIS."
      },
      {
        "date": "2024-05-29T14:32:00.000Z",
        "voteCount": 1,
        "content": "Although the self-hosted IR runs on Windows, it can connect to databases hosted on Linux servers. This is possible because the self-hosted IR uses database connectors and drivers that support remote connections to databases on different operating systems."
      },
      {
        "date": "2024-03-11T08:19:00.000Z",
        "voteCount": 1,
        "content": "SSIS is correct, albeit a bit overkill, because self hosted can only run on windows, this makes it the best option"
      },
      {
        "date": "2024-02-24T14:59:00.000Z",
        "voteCount": 2,
        "content": "This was a hard question, but indeed apparently Self-hosted integration runtime only supports windows, and Azure is for cloud platforms, so the answer is the SSIS, though it's a bit overkill because SSIS is meant for transformations"
      },
      {
        "date": "2024-01-20T15:54:00.000Z",
        "voteCount": 1,
        "content": "Azure-SQL Server Integration Services (SSIS) is currently supported for connecting to on-premises Linux servers. The other two options aren't."
      },
      {
        "date": "2024-01-15T13:41:00.000Z",
        "voteCount": 1,
        "content": "C\nAs self-hosted integration runtime is only supported in Windows OS, Azure IR only supports withing Azure cloud platforms"
      },
      {
        "date": "2024-01-09T07:25:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130995-exam-dp-203-topic-2-question-124-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Data Lake Storage account named account1.<br><br>You use an Azure Synapse Analytics serverless SQL pool to access sales data stored in account1.<br><br>You need to create a bar chart that displays sales by product. The solution must minimize development effort.<br><br>In which order should you perform the actions? To answer, move all actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-203/image366.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image367.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-20T06:39:00.000Z",
        "voteCount": 1,
        "content": "It's Correct guys"
      },
      {
        "date": "2024-02-24T01:10:00.000Z",
        "voteCount": 2,
        "content": "correct!"
      },
      {
        "date": "2024-01-12T18:43:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/microsoft/view/131719-exam-dp-203-topic-2-question-125-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to create a copy of the data warehouse and make the copy available for 28 days. The solution must minimize costs.<br><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-203/image368.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image369.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-29T06:19:00.000Z",
        "voteCount": 1,
        "content": "Kind of a weird way to ask this. The answer doesn't state it's a backup, it's solely a copy. You can't really infer from the details that it should be paused after copying."
      },
      {
        "date": "2024-01-21T02:01:00.000Z",
        "voteCount": 2,
        "content": "Correct.\n\nIn case you're looking for a Long-Term Backup (LTR) concept:\n\nCreate a new user-defined restore point, or you can use one of the automatically generated restore points.\nRestore from the newly created restore point to a new data warehouse.\nAfter you have restored, you have the dedicated SQL pool online. Pause it indefinitely to save compute costs. The paused database incurs storage charges at the Azure Synapse storage rate.\nIf you need an active copy of the restored data warehouse, you can resume, which should take only a few minutes.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/backup-and-restore?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130933-exam-dp-203-topic-2-question-126-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics workspace that contains an Apache Spark pool named Pool1.<br><br>You need to read data from a CSV file and write the data to a Delta table by using Pool1.<br><br>How should you complete the PySpark code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image370.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image371.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-18T23:00:00.000Z",
        "voteCount": 1,
        "content": "The answers are correct.\n\nBesides the link from @GermanGerman, check the databricks syntax here:\nhttps://docs.databricks.com/en/delta/tutorial.html\n\nThe page has only \"forName\". For the question, the similar \"forPath\" is the choice."
      },
      {
        "date": "2024-01-13T06:27:00.000Z",
        "voteCount": 3,
        "content": "correct. https://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-databricks/03-create-delta-tables"
      },
      {
        "date": "2024-01-12T02:38:00.000Z",
        "voteCount": 2,
        "content": "df.cache() ,convertToDelta()"
      },
      {
        "date": "2024-04-26T11:39:00.000Z",
        "voteCount": 2,
        "content": "why, explain?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130652-exam-dp-203-topic-2-question-127-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Lake Storage account that contains one CSV file per hour for January 1, 2020, through January 31, 2023. The files are partitioned by using the following folder structure.<br><br><img src=\"https://img.examtopics.com/dp-203/image372.png\"><br><br>You need to query the files by using an Azure Synapse Analytics serverless SQL pool. The solution must return the row count of each file created during the last three months of 2022.<br><br>How should you complete the query? To answer, select the appropriate options in the answer area.<br><br><img src=\"https://img.examtopics.com/dp-203/image373.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image374.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-08T13:33:00.000Z",
        "voteCount": 5,
        "content": "correct: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-specific-files"
      },
      {
        "date": "2024-08-01T19:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer\nMy reason: from the script provided, particularly from here\n                     \"From OPENROWSET ( BULK 'csv/system1/2022/*/*.csv',\nThe search on the file path uses a wildcard that begins after the year 2022 (i.e. the asterisks) and the first parameter is the month after 2022. \nHence, r.filepath(1) is correct."
      },
      {
        "date": "2024-06-21T23:22:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT:\nGiven the typical folder structure in your case, which is csv/system1/{year}/{month}/{filename}.csv, you need to access specific segments of this path to filter data by month:\n\nr.filepath(0) would return the first segment after the initial folder, likely csv.\nr.filepath(1) would then return system1.\nr.filepath(2) refers to the next segment, which, according to your folder structure, represents the year (2022).\nSo answer is r.filepath(2)."
      },
      {
        "date": "2024-03-07T13:56:00.000Z",
        "voteCount": 2,
        "content": "Correct answer"
      },
      {
        "date": "2024-02-14T05:08:00.000Z",
        "voteCount": 1,
        "content": "r.filepath(2) rerurn year (2022)"
      },
      {
        "date": "2024-01-21T02:36:00.000Z",
        "voteCount": 3,
        "content": "Correct. The term \"first wildcard\" refers to the asterisk that represents the month in the file path placeholder. For this specific example."
      },
      {
        "date": "2024-01-18T09:11:00.000Z",
        "voteCount": 4,
        "content": "Answers are correct:\nAs per next example, parameter (1) refers to year, and paremeter (2) to month.\n\nSELECT\n    r.filepath() AS filepath\n    ,r.filepath(1) AS [year]\n    ,r.filepath(2) AS [month]\n    ,COUNT_BIG(*) AS [rows]\nFROM OPENROWSET(\n        BULK 'csv/taxi/yellow_tripdata_*-*.csv',\n        DATA_SOURCE = 'SqlOnDemandDemo',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',        \n        FIRSTROW = 2\n    )\nWITH (\n    vendor_id INT\n) AS [r]\nWHERE\n    r.filepath(1) IN ('2017')\n    AND r.filepath(2) IN ('10', '11', '12')\nGROUP BY\n    r.filepath()\n    ,r.filepath(1)\n    ,r.filepath(2)\nORDER BY\n    filepath;"
      },
      {
        "date": "2024-02-02T01:31:00.000Z",
        "voteCount": 2,
        "content": "Just to be clear:\nThe right answer is filepath(1), because In your example the 2. parameter (star) is the month, but in the answer the year is fixed and the month is the first parameter."
      },
      {
        "date": "2024-01-16T05:47:00.000Z",
        "voteCount": 3,
        "content": "Second one is r.filepath(2) asper:\nWhen called without parameter, returns the full file path that a row originates from.\nWhen called with parameter, it returns part of path that matches the wildcard on position specified in the parameter. For example, parameter value 1 would return part of path that matches the first wildcard."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/microsoft/view/131086-exam-dp-203-topic-2-question-128-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool named Pool1 that contains an external table named Sales. Sales contains sales data. Each row in Sales contain data on a single sale, including the name of the salesperson.<br><br>You need to implement row-level security (RLS). The solution must ensure that the salespeople can access only their respective sales.<br><br>What should you do? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image375.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image376.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-07-14T22:13:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-04-04T05:29:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-13T07:42:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-13T06:42:00.000Z",
        "voteCount": 4,
        "content": "correct: https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/microsoft/view/132133-exam-dp-203-topic-2-question-129-discussion/",
    "body": "You have an Azure Data Factory pipeline named P1.<br><br>You need to schedule P1 to run at 10:15 AM, 12:15 PM, 2:15 PM, and 4:15 PM every day.<br><br>Which frequency and interval should you configure for the scheduled trigger?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrequency: Month -<br>Interval: 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrequency: Day -<br>Interval: 1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrequency: Minute -<br>Interval: 60",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrequency: Hour -<br>Interval: 2"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-25T06:11:00.000Z",
        "voteCount": 5,
        "content": "The frequency is a Day and interval 1, since every day should execute 4x. Within a day 4 times are fixed."
      },
      {
        "date": "2024-02-02T01:44:00.000Z",
        "voteCount": 1,
        "content": "Every 1 Day(s)\nExecutes at these times\nHours 10,12, 14, 16\nMinutes 15\nhttps://learn.microsoft.com/en-us/azure/data-factory/media/how-to-create-schedule-trigger/advanced.png\nsource: \nhttps://learn.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger?tabs=data-factory"
      },
      {
        "date": "2024-02-05T21:00:00.000Z",
        "voteCount": 1,
        "content": "Agreed it should be daily."
      },
      {
        "date": "2024-09-18T08:27:00.000Z",
        "voteCount": 1,
        "content": "Frequency is 1 day (every day).\nAfter choosing the frequency (recurrence), set the hours and the minutes.\nNo need to create one for each hour, because the setting accepts several hours and minutes.\n\nIn Linux, this would be a cron setting:\n15 10,12,14,16 * * *\n\nThe fields, separated by a space are:\nminute hour day-of-month month week-day\n\nThis means that the task will run:\nevery minute 15\non every hours 10, 12, 14 and 16\non every day of the month\non every month\nno matter which day-of-week it falls\n\nIt is possible to read that the frequency is \"every 2h, every day, every week, every month, and every year\", but for the Data Factory configuration, the recurrence is \"day\". Attention here: ADF calls it recurrence, not frequency. Looks the same, but the ogres that write these questions may decide to whatever they want.\n\nOther Linux cron doing the same, to be more clear:\n15 10-16/2 * * *\n15 10-16/2 */1 */1 *"
      },
      {
        "date": "2024-05-04T04:39:00.000Z",
        "voteCount": 1,
        "content": "I found this question on my exam 30/04/2024, and I put B. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-03-03T04:01:00.000Z",
        "voteCount": 2,
        "content": "Correct me if I am wrong guys, but if option B is correct does that mean you have to create 4 separate triggers with the Frequency: Day and Interval: 1?\n\nIf that is the case then I am more inclined to B, since D will still run even after 4:15PM. If we want it to run only for the 4 times specified above then B is a more accurate answer."
      },
      {
        "date": "2024-02-19T15:04:00.000Z",
        "voteCount": 2,
        "content": "Interval Day. You have to schedule 4 trigger but daily"
      },
      {
        "date": "2024-01-31T04:01:00.000Z",
        "voteCount": 4,
        "content": "The frequency of Hour with an interval of 2 would run the pipeline every 2 hours."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141945-exam-dp-203-topic-2-question-130-discussion/",
    "body": "You are creating an Azure Data Factory pipeline.<br><br>You need to add an activity to the pipeline. The activity must execute a Transact-SQL stored procedure that has the following characteristics:<br><br>\u2022\tReturns the number of sales invoices for a current date<br>\u2022\tDoes NOT require input parameters<br><br>Which type on activity should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStored Procedure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet Metadata",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend Variable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-18T08:41:00.000Z",
        "voteCount": 1,
        "content": "\"Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure\" - https://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity#:~:text=returns%20the%20result%20of%20executing%20a%20query%20or%20stored%20procedure\n\n\"When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet\" - https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure#:~:text=stored%20procedure%20activity%2C-,use%20lookup%20acitivty,-and%20Script%20activity"
      },
      {
        "date": "2024-07-05T02:47:00.000Z",
        "voteCount": 1,
        "content": "When the stored procedure has Output parameters, instead of using stored procedure activity, use lookup acitivty and Script activity. Stored procedure activity does not support calling SPs with Output parameter yet."
      },
      {
        "date": "2024-06-12T12:42:00.000Z",
        "voteCount": 2,
        "content": "The Stored Procedure Activity does not allow to return an output, so Lookup is the correct one. Refer to: https://learn.microsoft.com/en-us/azure/data-factory/transform-data-using-stored-procedure"
      },
      {
        "date": "2024-06-06T02:52:00.000Z",
        "voteCount": 1,
        "content": "Also ChatGPT:\nFor your specific requirement:\n\nRetrieve the Number of Sales Invoices for the Current Date:\nIf you simply need to call a stored procedure that returns the count and you want to use this count directly in your pipeline (e.g., for further processing or branching logic), the Lookup activity is preferable.\nIf the stored procedure's primary purpose is to perform operations (e.g., insert/update/delete) and the output is not directly needed within the pipeline, the Stored Procedure activity is more appropriate.\nGiven that your stored procedure returns the number of sales invoices and does not require input parameters, and assuming you want to use this count directly in the pipeline, the Lookup activity would be a good fit."
      },
      {
        "date": "2024-06-05T10:49:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt:\nThe Stored Procedure activity in Azure Data Factory is specifically designed to execute SQL stored procedures. It is the most suitable activity when you need to run a stored procedure and handle the results or output of that procedure.\nThe other options do not fit the requirements:\n\nB. Get Metadata is used to retrieve metadata information from data stores, not to execute stored procedures.\nC. Append Variable is used to append a value to an existing variable, not for executing stored procedures.\nD. Lookup is used to retrieve a dataset from a data store but is not typically used to execute stored procedures that return a single value without parameters."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141946-exam-dp-203-topic-2-question-131-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics workspace that contains three pipelines and three triggers named Trigger1, Trigger2, and Trigger3.<br><br>Trigger3 has the following definition.<br><br><img src=\"https://img.examtopics.com/dp-203/image385.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image386.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image387.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-07-03T04:22:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-06-26T03:22:00.000Z",
        "voteCount": 1,
        "content": "Strange: trigger is in disabled state then How it will fire :P"
      },
      {
        "date": "2024-06-26T03:24:00.000Z",
        "voteCount": 1,
        "content": "I mean stopped state just to be clear, we do this all the time for testing purposes."
      },
      {
        "date": "2024-06-05T10:58:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141760-exam-dp-203-topic-2-question-132-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Databricks deployment and a local file named /tmp/file1 that contains the following code.<br><br><img src=\"https://img.examtopics.com/dp-203/image388.png\"><br><br>You need to read /tmp/file1 into a data frame by using Scala.<br><br>How should you complete the code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image389.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image390.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-07-14T21:55:00.000Z",
        "voteCount": 2,
        "content": "The multiline option is necessary when reading JSON files that span multiple lines per record. In the given example, each JSON object in the array is spread over several lines, which is typical for nested or complex JSON structures. Without enabling the multiline option, Spark would expect each JSON record to be on a single line, and it would not be able to correctly parse the provided file."
      },
      {
        "date": "2024-06-29T13:03:00.000Z",
        "voteCount": 1,
        "content": "It should definitely be Multiline, json. The link shared by RutaP explains it."
      },
      {
        "date": "2024-06-14T10:46:00.000Z",
        "voteCount": 3,
        "content": "it should be Multiline and json I think. Refer this link https://learn.microsoft.com/en-us/azure/databricks/query/formats/json"
      },
      {
        "date": "2024-06-05T11:06:00.000Z",
        "voteCount": 3,
        "content": "Multiline, json"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141948-exam-dp-203-topic-2-question-133-discussion/",
    "body": "You have an Azure subscription that contains a Microsoft Purview account.<br><br>You need to search the Microsoft Purview Data Catalog to identify assets that have an assetType property of Table or View.<br><br>Which query should you run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassetType IN ('Table', 'View')",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassetType:Table OR assetType:view\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassetType = (Table OR View)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tassetType:(Table OR View)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T03:36:00.000Z",
        "voteCount": 1,
        "content": "Correct Purview syntax"
      },
      {
        "date": "2024-06-05T11:10:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "2"
  }
]