[
  {
    "topic": 20,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47073-exam-az-204-topic-20-question-1-discussion/",
    "body": "You need to ensure receipt processing occurs correctly.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob properties to prevent concurrency problems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob SnapshotTime to prevent concurrency problems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob metadata to prevent concurrency problems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob leases to prevent concurrency problems\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-17T21:42:00.000Z",
        "voteCount": 61,
        "content": "Answer is D: Use blob leases to prevent concurrency problems"
      },
      {
        "date": "2021-03-21T06:46:00.000Z",
        "voteCount": 3,
        "content": "I guess this could be a problem with the \"Processing\" scenario:\n\"Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.\"\nIt seems a lease will only be kept alive for 60 seconds, so it shouldn't be an option."
      },
      {
        "date": "2021-03-24T02:12:00.000Z",
        "voteCount": 4,
        "content": "Why should you avoid to use blob lease to guarantee access the blob file from the report link? Generally to give access to a blob from http you use SAS. In the SAS you can specify an expiration datetime according to your need. Blob lease is instead a way just to prevent concurrent access and puts a lock on blob only for write and delete operations. Any other can still view the content but can't modify or delete it. So in my opinion the correct answer is Blob lease. In addition I have found the same question on a udemy course test and the answer is just \"Blob Lease\".\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob"
      },
      {
        "date": "2021-03-30T23:49:00.000Z",
        "voteCount": 5,
        "content": "Agreed. The question is about the receipt processing. The case states \"Concurrent processing of a receipt must be prevented\", which can't be done with snapshots and leases are made for this specifically. So answer should be D."
      },
      {
        "date": "2021-03-23T22:57:00.000Z",
        "voteCount": 3,
        "content": "A lease does not lock a file for read operations, just for write and delete operations. The lock duration can be 15 to 60 seconds, or can be infinite.  So I guess the effect is the same?\nhttps://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob"
      },
      {
        "date": "2021-03-25T03:20:00.000Z",
        "voteCount": 11,
        "content": "I think it is lease (see this link)\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet"
      },
      {
        "date": "2023-07-02T08:46:00.000Z",
        "voteCount": 1,
        "content": "must be lease because if you use snapshot to do the processing another thread could write or update the original which could mean you are actually processing out of date data"
      },
      {
        "date": "2023-03-15T02:58:00.000Z",
        "voteCount": 3,
        "content": "D. Use blob leases to prevent concurrency problems.\n\nTo prevent concurrent processing of a receipt, we need to ensure that only one instance of the processor can access the receipt file at a given time. Blob leases can be used to implement this mechanism. A blob lease allows an application to acquire a lease on a blob for a specified period, during which no other application can modify the blob. When the lease expires or is released, another application can acquire the lease and modify the blob."
      },
      {
        "date": "2022-12-28T07:36:00.000Z",
        "voteCount": 1,
        "content": "The Lease Blob operation creates and manages a lock on a blob for write and delete operations. The lock duration can be 15 to 60 seconds, or can be infinite. In versions prior to 2012-02-12, the lock duration is 60 seconds \nhttps://learn.microsoft.com/en-us/rest/api/storageservices/lease-blob"
      },
      {
        "date": "2022-12-28T07:37:00.000Z",
        "voteCount": 1,
        "content": "also this \nPessimistic concurrency for blobs\nTo lock a blob for exclusive use, you can acquire a lease on it. When you acquire the lease, you specify the duration of the lease. A finite lease may be valid from between 15 to 60 seconds. A lease can also be infinite, which amounts to an exclusive lock. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet"
      },
      {
        "date": "2022-12-06T15:23:00.000Z",
        "voteCount": 1,
        "content": "B is correct, the issue is that one staff sent out the receipt but another staff deleted it. When the customer open, the receipt is gone. Extending the lease only give a limited time to access but it does not retain a snapshot of a copy for the customer to retrieve. So Snapshot is right."
      },
      {
        "date": "2022-12-06T15:28:00.000Z",
        "voteCount": 2,
        "content": "my bad, lease is right, it's a lock.\nhttps://learn.microsoft.com/en-us/rest/api/storageservices/lease-blob"
      },
      {
        "date": "2022-11-18T06:58:00.000Z",
        "voteCount": 1,
        "content": "D. Use blob leases to prevent concurrency problem"
      },
      {
        "date": "2022-10-02T05:22:00.000Z",
        "voteCount": 1,
        "content": "Sure, BlobLease is an option but that does not require me to read a huge scenario. I wonder what the scenario mean for this question. And I wonder why not A as well because Etag is a \"property\" !!"
      },
      {
        "date": "2022-08-29T06:27:00.000Z",
        "voteCount": 1,
        "content": "I think this one will close the discussion\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet\n\nTo lock a blob for exclusive use, you can acquire a lease on it.\nAnswer is D"
      },
      {
        "date": "2022-06-16T08:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct, although I have managed to solve such problem in real life using blobs metadata so I am not sure whether C could be correct aswell. But D is safer bet"
      },
      {
        "date": "2022-03-06T03:55:00.000Z",
        "voteCount": 2,
        "content": "Answer: D. Use blob leases to prevent concurrency problems \nReason: To lock a blob for exclusive use, you can acquire a lease on it. When you acquire the lease, you specify the duration of the lease. A finite lease may be valid from between 15 to 60 seconds. \n\nReference: https://docs.microsoft.com/en-us/azure/storage/blobs/concurrency-manage?tabs=dotnet#pessimistic-concurrency-for-blobs"
      },
      {
        "date": "2022-02-23T07:45:00.000Z",
        "voteCount": 3,
        "content": "Answer is D"
      },
      {
        "date": "2022-02-22T07:46:00.000Z",
        "voteCount": 2,
        "content": "Use blob leases to prevent concurrency problems"
      },
      {
        "date": "2022-01-28T23:18:00.000Z",
        "voteCount": 2,
        "content": "For processing purposes \"lease\" is the simplest option."
      },
      {
        "date": "2022-01-16T03:31:00.000Z",
        "voteCount": 1,
        "content": "It's quite vague. Receipts are uploaded to Azure Files. Some people place a file, wait for a while, then delete it, and upload it via the webinterface. Should we cater for this situation as well where duplicates may be introduced?\nFile will be copied to blob by the code, but it is not clear how long it will stay there, and how to prevent rereading the same file. I would either use blob storage events, or timestamps to filter the files to be processed. If there is a risk to read the same file twice (shortly after upload when filtering on timestamp) then a lease seems ok: if you can't acquire the lease then another instance of your code is already working on it."
      },
      {
        "date": "2022-01-16T03:37:00.000Z",
        "voteCount": 1,
        "content": "I guess the concurrency is caused by the CRON job every 5 minutes. If processing of the available files takes longer than 5 minutes, concurrency issues may be caused."
      },
      {
        "date": "2021-09-19T03:31:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/rest/api/storageservices/lease-blob\n\nThe answer would be Blob lease.\n\nRelease, to free the lease if it is no longer needed so that another client may immediately acquire a lease against the blob."
      },
      {
        "date": "2021-09-20T17:25:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2021-04-09T20:28:00.000Z",
        "voteCount": 2,
        "content": "Given answer is correct"
      },
      {
        "date": "2021-06-21T09:24:00.000Z",
        "voteCount": 2,
        "content": "No, it's not. It's D, use blob leases."
      }
    ],
    "examNameCode": "az-204",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48600-exam-az-204-topic-20-question-2-discussion/",
    "body": "You need to resolve the capacity issue.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the trigger on the Azure Function to an Azure Blob storage trigger",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the consumption plan is configured correctly to allow scaling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the Azure Function to a dedicated App Service Plan",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the loop starting on line PC09 to process items in parallel\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-05T07:45:00.000Z",
        "voteCount": 30,
        "content": "Receipts are uploaded to the File Storage (not Blob Storage) which does not support triggers.\nConcurrent processing of a (SINGLE!) receipt must be prevented - so parallel processing is OK.\nSo answer D."
      },
      {
        "date": "2021-07-27T10:57:00.000Z",
        "voteCount": 3,
        "content": "1000% D !!!!!!!! CORRECT!"
      },
      {
        "date": "2021-04-29T04:59:00.000Z",
        "voteCount": 22,
        "content": "Cleared AZ-204 today, the question appeared, the option \"D\" was not there, but a \"replace the solution with durable functions\". I went for that."
      },
      {
        "date": "2021-05-26T23:59:00.000Z",
        "voteCount": 3,
        "content": "Durable functions will let the consumer get an immediate (async) response, but the processing remains. The duration till the file appears on the website doesn't change. \nDoing the processing in parallel will make a change."
      },
      {
        "date": "2021-08-24T09:41:00.000Z",
        "voteCount": 3,
        "content": "Correct, if one instance of time trigger function is running, then there will not be a second instance starts, even when 5 minutes pass ... For a durable function, it can make sure immediate returns to allow second instance to start ..."
      },
      {
        "date": "2022-01-28T23:28:00.000Z",
        "voteCount": 2,
        "content": "That makes sense. Running parallel tasks is not good practice for functions. Here we cannot predict the degree of parallelizm. But using durable function is the best choice."
      },
      {
        "date": "2022-01-23T11:08:00.000Z",
        "voteCount": 1,
        "content": "If that is the case, the answer should be C?"
      },
      {
        "date": "2023-10-20T04:45:00.000Z",
        "voteCount": 4,
        "content": "I believe it's \"B\" - since issue happens in busy period when CPU is over-utilized. Then only reasonable action will be to scale. And for that we should properly configure Consumption Plan. \n\"D\" - could be an answer if Q was about slow speed of processing in normal situation, when CPU resources in enough. In this case, I/O operations are the bottleneck. But, if we try to spawn more thread when CPU is already super-busy, it would even worsen user experience.\nAnd it's not \"C\" since Dedicated Plan is used in very specific situation. Exerpt:\n\"Consider a dedicated App Service plan in the following situations:\n- You have existing, underutilized VMs that are already running other App Service instances.\n- You want to provide a custom image on which to run your functions.\""
      },
      {
        "date": "2022-11-18T07:00:00.000Z",
        "voteCount": 1,
        "content": "D. Update the loop starting on line PC09 to process items in parallel"
      },
      {
        "date": "2022-10-02T05:46:00.000Z",
        "voteCount": 2,
        "content": "\"Concurrent processing of a receipt must be prevented.\" \nMicrosoft has added this line as a red-herring to make the question taker not think parallelism as an option? What does \"processing\" mean here? What is \"a receipt\"? That in combination with the listFiles() method. Does \"a receipt\" contain multiple files? Does \"processing of a receipt\" in Microsoft dictionary mean uploading (processing, microsoft?) of the multiple files in \"a receipt\"\nIf the answer has durable functions then go for it without thinking deep. The requirements looks like a requirement for asynchronous processing because employees get an email (asynchronous) later. But any other answer is just not right and the question could send an intelligent developer (Microsoft excluded) into a loop of thoughts."
      },
      {
        "date": "2022-10-02T05:36:00.000Z",
        "voteCount": 1,
        "content": "Appreciate all the Microsoft-Technology-developers finding innovative reasons for the answers. But what is not clear is what that listFiles() method do. Which files are returned. That's a lot of assumptions to say you can do upload in parallel without knowing what files and their sizes. No wonder Microsoft-technologies are so buggy"
      },
      {
        "date": "2022-03-07T02:48:00.000Z",
        "voteCount": 13,
        "content": "A. Convert the trigger on the Azure Function to an Azure Blob storage trigger\n=&gt; won't help because we have Azure Fileshare\nB. Ensure that the consumption plan is configured correctly to allow scaling\n=&gt; Trigger is time based. Multiple instances scanning the same folder =&gt; bad idea; also clearly stated in the requirements that parallel processing is not allowed\nC. Move the Azure Function to a dedicated App Service Plan\n=&gt; the Trigger every 5 seconds should keep the function \"alive\". The function work is also not CPU bound so I cannot see a real benefit for ASP in this scenario\nD. Update the loop starting on line PC09 to process items in parallel\n=&gt; might help. \nD2 (alternative to D as by PaulMD) replace the solution with durable functions\n=&gt; looks even better than D\n\nIf D2 is an option I'd go for that. \n   Maybe they realized that the current \"D\" is not a really good solution and D2 is also way more \"azure\"\nOtherwise D."
      },
      {
        "date": "2024-08-15T04:39:00.000Z",
        "voteCount": 1,
        "content": "The trigger is every 5 hours, not seconds."
      },
      {
        "date": "2022-03-01T12:29:00.000Z",
        "voteCount": 1,
        "content": "The answer is C since this is a cold start problem.\n\"When using Azure Functions in the dedicated plan, the Functions host is always running, which means that cold start isn\u2019t really an issue.\"\nhttps://azure.microsoft.com/en-us/blog/understanding-serverless-cold-start/"
      },
      {
        "date": "2022-10-23T22:59:00.000Z",
        "voteCount": 1,
        "content": "No, trigger is timed [Timertrigger....], so function execution never sleeps.."
      },
      {
        "date": "2022-01-19T19:50:00.000Z",
        "voteCount": 1,
        "content": "The answer reference is about JavaScript, not C# :))))))"
      },
      {
        "date": "2022-01-06T05:07:00.000Z",
        "voteCount": 2,
        "content": "D is not correct - while this would speed up performance, the prompt states that users report high delay during BUSY PERIODS. Clearly, the fact that it does not upload files in parallel would not solve that.\n\nThe problem must be that the consumption plan is not scaling the function app correctly to handle the load. C could theoretically help, but B is better.\n\nCorrect answer: B"
      },
      {
        "date": "2021-08-24T07:44:00.000Z",
        "voteCount": 4,
        "content": "Only thing possible is D ...\nFile mount, is not blob storage, so cannot be trigger ...\nThis is a time trigger, so scale up will not help, only one instance will run ...\nOnly leave us with D"
      },
      {
        "date": "2021-05-28T03:53:00.000Z",
        "voteCount": 2,
        "content": "Nobody is given us a correct answer"
      },
      {
        "date": "2021-07-27T10:58:00.000Z",
        "voteCount": 1,
        "content": "D is 1000% correct"
      },
      {
        "date": "2022-01-19T01:08:00.000Z",
        "voteCount": 16,
        "content": "your 1000% comments under every single question does not help at all!"
      },
      {
        "date": "2021-05-03T02:56:00.000Z",
        "voteCount": 5,
        "content": "I vote for B. Reasoning: \n\nA. Convert the trigger on the Azure Function to an Azure Blob storage trigger\n&gt; We are not dealing with a defect, but a performance degradation, so this would not help. \n\nB. Ensure that the consumption plan is configured correctly to allow scaling\n&gt; It seems that \"Maximum Scale Out Limit\" is set to a value not appropriate for the usage pattern \n\nC. Move the Azure Function to a dedicated App Service Plan\n&gt; Wont help. \n\nD. Update the loop starting on line PC09 to process items in parallel\n&gt; I don't think it is a good idea to call an async method from within a foreach loop, also not from within Parallel.ForEach. \n\nhttps://stackoverflow.com/questions/23137393/parallel-foreach-and-async-await"
      },
      {
        "date": "2021-05-25T19:31:00.000Z",
        "voteCount": 2,
        "content": "well the function is started by a timer, meaning that the \"event\" that should trigger the scaling won't increase. Hence I do not think B is the correct choice (Ref: https://docs.microsoft.com/en-us/azure/azure-functions/event-driven-scaling). \nConsidering that we are uploading receipts to a Azure file storage A is also incorrect.\nIn the given scenario D is the one that makes the most sense."
      },
      {
        "date": "2023-04-06T04:23:00.000Z",
        "voteCount": 1,
        "content": "Don't use parallel extensions in Azure. There are special Azure constructions for it."
      },
      {
        "date": "2021-04-16T09:11:00.000Z",
        "voteCount": 4,
        "content": "So what is the answer?"
      },
      {
        "date": "2021-04-08T03:34:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer, since the loop picks up all files in the container and scaling would make the files being processed more than once, potentially.\nChange feed is not supported for file shares, so D is the only remaining option (though ugly as hell)."
      },
      {
        "date": "2021-04-03T06:25:00.000Z",
        "voteCount": 1,
        "content": "A and C - converting to blob trigger with dedicated plan not consumption to avoid cold start and high availability of the function\nD - is not enough since the the trigger is scheduled to every 5 mins - so users will still need to wait even it is already have been processed."
      },
      {
        "date": "2021-05-16T10:27:00.000Z",
        "voteCount": 6,
        "content": "Answer is C. A is not possible as reports can also be uploaded using Azure Files. Consumption plan has a cold start (up to 10 minutes), so moving to dedicated plan will help"
      },
      {
        "date": "2022-01-16T04:38:00.000Z",
        "voteCount": 2,
        "content": "2 questions about C:\nWill a cold start be an issue at all when it is triggered by a time trigger?\nCould it be a dedicated App Service plan has stronger CPU allowing to process the files faster?\nBesides that: if parallel processing is an option, I would go for that specially with the autoscaling options of a consumption plan (but where time trigger doesn't help?)"
      },
      {
        "date": "2023-07-02T09:05:00.000Z",
        "voteCount": 1,
        "content": "I agree,  parallel processing is not going to help much here since the  listfile() is already doing that. A dedicated plan will provide more resources"
      },
      {
        "date": "2021-03-31T11:07:00.000Z",
        "voteCount": 4,
        "content": "I think is better option change the trigger. A"
      },
      {
        "date": "2021-04-02T10:08:00.000Z",
        "voteCount": 3,
        "content": "Indeed: \"Concurrent processing of a receipt must be prevented.\""
      },
      {
        "date": "2021-04-05T07:47:00.000Z",
        "voteCount": 6,
        "content": "No one wants to process a single receipt concurrently, each distinct file will be processed in parallel."
      },
      {
        "date": "2021-04-08T03:31:00.000Z",
        "voteCount": 3,
        "content": "Only blobs support change feed, not fileshare, which is used here.\nhttps://docs.microsoft.com/nl-nl/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal"
      }
    ],
    "examNameCode": "az-204",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48172-exam-az-204-topic-20-question-3-discussion/",
    "body": "You need to resolve the log capacity issue.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Insights Telemetry Filter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the minimum log level in the host.json file for the function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Application Insights Sampling\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a LogCategoryFilter during startup"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-05-30T23:19:00.000Z",
        "voteCount": 31,
        "content": "I had the exam last Friday and I notice that they change one sentence in case study from:\nApplication Insights must always contain all log messages.\nto:\nApplication Insights currently contain all log messages.\nSo I chosen   C. Implement Application Insights Sampling."
      },
      {
        "date": "2022-10-02T23:34:00.000Z",
        "voteCount": 2,
        "content": "If that is the case then all three options are back on the table:\n1. Sampling\n2. Filtering\n3. Adjusting the log level\n4. Whatever the 4th mean\n\nIt totally depend on the situation. Practically you try the option and find the best solution. But I think Microsoft wants us to say Sampling."
      },
      {
        "date": "2022-10-23T23:42:00.000Z",
        "voteCount": 3,
        "content": "I don't agree. Sampling does not meet \"Application Insights must always contain ALL log messages\" since it drops items. \nhttps://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling#how-sampling-works\nI think it would be simply adjusting the log level in the app."
      },
      {
        "date": "2022-10-23T23:33:00.000Z",
        "voteCount": 1,
        "content": "The fourth means : filtering on log level (in webjobs SDK 2.0).\nhttps://learn.microsoft.com/en-us/azure/app-service/webjobs-sdk-how-to"
      },
      {
        "date": "2022-11-18T07:08:00.000Z",
        "voteCount": 2,
        "content": "so you mean \nIF Application Insights must always contain all log messages. then A. Create an Application Insights Telemetry Filter\n\nIf Application Insights currently contain all log messages then  C. Implement Application Insights Sampling."
      },
      {
        "date": "2022-02-24T14:09:00.000Z",
        "voteCount": 6,
        "content": "The answer is \"B\".\n\nFrom the problem description: \n\"Developers report that the number of log messages in the trace output for the processor is too high\"\n\nThe keywords are \"too high\".\n\nThe answer is to change the minimum logging level."
      },
      {
        "date": "2022-02-26T15:41:00.000Z",
        "voteCount": 2,
        "content": "agreed, it also said \"trace output for the processor is too high, resulting in lost log messages\" so it seems that sampling is already enabled and the logs are too many so resulting in lost messages since sampling only takes only portion of all the logs, so it helps if changing the minimum log level to reduce the logs or disable sampling but this is not in the answer selection."
      },
      {
        "date": "2022-02-26T15:51:00.000Z",
        "voteCount": 2,
        "content": "also this requirement \"Application Insights must always contain all log messages.\" will eliminate enable sampling answer because app insights will filter some logs during sampling"
      },
      {
        "date": "2022-10-02T07:11:00.000Z",
        "voteCount": 1,
        "content": "That's right, sherlock. But I don't think that is what Microsoft is looking for here. Those are red-herring options given to doom developers who never tried their flashy, state-of-the-art sampling feature. Yes, but these options depends on the developer and what exactly is shown in the logs and the level of logs."
      },
      {
        "date": "2024-02-18T05:28:00.000Z",
        "voteCount": 1,
        "content": "The purpose of Sampling is reducing the process of ingest log data..."
      },
      {
        "date": "2024-05-13T10:15:00.000Z",
        "voteCount": 1,
        "content": "\"Application Insights must always contain all log messages\".  Sampling will mean that not all log messages will get logged. Therefore changing log level from Trace to eg Information is the solution."
      },
      {
        "date": "2023-12-18T13:20:00.000Z",
        "voteCount": 1,
        "content": "B : TraceWriter and Application are distincts"
      },
      {
        "date": "2023-07-02T09:18:00.000Z",
        "voteCount": 1,
        "content": "Using sampling also allows you to preserve all your logs but reduces throttling"
      },
      {
        "date": "2023-06-20T06:35:00.000Z",
        "voteCount": 2,
        "content": "\"Application Insights must always contain all log messages.\" is a stupid requirement.  What does that even mean?  It would help if it indicated to what level.  If some jr programmer set the logging level to Verbose then that's the problem and the answer should be B change the minimum logging level.  A and C would indicate you are not meeting the requirement to log all messages.  D is not a real answer.  But I guess C sampling is what MS is looking for since \"\"Sampling also helps you avoid Application Insights throttling your telemetry.\" which is the issue.  https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling"
      },
      {
        "date": "2023-04-04T06:41:00.000Z",
        "voteCount": 1,
        "content": "According to the reference below: \"Sampling is a feature in Application Insights. It's the recommended way to reduce telemetry traffic, data costs, and storage costs, while preserving a statistically correct analysis of application data. Sampling also helps you avoid Application Insights throttling your telemetry. \"\nRef: https://learn.microsoft.com/en-us/azure/azure-monitor/app/sampling?tabs=net-core-new"
      },
      {
        "date": "2022-10-02T07:31:00.000Z",
        "voteCount": 3,
        "content": "Filtering or Sampling cannot be the answers considering, as far as I can guess from Microsoft documentation, that it is about NOT sending the telemetry (log entries?) to Application Insights. So, how can the argument that the requirement says \"must contain all log message\" (whatever that means in the real world). Sampling/Filtering and Chaning-log-levels have all the same effect. From the Microsoft documentation I could not really be clear of what exactly \"sampling\" is but it looks like it is not about \"trace\" but about metrics."
      },
      {
        "date": "2022-10-23T23:47:00.000Z",
        "voteCount": 1,
        "content": "Agree, exactly because of that: \nFiltering and Sampling drop messages. So if you must not drop messages between app and app Insights, the only thing that is left is to reduce logged messages (C)"
      },
      {
        "date": "2022-10-23T23:49:00.000Z",
        "voteCount": 1,
        "content": "I mean option B."
      },
      {
        "date": "2021-12-21T00:28:00.000Z",
        "voteCount": 1,
        "content": "We had similar a few weeks ago \nsampling worked fine"
      },
      {
        "date": "2021-11-01T00:37:00.000Z",
        "voteCount": 2,
        "content": "From https://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling\n\"Sampling reduces the volume of telemetry without affecting your statistics. \"\n\"Filtering with telemetry processors lets you filter out telemetry in the SDK before it's sent to the server\" (and the other options would also eliminate the log traces)\nSo only sampling will meet the requirement of containing all log messages."
      },
      {
        "date": "2021-08-24T07:57:00.000Z",
        "voteCount": 3,
        "content": "I am thinking of A to exclude TraceWrite logging ...\nSince all log messages are required, so you cannot sampling or change log levels, those are leading to lose log entries ...\nLogCategoryFilter I cannot find anything with that from documentation ..."
      },
      {
        "date": "2021-05-03T04:22:00.000Z",
        "voteCount": 5,
        "content": "A. Create an Application Insights Telemetry Filter\n\n&gt; A filter can be created by either implementing ITelemetryProcessor or by implementing ITelemetryInitializer. However MS recommends to use sampling\n\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/api-filtering-sampling#filtering\n\nB. Change the minimum log level in the host.json file for the function\n\n&gt; Can be ruled out because req. says app insights must always contain all log messages\n\nC. Implement Application Insights Sampling\n\n&gt; Is the recommended way to reduce telemetry traffic, data costs and storage costs\n&gt; https://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\n&gt; https://blog.ramondeklein.nl/2017/05/05/filtering-application-insights/\n\nD. Set a LogCategoryFilter during startup\n\n&gt; No idea what that is, but feels like pointint in the same direction as the B does"
      },
      {
        "date": "2022-10-02T07:14:00.000Z",
        "voteCount": 3,
        "content": "Bingo. It does say \"Application Insights must always contain all log messages.\" I did miss that sentence. This is an eyesight test than a developer exam."
      },
      {
        "date": "2021-05-01T17:02:00.000Z",
        "voteCount": 3,
        "content": "It can't be C. - AI Sampling.\nRule 1: Application Insights must always contain all log messages\nIf you read the Sampling doco carefully. It does filter logs and does not record all the transaction records (but yes it keeps a count for statistics only (See adaptive and fixed sampling)) https://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\n\nFact 2: The processor also has TraceWriter logging enabled.\nTraceWriter logs are generally used for debugging and are not \"Official\" transactional logs. https://stackify.com/logging-azure-functions/\nOptions A, B or D are candidates to filter these logs.\nOption B and D would stop the actual logs being generated but may also remove some transactional logs.  It would also not let the developers do their debugging (the purpose of TraceWriter in the first place).\n\nThus this leaves the only option A.\nA. Create an Application Insights Telemetry Filter (to filter the trace writer logging)"
      },
      {
        "date": "2021-03-25T11:19:00.000Z",
        "voteCount": 1,
        "content": "I think it could be the b"
      },
      {
        "date": "2021-03-31T00:38:00.000Z",
        "voteCount": 9,
        "content": "The case states \"The processor also has TraceWriter logging enabled.\nApplication Insights must always contain all log messages.\"\n\nFor this reason B isn't an option, and neither is A or D. These would all change what log messages are sent to AI, which isn't according to reqs. So this leaves C. Sampling just groups messages together with a count, causing less traffic to AI but the same results."
      },
      {
        "date": "2021-04-14T08:58:00.000Z",
        "voteCount": 2,
        "content": "B is correct.\nBecause it says \"Application Insights must always contain all log messages.\", sampling is not a valid answer.  In fact, sampling could be the very reason that log messages are lost.\nSee: https://docs.microsoft.com/en-us/azure/azure-monitor/app/asp-net-trace-logs\n\"I don't see some log entries that I expected\"."
      },
      {
        "date": "2022-10-02T07:44:00.000Z",
        "voteCount": 2,
        "content": "That's an excellent point. Kudos. I will need three days to prepare a legal answer for this question. How much did you say the exam allows? 180 minutes? \nSo, according to you the filter is also not an option? Nice, but I do not know how can I not see the vague requirement that says \"must contain all log messages\". Hope I do not read that line in the certification exam and so, I can say B is the right answer like a good developer's natural instinct."
      },
      {
        "date": "2022-12-21T15:17:00.000Z",
        "voteCount": 3,
        "content": "Thank you gmishra88 for the funny comments you left along the way throughout this painful study session."
      },
      {
        "date": "2023-03-04T05:34:00.000Z",
        "voteCount": 2,
        "content": "Haha so true ! I think he is the same guy who was from start but is now showin as \"Removed User\" xD"
      },
      {
        "date": "2021-04-08T03:43:00.000Z",
        "voteCount": 2,
        "content": "Sampling is enabled by default and can be turned off (althoiugh AI ingress may still drop entries when overloaded). For metrics sampling does exactly as MrZoom describes.\n\nAs I see it, irrelevant logs must be prevented. My first step would be to adjust the minimum loglevel,if possible. A telemetry processor to filter telemetry/logs technically could work to, but is intended for filtering specific entries."
      }
    ],
    "examNameCode": "az-204",
    "topicNumber": "20"
  }
]