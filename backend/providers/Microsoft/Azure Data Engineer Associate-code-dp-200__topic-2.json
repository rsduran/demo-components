[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/65327-exam-dp-200-topic-2-question-1-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Create an external data source pointing to the Azure storage account<br>2. Create a workload group using the Azure storage account name as the pool name<br>3. Load the data using the INSERT`\u00a6SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "You need to create an external file format and external table using the external data source.<br>You then load the data using the CREATE TABLE AS SELECT statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-11-02T05:24:00.000Z",
        "voteCount": 1,
        "content": "This is correct, please refer to: https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30498-exam-dp-200-topic-2-question-2-discussion/",
    "body": "You develop data engineering solutions for a company.<br>You must integrate the company's on-premises Microsoft SQL Server data with Microsoft Azure SQL Database. Data must be transformed incrementally.<br>You need to implement the data integration solution.<br>Which tool should you use to configure a pipeline to copy data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Copy Data tool with Blob storage linked service as the source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Azure PowerShell with SQL Server linked service as a source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Azure Data Factory UI with Blob storage linked service as a source",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the .NET Data Factory API with Blob storage linked service as the source"
    ],
    "answer": "C",
    "answerDescription": "The Integration Runtime is a customer managed data integration infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments.<br>A linked service defines the information needed for Azure Data Factory to connect to a data resource. We have three resources in this scenario for which linked services are needed:<br>\u2711 On-premises SQL Server<br>\u2711 Azure Blob Storage<br>\u2711 Azure SQL database<br>Note: Azure Data Factory is a fully managed cloud-based data integration service that orchestrates and automates the movement and transformation of data. The key concept in the ADF model is pipeline. A pipeline is a logical grouping of Activities, each of which defines the actions to perform on the data contained in<br>Datasets. Linked services are used to define the information needed for Data Factory to connect to the data resources.<br>References:<br>https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-05T08:36:00.000Z",
        "voteCount": 35,
        "content": "The answer is B. Use Azure PowerShell with SQL Server linked service as a source\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-powershell"
      },
      {
        "date": "2021-04-21T07:53:00.000Z",
        "voteCount": 5,
        "content": "This is the answer. 100%"
      },
      {
        "date": "2020-09-03T08:41:00.000Z",
        "voteCount": 7,
        "content": "C says the linked service is Blob storage. The question says it coming from SQL Server. Why would you use Blob storage as the source?"
      },
      {
        "date": "2021-05-13T00:33:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is B.\n\n1. Both *Azure Data Factory UI (Azure Portal)* and *Azure PowerShell* can be used to incrementally load data from multiple tables in SQL Server to a database in Azure SQL Database.\n2. Both *Azure Data Factory UI (Azure Portal)* and *Azure PowerShell* can use an SQL Server linked service as a source for this incremental copy.\n\nEvidence for these two statements:\n- *ADF GUI*: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-portal\n- *PowerShell*: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-multiple-tables-powershell\n\nHence, use *Azure Data Factory UI (Azure Portal) with SQL Server linked service* or use *Azure PowerShell with SQL Server linked service*.\n\nSince *ADF with SQL Server linked service* is not an option among the answers (\"Blob storage linked service as a source\" in answer C is really not necessary), the correct answer is B: Use Azure PowerShell with SQL Server linked service as a source."
      },
      {
        "date": "2021-05-06T09:46:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is : 100% is C"
      },
      {
        "date": "2021-04-29T23:32:00.000Z",
        "voteCount": 1,
        "content": "This should be ADF"
      },
      {
        "date": "2021-05-04T00:15:00.000Z",
        "voteCount": 1,
        "content": "Disregard this and the answer is 'B' as the other connection string relies on azure blob storage and the source is from on-prem SQL Server"
      },
      {
        "date": "2021-04-23T01:13:00.000Z",
        "voteCount": 3,
        "content": "The answer is B since the catch is the movement of data incrementally which can be done easily through powershell"
      },
      {
        "date": "2021-04-14T02:28:00.000Z",
        "voteCount": 1,
        "content": "In my opinion the key word is \"pipeline\". ADF UI is the only one that creates a pipeline"
      },
      {
        "date": "2021-05-18T14:17:00.000Z",
        "voteCount": 1,
        "content": "ADF is not good with incremental or delta loads. Besides all anwers except B refers to Blob as source and the source is on prem SQL, so it can only be B"
      },
      {
        "date": "2021-03-03T19:58:00.000Z",
        "voteCount": 2,
        "content": "looks like the question is based on a scenario presented in one of Microsoft help docs :\nhttps://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/move-sql-azure-adf\nthe scenario assuming the following:\nThe Scenario\nWe set up an ADF pipeline that composes two data migration activities. Together they move data on a daily basis between a SQL Server database and Azure SQL Database. The two activities are:\n\nCopy data from a SQL Server database to an Azure Blob Storage account\nCopy data from the Azure Blob Storage account to Azure SQL Database."
      },
      {
        "date": "2020-12-12T01:59:00.000Z",
        "voteCount": 1,
        "content": "hi to all,\nthe text says \"...tool...\" so for me that leaves out B and D.\nanswer C seems right but the source is SQL Server onprem, not the blob storage !\nso for me it's A because its source is SQL onPrem and because of this link:\nhttps://docs.microsoft.com/pt-pt/azure/data-factory/tutorial-hybrid-copy-data-tool\n\nso it's A for me.\n\nregards"
      },
      {
        "date": "2020-12-12T02:01:00.000Z",
        "voteCount": 1,
        "content": "Under New Linked Service, search for SQL Server, and then select Continue.\n\nIn the New Linked Service (SQL Server) dialog box, under Name, enter SqlServerLinkedService. Select +New under Connect via integration runtime. You must create a self-hosted integration runtime, download it to your machine, and register it with Data Factory. The self-hosted integration runtime copies data between your on-premises environment and the cloud."
      },
      {
        "date": "2020-09-17T10:16:00.000Z",
        "voteCount": 3,
        "content": "Can someone please explain how blob storage comes into picture in this scenario? The data transformation is being carried out using ADF, and the data moves between on premise SQL Server instance and Azure SQL DB."
      },
      {
        "date": "2020-09-22T22:00:00.000Z",
        "voteCount": 3,
        "content": "ADF need staging area on cloud using Blob"
      },
      {
        "date": "2020-11-05T01:21:00.000Z",
        "voteCount": 4,
        "content": "Not if you are using a self hosted integration runtime - ADF can draw directly from on-prem SQL"
      },
      {
        "date": "2021-05-18T14:19:00.000Z",
        "voteCount": 1,
        "content": "I confirm, ADF copy activity can draw from on prem, data flows need staging on cloud as they run on IR"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49461-exam-dp-200-topic-2-question-3-discussion/",
    "body": "HOTSPOT -<br>A company runs Microsoft Dynamics CRM with Microsoft SQL Server on-premises. SQL Server Integration Services (SSIS) packages extract data from Dynamics<br>CRM APIs, and load the data into a SQL Server data warehouse.<br>The datacenter is running out of capacity. Because of the network configuration, you must extract on premises data to the cloud over https. You cannot open any additional ports. The solution must implement the least amount of effort.<br>You need to create the pipeline system.<br>Which component should you use? To answer, select the appropriate technology in the dialog box in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0014500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0014600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Source -<br>For Copy activity, it requires source and sink linked services to define the direction of data flow.<br>Copying between a cloud data source and a data source in private network: if either source or sink linked service points to a self-hosted IR, the copy activity is executed on that self-hosted Integration Runtime.<br>Box 2: Self-hosted integration runtime<br>A self-hosted integration runtime can run copy activities between a cloud data store and a data store in a private network, and it can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network. The installation of a self-hosted integration runtime needs on an on-premises machine or a virtual machine (VM) inside a private network.<br>References:<br>https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-07T00:24:00.000Z",
        "voteCount": 17,
        "content": "For me are:\n- Self Hosted IR\n- Sink"
      },
      {
        "date": "2021-06-25T06:02:00.000Z",
        "voteCount": 7,
        "content": "So...what is the answer??? so anoying!"
      },
      {
        "date": "2023-08-28T02:36:00.000Z",
        "voteCount": 1,
        "content": "For me:\nExtract SQL data on-premises: Use Azure-SSIS Integration runtime to run SSIS packages in ADF to extract data from Dynamics.\nLoad SQL data warehouse: Use Self-hosted integration runtime to perform operations between on premises and azure. In this case loading data using ADF to SQL Server data warehouse"
      },
      {
        "date": "2021-10-14T09:05:00.000Z",
        "voteCount": 3,
        "content": "Bloc 1= Azure SSIS Integration runtine\".\nBloc 2 = Azure SSIS Integration runtine\".\nThe main problem is that The datacenter is running out of capacity. So we just need the cloud infrastructure with the underlying ressources (cpu, memory) to transfert data between the CRM and the Data warehouse. Since an existing SSIS package exists already, the minimum effort would be to lift and shift this exiting SSIS package to the cloud. To do that, the technology behind the scene is \"Azure SSIS Integration runtine\"."
      },
      {
        "date": "2021-06-21T09:54:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct!"
      },
      {
        "date": "2021-06-05T10:36:00.000Z",
        "voteCount": 2,
        "content": "Self Hosted IR for both answer\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"
      },
      {
        "date": "2021-05-16T09:49:00.000Z",
        "voteCount": 2,
        "content": "-Self Hosted\n-Self Hosted"
      },
      {
        "date": "2021-04-29T23:41:00.000Z",
        "voteCount": 4,
        "content": "The scenario retrieves data from on-prem database and loads it to on-prem data warehouse.\nADF function would be to orchestrate data and their link services would use \"SELF-HOSTED\" and this requires to implement gateway configuration on the server where the on-prem resides. Therefore, both SELF-HOSTED is the appropriate answer"
      },
      {
        "date": "2021-05-06T20:09:00.000Z",
        "voteCount": 2,
        "content": "On second though, the question states that there is a SSIS involve and it requires minimal effort, hence, Azure SSIS and Self Hosted are the answers"
      },
      {
        "date": "2021-04-19T22:37:00.000Z",
        "voteCount": 7,
        "content": "- Self Hosted IR ( to transfer the data from on premise to Azure)\n- Azure-SSIS ( to take care of the load from source to target warehouse)"
      },
      {
        "date": "2021-04-19T05:12:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer is:\n - Azure-SSIS integration runtime\n - Azure integration runtime"
      },
      {
        "date": "2021-04-09T09:56:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is:\n- Self Hosted IR\n- Sink\nSource: https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-data-tool"
      },
      {
        "date": "2021-05-16T09:56:00.000Z",
        "voteCount": 3,
        "content": "source is irrelevant. for me its;\nself host ir\nazure ssis"
      },
      {
        "date": "2021-04-08T23:17:00.000Z",
        "voteCount": 3,
        "content": "I'm not sure that \"Self Hosted IR\" can be the right choice for the first option. \nKeep in mind that on-premise data-center is already running \"SQL Server Integration Services (SSIS)\". \nSome related doc: https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49672-exam-dp-200-topic-2-question-4-discussion/",
    "body": "DRAG DROP -<br>You develop data engineering solutions for a company.<br>A project requires analysis of real-time Twitter feeds. Posts that contain specific keywords must be stored and processed on Microsoft Azure and then displayed by using Microsoft Power BI. You need to implement the solution.<br>Which five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0014700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0014800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an HDInisght cluster with the Spark cluster type<br>Step 2: Create a Jyputer Notebook<br><br>Step 3: Create a table -<br>The Jupyter Notebook that you created in the previous step includes code to create an hvac table.<br>Step 4: Run a job that uses the Spark Streaming API to ingest data from Twitter<br>Step 5: Load the hvac table into Power BI Desktop<br>You use Power BI to create visualizations, reports, and dashboards from the Spark cluster data.<br>References:<br>https://acadgild.com/blog/streaming-twitter-data-using-spark<br>https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-with-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-29T23:49:00.000Z",
        "voteCount": 6,
        "content": "The propose solution is correct.\nA table cannot be created if the notebook is not yet available, the scenario is in assumption the table is within the hdinsight spark cluster."
      },
      {
        "date": "2021-05-06T20:14:00.000Z",
        "voteCount": 2,
        "content": "Reference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-load-data-run-query"
      },
      {
        "date": "2021-04-19T22:45:00.000Z",
        "voteCount": 1,
        "content": "Agree with Pairon. Ideal to have Target table table  created first before Jupyter notebook"
      },
      {
        "date": "2021-04-13T11:08:00.000Z",
        "voteCount": 1,
        "content": "I agree with the answer, but maybe we can swapp second and third step?"
      },
      {
        "date": "2021-04-08T23:51:00.000Z",
        "voteCount": 1,
        "content": "With the proposed solution, \"who\" runs the Jupyter notebook? :-("
      },
      {
        "date": "2021-04-08T18:09:00.000Z",
        "voteCount": 3,
        "content": "answer appears correct. https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-bi-tools"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/9252-exam-dp-200-topic-2-question-5-discussion/",
    "body": "DRAG DROP -<br>Your company manages on-premises Microsoft SQL Server pipelines by using a custom solution.<br>The data engineering team must implement a process to pull data from SQL Server and migrate it to Azure Blob storage. The process must orchestrate and manage the data lifecycle.<br>You need to configure Azure Data Factory to connect to the on-premises SQL Server database.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0014900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0015000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create a virtual private network (VPN) connection from on-premises to Microsoft Azure.<br>You can also use IPSec VPN or Azure ExpressRoute to further secure the communication channel between your on-premises network and Azure.<br>Azure Virtual Network is a logical representation of your network in the cloud. You can connect an on-premises network to your virtual network by setting up IPSec<br>VPN (site-to-site) or ExpressRoute (private peering).<br>Step 2: Create an Azure Data Factory resource.<br>Step 3: Configure a self-hosted integration runtime.<br>You create a self-hosted integration runtime and associate it with an on-premises machine with the SQL Server database. The self-hosted integration runtime is the component that copies data from the SQL Server database on your machine to Azure Blob storage.<br>Note: A self-hosted integration runtime can run copy activities between a cloud data store and a data store in a private network, and it can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network. The installation of a self-hosted integration runtime needs on an on- premises machine or a virtual machine (VM) inside a private network.<br>References:<br>https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-powershell",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-07T04:53:00.000Z",
        "voteCount": 71,
        "content": "Create an Azure Data Factory\nConfigure a self-hosted integration runtime\nConfigure on-premises SQL Server Instance ................"
      },
      {
        "date": "2020-02-16T16:14:00.000Z",
        "voteCount": 5,
        "content": "totally agree"
      },
      {
        "date": "2020-03-26T23:15:00.000Z",
        "voteCount": 2,
        "content": "curious, what sort of configuration do we need in on-prem SQL Server instance? creating user account and so on?"
      },
      {
        "date": "2020-04-12T16:47:00.000Z",
        "voteCount": 1,
        "content": "Just the installation of the IR in the on-premise server. I guess it can be called configuration as well."
      },
      {
        "date": "2021-01-17T08:47:00.000Z",
        "voteCount": 1,
        "content": "You may need to create a db user to use in a link service in ADF."
      },
      {
        "date": "2019-11-28T00:39:00.000Z",
        "voteCount": 23,
        "content": "It's not necessary to Create a virtual private network (VPN) connection from on-premises to Microsoft Azure."
      },
      {
        "date": "2020-03-16T05:31:00.000Z",
        "voteCount": 8,
        "content": "\"The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network.\" - https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime\n\nImplies that a virtual private network is indeed required."
      },
      {
        "date": "2020-03-27T19:20:00.000Z",
        "voteCount": 1,
        "content": "I think it is not required when the machine is internet-facing with connectivity to on-prem SQL Server."
      },
      {
        "date": "2020-05-08T21:22:00.000Z",
        "voteCount": 1,
        "content": "Its not madatory. But its a security best practice to use VPN\n\nSee\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations"
      },
      {
        "date": "2020-07-03T09:39:00.000Z",
        "voteCount": 3,
        "content": "The company may have ExpressRoute setup."
      },
      {
        "date": "2021-04-29T23:51:00.000Z",
        "voteCount": 1,
        "content": "Create an Azure Data Factory\nConfigure a self-hosted integration runtime\nConfigure on-premises SQL Server Instance\n\nThese steps leads to installation of gateway to the on-prem server that links to ADF (Self-Hosted IR)"
      },
      {
        "date": "2021-03-24T13:01:00.000Z",
        "voteCount": 1,
        "content": "What is the correct answer for this?"
      },
      {
        "date": "2021-01-08T12:19:00.000Z",
        "voteCount": 4,
        "content": "You don't need to pick \"Configure on-premises SQL Server Instance ... \" because self-host runtime already does that. Look at the tuturial, you don't need to touch anything related to SQL Server Instance, you simply create a linked service through the self-host IR.  I would say it is redundant.\n\nCreating a VPN on the otherside, (although not mandatory) makes the solution more complete."
      },
      {
        "date": "2020-11-28T02:01:00.000Z",
        "voteCount": 3,
        "content": "I would say the answer given is correct\nVPN to establish connection as the first step followed by creating Azure Data Factory and configure self-hosted integration runtime"
      },
      {
        "date": "2020-09-20T04:16:00.000Z",
        "voteCount": 2,
        "content": "1: Deploy an Azure Data Factory\n2: From the on-premises network, install and configure a self-hosted runtime.\n3: Configure a linked service to connect to the SQL Server instance."
      },
      {
        "date": "2021-01-17T08:35:00.000Z",
        "voteCount": 1,
        "content": "Without link service there is no way to connect SQL Server from ADF. I guess this option is missing."
      },
      {
        "date": "2020-09-09T03:39:00.000Z",
        "voteCount": 7,
        "content": "1. Create an Azure Data Factory resource\n2. Configure a self-hosted integration runtime (configure means that you have ADF already running)\n3. Configure the on-premises SQL server instance with an integration runtime (means that you go into ADF and connect to the database using IR)\n\nYou don't need VPN - all communication from IR to ADF is over HTTPS"
      },
      {
        "date": "2020-12-08T02:35:00.000Z",
        "voteCount": 1,
        "content": "hi to all, this is the right answer 4 sure. please see this link, all the steps are there: https://docs.microsoft.com/pt-pt/azure/machine-learning/team-data-science-process/move-sql-azure-adf\n\nvpn no needed, the IR onpremises uses https as default\n\nregards"
      },
      {
        "date": "2020-12-08T09:09:00.000Z",
        "voteCount": 1,
        "content": "Create an Azure Data Factory\nThe instructions for creating a new Azure Data Factory and a resource group in the Azure portal are provided Create an Azure Data Factory. Name the new ADF instance adfdsp and name the resource group created adfdsprg.\n\nInstall and configure Azure Data Factory Integration Runtime\nThe Integration Runtime is a customer-managed data integration infrastructure used by Azure Data Factory to provide data integration capabilities across different network environments. This runtime was formerly called \"Data Management Gateway\".\n\nTo set up, follow the instructions for creating a pipeline\n\nCreate linked services to connect to the data resources\nA linked service defines the information needed for Azure Data Factory to connect to a data resource. We have three resources in this scenario for which linked services are needed:\n\nOn-premises SQL Server\nAzure Blob Storage\nAzure SQL Database"
      },
      {
        "date": "2020-06-03T08:53:00.000Z",
        "voteCount": 4,
        "content": "1: Create an Azure Data Factory\n2: Configure a self-hosted integration runtime\n3: Backup the DB and send it to Azure Blob storage"
      },
      {
        "date": "2021-05-18T14:40:00.000Z",
        "voteCount": 1,
        "content": "That's why you have ADF to use copy activity directly from table to blob, and you don't need to manually copy a backup to storage"
      },
      {
        "date": "2020-05-23T06:04:00.000Z",
        "voteCount": 5,
        "content": "VPN is not a mandatory requirement for self-hosted IR. It is only needed when to perform data integration securely in a private network environment, which doesn't have a direct line-of-sight from the public cloud environment. (https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime).\nSo, the correct answers should be \n1. Create an ADF;\n2. Create a self-hosted IR;\n3. Install and configure self-hosted IR on on-premise server;"
      },
      {
        "date": "2020-05-17T20:28:00.000Z",
        "voteCount": 1,
        "content": "Is VPN really a necessity?"
      },
      {
        "date": "2020-08-04T11:34:00.000Z",
        "voteCount": 1,
        "content": "The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network."
      },
      {
        "date": "2020-03-26T23:16:00.000Z",
        "voteCount": 2,
        "content": "Step 1 and 2 can be in any order."
      },
      {
        "date": "2020-03-04T01:06:00.000Z",
        "voteCount": 5,
        "content": "Given answer is correct"
      },
      {
        "date": "2020-02-26T09:46:00.000Z",
        "voteCount": 2,
        "content": "correct VPN is not needed"
      },
      {
        "date": "2020-08-04T11:33:00.000Z",
        "voteCount": 1,
        "content": "The installation of a self-hosted integration runtime needs an on-premises machine or a virtual machine inside a private network."
      },
      {
        "date": "2019-11-30T02:00:00.000Z",
        "voteCount": 6,
        "content": "The answer could be\nStep 1: Create an Azure Data Factory\nYou need to create a data factory and start the Data Factory UI to create a pipeline in the data factory.\n\nStep 2: Configure a self-hosted integration runtime\n\nStep 3: Configure a self-hosted integration runtime. \n?\u00bf?"
      },
      {
        "date": "2020-10-07T17:32:00.000Z",
        "voteCount": 9,
        "content": "Tu ere loko o k"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49477-exam-dp-200-topic-2-question-6-discussion/",
    "body": "HOTSPOT -<br>You are designing a new Lambda architecture on Microsoft Azure.<br>The real-time processing layer must meet the following requirements:<br>Ingestion:<br>\u2711 Receive millions of events per second<br>\u2711 Act as a fully managed Platform-as-a-Service (PaaS) solution<br>\u2711 Integrate with Azure Functions<br>Stream processing:<br>\u2711 Process on a per-job basis<br>\u2711 Provide seamless connectivity with Azure services<br>\u2711 Use a SQL-based query language<br>Analytical data store:<br>\u2711 Act as a managed service<br>\u2711 Use a document store<br>\u2711 Provide data encryption at rest<br>You need to identify the correct technologies to build the Lambda architecture using minimal effort. Which technologies should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0015200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0015300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Event Hubs -<br>This portion of a streaming architecture is often referred to as stream buffering. Options include Azure Event Hubs, Azure IoT Hub, and Kafka.<br>Incorrect Answers: Not HDInsight Kafka<br>Azure Functions need a trigger defined in order to run. There is a limited set of supported trigger types, and Kafka is not one of them.<br><br>Box 2: Azure Stream Analytics -<br>Azure Stream Analytics provides a managed stream processing service based on perpetually running SQL queries that operate on unbounded streams.<br>You can also use open source Apache streaming technologies like Storm and Spark Streaming in an HDInsight cluster.<br><br>Box 3: Azure Synapse Analytics -<br>Azure Synapse Analytics provides a managed service for large-scale, cloud-based data warehousing. HDInsight supports Interactive Hive, HBase, and Spark<br>SQL, which can also be used to serve data for analysis.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-07T05:32:00.000Z",
        "voteCount": 33,
        "content": "The analytical data store should be Azure Cosmos DB, as the question said \"use a document store\"."
      },
      {
        "date": "2021-04-07T03:33:00.000Z",
        "voteCount": 13,
        "content": "Analytical data store should be Cosmos DB, since, Synapse analytics is not a document store."
      },
      {
        "date": "2021-06-03T09:20:00.000Z",
        "voteCount": 1,
        "content": "Agree. Analytical store should be Cosmos DB"
      },
      {
        "date": "2021-05-18T03:45:00.000Z",
        "voteCount": 1,
        "content": "it should be cosmos DB"
      },
      {
        "date": "2021-04-29T23:53:00.000Z",
        "voteCount": 2,
        "content": "Same sentiment with the Analytics data store should be Azure Cosmos DB"
      },
      {
        "date": "2021-04-26T13:15:00.000Z",
        "voteCount": 2,
        "content": "Agree with samkslee &amp; ak08 - Synapse is not a document store, Cosmos DB is."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17539-exam-dp-200-topic-2-question-7-discussion/",
    "body": "You develop data engineering solutions for a company.<br>You need to ingest and visualize real-time Twitter data by using Microsoft Azure.<br>Which three technologies should you use? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Grid topic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Job that queries Twitter data from an Event Hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Job that queries Twitter data from an Event Grid",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLogic App that sends Twitter posts which have target keywords to Azure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Grid subscription",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEvent Hub instance"
    ],
    "answer": "BDF",
    "answerDescription": "You can use Azure Logic apps to send tweets to an event hub and then use a Stream Analytics job to read from event hub and send them to PowerBI.<br>References:<br>https://community.powerbi.com/t5/Integrations-with-Files-and/Twitter-streaming-analytics-step-by-step/td-p/9594",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-26T23:23:00.000Z",
        "voteCount": 12,
        "content": "Same as my thoughts. BDF"
      },
      {
        "date": "2020-07-03T09:44:00.000Z",
        "voteCount": 5,
        "content": "Shouldn't be EventGrid because that is for subscribing to Azure events, ie. New blob in a container."
      },
      {
        "date": "2021-05-04T00:40:00.000Z",
        "voteCount": 1,
        "content": "This should clear up the differences.\n\nReference: https://www.cognizantsoftvision.com/blog/azure-event-grid-vs-event-hubs/"
      },
      {
        "date": "2021-05-23T20:51:00.000Z",
        "voteCount": 2,
        "content": "My ans is BDF"
      },
      {
        "date": "2021-05-09T21:38:00.000Z",
        "voteCount": 1,
        "content": "Azure Event Hubs is a more suitable solution when we need a service that can receive and process millions of events per second and provide low-latency event processing. It can handle data from concurrent sources and route it to a variety of stream-processing infrastructure and analytics services, as I have already mentioned. Azure Event Hubs are used more for telemetry scenarios. \n\nOn the other hand, Azure Event Grid is ideal for reactive scenarios, like when an item has been shipped or an item has been added or updated on storage. We have to take into account also its native integrations with Functions, Logic Apps and Webhooks. Moreover, Event Grid is cheaper than Event Hubs and more suitable when we don\u2019t have to deal with big data.\n\nSo I'll go with CDF"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49150-exam-dp-200-topic-2-question-8-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We would need a High Concurrency cluster for the jobs.<br>Note:<br>Standard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.<br>A high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.<br>References:<br>https://docs.azuredatabricks.net/clusters/configure.html",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-05T01:46:00.000Z",
        "voteCount": 13,
        "content": "A workload for jobs that will run notebooks that use Python, Scala, and SQL --&gt; so Standard clusters because of Scala (High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.)"
      },
      {
        "date": "2021-05-13T07:36:00.000Z",
        "voteCount": 1,
        "content": "Correct. Create New Cluster UI for Jobs allows either Standard or Single Node. It does list high concurrency as an option\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/create"
      },
      {
        "date": "2021-04-28T01:30:00.000Z",
        "voteCount": 7,
        "content": "A is correct"
      },
      {
        "date": "2021-11-23T10:39:00.000Z",
        "voteCount": 1,
        "content": "Standard for jobs and high concurrency for darta scientists and data engineers"
      },
      {
        "date": "2021-07-24T09:11:00.000Z",
        "voteCount": 1,
        "content": "High Concurrency clusters can run workloads developed in SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\n\nmeans job can not be high concurrency\n\n\nso, ANSWER IS A"
      },
      {
        "date": "2021-05-10T20:07:00.000Z",
        "voteCount": 3,
        "content": "appropriate answer is A"
      },
      {
        "date": "2021-04-19T19:37:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is A"
      },
      {
        "date": "2021-04-19T05:29:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is:\nYes"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24341-exam-dp-200-topic-2-question-9-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a High Concurrency cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "We need a High Concurrency cluster for the data engineers and the jobs.<br>Note:<br>Standard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.<br>A high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.<br>References:<br>https://docs.azuredatabricks.net/clusters/configure.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-28T20:56:00.000Z",
        "voteCount": 31,
        "content": "High Concurrency does not work for Scala...should B is correct answer\n Note\n\nHigh concurrency clusters work only for SQL, Python, and R. The performance, security, and fault isolation of high concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\nThe Table Access Control checkbox is available only for high concurrency cluster\n\nhttps://docs.microsoft.com/en-gb/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-05-31T19:06:00.000Z",
        "voteCount": 1,
        "content": "actually Data Engineers do not require Scala - so High Concurrency is ok"
      },
      {
        "date": "2021-07-26T04:49:00.000Z",
        "voteCount": 1,
        "content": "What about reading the question again?"
      },
      {
        "date": "2021-06-25T06:50:00.000Z",
        "voteCount": 1,
        "content": "it says that the will use scala...so A"
      },
      {
        "date": "2020-07-12T02:03:00.000Z",
        "voteCount": 4,
        "content": "Data Engineers do not use Scala."
      },
      {
        "date": "2021-05-23T21:16:00.000Z",
        "voteCount": 1,
        "content": "Agreed with you"
      },
      {
        "date": "2020-06-29T13:58:00.000Z",
        "voteCount": 39,
        "content": "\"You create a Standard cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs\"\nData engineers share same cluster and only use Python and SQL -&gt; High Concurrency\nEach Data Scientists needs a cluster and they are going to work with R and Scala -&gt; Standard\nJobs are going to be executed with Scala -&gt; Standard.\n\nSo correct answer is Yes"
      },
      {
        "date": "2021-04-07T14:02:00.000Z",
        "voteCount": 6,
        "content": "That's not what the question says ..."
      },
      {
        "date": "2021-05-23T21:16:00.000Z",
        "voteCount": 2,
        "content": "What you have explained is correct hence the ans is B. As the ans contain high concurrence cluster for job which is wrong."
      },
      {
        "date": "2021-05-02T16:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is No\nHigh Concurrency clusters work only for SQL, Python, and R. The performance and security of High Concurrency clusters is provided by running user code in separate processes, which is not possible in Scala.\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-04-30T00:55:00.000Z",
        "voteCount": 2,
        "content": "This statement \"A workload for jobs that will run notebooks that use Python, Scala, and SQL\" pertains to the propose solution \"High Concurrency cluster for the jobs\" and as we all know, high concurrency doesn't work for SCALA. Therefore, the answer is NO."
      },
      {
        "date": "2021-04-28T02:11:00.000Z",
        "voteCount": 1,
        "content": "Answer is NO"
      },
      {
        "date": "2021-04-22T09:45:00.000Z",
        "voteCount": 1,
        "content": "https://github.com/Azure/AzureDatabricksBestPractices/blob/master/Table2.PNG\nAnswer is NO"
      },
      {
        "date": "2021-04-16T01:43:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B as High Concurrency does not work for Scala."
      },
      {
        "date": "2020-11-05T08:50:00.000Z",
        "voteCount": 3,
        "content": "No - because a single node cluster is appropriate for jobs. Not because of any other reasons...\n\nA Single Node cluster has no workers and runs Spark jobs on the driver node. In contrast, Standard mode clusters require at least one Spark worker node in addition to the driver node to execute Spark jobs.\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure#--single-node-clusters"
      },
      {
        "date": "2020-10-24T05:11:00.000Z",
        "voteCount": 2,
        "content": "Jobs should be standard cluster, correct answer is Yes"
      },
      {
        "date": "2020-09-30T00:45:00.000Z",
        "voteCount": 3,
        "content": "That is correct answer should be \"Yes\". High Concurrency does not support Scala"
      },
      {
        "date": "2020-07-29T14:37:00.000Z",
        "voteCount": 2,
        "content": "Answer is wrong as High Concurrency cluster can't support scala"
      },
      {
        "date": "2020-09-26T10:28:00.000Z",
        "voteCount": 1,
        "content": "There is no need to use Scala for the users of high concurrency. This is mentioned in the question."
      },
      {
        "date": "2020-10-21T11:05:00.000Z",
        "voteCount": 2,
        "content": "hart232 its mentioned as a requirement:\nA workload for jobs that will run notebooks that use Python, Spark, Scala, and SQL"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33323-exam-dp-200-topic-2-question-10-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a High Concurrency cluster for each data scientist, a High Concurrency cluster for the data engineers, and a Standard cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "No need for a High Concurrency cluster for each data scientist.<br>Standard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.<br>A high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.<br>References:<br>https://docs.azuredatabricks.net/clusters/configure.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-20T01:01:00.000Z",
        "voteCount": 9,
        "content": "A workload that data scientists will use to perform ad hoc analysis in Scala and R. High Concurrency clusters don't support Scala. Answer is \"No\"."
      },
      {
        "date": "2021-04-30T00:59:00.000Z",
        "voteCount": 1,
        "content": "Definitely the answer is NO"
      },
      {
        "date": "2021-04-16T01:44:00.000Z",
        "voteCount": 2,
        "content": "A workload that data scientists will use to perform ad hoc analysis in Scala and R. High Concurrency clusters don't support Scala. Answer is \"No\"."
      },
      {
        "date": "2021-02-18T07:08:00.000Z",
        "voteCount": 1,
        "content": "Answers should be \"yes\"..."
      },
      {
        "date": "2021-02-18T07:11:00.000Z",
        "voteCount": 1,
        "content": "sorry, its \"no\", because for data Scientist should be Standard cluster"
      },
      {
        "date": "2021-02-16T02:12:00.000Z",
        "voteCount": 2,
        "content": "No - Right answer"
      },
      {
        "date": "2020-10-01T02:44:00.000Z",
        "voteCount": 3,
        "content": "\"There is no need for a High concurrency cluster\" does not mean, it does not fulfil the goal. High concurrency clusters can be used for data scientists workload. It is just more expensive. There is no requirement regarding keeping the costs low, mentioned in the question. So the correct answer to this question is also \"A - Yes.\""
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37833-exam-dp-200-topic-2-question-11-discussion/",
    "body": "You have an Azure Stream Analytics query. The query returns a result set that contains 10,000 distinct values for a column named clusterID.<br>You monitor the Stream Analytics job and discover high latency.<br>You need to reduce the latency.<br>Which two actions should you perform? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a pass-through query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a temporal analytic function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale out the query by using PARTITION BY.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the query to a reference query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of streaming units."
    ],
    "answer": "CE",
    "answerDescription": "C: Scaling a Stream Analytics job takes advantage of partitions in the input or output. Partitioning lets you divide data into subsets based on a partition key. A process that consumes the data (such as a Streaming Analytics job) can consume and write different partitions in parallel, which increases throughput.<br>E: Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job. This capacity lets you focus on the query logic and abstracts the need to manage the hardware to run your<br>Stream Analytics job in a timely manner.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-26T05:11:00.000Z",
        "voteCount": 14,
        "content": "C and E are correct\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/repartition\nBoth touch on optimization"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52025-exam-dp-200-topic-2-question-12-discussion/",
    "body": "SIMULATION -<br><img src=\"/assets/media/exam-media/03872/0015900001.jpg\" class=\"in-exam-image\"><br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10277521 -<br>You plan to generate large amounts of real-time data that will be copied to Azure Blob storage.<br>You plan to create reports that will read the data from an Azure Cosmos DB database.<br>You need to create an Azure Stream Analytics job that will input the data from a blob storage named storage10277521 to the Cosmos DB database.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "Step 1: Create a Stream Analytics job<br>1. Sign in to the Azure portal.<br>2. Select Create a resource in the upper left-hand corner of the Azure portal.<br>3. Select Analytics &gt; Stream Analytics job from the results list.<br>4. Fill out the Stream Analytics job page.<br><img src=\"/assets/media/exam-media/03872/0016100001.jpg\" class=\"in-exam-image\"><br>5. Check the Pin to dashboard box to place your job on your dashboard and then select Create.<br>6. You should see a Deployment in progress... notification displayed in the top right of your browser window.<br><br>Step 2: Configure job input -<br>1. Navigate to your Stream Analytics job.<br>2. Select Inputs &gt; Add Stream input &gt; Azure Blob storage<br><img src=\"/assets/media/exam-media/03872/0016200001.jpg\" class=\"in-exam-image\"><br>3. In the Azure Blob storage setting choose: storage10277521. Leave other options to default values and select Save to save the settings.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-06T16:32:00.000Z",
        "voteCount": 8,
        "content": "Need to configure output section as well -- In the output select Azure cosmos db and select database and container or else need to create new ones"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55869-exam-dp-200-topic-2-question-14-discussion/",
    "body": "SIMULATION -<br><img src=\"/assets/media/exam-media/03872/0017100001.jpg\" class=\"in-exam-image\"><br>Use the following login credentials as needed:<br><br>Azure Username: xxxxx -<br><br>Azure Password: xxxxx -<br>The following information is for technical support purposes only:<br><br>Lab Instance: 10277521 -<br>You plan to create multiple pipelines in a new Azure Data Factory V2.<br>You need to create the data factory, and then create a scheduled trigger for the planned pipelines. The trigger must execute every two hours starting at 24:00:00.<br>To complete this task, sign in to the Azure portal.<br>",
    "options": [],
    "answer": "See the explanation below.",
    "answerDescription": "Step 1: Create a new Azure Data Factory V2<br>1. Go to the Azure portal.<br>2. Select Create a resource on the left menu, select Analytics, and then select Data Factory.<br><img src=\"/assets/media/exam-media/03872/0017300001.jpg\" class=\"in-exam-image\"><br>4. On the New data factory page, enter a name.<br>5. For Subscription, select your Azure subscription in which you want to create the data factory.<br>6. For Resource Group, use one of the following steps:<br>\u2711 Select Use existing, and select an existing resource group from the list.<br>\u2711 Select Create new, and enter the name of a resource group.<br>7. For Version, select V2.<br>8. For Location, select the location for the data factory.<br>9. Select Create.<br>10. After the creation is complete, you see the Data Factory page.<br>Step 2: Create a schedule trigger for the Data Factory<br>1. Select the Data Factory you created, and switch to the Edit tab.<br><img src=\"/assets/media/exam-media/03872/0017400003.png\" class=\"in-exam-image\"><br>2. Click Trigger on the menu, and click New/Edit.<br><img src=\"/assets/media/exam-media/03872/0017500001.png\" class=\"in-exam-image\"><br>3. In the Add Triggers page, click Choose trigger..., and click New.<br><img src=\"/assets/media/exam-media/03872/0017500002.png\" class=\"in-exam-image\"><br>4. In the New Trigger page, do the following steps:<br>a. Confirm that Schedule is selected for Type.<br>b. Specify the start datetime of the trigger for Start Date (UTC) to: 24:00:00 c. Specify Recurrence for the trigger. Select Every Hour, and enter 2 in the text box.<br><img src=\"/assets/media/exam-media/03872/0017600001.png\" class=\"in-exam-image\"><br>5. In the New Trigger window, check the Activated option, and click Next.<br>6. In the New Trigger page, review the warning message, and click Finish.<br>7. Click Publish to publish changes to Data Factory. Until you publish changes to Data Factory, the trigger does not start triggering the pipeline runs.<br><img src=\"/assets/media/exam-media/03872/0017700001.png\" class=\"in-exam-image\"><br>References:<br>https://docs.microsoft.com/en-us/azure/data-factory/quickstart-create-data-factory-portal https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-schedule-trigger",
    "votes": [],
    "comments": [
      {
        "date": "2021-07-18T06:48:00.000Z",
        "voteCount": 2,
        "content": "How appears this question type into the exam?, We need to do in the azure portal, or drag and drop box, or is in other way?. Help please"
      },
      {
        "date": "2021-06-22T16:09:00.000Z",
        "voteCount": 1,
        "content": "Correct."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14282-exam-dp-200-topic-2-question-15-discussion/",
    "body": "Each day, company plans to store hundreds of files in Azure Blob Storage and Azure Data Lake Storage. The company uses the parquet format.<br>You must develop a pipeline that meets the following requirements:<br>\u2711 Process data every six hours<br>\u2711 Offer interactive data analysis capabilities<br>\u2711 Offer the ability to process data using solid-state drive (SSD) caching<br>\u2711 Use Directed Acyclic Graph(DAG) processing mechanisms<br>\u2711 Provide support for REST API calls to monitor processes<br>\u2711 Provide native support for Python<br>\u2711 Integrate with Microsoft Power BI<br>You need to select the appropriate data technology to implement the pipeline.<br>Which data technology should you implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Apache Storm cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Apache Hadoop cluster using MapReduce",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Spark cluster"
    ],
    "answer": "B",
    "answerDescription": "Storm runs topologies instead of the Apache Hadoop MapReduce jobs that you might be familiar with. Storm topologies are composed of multiple components that are arranged in a directed acyclic graph (DAG). Data flows between the components in the graph. Each component consumes one or more data streams, and can optionally emit one or more streams.<br>Python can be used to develop Storm components.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-12T16:38:00.000Z",
        "voteCount": 30,
        "content": "this really looks like spark, https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview"
      },
      {
        "date": "2020-04-21T05:58:00.000Z",
        "voteCount": 21,
        "content": "Spark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics.\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview\nStorm processes streams of data in real time, and in the question is stated the data must be Processed every six hours"
      },
      {
        "date": "2021-05-26T15:28:00.000Z",
        "voteCount": 4,
        "content": "hadoop is dead. who cares"
      },
      {
        "date": "2021-04-27T10:33:00.000Z",
        "voteCount": 1,
        "content": "The answer is : \"E \""
      },
      {
        "date": "2021-04-16T02:36:00.000Z",
        "voteCount": 1,
        "content": "The answer is : \"E \""
      },
      {
        "date": "2021-03-04T01:58:00.000Z",
        "voteCount": 3,
        "content": "The answer is :  \"E \"\nSpark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics."
      },
      {
        "date": "2021-02-08T20:46:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview\n\nAnswer is : HDInsight Spark cluster"
      },
      {
        "date": "2021-01-23T16:33:00.000Z",
        "voteCount": 2,
        "content": "I think it must be spark too. I don't think there is a direct configuration from power bi to storm. You will need to process data in storm pass the processed data to SQL server, then visualize. Here there is no mention of SQL server , hence answer must be E. Spark.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-use-bi-tools"
      },
      {
        "date": "2021-01-18T05:17:00.000Z",
        "voteCount": 1,
        "content": "It must be E. HDInsight Spark cluster due to Offer the ability to process data using solid-state drive (SSD) caching\nhttps://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-improve-performance-iocache"
      },
      {
        "date": "2020-11-25T05:03:00.000Z",
        "voteCount": 4,
        "content": "I would say E as per link https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview:\nRead the parts on Caching on SSDs, The SparkContext connects to the Spark master and is responsible for converting an application to a directed graph (DAG) of individual tasks, REST APIs, Integration with BI Tools\nPython is mentioned as well"
      },
      {
        "date": "2020-12-08T08:18:00.000Z",
        "voteCount": 1,
        "content": "hi to all,\nit's E mostly because of the power BI, the text says exactly \"integrate with power BI\" and in the microsoft documentatio for hd insight spark cluster =&gt; \"...Integration with BI Tools\tSpark clusters in HDInsight provide connectors for BI tools such as Power BI for data analytics....\", so for me it's E\n\nregards"
      },
      {
        "date": "2020-10-05T02:04:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct. Checkout this - https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview.\nPython can also be used to develop Storm components.\nCreate solutions in multiple languages: You can write Storm components in the language of your choice, such as Java, C#, and Python."
      },
      {
        "date": "2020-09-28T02:44:00.000Z",
        "voteCount": 2,
        "content": "Storm does not provide Python (https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing) and is not used for 6hr batch processing (https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing).\nSpark supports all of the options. Hence, E is the correct answer."
      },
      {
        "date": "2020-09-12T12:19:00.000Z",
        "voteCount": 2,
        "content": "E. HDInsight Spark cluster \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-08-07T03:46:00.000Z",
        "voteCount": 3,
        "content": "it should be Spark because of (DAG + PowerBI integration)"
      },
      {
        "date": "2020-07-21T06:50:00.000Z",
        "voteCount": 7,
        "content": "Ans is HDInsight Spark cluster\nCaching on SSDs\nIntegration with BI Tools\nSpark master is responsible for converting an application to a directed graph (DAG) \nREST API-based Spark job server to remotely submit and monitor job\nfull reference: https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-overview"
      },
      {
        "date": "2020-06-02T13:12:00.000Z",
        "voteCount": 12,
        "content": "Question says need python native support. Azure Storm don't support Python. Check out this comparison chart:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-05-23T06:27:00.000Z",
        "voteCount": 8,
        "content": "I think HDInsight Apache Spark should be the correct answer.\n1. Offer interactive data analysis\n2. Offer caching \n3. DirectQuery (live connection) to PowerBI\nAnd Spark also use DAG too (https://docs.microsoft.com/en-au/azure/hdinsight/spark/apache-spark-streaming-exactly-once)."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49778-exam-dp-200-topic-2-question-16-discussion/",
    "body": "HOTSPOT -<br>A company is deploying a service-based data environment. You are developing a solution to process this data.<br>The solution must meet the following requirements:<br>\u2711 Use an Azure HDInsight cluster for data ingestion from a relational database in a different cloud service<br>\u2711 Use an Azure Data Lake Storage account to store processed data<br>\u2711 Allow users to download processed data<br>You need to recommend technologies for the solution.<br>Which technologies should you use? To answer, select the appropriate options in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0018000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0018100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Apache Sqoop -<br>Apache Sqoop is a tool designed for efficiently transferring bulk data between Apache Hadoop and structured datastores such as relational databases.<br>Azure HDInsight is a cloud distribution of the Hadoop components from the Hortonworks Data Platform (HDP).<br>Incorrect Answers:<br>DistCp (distributed copy) is a tool used for large inter/intra-cluster copying. It uses MapReduce to effect its distribution, error handling and recovery, and reporting.<br>It expands a list of files and directories into input to map tasks, each of which will copy a partition of the files specified in the source list. Its MapReduce pedigree has endowed it with some quirks in both its semantics and execution.<br>RevoScaleR is a collection of proprietary functions in Machine Learning Server used for practicing data science at scale. For data scientists, RevoScaleR gives you data-related functions for import, transformation and manipulation, summarization, visualization, and analysis.<br><br>Box 2: Apache Kafka -<br>Apache Kafka is a distributed streaming platform.<br>A streaming platform has three key capabilities:<br>Publish and subscribe to streams of records, similar to a message queue or enterprise messaging system.<br>Store streams of records in a fault-tolerant durable way.<br>Process streams of records as they occur.<br>Kafka is generally used for two broad classes of applications:<br>Building real-time streaming data pipelines that reliably get data between systems or applications<br>Building real-time streaming applications that transform or react to the streams of data<br><br>Box 3: Ambari Hive View -<br>You can run Hive queries by using Apache Ambari Hive View. The Hive View allows you to author, optimize, and run Hive queries from your web browser.<br>References:<br>https://sqoop.apache.org/<br>https://kafka.apache.org/intro<br>https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-26T15:10:00.000Z",
        "voteCount": 4,
        "content": "Apache Sqoop, Apache Hive, Ambari Hive View"
      },
      {
        "date": "2021-05-12T07:45:00.000Z",
        "voteCount": 1,
        "content": "For Process, I think it should be hive \nFor Download, the answer seems correct. But instead of 'Ambari Hive View', I think it should be 'Apache Hive View'"
      },
      {
        "date": "2021-04-16T02:39:00.000Z",
        "voteCount": 1,
        "content": "For Process it should be 'Hive' as it provide full storage mechanism"
      },
      {
        "date": "2021-04-10T03:22:00.000Z",
        "voteCount": 2,
        "content": "can't be \"hive\" for process task?"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/7753-exam-dp-200-topic-2-question-17-discussion/",
    "body": "A company uses Azure SQL Database to store sales transaction data. Field sales employees need an offline copy of the database that includes last year's sales on their laptops when there is no internet connection available.<br>You need to create the offline export copy.<br>Which three options can you use? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport to a BACPAC file by using Azure Cloud Shell, and save the file to an Azure storage account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport to a BACPAC file by using SQL Server Management Studio. Save the file to an Azure storage account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport to a BACPAC file by using the Azure portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport to a BACPAC file by using Azure PowerShell and save the file locally",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport to a BACPAC file by using the SqlPackage utility"
    ],
    "answer": "BCE",
    "answerDescription": "You can export to a BACPAC file using the Azure portal.<br>You can export to a BACPAC file using SQL Server Management Studio (SSMS). The newest versions of SQL Server Management Studio provide a wizard to export an Azure SQL database to a BACPAC file.<br>You can export to a BACPAC file using the SQLPackage utility.<br>Incorrect Answers:<br>D: You can export to a BACPAC file using PowerShell. Use the New-AzSqlDatabaseExport cmdlet to submit an export database request to the Azure SQL<br>Database service. Depending on the size of your database, the export operation may take some time to complete. However, the file is not stored locally.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-export",
    "votes": [],
    "comments": [
      {
        "date": "2019-11-06T09:41:00.000Z",
        "voteCount": 31,
        "content": "Shouldn't the solution be C, D and E ?\n\nAll tools are valid way to create the BACPAC file, but A and B store it online to Azure Storage, so it cannot be accessed offline"
      },
      {
        "date": "2019-11-20T09:13:00.000Z",
        "voteCount": 4,
        "content": "The question is not how to distribute the offline copy, but how to create the original export, which would then be distributed."
      },
      {
        "date": "2020-03-06T10:52:00.000Z",
        "voteCount": 3,
        "content": "agree with STH\n\"...includes last year's sales on their laptops when there is no internet connection available.\". If there is no internet connection, no way to connect to Azure BLOB storage"
      },
      {
        "date": "2020-07-04T07:45:00.000Z",
        "voteCount": 3,
        "content": "When it's not available != no network access ever."
      },
      {
        "date": "2021-04-10T22:10:00.000Z",
        "voteCount": 4,
        "content": "Without internet how you access azure portal?"
      },
      {
        "date": "2021-05-13T07:13:00.000Z",
        "voteCount": 2,
        "content": "You cannot really save a BACPAC file locally using PowerShell either - only to Azure Storage account.\n\nTo export with PowerShell, you must use command \"New-AzSqlDatabaseExport\", and you have to specify a parameter \"-StorageUri\" which specifies Blob location:\nhttps://docs.microsoft.com/en-us/powershell/module/az.sql/new-azsqldatabaseexport?view=azps-5.9.0\n\nSo, answer D also stores it online and hence is not correct."
      },
      {
        "date": "2020-01-08T04:52:00.000Z",
        "voteCount": 12,
        "content": "Correct Answer : B , C and E"
      },
      {
        "date": "2021-06-12T03:07:00.000Z",
        "voteCount": 1,
        "content": "its B,D,E - according to Whizlabs"
      },
      {
        "date": "2021-05-21T07:48:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL Managed Instance does not currently support exporting a database to a BACPAC file using Azure PowerShell. To export a managed instance into a BACPAC file, use SQL Server Management Studio or SQLPackage."
      },
      {
        "date": "2021-05-16T18:35:00.000Z",
        "voteCount": 2,
        "content": "C,D,E because A,B are save data to Azure storage account"
      },
      {
        "date": "2021-05-13T06:45:00.000Z",
        "voteCount": 4,
        "content": "Correct answer is B, C, E.\n\nI came to this conclusion by eliminating incorrect ones.\n\nThere are 4 ways to export an SQL Database to a BACPAC file:\n1. Export using Azure Portal -&gt; only to Storage Account\n2. Export using SQLPackage command-line utility -&gt; to local destination\n3. Export using SQL Server Management Studio -&gt; to local or to Storage Account\n4. Export using PowerShell -&gt; only to Storage Account\n\nSo, answer A is incorrect because you cannot export a BACPAC file by using Cloud Shell.\nAnswer D is incorrect because you cannot save a BACPAC file locally using PowerShell, but only to Storage account.\n\nAnother thing to note is that I think it is assumed that people will download BACPAC file from Azure Storage Account right after it is saved there and have them locally when the Internet breaks."
      },
      {
        "date": "2021-05-13T07:22:00.000Z",
        "voteCount": 1,
        "content": "Just to elaborate a bit more on D, I think that Microsoft purposely wrote the incorrect statement \"and save the file locally\" so we can interpret it as an invalid statement. (Even though you can save it to Storage Account and download it locally like with Azure portal.) \nNotice how they do not emphasize where to save files in answers C and E, and how they emphasize Storage Account in answers A and B. That's why I think that the emphasis is on the part \"and save the file locally\"."
      },
      {
        "date": "2021-05-11T00:01:00.000Z",
        "voteCount": 2,
        "content": "answer should be C, D &amp; E.\nmoreover Option A &amp; B are itself saying to save the file to Azure Storage Account and getting the file from Azure Storage Account is practically impossible with internet connection."
      },
      {
        "date": "2021-05-10T13:09:00.000Z",
        "voteCount": 1,
        "content": "\"Field sales employees need an offline copy of the database that includes last year\u05d2\u20ac\u2122s sales on their laptops when there is no internet connection available.\" -&gt; The BACPAC file contains only metadata, no data. How is it possible to include the last year sales?"
      },
      {
        "date": "2021-05-02T19:07:00.000Z",
        "voteCount": 2,
        "content": "CDE are the appropriate answers"
      },
      {
        "date": "2021-05-10T22:04:00.000Z",
        "voteCount": 1,
        "content": "B,C,D,E are ways to perform database export process though if the requirement is to have the file locally then the appropriate solution are C,D,E. Both C and D requires azure blob storage to store the copy and in option D, it was stated that it will save the file locally. The option E is straight forward the user indicates the target drive and folder before the export begins. As for option 'B' using it exports the data into the local drive but it was directed to azure storage account."
      },
      {
        "date": "2021-05-01T06:41:00.000Z",
        "voteCount": 1,
        "content": "CDE:\nhttps://www.sqlshack.com/how-to-perform-azure-sql-database-import-export-operations-using-powershell/"
      },
      {
        "date": "2021-04-16T02:42:00.000Z",
        "voteCount": 1,
        "content": "It should be C,D,E"
      },
      {
        "date": "2021-03-09T05:48:00.000Z",
        "voteCount": 1,
        "content": "the key is here \n\"on their laptops when there is no internet connection available\"\n\nIf you don't have an internet connection, how do you access a blob storage?\nI think C, D and E"
      },
      {
        "date": "2021-03-06T02:50:00.000Z",
        "voteCount": 2,
        "content": "B, D, E. Azure Portal cannot be an option. please see link below.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/database-export"
      },
      {
        "date": "2021-01-06T11:51:00.000Z",
        "voteCount": 1,
        "content": "I expect the issue with the PowerShell option is that while it can be used to export the BACPAC it is not exported to local storage. From the wording of that choice it seems to imply it is a direct export to local storage which is not possible."
      },
      {
        "date": "2021-01-04T18:03:00.000Z",
        "voteCount": 1,
        "content": "Option B , D &amp; E , since even though you can create a backup using the Azure Portal, the backup won\u2019t be available locally.\n\n\nExport to a BACPAC file by using SQL Server Management Studio. Save the file to an Azure storage account\nExport to a BACPAC file by using Azure PowerShell and save the file locally\nExport to a BACPAC file by using the SqlPackage utility"
      },
      {
        "date": "2020-11-25T05:14:00.000Z",
        "voteCount": 2,
        "content": "I would go for BCE as the link provided supports it"
      },
      {
        "date": "2020-09-27T08:42:00.000Z",
        "voteCount": 1,
        "content": "As far as I have searched\n- Export to a BACPAC file by using Azure Cloud Shell is not possible\n- Cannot save the BACPAC file locally via a straight forward Azure powershell command\nSo, the answers given are correct in the sense that they can produce a BACPAC file for consumption."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69067-exam-dp-200-topic-2-question-18-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Create an external data source pointing to the Azure Data Lake Gen 2 storage account<br>2. Create an external file format and external table using the external data source<br>3. Load the data using the CREATE TABLE AS SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "You need to create an external file format and external table using the external data source.<br>You load the data using the CREATE TABLE AS SELECT statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2022-05-06T07:57:00.000Z",
        "voteCount": 2,
        "content": "answer should be NO ..As it is from data source &amp; fileformat"
      },
      {
        "date": "2021-12-30T06:55:00.000Z",
        "voteCount": 3,
        "content": "should it be \"No\" as last step shall be Create external table ...with (location..,data_source=..,fileformat=..)?"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53423-exam-dp-200-topic-2-question-19-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Create a remote service binding pointing to the Azure Data Lake Gen 2 storage account<br>2. Create an external file format and external table using the external data source<br>3. Load the data using the CREATE TABLE AS SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "You need to create an external file format and external table from an external data source, instead from a remote service binding pointing.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-24T23:50:00.000Z",
        "voteCount": 1,
        "content": "1. Create an external data source pointing to the Azure Data Lake Gen 2 storage account\nvs\n1. Create a remote service binding pointing to the Azure Data Lake Gen 2 storage account\n\nWhich is correct?"
      },
      {
        "date": "2021-08-13T08:11:00.000Z",
        "voteCount": 2,
        "content": "A\nAnswer is correct"
      },
      {
        "date": "2021-05-23T08:37:00.000Z",
        "voteCount": 1,
        "content": "This should be the correct answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55159-exam-dp-200-topic-2-question-21-discussion/",
    "body": "You need to develop a pipeline for processing data. The pipeline must meet the following requirements:<br>\u2711 Scale up and down resources for cost reduction<br>\u2711 Use an in-memory data processing engine to speed up ETL and machine learning operations.<br>\u2711 Use streaming capabilities<br>\u2711 Provide the ability to code in SQL, Python, Scala, and R<br>Integrate workspace collaboration with Git<br><img src=\"/assets/media/exam-media/03872/0018500005.png\" class=\"in-exam-image\"><br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Spark Cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Hadoop Cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Kafka Cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Storm Cluster"
    ],
    "answer": "A",
    "answerDescription": "Aparch Spark is an open-source, parallel-processing framework that supports in-memory processing to boost the performance of big-data analysis applications.<br>HDInsight is a managed Hadoop service. Use it deploy and manage Hadoop clusters in Azure. For batch processing, you can use Spark, Hive, Hive LLAP,<br>MapReduce.<br>Languages: R, Python, Java, Scala, SQL<br>You can create an HDInsight Spark cluster using an Azure Resource Manager template. The template can be found in GitHub.<br>References:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-12T07:26:00.000Z",
        "voteCount": 4,
        "content": "Answer is CORECT. key word is in-memory data processing"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/36611-exam-dp-200-topic-2-question-22-discussion/",
    "body": "DRAG DROP -<br>You implement an event processing solution using Microsoft Azure Stream Analytics.<br>The solution must meet the following requirements:<br>\u2711 Ingest data from Blob storage<br>\u2711 Analyze data in real time<br>\u2711 Store processed data in Azure Cosmos DB<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0018700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0018800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Configure Blob storage as input; select items with the TIMESTAMP BY clause<br>The default timestamp of Blob storage events in Stream Analytics is the timestamp that the blob was last modified, which is BlobLastModifiedUtcTime. To process the data as a stream using a timestamp in the event payload, you must use the TIMESTAMP BY keyword.<br>Example:<br>The following is a TIMESTAMP BY example which uses the EntryTime column as the application time for events:<br>SELECT TollId, EntryTime AS VehicleEntryTime, LicensePlate, State, Make, Model, VehicleType, VehicleWeight, Toll, Tag<br>FROM TollTagEntry TIMESTAMP BY EntryTime<br>Step 2: Set up cosmos DB as the output<br>Creating Cosmos DB as an output in Stream Analytics generates a prompt for information as seen below.<br><img src=\"/assets/media/exam-media/03872/0018900001.jpg\" class=\"in-exam-image\"><br>Step 3: Create a query statement with the SELECT INTO statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-02T02:36:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-02-10T17:34:00.000Z",
        "voteCount": 2,
        "content": "given answer is correct"
      },
      {
        "date": "2020-11-09T14:22:00.000Z",
        "voteCount": 3,
        "content": "query: Select .. INTO.. FROM..Having... https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17540-exam-dp-200-topic-2-question-23-discussion/",
    "body": "HOTSPOT -<br>A company plans to use Platform-as-a-Service (PaaS) to create the new data pipeline process. The process must meet the following requirements:<br>Ingest:<br>\u2711 Access multiple data sources.<br>\u2711 Provide the ability to orchestrate workflow.<br>\u2711 Provide the capability to run SQL Server Integration Services packages.<br>Store:<br>\u2711 Optimize storage for big data workloads<br>\u2711 Provide encryption of data at rest.<br>\u2711 Operate with no size limits.<br>Prepare and Train:<br>\u2711 Provide a fully-managed and interactive workspace for exploration and visualization.<br>\u2711 Provide the ability to program in R, SQL, Python, Scala, and Java.<br>\u2711 Provide seamless user authentication with Azure Active Directory.<br>Model &amp; Serve:<br>\u2711 Implement native columnar storage.<br>\u2711 Support for the SQL language.<br>\u2711 Provide support for structured streaming.<br>You need to build the data integration pipeline.<br>Which technologies should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0019100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0019300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Ingest: Azure Data Factory -<br>Azure Data Factory pipelines can execute SSIS packages.<br>In Azure, the following services and tools will meet the core requirements for pipeline orchestration, control flow, and data movement: Azure Data Factory, Oozie on HDInsight, and SQL Server Integration Services (SSIS).<br><br>Store: Data Lake Storage -<br>Data Lake Storage Gen1 provides unlimited storage.<br>Note: Data at rest includes information that resides in persistent storage on physical media, in any digital format. Microsoft Azure offers a variety of data storage solutions to meet different needs, including file, disk, blob, and table storage. Microsoft also provides encryption to protect Azure SQL Database, Azure Cosmos<br>DB, and Azure Data Lake.<br>Prepare and Train: Azure Databricks<br>Azure Databricks provides enterprise-grade Azure security, including Azure Active Directory integration.<br>With Azure Databricks, you can set up your Apache Spark environment in minutes, autoscale and collaborate on shared projects in an interactive workspace.<br>Azure Databricks supports Python, Scala, R, Java and SQL, as well as data science frameworks and libraries including TensorFlow, PyTorch and scikit-learn.<br>Model and Serve: Azure Synapse Analytics<br>Azure Synapse Analytics/ SQL Data Warehouse stores data into relational tables with columnar storage.<br>Azure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse. Access SQL Data<br>Warehouse from Azure Databricks using the SQL Data Warehouse connector.<br>Note: Note: As of November 2019, Azure SQL Data Warehouse is now Azure Synapse Analytics.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/architecture/data-guide/technology-choices/pipeline-orchestration-data-movement https://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-28T03:14:00.000Z",
        "voteCount": 15,
        "content": "Answer given is correct"
      },
      {
        "date": "2020-08-07T04:22:00.000Z",
        "voteCount": 1,
        "content": "Search Results\nFeatured snippet from the web\nAzure SQL Data Warehouse connector now offers efficient and scalable structured streaming write support for SQL Data Warehouse."
      },
      {
        "date": "2020-07-21T04:51:00.000Z",
        "voteCount": 3,
        "content": "Prepare and train: Databricks, also as Spark does not support Azure AD and Databricks does"
      },
      {
        "date": "2020-06-05T01:35:00.000Z",
        "voteCount": 3,
        "content": "Not sure Databricks is the good answer. Databricks does not support Java. Should be HDInsight Spark."
      },
      {
        "date": "2020-06-10T16:51:00.000Z",
        "voteCount": 7,
        "content": "Databricks supports Java and all mentioned languages since it's optimized Apache Spark: https://azure.microsoft.com/en-us/services/databricks/#overview"
      },
      {
        "date": "2020-07-01T10:57:00.000Z",
        "voteCount": 4,
        "content": "I think the confusion of databricks not supporting java is because, there was a question in here answered saying so. But yes i agree. Databrick is the right answer as it supports the required languages including Java."
      },
      {
        "date": "2020-10-15T12:34:00.000Z",
        "voteCount": 3,
        "content": "Currently when I go to my Databricks cluster and I create a new notebook the dropdown does not list Java, I think that is the reason people think it is not supported. BUT you can create a  java JAR, and upload that to Databricks THEN access the classes in it."
      },
      {
        "date": "2020-03-27T00:09:00.000Z",
        "voteCount": 1,
        "content": "Anyone, any idea about Azure SQL Data Warehouse feature for \"structured streaming\"?"
      },
      {
        "date": "2020-04-11T22:05:00.000Z",
        "voteCount": 8,
        "content": "https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics#streaming-support"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8619-exam-dp-200-topic-2-question-24-discussion/",
    "body": "HOTSPOT -<br>A company plans to analyze a continuous flow of data from a social media platform by using Microsoft Azure Stream Analytics. The incoming data is formatted as one record per row.<br>You need to create the input stream.<br>How should you complete the REST API segment? To answer, select the appropriate configuration in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0019600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0019800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: CSV -<br>A comma-separated values (CSV) file is a delimited text file that uses a comma to separate values. A CSV file stores tabular data (numbers and text) in plain text.<br>Each line of the file is a data record.<br>JSON and AVRO are not formatted as one record per row.<br>Box 2: \"type\":\"Microsoft.ServiceBus/EventHub\",<br>Properties include \"EventHubName\"<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs https://en.wikipedia.org/wiki/Comma-separated_values",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-01T23:03:00.000Z",
        "voteCount": 17,
        "content": "Check this:\n{    \n   \"properties\":{    \n      \"type\":\"stream\",  \n      \"serialization\":{    \n         \"type\":\"CSV\",  \n         \"properties\":{    \n            \"fieldDelimiter\":\",\",  \n            \"encoding\":\"UTF8\"  \n         }  \n      },  \n      \"datasource\":{    \n         \"type\":\"Microsoft.ServiceBus/EventHub\",  \n         \"properties\":{    \n            \"serviceBusNamespace\":\"sampleServiceBus\",  \n            \"sharedAccessPolicyName\":\"SampleReceiver\",  \n            \"sharedAccessPolicyKey\":\"***/**********/*****************************\",  \n            \"eventHubName\":\"sampleEventHub\"  \n         }  \n      },  \n      \"compression\":{    \n         \"type\":\"GZip\" \n      }  \n   }  \n}\nhttps://docs.microsoft.com/es-es/rest/api/streamanalytics/stream-analytics-input"
      },
      {
        "date": "2020-03-04T03:30:00.000Z",
        "voteCount": 15,
        "content": "CSV does not have to be\"only comma separated\". Given answer is correct."
      },
      {
        "date": "2021-01-18T06:16:00.000Z",
        "voteCount": 2,
        "content": "AVRO is a serialized JSON. Therefore, it must be CSV"
      },
      {
        "date": "2020-05-13T14:39:00.000Z",
        "voteCount": 2,
        "content": "fieldDelimiter is ONLY for CSV type data in ASA Input Serialization JSON file. So, CSV is the right answer."
      },
      {
        "date": "2019-11-19T14:25:00.000Z",
        "voteCount": 3,
        "content": "CSV is comma separated, not dot.\nAvro is made for serialization, so it's probably the right format."
      },
      {
        "date": "2020-10-06T12:38:00.000Z",
        "voteCount": 1,
        "content": "wrong response for me.. csv may have comma, dot, star so may have any characters"
      },
      {
        "date": "2021-03-29T05:24:00.000Z",
        "voteCount": 3,
        "content": "\"One record per row\" -&gt; CSV. Avro is in columnar format for reads optimization."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49796-exam-dp-200-topic-2-question-25-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.<br>You have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.<br>You need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure<br>Data Lake Storage Gen2 for long-term retention.<br>Solution: You implement a Stream Analytics job that has one streaming input, one query, and two outputs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We need one reference data input for LocationIncomes, which rarely changes.<br>Note: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-10T06:09:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is B) , \"add reference input\" is missing.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26050-exam-dp-200-topic-2-question-26-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.<br>You have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.<br>You need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure<br>Data Lake Storage Gen2 for long-term retention.<br>Solution: You implement a Stream Analytics job that has one streaming input, one reference input, one query, and two outputs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We need one reference data input for LocationIncomes, which rarely changes.<br>We need two queries, on for in-store customers, and one for online customers.<br>For each query two outputs is needed.<br>Note: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-11T21:01:00.000Z",
        "voteCount": 21,
        "content": "This is the correct answer --&gt; \"You implement a Stream Analytics job that has one streaming input, one reference input, one query, and two outputs.\""
      },
      {
        "date": "2020-12-12T02:37:00.000Z",
        "voteCount": 6,
        "content": "hi to all,\nit's correct: A\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs\nin one query we can join reference data\ntwo outputs because of two different database systems\n\nregards"
      },
      {
        "date": "2020-12-12T02:40:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/stream-analytics-query/reference-data-join-azure-stream-analytics"
      },
      {
        "date": "2020-10-29T00:19:00.000Z",
        "voteCount": 2,
        "content": "We don't need streaming input, hence answer - no"
      },
      {
        "date": "2021-11-27T20:19:00.000Z",
        "voteCount": 1,
        "content": "No, Stream analytics jobs must have at least one stream input.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#data-stream-input"
      },
      {
        "date": "2020-09-03T08:23:00.000Z",
        "voteCount": 2,
        "content": "According to the documentation you can have two output types in a query. \"You can use a single output per job, or multiple outputs per streaming job (if you need them) by adding multiple INTO clauses to the query.\" I think the answer is A."
      },
      {
        "date": "2020-09-30T00:58:00.000Z",
        "voteCount": 1,
        "content": "I agree with BungyTex ... you only need one query with 2 output types .. i have done it in practice ... so can validate it works ..."
      },
      {
        "date": "2020-11-22T14:18:00.000Z",
        "voteCount": 1,
        "content": "To back BungyTex and sandGrain, here' the doc: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs#:~:text=An%20Azure%20Stream%20Analytics%20job,%2C%20query%2C%20and%20an%20output.&amp;text=When%20you%20design%20your%20Stream,INTO%20clauses%20to%20the%20query."
      },
      {
        "date": "2020-12-22T07:20:00.000Z",
        "voteCount": 1,
        "content": "Hi Jumby, in the link you provide, the text says that there can be multiple INTO statements in the \"query\". However, in the following link, it can be seen that two INTO statements require two SELECT statements: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#query-example-send-data-to-multiple-outputs\nIt seems that different authors at Microsoft disagree on exactly what a query is. If one query is equal to one SELECT statement, as most SQL literature defines it, then then two INTO statements would mean two queries. The author of the article on Streaming Analytics uses a different definition of \"query\". IMHO he should have said \"job\" instead of \"query\". All very confusing, I'd say, particularly since an exam question rests entirely of which version of the definition one uses."
      },
      {
        "date": "2020-12-22T07:13:00.000Z",
        "voteCount": 1,
        "content": "In your quote \"multiple outputs per streaming job\", the key word \"job\" seems different to me from a \"query\". The test question refers to number of queries, not the number of jobs. If one query is equal to one SELECT statement, then one job can have multiple queries."
      },
      {
        "date": "2020-07-20T21:05:00.000Z",
        "voteCount": 3,
        "content": "We need to two query to load in different targets as stated in question:\n \"You must output the data to Azure SQL Database for immediate use and to Azure\nData Lake Storage Gen2 for long-term retention.\""
      },
      {
        "date": "2021-04-13T11:32:00.000Z",
        "voteCount": 1,
        "content": "Yeah but that means you save the SAME result (only one query) in two services because of different purposes. So I think that one query is enough."
      },
      {
        "date": "2020-07-18T06:43:00.000Z",
        "voteCount": 3,
        "content": "I don't see why you couldn't use one query since both sources are in the one customer file.  It doesn't say you need two sets of queries for each source in the scenario.  I disagree."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25766-exam-dp-200-topic-2-question-27-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.<br>You have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.<br>You need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure<br>Data Lake Storage Gen2 for long-term retention.<br>Solution: You implement a Stream Analytics job that has one streaming input, one reference input, two queries, and four outputs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "We need one reference data input for LocationIncomes, which rarely changes.<br>We need two queries, on for in-store customers, and one for online customers.<br>For each query two outputs is needed.<br>Note: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-03T08:27:00.000Z",
        "voteCount": 11,
        "content": "It would meet the goal, but why would you even create 2 queries? You can get there with just 1 query and two outputs."
      },
      {
        "date": "2021-02-20T07:30:00.000Z",
        "voteCount": 1,
        "content": "no you can''t, you have 2 \"INTO\""
      },
      {
        "date": "2021-05-02T19:53:00.000Z",
        "voteCount": 1,
        "content": "NO is the answer\n\nReference: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns#query-example-send-data-to-multiple-outputs"
      },
      {
        "date": "2021-05-06T22:08:00.000Z",
        "voteCount": 2,
        "content": "Here is the scenario, there is a file that contains two dataset in-store and online customer and it requires to load it into Azure SQL and Data Lake. In coding perspective each query is equivalent to ONE select into which results to two queries to load the requirement. However, if we are going to combine the in-store and online customers then it is only ONE query. Since it was not stated here to unify the datasets then it is best to assume that they will be loaded into two tables. In azure SQL it will be loaded to two tables then in Azure Datalake there will be also two placeholder. Therefore, there are 2 queries and 4 outputs in total."
      },
      {
        "date": "2021-04-19T02:40:00.000Z",
        "voteCount": 1,
        "content": "\"NO\". we can handle two inputs i.e two SELECT statements in one query. so no need of two queries.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data#joining-multiple-reference-datasets-in-a-job"
      },
      {
        "date": "2020-12-12T08:03:00.000Z",
        "voteCount": 1,
        "content": "I think the problem is that its a bit confusing what is \"one query\". Is it all you can  do into query editor or is each time you use separated \"select - into - from\" ? If its the second one, so you need to create 2 queries, each one with different \"INTO output\""
      },
      {
        "date": "2020-12-22T06:57:00.000Z",
        "voteCount": 1,
        "content": "Good point about \"what is a query\"... \nTwo SELECT clauses are needed if there are two INTO statements, as per this link: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns: \n(quote from above link) \"Multiple SELECT statements can be used to output data to different output sinks. For example, one SELECT can output a threshold-based alert while another one can output events to blob storage.\""
      },
      {
        "date": "2020-12-12T02:44:00.000Z",
        "voteCount": 3,
        "content": "hi to all,\nfor it's false\njust because we have 1 query and not 2 queries; the query already joins the main data with the reference data so it's one query\nthe rest is OK\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-troubleshoot-query\n\nregards"
      },
      {
        "date": "2021-02-20T07:32:00.000Z",
        "voteCount": 1,
        "content": "it's one query, with 2 outputs and not 4 ?"
      },
      {
        "date": "2020-12-01T04:15:00.000Z",
        "voteCount": 3,
        "content": "Hmm...so long as it meets the goal the answer then the answer is yes then.\nYes the suggested solution does kind of \"overdo\" it but no harm in it I suppose.\nJust my own thoughts about it so I will go with Yes in this case."
      },
      {
        "date": "2020-10-01T03:05:00.000Z",
        "voteCount": 2,
        "content": "There is only one requirement in teh question \"You need to use an address to look up a median income based on location\". 1 Query and 2 outputs makes sense. Not sure why we need the second query for? anyone?"
      },
      {
        "date": "2020-09-07T19:20:00.000Z",
        "voteCount": 1,
        "content": "The online customers will provide a mailing address.\n\nYou need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure\nDoes this mean that we need to have 2 diferent queries to get this, if there are 2 queries then 4 outputs"
      },
      {
        "date": "2020-08-09T05:18:00.000Z",
        "voteCount": 3,
        "content": "Not clear why 4 outputs are required, as long as no specific requirement to provide separate outputs per customer type (in-store and online customers)\n2 query, 2 outputs shall be the right answer accordingly also to:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns\n\nAnyone, is an option with 2 queries and 2 outputs in the exam?"
      },
      {
        "date": "2020-07-14T13:46:00.000Z",
        "voteCount": 1,
        "content": "There are only two queries, why need for four outputs? I think the answer should be No."
      },
      {
        "date": "2020-07-17T10:14:00.000Z",
        "voteCount": 6,
        "content": "The query output need to be stored in two places: SQL DW and Data lake both.\nSo it's 2x2"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26059-exam-dp-200-topic-2-question-28-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to create an Azure Databricks workspace that has a tiered structure. The workspace will contain the following three workloads:<br>\u2711 A workload for data engineers who will use Python and SQL<br>\u2711 A workload for jobs that will run notebooks that use Python, Scala, and SQL<br>\u2711 A workload that data scientists will use to perform ad hoc analysis in Scala and R<br>The enterprise architecture team at your company identifies the following standards for Databricks environments:<br>\u2711 The data engineers must share a cluster.<br>\u2711 The job cluster will be managed by using a request process whereby data scientists and data engineers provide packaged notebooks for deployment to the cluster.<br>\u2711 All the data scientists must be assigned their own cluster that terminates automatically after 120 minutes of inactivity. Currently, there are three data scientists.<br>You need to create the Databricks clusters for the workloads.<br>Solution: You create a Standard cluster for each data scientist, a Standard cluster for the data engineers, and a High Concurrency cluster for the jobs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We need a High Concurrency cluster for the data engineers and the jobs.<br>Note:<br>Standard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.<br>A high concurrency cluster is a managed cloud resource. The key benefits of high concurrency clusters are that they provide Apache Spark-native fine-grained sharing for maximum resource utilization and minimum query latencies.<br>References:<br>https://docs.azuredatabricks.net/clusters/configure.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-18T13:21:00.000Z",
        "voteCount": 26,
        "content": "As job notebooks include scala and high concurrency clusters do not support scala, the answer should be no."
      },
      {
        "date": "2020-08-31T23:25:00.000Z",
        "voteCount": 3,
        "content": "Correct, check: https://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2022-09-10T00:43:00.000Z",
        "voteCount": 1,
        "content": "this is 100% correct, it's as simple as this, job requires scala, high concurrency does not support scala. The answer is no."
      },
      {
        "date": "2020-07-29T14:39:00.000Z",
        "voteCount": 9,
        "content": "If job needs to use scala then high concurrency cluster can't be used as that wont support Scala"
      },
      {
        "date": "2024-08-12T18:01:00.000Z",
        "voteCount": 1,
        "content": "B. as high concurrency clusters do not support Scala."
      },
      {
        "date": "2021-05-16T20:39:00.000Z",
        "voteCount": 2,
        "content": "B is corect \nThere are two options for cluster mode:\n\nStandard: Single user / small group clusters - can use any language.\nHigh Concurrency: A cluster built for minimizing latency in high concurrency workloads.\nThere are a few main reasons you would use a Standard cluster over a high concurrency cluster.  The first is if you are a single user of Databricks exploring the technology.  For most PoCs and exploration, a Standard cluster should suffice.  The second is if you are a Scala user, as high concurrency clusters do not support Scala. The third is if your use case simply does not require high concurrency processes.\n\nHigh concurrency clusters, in addition to performance gains, also allow you utilize table access control, which is not supported in Standard clusters.\n\nPlease note that High Concurrency clusters do not automatically set the auto shutdown field, whereas standard clusters default it to 120 minutes.\nhttps://www.mssqltips.com/sqlservertip/6604/azure-databricks-cluster-configuration/"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48906-exam-dp-200-topic-2-question-29-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Use Azure Data Factory to convert the parquet files to CSV files<br>2. Create an external data source pointing to the Azure Data Lake Gen 2 storage account<br>3. Create an external file format and external table using the external data source<br>4. Load the data using the CREATE TABLE AS SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "A",
    "answerDescription": "It is not necessary to convert the parquet files to CSV files.<br>You need to create an external file format and external table using the external data source.<br>You load the data using the CREATE TABLE AS SELECT statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-05T03:09:00.000Z",
        "voteCount": 9,
        "content": "It is not necessary to convert the parquet files to CSV files.--&gt; Answer No"
      },
      {
        "date": "2021-04-25T11:14:00.000Z",
        "voteCount": 14,
        "content": "indeed, it is not necessary to convert parquet to CSV, but the question is if that apporach meet the goal - A: yes"
      },
      {
        "date": "2021-11-23T11:41:00.000Z",
        "voteCount": 1,
        "content": "No need to convert to CSV File"
      },
      {
        "date": "2021-06-07T03:59:00.000Z",
        "voteCount": 1,
        "content": "why would i convert to csv ..When my further steps are going to read data from ADLS.\nExplanation says not required and ANSWER says YES."
      },
      {
        "date": "2021-05-23T01:47:00.000Z",
        "voteCount": 1,
        "content": "You can just use data factory to read parquest and ingest to Synapse with auto creating table, but per se the given solution meets the goal even if in not optimal way"
      },
      {
        "date": "2021-05-06T10:29:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is ( B-------- No )"
      },
      {
        "date": "2021-04-30T20:20:00.000Z",
        "voteCount": 2,
        "content": "The answer should be A: Yes. Since it meets the goal though it is not necessary to change the parquet to csv but you can change it."
      },
      {
        "date": "2021-04-19T18:50:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is B"
      },
      {
        "date": "2021-04-03T02:56:00.000Z",
        "voteCount": 2,
        "content": "It should be No as per the details provided."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37836-exam-dp-200-topic-2-question-30-discussion/",
    "body": "You need to implement complex stateful business logic within an Azure Stream Analytics service.<br>Which type of function should you create in the Stream Analytics topology?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJavaScript user-define functions (UDFs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJavaScript user-defined aggregates (UDA)"
    ],
    "answer": "C",
    "answerDescription": "Azure Stream Analytics supports user-defined aggregates (UDA) written in JavaScript, it enables you to implement complex stateful business logic. Within UDA you have full control of the state data structure, state accumulation, state decumulation, and aggregate result computation.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-aggregates",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-20T07:36:00.000Z",
        "voteCount": 2,
        "content": "keyword statufull, otherwise it would be 1 \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-functions\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-javascript-user-defined-aggregates"
      },
      {
        "date": "2020-11-26T05:22:00.000Z",
        "voteCount": 4,
        "content": "Link provided supports C as the answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49207-exam-dp-200-topic-2-question-31-discussion/",
    "body": "You have an Azure virtual machine that has Microsoft SQL Server installed. The database on the virtual machine contains a table named Table1.<br>You need to copy the data from Table1 to an Azure Data Lake Storage Gen2 account by using an Azure Data Factory V2 copy activity.<br>Which type of integration runtime should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tself-hosted integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure-SSIS integration runtime"
    ],
    "answer": "B",
    "answerDescription": "Copying between a cloud data source and a data source in private network: if either source or sink linked service points to a self-hosted IR, the copy activity is executed on that self-hosted Integration Runtime.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime#determining-which-ir-to-use",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-08T01:13:00.000Z",
        "voteCount": 15,
        "content": "Use the self-hosted integration runtime even if the data store is in the cloud on an Azure Infrastructure as a Service (IaaS) virtual machine.\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#considerations-for-using-a-self-hosted-ir"
      },
      {
        "date": "2021-05-24T01:27:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2021-05-10T22:52:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct either the SQL server resides in the traditional virtual server or azure virtual machine it requires \"self hosted integration runtime\" for the ADF to access the server."
      },
      {
        "date": "2021-05-03T13:16:00.000Z",
        "voteCount": 2,
        "content": "Why is B the correct answer? There is no mention that the VM running inside of a private network so shouldn't A be the correct answer?"
      },
      {
        "date": "2021-04-27T14:41:00.000Z",
        "voteCount": 1,
        "content": "correct answer is A"
      },
      {
        "date": "2021-05-01T19:12:00.000Z",
        "voteCount": 1,
        "content": "This is incorrect. Correct Answer is B.. Please refer to samkslee's reply."
      },
      {
        "date": "2021-04-05T07:20:00.000Z",
        "voteCount": 1,
        "content": "should be \"A\". All components are on cloud"
      },
      {
        "date": "2021-04-08T02:53:00.000Z",
        "voteCount": 6,
        "content": "It's SQL Server in the Azure VM server, not Azure SQL DB. Self-hosted integration runtime can be used for SQL Servers hosted on an on-premises machine or on a Azure VM."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17736-exam-dp-200-topic-2-question-32-discussion/",
    "body": "DRAG DROP -<br>Your company plans to create an event processing engine to handle streaming data from Twitter.<br>The data engineering team uses Azure Event Hubs to ingest the streaming data.<br>You need to implement a solution that uses Azure Databricks to receive the streaming data from the Azure Event Hubs.<br>Which three actions should you recommend be performed in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0020600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0020700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Deploy the Azure Databricks service<br>Create an Azure Databricks workspace by setting up an Azure Databricks Service.<br>Step 2: Deploy a Spark cluster and then attach the required libraries to the cluster.<br>To create a Spark cluster in Databricks, in the Azure portal, go to the Databricks workspace that you created, and then select Launch Workspace.<br>Attach libraries to Spark cluster: you use the Twitter APIs to send tweets to Event Hubs. You also use the Apache Spark Event Hubs connector to read and write data into Azure Event Hubs. To use these APIs as part of your cluster, add them as libraries to Azure Databricks and associate them with your Spark cluster.<br>Step 3: Create and configure a Notebook that consumes the streaming data.<br>You create a notebook named ReadTweetsFromEventhub in Databricks workspace. ReadTweetsFromEventHub is a consumer notebook you use to read the tweets from Event Hubs.<br>References:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-06T03:05:00.000Z",
        "voteCount": 6,
        "content": "Given answer is correct"
      },
      {
        "date": "2021-05-23T01:56:00.000Z",
        "voteCount": 1,
        "content": "But you don't need a lbrary in Spark to connect to Twotter as the data is already in EvenHub that you only need to connect to"
      },
      {
        "date": "2020-11-28T03:26:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs supports answer provided"
      },
      {
        "date": "2020-11-23T21:58:00.000Z",
        "voteCount": 1,
        "content": "Its mandatory to create Spark Cluster, and then create a Notebook. As the notebook needs a cluster to run."
      },
      {
        "date": "2020-06-08T04:08:00.000Z",
        "voteCount": 4,
        "content": "Always mandatory to create a spark cluster from databricks and then you can run a notebook (the notebook should be attached to an existing spark cluster)."
      },
      {
        "date": "2020-05-13T14:45:00.000Z",
        "voteCount": 2,
        "content": "Refer this article, https://docs.databricks.com/spark/latest/structured-streaming/streaming-event-hubs.html \nIn order to connect Azure Event Hub via Azure Event Hubs Spark Connector in Databricks, we need to deploy Spark Cluster and install the new created library in your Databricks workspace using the Maven coordinate."
      },
      {
        "date": "2020-05-13T14:47:00.000Z",
        "voteCount": 6,
        "content": "This tutorial explains more details. https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs"
      },
      {
        "date": "2020-03-31T10:27:00.000Z",
        "voteCount": 1,
        "content": "why is the Spark Cluster included in the answer?"
      },
      {
        "date": "2020-03-31T10:30:00.000Z",
        "voteCount": 1,
        "content": "I guess Spark is the only way to connect Databricks to an Event Hub?"
      },
      {
        "date": "2020-04-11T22:09:00.000Z",
        "voteCount": 4,
        "content": "Spark has built-in API to retrieve Twitter data"
      },
      {
        "date": "2020-04-30T23:01:00.000Z",
        "voteCount": 4,
        "content": "i guess to run notebook in databricks you need to first create spark cluster"
      },
      {
        "date": "2020-07-07T00:56:00.000Z",
        "voteCount": 8,
        "content": "Databricks is a spark-based technology. It is always mandatory to have a spark cluster for Databricks to work."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20460-exam-dp-200-topic-2-question-33-discussion/",
    "body": "HOTSPOT -<br>You develop data engineering solutions for a company.<br>A project requires an in-memory batch data processing solution.<br>You need to provision an HDInsight cluster for batch processing of data on Microsoft Azure.<br>How should you complete the PowerShell segment? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0020900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0021000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: New-AzStorageContainer -<br># Example: Create a blob container. This holds the default data store for the cluster.<br>New-AzStorageContainer `<br>-Name $clusterName `<br>-Context $defaultStorageContext<br>$sparkConfig = New-Object \"System.Collections.Generic.Dictionary``2[System.String,System.String]\"<br>$sparkConfig.Add(\"spark\", \"2.3\")<br><br>Box 2: Spark -<br>Spark provides primitives for in-memory cluster computing. A Spark job can load and cache data into memory and query it repeatedly. In-memory computing is much faster than disk-based applications than disk-based applications, such as Hadoop, which shares data through Hadoop distributed file system (HDFS).<br>Box 3: New-AzureRMHDInsightCluster<br># Create the HDInsight cluster. Example:<br>New-AzHDInsightCluster `<br>-ResourceGroupName $resourceGroupName `<br>-ClusterName $clusterName `<br>-Location $location `<br>-ClusterSizeInNodes $clusterSizeInNodes `<br>-ClusterType $\"Spark\" `<br>-OSType \"Linux\" `<br><br>Box 4: Spark -<br>HDInsight is a managed Hadoop service. Use it deploy and manage Hadoop clusters in Azure. For batch processing, you can use Spark, Hive, Hive LLAP,<br>MapReduce.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/spark/apache-spark-jupyter-spark-sql-use-powershell https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/spark/apache-spark-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-07T00:57:00.000Z",
        "voteCount": 13,
        "content": "This question is no more part of the DP-200 exam."
      },
      {
        "date": "2020-11-15T06:47:00.000Z",
        "voteCount": 2,
        "content": "why not?"
      },
      {
        "date": "2021-05-02T22:01:00.000Z",
        "voteCount": 2,
        "content": "Given answer is correct"
      },
      {
        "date": "2020-12-12T12:45:00.000Z",
        "voteCount": 1,
        "content": "imho the answer is correct, see https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-create-linux-clusters-azure-powershell"
      },
      {
        "date": "2020-05-13T05:22:00.000Z",
        "voteCount": 3,
        "content": "second answer is clearly spark because haddop is misspelled  :&gt;}"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17566-exam-dp-200-topic-2-question-34-discussion/",
    "body": "HOTSPOT -<br>A company plans to develop solutions to perform batch processing of multiple sets of geospatial data.<br>You need to implement the solutions.<br>Which Azure services should you use? To answer, select the appropriate configuration in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0021200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0021300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: HDInsight Tools for Visual Studio<br>Azure HDInsight Tools for Visual Studio Code is an extension in the Visual Studio Code Marketplace for developing Hive Interactive Query, Hive Batch Job and<br>PySpark Job against Microsoft HDInsight.<br><br>Box 2: Hive View -<br>You can use Apache Ambari Hive View with Apache Hadoop in HDInsight. The Hive View allows you to author, optimize, and run Hive queries from your web browser.<br><br>Box 3: HDInsight REST API -<br>Azure HDInsight REST APIs are used to create and manage HDInsight resources through Azure Resource Manager.<br>References:<br>https://visualstudiomagazine.com/articles/2019/01/25/vscode-hdinsight.aspx https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view https://docs.microsoft.com/en-us/rest/api/hdinsight/",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-23T07:54:00.000Z",
        "voteCount": 16,
        "content": "I think box 3 should also be HDInsight Tools for VS. \"Azure HDInsight Tools for Visual Studio Code is an extension in the Visual Studio Code Marketplace for developing Hive Interactive Query, Hive Batch Job and PySpark Job against Microsoft HDInsight.\" https://marketplace.visualstudio.com/items?itemName=mshdinsight.azure-hdinsight"
      },
      {
        "date": "2020-04-19T08:34:00.000Z",
        "voteCount": 15,
        "content": "Honestly, I think quesions and answes make no sense at all. \"Hive View\" is not a tool to run interactive queries and batch processes. On the other hand, \"REST API\" is not a tool to designed to develop \"batch applications\". It's very hard to try to make sense of these questions/answers..."
      },
      {
        "date": "2021-05-03T13:37:00.000Z",
        "voteCount": 1,
        "content": "I think that the third answer should also be HDInsight Tools for Visual Studio simply because the questions starts with \"Develop...\" which means we need to develop the batch job (and possibly push it) to the cluster. The HDInsight API only lets us publish batch jobs through pre-existing JARs which container the batch processing logic / code within itself."
      },
      {
        "date": "2020-11-28T03:46:00.000Z",
        "voteCount": 3,
        "content": "I would say HDInsight Tools for Visual Studio is the answer for both the 1st and 3rd dropdown\nHive view is answer for the 2nd dropdown\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-for-vscode\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-ambari-view"
      },
      {
        "date": "2020-12-01T05:53:00.000Z",
        "voteCount": 2,
        "content": "On second thought, Azure HDInsight REST APIs for batch processing of applications that use HDInsight\nhttps://docs.microsoft.com/en-us/rest/api/hdinsight/hdinsight-application"
      },
      {
        "date": "2020-04-28T05:40:00.000Z",
        "voteCount": 5,
        "content": "Hive View is designed to help you author, optimize, and execute queries. With Hive Views you can:\n\nBrowse databases.\nWrite queries or browse query results in full-screen mode, which can be particularly helpful with complex queries or large query results.\nManage query execution jobs and history.\nView existing databases, tables, and their statistics.\nCreate/upload tables and export table DDL to source control.\nView visual explain plans to learn more about query plan."
      },
      {
        "date": "2020-03-27T21:08:00.000Z",
        "voteCount": 6,
        "content": "I think box 1 and box 3 need to switch.\n- Visual Studio is a development environment where development is done.\n- REST API is used to call from the application being developed to HDInsight Cluster to perform actions.\nAny thoughts!"
      },
      {
        "date": "2020-04-17T03:25:00.000Z",
        "voteCount": 1,
        "content": "Don't think so. The only native application in the list is VS code, and REST api could be used to create a job processing."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55162-exam-dp-200-topic-2-question-35-discussion/",
    "body": "DRAG DROP -<br>You are creating a managed data warehouse solution on Microsoft Azure.<br>You must use PolyBase to retrieve data from Azure Blob storage that resides in parquet format and load the data into a large table called FactSalesOrderDetails.<br>You need to configure Azure Synapse Analytics to receive the data.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0021500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0021600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create a master key on the database<br>Create a master key on the database. This is required to encrypt the credential secret.<br>Step 2: Create an external data source for Azure Blob storage<br>Create an external data source with CREATE EXTERNAL DATA SOURCE..<br>Step 3: Create an external file format to map parquet files.<br>Create an external file format with CREATE EXTERNAL FILE FORMAT.<br>FORMAT TYPE: Type of format in Hadoop (DELIMITEDTEXT,  RCFILE, ORC, PARQUET).<br>Step 4: Create the external table FactSalesOrderDetails<br>To query the data in your Hadoop data source, you must define an external table to use in Transact-SQL queries.<br>Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE.<br>Note: PolyBase is a technology that accesses and combines both non-relational and relational data, all from within SQL Server. It allows you to run queries on external data in Hadoop or Azure blob storage.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-configure-azure-blob-storage",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-12T07:56:00.000Z",
        "voteCount": 5,
        "content": "The answer is correct \nStep to Configure an external table\n1. Create a master key on the database. The master key is required to encrypt the credential secret.\n2. Create a database scoped credential for Azure blob storage.\n3. Create an external data source with CREATE EXTERNAL DATA SOURCE..\n4. Create an external file format with CREATE EXTERNAL FILE FORMAT.\n5. Create an external table pointing to data stored in Azure storage with CREATE EXTERNAL TABLE\n6. Create statistics on an external table."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/37835-exam-dp-200-topic-2-question-36-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing a solution that will use Azure Stream Analytics. The solution will accept an Azure Blob storage file named Customers. The file will contain both in-store and online customer details. The online customers will provide a mailing address.<br>You have a file in Blob storage named LocationIncomes that contains median incomes based on location. The file rarely changes.<br>You need to use an address to look up a median income based on location. You must output the data to Azure SQL Database for immediate use and to Azure<br>Data Lake Storage Gen2 for long-term retention.<br>Solution: You implement a Stream Analytics job that has two streaming inputs, one query, and two outputs.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We need one reference data input for LocationIncomes, which rarely changes<br>Note: Stream Analytics also supports input known as reference data. Reference data is either completely static or changes slowly.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs#stream-and-reference-inputs",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-26T05:15:00.000Z",
        "voteCount": 4,
        "content": "You need a reference so answer is no"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14378-exam-dp-200-topic-2-question-37-discussion/",
    "body": "DRAG DROP -<br>You develop data engineering solutions for a company.<br>You need to deploy a Microsoft Azure Stream Analytics job for an IoT solution. The solution must:<br>\u2711 Minimize latency.<br>\u2711 Minimize bandwidth usage between the job and IoT device.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0021900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0022000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an Azure Blob Storage container<br>To prepare your Stream Analytics job to be deployed on an IoT Edge device, you need to associate the job with a container in a storage account. When you go to deploy your job, the job definition is exported to the storage container.<br>Step 2: Create an Azure Stream Analytics edge job and configure job definition save location<br>When you create an Azure Stream Analytics job to run on an IoT Edge device, it needs to be stored in a way that can be called from the device.<br>Step 3: Create and IoT hub and add the Azure Stream Analytics module to the IoT Hub namespace<br>An IoT Hub in Azure is required.<br>Stream Analytics accepts data incoming from several kinds of event sources including Event Hubs, IoT Hub, and Blob storage.<br><br>Step 4: Configure routes -<br>You are now ready to deploy the Azure Stream Analytics job on your IoT Edge device.<br>The routes that you declare define the flow of data through the IoT Edge device.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-add-inputs https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [],
    "comments": [
      {
        "date": "2020-02-17T20:55:00.000Z",
        "voteCount": 74,
        "content": "Wrong order:\n\nCorrect order can be found in the article \n\n1) storage\n2) Edge \n3) IOT \n4) route\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge"
      },
      {
        "date": "2021-01-29T23:30:00.000Z",
        "voteCount": 4,
        "content": "but the link doesnt give any mention about the order"
      },
      {
        "date": "2022-12-20T13:01:00.000Z",
        "voteCount": 1,
        "content": "what would in sequence mean to you ??"
      },
      {
        "date": "2021-05-07T01:11:00.000Z",
        "voteCount": 4,
        "content": "This is  wrong order, provided answer is correct. Refer the below link\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"
      },
      {
        "date": "2021-05-22T02:20:00.000Z",
        "voteCount": 2,
        "content": "SAMBIT's answer is correct. When you add the Stream Analytics module to the IoT hub you must select an existing Stream Analytics job. So you must create the Stream Analytics Edge job first"
      },
      {
        "date": "2021-05-24T02:45:00.000Z",
        "voteCount": 2,
        "content": "Correct ans"
      },
      {
        "date": "2020-09-08T22:52:00.000Z",
        "voteCount": 12,
        "content": "1. Create Storage\n2. Create ASA Job\n3. Create IOT Hug and Add ASA Job\n4. Configure Routes\nThis is correct order"
      },
      {
        "date": "2021-04-19T18:33:00.000Z",
        "voteCount": 2,
        "content": ": Create an IoT hub and add the Azure Stream Analytics module to the IoT Hub namespace\nAn IoT Hub in Azure is required. It\u2019s a Prerequisites. So the answer is correct"
      },
      {
        "date": "2021-03-24T02:25:00.000Z",
        "voteCount": 2,
        "content": "answer is in correct."
      },
      {
        "date": "2020-11-28T04:09:00.000Z",
        "voteCount": 5,
        "content": "From https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge:\nAzure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data. Azure Stream Analytics is designed for low latency, resiliency, efficient use of bandwidth, and compliance\n1) Create a storage container (create blob container)\n2) Create an ASA edge job (create edge job)\n3) Setup your IoT Edge environment on your device(s) (create IoT hub)\n4) Deploy ASA on your IoT Edge device(s) (configure routes)"
      },
      {
        "date": "2020-08-07T04:51:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct as per the article.. you need to create IOT hub first\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal.."
      },
      {
        "date": "2020-12-06T08:52:00.000Z",
        "voteCount": 1,
        "content": "yes. this is the answer, the link is perfect, it has the sequence, the only difference is that we use a Edge instead of Cloud Job, like the text says. Regards"
      },
      {
        "date": "2021-05-06T22:24:00.000Z",
        "voteCount": 2,
        "content": "Stated clearly in the documentation the procedure on the configuration. Cheers!"
      },
      {
        "date": "2021-05-10T23:44:00.000Z",
        "voteCount": 2,
        "content": "Reference: https://rangv.github.io/azureiotedgelab/streamanalytics/"
      },
      {
        "date": "2020-07-11T04:43:00.000Z",
        "voteCount": 1,
        "content": "why Azure blob storage only why not Azure Data Lake storage?"
      },
      {
        "date": "2020-09-27T00:34:00.000Z",
        "voteCount": 2,
        "content": "Data lake is not a accepted input in ASA only Blob, Iot Hib and Event Hub"
      },
      {
        "date": "2020-04-17T03:56:00.000Z",
        "voteCount": 1,
        "content": "Based on what I see, storage can be defined before or after the Edge, as long as it's defined before the input definition. However, it makes more sense to create Storage account before Hub, so it will be streamlines."
      },
      {
        "date": "2020-05-11T23:41:00.000Z",
        "voteCount": 2,
        "content": "i agree,\nCreate a storage account\nWhen you create an Azure Stream Analytics job to run on an IoT Edge device, it needs to be stored in a way that can be called from the device. You can use an existing Azure Storage account, or create a new one now.\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics"
      },
      {
        "date": "2020-03-06T04:26:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct refer below article\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"
      },
      {
        "date": "2020-03-25T00:26:00.000Z",
        "voteCount": 6,
        "content": "this article refers to stream analytics job hosting in the cloud (!). This question refers to the job on edge."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/70365-exam-dp-200-topic-2-question-38-discussion/",
    "body": "DRAG DROP -<br>You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a property formatted carriage return (/r) and line feed (/n).<br>You are implementing a pattern that batch loads the files daily into an enterprise data warehouse in Azure Synapse Analytics by using PolyBase.<br>You need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0022200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0022300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an external data source that uses the abfs location<br>Create External Data Source to reference Azure Data Lake Store Gen 1 or 2<br>Step 2: Create an external file format and set the First_Row option.<br>Create External File Format.<br>Step 3: Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages<br>To use PolyBase, you must create external tables to reference your external data.<br>Use reject options.<br>Note: REJECT options don't apply at the time this CREATE EXTERNAL TABLE AS SELECT statement is run. Instead, they're specified here so that the database can use them at a later time when it imports data from the external table. Later, when the CREATE TABLE AS SELECT statement selects data from the external table, the database will use the reject options to determine the number or percentage of rows that can fail to import before it stops the import.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-21T02:47:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39910-exam-dp-200-topic-2-question-39-discussion/",
    "body": "You are creating a new notebook in Azure Databricks that will support R as the primary language but will also support Scala and SQL.<br>Which switch should you use to switch between languages?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%&lt;language&gt;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\\\[&lt;language&gt;]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\\\(&lt;language&gt;)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t@&lt;Language&gt;"
    ],
    "answer": "A",
    "answerDescription": "You can override the primary language by specifying the language magic command %&lt;language&gt; at the beginning of a cell. The supported magic commands are:<br>%python, %r, %scala, and %sql.<br>References:<br>https://docs.databricks.com/user-guide/notebooks/notebook-use.html#mix-languages",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-15T06:23:00.000Z",
        "voteCount": 18,
        "content": "Correct Answer :)"
      },
      {
        "date": "2021-07-14T02:10:00.000Z",
        "voteCount": 1,
        "content": "not exactly. its %%"
      },
      {
        "date": "2021-09-19T04:11:00.000Z",
        "voteCount": 1,
        "content": "in Databricks you use just '%' e.g. %scala ... CODE"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49815-exam-dp-200-topic-2-question-40-discussion/",
    "body": "HOTSPOT -<br>You are implementing mapping data flows in Azure Data Factory to convert daily logs of taxi records into aggregated datasets.<br>You configure a data flow and receive the error shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03872/0022500001.jpg\" class=\"in-exam-image\"><br>You need to resolve the error.<br>Which setting should you configure? To answer, select the appropriate setting in the answer area.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0022500002.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0022600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "The Inspect tab provides a view into the metadata of the data stream that you're transforming. You can see column counts, the columns changed, the columns added, data types, the column order, and column references. Inspect is a read-only view of your metadata. You don't need to have debug mode enabled to see metadata in the Inspect pane.<br><img src=\"/assets/media/exam-media/03872/0022700001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-data-flow-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-10T08:57:00.000Z",
        "voteCount": 6,
        "content": "passengerCount column type"
      },
      {
        "date": "2021-04-13T11:45:00.000Z",
        "voteCount": 4,
        "content": "I agree...with inspect you can also look at data types, you can't edit it. So I would edit data type for passengerCount."
      },
      {
        "date": "2023-12-29T18:20:00.000Z",
        "voteCount": 1,
        "content": "passengerCount"
      },
      {
        "date": "2021-06-12T08:26:00.000Z",
        "voteCount": 2,
        "content": "Config PassengerCount column type to number"
      },
      {
        "date": "2021-05-23T02:35:00.000Z",
        "voteCount": 2,
        "content": "The anwer is incorrect as you cannot solve the error using read-only tab. The question was which section to configure and you cannot configure the read-only inspect. You need to configure schema in projection"
      },
      {
        "date": "2021-06-09T22:46:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/data-factory/data-flow-source"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52922-exam-dp-200-topic-2-question-41-discussion/",
    "body": "HOTSPOT -<br>You have an Azure SQL database named Database1 and two Azure event hubs named HubA and HubB. The data consumed from each source is shown in the following table.<br><img src=\"/assets/media/exam-media/03872/0022700002.png\" class=\"in-exam-image\"><br>You need to implement Azure Stream Analytics to calculate the average fare per mile by driver.<br>How should you configure the Stream Analytics input for each source? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0022800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0022900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "HubA: Stream -<br><br>HubB: Stream -<br><br>Database1: Reference -<br>Reference data (also known as a lookup table) is a finite data set that is static or slowly changing in nature, used to perform a lookup or to augment your data streams. For example, in an IoT scenario, you could store metadata about sensors (which don't change often) in reference data and join it with real time IoT data streams. Azure Stream Analytics loads reference data in memory to achieve low latency stream processing<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-16T22:45:00.000Z",
        "voteCount": 8,
        "content": "It is correct Answer"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51836-exam-dp-200-topic-2-question-42-discussion/",
    "body": "HOTSPOT -<br>You have a self-hosted integration runtime in Azure Data Factory.<br>The current status of the integration runtime has the following configurations:<br>\u2711 Status: Running<br>\u2711 Type: Self-Hosted<br>\u2711 Version: 4.4.7292.1<br>\u2711 Running / Registered Node(s): 1/1<br>\u2711 High Availability Enabled: False<br>\u2711 Linked Count: 0<br>\u2711 Queue Length: 0<br>\u2711 Average Queue Duration: 0.00s<br>The integration runtime has the following node details:<br>\u2711 Name: X-M<br>\u2711 Status: Running<br>\u2711 Version: 4.4.7292.1<br>\u2711 Available Memory: 7697MB<br>\u2711 CPU Utilization: 6%<br>\u2711 Network (In/Out): 1.21KBps/0.83KBps<br>\u2711 Concurrent Jobs (Running/Limit): 2/14<br>\u2711 Role: Dispatcher/Worker<br>\u2711 Credential Status: In Sync<br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0023100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0023100002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: fail until the node comes back online<br>We see: High Availability Enabled: False<br>Note: Higher availability of the self-hosted integration runtime so that it's no longer the single point of failure in your big data solution or cloud data integration with<br>Data Factory.<br><br>Box 2: lowered -<br>We see:<br>Concurrent Jobs (Running/Limit): 2/14<br>CPU Utilization: 6%<br>Note: When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-08T08:50:00.000Z",
        "voteCount": 12,
        "content": "If each job is similar and each uses 3% CPU - then even at current max 14,only 42% CPU utilization. Should INCREASE the limit - which is what the answer is also saying."
      },
      {
        "date": "2021-05-15T02:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with \"raise\". \nIf we lower the number of concurrent jobs, the CPU will become even more *under*utilized. Because then when it reaches the limit number of concurrent jobs, it will occupy even less CPU -&gt; CPU will never be optimally utilized.\nRAISE the number of concurrent jobs so that their CPU consumption comes closer to CPU's max.\n\nReference: https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime#scale-up"
      },
      {
        "date": "2021-08-24T23:53:00.000Z",
        "voteCount": 1,
        "content": "You are not increasing currently running jobs, but the limit of jobs that can run concurrently, which is 14 at the moment. So increasing it would only reduce cpu utilization."
      },
      {
        "date": "2021-06-15T05:55:00.000Z",
        "voteCount": 1,
        "content": "It should lower the concurrent Jobs limit."
      },
      {
        "date": "2021-06-14T01:38:00.000Z",
        "voteCount": 2,
        "content": "I agree with the answer. We should lower Concurrent Jobs (Running/Limit) value. It means we should lower the limit value. We don't need so big value if we use only 2 of 14."
      },
      {
        "date": "2021-05-04T13:36:00.000Z",
        "voteCount": 3,
        "content": "Shouldn't the second option be \"left as is\" since the number of concurrent jobs is 2/14? The answer states that if the number of concurrent jobs reaches the limit (14 in this case) and the CPU is under utilized then we should increase the number of concurrent jobs that can run. In this case it seems that 6% is a good amount of utilization given that 2/14 jobs (14%) are running, no?"
      },
      {
        "date": "2021-05-11T00:30:00.000Z",
        "voteCount": 2,
        "content": "\"When the processor and available RAM aren't well utilized, but the execution of concurrent jobs reaches a node's limits, scale up by increasing the number of concurrent jobs that a node can run. You might also want to scale up when activities time out because the self-hosted IR is overloaded. As shown in the following image, you can increase the maximum capacity for a node.\" \n\nThe above statement was quoted from the link provided below and on this scenario, it requires to increase the concurrent jobs (running/limit)\n\nReference: https://docs.microsoft.com/en-us/azure/data-factory/create-self-hosted-integration-runtime"
      },
      {
        "date": "2021-06-26T20:27:00.000Z",
        "voteCount": 1,
        "content": "I agree with Dangal, \"left as is\", because here you are not reaching a node's limits (it's just using 6%). It it was using the 14 jobs and it would be using 20% then I would say do raise the number of possible jobs, otherwise not. And also I would not lower the number of possible jobs because I don't know how many concurrent jobs I would need to run, this is just a \"snapshot\" of what was happening at that moment."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31840-exam-dp-200-topic-2-question-43-discussion/",
    "body": "You have an Azure Stream Analytics job that receives clickstream data from an Azure event hub.<br>You need to define a query in the Stream Analytics job. The query must meet the following requirements:<br>\u2711 Count the number of clicks within each 10-second window based on the country of a visitor.<br>\u2711 Ensure that each click is NOT counted more than once.<br>How should you define the query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, TumblingWindow(second, 10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Count(*) AS Count FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SessionWindow(second, 5, 10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, SlidingWindow(second, 10)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT Country, Avg(*) AS Average FROM ClickStream TIMESTAMP BY CreatedAt GROUP BY Country, HoppingWindow(second, 10, 2)"
    ],
    "answer": "A",
    "answerDescription": "Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.<br>Example:<br>Incorrect Answers:<br>B: Session windows group events that arrive at similar times, filtering out periods of time where there is no data.<br>C: Sliding windows, unlike Tumbling or Hopping windows, output events only for points in time when the content of the window actually changes. In other words, when an event enters or exits the window. Every window has at least one event, like in the case of Hopping windows, events can belong to more than one sliding window.<br>D: Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap, so events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-06T05:34:00.000Z",
        "voteCount": 6,
        "content": "answer is correct\n\nhttps://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48492-exam-dp-200-topic-2-question-44-discussion/",
    "body": "You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and to output the data to an Azure Blob storage account.<br>You need to output the count of tweets from the last five minutes every minute.<br>Which windowing function should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSliding",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTumbling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHopping"
    ],
    "answer": "D",
    "answerDescription": "Hopping window functions hop forward in time by a fixed period.<br>Incorrect Answers:<br>A: Sliding windows, unlike Tumbling or Hopping windows, output events only for points in time when the content of the window actually changes. In other words, when an event enters or exits the window.<br>B: Session window functions group events that arrive at similar times, filtering out periods of time where there is no data. A session window begins when the first event occurs. If another event occurs within the specified timeout from the last ingested event, then the window extends to include the new event. Otherwise if no events occur within the timeout, then the window is closed at the timeout.<br>C: Tumbling window functions are used to segment a data stream into distinct time segments. A Tumbling windows do not overlap, and an event cannot belong to more than one tumbling window.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-16T18:45:00.000Z",
        "voteCount": 12,
        "content": "Hopping Window is the correct answer"
      },
      {
        "date": "2021-11-27T22:01:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/media/stream-analytics-window-functions/stream-analytics-window-functions-hopping-intro.png"
      },
      {
        "date": "2021-06-14T01:15:00.000Z",
        "voteCount": 1,
        "content": "correct answer c  only"
      },
      {
        "date": "2021-07-08T01:27:00.000Z",
        "voteCount": 7,
        "content": "Don't be ridiculous, doesn't help other people either. It's a hopping window as the windows will overlap."
      },
      {
        "date": "2021-05-22T10:42:00.000Z",
        "voteCount": 4,
        "content": "Hopping"
      },
      {
        "date": "2021-05-14T00:12:00.000Z",
        "voteCount": 1,
        "content": "Sorry. Looks like D now."
      },
      {
        "date": "2021-05-14T00:08:00.000Z",
        "voteCount": 1,
        "content": "I think C. Tumbling"
      },
      {
        "date": "2021-05-14T00:07:00.000Z",
        "voteCount": 2,
        "content": "So what is the answer?"
      },
      {
        "date": "2021-04-25T11:53:00.000Z",
        "voteCount": 3,
        "content": "correct answer: D"
      },
      {
        "date": "2021-03-29T21:03:00.000Z",
        "voteCount": 3,
        "content": "Tumbling window functions are used to segment a data stream into distinct time segments and perform\na function against them, such as the example below. The key differentiators of a Tumbling window are\nthat they repeat, do not overlap, and an event cannot belong to more than one tumbling window."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/15847-exam-dp-200-topic-2-question-45-discussion/",
    "body": "You use Azure Stream Analytics to receive Twitter data from Azure Event Hubs and to output the data to an Azure Blob storage account.<br>You need to output the count of tweets during the last five minutes every five minutes. Each tweet must only be counted once.<br>Which windowing function should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Sliding window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Session window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Tumbling window",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta five-minute Hopping window that has a one-minute hop"
    ],
    "answer": "C",
    "answerDescription": "Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.<br><img src=\"/assets/media/exam-media/03872/0023500001.jpg\" class=\"in-exam-image\"><br>Incorrect Answers:<br>D: Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap, so events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-19T13:57:00.000Z",
        "voteCount": 20,
        "content": "I think answer is correct. TUMBLING WINDOW as event muse be counted once.\n\nTumbling window\nTumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window\n\nTumbling window\nTumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window"
      },
      {
        "date": "2021-06-28T23:03:00.000Z",
        "voteCount": 1,
        "content": "Actually, with the given options, answer is correct. Ans - C"
      },
      {
        "date": "2021-05-23T02:46:00.000Z",
        "voteCount": 3,
        "content": "Every tween must be counted only once - tumbling window, unlike hopping, guarantees the windows will not overlap"
      },
      {
        "date": "2021-04-11T01:09:00.000Z",
        "voteCount": 2,
        "content": "I think C is the correct answer. Definition of tumbling window: \"fixed size window with advance interval equals to the window size\". \nAnswer D is clearly wrong: \"a five-minute Hopping window that has a one-minute hop\" doesnt fit \"five minutes every five minutes\" requirement."
      },
      {
        "date": "2020-07-04T19:16:00.000Z",
        "voteCount": 2,
        "content": "A hoping window can act as tumbling window, if time unit and window size is same."
      },
      {
        "date": "2020-07-07T13:56:00.000Z",
        "voteCount": 4,
        "content": "D. a five-minute Hopping window that has one-minute hop \n\nOne minute hop Rahul, 1 min hop."
      },
      {
        "date": "2020-06-26T07:28:00.000Z",
        "voteCount": 1,
        "content": "It must be Hopping Window -To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size. So Answer is D"
      },
      {
        "date": "2020-06-28T09:25:00.000Z",
        "voteCount": 9,
        "content": "It would be if the hop is 5 minutes as well as the window, BUT the answer \"D\" says this: \n\"D. a five-minute Hopping window that has one-minute hop\"\nwhich means overlap will occur. \nSo, the answer is \"C\" and that's correct."
      },
      {
        "date": "2020-03-08T06:38:00.000Z",
        "voteCount": 1,
        "content": "I would say just Hopping Window."
      },
      {
        "date": "2020-03-11T09:33:00.000Z",
        "voteCount": 9,
        "content": "Hopping Window has overlapped and an item can be counted more than once"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26768-exam-dp-200-topic-2-question-46-discussion/",
    "body": "You are developing a solution that will stream to Azure Stream Analytics. The solution will have both streaming data and reference data.<br>Which input type should you use for the reference data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure IoT Hub"
    ],
    "answer": "C",
    "answerDescription": "Stream Analytics supports Azure Blob storage and Azure SQL Database as the storage layer for Reference Data.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-27T04:58:00.000Z",
        "voteCount": 12,
        "content": "For reference data Azure SQL database and Blob storage are supported."
      },
      {
        "date": "2020-11-25T05:36:00.000Z",
        "voteCount": 2,
        "content": "Link provided supports C as the answer"
      },
      {
        "date": "2020-11-20T12:21:00.000Z",
        "voteCount": 2,
        "content": "That is true, but it SQL DB is not in options"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53060-exam-dp-200-topic-2-question-47-discussion/",
    "body": "HOTSPOT -<br>You are implementing Azure Stream Analytics windowing functions.<br>Which windowing function should you use for each requirement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0023700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0023800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Tumbling -<br>Tumbling window functions are used to segment a data stream into distinct time segments and perform a function against them, such as the example below. The key differentiators of a Tumbling window are that they repeat, do not overlap, and an event cannot belong to more than one tumbling window.<br><img src=\"/assets/media/exam-media/03872/0023900001.png\" class=\"in-exam-image\"><br><br>Box 2: Hopping -<br>Hopping window functions hop forward in time by a fixed period. It may be easy to think of them as Tumbling windows that can overlap, so events can belong to more than one Hopping window result set. To make a Hopping window the same as a Tumbling window, specify the hop size to be the same as the window size.<br><img src=\"/assets/media/exam-media/03872/0024000001.jpg\" class=\"in-exam-image\"><br><br>Box 3: Sliding -<br>Sliding window functions, unlike Tumbling or Hopping windows, produce an output only when an event occurs. Every window will have at least one event and the window continuously moves forward by an \u05d2\u201a\u00ac (epsilon). Like hopping windows, events can belong to more than one sliding window.<br><img src=\"/assets/media/exam-media/03872/0024000002.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-window-functions",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-18T14:16:00.000Z",
        "voteCount": 6,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49229-exam-dp-200-topic-2-question-48-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Data Lake Storage Gen2 account that contains JSON files for customers. The files contain two attributes named FirstName and LastName.<br>You need to copy the data from the JSON files to an Azure Synapse Analytics table by using Azure Databricks. A new column must be created that concatenates the FirstName and LastName values.<br>You create the following components:<br>\u2711 A destination table in Azure Synapse<br>\u2711 An Azure Blob storage container<br>\u2711 A service principal<br>Which five actions should you perform in sequence next in a Databricks notebook? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0024100004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0024200001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Read the file into a data frame.<br>You can load the json files as a data frame in Azure Databricks.<br>Step 2: Perform transformations on the data frame.<br>Step 3:Specify a temporary folder to stage the data<br>Specify a temporary folder to use while moving data between Azure Databricks and Azure Synapse.<br>Step 4: Write the results to a table in Azure Synapse.<br>You upload the transformed data frame into Azure Synapse. You use the Azure Synapse connector for Azure Databricks to directly upload a dataframe as a table in a Azure Synapse.<br><br>Step 5: Drop the data frame -<br>Clean up resources. You can terminate the cluster. From the Azure Databricks workspace, select Clusters on the left. For the cluster to terminate, under Actions, point to the ellipsis (...) and select the Terminate icon.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-extract-load-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-02T23:47:00.000Z",
        "voteCount": 15,
        "content": "It requires to mount the ALDS gen 2 thus the sequence is right \"FHEAB\"."
      },
      {
        "date": "2021-05-21T07:50:00.000Z",
        "voteCount": 1,
        "content": "Can you explain what is \"FHEAB\"?"
      },
      {
        "date": "2021-05-23T02:56:00.000Z",
        "voteCount": 2,
        "content": "letter-numbers of steps to choose"
      },
      {
        "date": "2021-06-13T20:46:00.000Z",
        "voteCount": 1,
        "content": "Thanks!"
      },
      {
        "date": "2021-06-12T08:50:00.000Z",
        "voteCount": 10,
        "content": "Correct answer should be:\nStep 1: Mount the Data Lake Storage onto DBFS\nStep 2: Read the file into a data frame.\nStep 3: Perform transformations on the data frame.\nStep 4: Specify a temporary folder to stage the data\nStep 5: Write the results to a table in Azure Synapse."
      },
      {
        "date": "2021-08-07T02:20:00.000Z",
        "voteCount": 1,
        "content": "The Answer is Perfect. \nMounting is not Required.\nDrop Data Frame should be there.\n\nThe question never mentioned that you have to use Service Principle. Had it be 6 steps I would have added Mounting Steps. But Considering only 5 steps, the below 5 steps have more priority then Mounting (not an essential)."
      },
      {
        "date": "2021-09-26T23:34:00.000Z",
        "voteCount": 1,
        "content": "why drop dataframe ?cleanup the resource is about cluster not the DF ."
      },
      {
        "date": "2021-06-06T11:16:00.000Z",
        "voteCount": 4,
        "content": "Mount Data Lake Storage onto DBFS (Service Principal)\nRead the file into data frame\nPerform Transformation on the data frame\nSpecify the temp folder to stage data\nwrite results to synapse table\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse"
      },
      {
        "date": "2021-06-06T11:25:00.000Z",
        "voteCount": 1,
        "content": "small correction: I don't see the mount option in ADLS account configuration in the given URL. \nI feel the given answer might correct. The last one should be Drop the data frame which will do cleanup ..."
      },
      {
        "date": "2021-05-26T20:34:00.000Z",
        "voteCount": 1,
        "content": "Do we really need to stage the data? We could directly write the dataframe to Synapse.\nhttps://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/synapse-analytics"
      },
      {
        "date": "2021-05-27T09:57:00.000Z",
        "voteCount": 2,
        "content": "Yes, we do. tempDir (that stages data) MUST be specified for Synapse write method."
      },
      {
        "date": "2021-04-20T11:08:00.000Z",
        "voteCount": 5,
        "content": "I would go for FHEAB. Mount the storage -&gt; Read the file to a dataframe -&gt;transform it further -&gt; write the data to temporary folder in storage -&gt; and load to DWH"
      },
      {
        "date": "2021-05-14T02:35:00.000Z",
        "voteCount": 2,
        "content": "Agree.\n\nService Principal (which is given in the task) is used for mounting.\n\nMount an Azure Data Lake Gen 2 to DBFS (Databricks File System) using a Service Principal:\nhttps://kyleake.medium.com/mount-an-adls-gen-2-to-databricks-file-system-using-a-service-principal-and-oauth-2-0-ep-5-73172dd0ddeb"
      },
      {
        "date": "2021-04-11T05:40:00.000Z",
        "voteCount": 1,
        "content": "I agree with HEAB. But I don' know which is the missing one. I think there is no need to \"drop the DF\" or to \"mount the DL storage\"... :-( Does anybody know the right full answer?"
      },
      {
        "date": "2021-04-05T09:57:00.000Z",
        "voteCount": 2,
        "content": "wrong, should be F,H,E,A,B. The DataLake storage has to be mounted onto DBFS before red the file"
      },
      {
        "date": "2021-04-07T02:43:00.000Z",
        "voteCount": 2,
        "content": "Based on the provided link, I think the keyword here is \"mounted\". Datalake storage is not mounted onto DBFS, instead, it is called by Databricks via API. So the given answer is correct"
      },
      {
        "date": "2021-04-08T18:26:00.000Z",
        "voteCount": 2,
        "content": "After revising, I think FHEAB makes more sense"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52651-exam-dp-200-topic-2-question-49-discussion/",
    "body": "You have an Azure Storage account and a data warehouse in Azure Synapse Analytics in the UK South region.<br>You need to copy blob data from the storage account to the data warehouse by using Azure Data Factory.<br>The solution must meet the following requirements:<br>\u2711 Ensure that the data remains in the UK South region at all times.<br>\u2711 Minimize administrative effort.<br>Which type of integration runtime should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelf-hosted integration runtime",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure-SSIS integration runtime"
    ],
    "answer": "A",
    "answerDescription": "<img src=\"/assets/media/exam-media/03872/0024400001.png\" class=\"in-exam-image\"><br>Incorrect Answers:<br>B: Self-hosted integration runtime is to be used On-premises.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-13T16:59:00.000Z",
        "voteCount": 7,
        "content": "I would go for A. ADF allows to create Azure IR for a specific region &amp; \"UK South\" is listed as a region.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/create-azure-integration-runtime#create-azure-ir"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50524-exam-dp-200-topic-2-question-50-discussion/",
    "body": "You plan to perform batch processing in Azure Databricks once daily.<br>Which type of Databricks cluster should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tautomated",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tinteractive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHigh Concurrency"
    ],
    "answer": "A",
    "answerDescription": "Azure Databricks has two types of clusters: interactive and automated. You use interactive clusters to analyze data collaboratively with interactive notebooks. You use automated clusters to run fast and robust automated jobs.<br>Example: Scheduled batch workloads (data engineers running ETL jobs)<br>This scenario involves running batch job JARs and notebooks on a regular cadence through the Databricks platform.<br>The suggested best practice is to launch a new cluster for each run of critical jobs. This helps avoid any issues (failures, missing SLA, and so on) due to an existing workload (noisy neighbor) on a shared cluster.<br>Reference:<br>https://docs.databricks.com/administration-guide/cloud-configurations/aws/cmbp.html#scenario-3-scheduled-batch-workloads-data-engineers-running-etl-jobs",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-15T07:08:00.000Z",
        "voteCount": 1,
        "content": "The automated cluster type is called Job Cluster."
      },
      {
        "date": "2021-06-11T08:44:00.000Z",
        "voteCount": 1,
        "content": "Automated cluster is correct answer"
      },
      {
        "date": "2021-05-27T10:08:00.000Z",
        "voteCount": 2,
        "content": "There's no such thing as automated cluster. \"Databricks makes a distinction between all-purpose clusters and job clusters\". https://docs.databricks.com/clusters/index.html"
      },
      {
        "date": "2021-05-02T23:56:00.000Z",
        "voteCount": 3,
        "content": "The answer is \"automated\".\n\nReference: https://docs.databricks.com/clusters/index.html?_ga=2.1881073.1354805237.1620028392-1378612004.1617756501"
      },
      {
        "date": "2021-04-22T04:51:00.000Z",
        "voteCount": 1,
        "content": "It has discussed about type not mode what Nicks is referring to is mode. However question is about type"
      },
      {
        "date": "2021-04-19T23:10:00.000Z",
        "voteCount": 2,
        "content": "An automated Databricks cluster doesn't exists:\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-06-03T08:58:00.000Z",
        "voteCount": 3,
        "content": "So what is the answer?"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53086-exam-dp-200-topic-2-question-51-discussion/",
    "body": "You plan to build a structured streaming solution in Azure Databricks. The solution will count new events in five-minute intervals and report only events that arrive during the interval. The output will be sent to a Delta Lake table.<br>Which output mode should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcomplete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tupdate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tappend"
    ],
    "answer": "C",
    "answerDescription": "Append Mode: Only new rows appended in the result table since the last trigger are written to external storage. This is applicable only for the queries where existing rows in the Result Table are not expected to change.<br>Incorrect Answers:<br>A: Complete Mode: The entire updated result table is written to external storage. It is up to the storage connector to decide how to handle the writing of the entire table.<br>B: Update Mode: Only the rows that were updated in the result table since the last trigger are written to external storage. This is different from Complete Mode in that Update Mode outputs only the rows that have changed since the last trigger. If the query doesn't contain aggregations, it is equivalent to Append mode.<br>Reference:<br>https://docs.databricks.com/getting-started/spark/streaming.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-14T08:49:00.000Z",
        "voteCount": 1,
        "content": "\"The solution will count new events in five-minute intervals and report only events that arrive during the interval.\"\nit is COMPLETE, not append"
      },
      {
        "date": "2022-06-25T22:14:00.000Z",
        "voteCount": 1,
        "content": "It is a delta lake in destination so you could write the complete results to output as well but what would be the criteria of managing in delta lake is not clear so i would go with append as each five minutes interval count results would go in as a separate row in destination"
      },
      {
        "date": "2021-05-19T00:33:00.000Z",
        "voteCount": 4,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52992-exam-dp-200-topic-2-question-52-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics job to identify how much time a user spends interacting with a feature on a webpage.<br>The job receives events based on user actions on the webpage. Each row of data represents an event. Each event has a type of either 'start' or 'end'.<br>You need to calculate the duration between start and end events.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0024600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0024700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DATEDIFF -<br>DATEDIFF function returns the count (as a signed integer value) of the specified datepart boundaries crossed between the specified startdate and enddate.<br>Syntax: DATEDIFF ( datepart , startdate, enddate )<br><br>Box 2: LAST -<br>The LAST function can be used to retrieve the last event within a specific condition. In this example, the condition is an event of type Start, partitioning the search by PARTITION BY user and feature. This way, every user and feature is treated independently when searching for the Start event. LIMIT DURATION limits the search back in time to 1 hour between the End and Start events.<br>Example:<br><br>SELECT -<br>[user],<br>feature,<br>DATEDIFF(<br>second,<br>LAST(Time) OVER (PARTITION BY [user], feature LIMIT DURATION(hour, 1) WHEN Event = 'start'),<br><br>Time) as duration -<br><br>FROM input TIMESTAMP BY Time -<br><br>WHERE -<br>Event = 'end'<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-17T21:52:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49802-exam-dp-200-topic-2-question-53-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Stream Analytics job named ASA1.<br>The Diagnostic settings for ASA1 are configured to write errors to Log Analytics.<br>ASA1 reports an error, and the following message is sent to Log Analytics.<br><img src=\"/assets/media/exam-media/03872/0024900001.png\" class=\"in-exam-image\"><br>You need to write a Kusto query language query to identify all instances of the error and return the message field.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0024900002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0025000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DataErrorType -<br>The DataErrorType is InputDeserializerError.InvalidData.<br><br>Box 2: Message -<br>Retrieve the message.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/data-errors",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-10T07:06:00.000Z",
        "voteCount": 26,
        "content": "Should be parse_json(properties_s).DataErrorType and parse_json(properties_s).Message\nExample: https://github.com/MicrosoftDocs/LogAnalyticsExamples/blob/master/ResourceTypes/StreamAnalytics/List-all-input-deserialization-errors.txt"
      },
      {
        "date": "2021-05-03T00:06:00.000Z",
        "voteCount": 2,
        "content": "as quoted, this is the appropriate answer"
      },
      {
        "date": "2021-07-04T13:31:00.000Z",
        "voteCount": 2,
        "content": "Oh thank Christ. When I revealed their answer I was about to explode"
      },
      {
        "date": "2021-08-10T11:14:00.000Z",
        "voteCount": 1,
        "content": "with documents you provide, it shows first need .DataErrorType but after project TimeGenerated, we don't need to .Message. So, thank you for the good link, for second dropdown I will go for just Message."
      },
      {
        "date": "2021-06-06T11:51:00.000Z",
        "voteCount": 1,
        "content": "Given Answer is correct: \nNo need to user parse_json function. \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/media/stream-analytics-job-diagnostic-logs/logs-example.png"
      },
      {
        "date": "2021-06-24T23:54:00.000Z",
        "voteCount": 3,
        "content": "... the link you provided shows a parse_json function"
      },
      {
        "date": "2021-04-26T11:28:00.000Z",
        "voteCount": 3,
        "content": "You should use the parse_json function\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-job-diagnostic-logs"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50007-exam-dp-200-topic-2-question-54-discussion/",
    "body": "HOTSPOT -<br>You are processing streaming data from vehicles that pass through a toll booth.<br>You need to use Azure Stream Analytics to return the license plate, vehicle make, and hour the last vehicle passed during each 10-minute window.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0025100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0025200001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: MAX -<br>The first step on the query finds the maximum time stamp in 10-minute windows, that is the time stamp of the last event for that window. The second step joins the results of the first query with the original stream to find the event that match the last time stamps in each window.<br>Query:<br><br>WITH LastInWindow AS -<br>(<br><br>SELECT -<br><br>MAX(Time) AS LastEventTime -<br><br>FROM -<br><br>Input TIMESTAMP BY Time -<br><br>GROUP BY -<br>TumblingWindow(minute, 10)<br>)<br><br>SELECT -<br>Input.License_plate,<br>Input.Make,<br><br>Input.Time -<br><br>FROM -<br><br>Input TIMESTAMP BY Time -<br><br>INNER JOIN LastInWindow -<br>ON DATEDIFF(minute, Input, LastInWindow) BETWEEN 0 AND 10<br>AND Input.Time = LastInWindow.LastEventTime<br><br>Box 2: TumblingWindow -<br>Tumbling windows are a series of fixed-sized, non-overlapping and contiguous time intervals.<br><br>Box 3: DATEDIFF -<br>DATEDIFF is a date-specific function that compares and returns the time difference between two DateTime fields, for more information, refer to date functions.<br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/tumbling-window-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-13T04:23:00.000Z",
        "voteCount": 5,
        "content": "imo. the answer makes sense."
      },
      {
        "date": "2021-05-28T18:05:00.000Z",
        "voteCount": 1,
        "content": "to me it should be sliding windows, because if it's tumbling it will return only the last vehicle infor, using sliding windows will return every vehicle's infor passing by the booth, not just the last one"
      },
      {
        "date": "2021-05-28T18:09:00.000Z",
        "voteCount": 2,
        "content": "the question is a little ambiguous, if it needs to return every event in a window of 10m, it is Sliding windows, but if it only needs the last event in a window of 10m, so it is Tumbling windows"
      },
      {
        "date": "2021-04-18T01:09:00.000Z",
        "voteCount": 1,
        "content": "Why not COUNT ?"
      },
      {
        "date": "2021-04-25T12:16:00.000Z",
        "voteCount": 4,
        "content": "because you don't care about the number of vehicles, you need to find max timestamp to properly calculate 10 minutes diff."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55316-exam-dp-200-topic-2-question-55-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Stream Analytics job that is a Stream Analytics project solution in Microsoft Visual Studio. The job accepts data generated by IoT devices in the JSON format.<br>You need to modify the job to accept data generated by the IoT devices in the Protobuf format.<br>Which three actions should you perform from Visual Studio in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03872/0025400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0025500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Add an Azure Stream Analytics Custom Deserializer Project (.NET) project to the solution.<br><br>Create a custom deserializer -<br>1. Open Visual Studio and select File &gt; New &gt; Project. Search for Stream Analytics and select Azure Stream Analytics Custom Deserializer Project (.NET). Give the project a name, like Protobuf Deserializer.<br><img src=\"/assets/media/exam-media/03872/0025600001.jpg\" class=\"in-exam-image\"><br>2. In Solution Explorer, right-click your Protobuf Deserializer project and select Manage NuGet Packages from the menu. Then install the<br>Microsoft.Azure.StreamAnalytics and Google.Protobuf NuGet packages.<br>3. Add the MessageBodyProto class and the MessageBodyDeserializer class to your project.<br>4. Build the Protobuf Deserializer project.<br>Step 2: Add .NET deserializer code for Protobuf to the custom deserializer project<br>Azure Stream Analytics has built-in support for three data formats: JSON, CSV, and Avro. With custom .NET deserializers, you can read data from other formats such as Protocol Buffer, Bond and other user defined formats for both cloud and edge jobs.<br>Step 3: Add an Azure Stream Analytics Application project to the solution<br>Add an Azure Stream Analytics project<br>1. In Solution Explorer, right-click the Protobuf Deserializer solution and select Add &gt; New Project. Under Azure Stream Analytics &gt; Stream Analytics, choose<br>Azure Stream Analytics Application. Name it ProtobufCloudDeserializer and select OK.<br>2. Right-click References under the ProtobufCloudDeserializer Azure Stream Analytics project. Under Projects, add Protobuf Deserializer. It should be automatically populated for you.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/custom-deserializer",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-22T21:46:00.000Z",
        "voteCount": 6,
        "content": "I think for the last step it should be to \"change event serialization and reference the dll\" since the stream analytics project is assumed to already exist."
      },
      {
        "date": "2021-06-30T07:38:00.000Z",
        "voteCount": 1,
        "content": "I agree, it seems more logical"
      },
      {
        "date": "2021-06-14T07:08:00.000Z",
        "voteCount": 2,
        "content": "Hmmmm answer seems right"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51738-exam-dp-200-topic-2-question-57-discussion/",
    "body": "HOTSPOT -<br>You need to implement an Azure Databricks cluster that automatically connects to Azure Data Lake Storage Gen2 by using Azure Active Directory (Azure AD) integration.<br>How should you configure the new cluster? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0025800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0025800002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Premium -<br>Credential passthrough requires an Azure Databricks Premium Plan.<br>Incorrect Answers:<br>Support for Azure Data Lake Storage credential passthrough on standard clusters is in Public Preview.<br>Standard clusters with credential passthrough are supported on Databricks Runtime 5.5 and above and are limited to a single user.<br>Node: Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node.<br>Box 2: Azure Data Lake Storage Gen1 Credential Passthrough<br>You can authenticate automatically to Azure Data Lake Storage Gen1 and Azure Data Lake Storage Gen2 from Azure Databricks clusters using the same Azure<br>Active Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable your cluster for Azure Data Lake Storage credential passthrough, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage.<br>Reference:<br>https://docs.azuredatabricks.net/spark/latest/data-sources/azure/adls-passthrough.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-03T14:31:00.000Z",
        "voteCount": 5,
        "content": "this will be standard cluster not premium. For credential passthrough you only need a azure databricks premium plan but not premium cluster."
      },
      {
        "date": "2021-05-11T01:33:00.000Z",
        "voteCount": 5,
        "content": "i agree with the statement, cluster mode options are standard, high concurrency and single node. Thus, it make sense the choose \"STANDARD\" in the first box. Azure Databricks premium plan is needed to enable the passthrough feature.\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2021-08-13T12:53:00.000Z",
        "voteCount": 1,
        "content": "Credential passthrough requires an Azure Databricks Premium Plan. See Upgrade or Downgrade an Azure Databricks Workspace for details on upgrading a standard plan to a premium plan\nhttps://docs.microsoft.com/en-gb/azure/databricks/security/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2021-06-06T12:16:00.000Z",
        "voteCount": 1,
        "content": "Given explanation is correct: But as mentioned Azure Databricks supports three cluster modes: Standard, High Concurrency, and Single Node.\n\nThere is no premium cluster mode, thus we need to go with standard"
      },
      {
        "date": "2021-05-21T10:21:00.000Z",
        "voteCount": 3,
        "content": "From DataBricks Version 7.3, passthrough is supported on Standard clusters."
      },
      {
        "date": "2021-05-21T10:21:00.000Z",
        "voteCount": 1,
        "content": "From DataBricks Version 7.3, passthrough is supported on Standard clusters."
      },
      {
        "date": "2021-05-19T00:48:00.000Z",
        "voteCount": 1,
        "content": "1-standard\n2-data lake"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55069-exam-dp-200-topic-2-question-58-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics query that will receive input data from Azure IoT Hub and write the results to Azure Blob storage.<br>You need to calculate the difference in readings per sensor per hour.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0026000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0026000002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: LAG -<br>The LAG analytic operator allows one to look up a \u05d2\u20acprevious\u05d2\u20ac event in an event stream, within certain constraints. It is very useful for computing the rate of growth of a variable, detecting when a variable crosses a threshold, or when a condition starts or stops being true.<br><br>Box 2: LIMIT DURATION -<br>Example: Compute the rate of growth, per sensor:<br>SELECT sensorId,<br>growth = reading -<br>LAG(reading) OVER (PARTITION BY sensorId LIMIT DURATION(hour, 1))<br><br>FROM input -<br>Reference:<br>https://docs.microsoft.com/en-us/stream-analytics-query/lag-azure-stream-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-10T11:01:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50408-exam-dp-200-topic-2-question-59-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Use Azure Data Factory to convert the parquet files to CSV files<br>2. Create an external data source pointing to the Azure storage account<br>3. Create an external file format and external table using the external data source<br>4. Load the data using the INSERT`\u00a6SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "There is no need to convert the parquet files to CSV files.<br>You load the data using the CREATE TABLE AS SELECT statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-10T11:52:00.000Z",
        "voteCount": 2,
        "content": "This solution meet the goal, but it is not an optimized solution.\nThe answer should be Yes"
      },
      {
        "date": "2021-05-05T03:10:00.000Z",
        "voteCount": 1,
        "content": "1) Create the target table to load data from Azure Data Lake Storage.\n2) Use the COPY statement to load data into the data warehouse.\n\nt-sql COPY statement can directly read Parquet\n\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/copy-into-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true"
      },
      {
        "date": "2021-04-29T23:18:00.000Z",
        "voteCount": 2,
        "content": "No, instead of using data factory, use external data source and it requires creation of master key and database scope credential."
      },
      {
        "date": "2021-04-18T08:53:00.000Z",
        "voteCount": 2,
        "content": "The question says does this meet the goal?\nThe solution provided might not be the optimized solution, but does this meet the goal?\nPlease answer."
      },
      {
        "date": "2021-06-23T05:46:00.000Z",
        "voteCount": 2,
        "content": "No, the target table doesn't exist so insert statement won't work."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52892-exam-dp-200-topic-2-question-60-discussion/",
    "body": "Note: This question is a part of series of questions that present the same scenario. Each question in the series contains a unique solution. Determine whether the solution meets the stated goals.<br>You develop a data ingestion process that will import data to an enterprise data warehouse in Azure Synapse Analytics. The data to be ingested resides in parquet files stored in an Azure Data Lake Gen 2 storage account.<br>You need to load the data from the Azure Data Lake Gen 2 storage account into the Data Warehouse.<br>Solution:<br>1. Create an external data source pointing to the Azure storage account<br>2. Create an external file format and external table using the external data source<br>3. Load the data using the INSERT`\u00a6SELECT statement<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "You load the data using the CREATE TABLE AS SELECT statement.<br>References:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-18T14:13:00.000Z",
        "voteCount": 1,
        "content": "Doen't it meet the goal"
      },
      {
        "date": "2021-05-16T09:48:00.000Z",
        "voteCount": 3,
        "content": "answer is CORRECT"
      },
      {
        "date": "2021-06-21T18:49:00.000Z",
        "voteCount": 2,
        "content": "I think it's not correct unless the table (not the external table) was created already, which means you need a CREATE TABLE ... AS SELECT.. statement. Since it doesn't say the table exists you need to create it."
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56382-exam-dp-200-topic-2-question-61-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily.<br>The reference data input details for the file are shown in the Input exhibit.<br><img src=\"/assets/media/exam-media/03872/0026300001.png\" class=\"in-exam-image\"><br>The storage account container view is shown in the Refdata exhibit.<br><img src=\"/assets/media/exam-media/03872/0026400001.png\" class=\"in-exam-image\"><br>You need to configure the Stream Analytics job to pick up the new reference data.<br>What should you configure? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03872/0026500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03872/0026600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: {date}/product.csv -<br>In the 2nd exhibit we see: Location: refdata / 2020-03-20<br>Note: Path Pattern: This is a required property that is used to locate your blobs within the specified container. Within the path, you may choose to specify one or more instances of the following 2 variables:<br>{date}, {time}<br>Example 1: products/{date}/{time}/product-list.csv<br>Example 2: products/{date}/product-list.csv<br><br>Example 3: product-list.csv -<br><br>Box 2: YYYY-MM-DD -<br>Note: Date Format [optional]: If you have used {date} within the Path Pattern that you specified, then you can select the date format in which your blobs are organized from the drop-down of supported formats.<br>Example: YYYY/MM/DD, MM/DD/YYYY, etc.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-30T08:14:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-200",
    "topicNumber": "2"
  }
]