[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133398-exam-dp-600-topic-1-question-1-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br>Existing Environment -<br><br>Identity Environment -<br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br>Data Environment -<br>Contoso has the following data environment:<br>The Sales division uses a Microsoft Power BI Premium capacity.<br>The semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>The Research department uses an on-premises, third-party data warehousing product.<br>Fabric is enabled for contoso.com.<br>An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>A Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br>Requirements -<br><br>Planned Changes -<br>Contoso plans to make the following changes:<br>Enable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>Make all the data for the Sales division and the Research division available in Fabric.<br>For the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>In Productline1ws, create a lakehouse named Lakehouse1.<br>In Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br>Data Analytics Requirements -<br>Contoso identifies the following data analytics requirements:<br>All the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>The Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>All the semantic models and reports for the Research division must use version control that supports branching.<br><br>Data Preparation Requirements -<br>Contoso identifies the following data preparation requirements:<br>The Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br>Semantic Model Requirements -<br>Contoso identifies the following requirements for implementing and managing semantic models:<br>The number of rows added to the Orders table during refreshes must be minimized.<br>The semantic models in the Research division workspaces must use Direct Lake mode.<br><br>General Requirements -<br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br>Follow the principle of least privilege when applicable.<br>Minimize implementation and maintenance effort when possible.<br>You need to ensure that Contoso can use version control to meet the data analytics requirements and the general requirements.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore at the semantic models and reports in Data Lake Gen2 storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the settings of the Research workspaces to use a GitHub repository.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the settings of the Research division workspaces to use an Azure Repos repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all the semantic models and reports in Microsoft OneDrive."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-19T14:38:00.000Z",
        "voteCount": 14,
        "content": "\"Currently, only Git in Azure Repos is supported.\" https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration#considerations-and-limitations"
      },
      {
        "date": "2024-09-21T21:12:00.000Z",
        "voteCount": 8,
        "content": "IMHO, the Azure Repos is the answer.\n\nIt is directly mentioned here:\nhttps://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration\nConsiderations and limitations\nCurrently, only Git in Azure Repos with the same tenant as the Fabric tenant is supported.\nIf the workspace and Git repo are in two different geographical regions, the tenant admin must enable cross-geo exports.\nAzure DevOps on-prem isn't supported.\nSovereign clouds aren't supported."
      },
      {
        "date": "2024-07-30T03:08:00.000Z",
        "voteCount": 2,
        "content": "B. HitHub integration is now supported. (Preview)\nhttps://blog.fabric.microsoft.com/en-US/blog/announcing-github-integration-for-source-control-preview/"
      },
      {
        "date": "2024-07-30T03:01:00.000Z",
        "voteCount": 1,
        "content": "Now GitHub is supported for git integeration should we go with option B or C?\nhttps://learn.microsoft.com/en-us/fabric/cicd/git-integration/git-get-started?tabs=azure-devops%2CAzure%2Ccommit-to-git"
      },
      {
        "date": "2024-07-13T06:11:00.000Z",
        "voteCount": 1,
        "content": "Microsoft copilot recommend option C:"
      },
      {
        "date": "2024-06-20T14:53:00.000Z",
        "voteCount": 1,
        "content": "I guess C is correct"
      },
      {
        "date": "2024-05-31T05:39:00.000Z",
        "voteCount": 2,
        "content": "C - \"Currently, only Git in Azure Repos with the same tenant as the Fabric tenant is supported.\"\n\nhttps://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration#considerations-and-limitations"
      },
      {
        "date": "2024-05-27T14:11:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration\nOnly Git in Azure Repos is supported at the moment."
      },
      {
        "date": "2024-05-15T07:06:00.000Z",
        "voteCount": 2,
        "content": "Git in Azure Repos"
      },
      {
        "date": "2024-05-01T23:38:00.000Z",
        "voteCount": 1,
        "content": "Git in Azure Repos NOT Github Repos"
      },
      {
        "date": "2024-04-29T00:02:00.000Z",
        "voteCount": 1,
        "content": "I think B"
      },
      {
        "date": "2024-04-20T04:36:00.000Z",
        "voteCount": 1,
        "content": "Azure repos"
      },
      {
        "date": "2024-04-19T07:05:00.000Z",
        "voteCount": 1,
        "content": "C - makes the most sens. Github might pose problems."
      },
      {
        "date": "2024-04-18T04:31:00.000Z",
        "voteCount": 3,
        "content": "According to the https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration documentation, the correct option is C, due to the following sentence:\n Currently, only Git in Azure Repos with the same tenant as the Fabric tenant is supported."
      },
      {
        "date": "2024-04-15T01:36:00.000Z",
        "voteCount": 1,
        "content": "Most people are voting C but the \"correct answer\" is B? Who can make sure the answers provided are correct here?"
      },
      {
        "date": "2024-04-19T07:04:00.000Z",
        "voteCount": 5,
        "content": "dont trust the revealed \"correct answer\", always check the discussion. \nI guess the \"correct answer\" is from the person who took the exam or whatever and uploaded the question and answer sheets. Of course no one except Microsoft knows the real correct answers to these questions."
      },
      {
        "date": "2024-04-08T07:14:00.000Z",
        "voteCount": 4,
        "content": "Option C\nA. Storing semantic models and reports in Data Lake Gen2 storage might provide a storage solution, but it does not address the requirement for version control.\nB. While GitHub is a popular version control platform and supports branching, it may not integrate seamlessly with Contoso's Azure-based environment.\nD. Microsoft OneDrive is primarily a file hosting service and is not well-suited for version control of complex datasets, semantic models, and reports."
      },
      {
        "date": "2024-04-04T21:20:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133443-exam-dp-600-topic-1-question-2-discussion/",
    "body": "HOTSPOT -<br><br>Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br>Existing Environment -<br><br>Identity Environment -<br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br>Data Environment -<br>Contoso has the following data environment:<br>The Sales division uses a Microsoft Power BI Premium capacity.<br>The semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>The Research department uses an on-premises, third-party data warehousing product.<br>Fabric is enabled for contoso.com.<br>An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>A Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br>Requirements -<br><br>Planned Changes -<br>Contoso plans to make the following changes:<br>Enable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>Make all the data for the Sales division and the Research division available in Fabric.<br>For the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>In Productline1ws, create a lakehouse named Lakehouse1.<br>In Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br>Data Analytics Requirements -<br>Contoso identifies the following data analytics requirements:<br>All the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>The Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>All the semantic models and reports for the Research division must use version control that supports branching.<br><br>Data Preparation Requirements -<br>Contoso identifies the following data preparation requirements:<br>The Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br>Semantic Model Requirements -<br>Contoso identifies the following requirements for implementing and managing semantic models:<br>The number of rows added to the Orders table during refreshes must be minimized.<br>The semantic models in the Research division workspaces must use Direct Lake mode.<br><br>General Requirements -<br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br>Follow the principle of least privilege when applicable.<br>Minimize implementation and maintenance effort when possible.<br>You need to recommend a solution to group the Research division workspaces.<br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image1\" src=\"https://img.examtopics.com/dp-600/image1.png\">",
    "options": [],
    "answer": "<img title=\"image2\" src=\"https://img.examtopics.com/dp-600/image2.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-17T14:24:00.000Z",
        "voteCount": 47,
        "content": "Domain\\Fabric Admin Portal"
      },
      {
        "date": "2024-09-19T14:39:00.000Z",
        "voteCount": 13,
        "content": "https://learn.microsoft.com/en-us/fabric/governance/domains#create-a-domain\nFrom MS Learn\n1. \"In Fabric, a domain is a way to logically group together services in the organization\"\n2. You have to use Fabric Admin portal to do the groupings."
      },
      {
        "date": "2024-09-12T16:04:00.000Z",
        "voteCount": 1,
        "content": "its a capacity and fabric\n:  Domain Purpose: Domains are typically used for organizing and managing resources in a broader context and for access control rather than directly impacting performance and resource allocation.\nNot Ideal for Resource Management: While domains can help with organizational structure, they do not offer the performance benefits and cost management that a dedicated capacity does."
      },
      {
        "date": "2024-06-19T18:42:00.000Z",
        "voteCount": 1,
        "content": "ans: Domain and Fabric Admin portal ( to create the domain.)\nOnelake hub is to view those domains and underlying artifacts."
      },
      {
        "date": "2024-06-26T02:13:00.000Z",
        "voteCount": 2,
        "content": "grouping is done via Data Hub not Fabric Admin Tool"
      },
      {
        "date": "2024-06-11T06:41:00.000Z",
        "voteCount": 2,
        "content": "The right answer is Domain/Fabric Admin Portal.\nYou can create group with Domain, and Domain is managed at the Fabric Admin Portal level."
      },
      {
        "date": "2024-05-08T11:03:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/fabric/governance/domains#create-a-domain\n1- Domain is the way to logically group your resources.\n2- Domain is located in the Admin portal."
      },
      {
        "date": "2024-05-07T09:45:00.000Z",
        "voteCount": 3,
        "content": "IMHO, \n** Domain / One Lake Data Hub **\n\nLink: https://learn.microsoft.com/en-us/fabric/governance/domains#create-a-domain\n1. Because of this: In Fabric, a domain is a way of logically grouping together all the data in an organization that is relevant to a particular area or field.\n\n2. Because of this:\nFor instance, in the OneLake data hub, users can filter content by domain in order find content that is relevant to them. In addition, some tenant-level settings for managing and governing data can be delegated to the domain level, thus allowing domain-specific configuration of those settings."
      },
      {
        "date": "2024-05-15T11:02:00.000Z",
        "voteCount": 2,
        "content": "2nd is admin portal. You can explore/navigate domains and contents from the data hub."
      },
      {
        "date": "2024-05-07T09:50:00.000Z",
        "voteCount": 2,
        "content": "But, if the question is how to do this, than answer may be Fabric Admin Portal, because of that: To assign workspaces to a domain or subdomain in the admin portal, you must be a Fabric admin or a domain admin.\n\nSo, there is an ambiguity in a question"
      },
      {
        "date": "2024-04-29T03:06:00.000Z",
        "voteCount": 1,
        "content": "Grouping Method: Domain. This method allows for logical grouping based on department names, which supports OneLake data hub filtering as required.\nTool:  The Fabric Admin Portal provides the necessary tools for managing and organizing workspaces."
      },
      {
        "date": "2024-02-25T21:44:00.000Z",
        "voteCount": 1,
        "content": "Tenant and MS Entra ID\nThe grouping method should likely be \"Tenant,\" as this would allow for the organization of workspaces within the same Power BI service tenant, adhering to the principle of least privilege by keeping all resources under the same administrative boundary and simplifying management.\nThe tool to manage this could be \"The Microsoft Entra admin center,\" formerly known as the Azure Active Directory admin center, which is used for managing various aspects of Microsoft services, including user and group management within a tenant. This would align with minimizing implementation and maintenance effort, as it is a centralized management tool."
      },
      {
        "date": "2024-02-18T00:08:00.000Z",
        "voteCount": 3,
        "content": "- Domain\n- The Fabric Admin Portal\n\nhttps://learn.microsoft.com/en-us/fabric/governance/domains#configure-domain-settings"
      },
      {
        "date": "2024-02-16T01:20:00.000Z",
        "voteCount": 4,
        "content": "Group method: Domain\nTool: The Fabric Admin Portal\nThe tool here should mean the tool to implement the grouping solution. Thus, the answer should be the Fabric admin portal instead.\nhttps://learn.microsoft.com/en-us/fabric/governance/domains#create-a-domain"
      },
      {
        "date": "2024-02-16T12:21:00.000Z",
        "voteCount": 2,
        "content": "Thank you for the link.  I stand corrected."
      },
      {
        "date": "2024-02-15T17:47:00.000Z",
        "voteCount": 2,
        "content": "I think theseon is right\nOne Lake Data Hub is used to filter data based on domain, but It doesn't create the domain or group data\nWhen you want to make a domain, you should use Admin portal"
      },
      {
        "date": "2024-02-14T08:27:00.000Z",
        "voteCount": 1,
        "content": "One Lake Data Hub is the right answer according to the documentation link that you have provided."
      },
      {
        "date": "2024-02-14T07:04:00.000Z",
        "voteCount": 1,
        "content": "Domain: Group the Research division workspaces based on their departmental context.\nOne Lake Data Hub: Use One Lake Data Hub to filter and organize the Research division workspaces effectively.\nExplanation:\n\nDomain allows you to group workspaces based on their purpose or business context. In this case, grouping by department (Research division) aligns with the requirement.\nOne Lake Data Hub provides the necessary filtering capabilities for organizing workspaces based on department names."
      },
      {
        "date": "2024-02-09T05:41:00.000Z",
        "voteCount": 4,
        "content": "Should the tool not be Fabric Admin Portal?\n\nhttps://learn.microsoft.com/en-us/fabric/governance/domains#configure-domain-settings"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133444-exam-dp-600-topic-1-question-3-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br>Existing Environment -<br><br>Identity Environment -<br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br>Data Environment -<br>Contoso has the following data environment:<br>The Sales division uses a Microsoft Power BI Premium capacity.<br>The semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>The Research department uses an on-premises, third-party data warehousing product.<br>Fabric is enabled for contoso.com.<br>An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>A Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br>Requirements -<br><br>Planned Changes -<br>Contoso plans to make the following changes:<br>Enable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>Make all the data for the Sales division and the Research division available in Fabric.<br>For the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>In Productline1ws, create a lakehouse named Lakehouse1.<br>In Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br>Data Analytics Requirements -<br>Contoso identifies the following data analytics requirements:<br>All the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>The Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>All the semantic models and reports for the Research division must use version control that supports branching.<br><br>Data Preparation Requirements -<br>Contoso identifies the following data preparation requirements:<br>The Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br>Semantic Model Requirements -<br>Contoso identifies the following requirements for implementing and managing semantic models:<br>The number of rows added to the Orders table during refreshes must be minimized.<br>The semantic models in the Research division workspaces must use Direct Lake mode.<br><br>General Requirements -<br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br>Follow the principle of least privilege when applicable.<br>Minimize implementation and maintenance effort when possible.<br>You need to refresh the Orders table of the Online Sales department. The solution must meet the semantic model requirements.<br>What should you include in the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the minimum value of the OrderID column in the destination lakehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Factory pipeline that executes a dataflow to retrieve the minimum value of the OrderID column in the destination lakehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-09T05:46:00.000Z",
        "voteCount": 21,
        "content": "we need to retrieve the maximum OrderID in the destination table to minimize the number of rows added during refresh. this would be an incremental load. can be done with data flows"
      },
      {
        "date": "2024-03-31T15:15:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2"
      },
      {
        "date": "2024-02-25T21:57:00.000Z",
        "voteCount": 3,
        "content": "Totally agree on the max value to be retrieved on incremental load"
      },
      {
        "date": "2024-05-31T06:00:00.000Z",
        "voteCount": 9,
        "content": "D - As other people pointed out, the exact same use case for retrieving the max OrderID is showcased in the documentation \n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2#add-a-query-to-the-dataflow-to-filter-the-data-based-on-the-data-destination\n\nThought at first that A would be correct because SP support least privilege and because how real incremental refresh is not yet supported in data flow gen 2 https://ideas.fabric.microsoft.com/ideas/idea/?ideaid=4814b098-efff-ed11-a81c-6045bdb98602"
      },
      {
        "date": "2024-09-23T17:15:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2\n-&gt; \"You now have a query that returns the maximum OrderID in the lakehouse. This query is used to filter the data from the OData source. The next section adds a query to the dataflow to filter the data from the OData source based on the maximum OrderID in the lakehouse.\"\nDon't ask why\nThe problem is Fabric, so find the answer in the document\nIs this the first time you've seen a test in your life?"
      },
      {
        "date": "2024-09-21T21:13:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Factory \"pipelines\" is different from Azure Data Factory \"Data Flows\".\nAll the options are directing us to use Azure Data Factory \"pipelines\", so it should be using a stored procedure."
      },
      {
        "date": "2024-09-21T21:13:00.000Z",
        "voteCount": 1,
        "content": "we need to retrieve the maximum OrderID in the destination table to minimize the number of rows added during refresh. This can be achieved with both the dataflow and a stored procedure.\n\nIt mentions that \"All the semantic models and reports for the Research division must use version control that supports branching.\"\n\nDataflows are not supported in the git integration. Hence I choose A as the answer."
      },
      {
        "date": "2024-04-19T07:21:00.000Z",
        "voteCount": 1,
        "content": "only semantic model and reports must use version control. However, dataflows are not mentioned, therefore, irrelevant whether supported or not."
      },
      {
        "date": "2024-08-07T08:44:00.000Z",
        "voteCount": 2,
        "content": "I was initially leaning to A but got real confused when I read the choices again. Using FABRIC data factory (one would presume that what they would mean in a FABRIC exam), when you use a Stored Procedure activity, you only see Warehouses and other SQL sources and NOT Lakehouses. Using Azure Data Factory, one could add an Azure SQL DB linked service and connect to the SQL Endpoint of a Lakehouse and execute a stored procedure associated with that SQL Endpoint. Even for Fabric Pipelines, one could use an Azure SQL Database connection (instead of Lakehouse), connect to the SQL Endpoint of a Lakehouse and execute a stored procedure associated with that SQL Endpoint. This I believe is the most efficient way to do it. The issue I have with D is the fact that Dataflows require significant resources to spin up and execute. Good thing with it is that there is no ambiguity mentioned above and if you want to get the answer right, might not be the most efficient but without more verbosity in the choices, I painfully chose it."
      },
      {
        "date": "2024-06-20T16:21:00.000Z",
        "voteCount": 1,
        "content": "I asked chatgpt and I've got this: Based on the requirements for the semantic model of the Online Sales department, the best solution to refresh the Orders table would be to include an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse. This approach ensures that only new orders are processed, maintaining the sequence and integrity of the OrderID values as per the system of origin. Therefore, the correct answer is:\n\nA. an Azure Data Factory pipeline that executes a Stored procedure activity to retrieve the maximum value of the OrderID column in the destination lakehouse."
      },
      {
        "date": "2024-06-03T02:02:00.000Z",
        "voteCount": 3,
        "content": "IMHO, it has to be done with dataflow (D) because the semantic model uses an Import mode so I think it doesn't support a store procedure (SQL)"
      },
      {
        "date": "2024-05-14T01:37:00.000Z",
        "voteCount": 2,
        "content": "Here the key words are \"to retrieve\", so if you run a pipeline to execute something to retrieve a value then it should be a Store Procedure using the lookup activity. This is the most effective way to do it.\nThe answers are not telling you the entire process to insert the new data (which it could be with dataflow) else it is telling you what activity to use in the pipeline to retrieve the maximum value of the OrderID. At least this is what I understood."
      },
      {
        "date": "2024-05-07T13:25:00.000Z",
        "voteCount": 1,
        "content": "i think it should be A"
      },
      {
        "date": "2024-05-07T10:19:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \nit is very good explained here: https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2\n\nSo, it is not A, because of SP is not good to load data from sources in data lake. B and C - not an option at all, because it doesn't make sense to query the minimum order to make an incremental loading."
      },
      {
        "date": "2024-05-13T05:59:00.000Z",
        "voteCount": 1,
        "content": "Hey when can we use SP then ?"
      },
      {
        "date": "2024-05-13T14:43:00.000Z",
        "voteCount": 3,
        "content": "Additional evidence for D is, Storage Procedure Activity doesn't return value. It just runs the SP. The LookUp Activity does. But not SP. You can see it here on the screens:\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/stored-procedure-activity#step-3-choose-a-stored-procedure-and-configure-parameters"
      },
      {
        "date": "2024-04-29T03:30:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2"
      },
      {
        "date": "2024-04-28T12:54:00.000Z",
        "voteCount": 3,
        "content": "The solution is using Lakehouses. You can't create stored procedures in the sql endpoint of a Lakehouse."
      },
      {
        "date": "2024-05-03T05:34:00.000Z",
        "voteCount": 1,
        "content": "actually I just did it. so, yes, you can."
      },
      {
        "date": "2024-04-20T04:41:00.000Z",
        "voteCount": 1,
        "content": "For delta load, max order id is the PK so"
      },
      {
        "date": "2024-03-28T06:59:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A"
      },
      {
        "date": "2024-03-10T11:18:00.000Z",
        "voteCount": 7,
        "content": "The answer is A. You should query the last ID from the destination by lookup activity which uses the stored procedure, data flow is not used for this purpose.\nhttps://learn.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-portal"
      },
      {
        "date": "2024-07-06T21:53:00.000Z",
        "voteCount": 1,
        "content": "We need to find out least solution. In the requirement , they have not mentioned any services related to DB and If you want to use stored procedure, you need either Azure SQL DB or Azure SQL DW service. Hence Dataflow approach is optimal one for this requirement"
      },
      {
        "date": "2024-03-17T07:20:00.000Z",
        "voteCount": 5,
        "content": "I agree.\nAzure Data Factory \"pipelines\" is different from Azure Data Factory \"Data Flows\".\nAll the options are directing us to use Azure Data Factory \"pipelines\", so it should be using a stored procedure."
      },
      {
        "date": "2024-03-31T15:15:00.000Z",
        "voteCount": 2,
        "content": "That would be for Azure, not necessarily for Fabric. Please refer the following URL: https://learn.microsoft.com/en-au/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2"
      },
      {
        "date": "2024-03-18T04:41:00.000Z",
        "voteCount": 1,
        "content": "The  lookup activity can be a SQL statement, the link provided mentions to use a stored proc to update the watermark. I believe the desired way is to use dataflow gen 2, https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-setup-incremental-refresh-with-dataflows-gen2."
      },
      {
        "date": "2024-02-18T00:15:00.000Z",
        "voteCount": 1,
        "content": "D. an Azure Data Factory pipeline that executes a dataflow to retrieve the maximum value of the OrderID column in the destination lakehouse.\n\nA dataflow can be used to retrieve the max OrderID number (stored in the destination table - OrderID is a sequencial number). This number can be used to set from which row data must be added to the destination table (implementing an incremental load)."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134088-exam-dp-600-topic-1-question-4-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br>Existing Environment -<br><br>Identity Environment -<br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br>Data Environment -<br>Contoso has the following data environment:<br>The Sales division uses a Microsoft Power BI Premium capacity.<br>The semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>The Research department uses an on-premises, third-party data warehousing product.<br>Fabric is enabled for contoso.com.<br>An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>A Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br>Requirements -<br><br>Planned Changes -<br>Contoso plans to make the following changes:<br>Enable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>Make all the data for the Sales division and the Research division available in Fabric.<br>For the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>In Productline1ws, create a lakehouse named Lakehouse1.<br>In Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br>Data Analytics Requirements -<br>Contoso identifies the following data analytics requirements:<br>All the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>The Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>All the semantic models and reports for the Research division must use version control that supports branching.<br><br>Data Preparation Requirements -<br>Contoso identifies the following data preparation requirements:<br>The Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br>Semantic Model Requirements -<br>Contoso identifies the following requirements for implementing and managing semantic models:<br>The number of rows added to the Orders table during refreshes must be minimized.<br>The semantic models in the Research division workspaces must use Direct Lake mode.<br><br>General Requirements -<br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br>Follow the principle of least privilege when applicable.<br>Minimize implementation and maintenance effort when possible.<br>Which syntax should you use in a notebook to access the Research division data for Productline1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read.format(\u201cdelta\u201d).load(\u201cTables/productline1/ResearchProduct\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.sql(\u201cSELECT * FROM Lakehouse1.ResearchProduct \u201d)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texternal_table(\u2018Tables/ResearchProduct)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texternal_table(ResearchProduct)"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-20T17:56:00.000Z",
        "voteCount": 11,
        "content": "The correct answer is B.\nThe folder hierarchy of Tables in Lakehouse is incorrect for A."
      },
      {
        "date": "2024-03-31T23:38:00.000Z",
        "voteCount": 3,
        "content": "Yes B is correct answer as per the requirement specified in case study - For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints."
      },
      {
        "date": "2024-09-21T21:14:00.000Z",
        "voteCount": 5,
        "content": "df = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\ndisplay(df)\nOR\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")\ndisplay(df)\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts"
      },
      {
        "date": "2024-08-22T11:56:00.000Z",
        "voteCount": 1,
        "content": "answer is A. in B there is no productline1"
      },
      {
        "date": "2024-08-07T11:08:00.000Z",
        "voteCount": 4,
        "content": "With the recent introduction of schema-enabled Lakehouses, BOTH A and B are correct. That is assuming ResearchProduct table was created in a schema-enabled Lakehouse in the productline1 schema. I have tested A in a Fabric Spark notebook that is schema-enabled and it works."
      },
      {
        "date": "2024-07-01T00:15:00.000Z",
        "voteCount": 1,
        "content": "Hierarchy for answer A is not correct"
      },
      {
        "date": "2024-06-20T21:03:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts\n\ndf = spark.read.format(\"delta\").load(\"Tables/MyShortcut\")\nOR\ndf = spark.sql(\"SELECT * FROM MyLakehouse.MyShortcut LIMIT 1000\")"
      },
      {
        "date": "2024-05-21T05:57:00.000Z",
        "voteCount": 1,
        "content": "If answer B is the correct, why Files folder dosen`t appear between Lakehouse1 and ResearchProduct? It has to be like \"lakehouse1.Files.ResearchProduct\", hasn't it?"
      },
      {
        "date": "2024-05-07T10:59:00.000Z",
        "voteCount": 1,
        "content": "IMHO,\nI would go with A, because of there is a clear statement ProductLine1. \nTechnically, A and B should work, but B doesn't have \"ProductLine\", what is confusing"
      },
      {
        "date": "2024-05-13T14:52:00.000Z",
        "voteCount": 4,
        "content": "My bad, the right answer is B.\nThere is a requirement: Research Division - all tables should be managed. It means, no subfolders should be.\n\nhttps://learn.microsoft.com/en-us/training/modules/use-apache-spark-work-files-lakehouse/5-spark-sql"
      },
      {
        "date": "2024-05-07T02:35:00.000Z",
        "voteCount": 1,
        "content": "There is no productline1 used in Answer B. Then how it could be correct...."
      },
      {
        "date": "2024-05-07T01:38:00.000Z",
        "voteCount": 1,
        "content": "Yeah, the answer is B. Cause the folder hierarchy is kind of not true. \nIt should be something else...."
      },
      {
        "date": "2024-05-05T21:57:00.000Z",
        "voteCount": 2,
        "content": "The correct answer id B.\nThe productionline1 represents workspace \n so in option A \nspark.read.format(\u201cdelta\u201d).load(\u201cTables/productline1/ResearchProduct\u201d)\nthere they mentioned productionline1 which is workspace so it is wrong"
      },
      {
        "date": "2024-04-29T03:43:00.000Z",
        "voteCount": 1,
        "content": "B. spark.sql(\u201cSELECT * FROM Lakehouse1.ResearchProduct \u201d)"
      },
      {
        "date": "2024-04-20T04:43:00.000Z",
        "voteCount": 2,
        "content": "A syntax error"
      },
      {
        "date": "2024-03-20T01:13:00.000Z",
        "voteCount": 2,
        "content": "Should be A : This syntax uses the spark.read.format().load() method to read data from the specified location in the Delta format, which is a popular format for managing big data within data lakes or warehouses. It specifies the path where the data for Productline1's research division is stored.\n\nOption B, spark.sql(\"SELECT * FROM Lakehouse1.ResearchProduct\"), executes a SQL query to select all data from a table named ResearchProduct in a database/schema called Lakehouse1. However, it doesn't specify the path or format of the data, so it may not be appropriate for accessing specific data within a notebook, especially if it's stored in a Delta format in a specific location."
      },
      {
        "date": "2024-03-04T23:34:00.000Z",
        "voteCount": 2,
        "content": "B is the correct one"
      },
      {
        "date": "2024-02-25T08:40:00.000Z",
        "voteCount": 3,
        "content": "Cant be A: The path specified seems to assume a direct file system access rather than accessing through a lakehouse structure or shortcut. This syntax is used for reading data from a Delta Lake storage format.\n\nThe answer IS B: assuming 'Lakehouse1.ResearchProduct' refers to a structured dataset within Lakehouse1, This syntax correctly uses Spark SQL to query data. This is consistent with how lakehouse data is accessed through SQL queries.\n\nCant be C: Incomplete/incorrect format for accessing data in a spark environment. It is not the typical syntax used for Spark DataFrame or SQL API Calls.\n\nCant be D: Similar logic to C, it is missing the context/format needed to access the data. The syntax is not a correct Spark API call."
      },
      {
        "date": "2024-02-19T19:39:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is B.\nA should correct as :\ndf = spark.read.format(\"delta\").load(\"Tables/ResearchProduct\")\n\nThe Table folder could not handle two hierarchical"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133620-exam-dp-600-topic-1-question-5-discussion/",
    "body": "HOTSPOT -<br><br>Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br>Existing Environment -<br><br>Fabric Environment -<br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br>Available Data -<br>Litware has data that must be analyzed as shown in the following table.<br><img title=\"image4\" src=\"https://img.examtopics.com/dp-600/image4.png\"><br>The Product data contains a single table and the following columns.<br><img title=\"image5\" src=\"https://img.examtopics.com/dp-600/image5.png\"><br>The customer satisfaction data contains the following tables:<br><br>Survey -<br><br>Question -<br><br>Response -<br>For each survey submitted, the following occurs:<br>One row is added to the Survey table.<br>One row is added to the Response table for each question in the survey.<br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br>User Problems -<br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br>Requirements -<br><br>Planned Changes -<br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br>The following three workspaces will be created:<br>AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>DataSciPOC: Will contain all the notebooks and reports created by the data scientists<br>The following will be created in the AnalyticsPOC workspace:<br>A data store (type to be decided)<br><br>A custom semantic model -<br><br>A default semantic model -<br><br>Interactive reports -<br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br>Technical Requirements -<br>The data store must support the following:<br>Read access by using T-SQL or Python<br>Semi-structured and unstructured data<br>Row-level security (RLS) for users executing T-SQL queries<br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br>List prices that are less than or equal to 50 are in the low pricing group.<br>List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>List prices that are greater than 1,000 are in the high pricing group.<br><br>Security Requirements -<br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br>Fabric administrators will be the workspace administrators.<br>The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>The date dimension must be available to all users of the data store.<br>The principle of least privilege must be followed.<br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br>FabricAdmins: Fabric administrators<br>AnalyticsTeam: All the members of the analytics team<br>DataAnalysts: The data analysts on the analytics team<br>DataScientists: The data scientists on the analytics team<br>DataEngineers: The data engineers on the analytics team<br>AnalyticsEngineers: The analytics engineers on the analytics team<br><br>Report Requirements -<br>The data analysts must create a customer satisfaction report that meets the following requirements:<br>Enables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>Shows data as soon as the data is updated in the data store.<br>Ensures that the report and the semantic model only contain data from the current and previous year.<br>Ensures that the report respects any table-level security specified in the source data store.<br>Minimizes the execution time of report queries.<br>You need to assign permissions for the data store in the AnalyticsPOC workspace. The solution must meet the security requirements.<br>Which additional permissions should you assign when you share the data store? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image6\" src=\"https://img.examtopics.com/dp-600/image6.png\">",
    "options": [],
    "answer": "<img title=\"image7\" src=\"https://img.examtopics.com/dp-600/image7.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-14T10:22:00.000Z",
        "voteCount": 61,
        "content": "Here is my take on it:\nData Engineers: Read all Apache Spark - because they need to be able to work with Spark for Data curation.\nData Analysts: Build Reports on the default dataset - because they are report builders\nData Scientists: Read SQL Endpoints - They leverage curated data (by engineers) to do predictive analytics.\nLet me know what you think."
      },
      {
        "date": "2024-06-28T12:27:00.000Z",
        "voteCount": 1,
        "content": "I think Data Analysts Building Reports on the default dataset is correct"
      },
      {
        "date": "2024-06-13T22:11:00.000Z",
        "voteCount": 2,
        "content": "it says the data scientist need to use spark - so spark for them in my opinion"
      },
      {
        "date": "2024-05-28T02:17:00.000Z",
        "voteCount": 4,
        "content": "but data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook. SQL Endpoints not going to do."
      },
      {
        "date": "2024-04-28T03:13:00.000Z",
        "voteCount": 5,
        "content": "I agree. With the SQL Endpoint you can not write data to it, so it's useless for the data engineers but great for the data scientist."
      },
      {
        "date": "2024-05-21T18:48:00.000Z",
        "voteCount": 1,
        "content": "Yeah, that's right"
      },
      {
        "date": "2024-02-18T22:31:00.000Z",
        "voteCount": 32,
        "content": "Data Engineers = Read sql endpoints\nData Analyst = Build Reports\nData Scientist = They prefer Spark Always"
      },
      {
        "date": "2024-07-23T02:10:00.000Z",
        "voteCount": 1,
        "content": "as the provided data (The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.), according to this It would not be possible for them to build report using the default dataset."
      },
      {
        "date": "2024-05-18T04:29:00.000Z",
        "voteCount": 7,
        "content": "Data Analysts \u2013 Build Reports\nData Scientists \u2013 Read All Apache Spark.\nThe confusion is about the Data Engineers: They must be able to read and write to the data store. The SQL end point is read only. They should not have build reports. Hence the remaining option is read all Apache spark. Hence in this question no one gets access to the SQL Endpoint."
      },
      {
        "date": "2024-05-14T20:34:00.000Z",
        "voteCount": 6,
        "content": "i think this is part of a solutio of the security requirement and data engineer cannot perform write action with any of these permission so i think\nData Engineers: Read SQL Endpoints\nData Analysts: Build Reports on the default dataset\nData Scientists:  Read all Apache Spark"
      },
      {
        "date": "2024-05-07T15:10:00.000Z",
        "voteCount": 12,
        "content": "IMHO,\n- DEs - Spark, \n- Analysts - Build Report, \n- DS - Spark (as well). \n\nWhy?\n\n1. Here is the description of the options: https://blog.fabric.microsoft.com/en-us/blog/data-warehouse-sharing/\n\n2. Here is the role description in the question itself:\nThe data engineers must be able to !!!read from and write!!! to the data store. No access must be granted to datasets or reports.\nThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a !!! Spark notebook\nThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by !!! using the semantic models created by the analytics engineers.\nThe date dimension must be available to all users of the data store."
      },
      {
        "date": "2024-04-29T04:59:00.000Z",
        "voteCount": 6,
        "content": "DataEngineers: ReadAll Apache Spark\nDataAnalyst: Build Reports on the default dataset\nDataScientist: ReadAll Apache Spark"
      },
      {
        "date": "2024-03-24T03:41:00.000Z",
        "voteCount": 5,
        "content": "\"The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook\" so for data scientists we have to give  Read All Apache Spark"
      },
      {
        "date": "2024-03-22T12:08:00.000Z",
        "voteCount": 2,
        "content": "Feeding the question to ChatGPT it says given the option of A, B and C respectively:\nData Engineers: Option B: Read All Apache Spark (Allows reading data from Apache Spark notebooks)\nOption C: Read All SQL analytics endpoint data (Allows reading data from SQL analytics endpoint)\n\nData Scientists: Option B: Read All Apache Spark (Allows reading data from Apache Spark notebooks)\nData Analysts: Option A: Build Reports on the default dataset (Allows creating reports)"
      },
      {
        "date": "2024-04-23T06:06:00.000Z",
        "voteCount": 9,
        "content": "stop using chatgpt as a source, it's not trustworthy"
      },
      {
        "date": "2024-02-22T06:29:00.000Z",
        "voteCount": 3,
        "content": "My take: the SQL Analytics Endpoint is read-only, so it's perfect for the DataScientist. DataEngineers need read and write, so Spark. The Analysts need the report capabilities obviously."
      },
      {
        "date": "2024-06-22T07:59:00.000Z",
        "voteCount": 1,
        "content": "SQL Endpoints do not support Spark Notebooks"
      },
      {
        "date": "2024-04-24T07:27:00.000Z",
        "voteCount": 3,
        "content": "All of these are read-only. It confuses me"
      },
      {
        "date": "2024-02-20T18:08:00.000Z",
        "voteCount": 9,
        "content": "DataEngineers: ReadAll Apache Spark\nDataAnalyst: Build Reports on the default dataset\nDataScientist: ReadAll Apache Spark\n\nData engineers will use Pyspark in notebooks to transform data from the Files folder in the Lakehouse.\nData analysts will build reports and dashboards from the prepared dataset.\nData scientists will use MLLib in notebooks to build models.\nThe \"Read All SQL analytics endpoint data\" should be for the AnalyticsEngineers. The analytics team has four types of members."
      },
      {
        "date": "2024-03-08T22:42:00.000Z",
        "voteCount": 1,
        "content": "This is about access in AnalyticsPOC, Data Scientists don't need to access Apache Spark in this workspace they should only be able to read from SQL endpoint, they will create Spark notebooks in their own workspace and this question is not concerned about that workspace."
      },
      {
        "date": "2024-05-03T05:44:00.000Z",
        "voteCount": 1,
        "content": "actually it says that data scientists will use notebooks so they need ReadAll Apache Spark"
      },
      {
        "date": "2024-02-19T09:13:00.000Z",
        "voteCount": 5,
        "content": "Data Engineers = ReadAll Apache Spark\nData Analyst = Build Reports\nData Scientist = ReadAll Apache Spark"
      },
      {
        "date": "2024-02-18T00:33:00.000Z",
        "voteCount": 4,
        "content": "Data Engineers: Read all SQL Analytics Endpoint data (use SQL to explore and create/modify tables, views, stored procedures).\nData Analysts: Build reports on the default dataset (using Power BI).\nData Scientists: Read all Apache Spark (they will use Notebooks to analyze data and apply ML models)."
      },
      {
        "date": "2024-02-17T14:46:00.000Z",
        "voteCount": 4,
        "content": "Engineers = spark\nAnalyst = report\nScientist = Endpoint"
      },
      {
        "date": "2024-02-13T07:38:00.000Z",
        "voteCount": 5,
        "content": "For Data Engineers: Read SQL Endpoints\nFor Data Analysts: Build Reports on the default dataset\nFor Data Scientists: Read all Apache Spark"
      },
      {
        "date": "2024-02-13T07:59:00.000Z",
        "voteCount": 4,
        "content": "Maybe we can have multiple options for each as it is in Fabric :\nFor Data Engineers: Read SQL Endpoints and Read all Apache Spark\nFor Data Analysts: Build Reports on the default dataset and Read SQL Endpoints\nFor Data Scientists: Read all Apache Spark"
      },
      {
        "date": "2024-02-12T08:19:00.000Z",
        "voteCount": 2,
        "content": "For Data Engineers: Read all Apache Spark\nFor Data Analysts: Read SQL Endpoints\nFor Data Scientists: the last one remaining."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133445-exam-dp-600-topic-1-question-6-discussion/",
    "body": "HOTSPOT -<br><br>Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br>Existing Environment -<br><br>Fabric Environment -<br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br>Available Data -<br>Litware has data that must be analyzed as shown in the following table.<br><img title=\"image8\" src=\"https://img.examtopics.com/dp-600/image4.png\"><br>The Product data contains a single table and the following columns.<br><img title=\"image9\" src=\"https://img.examtopics.com/dp-600/image5.png\"><br>The customer satisfaction data contains the following tables:<br><br>Survey -<br><br>Question -<br><br>Response -<br>For each survey submitted, the following occurs:<br>One row is added to the Survey table.<br>One row is added to the Response table for each question in the survey.<br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br>User Problems -<br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br>Requirements -<br><br>Planned Changes -<br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br>The following three workspaces will be created:<br>AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>DataSciPOC: Will contain all the notebooks and reports created by the data scientists<br>The following will be created in the AnalyticsPOC workspace:<br>A data store (type to be decided)<br><br>A custom semantic model -<br><br>A default semantic model -<br><br>Interactive reports -<br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br>Technical Requirements -<br>The data store must support the following:<br>Read access by using T-SQL or Python<br>Semi-structured and unstructured data<br>Row-level security (RLS) for users executing T-SQL queries<br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br>List prices that are less than or equal to 50 are in the low pricing group.<br>List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>List prices that are greater than 1,000 are in the high pricing group.<br><br>Security Requirements -<br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br>Fabric administrators will be the workspace administrators.<br>The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>The date dimension must be available to all users of the data store.<br>The principle of least privilege must be followed.<br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br>FabricAdmins: Fabric administrators<br>AnalyticsTeam: All the members of the analytics team<br>DataAnalysts: The data analysts on the analytics team<br>DataScientists: The data scientists on the analytics team<br>DataEngineers: The data engineers on the analytics team<br>AnalyticsEngineers: The analytics engineers on the analytics team<br><br>Report Requirements -<br>The data analysts must create a customer satisfaction report that meets the following requirements:<br>Enables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>Shows data as soon as the data is updated in the data store.<br>Ensures that the report and the semantic model only contain data from the current and previous year.<br>Ensures that the report respects any table-level security specified in the source data store.<br>Minimizes the execution time of report queries.<br>You need to create a DAX measure to calculate the average overall satisfaction score.<br>How should you complete the DAX code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image10\" src=\"https://img.examtopics.com/dp-600/image8.png\">",
    "options": [],
    "answer": "<img title=\"image11\" src=\"https://img.examtopics.com/dp-600/image9.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-09T05:56:00.000Z",
        "voteCount": 75,
        "content": "Iwould say:\nAVERAGE('Survery'[Respone Value]) and Period\n\nI do not unterstand why we would use the CustomerKey for Average Calculation."
      },
      {
        "date": "2024-03-19T10:11:00.000Z",
        "voteCount": 7,
        "content": "Average('Survey'[Response Value]) and Period"
      },
      {
        "date": "2024-03-29T12:05:00.000Z",
        "voteCount": 2,
        "content": "Response Value is the only one that even likely could be aggregated. The question text and customer key do not make sense."
      },
      {
        "date": "2024-09-21T21:16:00.000Z",
        "voteCount": 6,
        "content": "AVERAGE('Survey'[Response Value]),\nPeriod.\n\n\nAVERAGEA('Question'[Question Text]),\nIt doesn't make much sense to average a text field when you have to extract a value from it, although AVERAGEA can do it.\n\nAVERAGEX(VALUES('Survey'[Customer Key]),\nSyntax is wrong, discarded. AVERAGEX(&lt;table&gt;, &lt;expression&gt;)\nAdditionally VALUES leaves only the distinct values\n\n\nLastCurrentDate,\nIn this case we would have only half a day. Has no sense.\n\nNumberOfMonths,\nWe would be filtering by 12. It doesn't make sense.\n\nPeriod,\nIn the variable we have the dates of the last 12 months."
      },
      {
        "date": "2024-09-21T21:16:00.000Z",
        "voteCount": 1,
        "content": "Rolling 12 Overall Satisfaction =\nVAR NumberOfMonths = 12\nVAR LastCurrentDate = MAX ( 'Date' [Date] )\nVAR Period = DATESINPERIOD ( 'Date' [Date], LastCurrentDate, -NumberOfMonths, MONTH )\nVAR Result =\n    CALCULATE (\n        AVERAGEX (\n            VALUES('Survey'[Customer Key]),\n            CALCULATE(\n                AVERAGE('Survey'[Response Value])\n            )\n        ),\n        Period,\n        'Survey Question'[Question Title] = \"Overall Satisfaction\"\n    )\nRETURN\n    Result\n**There should be some missing part in this screenshot, but the correct answer would be the VALUES('Survey'[Customer Key]) with the missing part CALCULATE(AVERAGE('Survey'[Response Value]))"
      },
      {
        "date": "2024-06-08T23:28:00.000Z",
        "voteCount": 2,
        "content": "It should be :\n1. Average(Survey[Response Value]) as this one is the only value that can be aggregated.\nCustomer Key Does not make any sense , as it just a key to identify a customer, this can never provide us the score.\n2. Period : Variable is defined as to select 1 year of date range. Can directly be passed in the Filter context of Calculate formula. Thanks!"
      },
      {
        "date": "2024-05-08T11:24:00.000Z",
        "voteCount": 1,
        "content": "1 = AVERAGE('Survery'[Respone Value]). customer satisfaction can be determind by response value.\n2= Period. choose this just because it state overall no specific time or requires a date range."
      },
      {
        "date": "2024-05-07T15:24:00.000Z",
        "voteCount": 2,
        "content": "IMHO,\nAVERAGE('Survery'[Respone Value]) and Period\n\nWhy?\nThe first one defines actually the metric, which is avg from survey(response value), \nthe second one defines the date range, which is precalculated before as a variable Period."
      },
      {
        "date": "2024-04-29T05:04:00.000Z",
        "voteCount": 1,
        "content": "AVERAGE('Survery'[Respone Value]) , Period"
      },
      {
        "date": "2024-02-18T00:39:00.000Z",
        "voteCount": 5,
        "content": "For the first part, \"AVERAGE('Survey'[Response Value])\" because the second option uses a text column as argument, and the third option is not relevant in this context (no need to perform row calculations).\nFor the second part, \"Period\": data is filtered to compute the average in the last 12 months (interval defined in the variable)."
      },
      {
        "date": "2024-02-17T14:47:00.000Z",
        "voteCount": 5,
        "content": "Average and Period"
      },
      {
        "date": "2024-02-13T11:28:00.000Z",
        "voteCount": 4,
        "content": "Response Value and Period"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133446-exam-dp-600-topic-1-question-7-discussion/",
    "body": "HOTSPOT -<br><br>Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br>Existing Environment -<br><br>Fabric Environment -<br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br>Available Data -<br>Litware has data that must be analyzed as shown in the following table.<br><img title=\"image12\" src=\"https://img.examtopics.com/dp-600/image4.png\"><br>The Product data contains a single table and the following columns.<br><img title=\"image13\" src=\"https://img.examtopics.com/dp-600/image5.png\"><br>The customer satisfaction data contains the following tables:<br><br>Survey -<br><br>Question -<br><br>Response -<br>For each survey submitted, the following occurs:<br>One row is added to the Survey table.<br>One row is added to the Response table for each question in the survey.<br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br>User Problems -<br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br>Requirements -<br><br>Planned Changes -<br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br>The following three workspaces will be created:<br>AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>DataSciPOC: Will contain all the notebooks and reports created by the data scientists<br>The following will be created in the AnalyticsPOC workspace:<br>A data store (type to be decided)<br><br>A custom semantic model -<br><br>A default semantic model -<br><br>Interactive reports -<br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br>Technical Requirements -<br>The data store must support the following:<br>Read access by using T-SQL or Python<br>Semi-structured and unstructured data<br>Row-level security (RLS) for users executing T-SQL queries<br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br>List prices that are less than or equal to 50 are in the low pricing group.<br>List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>List prices that are greater than 1,000 are in the high pricing group.<br><br>Security Requirements -<br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br>Fabric administrators will be the workspace administrators.<br>The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>The date dimension must be available to all users of the data store.<br>The principle of least privilege must be followed.<br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br>FabricAdmins: Fabric administrators<br>AnalyticsTeam: All the members of the analytics team<br>DataAnalysts: The data analysts on the analytics team<br>DataScientists: The data scientists on the analytics team<br>DataEngineers: The data engineers on the analytics team<br>AnalyticsEngineers: The analytics engineers on the analytics team<br><br>Report Requirements -<br>The data analysts must create a customer satisfaction report that meets the following requirements:<br>Enables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>Shows data as soon as the data is updated in the data store.<br>Ensures that the report and the semantic model only contain data from the current and previous year.<br>Ensures that the report respects any table-level security specified in the source data store.<br>Minimizes the execution time of report queries.<br>You need to resolve the issue with the pricing group classification.<br>How should you complete the T-SQL statement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image14\" src=\"https://img.examtopics.com/dp-600/image10.png\">",
    "options": [],
    "answer": "<img title=\"image15\" src=\"https://img.examtopics.com/dp-600/image11.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-18T00:46:00.000Z",
        "voteCount": 47,
        "content": "* VIEW: from an existing table.\n* CASE: correct syntax before the WHENs.\n* WHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium': the other two options miss value 1000; on the other hand, the BETWEEN includes both 50 and 1000."
      },
      {
        "date": "2024-05-03T06:22:00.000Z",
        "voteCount": 5,
        "content": "* VIEW: from an existing table.\n* CASE: correct syntax before the WHENs.\n* WHEN(ListPrice &gt; 50 AND ListPrice &lt; 1000 THEN 'medium': The second option will generate 2 rows for any '50' answer; the third option has the same problem plus a miss spelled  ' ) ' that will end up generating a syntax error.\n\nThe only answer that will not generate a cartesian product is the first one, although it will miss all the '1000' answers."
      },
      {
        "date": "2024-05-25T00:04:00.000Z",
        "voteCount": 2,
        "content": "wrong 3rd option. You get NULL PriceGroup when price is exactly 1000"
      },
      {
        "date": "2024-05-24T17:19:00.000Z",
        "voteCount": 4,
        "content": "The 3rd option is still the best--when doing a select with CASE WHEN, the logic is evaluated linearly. If the value is 50, the first case is true and the returned value is therefore \"low\". This can be tested with dummy data--just upload a csv file of numbers from 1 to 20 into a test warehouse and query in Fabric.\n\nselect number, \ncase when number &lt;= 5 then 'low'\nwhen number between 5 and 10 then 'med'\nwhen number &gt; 10 then 'high'\nend as test\nfrom dbo.numbers\norder by number asc\n\nAs for the missing '(' or extra ')', I assume this is a typo."
      },
      {
        "date": "2024-04-15T22:44:00.000Z",
        "voteCount": 2,
        "content": "I agree that 1000 must be included (in the medium), but if the BETWEEN includes 50, and low should include 50, and it states before that &lt;= 50 is low, then 50 shouldn't be included... ?"
      },
      {
        "date": "2024-02-22T06:38:00.000Z",
        "voteCount": 27,
        "content": "All options for the last one are wrong, which is irritating. Should be &gt; 50 AND &lt;= 1000."
      },
      {
        "date": "2024-03-25T04:14:00.000Z",
        "voteCount": 10,
        "content": "The range BETWEEN 50 AND 1000 is also a valid answer. However, if the value is 50, it will be returned from the previous \u2018when\u2019 clause, ListPrice &lt;= 50 Then 'low', before reaching the second \u2018when\u2019 clause.\u201d"
      },
      {
        "date": "2024-09-12T16:22:00.000Z",
        "voteCount": 2,
        "content": "view, case - no correct answer - my guess is they miseed the = on a"
      },
      {
        "date": "2024-08-13T07:50:00.000Z",
        "voteCount": 1,
        "content": "* VIEW\n* CASE\n* WHEN ListPrice Between 50 and 1000 Then Medium: The first when is includes 50, so even though the between should be from 51 to 1000, it is (in this case) irrelevant, because in this execution order the frist WHEN wins. Therefor between should produce correct results."
      },
      {
        "date": "2024-06-19T09:56:00.000Z",
        "voteCount": 1,
        "content": "The valid answer for 3rd is first one as the 1st case is When List Price &lt;= 50 then 'low', so the next case should not be include equal to 50 in anyway, but option 2 has List Price &gt;= 50 which is wrong and between function will also consider = 50 values so it is also wrong."
      },
      {
        "date": "2024-06-19T05:45:00.000Z",
        "voteCount": 1,
        "content": "View\nTo satisfy \"Shows data as soon as the data is updated in the data store\"\nYou cannot use CTAS Statement as it will not update when source data is updated.\n\nCase\nThe usual SQL Case Statement\n\nLast one is...... no correct answer\nEither question is wrong or answer options are wrong.\nBETWEEN is inclusive by the way"
      },
      {
        "date": "2024-06-16T20:34:00.000Z",
        "voteCount": 17,
        "content": "I took the test a few days ago. This question was included, but Microsoft has made a change to: WHEN(ListPrice &gt; 50 AND ListPrice &lt;= 1000) THEN 'medium'. Therefore, I chose this option."
      },
      {
        "date": "2024-06-15T06:51:00.000Z",
        "voteCount": 1,
        "content": "View\nCASE \nWHEN(ListPrice &gt;= 50 AND ListPrice &lt; 1000) then as between is inclusive of 50 it will provide incorrect result"
      },
      {
        "date": "2024-06-08T23:26:00.000Z",
        "voteCount": 2,
        "content": "It should be \n1. Table : As it should be part of Default Semantic Model. With view it's not possible to achieve this.\n2. Case Statement : As we have \"When\" and \"Then\"\n3. Between 51* and 1000  or it should be Listprice&gt;50 and Listprice &lt;=1000. Thanks!"
      },
      {
        "date": "2024-06-20T07:17:00.000Z",
        "voteCount": 1,
        "content": "Within the warehouse, a user can add warehouse objects - tables or views to their default Power BI semantic model.  I would go with VIEW as view Shows data as soon as the data is updated in the data store.https://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model"
      },
      {
        "date": "2024-06-08T10:59:00.000Z",
        "voteCount": 1,
        "content": "Is the solution provided the true correct answer? In many cases it's not, which is misleading. It definitely should be a view and not a table because the Product table changes and thus the pricing group being updated. Where do the solutions come from??"
      },
      {
        "date": "2024-05-27T09:04:00.000Z",
        "voteCount": 3,
        "content": "CREATE VIEW [dbo].[ProductsWithPricingGroup]\nAS\nSELECT \n    ProductId,\n    ProductName,\n    ProductCategory,\n    ListPrice,\n    CASE \n        WHEN ListPrice &lt;= 50 THEN 'low'\n        WHEN ListPrice &gt; 50 AND ListPrice &lt;= 1000 THEN 'medium'\n        WHEN ListPrice &gt; 1000 THEN 'high'\n    END AS PricingGroup\nFROM dbo.Products;"
      },
      {
        "date": "2024-05-18T21:39:00.000Z",
        "voteCount": 4,
        "content": "VIEW\nCASE\nWHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium'\n\nLook at the requirement:\nList prices that are less than or equal to 50 -&gt; low\nList prices that are greater than 50 and less than or equal to 1,000 -&gt; medium\nList prices that are greater than 1,000 -&gt; high\n\nOnly using option \"BETWEEN 50 AND 1000\" will be able to get the correct answer. If the ListPrice is 50, the case will resolve it as \"low\" in the first condition and return the value. Even though 50 is included in the next case \"BETWEEN 50 AND 1000\", it will not fall into this condition as it has already been resolved with the first condition."
      },
      {
        "date": "2024-05-10T10:40:00.000Z",
        "voteCount": 2,
        "content": "select country,first_name, age , case when age &lt;=25 then \"child\"\nwhen age between 25 and 28 then \"adult\"\nwhen age &gt; 28 then \"old\" \nend as ages\nfrom customers;  \ncountry\tfirst_name\tage\tages\nUSA\tJohn\t31\told\nUSA\tRobert\t22\tchild\nUK\tDavid\t22\tchild\nUK\tJohn\t25\tchild\nUAE\tBetty\t28\tadult\n\n\nCorrect answer will be VIEW, CASE AND BETWEEN."
      },
      {
        "date": "2024-05-07T15:34:00.000Z",
        "voteCount": 2,
        "content": "IMHO,\nVIEW -&gt; CASE -&gt; ... BEETWEEN ...\n\nWhy BETWEEN? It is a tricky stuff. It should be between 51 &amp; 1000. This option works well as well because the SQL engine after processing the first one, will ignore the second condition.\n\nList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group."
      },
      {
        "date": "2024-05-03T06:26:00.000Z",
        "voteCount": 1,
        "content": "* VIEW: Shows data as soon as the data is updated in the data store.\n* CASE: correct syntax before the WHENs.\n* WHEN(ListPrice &gt; 50 AND ListPrice &lt; 1000 THEN 'medium': The second option will generate 2 rows for any '50' answer; the third option has the same problem plus a miss spelled ' ) ' that will end up generating a syntax error.\n\nThe only answer that will not generate a cartesian product is the first one, although it will miss all the '1000' answers. That's a smaller problem than a Cartesian product."
      },
      {
        "date": "2024-04-29T05:09:00.000Z",
        "voteCount": 2,
        "content": "View\nCase\nWHEN ListPrice BETWEEN 50 AND 1000 THEN 'medium"
      },
      {
        "date": "2024-04-22T04:21:00.000Z",
        "voteCount": 1,
        "content": "This is the 7th question and in all of them there were mistakes!\nIs there someone responsible to verify the quality?!\nI know it's a beta version, but still..."
      },
      {
        "date": "2024-06-02T01:23:00.000Z",
        "voteCount": 1,
        "content": "welcome to examtopics where the community decides on what is right and wrong to enable providing this service for free"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133447-exam-dp-600-topic-1-question-8-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br>Existing Environment -<br><br>Fabric Environment -<br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br>Available Data -<br>Litware has data that must be analyzed as shown in the following table.<br><img title=\"image16\" src=\"https://img.examtopics.com/dp-600/image4.png\"><br>The Product data contains a single table and the following columns.<br><img title=\"image17\" src=\"https://img.examtopics.com/dp-600/image5.png\"><br>The customer satisfaction data contains the following tables:<br><br>Survey -<br><br>Question -<br><br>Response -<br>For each survey submitted, the following occurs:<br>One row is added to the Survey table.<br>One row is added to the Response table for each question in the survey.<br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br>User Problems -<br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br>Requirements -<br><br>Planned Changes -<br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br>The following three workspaces will be created:<br>AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>DataSciPOC: Will contain all the notebooks and reports created by the data scientists<br>The following will be created in the AnalyticsPOC workspace:<br>A data store (type to be decided)<br><br>A custom semantic model -<br><br>A default semantic model -<br><br>Interactive reports -<br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br>Technical Requirements -<br>The data store must support the following:<br>Read access by using T-SQL or Python<br>Semi-structured and unstructured data<br>Row-level security (RLS) for users executing T-SQL queries<br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br>List prices that are less than or equal to 50 are in the low pricing group.<br>List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>List prices that are greater than 1,000 are in the high pricing group.<br><br>Security Requirements -<br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br>Fabric administrators will be the workspace administrators.<br>The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>The date dimension must be available to all users of the data store.<br>The principle of least privilege must be followed.<br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br>FabricAdmins: Fabric administrators<br>AnalyticsTeam: All the members of the analytics team<br>DataAnalysts: The data analysts on the analytics team<br>DataScientists: The data scientists on the analytics team<br>DataEngineers: The data engineers on the analytics team<br>AnalyticsEngineers: The analytics engineers on the analytics team<br><br>Report Requirements -<br>The data analysts must create a customer satisfaction report that meets the following requirements:<br>Enables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>Shows data as soon as the data is updated in the data store.<br>Ensures that the report and the semantic model only contain data from the current and previous year.<br>Ensures that the report respects any table-level security specified in the source data store.<br>Minimizes the execution time of report queries.<br>What should you recommend using to ingest the customer data into the data store in the AnalyticsPOC workspace?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta stored procedure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta pipeline that contains a KQL activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Spark notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dataflow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-10T07:04:00.000Z",
        "voteCount": 6,
        "content": "D. a dataflow\nEs la mejor opci\u00f3n.\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\"\n\"Data will be loaded without transformation in one area of the AnalyticsPOC data store\"\n\n\nA. a stored procedure\nNo tiene sentido usar un procedimiento almacenado para hacer la carga.\n\nB. a pipeline that contains a KQL activity\nKQL es para datos en tiempo real\n\nC. a Spark notebook\nPodr\u00eda valer, pero el texto pone,\n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\""
      },
      {
        "date": "2024-04-21T23:14:00.000Z",
        "voteCount": 2,
        "content": "i feel answer is pipeline with KQL and using kql we get real time data"
      },
      {
        "date": "2024-02-21T01:32:00.000Z",
        "voteCount": 5,
        "content": "In the Interactive reports requirement, it stated, \"Whenever possible, the data engineers will use low-code tools for data ingestion\"."
      },
      {
        "date": "2024-10-15T03:18:00.000Z",
        "voteCount": 1,
        "content": "Strange, no one had got the correct answer yet: it is Spark Notebook.\nIf you had 50 GB, then dataflow is fine, but premium Fabric won't manage to handle 500 GB with dataflow if you need to do any updates on the data. And you should manage with less than premium/F64 capacity for these requirements. So the extra cost for Notebook programming will be saved many times over with lower capacity requirements."
      },
      {
        "date": "2024-05-07T15:40:00.000Z",
        "voteCount": 4,
        "content": "IMHO,\nD (dataflow), because of this:  \n\"Whenever possible, the data engineers will use low-code tools for data ingestion.\""
      },
      {
        "date": "2024-04-29T05:13:00.000Z",
        "voteCount": 2,
        "content": "Whenever possible, the data engineers will use low-code tools for data ingestion."
      },
      {
        "date": "2024-04-27T02:40:00.000Z",
        "voteCount": 1,
        "content": "\"Shows data as soon as the data is updated in the data store\" \n\nKQL?"
      },
      {
        "date": "2024-04-27T02:41:00.000Z",
        "voteCount": 2,
        "content": "nevermind, it specified as soon as it gets to the data store, that implies as soon as it's loaded in fabric"
      },
      {
        "date": "2024-04-21T23:13:00.000Z",
        "voteCount": 1,
        "content": "i feel answer is pipeline with KQL and using kql we get real time data"
      },
      {
        "date": "2024-04-06T09:02:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-03-19T10:37:00.000Z",
        "voteCount": 2,
        "content": "Dataflow is the best choice."
      },
      {
        "date": "2024-03-02T02:35:00.000Z",
        "voteCount": 1,
        "content": "Guys, what about this requirement ? \"Shows data as soon as the data is updated in the data store.\""
      },
      {
        "date": "2024-03-09T04:39:00.000Z",
        "voteCount": 3,
        "content": "That's a report requirement, not a data ingestion requirement. You are talking here how to get data from a place to the analyticsPOC, not getting the data from a semantic model directly into the report."
      },
      {
        "date": "2024-04-21T23:14:00.000Z",
        "voteCount": 1,
        "content": "i feel answer is pipeline with KQL and using kql we get real time data"
      },
      {
        "date": "2024-03-19T10:37:00.000Z",
        "voteCount": 1,
        "content": "That is setting things up for something in Power BI to use direct lake I think, not related to the data as the data has not arrived in essence if not moved there yet."
      },
      {
        "date": "2024-02-18T00:49:00.000Z",
        "voteCount": 5,
        "content": "D. a dataflow\n\nEven though the text reads \"Data will be loaded without transformation in one area of the AnalyticsPOC data store\": in general, dataflows are used when data transformations are involved after ingestion. As suggested by user BHARAT, the Copy Activity should be the optimal solution."
      },
      {
        "date": "2024-04-21T23:14:00.000Z",
        "voteCount": 1,
        "content": "i feel answer is pipeline with KQL and using kql we get real time data"
      },
      {
        "date": "2024-02-17T14:49:00.000Z",
        "voteCount": 1,
        "content": "D see Bharat comment."
      },
      {
        "date": "2024-02-14T10:37:00.000Z",
        "voteCount": 4,
        "content": "Ideally, It should be the COPY activity of the pipeline, but that is not given as a choice"
      },
      {
        "date": "2024-02-12T07:45:00.000Z",
        "voteCount": 3,
        "content": "\"Whenever possible, the data engineers will use low-code tools for data ingestion.\""
      },
      {
        "date": "2024-02-09T06:08:00.000Z",
        "voteCount": 2,
        "content": "Spark Notebook is also possible but i would say D is correct: \n\"Data will be loaded without transformation in one area of the AnalyticsPOC data store\""
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133448-exam-dp-600-topic-1-question-9-discussion/",
    "body": "Case study -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br>Existing Environment -<br><br>Fabric Environment -<br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br>Available Data -<br>Litware has data that must be analyzed as shown in the following table.<br><img title=\"image19\" src=\"https://img.examtopics.com/dp-600/image4.png\"><br>The Product data contains a single table and the following columns.<br><img title=\"image20\" src=\"https://img.examtopics.com/dp-600/image5.png\"><br>The customer satisfaction data contains the following tables:<br><br>Survey -<br><br>Question -<br><br>Response -<br>For each survey submitted, the following occurs:<br>One row is added to the Survey table.<br>One row is added to the Response table for each question in the survey.<br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br>User Problems -<br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br>Requirements -<br><br>Planned Changes -<br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br>The following three workspaces will be created:<br>AnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>DataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>DataSciPOC: Will contain all the notebooks and reports created by the data scientists<br>The following will be created in the AnalyticsPOC workspace:<br>A data store (type to be decided)<br><br>A custom semantic model -<br><br>A default semantic model -<br><br>Interactive reports -<br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br>Technical Requirements -<br>The data store must support the following:<br>Read access by using T-SQL or Python<br>Semi-structured and unstructured data<br>Row-level security (RLS) for users executing T-SQL queries<br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br>List prices that are less than or equal to 50 are in the low pricing group.<br>List prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>List prices that are greater than 1,000 are in the high pricing group.<br><br>Security Requirements -<br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br>Fabric administrators will be the workspace administrators.<br>The data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>The analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>The data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>The data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>The date dimension must be available to all users of the data store.<br>The principle of least privilege must be followed.<br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br>FabricAdmins: Fabric administrators<br>AnalyticsTeam: All the members of the analytics team<br>DataAnalysts: The data analysts on the analytics team<br>DataScientists: The data scientists on the analytics team<br>DataEngineers: The data engineers on the analytics team<br>AnalyticsEngineers: The analytics engineers on the analytics team<br><br>Report Requirements -<br>The data analysts must create a customer satisfaction report that meets the following requirements:<br>Enables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>Displays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>Shows data as soon as the data is updated in the data store.<br>Ensures that the report and the semantic model only contain data from the current and previous year.<br>Ensures that the report respects any table-level security specified in the source data store.<br>Minimizes the execution time of report queries.<br>Which type of data store should you recommend in the AnalyticsPOC workspace?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta data lake",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta lakehouse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan external Hive metastore"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-07T15:44:00.000Z",
        "voteCount": 7,
        "content": "IMHO,\nmy answer is C. Because of that: \"\"\"Semi-structured and unstructured data\"\"\" in the question"
      },
      {
        "date": "2024-04-10T07:13:00.000Z",
        "voteCount": 6,
        "content": "C. to lakehouse\nAs a concept it exists in Fabric and is the most logical response for the given requirements,\nTechnical Requirements -\nSemi-structured and unstructured data\nFiles loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.\n\nA. a data lake\nAs an answer it could be valid, but it is not really a concept that we have in Fabric.\n\nB. a warehouse\nIt is not valid, because there is unstructured data.\n\nD. an external Hive metastore\nHas no sense.\nThe Hive Metastore is a centralized repository that stores metadata related to the tables, partitions, columns, schemas, and other data structures used by Hive and Spark.\nContains information about the PHYSICAL location of the data, the schemas of the tables, and the relationships between them."
      },
      {
        "date": "2024-04-29T05:16:00.000Z",
        "voteCount": 1,
        "content": "Semi-structured and unstructured data"
      },
      {
        "date": "2024-04-10T00:11:00.000Z",
        "voteCount": 2,
        "content": "Semi-structured and unstructured data"
      },
      {
        "date": "2024-03-19T10:45:00.000Z",
        "voteCount": 1,
        "content": "Lakehouse makes the most sense."
      },
      {
        "date": "2024-02-25T08:48:00.000Z",
        "voteCount": 1,
        "content": "lakehouse 100%"
      },
      {
        "date": "2024-02-21T01:37:00.000Z",
        "voteCount": 4,
        "content": "In the technical requirement, it stated \"Semi-structured and unstructured data\" for the AnalyticsPOC data store. Thus, it must be a lakehouse."
      },
      {
        "date": "2024-02-18T00:53:00.000Z",
        "voteCount": 3,
        "content": "C. a lakehouse\n\nThe data store must handle semi-structured and unstructured data, therefore a Lakehouse should be the optimal solution supporting read access with T-SQL and Python."
      },
      {
        "date": "2024-02-17T14:50:00.000Z",
        "voteCount": 1,
        "content": "Analytic=lakehouse"
      },
      {
        "date": "2024-02-12T07:47:00.000Z",
        "voteCount": 4,
        "content": "\"Read access by using T-SQL or Python\nSemi-structured and unstructured data\""
      },
      {
        "date": "2024-02-09T06:09:00.000Z",
        "voteCount": 2,
        "content": "C. a lakehouse"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133449-exam-dp-600-topic-1-question-10-discussion/",
    "body": "You have a Fabric warehouse that contains a table named Staging.Sales. Staging.Sales contains the following columns.<br><img title=\"image21\" src=\"https://img.examtopics.com/dp-600/image13.png\"><br>You need to write a T-SQL query that will return data for the year 2023 that displays ProductID and ProductName and has a summarized Amount that is higher than 10,000.<br>Which query should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image22\" src=\"https://img.examtopics.com/dp-600/image14.png\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image23\" src=\"https://img.examtopics.com/dp-600/image15.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image24\" src=\"https://img.examtopics.com/dp-600/image16.png\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img title=\"image25\" src=\"https://img.examtopics.com/dp-600/image17.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-25T08:51:00.000Z",
        "voteCount": 16,
        "content": "Anything without HAVING() + an aggregate is incorrect. HAVING was created for SQL to deal with filtering using an aggregate. Any option that references TotalAmount is incorrect because there is no nested statement in the syntax. Anything that uses HAVING() + DATEPART() is incorrect because you use a where clause for that. The answer is A."
      },
      {
        "date": "2024-07-17T03:46:00.000Z",
        "voteCount": 1,
        "content": "I am confused between A and B. But A is a correct Answer. Here is the explanation-\nIf we use non-aggregate function to Having clause it will throw below error-\nColumn 'SalesDate' is invalid in the HAVING clause because it is not contained in either an aggregate function or the GROUP BY clause."
      },
      {
        "date": "2024-06-07T04:47:00.000Z",
        "voteCount": 1,
        "content": "Actually both D &amp; A are correct, aren't they?\n\"TotalAmount\" is already available after the GROUP BY clause, or are there any reasons D is not correct?"
      },
      {
        "date": "2024-06-08T08:43:00.000Z",
        "voteCount": 4,
        "content": "In D TotalAmount is an alias (HAVING TotalAmount &gt; 10000) and only ORDER BY accepts aliases."
      },
      {
        "date": "2024-05-28T20:38:00.000Z",
        "voteCount": 2,
        "content": "In some instances, you might want to exclude individual rows from groups (using a WHERE clause) before applying a condition to groups as a whole (using a HAVING clause).\n\nA HAVING clause is like a WHERE clause, but applies only to groups as a whole (that is, to the rows in the result set representing groups), whereas the WHERE clause applies to individual rows. A query can contain both a WHERE clause and a HAVING clause. In that case:\n\nThe WHERE clause is applied first to the individual rows in the tables or table-valued objects in the Diagram pane. Only the rows that meet the conditions in the WHERE clause are grouped.\n\nThe HAVING clause is then applied to the rows in the result set. Only the groups that meet the HAVING conditions appear in the query output. You can apply a HAVING clause only to columns that also appear in the GROUP BY clause or in an aggregate function."
      },
      {
        "date": "2024-05-14T10:53:00.000Z",
        "voteCount": 1,
        "content": "the table includes 'SalesDate' but every answer includes 'SaleDate' so they are all wrong!!! E. None of the above"
      },
      {
        "date": "2024-06-06T06:58:00.000Z",
        "voteCount": 1,
        "content": "The Table does not have to include SalesDate. It just return data for the year 2023, which can be filtered under the hood and not to be shown in SELECT."
      },
      {
        "date": "2024-05-07T15:47:00.000Z",
        "voteCount": 1,
        "content": "IMHO, A"
      },
      {
        "date": "2024-05-06T05:29:00.000Z",
        "voteCount": 3,
        "content": "I would say Option D. \nSum(Amount) AS TotalAmount is effectively TotalAmount &gt; 10000"
      },
      {
        "date": "2024-05-02T06:10:00.000Z",
        "voteCount": 3,
        "content": "SELECT ProductID, ProductName, SUM(Amount) AS TotalAmount\nFROM Staging.Sales\nWHERE DATEPART(YEAR, SaleDate) = '2023'\nGROUP BY ProductID, ProductName\nHAVING SUM(Amount) &gt; 10000"
      },
      {
        "date": "2024-04-30T22:46:00.000Z",
        "voteCount": 1,
        "content": "second one could be correct but saledate is neither used in group by nor in any aggregate function. Hence A is the correct answer."
      },
      {
        "date": "2024-03-01T14:03:00.000Z",
        "voteCount": 2,
        "content": "BTW spark sql allows us to refer to the count() alias"
      },
      {
        "date": "2024-02-21T01:40:00.000Z",
        "voteCount": 2,
        "content": "The answer is A, no-brainer."
      },
      {
        "date": "2024-02-18T00:59:00.000Z",
        "voteCount": 1,
        "content": "A. SELECT ProductID, ProductName, SUM(Amount) AS TotalAmount\nFROM Staging.Sales\nWHERE DATEPART(YEAR, SaleDate) = '2023'\nGROUP BY ProductID, ProductName\nHAVING SUM(Amount) &gt; 10000\n\nSelected data is first filtered by Year, then grouped by ProductID and ProductName to compute the TotalAmount. Finally, only SUM(Amount) cane be used after HAVING (not the alias)."
      },
      {
        "date": "2024-02-17T14:53:00.000Z",
        "voteCount": 2,
        "content": "Where to filter year data\nHaving to filter summerized data\nAlias like TotalAmount not work in having"
      },
      {
        "date": "2024-02-10T04:04:00.000Z",
        "voteCount": 4,
        "content": "TotalAmount can not be used with HAVING. You must use SUM(Amount)"
      },
      {
        "date": "2024-02-09T06:13:00.000Z",
        "voteCount": 3,
        "content": "Summarized Amount by ProductID and ProductName -&gt; Group BY\nAbove 10.000 -&gt; HAVING SUM(Amount) ..."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133613-exam-dp-600-topic-1-question-11-discussion/",
    "body": "HOTSPOT -<br>You have a data warehouse that contains a table named Stage.Customers. Stage.Customers contains all the customer record updates from a customer relationship management (CRM) system. There can be multiple updates per customer.<br>You need to write a T-SQL query that will return the customer ID, name. postal code, and the last updated time of the most recent row for each customer ID.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image26\" src=\"https://img.examtopics.com/dp-600/image18.png\">",
    "options": [],
    "answer": "Box 1: ROW_NUMBER()<br>Box 2: WHERE X = 1",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-12T07:04:00.000Z",
        "voteCount": 20,
        "content": "The correct query is:\nWITH CUSTOMERBASE AS (\n     SELECT CustomerID, CustomerName, PostalCode, LastUpdated,\n          ROW_NUMBER() OVER(PARTITION BY CustomerID ORDER BY LastUpdated DESC) as X\n     FROM LakehousePOC.dbo.CustomerChanges\n     )\n\nSELECT CustomerID, CustomerName, PostalCode, LastUpdated\nFROM CUSTOMERBASE\nWHERE X = 1"
      },
      {
        "date": "2024-03-14T19:07:00.000Z",
        "voteCount": 12,
        "content": "Answer is RowNumber and X=1 -- No Brainer"
      },
      {
        "date": "2024-07-18T09:35:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct \n RowNumber and X=1"
      },
      {
        "date": "2024-06-09T00:14:00.000Z",
        "voteCount": 1,
        "content": "Its straight forwarded. Provided answer is correct."
      },
      {
        "date": "2024-05-07T15:51:00.000Z",
        "voteCount": 3,
        "content": "IMHO,\n1. Row_Number()\n2. x = 1\n\nIt is a typical pattern. We are numerating by desc, and then filtering the first one (actually the last because of DESC order)"
      },
      {
        "date": "2024-02-25T08:52:00.000Z",
        "voteCount": 5,
        "content": "ROW_NUMBER() + X = 1"
      },
      {
        "date": "2024-02-21T01:46:00.000Z",
        "voteCount": 5,
        "content": "First drop-down box:  ROW_NUMBER()\nSecond drop-down box:  WHERE X = 1 \nAs ORDER BY LastUpdated DESC was used, the first row will be the most recent row."
      },
      {
        "date": "2024-02-17T14:55:00.000Z",
        "voteCount": 5,
        "content": "Row_number\nX=1\n\nRow_number give row position and start from 1"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134062-exam-dp-600-topic-1-question-12-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant.<br>You plan to create a Fabric notebook that will use Spark DataFrames to generate Microsoft Power BI visuals.<br>You run the following code.<br><img title=\"image27\" src=\"https://img.examtopics.com/dp-600/image19.png\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image28\" src=\"https://img.examtopics.com/dp-600/image20.png\">",
    "options": [],
    "answer": "<img title=\"image29\" src=\"https://img.examtopics.com/dp-600/image21.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-17T14:58:00.000Z",
        "voteCount": 36,
        "content": "I think : No yes no\nTo display summary its describe() or sumarry()"
      },
      {
        "date": "2024-06-14T05:43:00.000Z",
        "voteCount": 4,
        "content": "The code displays a summary of the DataFrame.\n\nThe last one should be Yes: The QuickVisualize function generates and displays a summary visualization of the DataFrame."
      },
      {
        "date": "2024-05-25T04:02:00.000Z",
        "voteCount": 1,
        "content": "Correct! but display() function is MS fabric native function which gives more control on summarizing df including various vizualizations"
      },
      {
        "date": "2024-02-25T08:53:00.000Z",
        "voteCount": 13,
        "content": "Answer is No, Yes, Yes\n\nThe code is displaying a summary of the data frame using the visualizations. Point 3 is a technicality and I think to some extent depends on your interpretation of the question. I am going with Yes."
      },
      {
        "date": "2024-04-27T03:08:00.000Z",
        "voteCount": 3,
        "content": "I understand your reasoning but i would go with No in 3, when I think in a summary of a dataframe I'm imagining something different."
      },
      {
        "date": "2024-08-27T14:06:00.000Z",
        "voteCount": 1,
        "content": "No, Yes, Yes"
      },
      {
        "date": "2024-06-13T15:21:00.000Z",
        "voteCount": 9,
        "content": "The answer is No Yes Yes.\nSee the documentation link, contains the exact code that is in the question\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization"
      },
      {
        "date": "2024-06-13T15:23:00.000Z",
        "voteCount": 3,
        "content": "The doc clearly states that the last code line i.e. PBI_visualize renders new report"
      },
      {
        "date": "2024-06-02T01:55:00.000Z",
        "voteCount": 2,
        "content": "Creates a PBI report? Not so sure, it shows a visualization like a PBI report, does not save it as such until you hit save."
      },
      {
        "date": "2024-05-27T09:37:00.000Z",
        "voteCount": 5,
        "content": "The code embeds an existing Power BI report:\n\nNo: Typically, embedding an existing Power BI report involves using Power BI REST API or embedding URLs, not Spark DataFrame operations within a notebook. The provided code snippet seems more focused on data manipulation and visual creation within the notebook itself, rather than embedding an external report.\nThe code creates a Power BI report:\n\nYes: When working with Spark DataFrames in a Fabric notebook, you can generate visuals that can be published to Power BI. This involves converting DataFrames to a format that Power BI can consume and using integration features provided by Fabric to create reports based on this data.\nThe code displays a summary of a DataFrame:\n\nYes: If the code snippet includes functions like display(df), df.show(), or similar methods, it is intended to display a summary or the contents of the DataFrame. These functions are commonly used to provide a quick overview of the data within the notebook environment."
      },
      {
        "date": "2024-05-26T23:09:00.000Z",
        "voteCount": 2,
        "content": "No yes no\nThe third one is a little bit confused though.\nAnyway, the QuickVisualize is using the DataFrame, but it doesn't directly display a summary of the DataFrame itself\nYou'd need to use another method like describe() or summary()"
      },
      {
        "date": "2024-06-14T05:42:00.000Z",
        "voteCount": 1,
        "content": "The code displays a summary of the DataFrame.\n\nThe last one should be Yes: The QuickVisualize function generates and displays a summary visualization of the DataFrame."
      },
      {
        "date": "2024-05-07T15:54:00.000Z",
        "voteCount": 8,
        "content": "IMHO,\nNo -&gt; Yes -&gt; Yes.\n\nLink is here: https://learn.microsoft.com/en-us/power-bi/create-reports/jupyter-quick-report#create-and-render-a-quick-visualize-instance\n\nIt is not embedded, it is an actual report. The 3rd one - is philosophical, but it summarizes the info from the data frame, so it makes sense to have \"Yes\" there"
      },
      {
        "date": "2024-05-07T15:56:00.000Z",
        "voteCount": 2,
        "content": "Probably, as colleagues said before, maybe the last one is No (it would be 100% yes for describe() )"
      },
      {
        "date": "2024-05-07T03:16:00.000Z",
        "voteCount": 1,
        "content": "No, Yes and According to https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization#displaydf-summary-view the third should be No"
      },
      {
        "date": "2024-05-02T06:52:00.000Z",
        "voteCount": 2,
        "content": "No, Yes, No\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/jupyter-quick-report#create-and-render-a-quick-visualize-instance"
      },
      {
        "date": "2024-04-27T01:19:00.000Z",
        "voteCount": 4,
        "content": "No, Yes, Yes. The last question is a bit confusing but the visualisation will have a title quick summary so I guess it is one. https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization"
      },
      {
        "date": "2024-05-25T04:01:00.000Z",
        "voteCount": 1,
        "content": "No confusion. Here QuickVisulize is used not display(), display() will summarize the df in table by default and you can quickly vizulize using various options you get to see them."
      },
      {
        "date": "2024-04-24T13:36:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/create-reports/jupyter-quick-report#create-and-render-a-quick-visualize-instance\n\nYes, Yes, No"
      },
      {
        "date": "2024-04-10T08:09:00.000Z",
        "voteCount": 5,
        "content": "The code embeds an existing PowerBI report. NO\nCode creates a new PBI Report since it's based on a df\n\nThe code creates a PowerBI report. YES\ncreates new power PBI Report\n\nThe code displays a summary of the DateFrame. NO\nNo: it creates a report\n\nhttps://powerbi.microsoft.com/en-us/blog/create-power-bi-reports-in-jupyter-notebooks/"
      },
      {
        "date": "2024-03-27T08:47:00.000Z",
        "voteCount": 2,
        "content": "simplest way to confirm is to test :\nthis is creating a brand new report based on the dataframe df, which provide insights about the data in df\n\nthen answer is No, Yes, Yes"
      },
      {
        "date": "2024-03-26T05:55:00.000Z",
        "voteCount": 5,
        "content": "1) No:  Code creates a new PBI Report since it's based on a df\n2) Yes: creates new power PBI Report (see thuss links)\n3) No: it creates a report and does not display a summary of a df (e.g. like df.summary() does)"
      },
      {
        "date": "2024-02-22T06:59:00.000Z",
        "voteCount": 2,
        "content": "The code snippet as well as the third option don't make much sense.\nI think they want yes for the first option, because they import Report, which is used for this purpose but would require several other steps, and is not used in the rest of the code at all. (https://learn.microsoft.com/en-us/javascript/api/overview/powerbi/powerbi-jupyter#embed-a-report)\nSecond answer is definitely yes, see https://learn.microsoft.com/en-us/power-bi/create-reports/jupyter-quick-report#create-and-render-a-quick-visualize-instance\nThird answer... who knows what they actually mean with that wording? I'd put no."
      },
      {
        "date": "2024-03-19T11:15:00.000Z",
        "voteCount": 1,
        "content": "To embed there needs to be more details like the workspace and report GUIDs not given. So I think while possible, not what is asked for and we have to not over think the question. However I agree the 3rd question depends on what it means. It is a graphical version interpreted, but not the same as the describe summary."
      },
      {
        "date": "2024-02-17T07:40:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/create-reports/jupyter-quick-report\n1) No - based on dataframe (df)\n2) Yes\n3) Yes/No? they need to be more specific about it, because the code does not explicitly display a summary of the DataFrame, but PowerBI report based on DataFrame"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133434-exam-dp-600-topic-1-question-13-discussion/",
    "body": "You are the administrator of a Fabric workspace that contains a lakehouse named Lakehouse1. Lakehouse1 contains the following tables:<br>Table1: A Delta table created by using a shortcut<br>Table2: An external table created by using Spark<br><br>Table3: A managed table -<br>You plan to connect to Lakehouse1 by using its SQL endpoint.<br>What will you be able to do after connecting to Lakehouse1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead Table3.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the data Table3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRead Table2.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the data in Table1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 78,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-10T11:02:00.000Z",
        "voteCount": 51,
        "content": "B &amp; D is out as you can\u2019t update a table in lakehouse using SQL endpoint as this is read only. You will need to use spark or dataflows. \nC is out because when you create external table using spark, you can see the table from the lakehouse but you can\u2019t see the table from SQL endpoint let alone ready.\n\nA is the answer, as I was able to see and read a managed table using SQL Endpoint"
      },
      {
        "date": "2024-05-30T04:37:00.000Z",
        "voteCount": 2,
        "content": "It seems  there is a way to have an external table created using Spark viewed using the Lkehouse SQL End point .\nThis is the spark code I used for testing: \ndf.write.format(\"delta\").option(\"path\",\"Tables/externalfolder\").saveAsTable(\"testmanagedcsveditors\")\nWhen I run the catalog query :\nspark.catalog.listTables(), it is returning as below :\nTable(name='unmanagedcsveditors', catalog='spark_catalog', namespace=['training_lakehouse'], description=None, tableType='EXTERNAL', isTemporary=False)\nNow if I go to the SQL end point, I can see a table named \"externalfolder\" .\nI agree that the table name should be \"umanagedcsveditors\", I can infact see 2 tables in the Lakehouse explorer - externalfolder and unmanagedcsveditors"
      },
      {
        "date": "2024-08-27T08:25:00.000Z",
        "voteCount": 1,
        "content": "The SQL analytics endpoint operates in read-only mode over lakehouse Delta tables. You can only read data from Delta tables using the SQL analytics endpoint. They can save functions, views, and set SQL object-level security.\nHence, B &amp; D are out. \nExternal Delta tables created with Spark code won't be visible to the SQL analytics endpoint. Use shortcuts in Table space to make external Delta tables visible to the SQL analytics endpoint. \nHence C is out. \n\nCorrect Answer: A"
      },
      {
        "date": "2024-07-29T06:53:00.000Z",
        "voteCount": 2,
        "content": "The SQL analytics endpoint operates in read-only mode over lakehouse Delta tables.\nExternal Delta tables created with Spark code won't be visible to the SQL analytics endpoint. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint"
      },
      {
        "date": "2024-07-22T20:22:00.000Z",
        "voteCount": 1,
        "content": "SQL Endpoint cannot update table --&gt; B &amp; D are outed.\nTable 2 is a external table so it won't be displayed in SQL Endpoint view --&gt; C is outed."
      },
      {
        "date": "2024-07-14T09:59:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer.\nIn a lakehouse, the tables that can be edited using SQL endpoints are primarily&nbsp;Delta tables.&nbsp;These tables are specifically designed to be compatible with SQL analytics endpoints, allowing you to perform various SQL operations such as querying, creating views, and applying SQL security.\n\nOther table formats like&nbsp;Parquet, CSV, and JSON&nbsp;are not directly editable through SQL endpoints; they need to be converted to Delta format first."
      },
      {
        "date": "2024-07-04T23:16:00.000Z",
        "voteCount": 2,
        "content": "External Delta tables created with Spark code won't be visible to the SQL analytics endpoint. \nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint\n\nSo C is out. \n\nB and D are also out since with a SQL endpoint we can't update tables."
      },
      {
        "date": "2024-06-12T06:56:00.000Z",
        "voteCount": 1,
        "content": "We can read table 1 and table 3  not table 2\nand can't write any of them....\nso ans will be A"
      },
      {
        "date": "2024-05-27T09:46:00.000Z",
        "voteCount": 2,
        "content": "Managed tables are fully controlled by the database system. These tables typically allow both read and write operations, including updates, through SQL endpoints.\n\nAfter connecting to Lakehouse1 using its SQL endpoint, you will be able to:\n\n- **Read Table3** (A)\n- **Update the data in Table3** (B)\n\nSo, the correct options are:\n\n**A. Read Table3.**\n**B. Update the data in Table3.**"
      },
      {
        "date": "2024-05-07T16:03:00.000Z",
        "voteCount": 4,
        "content": "IMHO, \nthe answer is A.\n\nupdates are prohibited at all. reading spark external table is a big no-no.\nLink: https://www.linkedin.com/pulse/use-shortcuts-instead-external-tables-reference-data-fabric-popovic/"
      },
      {
        "date": "2024-05-02T07:01:00.000Z",
        "voteCount": 2,
        "content": "C is not correct because the external tables are not accessible from the endpoint.\nB &amp; D is out of question cause as SQL endpoint is read only."
      },
      {
        "date": "2024-04-27T15:13:00.000Z",
        "voteCount": 2,
        "content": "D is completely wrong is not typically feasible through a shortcut in SQL endpoint setup. \nB Generally Supported but depends on the SQL endpoint permissions for such operations \n\nA  you should be able to read data from table3 since it is a managed table and such operations are standard through Sql Endpoints"
      },
      {
        "date": "2024-04-10T08:05:00.000Z",
        "voteCount": 2,
        "content": "A is correct. Tested!"
      },
      {
        "date": "2024-03-19T11:24:00.000Z",
        "voteCount": 4,
        "content": "A is correct, D is not. SQL endpoint is read-only.\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint"
      },
      {
        "date": "2024-03-19T11:25:00.000Z",
        "voteCount": 1,
        "content": "In that link it tells you as well to modify you have to switch\n\"To modify data in Lakehouse delta tables, you have to switch to lakehouse mode and use Apache Spark.\""
      },
      {
        "date": "2024-03-19T10:06:00.000Z",
        "voteCount": 2,
        "content": "Options B and D are incorrect because an endpoint does not support DML operations. Option C is not correct because the external tables are not accessible from the endpoint. The correct answer is A, the managed tables can be read from the connection point."
      },
      {
        "date": "2024-03-06T02:03:00.000Z",
        "voteCount": 3,
        "content": "A is answer"
      },
      {
        "date": "2024-02-26T08:33:00.000Z",
        "voteCount": 3,
        "content": "With SQL endpoint in a Lakehouse you can only read tables not update them. But that only applies to managed tables. External tables have to be read by logging inside the Lakehouse and using Spark dataframe commands."
      },
      {
        "date": "2024-02-25T08:54:00.000Z",
        "voteCount": 2,
        "content": "A. Read Table 3"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133594-exam-dp-600-topic-1-question-14-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse.<br>You use a dataflow to load a new dataset from OneLake to the warehouse.<br>You need to add a PowerQuery step to identify the maximum values for the numeric columns.<br>Which function should you include in the step?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.MaxN",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.Max",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.Range",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.Profile\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 44,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T03:15:00.000Z",
        "voteCount": 26,
        "content": "D. Table.Profile\n\nhttps://learn.microsoft.com/en-us/powerquery-m/table-profile"
      },
      {
        "date": "2024-07-29T08:43:00.000Z",
        "voteCount": 1,
        "content": "Not correct, will not add another step."
      },
      {
        "date": "2024-08-01T02:17:00.000Z",
        "voteCount": 2,
        "content": "Yes it will add a step check here please\nhttps://community.fabric.microsoft.com/t5/Community-Blog/Enhanced-Data-Profiling-in-Power-Query-GUI-and-Table-Profile/ba-p/3799762#:~:text=Fortunately%2C%20Power%20Query%20has%20a,step%20%23%22Changed%20Type%22."
      },
      {
        "date": "2024-04-21T23:35:00.000Z",
        "voteCount": 9,
        "content": "you will get summary in that max will include but it doesn't add as power query step so i think it is option b"
      },
      {
        "date": "2024-04-24T08:03:00.000Z",
        "voteCount": 5,
        "content": "Table.Max provides only a row with the max value per a single column. We have multiple of columns. It's d"
      },
      {
        "date": "2024-08-01T02:14:00.000Z",
        "voteCount": 2,
        "content": "PazaBlandData your description is Incorrect\nTable.Max returns the largest row in the table, given the comparisonCriteria\nhttps://learn.microsoft.com/en-us/powerquery-m/table-max"
      },
      {
        "date": "2024-06-08T18:55:00.000Z",
        "voteCount": 8,
        "content": "B guys, \"you need to add a step...\""
      },
      {
        "date": "2024-06-18T04:55:00.000Z",
        "voteCount": 3,
        "content": "Sorry Guys, I changed my mind. It is the letter D."
      },
      {
        "date": "2024-10-03T23:21:00.000Z",
        "voteCount": 1,
        "content": "Table.Profile gives Min,Max,Average,StandardDeviation,Count,NullCount,DistinctCount for all columns \nTable.Profile(#\"last step name\")\n\nTable.Max gives only the maximum value for the specified column(entire row of the maximum value column)\nTable.Max(#\"last step name\",\"column name\")"
      },
      {
        "date": "2024-08-08T07:34:00.000Z",
        "voteCount": 2,
        "content": "I vote D too. The ask is for maximum value for numeric columnS. Note plural. Table.MaxN and Table.Max will return entire row(s) with max value for a specific column (singlular) so they are out. Table.Profile returns the following stats for all columns where applicable:-\nminimum\nmaximum\naverage\nstandard deviation\ncount\nnull count\ndistinct count\n\nso that fits the bill."
      },
      {
        "date": "2024-08-01T02:22:00.000Z",
        "voteCount": 1,
        "content": "Not option B Table.Max because it will return the largest row even in that row if a single column has a value zero and other rows have some positive values it would be identified as maximum value and it it is not correct. \nQuestion states \"identify the maximum values for the numeric columns\"\nOption D is definitely correct for those who are saying it would not add a step check this \nhttps://community.fabric.microsoft.com/t5/Community-Blog/Enhanced-Data-Profiling-in-Power-Query-GUI-and-Table-Profile/ba-p/3799762#:~:text=Fortunately%2C%20Power%20Query%20has%20a,step%20%23%22Changed%20Type%22."
      },
      {
        "date": "2024-07-29T08:44:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C Incorrect, other logic\nD -&gt; Will not add another power query step, will only display\nB it is"
      },
      {
        "date": "2024-07-28T18:11:00.000Z",
        "voteCount": 2,
        "content": "The reason why Table.Max is not the correct choice is that Table.Max returns the row in a table that contains the maximum value for a specified column, rather than providing the maximum values for all numeric columns."
      },
      {
        "date": "2024-07-13T00:48:00.000Z",
        "voteCount": 2,
        "content": "The Table.Max function should be used in a Power Query step to identify the maximum values for the numeric columns. This function is designed to calculate the maximum value across each column in a table, which suits the requirement of finding maximum values for numeric columns. References = For detailed information on Power Query functions, including Table.Max, please refer to Power Query M function reference."
      },
      {
        "date": "2024-06-20T04:03:00.000Z",
        "voteCount": 1,
        "content": "The good answer is B (Table.Max) : https://learn.microsoft.com/en-us/powerquery-m/table-max"
      },
      {
        "date": "2024-06-21T13:29:00.000Z",
        "voteCount": 1,
        "content": "The Syntax is\nTable.Max(table as table, comparisonCriteria as any, optional default as any) as any\nthe last *as any* after the () is not optional, that's because you should define the column and the output will be row based which means only one row for the defined coulmn \nWhile in the the Question (identify the maximum values for the numeric *columns*) which means Table.Profile Answer D is the correct answer"
      },
      {
        "date": "2024-06-07T10:12:00.000Z",
        "voteCount": 2,
        "content": "Table.Max"
      },
      {
        "date": "2024-05-25T11:10:00.000Z",
        "voteCount": 4,
        "content": "Table.Max because we need only max values as per the question (Table.Profile also gives the result but why we need to run it when we need just max value)"
      },
      {
        "date": "2024-05-25T11:06:00.000Z",
        "voteCount": 4,
        "content": "Table profiling is a standard process for analyzing dataset. Anyone working with new dataset would do profiling and answer is D because question is looking for \"maximum values for the numeric columns.\""
      },
      {
        "date": "2024-05-25T11:09:00.000Z",
        "voteCount": 3,
        "content": "Ignore this. Correct answer is B. Since we are interested only in maximum values not all standard profiling columns which includes other functions such as min, count etc., which is not required and slower/expensive too."
      },
      {
        "date": "2024-05-08T09:56:00.000Z",
        "voteCount": 3,
        "content": "Definitely B.  \nA. Table.MaxN: This function finds the top N rows in a table based on a comparison criterion. While it could be used to find a maximum value indirectly, it's less efficient for the task at hand.\nC. Table.Range: This function creates a list of values within a specified range. It's not directly related to finding maximum values.\nD. Table.Profile: This provides statistical summaries of columns (min, max, mean, etc.). Using Table.Profile followed by extracting the relevant information would work but is more roundabout than just using Table.Max."
      },
      {
        "date": "2024-05-08T03:15:00.000Z",
        "voteCount": 2,
        "content": "D Table.Profile, just tried it out in Power Query, I am going to use it more in the future ;-)"
      },
      {
        "date": "2024-05-07T06:34:00.000Z",
        "voteCount": 4,
        "content": "IMHO, the answer is B:\nhttps://learn.microsoft.com/en-us/powerquery-m/table-max"
      },
      {
        "date": "2024-05-07T16:08:00.000Z",
        "voteCount": 5,
        "content": "Sorry, was wrong. Answer id D, because of \"for the numeric columns.\", not only for one column"
      },
      {
        "date": "2024-05-02T07:06:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/powerquery-m/table-profile"
      },
      {
        "date": "2024-04-27T15:19:00.000Z",
        "voteCount": 2,
        "content": "B Does not work on multiple columns simultaneously.\nA t's not specifically used for finding maximum values across all numeric columns, but rather for retrieving a set number of top records\nC This function is used to retrieve a specific number of rows, starting at a particular index in a table. It's not related to calculating maximum values at all \n\nD Using Table.Profile, you can efficiently obtain maximum values along with other statistical insights for each column in your dataset without needing to specify each column manually."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133734-exam-dp-600-topic-1-question-15-discussion/",
    "body": "You have a Fabric tenant that contains a machine learning model registered in a Fabric workspace.<br>You need to use the model to generate predictions by using the PREDICT function in a Fabric notebook.<br>Which two languages can you use to perform model scoring? Each correct answer presents a complete solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tT-SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPySpark\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T10:21:00.000Z",
        "voteCount": 10,
        "content": "Notebook only accepts the languages: PySpark, Spark, Spark SQL and SparkR"
      },
      {
        "date": "2024-06-04T02:11:00.000Z",
        "voteCount": 3,
        "content": "yes but you can use \"%%sql\" in a notebook"
      },
      {
        "date": "2024-06-23T03:20:00.000Z",
        "voteCount": 2,
        "content": "that indicates to use spark sql language."
      },
      {
        "date": "2024-06-19T18:10:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/fabric/data-science/tutorial-data-science-batch-scoring\nou'll load the test dataset into a spark DataFrame and create an MLFlowTransformer object to generate batch predictions. You can then invoke the PREDICT function using one of following three ways:\n\nTransformer API from SynapseML\nSpark SQL API\nPySpark user-defined function (UDF)"
      },
      {
        "date": "2024-06-11T00:42:00.000Z",
        "voteCount": 1,
        "content": "Notebook only accepts the languages: PySpark, Spark, Spark SQL and SparkR"
      },
      {
        "date": "2024-06-04T02:07:00.000Z",
        "voteCount": 1,
        "content": "We can use Predict function with T-SQL:\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16\nBut I cannot found the Predict function in Spark SQL. Is different to run a predict process in PySpark"
      },
      {
        "date": "2024-05-07T16:17:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \n\nthe answer is C &amp; D.\nLink is here: https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/tutorial-score-model-predict-spark-pool\nHere, \"\"\"You can call PREDICT three ways, using Spark SQL API, using User define function (UDF), and using Transformer API. \"\"\". \n\nThat means, UDF = PySpark in our case."
      },
      {
        "date": "2024-05-02T07:40:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/tutorial-score-model-predict-spark-pool"
      },
      {
        "date": "2024-02-21T03:25:00.000Z",
        "voteCount": 1,
        "content": "Using Fabric notebook, thus must be C and D."
      },
      {
        "date": "2024-02-18T05:40:00.000Z",
        "voteCount": 1,
        "content": "The mention of Fabric Notebook, gives the hint to using Spark. Thus I went with CD"
      },
      {
        "date": "2024-02-17T15:01:00.000Z",
        "voteCount": 2,
        "content": "C &amp; d in notebook"
      },
      {
        "date": "2024-02-17T08:00:00.000Z",
        "voteCount": 2,
        "content": "CD - https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/tutorial-score-model-predict-spark-pool"
      },
      {
        "date": "2024-02-13T03:46:00.000Z",
        "voteCount": 3,
        "content": "Answer CD Correct\nT-SQL Cannot be used, nor DAX"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134063-exam-dp-600-topic-1-question-16-discussion/",
    "body": "You are analyzing the data in a Fabric notebook.<br>You have a Spark DataFrame assigned to a variable named df.<br>You need to use the Chart view in the notebook to explore the data manually.<br>Which function should you run to make the data available in the Chart view?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdisplayHTML",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twrite",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdisplay\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-03T23:37:00.000Z",
        "voteCount": 1,
        "content": "The .show function displays the contents of a Spark DataFrame in a tabular format within the notebook but does not enable interactive data exploration with the Chart view. To make the data available in the Chart view for interactive visualization, you should use the display function (Answer D)"
      },
      {
        "date": "2024-07-01T00:46:00.000Z",
        "voteCount": 2,
        "content": "Display dataframe function should be correct"
      },
      {
        "date": "2024-06-18T09:10:00.000Z",
        "voteCount": 1,
        "content": "D. DISPLAY"
      },
      {
        "date": "2024-06-15T03:00:00.000Z",
        "voteCount": 1,
        "content": "Show is the correct answer\nThank you."
      },
      {
        "date": "2024-06-09T05:25:00.000Z",
        "voteCount": 2,
        "content": "B: show is correct. plt.show()"
      },
      {
        "date": "2024-06-07T10:13:00.000Z",
        "voteCount": 1,
        "content": "display(df)"
      },
      {
        "date": "2024-05-07T16:19:00.000Z",
        "voteCount": 4,
        "content": "IMHO, \n\nthe answer is D.\nLink is here: https://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization\n\nPoint is: You can use the display function on dataframes that created in PySpark and Scala on Spark DataFrames or Resilient Distributed Datasets (RDD) functions to produce the rich dataframe table view and chart view."
      },
      {
        "date": "2024-06-23T18:48:00.000Z",
        "voteCount": 1,
        "content": "Out of all the reviewers, I like the way you explain all the answers. Just wondering if you were able to ace the exam"
      },
      {
        "date": "2024-05-07T10:21:00.000Z",
        "voteCount": 1,
        "content": "Display(df)\nAllows you to  view the data in chart"
      },
      {
        "date": "2024-03-19T10:25:00.000Z",
        "voteCount": 2,
        "content": "Checked, the correct option is Display."
      },
      {
        "date": "2024-02-25T08:57:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer (the keyword is chart view)\n\nA is another possibility for displaying data but not based on the requirements of this question,  it is separate from chart view. \n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization"
      },
      {
        "date": "2024-02-17T15:07:00.000Z",
        "voteCount": 2,
        "content": "Display allow to see chart and inspect statistiques"
      },
      {
        "date": "2024-02-17T08:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133441-exam-dp-600-topic-1-question-17-discussion/",
    "body": "You have a Fabric tenant that contains a Microsoft Power BI report named Report1. Report1 includes a Python visual.<br>Data displayed by the visual is grouped automatically and duplicate rows are NOT displayed.<br>You need all rows to appear in the visual.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReference the columns in the Python code by index.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Sort Column By property for all columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a unique field to each row.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the Summarize By property for all columns."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-25T08:59:00.000Z",
        "voteCount": 53,
        "content": "A - often the Microsoft learn pages give you the exact answer:\nUnder tips on the link I posted:\n\nIn some cases, you might not want automatic grouping to occur, or you might want all rows to appear, including duplicates. In those cases, you can add an index field to your dataset that causes all rows to be considered unique and prevents grouping.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals\n\nThere are always multiple approaches in application but if you do the MS exam give the MS answer."
      },
      {
        "date": "2024-06-25T11:30:00.000Z",
        "voteCount": 4,
        "content": "sorry but that (A) sounds incorrect. This is from the MS documentation: \"In those cases, you can add an index field to your dataset that causes all rows to be considered unique and prevents grouping.\" and it means create new column that has a unique value for each and every row of the dataset. *C* is correct"
      },
      {
        "date": "2024-04-25T05:37:00.000Z",
        "voteCount": 24,
        "content": "Adding an index field to your dataset means that you add a column/field that's unique to each row, meaning that the correct answer is C, not A. \"Reference columns by index\" means writing df.iloc[indexNo] instead of df[\"fieldName\"], and that doesn't help anything."
      },
      {
        "date": "2024-02-27T00:35:00.000Z",
        "voteCount": 3,
        "content": "It even says \"The default aggregation is Don't summarize\" already. Helpful link, definitely correct answer."
      },
      {
        "date": "2024-02-09T05:17:00.000Z",
        "voteCount": 12,
        "content": "The right answer is D: By setting the \"Summarize By\" property to \"None\" for all columns, you disable automatic aggregation and ensure all rows, including duplicates, are displayed in the Python visual."
      },
      {
        "date": "2024-09-26T08:10:00.000Z",
        "voteCount": 1,
        "content": "A: https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals\n\nIn some cases, you might not want automatic grouping to occur, or you might want all rows to appear, including duplicates. In those cases, you can add an index field to your dataset that causes all rows to be considered unique and prevents grouping."
      },
      {
        "date": "2024-08-28T02:45:00.000Z",
        "voteCount": 2,
        "content": "MS ANSWER.\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals"
      },
      {
        "date": "2024-08-27T09:00:00.000Z",
        "voteCount": 1,
        "content": "As much as C is tempting, let's focus on the wording. \"Add a unique field to each row\". You simply say, \"Add another column for unique id\" or something like that. \nIn terms of Microsoft ways of asking, it's A."
      },
      {
        "date": "2024-08-09T16:54:00.000Z",
        "voteCount": 1,
        "content": "Power BI tends to automatically summarize data in visuals, which can cause rows to be grouped and duplicates to be hidden.\nThe \"Summarize By\" property controls how data is aggregated in a visual. By setting it to \"Do Not Summarize,\" you prevent the automatic grouping of data, ensuring that all rows, including duplicates, are displayed.\nNone of the other options directly address the issue of rows being grouped and duplicates being hidden in a Power BI visual.\n\nSo, D is the right choice."
      },
      {
        "date": "2024-08-09T03:41:00.000Z",
        "voteCount": 1,
        "content": "It seems not everyone can read and understand documentation. The following Python script is execute EVERY TIME you use a Python visual:\n\ndataset = pandas.DataFrame(column1, column2, ...)\ndataset = dataset.drop_duplicates()\n\nSo the ONLY way to prevent the duplicates from being dropped is to have a unique identifier.\nErgo, option C is the only possible answer here."
      },
      {
        "date": "2024-06-21T07:47:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals\n--- Respostas \"A\"... da MS... \nEm alguns casos, talvez voc\u00ea n\u00e3o queira que o agrupamento autom\u00e1tico ocorra ou queira que todas as linhas apare\u00e7am, inclusive as duplicadas. Nesses casos, voc\u00ea pode adicionar um campo de \u00edndice ao seu conjunto de dados que faz com que todas as linhas sejam consideradas exclusivas e evita o agrupamento."
      },
      {
        "date": "2024-06-20T04:20:00.000Z",
        "voteCount": 1,
        "content": "ANSWER IS C:\n\nIn some cases, you might not want automatic grouping to occur, or you might want all rows to appear, including duplicates. In those cases, you can add an index field to your dataset that causes all rows to be considered unique and prevents grouping.\nThis means you need to add an index field column in which case each row will have a unique index value per row"
      },
      {
        "date": "2024-06-18T09:18:00.000Z",
        "voteCount": 1,
        "content": "C- ADIONAR UMA COLUA INDICE"
      },
      {
        "date": "2024-06-16T11:07:00.000Z",
        "voteCount": 2,
        "content": "as clearly stated here\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals"
      },
      {
        "date": "2024-06-13T08:13:00.000Z",
        "voteCount": 1,
        "content": "IMO key words in question are \"...the visual is grouped automatically...\" so reason while duplicate are NOT displayed is the visual, to revert simply change visual property"
      },
      {
        "date": "2024-06-12T00:17:00.000Z",
        "voteCount": 1,
        "content": "D. Modify the Summarize By property for all columns"
      },
      {
        "date": "2024-06-08T18:57:00.000Z",
        "voteCount": 2,
        "content": "Index column always guys..."
      },
      {
        "date": "2024-07-01T11:00:00.000Z",
        "voteCount": 1,
        "content": "Because index is never repeated right !"
      },
      {
        "date": "2024-05-27T10:40:00.000Z",
        "voteCount": 1,
        "content": "To ensure all rows appear in the Python visual in your Power BI report, you need to modify how the data is being summarized. Power BI often automatically groups data and displays unique rows based on the \"Summarize By\" property of the columns. If this property is set to summarize (like sum, average, etc.), it will group the data, and you may not see duplicate rows.\n\nTo display all rows, including duplicates, you should:\n\nD. Modify the Summarize By property for all columns"
      },
      {
        "date": "2024-05-13T15:50:00.000Z",
        "voteCount": 2,
        "content": "The default aggregation is Don't summarize. so no need for etting the \"Summarize By\" property to \"None\" for all columns , C is correct Answer"
      },
      {
        "date": "2024-05-08T10:05:00.000Z",
        "voteCount": 1,
        "content": "D is the most correct answer, A could be used but not as direct as D is. \nIn Power BI, the default behavior for visuals (including Python visuals) is to aggregate data. This means it groups by unique values and may omit duplicate rows. By changing the \"Summarize By\" property to \"Don't Summarize\" for the relevant columns, you instruct Power BI to send the raw, unaggregated data to the Python visual.\n\nWhy other options are less ideal:\n\nA. Reference the columns in the Python code by index: This could be a workaround, but it doesn't address the root cause of the issue (automatic aggregation) and makes your Python code potentially less readable.\nB. Modify the Sort Column By property for all columns: Sorting doesn't prevent aggregation and wouldn't influence the number of rows displayed.\nC. Add a unique field to each row: This is a valid solution but adds overhead to your data preparation. Changing the \"Summarize By\" property is likely a cleaner approach."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134064-exam-dp-600-topic-1-question-18-discussion/",
    "body": "DRAG DROP -<br>You have a Fabric tenant that contains a semantic model. The model contains data about retail stores.<br>You need to write a DAX query that will be executed by using the XMLA endpoint. The query must return a table of stores that have opened since December 1, 2023.<br>How should you complete the DAX expression? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image31\" src=\"https://img.examtopics.com/dp-600/image22.png\">",
    "options": [],
    "answer": "<img title=\"image32\" src=\"https://img.examtopics.com/dp-600/image23.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-17T08:17:00.000Z",
        "voteCount": 55,
        "content": "1) DEFINE\n2) EVALUATE\n3) SUMMARIZE"
      },
      {
        "date": "2024-03-20T04:40:00.000Z",
        "voteCount": 5,
        "content": "The VAR_SalesSince should have a space between them to be:\nVAR _SalesSince."
      },
      {
        "date": "2024-09-15T01:09:00.000Z",
        "voteCount": 1,
        "content": "I have tested it in dax.do changing fields and ans is DEFINE, EVALUATE and SUMMARIZE:\n\nDEFINE\n    VAR _SalesSince =\n        DATE ( 2000, 12, 01 )\n\nEVALUATE\nFILTER (\n    SUMMARIZE ( 'Product', 'Product'[Product Name], 'Product'[Available Date] ),\n    'Product'[Available Date] &gt;= _SalesSince\n)"
      },
      {
        "date": "2024-09-14T08:21:00.000Z",
        "voteCount": 1,
        "content": "DEFINE\nEVALUATE\nSUMMARIZE"
      },
      {
        "date": "2024-08-27T12:02:00.000Z",
        "voteCount": 1,
        "content": "Just to save people some time, I haven't seen a function called TABLE, so the last one is SUMMARIZE."
      },
      {
        "date": "2024-07-09T15:41:00.000Z",
        "voteCount": 1,
        "content": "The correct syntax is:\nDefine\nEvaluate\nTable\n\nhttps://learn.microsoft.com/en-us/dax/define-statement-dax\nhttps://learn.microsoft.com/en-us/dax/evaluate-statement-dax"
      },
      {
        "date": "2024-06-21T00:23:00.000Z",
        "voteCount": 2,
        "content": "Was in exam. Scored 95%\nChose\n1) DEFINE\n2) EVALUATE\n3) SUMMARIZE"
      },
      {
        "date": "2024-06-06T04:53:00.000Z",
        "voteCount": 2,
        "content": "DEFINE\nVAR _SalesSince = DATE(2023, 12, 01)\nEVALUATE\nFILTER(\n    SUMMARIZE(\n        Store, Store[Name], Store[OpenDate]\n    ),\n    Store[OpenDate] &gt;= _SalesSince\n)"
      },
      {
        "date": "2024-06-05T02:46:00.000Z",
        "voteCount": 1,
        "content": "Define, Evaluate,Summarize.\nOption Table does not exist"
      },
      {
        "date": "2024-06-04T03:56:00.000Z",
        "voteCount": 1,
        "content": "Why is it necessary to SUMMARIZE? the stores are supposed to have different names, wouldn't a FILRTER be enough?"
      },
      {
        "date": "2024-05-11T21:44:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/69047367/table-visual-is-unintuitively-aggregating-my-data\nRefer to this post. \nIt explains why the don't summarize option is not correct."
      },
      {
        "date": "2024-04-03T07:19:00.000Z",
        "voteCount": 3,
        "content": "1) DEFINE\n2) EVALUATE\n3) SUMMARIZE"
      },
      {
        "date": "2024-03-28T03:13:00.000Z",
        "voteCount": 2,
        "content": "Define, Evaluate and Summarize. Please, update the correct answer."
      },
      {
        "date": "2024-02-19T16:56:00.000Z",
        "voteCount": 4,
        "content": "DEFINE\nEVALUATE\nSUMMARIZE"
      },
      {
        "date": "2024-02-17T15:14:00.000Z",
        "voteCount": 4,
        "content": "Define - evaluate - summarize"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133523-exam-dp-600-topic-1-question-19-discussion/",
    "body": "You have a Fabric workspace named Workspace1 that contains a dataflow named Dataflow1. Dataflow1 has a query that returns 2,000 rows.<br>You view the query in Power Query as shown in the following exhibit.<br><img title=\"image33\" src=\"https://img.examtopics.com/dp-600/image24.png\"><br>What can you identify about the pickupLongitude column?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column has duplicate values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll the table rows are profiled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe column has missing values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are 935 values that occur only once."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-11T06:15:00.000Z",
        "voteCount": 23,
        "content": "Answer A.\n\nB - Not all the rows are profiled in the sample (only 1000 of 2000)\nC- From the column statistics, you don't have any missing values in the sample\nD- The values that occur only once are 871 (unique count)"
      },
      {
        "date": "2024-02-17T08:19:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2024-07-02T00:48:00.000Z",
        "voteCount": 2,
        "content": "Difficult to say if BCD are correct since we only see 1000 rows and not all columns. One thing is for sure though, there are columns that have values that occur more than once."
      },
      {
        "date": "2024-05-29T16:33:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\nDistinct mean : count all the values as 1, even if there was more than one.\nUnique mean : count only the value that are not repeated in the particular column"
      },
      {
        "date": "2024-05-07T18:13:00.000Z",
        "voteCount": 2,
        "content": "IMHO, it is A, well explained below"
      },
      {
        "date": "2024-03-27T13:15:00.000Z",
        "voteCount": 4,
        "content": "A\nWe see a count of 1000 (which is the limit by default) we do not know all the data is read, but we can see of the 1000 distinct is less and so we have duplicate values."
      },
      {
        "date": "2024-03-26T07:18:00.000Z",
        "voteCount": 2,
        "content": "A\nmy reasoning: every other answer option cannot be answered for sure since only 1000 values out of 2000 are profiled. -&gt; B: only 1000 rows are profiled -&gt; C: the column might have missings -&gt; D: there might be more unique/distinct counts."
      },
      {
        "date": "2024-03-20T04:44:00.000Z",
        "voteCount": 1,
        "content": "A is Best choice based on the picture."
      },
      {
        "date": "2024-02-17T15:17:00.000Z",
        "voteCount": 4,
        "content": "Its A\nOnly one column selected here\nNo informations about missing values\nDistinct count not mean exist only once"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133878-exam-dp-600-topic-1-question-20-discussion/",
    "body": "You have a Fabric tenant named Tenant1 that contains a workspace named WS1. WS1 uses a capacity named C1 and contains a dataset named DS1.<br>You need to ensure read-write access to DS1 is available by using XMLA endpoint.<br>What should be modified first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe DS1 settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe WS1 settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe C1 settings\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Tenant1 settings"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-20T04:54:00.000Z",
        "voteCount": 18,
        "content": "As XMLA is set to Read-Only first, you must go to the capacity settings to enable read-write.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#enable-xmla-read-write"
      },
      {
        "date": "2024-03-28T08:03:00.000Z",
        "voteCount": 8,
        "content": "The question concerns changing from read-only to read-write (in the Capacity settings), not about enabling XMLA endpoints (in the Tenant settings), which, as per the query, are already set up."
      },
      {
        "date": "2024-07-09T15:59:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is C\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#:~:text=To%20enable%20read,to%20the%20capacity."
      },
      {
        "date": "2024-06-19T13:45:00.000Z",
        "voteCount": 1,
        "content": "Enable XMLA read-write:\nSelect Settings &gt; Admin portal.\nIn the Admin portal, select Capacity settings &gt; Power BI Premium &gt; capacity name.\nExpand Workloads. In the XMLA Endpoint setting, select Read Write. The XMLA Endpoint setting applies to all workspaces and semantic models assigned to the capacity."
      },
      {
        "date": "2024-06-18T00:21:00.000Z",
        "voteCount": 1,
        "content": "The right answer is C,\nYou use tenant setting to Allow XMLA endpoints and Analyze in Excel with on-premises datasets. It is read only access but with Capacity settings you can choose between off, read only and read-write"
      },
      {
        "date": "2024-06-01T00:23:00.000Z",
        "voteCount": 3,
        "content": "The questions asked what should be modified FIRST. Therefore, I would go with D because C requires that D is activated. Although, I am not sure if D is not activated by default, thus making C the ONLY modification required.\nI hate it when MS exam questions leave room for interpretation. Why don't they ask for two answers...\n\nWeirdly enough Questions 21 covers basically the same topic and there people agree that Tenant settings are definitely required next to capacity settings."
      },
      {
        "date": "2024-08-27T12:05:00.000Z",
        "voteCount": 1,
        "content": "Those are other 2 settings in the Tenant settings are enabled by default. If it explicitly said XMLA Read/Write then I'd want to go with the Capacity settings like others have said as it seems the most applicable. I do get what you mean though, I was having the same doubts."
      },
      {
        "date": "2024-05-27T11:00:00.000Z",
        "voteCount": 1,
        "content": "To ensure read-write access to a dataset (DS1) using the XMLA endpoint in your Fabric tenant, you need to modify the settings that control XMLA endpoint access. This typically involves enabling read-write capabilities for the XMLA endpoint at the capacity level. Therefore, you should modify:\n\nC. the C1 settings"
      },
      {
        "date": "2024-05-07T18:27:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \n\nthe answer is C. As described here: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#enable-xmla-read-write\n\nin the section \"To enable read-write for a Premium capacity\""
      },
      {
        "date": "2024-04-19T07:48:00.000Z",
        "voteCount": 2,
        "content": "we need to first configure tenant settings:\nAllow XMLA endpoints and Analyze in Excel with on-premises semantic models:"
      },
      {
        "date": "2024-05-05T12:40:00.000Z",
        "voteCount": 2,
        "content": "Yes, but the question refers to enabling read-write to the capacity."
      },
      {
        "date": "2024-04-13T04:22:00.000Z",
        "voteCount": 1,
        "content": "To ensure read-write access to the dataset DS1 via the XMLA endpoint, you should modify the settings at the workspace level (WS1). Specifically, enable read-write operations for the XMLA endpoint within the workspace configuration."
      },
      {
        "date": "2024-05-05T12:36:00.000Z",
        "voteCount": 2,
        "content": "No. You are wrong."
      },
      {
        "date": "2024-03-12T03:11:00.000Z",
        "voteCount": 4,
        "content": "Again, bad wording in the question. If it is specifically about read-write, it's C. If it's about having XMLA endpoints available in the first place, it's D. Judging by Microsoft's wordings and ways of referring to their own documentation, I guess they want us to go for C."
      },
      {
        "date": "2024-03-09T12:43:00.000Z",
        "voteCount": 2,
        "content": "We need to configure several settings but in terms of hierarchy we need to first configure tenant settings:\nAllow XMLA endpoints and Analyze in Excel with on-premises semantic models: Users in the organization can use Excel to view and interact with on-premises Power BI semantic models. This also allows connections to XMLA endpoints.\nhttps://learn.microsoft.com/en-us/fabric/admin/tenant-settings-index"
      },
      {
        "date": "2024-02-26T09:05:00.000Z",
        "voteCount": 1,
        "content": "C is the right answer. The read/write permission is set in capacity settings in Admin Portal."
      },
      {
        "date": "2024-02-17T15:18:00.000Z",
        "voteCount": 1,
        "content": "First check on capacity level"
      },
      {
        "date": "2024-02-17T08:27:00.000Z",
        "voteCount": 3,
        "content": "C1 \nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools"
      },
      {
        "date": "2024-02-14T11:55:00.000Z",
        "voteCount": 2,
        "content": "C1 is Correct."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134090-exam-dp-600-topic-1-question-21-discussion/",
    "body": "You have a Fabric tenant that contains a workspace named Workspace1. Workspace1 is assigned to a Fabric capacity.<br>You need to recommend a solution to provide users with the ability to create and publish custom Direct Lake semantic models by using external tools. The solution must follow the principle of least privilege.<br>Which three actions in the Fabric Admin portal should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, set Allow XMLA Endpoints and Analyze in Excel with on-premises datasets to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, set Allow Azure Active Directory guest users to access Microsoft Fabric to Enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, select Users can edit data model in the Power BI service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Capacity settings, set XMLA Endpoint to Read Write.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, set Users can create Fabric items to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, enable Publish to Web."
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "ADF",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "ACD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T11:31:00.000Z",
        "voteCount": 22,
        "content": "D - The XMLA endpoint needs to be enabled from the capacity settings as it is crucial for allowing external tools to only read, but also, publish and manage custom Direct Lake semantic models.\n\nA - Again from Tenant settings, we need to enable XMLA endpoints ; This is to ensure that external tools interact with the data models as and when necessary within the tenant's workspace ; The analyze in excel is just complimentary to this setting in Fabric and is irrelevant\n\nE - Sets appropriate permissions to users by allowing them to edit models and publish them as when required abiding by the principle of least privilege.\n\nB&amp;C have concerns with the principle of least privilege and security concerns while publishing to the Web"
      },
      {
        "date": "2024-06-06T05:08:00.000Z",
        "voteCount": 7,
        "content": "A. Enabling XMLA Endpoints is crucial because it allows external tools (like SQL Server Management Studio, Tabular Editor, or any other tool that can connect via XMLA) to connect to Power BI datasets. This is essential for users who need to create, modify, or manage semantic models using these external tools. \nD. Setting the XMLA Endpoint to Read Write allows users not only to read the data from the Power BI service but also to write back to it. This is necessary for users to create and publish custom Direct Lake semantic models. Without write access, users would be unable to publish or update their models, which is a critical part of managing semantic models.\nE. Enabling users to create Fabric items is essential because it allows users to create new Power BI datasets, dataflows, and other necessary components within the workspace."
      },
      {
        "date": "2024-08-08T08:46:00.000Z",
        "voteCount": 1,
        "content": "C is not correct for those voting for it. Below is the (verbatim) description for the \"Users can edit data models in the Power BI service (preview)\" Tenant setting in the Admin portal. \n\n\"Turn on this setting to allow users to edit data models in the service. This setting DOESN'T apply to DirectLake semantic models or editing a semantic model through an API or XMLA endpoint.\"\n\nMore information on that Tenant setting from the link below ....\n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/service-edit-data-models#enabling-data-model-editing-in-the-admin-portal"
      },
      {
        "date": "2024-07-23T21:59:00.000Z",
        "voteCount": 1,
        "content": "C is wrong for 2 reasons, this setting is not in the tenant, it is in workspace settings. The second reason is that this feature is still in preview till date of my comment."
      },
      {
        "date": "2024-06-16T03:55:00.000Z",
        "voteCount": 2,
        "content": "ADE\nOPTION C should not be consider as :\nUsers can edit data models in the Power BI service (preview)\nEnabled for the entire organization\nTurn on this setting to allow users to edit data models in the service. This setting doesn't apply to DirectLake semantic models or editing a semantic model through an API or XMLA endpoint."
      },
      {
        "date": "2024-05-08T10:34:00.000Z",
        "voteCount": 2,
        "content": "ADE  \nB. From the Tenant settings, set Allow Azure Active Directory guest users to access Microsoft Fabric to Enabled. This focuses on external user access, which might not be necessary for your scenario.\n\nC. From the Tenant settings, select Users can edit data model in the Power BI service. This primarily affects in-browser editing of Power BI models and is less relevant to creating custom models with external tools.\n\nF. From the Tenant settings, enable Publish to Web. This is unrelated to external tool development and is concerned with sharing Power BI reports publicly."
      },
      {
        "date": "2024-05-07T19:05:00.000Z",
        "voteCount": 2,
        "content": "A -&gt; D -&gt; E"
      },
      {
        "date": "2024-04-24T14:05:00.000Z",
        "voteCount": 1,
        "content": "A. From the Tenant settings, set Allow XMLA Endpoints and Analyze in Excel with on-premises datasets to Enabled (https://learn.microsoft.com/en-us/fabric/admin/service-admin-portal-integration) . C. From the Tenant settings, select Users can edit data model in the Power BI service (https://learn.microsoft.com/en-us/fabric/admin/service-admin-portal-data-model) . D. From the Capacity settings, set XMLA Endpoint to Read Write (https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools) ."
      },
      {
        "date": "2024-06-22T18:15:00.000Z",
        "voteCount": 2,
        "content": "From the link you https://learn.microsoft.com/en-us/fabric/admin/service-admin-portal-data-model provided - It clearly says that \"This setting doesn't apply to DirectLake datasets or editing a dataset through an API or XMLA endpoint.\" so it is not C."
      },
      {
        "date": "2024-04-24T08:16:00.000Z",
        "voteCount": 2,
        "content": "AD for sure\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#security"
      },
      {
        "date": "2024-04-21T15:44:00.000Z",
        "voteCount": 2,
        "content": "A\tAllow XMLA Endpoints and Analyze in Excel with on-premises datasets:\n\tFrom the Tenant settings, set Allow XMLA Endpoints and Analyze in Excel with on-premises datasets to Enabled. This allows users to interact with the dataset via XMLA endpoints and analyze data using Excel.\nD.\tSet XMLA Endpoint to Read Write:\n\tIn the Capacity settings, configure the XMLA Endpoint to Read Write. This ensures that users have the necessary permissions to create and modify semantic models through external tools.\nE\tEnable Users to Create Fabric Items:\n\tFrom the Tenant settings, set Users can create Fabric items to Enabled. This grants users the ability to create custom semantic models within the Fabric workspace."
      },
      {
        "date": "2024-04-19T18:37:00.000Z",
        "voteCount": 3,
        "content": "sraakesh95 Highly Voted  1 month, 3 weeks ago\nD - The XMLA endpoint needs to be enabled from the capacity settings as it is crucial for allowing external tools to only read, but also, publish and manage custom Direct Lake semantic models.\n\nA - Again from Tenant settings, we need to enable XMLA endpoints ; This is to ensure that external tools interact with the data models as and when necessary within the tenant's workspace ; The analyze in excel is just complimentary to this setting in Fabric and is irrelevant\n\nE - Sets appropriate permissions to users by allowing them to edit models and publish them as when required abiding by the principle of least privilege.\n\nB&amp;C have concerns with the principle of least privilege and security concerns while publishing to the Web\n   upvoted 8 times\n\nJUST TO VOTE GUYS"
      },
      {
        "date": "2024-04-16T13:15:00.000Z",
        "voteCount": 1,
        "content": "Not C : https://learn.microsoft.com/en-us/power-bi/transform-model/service-edit-data-models#enabling-data-model-editing-in-the-admin-portal"
      },
      {
        "date": "2024-03-20T05:23:00.000Z",
        "voteCount": 1,
        "content": "ADE seems most appropriate"
      },
      {
        "date": "2024-03-19T13:35:00.000Z",
        "voteCount": 2,
        "content": "Most probably ADE, I put E because users have the necessary permissions to create custom Direct Lake semantic models within the Fabric workspace."
      },
      {
        "date": "2024-03-09T16:25:00.000Z",
        "voteCount": 3,
        "content": "ADF seems correct answer"
      },
      {
        "date": "2024-03-09T12:38:00.000Z",
        "voteCount": 3,
        "content": "It's ADE\nfrom https://learn.microsoft.com/en-us/fabric/admin/tenant-settings-index\nA: We need this to connect to XMLA endpoint.\nD: Obviously this is required for Read Write access throufg XMLA endpoint.\nE: This is required to create a direct lake semantic model which is a fabrci item.\n\nC:from link above: \"This setting doesn't apply to DirectLake semantic models or editing a semantic model through an API or XMLA endpoint\"\nB: This setting allows guest users to access our tenant, it's Irrelevant.\nF: This is Irrelevant too, it allows us to publish a report to web."
      },
      {
        "date": "2024-02-28T04:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is ADF\nCan't be C because the portal says \"This setting doesn't apply to DirectLake semantic models or editing a semantic model through an API or XMLA endpoint\""
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133461-exam-dp-600-topic-1-question-22-discussion/",
    "body": "You are creating a semantic model in Microsoft Power BI Desktop.<br>You plan to make bulk changes to the model by using the Tabular Model Definition Language (TMDL) extension for Microsoft Visual Studio Code.<br>You need to save the semantic model to a file.<br>Which file format should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIP\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIX",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIDS"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-19T10:35:00.000Z",
        "voteCount": 19,
        "content": "The correct option is PBIP - https://powerbiblogscdn.azureedge.net/wp-content/uploads/2024/02/tmdlPreviewFeature.png"
      },
      {
        "date": "2024-05-26T16:04:00.000Z",
        "voteCount": 11,
        "content": "The correct file format for this purpose is:\n\nA. PBIP\n\nExplanation:\n\nPBIP (Power BI Project): This format is specifically designed for managing Power BI projects, including semantic models, in a way that supports bulk editing and version control. It allows you to use tools like Visual Studio Code to edit the model's metadata directly.\nOther formats:\n\nPBIX (Power BI Desktop File): This is the standard file format for Power BI Desktop reports, but it is not designed for direct bulk editing using TMDL.\nPBIT (Power BI Template File): This format is used for Power BI templates, which are useful for creating new reports based on a predefined structure, but it is not suitable for bulk editing using TMDL.\nPBIDS (Power BI Data Source File): This format is used for defining data sources for Power BI reports, not for semantic models."
      },
      {
        "date": "2024-09-16T21:50:00.000Z",
        "voteCount": 1,
        "content": "To make bulk changes using the Tabular Model Definition Language (TMDL) extension is A. PBIP.\n\nExplanation:\n\n    PBIP (Power BI Project) is a file format that supports the open-source TMDL format and is designed for integrating Power BI with external development environments like Visual Studio Code.\n    PBIX is the common Power BI report file format but is not intended for bulk edits through TMDL.\n    PBIT is a Power BI template file, used for creating new reports based on an existing structure but not for bulk editing in Visual Studio Code.\n    PBIDS is for creating Power BI dataset connections and is unrelated to TMDL editing."
      },
      {
        "date": "2024-06-08T19:03:00.000Z",
        "voteCount": 2,
        "content": "PIBP is the correct one"
      },
      {
        "date": "2024-05-31T11:06:00.000Z",
        "voteCount": 1,
        "content": "PBIP for source code"
      },
      {
        "date": "2024-05-19T13:55:00.000Z",
        "voteCount": 1,
        "content": "https://powerbi.microsoft.com/en-us/blog/tmdl-in-power-bi-desktop-developer-mode-preview/"
      },
      {
        "date": "2024-05-19T01:13:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview"
      },
      {
        "date": "2024-05-08T13:37:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \nI go with \"A\".\n\nHere: https://powerbi.microsoft.com/en-us/blog/tmdl-in-power-bi-desktop-developer-mode-preview/\n\"\"\"\nSaving as a PBIP using TMDL is currently in preview. Before giving it a try, you must first enable this feature in Preview features: go to File &gt; Options and settings &gt; Options &gt; Preview features and check the box next to \u201cStore semantic model using TMDL format\u201d.\n\"\"\""
      },
      {
        "date": "2024-05-08T10:41:00.000Z",
        "voteCount": 1,
        "content": "C - PBIT\nHere's why:\n\nPBIT: The PBIT format is designed specifically for storing the semantic model definition in a way that is compatible with the Tabular Model Definition Language (TMDL) and readily editable in Visual Studio Code using the TMDL extension.\nLet's clarify the other options:\n\nPBIP, PBIX: These are standard Power BI Desktop file formats. They contain the semantic model but also include data, reports, visualizations, and other elements of a Power BI project.\nPBIDS: This file format is related to Power BI datasets hosted in the Power BI service, not for local editing with TMDL."
      },
      {
        "date": "2024-05-08T03:14:00.000Z",
        "voteCount": 1,
        "content": "But it is the semantic model we are focusing on here? \nPBIX may be correct then? \nhttps://learn.microsoft.com/en-us/power-bi/create-reports/service-export-to-pbix#download-a-pbix-file-from-a-semantic-model"
      },
      {
        "date": "2024-03-20T05:26:00.000Z",
        "voteCount": 3,
        "content": "A https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview"
      },
      {
        "date": "2024-02-27T11:36:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview\n\nCheckout the title in the above link: Programmatic generation and editing artifact definitions"
      },
      {
        "date": "2024-02-27T04:55:00.000Z",
        "voteCount": 1,
        "content": "Also think it's A: https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview"
      },
      {
        "date": "2024-02-20T04:20:00.000Z",
        "voteCount": 3,
        "content": "The answer is A\nThe PBIP will create one file and two folders, PBIP.Dataset contains definition folder that is use to host the .tmdl files"
      },
      {
        "date": "2024-02-19T03:40:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-templates"
      },
      {
        "date": "2024-02-17T15:35:00.000Z",
        "voteCount": 2,
        "content": "its A.\nPbit iS only template.\nFor source control use pbip it also generate bim file for TMDL."
      },
      {
        "date": "2024-02-09T11:19:00.000Z",
        "voteCount": 4,
        "content": "\".pbit\" file is a Power BI Desktop template file. It includes both the data model and any reports or visuals you've created."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134091-exam-dp-600-topic-1-question-23-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant that contains a warehouse named Warehouse1. Warehouse1 contains three schemas named schemaA, schemaB, and schemaC.<br>You need to ensure that a user named User1 can truncate tables in schemaA only.<br>How should you complete the T-SQL statement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image36\" src=\"https://img.examtopics.com/dp-600/image27.png\">",
    "options": [],
    "answer": "<img title=\"image37\" src=\"https://img.examtopics.com/dp-600/image28.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-25T06:42:00.000Z",
        "voteCount": 24,
        "content": "ALTER/SCHEMA \nThis statement allows to alter (which includes truncating) tables within the specified schema. It ensures that the permission is restricted to schemaA and does not grant access to other schemas or objects."
      },
      {
        "date": "2024-06-03T05:50:00.000Z",
        "voteCount": 5,
        "content": "truncating tables is not even supported in warehouse so this question is pointless, good job microsoft..."
      },
      {
        "date": "2024-09-16T22:25:00.000Z",
        "voteCount": 3,
        "content": "GRANT ALTER ON SCHEMA::schemaA TO User1;"
      },
      {
        "date": "2024-06-12T04:42:00.000Z",
        "voteCount": 1,
        "content": "ALTER/SCHEMA\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-ver16#permissions\nhttps://learn.microsoft.com/th-th/sql/t-sql/statements/alter-schema-transact-sql?view=fabric&amp;preserve-view=true"
      },
      {
        "date": "2024-05-08T13:45:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \nALTER -&gt; SCHEMA.\n\nAs well said below, \n1) ALTER is the thing to have for truncating:\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-ver16&amp;viewFallbackFrom=fabric\n2) SCHEMA:schema_name to define a particular schema for tables"
      },
      {
        "date": "2024-04-11T07:48:00.000Z",
        "voteCount": 3,
        "content": "ALTER/SCHEMA\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/grant-schema-permissions-transact-sql?view=sql-server-ver16\n\nGRANT permission  [ ,...n ] ON SCHEMA :: schema_name  \n    TO database_principal [ ,...n ]  \n    [ WITH GRANT OPTION ]  \n    [ AS granting_principal ]"
      },
      {
        "date": "2024-03-20T13:31:00.000Z",
        "voteCount": 2,
        "content": "According to https://learn.microsoft.com/en-us/sql/t-sql/statements/grant-object-permissions-transact-sql?view=sql-server-ver16, answer seems to ALTER and OBJECT."
      },
      {
        "date": "2024-06-22T18:35:00.000Z",
        "voteCount": 1,
        "content": "It would have worked if the question said one table.. But it says \"user named User1 can truncate  TABLE\"S\" in schemaA only\"  so I would go with option C. SCHEMA."
      },
      {
        "date": "2024-03-21T03:21:00.000Z",
        "voteCount": 2,
        "content": "For schemas:\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/grant-schema-permissions-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-02-29T20:41:00.000Z",
        "voteCount": 2,
        "content": "ALTER is DDL not DML so the right answer is EXECUTE SCHEMA."
      },
      {
        "date": "2024-05-31T11:09:00.000Z",
        "voteCount": 2,
        "content": "TRUNCATE is DML"
      },
      {
        "date": "2024-05-31T11:09:00.000Z",
        "voteCount": 1,
        "content": "Ignore my prevous comment.\nTRUNCATE is DDL so ALTER is what we need"
      },
      {
        "date": "2024-03-06T02:39:00.000Z",
        "voteCount": 3,
        "content": "ALTER is required for Truncate https://learn.microsoft.com/en-us/sql/t-sql/statements/truncate-table-transact-sql?view=sql-server-ver16&amp;viewFallbackFrom=fabric"
      },
      {
        "date": "2024-02-17T15:36:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134093-exam-dp-600-topic-1-question-24-discussion/",
    "body": "You plan to deploy Microsoft Power BI items by using Fabric deployment pipelines. You have a deployment pipeline that contains three stages named Development, Test, and Production. A workspace is assigned to each stage.<br>You need to provide Power BI developers with access to the pipeline. The solution must meet the following requirements:<br>Ensure that the developers can deploy items to the workspaces for Development and Test.<br>Prevent the developers from deploying items to the workspace for Production.<br>Follow the principle of least privilege.<br>Which three levels of access should you assign to the developers? Each correct answer presents part of the solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild permission to the production semantic models",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdmin access to the deployment pipeline\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tViewer access to the Development and Test workspaces",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tViewer access to the Production workspace\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContributor access to the Development and Test workspaces\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContributor access to the Production workspace"
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "CDE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ADE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T11:48:00.000Z",
        "voteCount": 17,
        "content": "As pointed out by XiltroX,\nB - Admin access is provided to the developers for the developers to manage the deployent process across the various stages (in this case Dev and Test). This is a basic necessary.\n\nD - To restrict the access on the Production workspace, provide an overriding Viewer access which lets the developers only view the Production environment and not make any changes.\n\nE - This is to provide the developers with the permissions to develop, edit and update the Dev and Test pipelines."
      },
      {
        "date": "2024-09-17T00:27:00.000Z",
        "voteCount": 1,
        "content": "To meet the requirements while following the principle of least privilege, you should assign the following levels of access to the developers:\n\n    B. Admin access to the deployment pipeline - This is needed so developers can manage the deployment pipeline itself, including moving items between stages.\n\n    E. Contributor access to the Development and Test workspaces - This allows developers to deploy and make changes to the items in the Development and Test workspaces.\n\n    D. Viewer access to the Production workspace - This ensures developers can view the Production workspace but cannot make any changes or deploy items to it."
      },
      {
        "date": "2024-08-27T12:11:00.000Z",
        "voteCount": 1,
        "content": "B - There's only one role for pipelines so it's either having an Admin role or no access to a pipeline. You have to have an Admin role on a pipeline in order to do any deployments.\n\nD - Pipeline permissions and workspace permissions are kind of linked but the way it works is if a user has viewer permissions on a workspace and Admin on the Pipeline - They won't be able to do a deployment to that workspace, but they can still see things inside of it (as is the case as a viewer in any workspace).\n\nE - If you have the Contributer/Member/Admin role on a workspace AND Admin permissions on a pipeline, then you can deploy to that workspace."
      },
      {
        "date": "2024-08-27T12:12:00.000Z",
        "voteCount": 1,
        "content": "One common misconception is people are thinking (and I initially thought) that D is selected to limit the user. This isn't the case. The workspace roles are separate and if you don't give any role to the Prod workspace, then the user won't be able to access it at all. By giving viewer, they can atleast see what's in the workspace."
      },
      {
        "date": "2024-08-25T17:44:00.000Z",
        "voteCount": 1,
        "content": "It does not say Developers need to manage deployments, just access items within the pipeline.  So A, D, and E"
      },
      {
        "date": "2024-07-17T13:08:00.000Z",
        "voteCount": 1,
        "content": "A D E because: In Power BI, having Viewer access to a workspace does not automatically grant Build permission for a semantic model within that workspace. The Viewer role allows a user to view and interact with items in the workspace, but it does not include the ability to create new content or edit existing content.\n\nTo have Build permission, a user must be explicitly granted that permission, which allows them to build new content from the semantic model, access reports that use composite models on Power BI Pro workspaces, and pull data into Analyze in Excel, among other capabilities. This permission can be given by the workspace Admin or Member who has the authority to manage semantic model permissions"
      },
      {
        "date": "2024-06-23T17:53:00.000Z",
        "voteCount": 1,
        "content": "B: It's admin or nothing for pipeline access.\nD: Viewer, so they cannot create it in production.\nE: You need Contributor on the workspaces you want to deploy and create."
      },
      {
        "date": "2024-06-21T08:16:00.000Z",
        "voteCount": 1,
        "content": "Respostas  BDE"
      },
      {
        "date": "2024-06-21T00:25:00.000Z",
        "voteCount": 1,
        "content": "Was in exam. Scored 95%\nChose BDE \n\nAnswers swtichted Contributor for Member role"
      },
      {
        "date": "2024-06-20T21:36:00.000Z",
        "voteCount": 1,
        "content": "I agree with BDE"
      },
      {
        "date": "2024-05-27T11:23:00.000Z",
        "voteCount": 2,
        "content": "E. Contributor access to the Development and Test workspaces:\nThis allows developers to deploy and manage content in the Development and Test workspaces, meeting the requirement to allow deployments in these stages.\n\nD. Viewer access to the Production workspace: \nThis provides developers with read-only access to the Production workspace, ensuring they can view content but cannot deploy or make changes, which aligns with the requirement to prevent deployments to Production.\n\nB. Admin access to the deployment pipeline:\nThis allows developers to manage the deployment pipeline itself, including deploying items to the Development and Test stages but not to the Production stage. This ensures they can oversee the deployment process without overstepping into the Production environment."
      },
      {
        "date": "2024-05-19T01:21:00.000Z",
        "voteCount": 2,
        "content": "Admin access is needed in the development workspace for pipeline."
      },
      {
        "date": "2024-05-09T02:53:00.000Z",
        "voteCount": 1,
        "content": "To deploy from one stage to another in the pipeline, you must be a pipeline admin, and either a member or an admin of the workspaces assigned to the stages involved. For example, a pipeline admin that isn't assigned a workspace role, can view the pipeline and share it with others. However, this user can't view the content of the workspace in the pipeline, or in the service, and can't perform deployments.\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process#permissions/?azure-portal=true\n\nFrom task: Prevent the developers from deploying to the production.\n\nSo B is not correct, we can't give pipeline admin access?"
      },
      {
        "date": "2024-05-15T07:15:00.000Z",
        "voteCount": 2,
        "content": "we can and wee need viewer access to the Production, so that developers can't deploy at this stage. Without admin access they can't deploy from dev to test stages."
      },
      {
        "date": "2024-05-08T13:58:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \nB-&gt;D-&gt;E\n\nB. Admin access to the deployment pipeline - to be able to run\nD. Viewer access to the Production workspace - to not be able to run Prod\nE. Contributor access to the Development and Test workspaces - to be able to run Dev/Test"
      },
      {
        "date": "2024-05-15T07:17:00.000Z",
        "voteCount": 3,
        "content": "I see that contributors aren't able to deploy. Only members\n\nWorkspace contributor\n(and pipeline admin)\t\nConsume content\u200b\nCompare stages\nView semantic models\nUnassign a workspace from a stage\n\nWorkspace member\n(and pipeline admin)\t\nView workspace content\u200b\nCompare stages\nDeploy items (must be a member or admin of both source and target workspaces)\nUpdate semantic models\nUnassign a workspace from a stage\nConfigure semantic model rules (you must be the semantic model owner)\n\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process#permissions-table"
      },
      {
        "date": "2024-07-27T03:02:00.000Z",
        "voteCount": 1,
        "content": "So what does this mean referring to the ausw\u00e4rts? Do we have 3 correct answers at all? Obviously ist's Not Contributor to Test (Contributor to Dev would be enough because you only want to deploy FROM Dev but not TO Dev)."
      },
      {
        "date": "2024-05-15T00:56:00.000Z",
        "voteCount": 1,
        "content": "I like your answers"
      },
      {
        "date": "2024-04-24T09:47:00.000Z",
        "voteCount": 2,
        "content": "Member or Admin rights on the workspace level are required to deploy datasets so E doesn't match the requirements.\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process#permissions-table"
      },
      {
        "date": "2024-03-26T07:44:00.000Z",
        "voteCount": 2,
        "content": "my reasoning:\nA: nope since they should not be able to build things there\nB: not sure\nC: nope since this would prevent them from deploying stuff there\nD: yes because of least privilege\nE: yes so they can build/deploy stuff there\nF: nope. Too much permissions.\n--&gt; so this leaves option B as the third answer."
      },
      {
        "date": "2024-03-28T08:13:00.000Z",
        "voteCount": 1,
        "content": "yes, admin permission on the pipeline is required to manage the deployments between workspaces\n\nso BDE confirmed"
      },
      {
        "date": "2024-03-20T10:32:00.000Z",
        "voteCount": 1,
        "content": "I think it is B,D,E, but the admin title throws you off as it can deploy items and is a key to what it asks. Build permission allows to create new content in the workspace, so not sure that is right, answer A.\nhttps://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/understand-the-deployment-process?WT.mc_id=access_pane#permissions"
      },
      {
        "date": "2024-02-27T05:00:00.000Z",
        "voteCount": 3,
        "content": "I get a feeling the \"trick\" is making you think that B would allow publishing on Prod, but I think if you don't give them access to the workspace, they can indeed have admin rights on the build pipeline but not deploy on Prod? So BDE would be my guess. Also B is the only option granting access to the pipeline at all, and D and E are a must."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133635-exam-dp-600-topic-1-question-25-discussion/",
    "body": "You have a Fabric workspace that contains a DirectQuery semantic model. The model queries a data source that has 500 million rows.<br>You have a Microsoft Power Bi report named Report1 that uses the model. Report1 contains visuals on multiple pages.<br>You need to reduce the query execution time for the visuals on all the pages.<br>What are two features that you can use? Each correct answer presents a complete solution,<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuser-defined aggregations\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tautomatic aggregation\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tquery caching",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOneLake integration"
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T11:59:00.000Z",
        "voteCount": 11,
        "content": "Agree with lengzhai's reference of the 2 links:\nA - Custom aggregations enables PBI to not perform a Full Scan of the underlying datasets.\nB - The AutoAggregations feature automatically creates aggregations on large datasets and based on query optimization determines the total number of rows that requires processing based on the generated query plan.\n\nIncorrect to this question context:\nC - Although caching helps improve performance on large datasets, it doesn't support DirectQuery (Important note in https://learn.microsoft.com/en-us/power-bi/connect-data/power-bi-query-caching) ; Also, it is a feature available in PBI Service that is automatic and needs no intervention from the user."
      },
      {
        "date": "2024-02-16T17:39:00.000Z",
        "voteCount": 10,
        "content": "D: onelake integration not for Direct Query\nC: only at loading for first page\n\nSo AV"
      },
      {
        "date": "2024-02-18T06:53:00.000Z",
        "voteCount": 3,
        "content": "Agreed to AB.\nBoth UDA's and AA optimize direct query performance. One just requires more manual work and in depth knowledge data modelling and query optimization techniques (UDA), whereas the other makes simplifies this process through the use of ML algorithms (AA)."
      },
      {
        "date": "2024-02-16T17:40:00.000Z",
        "voteCount": 1,
        "content": "I mean AB*"
      },
      {
        "date": "2024-07-09T17:48:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D are correct.\nDirect Lakes are great for performance in the OneLake integration \nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview"
      },
      {
        "date": "2024-10-16T22:35:00.000Z",
        "voteCount": 1,
        "content": "This was DirectQuery, not Direct Lake."
      },
      {
        "date": "2024-05-27T11:28:00.000Z",
        "voteCount": 1,
        "content": "A&amp;B\nWhile query caching can be beneficial in certain scenarios, user-defined aggregations and automatic aggregations are typically more effective for improving query performance in Power BI reports with large datasets and complex queries. These methods reduce the volume of data processed in real-time queries, directly addressing the performance bottlenecks associated with querying large datasets."
      },
      {
        "date": "2024-05-17T08:33:00.000Z",
        "voteCount": 1,
        "content": "CHATGPT saya AC"
      },
      {
        "date": "2024-05-08T14:04:00.000Z",
        "voteCount": 1,
        "content": "IMHO, A &amp; B looks good"
      },
      {
        "date": "2024-02-26T07:33:00.000Z",
        "voteCount": 4,
        "content": "Agree with A B\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/aggregations-advanced\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/aggregations-auto"
      },
      {
        "date": "2024-02-25T08:16:00.000Z",
        "voteCount": 2,
        "content": "Agreed AB. \nAlthough query caching (C) will reduce query execution time too, you risk outdated cached results when working with real-time or dynamic data."
      },
      {
        "date": "2024-02-13T01:17:00.000Z",
        "voteCount": 4,
        "content": "A. User-defined aggregations (UDAs) allow you to pre-aggregate specific calculations directly in the semantic model. This reduces the amount of data that needs to be retrieved from the source each time a visual requires the calculation, significantly improving query execution time.\nC. Power BI Desktop enables query caching for DirectQuery models. This stores frequently used queries on the client machine, eliminating the need to re-send them to the source data for subsequent interactions."
      },
      {
        "date": "2024-03-28T08:24:00.000Z",
        "voteCount": 2,
        "content": "Question is about Fabric workspace... not Power BI Desktop !"
      },
      {
        "date": "2024-02-12T10:26:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/aggregations-auto\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/onelake-integration-overview"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133726-exam-dp-600-topic-1-question-26-discussion/",
    "body": "You have a Fabric tenant that contains 30 CSV files in OneLake. The files are updated daily.<br>You create a Microsoft Power BI semantic model named Model1 that uses the CSV files as a data source. You configure incremental refresh for Model1 and publish the model to a Premium capacity in the Fabric tenant.<br>When you initiate a refresh of Model1, the refresh fails after running out of resources.<br>What is a possible cause of the failure?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery folding is occurring.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly refresh complete days is selected.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tXMLA Endpoint is set to Read Only.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery folding is NOT occurring.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe delta type of the column used to partition the data has changed."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "E",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-27T11:35:00.000Z",
        "voteCount": 10,
        "content": "D. Query folding is NOT occurring.\nQuery folding refers to the ability of Power Query to push data transformation logic back to the data source, which can perform the transformations more efficiently. When query folding does not occur, all the data is pulled into Power BI and transformations are applied locally, which can be resource-intensive and lead to running out of resources, especially with large datasets like your 30 CSV files.\nE. The delta type of the column used to partition the data has changed: While this could cause issues with incremental refresh accuracy, it would not typically result in \"running out of resources\" during the refresh."
      },
      {
        "date": "2024-03-26T07:55:00.000Z",
        "voteCount": 10,
        "content": "i don't really know.\nBut A and D regarding query folding seem for me not to be relevant since query folding does not make sense anyway on CSV/Flatfile connections. In my understanding, query folding sends a query back to the source. And what kind of query would that be to a CSV source?"
      },
      {
        "date": "2024-06-04T12:19:00.000Z",
        "voteCount": 2,
        "content": "Don't think so too\nhttps://learn.microsoft.com/en-us/power-query/query-folding-examples#no-query-folding-example"
      },
      {
        "date": "2024-06-21T00:29:00.000Z",
        "voteCount": 4,
        "content": "Was in exam. Scored 95%\nChose D.\n\nHonestly, I was guessing. No clue. However, query folding does occur for CSV files stored IN OneLake. OneLake does the work. This different from semantic models created with files stored on a normal machine and what had been common knowledge for a Power BI user."
      },
      {
        "date": "2024-05-10T10:56:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2024-05-09T19:52:00.000Z",
        "voteCount": 4,
        "content": "For efficient data processing, Power BI aims to push as much of the filtering and calculations as possible to the source system (OneLake in this case). This is called query folding. When query folding fails, Power BI needs to pull all the raw data into the semantic model and perform operations there, increasing memory and processing strain.\n\nE This could lead to refresh errors but is less likely to cause the specific behavior of running out of resources."
      },
      {
        "date": "2024-05-08T14:13:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \n\nThe answer is D.\nLink: https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-troubleshoot#cause-the-data-source-doesnt-support-query-folding\n\nCause: Data source queries aren't being folded\nWhile problems with query folding can usually be determined in Power BI Desktop before publishing to the service, it's possible that model refresh queries aren't being folded, leading to excessive refresh times and query mashup engine resource utilization. This situation happens because a query is created for every partition in the model. If the queries aren't being folded, and data isn't being filtered at the data source, the engine then attempts to filter the data."
      },
      {
        "date": "2024-05-07T00:36:00.000Z",
        "voteCount": 1,
        "content": "D. Query folding is NOT occurring.\nAs described in Incremental refresh and real-time data for models - Requirements, incremental refresh is designed for data sources that support query folding. Make sure data source queries are being folded in Power BI Desktop before publishing to the service, where query folding issues can be significantly compounded.\nSo having said this, csv files is not a data source that support query folding.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-troubleshoot#cause-the-data-source-doesnt-support-query-folding"
      },
      {
        "date": "2024-05-04T11:42:00.000Z",
        "voteCount": 1,
        "content": "If you connect power bi to the datasource through the SQL endpoint , you basically use as if it was a SQL server. Therefore, you might be able to use query folding. Am I correct ?"
      },
      {
        "date": "2024-04-24T05:10:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: \"E. The delta type of the column used to partition the data has changed.\"\n\nExplanation:\nQuery folding is not applicable with csv files, which rules out A,D answers.\nThe provided Microsoft link related to \"problem-loading-data-takes-too-long\", states two causes, one related to query folding (we've already ruled it out) and another one related to the data type, which in turn leads us to answer E."
      },
      {
        "date": "2024-04-27T09:33:00.000Z",
        "voteCount": 1,
        "content": "But DELTA type? I have not found anything related to that concept..."
      },
      {
        "date": "2024-02-27T12:12:00.000Z",
        "voteCount": 2,
        "content": "Combining the references by XiltroX and Momoanwar:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-troubleshoot#problem-loading-data-takes-too-long\nhttps://learn.microsoft.com/en-us/power-query/power-query-folding"
      },
      {
        "date": "2024-04-08T12:22:00.000Z",
        "voteCount": 1,
        "content": "as per the documentation: \nhttps://learn.microsoft.com/en-us/power-query/query-folding-examples#no-query-folding-example\n\n\"Queries that rely solely on unstructured data sources or that don't have a compute engine, such as CSV or Excel files, don't have query folding capabilities. This means that Power Query evaluates all the required data transformations using the Power Query engine.\"\n\nso not sure what this has to do with query folding at all"
      },
      {
        "date": "2024-02-26T09:36:00.000Z",
        "voteCount": 1,
        "content": "D is the right choice. Here's why: \nhttps://learn.microsoft.com/en-us/power-query/power-query-folding"
      },
      {
        "date": "2024-02-25T08:50:00.000Z",
        "voteCount": 2,
        "content": "Without considering external tooling (C), without further context I can already identify B, D and E as possible causes. Question should be: What could NOT be a cause?\nI add answer B to this: Resource exhaustion due to partial-day refresh, when Only refresh complete days is selected while configuring incremental refresh."
      },
      {
        "date": "2024-02-16T18:02:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-troubleshoot#problem-loading-data-takes-too-long"
      },
      {
        "date": "2024-02-13T12:53:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-troubleshoot#problem-loading-data-takes-too-long"
      },
      {
        "date": "2024-02-13T01:23:00.000Z",
        "voteCount": 2,
        "content": "C. XMLA Endpoint is set to Read Only: If the XMLA endpoint for the Premium capacity is set to Read Only, any attempt to update or refresh the model through this endpoint, including incremental refresh, would fail. This configuration directly explains the resource exhaustion during a refresh operation as the read-only mode wouldn't allow the necessary updates to occur."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133645-exam-dp-600-topic-1-question-27-discussion/",
    "body": "You have a Fabric tenant that uses a Microsoft Power BI Premium capacity.<br>You need to enable scale-out for a semantic model.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt the semantic model level, set Large dataset storage format to Off.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt the tenant level, set Create and use Metrics to Enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt the semantic model level, set Large dataset storage format to On.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAt the tenant level, set Data Activator to Enabled."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-12T11:32:00.000Z",
        "voteCount": 10,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-scale-out-configure"
      },
      {
        "date": "2024-06-10T03:03:00.000Z",
        "voteCount": 2,
        "content": "Defini...defini...definitively is C"
      },
      {
        "date": "2024-02-17T16:02:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133464-exam-dp-600-topic-1-question-28-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse. The warehouse uses row-level security (RLS).<br>You create a Direct Lake semantic model that uses the Delta tables and RLS of the warehouse.<br>When users interact with a report built from the model, which mode will be used by the DAX queries?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirect Lake\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T09:26:00.000Z",
        "voteCount": 39,
        "content": "A. Direct Query \"Row-level security only applies to queries on a Warehouse or SQL analytics endpoint in Fabric. Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security.\"\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security"
      },
      {
        "date": "2024-08-09T05:22:00.000Z",
        "voteCount": 2,
        "content": "Confirm:\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\n\"Queries using row-level security against tables in the warehouse (including the Lakehouse SQL analytics endpoint) will fall back to DirectQuery mode.\""
      },
      {
        "date": "2024-04-27T17:37:00.000Z",
        "voteCount": 18,
        "content": "Direct lake . When users interact with a report built from a Direct Lake semantic model that uses Delta tables and RLS of the warehouse, the DAX queries will operate in Direct Lake mode. This mode is specifically designed for analyzing large data volumes in Power BI and is based on loading parquet-formatted files directly from a data lake without querying a Lakehouse or Warehouse endpoint. Unlike DirectQuery, there is no translation from DAX to other query languages, and it does not execute queries on other database systems. This results in performance similar to import mode, with the added benefit of picking up any changes at the data source as they occur12.\n\nDirect Lake mode supports row-level security (RLS), ensuring that users only see the data they have permission to view. It combines the advantages of both DirectQuery and import modes while avoiding their disadvantages, making it an ideal choice for very large models and models with frequent updates at the data source2."
      },
      {
        "date": "2024-07-04T03:50:00.000Z",
        "voteCount": 2,
        "content": "Because it says \"RLS of the warehouse\" I'd be more likely to go for DirectQuery than Direct Lake"
      },
      {
        "date": "2024-06-21T04:34:00.000Z",
        "voteCount": 1,
        "content": "A. Direct lake - Row-level security only applies to queries on a Warehouse or SQL analytics endpoint in Fabric. Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security."
      },
      {
        "date": "2024-06-17T03:23:00.000Z",
        "voteCount": 1,
        "content": "Its A.\n\nBased on https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security"
      },
      {
        "date": "2024-06-14T17:55:00.000Z",
        "voteCount": 4,
        "content": "Both DirectQuery and DirectLake support RLS.The key word here is that tenant already had a Semantic Warehouse built.To query that one you need to use DirectQuery.Therefore the answer is A.But for the real user they can use DirectLake as well hear sinc eboth of them support RLS.Answer is A for this question.Thank you."
      },
      {
        "date": "2024-06-10T04:32:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: D\nD. https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\nOn the other hand, with import mode, performance can be better because the data is cached and optimized for DAX and MDX report queries without having to translate and pass SQL or other types of queries to the data source."
      },
      {
        "date": "2024-06-10T03:23:00.000Z",
        "voteCount": 1,
        "content": "c in my opinion"
      },
      {
        "date": "2024-06-01T04:54:00.000Z",
        "voteCount": 6,
        "content": "C. https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview\n\"Before using Direct Lake, you must provision a lakehouse (or a warehouse) with one or more Delta tables in a workspace...Direct Lake also supports row-level security and object-level security so users\"\n\nAnd it is Fabric exam, so the obvious correct answers is a Fabric feature."
      },
      {
        "date": "2024-05-27T11:41:00.000Z",
        "voteCount": 1,
        "content": "Direct Lake mode is a new feature in Power BI that allows direct querying of Delta tables in a data lake without the need to import data into the Power BI model. This mode \"combines\" the advantages of DirectQuery and Import modes, providing real-time access to data while leveraging the performance benefits of in-memory caching for certain queries."
      },
      {
        "date": "2024-05-19T08:58:00.000Z",
        "voteCount": 1,
        "content": "While Direct Lake mode doesn't query the SQL endpoint when loading data directly from OneLake, it's required when a Direct Lake model must seamlessly fall back to DirectQuery mode, such as when the data source uses specific features like advanced security or views that can't be read through Direct Lake."
      },
      {
        "date": "2024-05-17T08:39:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says the answer is C- Direct Lake"
      },
      {
        "date": "2024-04-11T03:08:00.000Z",
        "voteCount": 2,
        "content": "A. https://fabric.guru/power-bi-direct-lake-mode-frequently-asked-questions"
      },
      {
        "date": "2024-03-26T07:59:00.000Z",
        "voteCount": 2,
        "content": "A\nif the semantic model has direct lake connection to the source, then the report connecting to this semantic model has direct query."
      },
      {
        "date": "2024-03-20T10:57:00.000Z",
        "voteCount": 1,
        "content": "A as has been shared before https://learn.microsoft.com/en-us/fabric/data-warehouse/row-level-security"
      },
      {
        "date": "2024-03-28T11:10:00.000Z",
        "voteCount": 2,
        "content": "Power BI queries on a warehouse in Direct Lake mode will fall back to Direct Query mode to abide by row-level security."
      },
      {
        "date": "2024-02-27T08:41:00.000Z",
        "voteCount": 3,
        "content": "What the heck is \"Import\" in DAX? Its DirectQuery only."
      },
      {
        "date": "2024-03-20T16:11:00.000Z",
        "voteCount": 1,
        "content": "Probably a reference to table types in PBI where import is commonly used"
      },
      {
        "date": "2024-02-17T16:03:00.000Z",
        "voteCount": 2,
        "content": "Dax and fallback its direct query"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134094-exam-dp-600-topic-1-question-29-discussion/",
    "body": "You have a Fabric tenant that contains a complex semantic model. The model is based on a star schema and contains many tables, including a fact table named Sales.<br>You need to create a diagram of the model. The diagram must contain only the Sales table and related tables.<br>What should you use from Microsoft Power BI Desktop?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata categories",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModel view\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX query view"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-08T14:42:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \"C\"\n\nLink: https://learn.microsoft.com/en-us/power-bi/transform-model/desktop-relationship-view"
      },
      {
        "date": "2024-02-26T09:40:00.000Z",
        "voteCount": 4,
        "content": "Well this was kinda Captain Obvious."
      },
      {
        "date": "2024-02-18T23:36:00.000Z",
        "voteCount": 1,
        "content": "C. Model view\n\nIn the Model view, it is possible to analyze the semantic model and create new layouts."
      },
      {
        "date": "2024-02-17T16:04:00.000Z",
        "voteCount": 1,
        "content": "Model = model view"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134040-exam-dp-600-topic-1-question-30-discussion/",
    "body": "You have a Fabric tenant that contains a semantic model. The model uses Direct Lake mode.<br>You suspect that some DAX queries load unnecessary columns into memory.<br>You need to identify the frequently used columns that are loaded into memory.<br>What are two ways to achieve the goal? Each correct answer presents a complete solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Analyze in Excel feature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Vertipaq Analyzer tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS dynamic management view (DMV).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery the DISCOVER_MEMORYGRANT dynamic management view (DMV)."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-09T18:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is A&amp;B"
      },
      {
        "date": "2024-06-24T02:06:00.000Z",
        "voteCount": 4,
        "content": "Other Options: WRONG\nA. Use the Analyze in Excel feature: This feature allows for interaction with the model data in Excel but does not provide detailed insights into column-level memory usage.\n\nD. Query the DISCOVER_MEMORYGRANT DMV: This DMV provides information about memory grants for queries but does not provide detailed information about the columns loaded into memory."
      },
      {
        "date": "2024-05-27T12:14:00.000Z",
        "voteCount": 3,
        "content": "Methods to Identify Frequently Used Columns:\n\nB. Use the Vertipaq Analyzer tool.\nVertipaq Analyzer: This tool helps analyze the internal structure of your Power BI model. It provides detailed information about the storage and memory usage of your model, including which columns are frequently accessed and loaded into memory. This can help you identify unnecessary columns that are consuming resources.\nSteps:\nExport your Power BI model to a .pbix file.\nOpen the .pbix file in Power BI Desktop.\nUse the Vertipaq Analyzer tool to analyze the model and review the column usage statistics.\n\nC. Query the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS dynamic management view (DMV).\nDMVs: Dynamic Management Views (DMVs) provide detailed information about the operations of your Power BI models. Specifically, the $System.DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS DMV can give you insights into the storage and usage patterns of individual columns within your model."
      },
      {
        "date": "2024-05-08T14:53:00.000Z",
        "voteCount": 3,
        "content": "IMHO, \n\nB &amp; C\nBecause:\n1. The DISCOVER_STORAGE_TABLE_COLUMN_SEGMENTS schema rowset returns information about the column segments used for storing data for in-memory tables.&lt;336&gt;\n2. Very often there could be a few columns that are not required in your Power BI model, but they take up a lot of space. This is easy to find with Vertipaq Analyzer.\n\nLinks: https://learn.microsoft.com/en-us/openspecs/sql_server_protocols/ms-ssas/948d5135-5bf4-4cf7-82c5-3a38746c2fb8\nhttps://www.fourmoo.com/2020/11/11/how-to-use-vertipaq-analyzer-with-dax-studio-for-power-bi-model-analysis/"
      },
      {
        "date": "2024-04-17T04:50:00.000Z",
        "voteCount": 1,
        "content": "B and C\nA. It\u2019s more about data exploration and visualization.\nD. Provides information about memory grants for queries"
      },
      {
        "date": "2024-03-17T14:02:00.000Z",
        "voteCount": 1,
        "content": "B and C is the right answer."
      },
      {
        "date": "2024-02-26T09:41:00.000Z",
        "voteCount": 3,
        "content": "B and C is the right answer."
      },
      {
        "date": "2024-02-16T18:26:00.000Z",
        "voteCount": 4,
        "content": "I think BC.\nA is only tobread data and D only memory allocations"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133728-exam-dp-600-topic-1-question-31-discussion/",
    "body": "HOTSPOT -<br>You have the source data model shown in the following exhibit.<br><img title=\"image41\" src=\"https://img.examtopics.com/dp-600/image32.png\"><br>The primary keys of the tables are indicated by a key symbol beside the columns involved in each key.<br>You need to create a dimensional data model that will enable the analysis of order items by date, product, and customer.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image42\" src=\"https://img.examtopics.com/dp-600/image33.png\">",
    "options": [],
    "answer": "<img title=\"image43\" src=\"https://img.examtopics.com/dp-600/image34.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-28T04:48:00.000Z",
        "voteCount": 26,
        "content": "I think what they are trying to get at is that you denormalize the company information into both tables so that you do not need the CompanyID anymore, which would then make the relationship between OrderItem and Product only based on ProductID. Hence I think A and C.\nBut honestly that whole model is just weird."
      },
      {
        "date": "2024-04-24T05:40:00.000Z",
        "voteCount": 6,
        "content": "Agree up to the denormalization part. \n\nBut that does not imply that the relationship between OrderItem and Product should only be based on ProductID. The Product table has two columns as a primary key, ProductID and CompanyID.\n\nAfter denormalization, a two columns join should be performed in order to establish the proper relationships and proper analysis.\n\nSo correct answers are B, C i.e. the given one."
      },
      {
        "date": "2024-04-29T03:45:00.000Z",
        "voteCount": 14,
        "content": "the question says : \"You need to create a dimensional data model that will enable the analysis of order items by date, product, and customer.\"\nTherefore, the analysis of order item by date, product and customer doesn't need the company table (it can be omitted) ! \nThe answer is A and A"
      },
      {
        "date": "2024-07-22T15:43:00.000Z",
        "voteCount": 1,
        "content": "Company doesn't have any more attribute. just company id and it's already on customer and product entities so it can be omitted."
      },
      {
        "date": "2024-07-09T18:58:00.000Z",
        "voteCount": 3,
        "content": "I think Company table should be completely omitted as this is extra data that is not mentioned in the requirement (You need to create a dimensional data model that will enable the analysis of order items by date, product, and customer).\nSo the answer should be  The Product ID Column and Omitted."
      },
      {
        "date": "2024-06-16T09:51:00.000Z",
        "voteCount": 1,
        "content": "By denormalizing the relationships will become many to many no ?"
      },
      {
        "date": "2024-06-16T09:52:00.000Z",
        "voteCount": 1,
        "content": "forget what i said"
      },
      {
        "date": "2024-06-04T03:37:00.000Z",
        "voteCount": 3,
        "content": "Given the Primary key for Product is CompanyID and ProductID, how do you model two (2) \"active\" one-to-many relationships between Product (dimension) and Orderitem (fact) objects?"
      },
      {
        "date": "2024-06-03T22:45:00.000Z",
        "voteCount": 4,
        "content": "C and C \nThe company ID and Product ID combined together will make a unique Identifier.  You have to denormalise to reduce the number of joins in the model."
      },
      {
        "date": "2024-07-16T11:00:00.000Z",
        "voteCount": 1,
        "content": "Correct. C and C\nC. A new key that combines the CompanyID and ProductID columns\nIn a dimensional data model, especially in a star schema, it is important to create relationships that ensure uniqueness and properly link the fact table to the dimension tables.\n\nC. Denormalized into the Customer and Product entities\nThe Company table should be denormalized into both the Customer and Product entities. This denormalization simplifies the model by embedding company information directly into the related tables, reducing the need for additional joins and improving query performance."
      },
      {
        "date": "2024-05-27T12:21:00.000Z",
        "voteCount": 1,
        "content": "Relationship based on CompanyID and ProductID:\nWhy: In many real-world scenarios, products might be uniquely identified only when combined with the company context. Thus, ensuring the correct identification and linkage between OrderItem and Product necessitates using both columns. Create a composite key or use both columns to form the relationship between OrderItem and Product.\n\nDenormalizing the Company entity:\nWhy: Denormalizing Company data into Customer and Product helps to flatten the structure, making it easier to query and reducing the complexity of joins.\nHow: Add the relevant Company attributes directly into the Customer and Product tables. For instance, each Customer and Product entry will carry information about the Company they are associated with."
      },
      {
        "date": "2024-05-15T20:28:00.000Z",
        "voteCount": 4,
        "content": "The requirement for Analysis \"date, product, and customer\" so we only need productId and nothing about the company so Omitted. I'm interested on why many answers go for including company in the model? would love to hear about that."
      },
      {
        "date": "2024-05-26T01:41:00.000Z",
        "voteCount": 1,
        "content": "Because It seems more then one product will have the same ProductID if you omit the company, otherwise the current key of Product would not be ProductID and CompanyID"
      },
      {
        "date": "2024-05-09T09:11:00.000Z",
        "voteCount": 4,
        "content": "The request about creating dimensional data model don't include Company so in my opinion  the model can be achieved by A (ProductID Column) and A(Omitted) so no need to include it."
      },
      {
        "date": "2024-05-08T15:11:00.000Z",
        "voteCount": 2,
        "content": "IMHO, B &amp; C\n\nC - because there is initially compound PK. It means we can't just drop one, because it may be a case that for example different companies have same product_id which is actually different product"
      },
      {
        "date": "2024-05-08T15:13:00.000Z",
        "voteCount": 1,
        "content": "Also, Company Entity may be omitted, if follow the rules literally - \"no need in company\"."
      },
      {
        "date": "2024-05-03T02:06:00.000Z",
        "voteCount": 2,
        "content": "it can have all the answers\nC : A new surrogate key which combines Product and Company\nB:Denormalized Company into product table"
      },
      {
        "date": "2024-05-02T22:29:00.000Z",
        "voteCount": 1,
        "content": "The Product and customer Table already contains the CompanyId Information. As long as there are no futher information in the Company Table it can be omitted. If it can be omitted there is no need for changing the relationship, because in the current situation the one to many relation already works. In my opinion it should be A + A."
      },
      {
        "date": "2024-04-27T10:01:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why not the first one is C. I think we need more information but for example in power bi models you ca nnot do relationships based in two columns, you need a combined key so I think all Fabric datamodels should work the same way."
      },
      {
        "date": "2024-04-22T01:02:00.000Z",
        "voteCount": 3,
        "content": "C And C"
      },
      {
        "date": "2024-03-15T10:37:00.000Z",
        "voteCount": 5,
        "content": "If company has no other attribute it can be omitted"
      },
      {
        "date": "2024-03-09T15:43:00.000Z",
        "voteCount": 9,
        "content": "The given answer is correct. \nThe company entity should be denormalized into both product and customer tables. Company ID is part of the customer table and you cannot just denormalize it into the product table.\nand for the relationship between OrderItem and Product, you cannot just reference a part of the primary key. This is not how referencing works, your relationship would be wrong, if you only use product id and a product id is repeated for two different company IDs your relationship would consider both of them as the same product."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133729-exam-dp-600-topic-1-question-32-discussion/",
    "body": "You have a Fabric tenant that contains a semantic model named Model1. Model1 uses Import mode. Model1 contains a table named Orders. Orders has 100 million rows and the following fields.<br><img title=\"image44\" src=\"https://img.examtopics.com/dp-600/image35.png\"><br>You need to reduce the memory used by Model1 and the time it takes to refresh the model.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit OrderDateTime into separate date and time columns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace TotalQuantity with a calculated column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert Quantity into the Text data type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace TotalSalesAmount with a measure.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-16T18:51:00.000Z",
        "voteCount": 22,
        "content": "A : Best practice\nD : measure better than column"
      },
      {
        "date": "2024-02-16T18:52:00.000Z",
        "voteCount": 7,
        "content": "Its AD"
      },
      {
        "date": "2024-07-09T19:15:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D are correct."
      },
      {
        "date": "2024-06-08T19:20:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2024-05-27T15:59:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is A and D"
      },
      {
        "date": "2024-05-08T15:15:00.000Z",
        "voteCount": 1,
        "content": "IMHO, A &amp; D"
      },
      {
        "date": "2024-04-15T07:40:00.000Z",
        "voteCount": 2,
        "content": "It's A D"
      },
      {
        "date": "2024-02-14T10:37:00.000Z",
        "voteCount": 3,
        "content": "A should compress the memory size\nD should reduce the memory usage but"
      },
      {
        "date": "2024-02-13T01:48:00.000Z",
        "voteCount": 5,
        "content": "I was under the impression that A should be correct due to the fact that separate date and time column achieve higher columnar redundancy and allow better data compression. Whereas solution B, an additional calculated column, would inflate the memory usage. Measure are not stored in memory and would therefore be favorable concerning the stated objective of reducing memory used and minimizing refresh times."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133465-exam-dp-600-topic-1-question-33-discussion/",
    "body": "You have a Fabric tenant that contains a semantic model.<br>You need to prevent report creators from populating visuals by using implicit measures.<br>What are two tools that you can use to achieve the goal? Each correct answer presents a complete solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Power BI Desktop\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabular Editor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server Management Studio (SSMS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio"
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T09:03:00.000Z",
        "voteCount": 12,
        "content": "Answer is A and B. You can prevent user-defined implicit measures by turning them off in PBI Desktop or Tabular Editor."
      },
      {
        "date": "2024-03-21T04:04:00.000Z",
        "voteCount": 6,
        "content": "A,B turn enable discourage implicit measure for the semantic model. \nhttps://learn.microsoft.com/en-us/power-bi/transform-model/model-explorer#discourage-implicit-measures\nhttps://docs.tabulareditor.com/api/TabularEditor.TOMWrapper.Model.html#TabularEditor_TOMWrapper_Model_DiscourageImplicitMeasures"
      },
      {
        "date": "2024-06-08T19:22:00.000Z",
        "voteCount": 1,
        "content": "Just tabular editor and PowerBI can prevent user defined implicit measures"
      },
      {
        "date": "2024-05-27T16:02:00.000Z",
        "voteCount": 1,
        "content": "A. Microsoft Power BI Desktop: Power BI Desktop allows you to control and manage how measures are used within your reports. By carefully defining and using explicit measures within your data model, you can ensure that report creators use only these predefined measures instead of creating implicit measures automatically.\n\nB. Tabular Editor: Tabular Editor is a powerful tool for managing and editing Power BI and Analysis Services tabular models. It allows you to enforce best practices, such as disabling implicit measures, by modifying the model's properties and ensuring that only explicit measures are available for use in reports."
      },
      {
        "date": "2024-05-08T15:59:00.000Z",
        "voteCount": 3,
        "content": "IMHO, A &amp; B.\n\nSSMS - big NO, Dax Studio - only for DAX programming."
      },
      {
        "date": "2024-02-25T11:01:00.000Z",
        "voteCount": 4,
        "content": "Microsoft Power BI Desktop and Tabular Editor are your go-to for creating explicit measures."
      },
      {
        "date": "2024-02-25T11:08:00.000Z",
        "voteCount": 2,
        "content": "and you can hide or disable implicit measures within the model, which prevents report builders from using them."
      },
      {
        "date": "2024-02-17T16:09:00.000Z",
        "voteCount": 3,
        "content": "Ssms have nothing to do here and dax studio not alter model"
      },
      {
        "date": "2024-02-09T12:29:00.000Z",
        "voteCount": 3,
        "content": "AB: To prevent report creators from populating visuals using implicit measures in a Power BI semantic model within a Fabric tenant, you can utilize the following tools:\n1.\tTabular Editor:\n2.\tPower BI Desktop (Data Model View):"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134042-exam-dp-600-topic-1-question-34-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant that contains two lakehouses.<br>You are building a dataflow that will combine data from the lakehouses. The applied steps from one of the queries in the dataflow is shown in the following exhibit.<br><img title=\"image46\" src=\"https://img.examtopics.com/dp-600/image37.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image47\" src=\"https://img.examtopics.com/dp-600/image38.png\">",
    "options": [],
    "answer": "<img title=\"image48\" src=\"https://img.examtopics.com/dp-600/image39.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-16T19:03:00.000Z",
        "voteCount": 21,
        "content": "1: Some.blue icone fold and red dont fold\n2 : on power query. all process after the step that dont fold are on power query"
      },
      {
        "date": "2024-02-18T01:04:00.000Z",
        "voteCount": 5,
        "content": "The red icon suggests where query folding is interrupted, therefore only SOME steps are folded. All the other steps after the red icon will not fold and will be executed by the POWER QUERY engine."
      },
      {
        "date": "2024-03-21T04:12:00.000Z",
        "voteCount": 3,
        "content": "Correct as shown. If the query folded the work is pushed to the data source. the icons help to tell you if folding is occurring or not.\n\nhttps://learn.microsoft.com/en-us/power-query/step-folding-indicators#step-diagnostics-indicators"
      },
      {
        "date": "2024-02-27T09:13:00.000Z",
        "voteCount": 2,
        "content": "The answers are correct. Some steps will be performed in Query Folding and query folding is only available in MS Power Query engine."
      },
      {
        "date": "2024-02-22T01:18:00.000Z",
        "voteCount": 2,
        "content": "Reference in MS Learn: \nhttps://learn.microsoft.com/en-us/power-query/step-folding-indicators"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133466-exam-dp-600-topic-1-question-35-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse named Lakehouse\u2019. Lakehouse1 contains a table named Tablet.<br>You are creating a new data pipeline.<br>You plan to copy external data to Table\u2019. The schema of the external data changes regularly.<br>You need the copy operation to meet the following requirements:<br>Replace Table1 with the schema of the external data.<br>Replace all the data in Table1 with the rows in the external data.<br>You add a Copy data activity to the pipeline.<br>What should you do for the Copy data activity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Source tab, add additional columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Destination tab, set Table action to Overwrite.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Settings tab, select Enable staging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Source tab, select Enable partition discovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Source tab, select Recursively."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-09T09:46:00.000Z",
        "voteCount": 2,
        "content": "B Since the plan is to drop the destination schema and replace it with data source schema."
      },
      {
        "date": "2024-05-08T16:16:00.000Z",
        "voteCount": 1,
        "content": "IMHO, B and only B.\n\nNot D, because it is about partitions, not A, C, D - because of completely different purpose. A may be, generally, but it is not what a question about."
      },
      {
        "date": "2024-04-17T06:54:00.000Z",
        "voteCount": 1,
        "content": "Overwrite \nhttps://learn.microsoft.com/en-us/fabric/data-factory/copy-data-activity#configure-your-source-under-the-source-tab\nExpand Advanced for more advanced settings."
      },
      {
        "date": "2024-03-21T04:41:00.000Z",
        "voteCount": 1,
        "content": "B, the tool tip for Overwrite tells you:\n\"Enable overwrite table option to overwrite the existing data and schema in the table using the new values\""
      },
      {
        "date": "2024-02-17T16:22:00.000Z",
        "voteCount": 4,
        "content": "Replace all with overwrite"
      },
      {
        "date": "2024-02-09T12:36:00.000Z",
        "voteCount": 2,
        "content": "Correct: Enable \"Truncate table\" option: This option truncates the target table before copying data, ensuring that all existing data is replaced with the new data from the external source."
      },
      {
        "date": "2024-04-16T00:01:00.000Z",
        "voteCount": 2,
        "content": "this answer option is not available. Additionally, truncate a table does only delete the data but does not touch schema."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133467-exam-dp-600-topic-1-question-36-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse.<br>You plan to query sales data files by using the SQL endpoint. The files will be in an Amazon Simple Storage Service (Amazon S3) storage bucket.<br>You need to recommend which file format to use and where to create a shortcut.<br>Which two actions should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shortcut in the Files section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Parquet format\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the CSV format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shortcut in the Tables section.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the delta format."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 15,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-28T06:44:00.000Z",
        "voteCount": 19,
        "content": "Answer is DE.\nTo be able to use the SQL Endpoint you need to create the shortcut in the Tables section. The file also needs to be in the delta format to be recognised as a managed table. If you try to add a parquet file to the tables section, it will not be recognised as a table object and you won't be able to query it."
      },
      {
        "date": "2024-04-24T06:33:00.000Z",
        "voteCount": 5,
        "content": "A parquet file shortcut is recognized as a managed table. Delta format not supported by S3. The answer cannot be E."
      },
      {
        "date": "2024-04-24T06:32:00.000Z",
        "voteCount": 14,
        "content": "B,D are the correct answers, since the Parquet file format within S3, and the pointing to it via a lakehouse Table shortcut, allows the shorcut to be recognized by Fabric as a Delta table and hence provide access to the SQL endpoint.\n\n\nExplanation:\nAs per the provided links: \"If the target of the shortcut contains data in the Delta\\Parquet format, the lakehouse automatically synchronizes the metadata and recognizes the folder as a table\".\nIt is stated that S3 contains sales data files, that is files not tables. Besides, delta tables are not supported in S3. So the file format needs to be either CSV or Parquet, leading us to choose the latter for all the reasons stated in other comments.\nSo creating a shortcut in Table section that points to S3 Parquet files, will allow Fabric to recognise it as a Delta table, which in turn enables the SQL endpoint."
      },
      {
        "date": "2024-09-05T07:06:00.000Z",
        "voteCount": 2,
        "content": "Delta uis supported:\nhttps://blog.fabric.microsoft.com/en-us/blog/public-preview-of-onelake-shortcuts-to-s3-compatible-data-sources?ft=All\n\n\"\"If your data is already in the Delta Lake format, create your shortcut in the Tables section of your lakehouse. This will allow your table shortcut to benefit from metadata synchronization across Fabric engines, letting you use this structured data where tables are used in Fabric.\"\""
      },
      {
        "date": "2024-07-29T20:43:00.000Z",
        "voteCount": 1,
        "content": "chat GBT also agree with BD"
      },
      {
        "date": "2024-07-21T05:00:00.000Z",
        "voteCount": 2,
        "content": "CSV is best if the data is not that large"
      },
      {
        "date": "2024-07-21T04:58:00.000Z",
        "voteCount": 1,
        "content": "There is no mention of how big the data is , using CSV will be better if the data is small"
      },
      {
        "date": "2024-07-16T19:11:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT is saying amazon S3 supports both Delta and Parquet files"
      },
      {
        "date": "2024-07-10T17:46:00.000Z",
        "voteCount": 2,
        "content": "B- Because Parquet format is supported in Amazon S3\nhttps://learn.microsoft.com/en-us/azure/data-factory/format-parquet\nD- Because you need to use the Shortcuts as managed portion of the lakehouse."
      },
      {
        "date": "2024-06-18T06:46:00.000Z",
        "voteCount": 1,
        "content": "I guess MS documentation says Delta rather than Parquet? https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sql-analytics-endpoint"
      },
      {
        "date": "2024-06-12T21:49:00.000Z",
        "voteCount": 3,
        "content": "D and E. Tested. You can turn delta files available in your AWS Service and create a shortcut here."
      },
      {
        "date": "2024-06-06T21:08:00.000Z",
        "voteCount": 3,
        "content": "why not 'A'? The shortcut will be reading files from the S3 instead of tables."
      },
      {
        "date": "2024-05-27T16:19:00.000Z",
        "voteCount": 2,
        "content": "To effectively query sales data files stored in an Amazon S3 bucket using the SQL endpoint in your Fabric tenant's lakehouse, you should:\nB. Use the Parquet format\nD. Create a shortcut in the Tables section\n\n**Creating a shortcut in the Files section would treat the data more like raw files, which might not leverage the full potential of SQL querying capabilities. Creating the shortcut in the Tables section aligns more closely with the structured query requirements and provides a better-integrated experience."
      },
      {
        "date": "2024-05-27T16:18:00.000Z",
        "voteCount": 2,
        "content": "To effectively query sales data files stored in an Amazon S3 bucket using the SQL endpoint in your Fabric tenant's lakehouse, you should:\nB. Use the Parquet format\nD. Create a shortcut in the Tables section\n**Creating a shortcut in the Files section would treat the data more like raw files, which might not leverage the full potential of SQL querying capabilities. Creating the shortcut in the Tables section aligns more closely with the structured query requirements and provides a better-integrated experience."
      },
      {
        "date": "2024-05-19T21:35:00.000Z",
        "voteCount": 3,
        "content": "If you need just query, as they mentioned. Parquet (read in parallel) and shortcut in the table solve this problem. you don't need delta."
      },
      {
        "date": "2024-05-08T16:19:00.000Z",
        "voteCount": 1,
        "content": "IMHO, D &amp; E\n\nBecause of that: Shortcuts aren't supported in other subdirectories of the Tables folder. If the target of the shortcut contains data in the Delta\\Parquet format, the lakehouse automatically synchronizes the metadata and recognizes the folder as a table.\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#lakehouse"
      },
      {
        "date": "2024-04-27T17:59:00.000Z",
        "voteCount": 2,
        "content": "For querying sales data files using the SQL endpoint with files stored in an Amazon S3 storage bucket, I recommend the following two actions:\n\nFile Format: Choose the Apache Parquet file format for the sales data files. Parquet is a columnar storage file format that is optimized for complex queries and is efficient for use with SQL queries due to its data compression and encoding schemes. It supports complex data types and is ideal for large datasets1.\nShortcut Creation: Create a shortcut in the Lakehouse Explorer under the Tables section. When you create a shortcut to a Delta formatted table under Tables in Lakehouse Explorer, it will automatically register it as a table, enabling data access through Spark, SQL endpoint, and the default semantic model2.\nThese actions will help ensure efficient querying and easy accessibility of your sales data within the Fabric tenant\u2019s lakehouse environment."
      },
      {
        "date": "2024-04-25T05:29:00.000Z",
        "voteCount": 1,
        "content": "- E and D\nE. Use the delta format - Choosing Delta format is most suitable for complex environments that benefit from features like ACID transactions, schema enforcement, and historical data tracking. This choice is particularly effective in lakehouse architectures where maintaining data integrity and supporting complex queries are paramount.\nD. Create a shortcut in the Tables section - Given the structured nature of Delta format files and the need to perform SQL queries, creating a shortcut in the Tables section is most beneficial. This approach allows the SQL endpoint to efficiently query the data using table semantics, leveraging the optimizations provided by the lakehouse."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133468-exam-dp-600-topic-1-question-37-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a subfolder named Subfolder1 that contains CSV files.<br>You need to convert the CSV files into the delta format that has V-Order optimization enabled.<br>What should you do from Lakehouse explorer?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Load to Tables feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new shortcut in the Files section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new shortcut in the Tables section.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Optimize feature."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T16:14:00.000Z",
        "voteCount": 9,
        "content": "With ''Load to tables'' : tables are always loaded using the Delta Lake table format with V-Order optimization enabled.\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/load-to-tables#load-to-table-capabilities-overview"
      },
      {
        "date": "2024-08-27T11:11:00.000Z",
        "voteCount": 1,
        "content": "A, since optimize alone won't convert the CSV to Delta Format"
      },
      {
        "date": "2024-08-27T11:10:00.000Z",
        "voteCount": 1,
        "content": "Optimize feature alone does not convert CSV files to Delta format. It is used for optimizing Delta tables after they have been created.\n\nTo fully address the requirement of converting CSV files into Delta format with V-Order optimization, the correct approach involves:\n\nLoading CSV files into a Delta table: This is typically done using features that allow for the import and conversion of data into Delta format.\nApplying V-Order optimization: Once the data is in Delta format, you use the Optimize feature to apply V-Order.\nHence, D"
      },
      {
        "date": "2024-08-20T01:04:00.000Z",
        "voteCount": 1,
        "content": "The Optimize feature in a Lakehouse environment is specifically designed to optimize data, including converting it into more efficient formats like Delta and applying optimizations like V-Order. This feature is what you would use to convert CSV files into the Delta format and enable V-Order optimization, which reorganizes the data to improve query performance."
      },
      {
        "date": "2024-06-02T08:01:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-05-27T16:22:00.000Z",
        "voteCount": 2,
        "content": "The \"Load to Tables\" feature in Lakehouse explorer allows you to import data from various file formats (such as CSV) into a table within the lakehouse. During this process, you can specify the file format for the table, and by choosing the delta format, you can enable optimizations like V-Order."
      },
      {
        "date": "2024-05-08T18:51:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \"A\" is right.\n\nBecause: The Lakehouse in Microsoft Fabric provides a feature to efficiently load common file types to an optimized Delta table ready for analytics. The Load to Table feature allows users to load a single file or a folder of files to a table. \n\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/load-to-tables"
      },
      {
        "date": "2024-04-05T09:13:00.000Z",
        "voteCount": 2,
        "content": "I think Optimize ( Ans D) - https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"
      },
      {
        "date": "2024-02-27T19:58:00.000Z",
        "voteCount": 4,
        "content": "A is the correct answer."
      },
      {
        "date": "2024-02-17T16:40:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2024-02-13T14:07:00.000Z",
        "voteCount": 2,
        "content": "\"Load to Tables\" functionality, the Optimize check mark is set by default."
      },
      {
        "date": "2024-02-13T02:26:00.000Z",
        "voteCount": 2,
        "content": "Hi IshtarSQL, \nthe \"Optimize\" feature is only applicable on already existing tables and cannot convert CSV files as far as I know. When loading a CSV file as table using the \"Load to Tables\" functionality, the Optimize check mark is set by default. Therefore, A should be correct. \nCheers fabric1"
      },
      {
        "date": "2024-02-09T13:07:00.000Z",
        "voteCount": 1,
        "content": "The \"New Optimize\" feature in the Lakehouse Explorer to convert CSV files into Delta format with V-Order optimization enabled.\nThe \"New Optimize\" feature allows you to optimize your data in Delta Lake format, including enabling V-Order optimization. V-Order optimization improves query performance by organizing data according to the values of frequently queried columns."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134050-exam-dp-600-topic-1-question-38-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains an unpartitioned table named Table1.<br>You plan to copy data to Table1 and partition the table based on a date column in the source data.<br>You create a Copy activity to copy the data to Table1.<br>You need to specify the partition column in the Destination settings of the Copy activity.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Destination tab, set Mode to Append.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Destination tab, select the partition column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Source tab, select Enable partition discovery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Destination tabs, set Mode to Overwrite.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-18T01:10:00.000Z",
        "voteCount": 13,
        "content": "D. From the Destination tabs, set Mode to Overwrite.\n\nWhen setting up the Copy Activity, you need to choose the Overwrite mode to make the partition option appear (not visibile in Append mode)."
      },
      {
        "date": "2024-02-25T13:17:00.000Z",
        "voteCount": 6,
        "content": "A makes no sense, it leaves data that is already in table1 unpartitioned\nB makes no sense, it is the statement that you are asked to decide for the step before that. \nC MIGHT assign date column in source data as requested partition column, but only and if source data is complete and unambiguous\nD re-creates table1, ensuring that the partitioning is applied consistently.\n\nHoping that the data that is already in table1 is also in this source data, D should be recommended answer, I guess?"
      },
      {
        "date": "2024-09-09T01:05:00.000Z",
        "voteCount": 1,
        "content": "C. The \"first\" thing to do is to Enable Partition Discovery from the Source tab, this will enable the destination tab identify the partition column, then the destination tab can be set to overwrite.\nhttps://learn.microsoft.com/en-us/fabric/data-factory/connector-lakehouse-copy-activity"
      },
      {
        "date": "2024-07-29T20:54:00.000Z",
        "voteCount": 1,
        "content": "Enabling partition discovery on the Source tab allows the Copy activity to recognize the partition structure of the incoming data. This step is essential because it ensures that the data is copied into the destination table (Table1) with the correct partitions based on the specified date column."
      },
      {
        "date": "2024-06-23T19:55:00.000Z",
        "voteCount": 1,
        "content": "those who still think B is the answer, emphasis on \"What should you do first?\""
      },
      {
        "date": "2024-06-22T14:23:00.000Z",
        "voteCount": 1,
        "content": "D. From the Destination tabs, set Mode to Overwrite.\n\nin the question, it is specified that \"we are going to specify the partition column\". Yes, we will do that, BUT before that we have to set the \"Table action\" option to \"Overwrite\".\n\nsay, if you first set the partition column &amp; after that you select the \"overwrite\" mode, then the selected partition column will be cleared &amp; you will have to select that again.\n\nAnd \"overwrite\" is necessary because the table is not currently partitioned. we need to re-write the existing data leveraging partition."
      },
      {
        "date": "2024-05-27T16:33:00.000Z",
        "voteCount": 3,
        "content": "Enable Partition Discovery: This option ensures that the Copy activity can identify the partition column and apply it correctly to the destination table. By enabling partition discovery, you make sure that the source data's partitioning information is considered during the copy process.\nWhy Not: \nA. From the Destination tab, set Mode to Append: This setting controls how data is added to the existing table (whether new data is appended or existing data is overwritten). It does not directly address partitioning setup.\nB. From the Destination tab, select the partition column: While this is necessary to specify the partition column, it is logically the next step after enabling partition discovery. The system needs to recognize partitions before you can configure them.\nD. From the Destination tabs, set Mode to Overwrite: Similar to the Append mode, this setting determines how data is handled during the copy process but does not enable partitioning."
      },
      {
        "date": "2024-05-20T08:45:00.000Z",
        "voteCount": 1,
        "content": "Partition option need to switch to Overwrite mode first."
      },
      {
        "date": "2024-05-08T19:05:00.000Z",
        "voteCount": 3,
        "content": "IMHO, the answer is \"D\".\n\nBecause:\nExpand Advanced, in Table action, select Overwrite, and then select Enable partition, under Partition columns, select Add column, and choose the column you want to use as the partition column. You can choose to use a single column or multiple columns as the partition column.\n\nin https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-lakehouse-partition#load-data-to-lakehouse-using-partition-columns"
      },
      {
        "date": "2024-04-22T01:19:00.000Z",
        "voteCount": 5,
        "content": "D. From the Destination tabs, set Mode to Overwrite.\n\nhttps://learn.microsoft.com/en-us/fabric/data-factory/tutorial-lakehouse-partition#load-data-to-lakehouse-using-partition-columns\n\nExpand Advanced, in Table action, select Overwrite, and then select Enable partition, under Partition columns, select Add column, and choose the column you want to use as the partition column. You can choose to use a single column or multiple columns as the partition column."
      },
      {
        "date": "2024-04-15T01:45:00.000Z",
        "voteCount": 3,
        "content": "From the Destination tab, select the partition column. This ensures that the data is partitioned based on the specified column in the destination table"
      },
      {
        "date": "2024-02-19T07:12:00.000Z",
        "voteCount": 4,
        "content": "D is correct. Partition is available when Overwrite is checked"
      },
      {
        "date": "2024-02-16T22:19:00.000Z",
        "voteCount": 3,
        "content": "I think its C. For partition in pipeline we have to specify column in source"
      },
      {
        "date": "2024-02-17T11:58:00.000Z",
        "voteCount": 1,
        "content": "The table is unpartitioned to begin with."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134095-exam-dp-600-topic-1-question-39-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant that contains a warehouse named Warehouse1. Warehouse1 contains a fact table named FactSales that has one billion rows.<br>You run the following T-SQL statement.<br>CREATE TABLE test.FactSales AS CLONE OF Dbo.FactSales;<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image50\" src=\"https://img.examtopics.com/dp-600/image40.png\">",
    "options": [],
    "answer": "<img title=\"image51\" src=\"https://img.examtopics.com/dp-600/image41.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-12T07:04:00.000Z",
        "voteCount": 17,
        "content": "Just took the exam. In the actual answers the typo in A is gone, and c actually reads \"Additional data changes\"."
      },
      {
        "date": "2024-07-22T17:34:00.000Z",
        "voteCount": 1,
        "content": "for additional data changes will apply, did you select \"YES\"?"
      },
      {
        "date": "2024-10-06T03:06:00.000Z",
        "voteCount": 1,
        "content": "The answer would be No. From the documentation: Table clones help to create historical reports that reflect the state of data as it existed as of a specific point-in-time in the past."
      },
      {
        "date": "2024-04-01T02:31:00.000Z",
        "voteCount": 17,
        "content": "That was the same for my exam. There were many other new questions, but I found at the end when I was reviewing there is the Microsoft Learn documentation that can be opened in the exam and really wish I had known that earlier as you are permitted to use it. I passed the exam, but sharing that other is there as there was some questions for new syntax I had not used, but once I looked up I had guessed right for them. One was a SQL statement to use a function called GREATEST and another was a question about merging schema in python. There were many questions of putting things in the right order to have something work proper. One was related to reviewing DAX performance and another related to order of pipeline for establishing a Bronze, Silver, Gold set of data."
      },
      {
        "date": "2024-02-18T01:17:00.000Z",
        "voteCount": 8,
        "content": "Y - N - N\n\nThe AS CLONE AS creates a replica of the original table by copying the metadata (no data).\nThe two copies are independent therefore any changes will not be inherited.\n\nSource: https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table"
      },
      {
        "date": "2024-06-23T20:05:00.000Z",
        "voteCount": 2,
        "content": "actual typo is in the table name its going to create. its not dbo.Sales for sure. 1st option is yes only if there is no typo"
      },
      {
        "date": "2024-06-02T10:13:00.000Z",
        "voteCount": 3,
        "content": "If there are no typos in the choices first choice is NO because we are not closing dbo.Sales but dbo.FactSales"
      },
      {
        "date": "2024-05-08T19:20:00.000Z",
        "voteCount": 2,
        "content": "IMHO,  Y -&gt; N -&gt; N\n\nBecause of:\nCreates a new table as a zero-copy clone of another table in Warehouse in Microsoft Fabric. Only the metadata of the table is copied. The underlying data of the table, stored as parquet files, is not copied.\nin https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql?view=fabric&amp;preserve-view=true\n\nand\n\nAny changes made through DML or DDL on the source of the clone table are not reflected in the clone table.\nSimilarly, any changes made through DDL or DML on the table clone are not reflected on the source of the clone table.\n\nin https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#separate-and-independent"
      },
      {
        "date": "2024-02-27T09:31:00.000Z",
        "voteCount": 2,
        "content": "This is such a weird question. in the first statement, there is a typo and the question contains dbo.Sales when in fact, the original table is dbo.FactSales. I guess there was a typo so I'll let that pass and the answer is that YES, a replica is created of the original table. \n\nThe 2nd and 3rd questions are exact replicas of each other so both of their answers are NO. Any changes done after the cloning will not directly affect the cloned table."
      },
      {
        "date": "2024-02-25T13:36:00.000Z",
        "voteCount": 2,
        "content": "YNN is correct now. \nSecond and third statements are equal \"Additional schema changes to dbo.FactSales will also apply to test.FactSales\"\nWhat happens with this statement:  \n- A new table called test.FactSales is created.\n- The structure (columns, data types, constraints) of test.FactSales will be identical to that of Dbo.FactSales at the time of creation.\n- The data in test.FactSales will be a snapshot of the data in Dbo.FactSales at the moment of table creation."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133469-exam-dp-600-topic-1-question-40-discussion/",
    "body": "You have source data in a folder on a local computer.<br>You need to create a solution that will use Fabric to populate a data store. The solution must meet the following requirements:<br>Support the use of dataflows to load and append data to the data store.<br>Ensure that Delta tables are V-Order optimized and compacted automatically.<br>Which type of data store should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta lakehouse\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure SQL database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta KQL database"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-21T07:21:00.000Z",
        "voteCount": 9,
        "content": "The answer is correct - C Warehouse\nThe key to this question is \"Ensure that Delta tables are V-Order optimized\".\nV-Order optimization isn't guaranteed in Lakehouse, and there are times when you need to run OPTIMIZE to ensure the tables are V-Order Optimized.\nThis link here shows the answer:\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data#best-practices\nThe Note says \"Regardless of how you ingest data into warehouses, the parquet files produced by the data ingestion task will be optimized using V-Order write optimization... Unlike in Fabric Data Engineering, V-Order is a global setting in Synapse Data Warehouse that cannot be disabled.\""
      },
      {
        "date": "2024-02-27T09:34:00.000Z",
        "voteCount": 7,
        "content": "A - The only logical answer here. B is an Azure SQL database and is an Azure product but doesn't have V-Order. C is a just a generic warehouse and once again, doesn't necessarily contain any V-Order feature. D is a database that uses KQL and is irrelevant of the question."
      },
      {
        "date": "2024-06-10T07:47:00.000Z",
        "voteCount": 5,
        "content": "https://learn.microsoft.com/en-us/fabric/data-warehouse/ingest-data#best-practices\nThe Note says \"Regardless of how you ingest data into warehouses, the parquet files produced by the data ingestion task will be optimized using V-Order write optimization... Unlike in Fabric Data Engineering, V-Order is a global setting in Synapse Data Warehouse that cannot be disabled.\"\n\nTherefore the answer is warehouse"
      },
      {
        "date": "2024-09-10T00:48:00.000Z",
        "voteCount": 1,
        "content": "Read your quote one more time. It doesn't mean that the answer is a warehouse"
      },
      {
        "date": "2024-08-20T01:41:00.000Z",
        "voteCount": 1,
        "content": "n Microsoft Fabric, a warehouse is a specialized data store optimized for analytics and query performance. It uses V-Order write optimization, which is specifically designed to enhance read performance for parquet files across various compute engines such as Power BI, SQL, and Spark. This feature is automatically applied in Synapse Data Warehouse and cannot be disabled, ensuring that data stored in the warehouse is always optimized."
      },
      {
        "date": "2024-07-10T18:08:00.000Z",
        "voteCount": 1,
        "content": "Both A &amp; C are correct"
      },
      {
        "date": "2024-06-21T08:50:00.000Z",
        "voteCount": 4,
        "content": "I have recently taken the exam and this question was asked.Its a multiple choice question.we need to select 2 options.so as per the comments both lakehouse and warehouse are support delta tables and v-order optimization.Its A,C"
      },
      {
        "date": "2024-06-20T18:55:00.000Z",
        "voteCount": 1,
        "content": "C - Warehouse as for warehouse the optimized V-order is automatically enabled but for Lakehouse is setting that you need to enable or disable."
      },
      {
        "date": "2024-05-27T16:45:00.000Z",
        "voteCount": 3,
        "content": "Lakehouses can automatically handle the optimization and compaction of Delta tables, including V-Order optimization, which arranges data in an optimal order to improve query performance.\nWhy Not the Other Options?\nAzure SQL Database (Option B):Azure SQL Database is a relational database service that does not natively support Delta tables or V-Order optimization. It is more suited for traditional OLTP workloads.\nWarehouse (Option C):While warehouses are excellent for structured data and support dataflows, they may not provide the same level of native support for Delta tables and automatic V-Order optimization as a lakehouse.\nKQL Database (Option D):KQL (Kusto Query Language) databases are optimized for log and telemetry data, primarily used with Azure Data Explorer. They are not designed to support Delta tables or the specific optimizations required for large-scale transactional data processing."
      },
      {
        "date": "2024-05-08T19:35:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \"A\"\n\nbecause:\nThe Lakehouse and the Delta Lake table format are central to Microsoft Fabric, assuring that tables are optimized for analytics is a key requirement. ...\nin \nhttps://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql"
      },
      {
        "date": "2024-04-10T04:17:00.000Z",
        "voteCount": 3,
        "content": "lakehouse doesn't automaticly store data in delta tables while warehouse does."
      },
      {
        "date": "2024-03-21T06:54:00.000Z",
        "voteCount": 2,
        "content": "The whole new platform is focused on the lakehouse and optimization for it, so answer A."
      },
      {
        "date": "2024-02-17T16:46:00.000Z",
        "voteCount": 4,
        "content": "Delta table... = Lakehouse"
      },
      {
        "date": "2024-02-09T13:22:00.000Z",
        "voteCount": 4,
        "content": "To meet the requirements of supporting dataflows to load and append data to the data store while ensuring that Delta tables are V-Order optimized and compacted automatically, you should use a lakehouse in Fabric as your solution."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134096-exam-dp-600-topic-1-question-41-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant that contains a lakehouse.<br>You are using a Fabric notebook to save a large DataFrame by using the following code. df.write.partitionBy(\u201cyear\u201d, \u201cmonth\u201d, \u201cday\u201d).mode(\u201coverwrite\u201d).parquet(\u201cFiles/SalesOrder\u201d)<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image53\" src=\"https://img.examtopics.com/dp-600/image43.png\">",
    "options": [],
    "answer": "<img title=\"image54\" src=\"https://img.examtopics.com/dp-600/image44.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-17T16:54:00.000Z",
        "voteCount": 24,
        "content": "I think yes yes yes. \nParquet= compression"
      },
      {
        "date": "2024-02-28T01:39:00.000Z",
        "voteCount": 7,
        "content": "I think the same"
      },
      {
        "date": "2024-08-23T01:39:00.000Z",
        "voteCount": 1,
        "content": "Yup, agreed"
      },
      {
        "date": "2024-02-25T13:43:00.000Z",
        "voteCount": 6,
        "content": "I think so too: YYY\ncode snippet according to Learn"
      },
      {
        "date": "2024-02-25T13:52:00.000Z",
        "voteCount": 5,
        "content": "additional: Parquet files are compressed by default, and you don\u2019t need to take any additional actions to enable compression. When writing Parquet files, you can specify the desired compression codec (if needed) to further optimize storage and performance"
      },
      {
        "date": "2024-09-09T06:36:00.000Z",
        "voteCount": 1,
        "content": "Technically the results will not form a hierarchy of folders for EACH partition key right? Because for the day partition key, files are created. If it was phrased generically without the each-part, would've been better. Did someone have it on the exam?"
      },
      {
        "date": "2024-06-03T09:49:00.000Z",
        "voteCount": 3,
        "content": "Yes Yes Yes\nThe compression is optional parameter which uses 'snappy' compression by default. Unless we specifiy none, compression happens."
      },
      {
        "date": "2024-05-27T16:50:00.000Z",
        "voteCount": 1,
        "content": "Y-Y-N\nThe results will form a hierarchy of folders for each partition key:\nYes: When using partitionBy in Spark, the data is organized into a hierarchical directory structure based on the specified partition keys. Therefore, you will see directories like year=YYYY/month=MM/day=DD within the specified output path.\n\nThe resulting file partitions can be read in parallel across multiple nodes:\nYes: Parquet files are designed for efficient querying and support parallel processing. Spark can read these partitions in parallel, enabling distributed query execution across multiple nodes.\n\nThe resulting file partitions will use file compression:\nNo: While Parquet format supports compression, it is not enabled by default in the code snippet provided. Compression needs to be explicitly specified if required. For example, you could use .option(\"compression\", \"snappy\") to enable Snappy compression."
      },
      {
        "date": "2024-06-06T00:20:00.000Z",
        "voteCount": 3,
        "content": "Snappy is the default compression type of parquet, though. So it will create chunks of your file by default and compress them. If you want to have it all in one file, that's when you have to overwrite your default compression. So I disagree with the 'No'"
      },
      {
        "date": "2024-05-08T19:39:00.000Z",
        "voteCount": 1,
        "content": "IMHO, fully agree with colleagues below - Y -&gt; Y -&gt; Y"
      },
      {
        "date": "2024-04-20T00:37:00.000Z",
        "voteCount": 2,
        "content": "i think YYY as well"
      },
      {
        "date": "2024-02-27T09:37:00.000Z",
        "voteCount": 1,
        "content": "I think it should be NYY as there is no mention in the code to form a hierarchy. Please correct me if I'm wrong."
      },
      {
        "date": "2024-03-21T06:16:00.000Z",
        "voteCount": 3,
        "content": "partitionBy will create it"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134097-exam-dp-600-topic-1-question-42-discussion/",
    "body": "You have a Fabric workspace named Workspace1 that contains a data flow named Dataflow1 contains a query that returns the data shown in the following exhibit.<br><img title=\"image55\" src=\"https://img.examtopics.com/dp-600/image45.png\"><br>You need to transform the data columns into attribute-value pairs, where columns become rows.<br>You select the VendorID column.<br>Which transformation should you select from the context menu of the VendorID column?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup by",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnpivot columns",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnpivot other columns\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove other columns"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T16:57:00.000Z",
        "voteCount": 7,
        "content": "Correct"
      },
      {
        "date": "2024-05-08T19:54:00.000Z",
        "voteCount": 3,
        "content": "IMHO, \"C\",\n\nBecause of: Unpivot other columns\n This command unpivots unselected columns. Use this command in a query when not all columns are known. New columns added during a refresh operation are also unpivoted.\n\ndescription is here: https://support.microsoft.com/en-us/office/unpivot-columns-power-query-0f7bad4b-9ea1-49c1-9d95-f588221c7098#:~:text=Select%20the%20columns%20you%20do,Transform%20%3E%20Unpivot%20Only%20Selected%20Columns."
      },
      {
        "date": "2024-04-16T00:30:00.000Z",
        "voteCount": 1,
        "content": "clearly C"
      },
      {
        "date": "2024-02-27T09:39:00.000Z",
        "voteCount": 3,
        "content": "C is the correct answer"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133470-exam-dp-600-topic-1-question-43-discussion/",
    "body": "You have a Fabric tenant that contains a data pipeline.<br>You need to ensure that the pipeline runs every four hours on Mondays and Fridays.<br>To what should you set Repeat for the schedule?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDaily",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBy the minute",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeekly\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHourly"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T17:04:00.000Z",
        "voteCount": 14,
        "content": "Answer C : Weekly.\nThe only way to do this is to set the schedule to ''Weekly'', set the days on Monday and Friday and add manually 6 Time of 4 hour intervals."
      },
      {
        "date": "2024-09-03T11:59:00.000Z",
        "voteCount": 1,
        "content": "Weekly"
      },
      {
        "date": "2024-05-27T16:56:00.000Z",
        "voteCount": 2,
        "content": "Schedule Type: Weekly\nDays: Monday, Friday\nInterval: Every 4 hours"
      },
      {
        "date": "2024-05-08T19:56:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \"C\" is good"
      },
      {
        "date": "2024-04-16T00:37:00.000Z",
        "voteCount": 2,
        "content": "C - individual days can be selected only in Weekly schedule, and then add additional Times manually for each interval within a day."
      },
      {
        "date": "2024-04-07T07:16:00.000Z",
        "voteCount": 2,
        "content": "I would pick D.\nTo ensure that the pipeline runs every four hours on Mondays and Fridays, you should set the Repeat for the schedule to \"4 hours\"."
      },
      {
        "date": "2024-02-27T09:40:00.000Z",
        "voteCount": 4,
        "content": "This is a no-brainer as the only way to make this work is to set the schedule as weekly and specify the pipeline to run on Mondays and Wednesdays."
      },
      {
        "date": "2024-02-17T16:58:00.000Z",
        "voteCount": 3,
        "content": "Weekly"
      },
      {
        "date": "2024-02-14T06:42:00.000Z",
        "voteCount": 4,
        "content": "Weekly allow to choose the days and the time"
      },
      {
        "date": "2024-02-13T02:47:00.000Z",
        "voteCount": 3,
        "content": "Hi IshtarSQL, \nthe selection of individual week days is only possible in the \"Weekly\" schedule option. Therefore, C is correct here."
      },
      {
        "date": "2024-02-09T13:31:00.000Z",
        "voteCount": 1,
        "content": "To ensure that the pipeline runs every four hours on Mondays and Fridays, you should set the \"Repeat\" frequency for the schedule to \"Daily\" and set the \"Interval\" to 4 hours. Then, you can specify the days of the week when the pipeline should run by selecting only Mondays and Fridays."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134098-exam-dp-600-topic-1-question-44-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse.<br>Several times a day, the performance of all warehouse queries degrades. You suspect that Fabric is throttling the compute used by the warehouse.<br>What should you use to identify whether throttling is occurring?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Capacity settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Monitoring hub",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic management views (DMVs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Microsoft Fabric Capacity Metrics app\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T09:43:00.000Z",
        "voteCount": 9,
        "content": "From MS Learn: \nThe Microsoft Capacity Metrics app, also known as the metrics app, serves as a monitoring tool within the Microsoft Power BI service. It offers functionalities to track and analyze the resource utilization"
      },
      {
        "date": "2024-05-25T06:03:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/fabric/data-warehouse/compute-capacity-smoothing-throttling\n\"Just like most Warehouse operations, dynamic management views (DMVs) are also classified as background and covered by the \"Background Rejection\" policy. As a result, DMVs cannot be queried when capacity is throttled. Even though DMVs are not available, capacity admins can go to Microsoft Fabric Capacity Metrics app to understand the root cause.\""
      },
      {
        "date": "2024-05-08T20:01:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \"D\"\n\nBecause:\nTrack rejected operations\nThe Microsoft Fabric Capacity Metrics app drilldown allows admins to see operations that were rejected during a throttling event. \n\nHere: https://learn.microsoft.com/en-us/fabric/enterprise/throttling#track-rejected-operations"
      },
      {
        "date": "2024-04-05T09:21:00.000Z",
        "voteCount": 1,
        "content": "yes Ans D - https://learn.microsoft.com/en-us/fabric/enterprise/metrics-app"
      },
      {
        "date": "2024-02-17T16:59:00.000Z",
        "voteCount": 2,
        "content": "Microsoft Fabric Capacity Metrics app for trottling"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134102-exam-dp-600-topic-1-question-45-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric workspace that uses the default Spark starter pool and runtime version 1.2.<br>You plan to read a CSV file named Sales_raw.csv in a lakehouse, select columns, and save the data as a Delta table to the managed area of the lakehouse. Sales_raw.csv contains 12 columns.<br>You have the following code.<br><img title=\"image61\" src=\"https://img.examtopics.com/dp-600/image51.png\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image62\" src=\"https://img.examtopics.com/dp-600/image52.png\">",
    "options": [],
    "answer": "<img title=\"image63\" src=\"https://img.examtopics.com/dp-600/image53.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-09T20:49:00.000Z",
        "voteCount": 27,
        "content": "1. No, this is called filter pushdown / predicate pushdown / column pruning. This config is available when reading from a columnar type like parquet, I didn't find anything related to csv, I know that you can pushdown a predicate on csv to make it only read some rows in that case it works but it probably doesn't work for selecting columns so spark will read the entire file then filters the columns.\n2. Yes partitioning creates some overhead since Spark needs to create more files\n3. Yes infereSchema forces spark to read the file twice once for schema and once for data"
      },
      {
        "date": "2024-05-28T06:20:00.000Z",
        "voteCount": 13,
        "content": "N-N-Y\n\u2022No: The Spark engine will initially read all columns from the CSV file because the .select() transformation is applied after the data has been read into memory. Therefore, all 12 columns from Sales_raw.csv are read before the selection of specific columns is applied.\n\u2022No: Removing the partition might not necessarily reduce the execution time. While there might be some overhead in writing data to partitions, the overall impact on read performance, especially for large datasets, is usually beneficial. The query execution time for saving might be higher due to partitioning, but the read performance improvement usually outweighs this cost.\n\u2022Yes: Adding inferSchema = 'true' will increase the execution time of the query because Spark will need to read through the entire dataset to determine the data types of each column. This extra pass over the data adds to the initial read time."
      },
      {
        "date": "2024-06-03T02:00:00.000Z",
        "voteCount": 1,
        "content": "This is correct"
      },
      {
        "date": "2024-07-17T15:54:00.000Z",
        "voteCount": 1,
        "content": "I'm going to Y-N-N"
      },
      {
        "date": "2024-06-15T02:39:00.000Z",
        "voteCount": 9,
        "content": "I took the test today. This question was included, but the option 'Removing the partition will reduce the execution time of the query' has been replaced by 'Will the Year column replace the OrderDate column?'. My answer was No."
      },
      {
        "date": "2024-05-27T07:16:00.000Z",
        "voteCount": 1,
        "content": "just tried it, it only writes the columns that were selected.\nAnswer: YNY"
      },
      {
        "date": "2024-05-27T07:33:00.000Z",
        "voteCount": 2,
        "content": "also, what spark does is perform a lazy evaluation approach, it does not read each method(read,load,option) into memory. The actual reading happens when an action is performed such as(display,show,write). Spark will create a plan on how to execute the entire query and will optimize this plan for efficient execution."
      },
      {
        "date": "2024-05-08T20:11:00.000Z",
        "voteCount": 4,
        "content": "IMHO, \n1. N\n2. N - arguable\n3. Y\n\n1 No - because it is CSV. It will be read in full (in contrast to parquet)\n2 No - well, maybe 0.5% slower due to creating a new files. But actually - no\n3 Yes - because infering schema - it is additional process"
      },
      {
        "date": "2024-05-07T04:34:00.000Z",
        "voteCount": 3,
        "content": "No, CSV will be read in full and then filtered. \nNo: Using the partition by clause in Spark's Delta format can impact write performance in several ways:\n\nIncreased Write Throughput: Partitioning your data can potentially increase write throughput by distributing the write workload across multiple partitions. This parallelism allows Spark to write data to different partitions concurrently, improving overall write performance, especially when dealing with large datasets.\nY. Infer schema will slow the performance"
      },
      {
        "date": "2024-04-28T00:29:00.000Z",
        "voteCount": 2,
        "content": "I would go with NYY.\n\nIt's a CSV it is a row format, I don't think you can separate it by columns before reading the entire content.\n\nPartitioning takes extra work, so it may slow down the proccess.\n\nInferSchema requires an extra scan of the document or I think so, so maybe, I will go with yes."
      },
      {
        "date": "2024-04-27T04:49:00.000Z",
        "voteCount": 1,
        "content": "1. Yes: Reason: Select columns: The code selects specific columns from the DataFrame  using the select method. The selected columns are \"SalesOrderNumber\", \"OrderDate\", \"CustomerName\", and \"UnitPrice\".\n2. Yes: Reason: removing the partitionBy will simplify the process. Partitioning data involves some overhead in organizing the data into separate folders/files based on the partitioning column.\n3. No: Reason: Potentially Slower: Enabling inferSchema generally results in a slightly slower initial read operation. This is because Spark needs to do an additional scan of a portion of your data to analyze and determine data types before loading it."
      },
      {
        "date": "2024-03-03T11:27:00.000Z",
        "voteCount": 5,
        "content": "After read all file, engine will select just some. But, initially it runs the entire file."
      },
      {
        "date": "2024-02-27T09:48:00.000Z",
        "voteCount": 11,
        "content": "The answer is probably YNY\n1. Those are exactly the columns that are being read. So Yes\n2. Removing the PartitionBy line would not result in any performance changes. So NO\n3. Adding inferSchema as True WILL result in extra time in execution as it will make the engine go over the data twice (one to read data and the other time to read Schema). So YES."
      },
      {
        "date": "2024-02-25T14:25:00.000Z",
        "voteCount": 2,
        "content": "full of typos, this one.\nAnyhow, my guess:\nYNN\ninferSchema=true helps automatically determine column data types, but it needs a extra pass over the data, which comes with a slight query performance cost. So last statement = No"
      },
      {
        "date": "2024-02-17T17:30:00.000Z",
        "voteCount": 3,
        "content": "Its read not red.\nThis question is ambiguous would say : no no yes.\nFor the point 1 :  with case sensitivity sales_raw is not Sales_raw"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134103-exam-dp-600-topic-1-question-46-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse.<br>A user discovers that a report that usually takes two minutes to render has been running for 45 minutes and has still not rendered.<br>You need to identify what is preventing the report query from completing.<br>Which dynamic management view (DMV) should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_exec_requests\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_exec_sessions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_exec_connections",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsys.dm_pdw_exec_requests"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-19T17:33:00.000Z",
        "voteCount": 11,
        "content": "Answer is A. \nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/monitor-using-dmv"
      },
      {
        "date": "2024-02-28T07:34:00.000Z",
        "voteCount": 2,
        "content": "Yeah they finally got rid of the PDW bit."
      },
      {
        "date": "2024-03-21T08:00:00.000Z",
        "voteCount": 1,
        "content": "If you look at docs it is still out there for a parallel datawarehouse, but considering this is an exam about fabric, then the link mentioned for fabric monitor would make the answer A I agree."
      },
      {
        "date": "2024-07-29T23:02:00.000Z",
        "voteCount": 2,
        "content": "sys.dm_pdw_exec_requests: This DMV provides information about the status of requests (queries) executed in SQL Data Warehouse (now known as Azure Synapse Analytics) environments. It includes details about query execution, such as start time, end time, status, and any error messages. This DMV is particularly useful for diagnosing long-running or stuck queries."
      },
      {
        "date": "2024-05-13T10:43:00.000Z",
        "voteCount": 3,
        "content": "D.   A or D are really close, but since it is Azure Synapse Analytics D is probably the best answer.   Why sys.dm_pdw_exec_requests is better for this Fabric scenario:\n\nFabric's MPP architecture: Fabric warehouses utilize a distributed architecture where queries are broken down and processed across multiple compute nodes. This is why sys.dm_pdw_exec_requests is the ideal tool.\nMPP-specific insights: This DMV gives you visibility into how the query is being executed across the nodes, which can reveal bottlenecks or performance issues that wouldn't be apparent in sys.dm_exec_requests.\nTargeted troubleshooting: With the MPP-specific data from sys.dm_pdw_exec_requests, you can pinpoint the exact steps or nodes causing the slow performance, leading to a more efficient resolution."
      },
      {
        "date": "2024-05-16T05:26:00.000Z",
        "voteCount": 3,
        "content": "sys.dm_exec_requests is the correct answer after a lot more review and searching, can't delete my original comment, but here is why....In the context of Microsoft Fabric, sys.dm_exec_requests would be the appropriate DMV to use for monitoring running queries, as it is specifically designed for the SQL engine within Fabric.\n\nsys.dm_pdw_exec_requests is a DMV designed for Azure Synapse Analytics dedicated SQL pools (formerly SQL Data Warehouse). While Fabric's warehouse engine is based on SQL Server technology, it's not directly compatible with the DMVs specific to Azure Synapse Analytics."
      },
      {
        "date": "2024-05-08T20:14:00.000Z",
        "voteCount": 1,
        "content": "IMHO, \"A\",\n\nit may be found here: https://learn.microsoft.com/en-us/fabric/data-warehouse/monitor-using-dmv#identify-and-kill-a-long-running-query"
      },
      {
        "date": "2024-04-27T18:17:00.000Z",
        "voteCount": 1,
        "content": "You can use the sys.dm_exec_requests dynamic management view (DMV) to identify what is preventing the report query from completing 1."
      },
      {
        "date": "2024-04-07T07:14:00.000Z",
        "voteCount": 3,
        "content": "Answer would be D, I guess...\nHere the key word might be warehouse...\nThe sys.dm_exec_requests dynamic management view is used in SQL Server to provide information about each request that is executing within SQL Server. On the other hand, the sys.dm_pdw_exec_requests dynamic management view is used in Azure SQL Data Warehouse to provide similar information about each request executing within the data warehouse environment. Both views offer insights into the current status and details of queries running in their respective environments."
      },
      {
        "date": "2024-03-10T11:39:00.000Z",
        "voteCount": 2,
        "content": "A would be correct"
      },
      {
        "date": "2024-02-29T17:59:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\nhttps://learn.microsoft.com/en-us/sql/relational-databases/system-dynamic-management-views/sys-dm-exec-connections-transact-sql?view=sql-server-ver16"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134070-exam-dp-600-topic-1-question-47-discussion/",
    "body": "DRAG DROP -<br>You are creating a data flow in Fabric to ingest data from an Azure SQL database by using a T-SQL statement.<br>You need to ensure that any foldable Power Query transformation steps are processed by the Microsoft SQL Server engine.<br>How should you complete the code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image64\" src=\"https://img.examtopics.com/dp-600/image54.png\">",
    "options": [],
    "answer": "<img title=\"image65\" src=\"https://img.examtopics.com/dp-600/image55.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-18T01:22:00.000Z",
        "voteCount": 19,
        "content": "* Value\n* NativeQuery\n* EnableFolding\n\nSource: https://learn.microsoft.com/en-us/power-query/native-query-folding"
      },
      {
        "date": "2024-05-28T06:32:00.000Z",
        "voteCount": 2,
        "content": "\u2022Value.NativeQuery: This function is used to execute a native SQL query directly against the data source. This ensures that the query is processed on the server-side, which can significantly improve performance by leveraging the database engine's capabilities.\n\u2022EnableFolding: By setting EnableFolding to true, you are instructing Power Query to allow query folding for any subsequent transformations that can be pushed down to the SQL Server engine. This means that Power Query will try to push as much of the query logic as possible to the database engine."
      },
      {
        "date": "2024-05-08T20:20:00.000Z",
        "voteCount": 2,
        "content": "IMHO,\nValue -&gt; NativeQuery -&gt; EnableFolding is good :)\n\nBecause of here:\nValue.NativeQuery(Source, \"SELECT DepartmentID, Name FROM HumanResources.Department WHERE GroupName = 'Research and Development'  \", null, [EnableFolding = true])\n\nLink: \nhttps://learn.microsoft.com/en-us/power-query/native-query-folding#use-valuenativequery-function"
      },
      {
        "date": "2024-02-27T10:02:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      },
      {
        "date": "2024-02-17T11:27:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134071-exam-dp-600-topic-1-question-48-discussion/",
    "body": "DRAG DROP -<br>You have a Fabric tenant that contains a lakehouse named Lakehouse1.<br>Readings from 100 IoT devices are appended to a Delta table in Lakehouse1. Each set of readings is approximately 25 KB. Approximately 10 GB of data is received daily.<br>All the table and SparkSession settings are set to the default.<br>You discover that queries are slow to execute. In addition, the lakehouse storage contains data and log files that are no longer used.<br>You need to remove the files that are no longer used and combine small files into larger files with a target size of 1 GB per file.<br>What should you do? To answer, drag the appropriate actions to the correct requirements. Each action may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image67\" src=\"https://img.examtopics.com/dp-600/image67.png\">",
    "options": [],
    "answer": "<img title=\"image68\" src=\"https://img.examtopics.com/dp-600/image68.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-18T07:58:00.000Z",
        "voteCount": 18,
        "content": "VACUUM: to remove old files no longer referenced.\nOPTIMIZE: to create fewer files with a larger size.\n\nSources:\n* https://learn.microsoft.com/en-us/fabric/data-engineering/delta-optimization-and-v-order?tabs=sparksql\n* VACUUM: https://docs.delta.io/latest/delta-utility.html#-delta-vacuum\n* OPTIMIZE: https://docs.delta.io/latest/optimizations-oss.html"
      },
      {
        "date": "2024-07-17T16:07:00.000Z",
        "voteCount": 2,
        "content": "answer is correct :)"
      },
      {
        "date": "2024-05-28T06:37:00.000Z",
        "voteCount": 1,
        "content": "\u2022 Remove the files that are no longer used:\nRun the VACUUM command on a schedule: The VACUUM command cleans up old files and log files that are no longer needed by the Delta table, helping to free up storage and potentially improve performance by reducing the number of files the query engine needs to consider.\n\u2022 Combine small files into larger files:\nRun the OPTIMIZE command on a schedule: The OPTIMIZE command compacts small files into larger ones, improving read performance by reducing the overhead associated with opening many small files. This can be particularly useful when you have a large number of small files due to frequent appends of small data sets."
      },
      {
        "date": "2024-05-08T20:23:00.000Z",
        "voteCount": 1,
        "content": "IMHO,\n\nVacuum &amp; Optimize are good for optimizing Delta Lake :)"
      },
      {
        "date": "2024-03-05T06:34:00.000Z",
        "voteCount": 2,
        "content": "I agree that it is VACUUM and OPTIMIZE, but I would say Set the optimizeWrite table setting (B) and not Run the OPTIMIZE command on a schedule (E)."
      },
      {
        "date": "2024-03-12T07:07:00.000Z",
        "voteCount": 1,
        "content": "Isn't optimizeWrite set by default though? However that would only optimize the data as it is written, not over time."
      },
      {
        "date": "2024-02-17T11:41:00.000Z",
        "voteCount": 3,
        "content": "Correct :\nOPTIMIZE\tImproves query performance by optimizing file sizes. See Compact data files with optimize on Delta Lake.\nVACUUM\tReduces storage costs by deleting data files no longer referenced by the table. See Remove unused data files with vacuum."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133737-exam-dp-600-topic-1-question-49-discussion/",
    "body": "You need to create a data loading pattern for a Type 1 slowly changing dimension (SCD).<br>Which two actions should you include in the process? Each correct answer presents part of the solution.<br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate rows when the non-key attributes have changed.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert new rows when the natural key exists in the dimension table, and the non-key attribute values have changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the effective end date of rows when the non-key attribute values have changed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert new records when the natural key is a new value in the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 29,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-13T04:28:00.000Z",
        "voteCount": 17,
        "content": "Type 1 SCD does not preserve history, therefore no end dates for table entries exists. A and D are correct."
      },
      {
        "date": "2024-09-03T23:35:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2024-07-19T05:45:00.000Z",
        "voteCount": 1,
        "content": "AD because C is representing type2 SCD"
      },
      {
        "date": "2024-06-01T10:11:00.000Z",
        "voteCount": 1,
        "content": "Come on! A and D, for sure. Basic question about data warehouse (SCD)"
      },
      {
        "date": "2024-05-28T06:41:00.000Z",
        "voteCount": 1,
        "content": "A. Update rows when the non-key attributes have changed: In a Type 1 SCD, when a change is detected in any of the non-key attributes of an existing row, the current row is updated with the new values. This type of SCD does not keep any historical data; it simply overwrites the old data with the new data.\nD. Insert new records when the natural key is a new value in the table: When a new record (with a new natural key) is encountered that does not already exist in the dimension table, it is inserted as a new row. This is necessary to ensure that all new entities are captured in the dimension."
      },
      {
        "date": "2024-05-10T07:07:00.000Z",
        "voteCount": 1,
        "content": "I would go with A and C: Type1 use case to make changes or correction without involving histohttps://www.examtopics.com/exams/microsoft/dp-600/view/13/#ric data. D in my opinion is type 2 inserting new record with natural key for versioning."
      },
      {
        "date": "2024-05-08T20:26:00.000Z",
        "voteCount": 2,
        "content": "IMHO, A &amp; D is good.\n\nTaking into account this: A Type 1 SCD always reflects the latest values, and when changes in source data are detected, the dimension table data is overwritten. \n\nLink: https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
      },
      {
        "date": "2024-04-16T05:50:00.000Z",
        "voteCount": 1,
        "content": "AD - other options do not concern scd type 1 (overwrite) but scd type 2 (insert new row)"
      },
      {
        "date": "2024-03-01T07:31:00.000Z",
        "voteCount": 4,
        "content": "AD makes sense given the type of SCD as no history is maintained and any updates on the columns are taken care of by replacing the values then and there.\n\nBut, still a bit skeptical about C, since that is a part of SCD too in case we have certain audit columns that we would have to update. If the end_date is an audit column to update the date and time of last entry into a record, then, it would be required to be updated too."
      },
      {
        "date": "2024-02-28T07:42:00.000Z",
        "voteCount": 2,
        "content": "AD = SCD1, BC = SCD2"
      },
      {
        "date": "2024-02-17T11:43:00.000Z",
        "voteCount": 1,
        "content": "No history for scd1"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133471-exam-dp-600-topic-1-question-50-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric workspace named Workspace1 and an Azure Data Lake Storage Gen2 account named storage1. Workspace1 contains a lakehouse named Lakehouse1.<br>You need to create a shortcut to storage1 in Lakehouse1.<br>Which connection and endpoint should you specify? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image70\" src=\"https://img.examtopics.com/dp-600/image70.png\">",
    "options": [],
    "answer": "Box 1: abfss -<br>Box 2: dfs",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-03T12:12:00.000Z",
        "voteCount": 26,
        "content": "HTTPS, DFS\nhttps://learn.microsoft.com/en-us/fabric/onelake/create-adls-shortcut"
      },
      {
        "date": "2024-02-28T07:21:00.000Z",
        "voteCount": 7,
        "content": "It's asking to create a shortcut in the lakehouse. To do that, the URL should be https://adls.dfs.core.windows.net/file.\nTo access the shortcut in Fabric, you use the abfss path"
      },
      {
        "date": "2024-06-19T04:59:00.000Z",
        "voteCount": 1,
        "content": "I have just passed the exam, instead of 'connection' the question said: 'protocol'"
      },
      {
        "date": "2024-06-13T03:41:00.000Z",
        "voteCount": 3,
        "content": "HTTPS, DFS\nhttps://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#access"
      },
      {
        "date": "2024-06-12T23:01:00.000Z",
        "voteCount": 1,
        "content": "Connection: 2. abfss\nEndpoint: 2. dfs"
      },
      {
        "date": "2024-05-28T06:44:00.000Z",
        "voteCount": 6,
        "content": "\u2022 Connection (abfss): The abfss (Azure Blob File System Secure) protocol is used for secure connections to Azure Data Lake Storage Gen2. This protocol ensures that the connection is encrypted, providing a secure method to access the storage.\n\u2022 Endpoint (dfs): The dfs (Data Lake Storage) endpoint is used to connect to the hierarchical namespace of Azure Data Lake Storage Gen2, which is optimized for big data analytics workloads. It allows for file and directory-based operations, making it suitable for data lake scenarios."
      },
      {
        "date": "2024-03-21T08:41:00.000Z",
        "voteCount": 3,
        "content": "https, dfs\nIf you look at the tooltip in the connection settings it has the following about the endpoint, where the example shows https:\n\"The URL of the ADLSG2 endpoint to connect to. To avoid invalid credential errors, be aware to use the '.dfs' rather than '.blob' endpoint, ensure you are assigned a blob-specific role, and have the networking access set appropriately.\""
      },
      {
        "date": "2024-03-05T06:45:00.000Z",
        "voteCount": 4,
        "content": "Just tried it and when copying https://strorage.dfs.core.windows.net/ it worked so definitely http, dfs"
      },
      {
        "date": "2024-02-17T11:51:00.000Z",
        "voteCount": 4,
        "content": "Correct path exemple :\nabfss://Dev@onelake.dfs.fabric.microsoft.com/lakehouse1..."
      },
      {
        "date": "2024-02-09T15:04:00.000Z",
        "voteCount": 3,
        "content": "abfs, dfs"
      },
      {
        "date": "2024-02-17T12:02:00.000Z",
        "voteCount": 1,
        "content": "Access Azure storage\nOnce you have properly configured credentials to access your Azure storage container, you can interact with resources in the storage account using URIs. Databricks recommends using the abfss driver for greater security.\n\nPython\n\nCopy\nspark.read.load(\"abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/&lt;path-to-data&gt;\")\nhttps://learn.microsoft.com/en-us/azure/databricks/connect/storage/azure-storage"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134072-exam-dp-600-topic-1-question-51-discussion/",
    "body": "You are analyzing customer purchases in a Fabric notebook by using PySpark.<br>You have the following DataFrames:<br>transactions: Contains five columns named transaction_id, customer_id, product_id, amount, and date and has 10 million rows, with each row representing a transaction. customers: Contains customer details in 1,000 rows and three columns named customer_id, name, and country.<br>You need to join the DataFrames on the customer_id column. The solution must minimize data shuffling.<br>You write the following code.<br>from pyspark.sql import functions as F<br>results =<br>Which code should you run to populate the results DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttransactions.join(F.broadcast(customers), transactions.customer_id == customers.customer_id)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttransactions.join(customers, transactions.customer_id == customers.customer_id).distinct()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttransactions.join(customers, transactions.customer_id == customers.customer_id)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttransactions.crossJoin(customers).where(transactions.customer_id == customers.customer_id)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 39,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T12:25:00.000Z",
        "voteCount": 26,
        "content": "In Apache Spark, broadcasting refers to an optimization technique for join operations. When you join two DataFrames or RDDs and one of them is significantly smaller than the other, Spark can \"broadcast\" the smaller table to all nodes in the cluster. This approach avoids the need for network shuffles for each row of the larger table, significantly reducing the execution time of the join operation."
      },
      {
        "date": "2024-03-01T07:37:00.000Z",
        "voteCount": 7,
        "content": "A - Broadcasting generates a copy of the data across all the nodes in the Spark cluster. Therefore, during a join operation, it won't require any I/Os from other nodes, thereby, reducing the shuffling requirement."
      },
      {
        "date": "2024-05-28T07:04:00.000Z",
        "voteCount": 1,
        "content": "Broadcasting: The F.broadcast(customers) function is used to broadcast the smaller DataFrame (customers). This ensures that the smaller DataFrame is replicated across all nodes, and each node can perform the join locally with its partition of the larger DataFrame (transactions). This significantly reduces the data movement (shuffling) required during the join operation."
      },
      {
        "date": "2024-05-09T18:43:00.000Z",
        "voteCount": 3,
        "content": "IMHO, \"A\" is correct!\n\nBroadcast joining copies the smaller table to each worker in Spark, which may significantly improve performance by reducing shuffling"
      },
      {
        "date": "2024-02-18T01:31:00.000Z",
        "voteCount": 2,
        "content": "A. transactions.join(F.broadcast(customers), transactions.customer_id == customers.customer_id)\n\nOptimized method to perform a join between a very large table and a smaller one.\n\nSource: https://sparkbyexamples.com/spark/broadcast-join-in-spark/\""
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134073-exam-dp-600-topic-1-question-52-discussion/",
    "body": "HOTSPOT -<br>You have a Microsoft Power BI report and a semantic model that uses Direct Lake mode.<br>From Power BI Desktop, you open Performance analyzer as shown in the following exhibit.<br><img title=\"image71\" src=\"https://img.examtopics.com/dp-600/image71.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image72\" src=\"https://img.examtopics.com/dp-600/image72.png\">",
    "options": [],
    "answer": "<img title=\"image73\" src=\"https://img.examtopics.com/dp-600/image73.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-01T02:37:00.000Z",
        "voteCount": 22,
        "content": "The answer is Automatic and Direct Lake, actually the picture comes from\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlake-analyze-qp \nIn this article you can see there are table1 and view1, performance analyser shows: \n\u2022\tFirst card is linked to Table1 so direct lake is used\n\u2022\tSecond card is linked to View1 so it does direct query\nAs the model can use direct lake and direct query you can conclude that the fallback behavior is automatic. \nFor direct lake behavior you can read this: https://powerbi.microsoft.com/en-us/blog/leveraging-pure-direct-lake-mode-for-maximum-query-performance"
      },
      {
        "date": "2024-08-06T07:44:00.000Z",
        "voteCount": 1,
        "content": "In the link there is only a single card visual placed not two card visuals\nSee image below point\nPlace a card visual on the report canvas, select a data column to create a basic report, and then on the View menu, select Performance analyzer."
      },
      {
        "date": "2024-03-03T12:30:00.000Z",
        "voteCount": 6,
        "content": "There is no table visual in this image."
      },
      {
        "date": "2024-05-08T18:52:00.000Z",
        "voteCount": 1,
        "content": "The table visual is on the bottom of the link, it says \"processed in Direct Lake Mode.\" on top of the table."
      },
      {
        "date": "2024-06-21T08:59:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-analyze-query-processing\nDirect Lake Mode, Direct Query"
      },
      {
        "date": "2024-05-28T07:16:00.000Z",
        "voteCount": 1,
        "content": "1. The Direct Lake fallback behavior is set to: DirectQueryOnly\nThe Performance analyzer shows that the query type is \"Direct query\". This indicates that the fallback behavior is set to only use DirectQuery and not Direct Lake. If the behavior were set to \"DirectLakeOnly\", the query would fail if Direct Lake could not be used. \"Automatic\" would use Direct Lake when possible and fall back to DirectQuery, but since it's specifically showing \"Direct query\", it suggests \"DirectQueryOnly\".\n\n2.The query for the table visual is executed by using: Direct Query\nThe Performance analyzer directly mentions \"Direct query\" as the query type for the visual. This confirms that the data is being retrieved using DirectQuery mode, not Direct Lake or any composite model."
      },
      {
        "date": "2024-05-25T06:40:00.000Z",
        "voteCount": 5,
        "content": "Asnwer: Automatic - DirectLake.\n the image is cut bad. if you follow this link: https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-analyze-query-processing you can see that in Data section of Power BI they are using two different tables"
      },
      {
        "date": "2024-06-10T16:35:00.000Z",
        "voteCount": 1,
        "content": "In the link provided says: If the semantic model falls back to DirectQuery mode to process the visual\u2019s DAX query, you see a \"Direct query\" performance metric, as shown in the following image (the image of question). Is that correct?"
      },
      {
        "date": "2024-05-09T18:53:00.000Z",
        "voteCount": 2,
        "content": "IMHO, Automatic &amp; DirectQuery"
      },
      {
        "date": "2024-05-09T18:54:00.000Z",
        "voteCount": 1,
        "content": "sorry, Automatic &amp; Direct Lake"
      },
      {
        "date": "2024-03-05T06:54:00.000Z",
        "voteCount": 3,
        "content": "The question itself is uniquely confusing and ambiguous and at times simply wrong, but I would go with Automatic and DirectQuery bc DAX falls back to DirectQuery always (it is a known shortcoming --&gt; all explained here: https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-overview)"
      },
      {
        "date": "2024-03-05T07:00:00.000Z",
        "voteCount": 5,
        "content": "Additionally, this link (https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-analyze-qp) explicitly says the following: If the dataset falls back to DirectQuery mode to process the visual\u2019s DAX query, you see a Direct query performance metric, as shown in the following image --&gt; and the image is the exact same as in the question."
      },
      {
        "date": "2024-03-01T08:25:00.000Z",
        "voteCount": 4,
        "content": "Automatic, DirectLake\nIMO, as the fallback method is not explicitly mentioned, by default any DirectLake query falls. back to DirectQuery, hence, it is automatic.\nAfter the changes have been made, the query method has fallen back to DirectQuery, hence, I would choose DirectQuery.\nWould be great to know further opinion into this."
      },
      {
        "date": "2024-02-17T12:40:00.000Z",
        "voteCount": 3,
        "content": "I think automatic and direct lake : we have fallback direct query for card and no informations about table visual so its direct lake."
      },
      {
        "date": "2024-03-29T04:21:00.000Z",
        "voteCount": 3,
        "content": "The final block of card shows Direct Query. I think the wording was wrong as there are no tables shown. So I suggest automatic and Direct query as noted by others."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/133739-exam-dp-600-topic-1-question-53-discussion/",
    "body": "HOTSPOT -<br>You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a table named Nyctaxi_raw. Nyctaxi_row contains the following table:<br><img title=\"image75\" src=\"https://img.examtopics.com/dp-600/image75.png\"><br>You create a Fabric notebook and attach it to Lakehouse1.<br>You need to use PySpark code to transform the data. The solution must meet the following requirements:<br>Add a column named pickupDate that will contain only the date portion of pickupDateTime.<br>Filter the DataFrame to include only rows where fareAmount is a positive number that is less than 100.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img title=\"image76\" src=\"https://img.examtopics.com/dp-600/image76.png\">",
    "options": [],
    "answer": "<img title=\"image77\" src=\"https://img.examtopics.com/dp-600/image77.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-27T10:16:00.000Z",
        "voteCount": 27,
        "content": "The correct answers:\n1. withColumn\n2. cast('date')\n3. .filter('fareAmount......."
      },
      {
        "date": "2024-02-17T13:38:00.000Z",
        "voteCount": 8,
        "content": "We need to add column not rename existing column. Here is the correct answer :\ndf.withColumn('pickupDate', df['pickupDateTime'].cast(DateType())) \\\n             .filter(\"fareAmount &gt; 0 AND fareAmount &lt; 100\")"
      },
      {
        "date": "2024-08-13T07:55:00.000Z",
        "voteCount": 1,
        "content": "Tested answer\ndf = spark.read.format(\"delta\").load(\"Tables/factinternetsales\")\ndf2 = df.withColumn(\"pickupdate\", df['pickupDateTime'].cast('date')).filter(\"fareAmount &gt; 0 AND fareAmount &lt; 100\")\ndf2.show()"
      },
      {
        "date": "2024-06-09T10:35:00.000Z",
        "voteCount": 7,
        "content": "The correct answers:\n1. withColumn\n2. cast('date')\n3. .filter('fareAmount\nTested!!!"
      },
      {
        "date": "2024-05-20T06:18:00.000Z",
        "voteCount": 1,
        "content": "why the expression with where is not correct? Is it because it includes 100? not less than 100?"
      },
      {
        "date": "2024-05-12T20:24:00.000Z",
        "voteCount": 1,
        "content": ".filter(fareamount &gt;0 and &lt;100) does not work, I tried the code myself. You have to use same condition but with \"col\" like this  .filter(col(fareamount) &gt; 0 and col(fareamount) &lt; 100)"
      },
      {
        "date": "2024-05-09T19:18:00.000Z",
        "voteCount": 5,
        "content": "IMHO, \nwithColumn -&gt; cast(dateType()) -&gt; filter(\"fareAmount &gt; 0 AND fareAmount &lt; 100\")\nis correct.\n\nAs colleagues said, cast(dateType()), not cast('date'). Let's consider it a typo from Microsoft."
      },
      {
        "date": "2024-02-24T13:28:00.000Z",
        "voteCount": 7,
        "content": "A bit incorrect.\n.withcolumn\n.cast(date)\n.filter(fareamount &gt;0 and &lt;100)"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134075-exam-dp-600-topic-1-question-54-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a new semantic model in OneLake.<br>You use a Fabric notebook to read the data into a Spark DataFrame.<br>You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.<br>Solution: You use the following PySpark expression:<br>df.explain()<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T07:31:00.000Z",
        "voteCount": 6,
        "content": "The df.explain() method in PySpark is used to print the logical and physical plans of a DataFrame, which helps in understanding how Spark plans to execute the query. It does not compute any statistical values like min, max, mean, or standard deviation.\n**To achieve the goal, you should use: df.describe().show()"
      },
      {
        "date": "2024-05-09T19:21:00.000Z",
        "voteCount": 1,
        "content": "IMHO, NOOO\n\nexplain() shows the execution plan..."
      },
      {
        "date": "2024-03-21T09:15:00.000Z",
        "voteCount": 2,
        "content": "describe is how you get the information."
      },
      {
        "date": "2024-02-18T01:42:00.000Z",
        "voteCount": 4,
        "content": "The correct syntax is df.describe().\n\nSources:\n* describe --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html\n* explain --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.explain.html"
      },
      {
        "date": "2024-02-18T01:46:00.000Z",
        "voteCount": 1,
        "content": "Also df.summary() is a valid solution.\n\nSource ---&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html"
      },
      {
        "date": "2024-02-17T13:43:00.000Z",
        "voteCount": 4,
        "content": "No explain is for the execut plan"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134076-exam-dp-600-topic-1-question-55-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a new semantic model in OneLake.<br>You use a Fabric notebook to read the data into a Spark DataFrame.<br>You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.<br>Solution: You use the following PySpark expression:<br>df.show()<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T07:33:00.000Z",
        "voteCount": 3,
        "content": "Correct methods: Use df.describe().show() for basic statistics and df.agg() with appropriate functions (min, max, mean, stddev) for detailed statistics."
      },
      {
        "date": "2024-05-09T19:22:00.000Z",
        "voteCount": 2,
        "content": "IMHO, NOOOOO\n\ndf.show() - shows the data in the dataframe"
      },
      {
        "date": "2024-03-21T09:16:00.000Z",
        "voteCount": 2,
        "content": "Use describe"
      },
      {
        "date": "2024-02-27T09:57:00.000Z",
        "voteCount": 1,
        "content": "df.summary() is the only right answer."
      },
      {
        "date": "2024-02-18T01:43:00.000Z",
        "voteCount": 1,
        "content": "The correct syntax is df.describe().\n\nSources:\n* describe --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html\n* show --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.show.html"
      },
      {
        "date": "2024-02-18T01:45:00.000Z",
        "voteCount": 1,
        "content": "Also df.summary() is a valid solution.\n\nSource ---&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html"
      },
      {
        "date": "2024-02-17T13:44:00.000Z",
        "voteCount": 1,
        "content": "No show is to display data"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134077-exam-dp-600-topic-1-question-56-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a new semantic model in OneLake.<br>You use a Fabric notebook to read the data into a Spark DataFrame.<br>You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.<br>Solution: You use the following PySpark expression:<br>df.summary()<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-07-30T15:08:00.000Z",
        "voteCount": 1,
        "content": "Using df.summary() in PySpark will provide summary statistics, including min, max, mean, and standard deviation for all numeric columns. However, it will not provide these statistics for string columns since summary statistics like min, max, mean, and standard deviation are not applicable to string data."
      },
      {
        "date": "2024-09-03T02:06:00.000Z",
        "voteCount": 1,
        "content": "so the questions doesn't make sense if you are asked to calculate things that aren't defined"
      },
      {
        "date": "2024-07-12T17:49:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-07-08T11:44:00.000Z",
        "voteCount": 2,
        "content": "In pandas, use df.describe() for summary statistics of numeric columns.\nIn PySpark, use df.summary() for summary statistics of both numeric and string columns in a distributed computing environment."
      },
      {
        "date": "2024-05-28T07:36:00.000Z",
        "voteCount": 4,
        "content": "while df.summary() does provide valuable information for numeric columns, it does not fully meet the goal of evaluating both string and numeric columns with the required statistical measures. Use df.summary() and df.agg() to cover numeric columns, and additional custom aggregations for string columns."
      },
      {
        "date": "2024-05-09T19:25:00.000Z",
        "voteCount": 4,
        "content": "IMHO, A\n\nExample:\ndf1 = spark.createDataFrame([(1, 10), (2, 10), (2, 15)], schema = ['fruit_id', 'amount'])\ndf1.summary()\n\nsummary\tfruit_id\tamount\ncount\t3\t3\nmean\t1.6666666666666667\t11.666666666666666\nstddev\t0.5773502691896257\t2.886751345948129\nmin\t1\t10\n25%\t1\t10\n50%\t2\t10\n75%\t2\t15\nmax\t2\t15"
      },
      {
        "date": "2024-02-27T09:56:00.000Z",
        "voteCount": 1,
        "content": "df.summary() is the only option where you can get MIX, MAX and AVG"
      },
      {
        "date": "2024-02-18T01:47:00.000Z",
        "voteCount": 4,
        "content": "Also df.describe() is a valid solution.\n\nSources:\n* summary --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.summary.html\n* describe --&gt; https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/api/pyspark.sql.DataFrame.describe.html"
      },
      {
        "date": "2024-02-17T13:44:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134079-exam-dp-600-topic-1-question-57-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.<br>When you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.<br>You need to identify whether maintenance tasks were performed on Customer.<br>Solution: You run the following Spark SQL statement:<br><br>DESCRIBE HISTORY customer -<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-10-06T03:27:00.000Z",
        "voteCount": 1,
        "content": "OPTIMIZE and VACUUM activities in the last 30 days are stored in the history. So, yes, you can see using this query whether maintenance took place in the last 30 days. https://learn.microsoft.com/en-us/azure/databricks/delta/history"
      },
      {
        "date": "2024-05-28T07:39:00.000Z",
        "voteCount": 3,
        "content": "Yes, running DESCRIBE HISTORY customer meets the goal of identifying whether maintenance tasks were performed on the Delta table."
      },
      {
        "date": "2024-05-09T19:32:00.000Z",
        "voteCount": 3,
        "content": "IMHO, A\n\nLink: https://learn.microsoft.com/en-us/azure/databricks/delta/history"
      },
      {
        "date": "2024-03-21T09:24:00.000Z",
        "voteCount": 2,
        "content": "A correct.\ndisplay(spark.sql('describe history customer'))"
      },
      {
        "date": "2024-02-27T09:58:00.000Z",
        "voteCount": 2,
        "content": "Right answer. Describe HISTORY"
      },
      {
        "date": "2024-02-17T14:07:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134080-exam-dp-600-topic-1-question-58-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.<br>When you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.<br>You need to identify whether maintenance tasks were performed on Customer.<br>Solution: You run the following Spark SQL statement:<br><br>REFRESH TABLE customer -<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T07:41:00.000Z",
        "voteCount": 3,
        "content": "No, running REFRESH TABLE customer does not meet the goal of identifying whether maintenance tasks were performed on the Delta table. This Spark SQL command is used to refresh the metadata of a table. It ensures that the latest schema and data are available for queries but does not give any historical information about maintenance operations."
      },
      {
        "date": "2024-05-09T19:32:00.000Z",
        "voteCount": 1,
        "content": "IMHO, NOOO"
      },
      {
        "date": "2024-04-28T00:57:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-02-17T14:10:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134083-exam-dp-600-topic-1-question-59-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.<br>When you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.<br>You need to identify whether maintenance tasks were performed on Customer.<br>Solution: You run the following Spark SQL statement:<br><br>EXPLAIN TABLE customer -<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T07:43:00.000Z",
        "voteCount": 3,
        "content": "The EXPLAIN statement in Spark SQL is used to display the execution plan of a query. This plan shows how Spark will execute the query, including details about the operations and stages involved. While it is useful for understanding and optimizing query performance, it does not provide historical information about maintenance tasks like optimization, compaction, or vacuuming performed on the table."
      },
      {
        "date": "2024-05-09T19:33:00.000Z",
        "voteCount": 1,
        "content": "IMHO, NOOOO"
      },
      {
        "date": "2024-03-05T14:36:00.000Z",
        "voteCount": 2,
        "content": "Good luck !!!"
      },
      {
        "date": "2024-02-29T18:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is DESCRIBE HISTORY customer. \nGood luck everyone."
      },
      {
        "date": "2024-02-24T13:21:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-02-17T14:16:00.000Z",
        "voteCount": 2,
        "content": "Given answer is correct. Explain is for query execution plan"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139283-exam-dp-600-topic-1-question-60-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br><br>Existing Environment -<br><br><br>Fabric Environment -<br><br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br><br>Available Data -<br><br>Litware has data that must be analyzed as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image78.png\"><br><br>The Product data contains a single table and the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image79.png\"><br><br>The customer satisfaction data contains the following tables:<br><br>\u2022\tSurvey<br>\u2022\tQuestion<br>\u2022\tResponse<br><br>For each survey submitted, the following occurs:<br><br>\u2022\tOne row is added to the Survey table.<br>\u2022\tOne row is added to the Response table for each question in the survey.<br><br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br><br>User Problems -<br><br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br><br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br><br>The following three workspaces will be created:<br><br>\u2022\tAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>\u2022\tDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>\u2022\tDataSciPOC: Will contain all the notebooks and reports created by the data scientists<br><br>The following will be created in the AnalyticsPOC workspace:<br><br>\u2022\tA data store (type to be decided)<br>\u2022\tA custom semantic model<br>\u2022\tA default semantic model<br>\u2022\tInteractive reports<br><br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br><br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br><br>Technical Requirements -<br><br>The data store must support the following:<br><br>\u2022\tRead access by using T-SQL or Python<br>\u2022\tSemi-structured and unstructured data<br>\u2022\tRow-level security (RLS) for users executing T-SQL queries<br><br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br><br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br><br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br><br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br><br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br><br>\u2022\tList prices that are less than or equal to 50 are in the low pricing group.<br>\u2022\tList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>\u2022\tList prices that are greater than 1,000 are in the high pricing group.<br><br><br>Security Requirements -<br><br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br><br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br><br>\u2022\tFabric administrators will be the workspace administrators.<br>\u2022\tThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>\u2022\tThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>\u2022\tThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>\u2022\tThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>\u2022\tThe date dimension must be available to all users of the data store.<br>\u2022\tThe principle of least privilege must be followed.<br><br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br><br>\u2022\tFabricAdmins: Fabric administrators<br>\u2022\tAnalyticsTeam: All the members of the analytics team<br>\u2022\tDataAnalysts: The data analysts on the analytics team<br>\u2022\tDataScientists: The data scientists on the analytics team<br>\u2022\tDataEngineers: The data engineers on the analytics team<br>\u2022\tAnalyticsEngineers: The analytics engineers on the analytics team<br><br><br>Report Requirements -<br><br>The data analysts must create a customer satisfaction report that meets the following requirements:<br><br>\u2022\tEnables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>\u2022\tDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>\u2022\tShows data as soon as the data is updated in the data store.<br>\u2022\tEnsures that the report and the semantic model only contain data from the current and previous year.<br>\u2022\tEnsures that the report respects any table-level security specified in the source data store.<br>\u2022\tMinimizes the execution time of report queries.<br><br>You need to recommend a solution to prepare the tenant for the PoC.<br><br>Which two actions should you recommend performing from the Fabric Admin portal? Each correct answer presents part of the solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Users can try Microsoft Fabric paid features option for the entire organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Users can try Microsoft Fabric paid features option for specific security groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Allow Azure Active Directory guest users to access Microsoft Fabric option for specific security groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Users can create Fabric items option and exclude specific security groups.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Users can create Fabric items option for specific security groups.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-07T08:05:00.000Z",
        "voteCount": 6,
        "content": "Enable the Users can try Microsoft Fabric paid features option for specific security groups: This will allow specific security groups (like the AnalyticsTeam, DataAnalysts, DataScientists, DataEngineers, and AnalyticsEngineers) to access and use the paid features of Microsoft Fabric necessary for the PoC. This is important to ensure that only the relevant team members can utilize these advanced features while preventing unnecessary access for other users.\nEnable the Users can create Fabric items option for specific security groups: This will allow only specific security groups to create Fabric items, ensuring that the creation of these items is controlled and managed by the appropriate team members. This helps maintain the principle of least privilege and ensures that only authorized personnel can create and manage Fabric items during the PoC."
      },
      {
        "date": "2024-05-09T19:43:00.000Z",
        "voteCount": 2,
        "content": "IMHO, B &amp; E looks good.\n\nHere, we use only for \"specific groups\", but not \"C\" which is for guests."
      },
      {
        "date": "2024-04-23T06:23:00.000Z",
        "voteCount": 2,
        "content": "I agree with the answer. The POC is only for a certain group of users."
      },
      {
        "date": "2024-04-21T02:07:00.000Z",
        "voteCount": 2,
        "content": "This is a POC with internal users, B &amp; E make sense on the principle of least privilege."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138090-exam-dp-600-topic-1-question-61-discussion/",
    "body": "HOTSPOT<br> -<br><br><br>Case study<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br><br>Existing Environment<br> -<br><br><br>Fabric Environment<br> -<br><br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br><br>Available Data<br> -<br><br>Litware has data that must be analyzed as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image78.png\"><br><br>The Product data contains a single table and the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image79.png\"><br><br>The customer satisfaction data contains the following tables:<br><br>\u2022\tSurvey<br>\u2022\tQuestion<br>\u2022\tResponse<br><br>For each survey submitted, the following occurs:<br><br>\u2022\tOne row is added to the Survey table.<br>\u2022\tOne row is added to the Response table for each question in the survey.<br><br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br><br>User Problems<br> -<br><br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br><br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br><br>The following three workspaces will be created:<br><br>\u2022\tAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>\u2022\tDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>\u2022\tDataSciPOC: Will contain all the notebooks and reports created by the data scientists<br><br>The following will be created in the AnalyticsPOC workspace:<br><br>\u2022\tA data store (type to be decided)<br>\u2022\tA custom semantic model<br>\u2022\tA default semantic model<br>\u2022\tInteractive reports<br><br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br><br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br><br>Technical Requirements<br> -<br><br>The data store must support the following:<br><br>\u2022\tRead access by using T-SQL or Python<br>\u2022\tSemi-structured and unstructured data<br>\u2022\tRow-level security (RLS) for users executing T-SQL queries<br><br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br><br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br><br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br><br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br><br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br><br>\u2022\tList prices that are less than or equal to 50 are in the low pricing group.<br>\u2022\tList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>\u2022\tList prices that are greater than 1,000 are in the high pricing group.<br><br><br>Security Requirements<br> -<br><br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br><br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br><br>\u2022\tFabric administrators will be the workspace administrators.<br>\u2022\tThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>\u2022\tThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>\u2022\tThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>\u2022\tThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>\u2022\tThe date dimension must be available to all users of the data store.<br>\u2022\tThe principle of least privilege must be followed.<br><br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br><br>\u2022\tFabricAdmins: Fabric administrators<br>\u2022\tAnalyticsTeam: All the members of the analytics team<br>\u2022\tDataAnalysts: The data analysts on the analytics team<br>\u2022\tDataScientists: The data scientists on the analytics team<br>\u2022\tDataEngineers: The data engineers on the analytics team<br>\u2022\tAnalyticsEngineers: The analytics engineers on the analytics team<br><br><br>Report Requirements<br> -<br><br>The data analysts must create a customer satisfaction report that meets the following requirements:<br><br>\u2022\tEnables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>\u2022\tDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>\u2022\tShows data as soon as the data is updated in the data store.<br>\u2022\tEnsures that the report and the semantic model only contain data from the current and previous year.<br>\u2022\tEnsures that the report respects any table-level security specified in the source data store.<br>\u2022\tMinimizes the execution time of report queries.<br><br><br>You need to design a semantic model for the customer satisfaction report.<br><br>Which data source authentication method and mode should you use? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image80.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image81.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-08T22:47:00.000Z",
        "voteCount": 10,
        "content": "Direct Lake also supports row-level security and object-level security so that users only see the data they have permission to see.\nhttps://learn.microsoft.com/es-es/power-bi/enterprise/directlake-overview"
      },
      {
        "date": "2024-09-09T06:56:00.000Z",
        "voteCount": 1,
        "content": "It's also saying \"The analytics team has large volumes of data, some of which is semi-structured\",as far as we know Direct Query can't handle semi-structured data, but DirectLake as well."
      },
      {
        "date": "2024-06-01T22:39:00.000Z",
        "voteCount": 5,
        "content": "I am hesitant... I wouldn't say that Direct Lake mode is super obvious here. Direct Lake does support RLS/OBS and you can configure it via web interface... BUT! From what I understood, if data source (e.g. database) has RLS/OBS configured, semantic model will fall back to Direct Query and obey RLS/OBS at data source.\nAm I wrong where?"
      },
      {
        "date": "2024-08-28T20:20:00.000Z",
        "voteCount": 1,
        "content": "Where does it say the the Customer Survey Report needs RLS or am I missing something? I see that the report needs Table-level security which is in DirectQuery mode. \nIMHO, Service Principle and DirectQuery are the correct answers."
      },
      {
        "date": "2024-06-15T05:27:00.000Z",
        "voteCount": 2,
        "content": "I'll select: SSO and Direct lake\nhttps://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview#single-sign-on-sso-enabled-by-default"
      },
      {
        "date": "2024-05-19T23:59:00.000Z",
        "voteCount": 3,
        "content": "SSO and Direct Lake"
      },
      {
        "date": "2024-05-09T19:56:00.000Z",
        "voteCount": 2,
        "content": "IMHO, Service Principal &amp; Direct Lake looks good.\n\nAnd yes, Direct Lake supports RLS, proof is here: \nDirect Lake also supports row-level security and object-level security so users only see the data they have permission to see.\nLink: https://learn.microsoft.com/en-us/fabric/get-started/direct-lake-overview"
      },
      {
        "date": "2024-05-14T10:53:00.000Z",
        "voteCount": 6,
        "content": "According to the link you shared, it says \"By default, Direct Lake models use single sign-on (SSO), so the effective permissions of the interactive user determine if the user is allowed or denied access to the data. If the Direct Lake model is configured to use a fixed identity, the effective permission of the fixed identity determines if users interacting with the semantic model can access the data.\"\nTherefore, why do you think that the authentication method is Service principal (fixed identity) instead of SSO (interactive user)?."
      },
      {
        "date": "2024-04-08T23:06:00.000Z",
        "voteCount": 4,
        "content": "it should be service principle and direct lake.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/directlake-fixed-identity"
      },
      {
        "date": "2024-04-07T11:25:00.000Z",
        "voteCount": 2,
        "content": "Direct lake wont support RLS. so it would be DQ"
      },
      {
        "date": "2024-04-28T08:09:00.000Z",
        "voteCount": 6,
        "content": "Direct lake supports RLS"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137913-exam-dp-600-topic-1-question-62-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br><br>Existing Environment -<br><br><br>Fabric Environment -<br><br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br><br>Available Data -<br><br>Litware has data that must be analyzed as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image78.png\"><br><br>The Product data contains a single table and the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image79.png\"><br><br>The customer satisfaction data contains the following tables:<br><br>\u2022\tSurvey<br>\u2022\tQuestion<br>\u2022\tResponse<br><br>For each survey submitted, the following occurs:<br><br>\u2022\tOne row is added to the Survey table.<br>\u2022\tOne row is added to the Response table for each question in the survey.<br><br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br><br>User Problems -<br><br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br><br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br><br>The following three workspaces will be created:<br><br>\u2022\tAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>\u2022\tDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>\u2022\tDataSciPOC: Will contain all the notebooks and reports created by the data scientists<br><br>The following will be created in the AnalyticsPOC workspace:<br><br>\u2022\tA data store (type to be decided)<br>\u2022\tA custom semantic model<br>\u2022\tA default semantic model<br>\u2022\tInteractive reports<br><br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br><br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br><br>Technical Requirements -<br><br>The data store must support the following:<br><br>\u2022\tRead access by using T-SQL or Python<br>\u2022\tSemi-structured and unstructured data<br>\u2022\tRow-level security (RLS) for users executing T-SQL queries<br><br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br><br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br><br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br><br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br><br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br><br>\u2022\tList prices that are less than or equal to 50 are in the low pricing group.<br>\u2022\tList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>\u2022\tList prices that are greater than 1,000 are in the high pricing group.<br><br><br>Security Requirements -<br><br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br><br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br><br>\u2022\tFabric administrators will be the workspace administrators.<br>\u2022\tThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>\u2022\tThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>\u2022\tThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>\u2022\tThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>\u2022\tThe date dimension must be available to all users of the data store.<br>\u2022\tThe principle of least privilege must be followed.<br><br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br><br>\u2022\tFabricAdmins: Fabric administrators<br>\u2022\tAnalyticsTeam: All the members of the analytics team<br>\u2022\tDataAnalysts: The data analysts on the analytics team<br>\u2022\tDataScientists: The data scientists on the analytics team<br>\u2022\tDataEngineers: The data engineers on the analytics team<br>\u2022\tAnalyticsEngineers: The analytics engineers on the analytics team<br><br><br>Report Requirements -<br><br>The data analysts must create a customer satisfaction report that meets the following requirements:<br><br>\u2022\tEnables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>\u2022\tDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>\u2022\tShows data as soon as the data is updated in the data store.<br>\u2022\tEnsures that the report and the semantic model only contain data from the current and previous year.<br>\u2022\tEnsures that the report respects any table-level security specified in the source data store.<br>\u2022\tMinimizes the execution time of report queries.<br><br><br>You need to implement the date dimension in the data store. The solution must meet the technical requirements.<br><br>What are two ways to achieve the goal? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the date dimension table by using a dataflow.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the date dimension table by using a Copy activity in a pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the date dimension view by using T-SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the date dimension table by using a Stored procedure activity in a pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T11:32:00.000Z",
        "voteCount": 24,
        "content": "as per the technical requirements - \n\"The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.\"\n\n\nNo existing data source &amp; The date dimension must always contain dates from 2010 through the end of the current year is the Key Point here\n\n\nusing the elimination method\nView wont be appropriate and COPY activity cant be used since there is no data source for DATE table. \n\nso the answer is A and D"
      },
      {
        "date": "2024-05-20T06:11:00.000Z",
        "voteCount": 8,
        "content": "C,D.\nI always create dimDate using a views (T-Sql select statement with recursive CTE).\nA. is too complicated."
      },
      {
        "date": "2024-06-07T00:38:00.000Z",
        "voteCount": 1,
        "content": "I am not able to create a view with CTE. Could you let us know how?"
      },
      {
        "date": "2024-09-18T07:24:00.000Z",
        "voteCount": 2,
        "content": "It is specified: \"The data store must support the following: Semi-structured and unstructured data\".\nSo the data store is a lakehouse.\nStored procedures don't exist in a lakehouse.\nI think A and C"
      },
      {
        "date": "2024-07-25T13:10:00.000Z",
        "voteCount": 1,
        "content": "C, D \nit says POPULATE, not CREATE... you cannot create tables with SPs in a lakehouse, BUT you CAN populate already existing ones!"
      },
      {
        "date": "2024-06-19T14:15:00.000Z",
        "voteCount": 1,
        "content": "ChatGDP chose AB. It says \"Copy activity in tools like Azure Data Factory (ADF) allows you to copy data from various sources to your data warehouse. For a date dimension, you can generate the required dates using a source query or script and use the Copy activity to load them into your dimension table.\" and \"While using a stored procedure can also achieve this, it typically requires more complex management and might not be as straightforward or flexible as using dataflows or copy activities, which are specifically designed for ETL (Extract, Transform, Load) processes.\""
      },
      {
        "date": "2024-06-16T14:38:00.000Z",
        "voteCount": 1,
        "content": "correction: ChatGDP says A and C"
      },
      {
        "date": "2024-06-16T14:37:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says the answer should be A and D :)"
      },
      {
        "date": "2024-06-15T05:36:00.000Z",
        "voteCount": 2,
        "content": "I prefered A &amp; C. B&amp;D need more steps to be created"
      },
      {
        "date": "2024-07-11T10:00:00.000Z",
        "voteCount": 1,
        "content": "I agree"
      },
      {
        "date": "2024-06-12T10:50:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D looks correct. A will be completed. C would need a new object to be created, rather we can simply use a one time script."
      },
      {
        "date": "2024-05-31T22:11:00.000Z",
        "voteCount": 1,
        "content": "A and D should be the right answer to go."
      },
      {
        "date": "2024-05-12T12:55:00.000Z",
        "voteCount": 3,
        "content": "IMHO, AD may be good\n\nAgree, B is not good, C is just a weird way. But possible.\nSo, the question is kind of ambiguous, with no particular directions from the Microsoft site. Among ABD, I am choosing AD"
      },
      {
        "date": "2024-05-12T12:56:00.000Z",
        "voteCount": 2,
        "content": "Sorry, among ACD, I am choosing AD. That's what I meant"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139284-exam-dp-600-topic-1-question-63-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Litware, Inc. is a manufacturing company that has offices throughout North America. The analytics team at Litware contains data engineers, analytics engineers, data analysts, and data scientists.<br><br><br>Existing Environment -<br><br><br>Fabric Environment -<br><br>Litware has been using a Microsoft Power BI tenant for three years. Litware has NOT enabled any Fabric capacities and features.<br><br><br>Available Data -<br><br>Litware has data that must be analyzed as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image78.png\"><br><br>The Product data contains a single table and the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image79.png\"><br><br>The customer satisfaction data contains the following tables:<br><br>\u2022\tSurvey<br>\u2022\tQuestion<br>\u2022\tResponse<br><br>For each survey submitted, the following occurs:<br><br>\u2022\tOne row is added to the Survey table.<br>\u2022\tOne row is added to the Response table for each question in the survey.<br><br>The Question table contains the text of each survey question. The third question in each survey response is an overall satisfaction score. Customers can submit a survey after each purchase.<br><br><br>User Problems -<br><br>The analytics team has large volumes of data, some of which is semi-structured. The team wants to use Fabric to create a new data store.<br><br>Product data is often classified into three pricing groups: high, medium, and low. This logic is implemented in several databases and semantic models, but the logic does NOT always match across implementations.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Litware plans to enable Fabric features in the existing tenant. The analytics team will create a new data store as a proof of concept (PoC). The remaining Liware users will only get access to the Fabric features once the PoC is complete. The PoC will be completed by using a Fabric trial capacity<br><br>The following three workspaces will be created:<br><br>\u2022\tAnalyticsPOC: Will contain the data store, semantic models, reports pipelines, dataflow, and notebooks used to populate the data store<br>\u2022\tDataEngPOC: Will contain all the pipelines, dataflows, and notebooks used to populate OneLake<br>\u2022\tDataSciPOC: Will contain all the notebooks and reports created by the data scientists<br><br>The following will be created in the AnalyticsPOC workspace:<br><br>\u2022\tA data store (type to be decided)<br>\u2022\tA custom semantic model<br>\u2022\tA default semantic model<br>\u2022\tInteractive reports<br><br>The data engineers will create data pipelines to load data to OneLake either hourly or daily depending on the data source. The analytics engineers will create processes to ingest, transform, and load the data to the data store in the AnalyticsPOC workspace daily. Whenever possible, the data engineers will use low-code tools for data ingestion. The choice of which data cleansing and transformation tools to use will be at the data engineers\u2019 discretion.<br><br>All the semantic models and reports in the Analytics POC workspace will use the data store as the sole data source.<br><br><br>Technical Requirements -<br><br>The data store must support the following:<br><br>\u2022\tRead access by using T-SQL or Python<br>\u2022\tSemi-structured and unstructured data<br>\u2022\tRow-level security (RLS) for users executing T-SQL queries<br><br>Files loaded by the data engineers to OneLake will be stored in the Parquet format and will meet Delta Lake specifications.<br><br>Data will be loaded without transformation in one area of the AnalyticsPOC data store. The data will then be cleansed, merged, and transformed into a dimensional model<br><br>The data load process must ensure that the raw and cleansed data is updated completely before populating the dimensional model<br><br>The dimensional model must contain a date dimension. There is no existing data source for the date dimension. The Litware fiscal year matches the calendar year. The date dimension must always contain dates from 2010 through the end of the current year.<br><br>The product pricing group logic must be maintained by the analytics engineers in a single location. The pricing group data must be made available in the data store for T-SOL. queries and in the default semantic model. The following logic must be used:<br><br>\u2022\tList prices that are less than or equal to 50 are in the low pricing group.<br>\u2022\tList prices that are greater than 50 and less than or equal to 1,000 are in the medium pricing group.<br>\u2022\tList prices that are greater than 1,000 are in the high pricing group.<br><br><br>Security Requirements -<br><br>Only Fabric administrators and the analytics team must be able to see the Fabric items created as part of the PoC.<br><br>Litware identifies the following security requirements for the Fabric items in the AnalyticsPOC workspace:<br><br>\u2022\tFabric administrators will be the workspace administrators.<br>\u2022\tThe data engineers must be able to read from and write to the data store. No access must be granted to datasets or reports.<br>\u2022\tThe analytics engineers must be able to read from, write to, and create schemas in the data store. They also must be able to create and share semantic models with the data analysts and view and modify all reports in the workspace.<br>\u2022\tThe data scientists must be able to read from the data store, but not write to it. They will access the data by using a Spark notebook<br>\u2022\tThe data analysts must have read access to only the dimensional model objects in the data store. They also must have access to create Power BI reports by using the semantic models created by the analytics engineers.<br>\u2022\tThe date dimension must be available to all users of the data store.<br>\u2022\tThe principle of least privilege must be followed.<br><br>Both the default and custom semantic models must include only tables or views from the dimensional model in the data store. Litware already has the following Microsoft Entra security groups:<br><br>\u2022\tFabricAdmins: Fabric administrators<br>\u2022\tAnalyticsTeam: All the members of the analytics team<br>\u2022\tDataAnalysts: The data analysts on the analytics team<br>\u2022\tDataScientists: The data scientists on the analytics team<br>\u2022\tDataEngineers: The data engineers on the analytics team<br>\u2022\tAnalyticsEngineers: The analytics engineers on the analytics team<br><br><br>Report Requirements -<br><br>The data analysts must create a customer satisfaction report that meets the following requirements:<br><br>\u2022\tEnables a user to select a product to filter customer survey responses to only those who have purchased that product.<br>\u2022\tDisplays the average overall satisfaction score of all the surveys submitted during the last 12 months up to a selected dat.<br>\u2022\tShows data as soon as the data is updated in the data store.<br>\u2022\tEnsures that the report and the semantic model only contain data from the current and previous year.<br>\u2022\tEnsures that the report respects any table-level security specified in the source data store.<br>\u2022\tMinimizes the execution time of report queries.<br><br><br>You need to ensure the data loading activities in the AnalyticsPOC workspace are executed in the appropriate sequence. The solution must meet the technical requirements.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataflow that has multiple steps and schedule the dataflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and schedule a Spark notebook.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and schedule a Spark job definition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pipeline that has dependencies between activities and schedule the pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-04T22:30:00.000Z",
        "voteCount": 2,
        "content": "D is correct.\n\nThe technical requirements specify that the data loading process must ensure the raw and cleansed data is updated completely before populating the dimensional model. This suggests that the data loading process involves multiple steps that need to be executed in a specific order.\n\nUsing a pipeline with dependencies between activities is the best approach to achieve this. A pipeline allows you to create a sequence of activities, with each activity dependent on the successful completion of the previous one. This ensures the data loading activities are executed in the appropriate order, meeting the technical requirements."
      },
      {
        "date": "2024-05-12T13:04:00.000Z",
        "voteCount": 2,
        "content": "IMHO, D is correct.\n\nI love Microsoft, ambiguity as is. But yes, generally, according to Microsoft BP, a pipeline in ADF with running all activities is the suggested solution. From it, it is easy to trigger different notebooks, for example"
      },
      {
        "date": "2024-04-21T02:43:00.000Z",
        "voteCount": 2,
        "content": "D is correct. Pipeline can ensure the activities follow the required sequence.\nhttps://learn.microsoft.com/en-us/fabric/data-factory/activity-overview#data-transformation-activities"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137927-exam-dp-600-topic-1-question-64-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br><br>Existing Environment -<br><br><br>Identity Environment -<br><br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br><br>Data Environment -<br><br>Contoso has the following data environment:<br><br>\u2022\tThe Sales division uses a Microsoft Power BI Premium capacity.<br>\u2022\tThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>\u2022\tThe Research department uses an on-premises, third-party data warehousing product.<br>\u2022\tFabric is enabled for contoso.com.<br>\u2022\tAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>\u2022\tA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Contoso plans to make the following changes:<br><br>\u2022\tEnable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>\u2022\tMake all the data for the Sales division and the Research division available in Fabric.<br>\u2022\tFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>\u2022\tIn Productline1ws, create a lakehouse named Lakehouse1.<br>\u2022\tIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br><br>Data Analytics Requirements -<br><br>Contoso identifies the following data analytics requirements:<br><br>\u2022\tAll the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>\u2022\tThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>\u2022\tThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>\u2022\tAll the semantic models and reports for the Research division must use version control that supports branching.<br><br><br>Data Preparation Requirements -<br><br>Contoso identifies the following data preparation requirements:<br><br>\u2022\tThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>\u2022\tAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br><br>Semantic Model Requirements -<br><br>Contoso identifies the following requirements for implementing and managing semantic models:<br><br>\u2022\tThe number of rows added to the Orders table during refreshes must be minimized.<br>\u2022\tThe semantic models in the Research division workspaces must use Direct Lake mode.<br><br><br>General Requirements -<br><br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br><br>\u2022\tFollow the principle of least privilege when applicable.<br>\u2022\tMinimize implementation and maintenance effort when possible.<br><br><br>You need to recommend which type of Fabric capacity SKU meets the data analytics requirements for the Research division.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tP",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tF\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-21T04:22:00.000Z",
        "voteCount": 8,
        "content": "Fabric capacity SKUs all start with F, they allows for Pay as you go by the minute. Reserved capacity is also available. https://azure.microsoft.com/en-us/pricing/details/microsoft-fabric/"
      },
      {
        "date": "2024-06-07T10:03:00.000Z",
        "voteCount": 1,
        "content": "Dedicated, On-Demand Capacity with Per-Minute Billing\nSupport for All Fabric Experiences\nGrouping for OneLake Data Hub Filtering\nead Access via SQL Endpoints and Lakehouse Explorer"
      },
      {
        "date": "2024-05-17T08:29:00.000Z",
        "voteCount": 2,
        "content": "fabric capacity SKU only start with F, so F is the only option."
      },
      {
        "date": "2024-05-12T13:24:00.000Z",
        "voteCount": 2,
        "content": "IMHO, D,\n\nRequirement:  The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.\n\nWhy: Microsoft Fabric Capacity (Pay-as-you-go or Reservation)\nGet a shared pool of capacity that powers all capabilities in Microsoft Fabric, from data modeling and data warehousing to business intelligence and AI experiences (one minute minimum).\nIt is F license. Link: https://azure.microsoft.com/en-us/pricing/details/microsoft-fabric/"
      },
      {
        "date": "2024-04-20T01:39:00.000Z",
        "voteCount": 3,
        "content": "D. F  because : \"The Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.\""
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138717-exam-dp-600-topic-1-question-65-discussion/",
    "body": "HOTSPOT<br> -<br><br><br>Case study<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br><br>Existing Environment<br> -<br><br><br>Identity Environment<br> -<br><br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br><br>Data Environment<br> -<br><br>Contoso has the following data environment:<br><br>\u2022\tThe Sales division uses a Microsoft Power BI Premium capacity.<br>\u2022\tThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>\u2022\tThe Research department uses an on-premises, third-party data warehousing product.<br>\u2022\tFabric is enabled for contoso.com.<br>\u2022\tAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>\u2022\tA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Contoso plans to make the following changes:<br><br>\u2022\tEnable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>\u2022\tMake all the data for the Sales division and the Research division available in Fabric.<br>\u2022\tFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>\u2022\tIn Productline1ws, create a lakehouse named Lakehouse1.<br>\u2022\tIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br><br>Data Analytics Requirements<br> -<br><br>Contoso identifies the following data analytics requirements:<br><br>\u2022\tAll the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>\u2022\tThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>\u2022\tThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>\u2022\tAll the semantic models and reports for the Research division must use version control that supports branching.<br><br><br>Data Preparation Requirements<br> -<br><br>Contoso identifies the following data preparation requirements:<br><br>\u2022\tThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>\u2022\tAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br><br>Semantic Model Requirements<br> -<br><br>Contoso identifies the following requirements for implementing and managing semantic models:<br><br>\u2022\tThe number of rows added to the Orders table during refreshes must be minimized.<br>\u2022\tThe semantic models in the Research division workspaces must use Direct Lake mode.<br><br><br>General Requirements<br> -<br><br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br><br>\u2022\tFollow the principle of least privilege when applicable.<br>\u2022\tMinimize implementation and maintenance effort when possible.<br><br><br>You need to migrate the Research division data for Productline1. The solution must meet the data preparation requirements.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image82.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image83.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-15T01:12:00.000Z",
        "voteCount": 22,
        "content": "Correct.\n- \"delta\"\n- \"Tables/productline1\"\n\nRequirements: Use managed tables.\n\nIf you use saveAsTable() you don't need to specify the path \"Table/\"\nIf you you save() you specify the full path\n\nIgnore my previous comment!"
      },
      {
        "date": "2024-04-23T08:15:00.000Z",
        "voteCount": 8,
        "content": "I just checked it in Fabric,\n\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"productline1\")\nIt gives an error when executed, \"productline1\" would be valid if we use saveAsTable().\n\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/productline2\")\nIt works, it does what we expect, it is the correct solution.\nCreate a managed table.\n\ndf.write.mode(\"overwrite\").format(\"delta\").save(\"Tables/research/productline3\")\nWhat it does is load a folder with our dataframe into the lakehouse."
      },
      {
        "date": "2024-07-30T22:30:00.000Z",
        "voteCount": 1,
        "content": "delta and productline1"
      },
      {
        "date": "2024-08-15T21:02:00.000Z",
        "voteCount": 1,
        "content": "sorry correcting myself Tables/productline1"
      },
      {
        "date": "2024-05-20T09:36:00.000Z",
        "voteCount": 3,
        "content": "can anyone explain why in the code we read csv file, not delta? if we have in the task:\n An Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.\nThank you."
      },
      {
        "date": "2024-06-15T16:01:00.000Z",
        "voteCount": 1,
        "content": "the option read csv in first line is not important. The question is in 2 line.\nI in source the file could be csv. but it is not important for this exam"
      },
      {
        "date": "2024-05-12T13:30:00.000Z",
        "voteCount": 2,
        "content": "IMHO, delta and Tables/productline1\n\nData Preparation Requirements:\n\u2022 The Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.\n\u2022 All the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.\n\nAnswers:\n- delta\n- Tables/productline1 - because of the root folder in Tables, no subfolder"
      },
      {
        "date": "2024-04-15T01:11:00.000Z",
        "voteCount": 3,
        "content": "Correct.\n- \"delta\" \n- \"productline1\"\n\nRequirements: Use managed tables.\n\nIf you use saveAsTable()  you don't need to specify the path \"Table/\"\nIf you you save() you specify the full path"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137923-exam-dp-600-topic-1-question-66-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br><br>Existing Environment -<br><br><br>Identity Environment -<br><br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br><br>Data Environment -<br><br>Contoso has the following data environment:<br><br>\u2022\tThe Sales division uses a Microsoft Power BI Premium capacity.<br>\u2022\tThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>\u2022\tThe Research department uses an on-premises, third-party data warehousing product.<br>\u2022\tFabric is enabled for contoso.com.<br>\u2022\tAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>\u2022\tA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Contoso plans to make the following changes:<br><br>\u2022\tEnable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>\u2022\tMake all the data for the Sales division and the Research division available in Fabric.<br>\u2022\tFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>\u2022\tIn Productline1ws, create a lakehouse named Lakehouse1.<br>\u2022\tIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br><br>Data Analytics Requirements -<br><br>Contoso identifies the following data analytics requirements:<br><br>\u2022\tAll the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>\u2022\tThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>\u2022\tThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>\u2022\tAll the semantic models and reports for the Research division must use version control that supports branching.<br><br><br>Data Preparation Requirements -<br><br>Contoso identifies the following data preparation requirements:<br><br>\u2022\tThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>\u2022\tAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br><br>Semantic Model Requirements -<br><br>Contoso identifies the following requirements for implementing and managing semantic models:<br><br>\u2022\tThe number of rows added to the Orders table during refreshes must be minimized.<br>\u2022\tThe semantic models in the Research division workspaces must use Direct Lake mode.<br><br><br>General Requirements -<br><br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br><br>\u2022\tFollow the principle of least privilege when applicable.<br>\u2022\tMinimize implementation and maintenance effort when possible.<br><br><br>What should you use to implement calculation groups for the Research division semantic models?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Power BI Desktop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Power BI service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabular Editor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-04T15:18:00.000Z",
        "voteCount": 10,
        "content": "The answer is D\nhttps://www.bing.com/videos/riverview/relatedvideo?q=how+to+implement+calculation+group+in+fabric&amp;mid=DE305C502B1A6DA8AA03DE305C502B1A6DA8AA03&amp;FORM=VIRE"
      },
      {
        "date": "2024-04-06T07:55:00.000Z",
        "voteCount": 4,
        "content": "seems the answer is really D\nhttps://powerbi.microsoft.com/en-us/blog/announcing-calculation-groups-for-direct-lake-datasets/"
      },
      {
        "date": "2024-07-03T01:10:00.000Z",
        "voteCount": 1,
        "content": "I was studying with practice assessment on Microsoft Learn's own page. I came across to a similar question:\n\"You have a Fabric workspace that contains a lakehouse named Lakehouse1.\nLakehouse1 requires additional time intelligence calculations added to its semantic model. The model has XMLA read/write permissions enabled.\nYou need to add a calculation group to the Lakehouse1 semantic model.\nWhat should you use?\"\nThe answer is Tabular Editor and explanation is as follows:\n\nOnly Tabular Editor 2/3 can currently add calculation groups to a lakehouse semantic model. https://learn.microsoft.com/en-us/training/modules/create-calculation-groups/"
      },
      {
        "date": "2024-06-09T02:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2024-06-04T10:51:00.000Z",
        "voteCount": 1,
        "content": "The answer is B The power Bi service\nhttps://powerbi.microsoft.com/en-us/blog/model-explorer-and-calculation-groups-authoring-is-now-available-in-power-bi-service-including-direct-lake-semantic-models/"
      },
      {
        "date": "2024-05-12T13:36:00.000Z",
        "voteCount": 2,
        "content": "IMHO, D (Tabular Editor)\n\nRequirements:\nThe semantic models in the Research division workspaces must use Direct Lake mode.\n\nConsidering Contoso's requirement to minimize implementation and maintenance effort (general requirement), Tabular Editor offers a more efficient way to define calculation groups compared to manually writing DAX code. Additionally, since calculation groups are part of the semantic model itself, they can be deployed and managed alongside the model, simplifying maintenance."
      },
      {
        "date": "2024-05-11T09:05:00.000Z",
        "voteCount": 1,
        "content": "The safe answer is D:  C DAX work only with explicit measures, power BI generate dax calculation so I'm not sure but safely D"
      },
      {
        "date": "2024-05-05T11:38:00.000Z",
        "voteCount": 2,
        "content": "Since a few moths ago you can create calculation group with a recent PowerBI Desktop version too.\nhttps://learn.microsoft.com/it-it/power-bi/transform-model/calculation-groups\nHowever, if I was to choose one only answer I'd go for Tabular Editor. You can create calculation group with Tabular editor since PowerBI desktop first version (years ago)."
      },
      {
        "date": "2024-04-22T15:14:00.000Z",
        "voteCount": 1,
        "content": "Should be multiple choice, D is probably the safest option here."
      },
      {
        "date": "2024-04-13T05:13:00.000Z",
        "voteCount": 2,
        "content": "ANSWER IS D"
      },
      {
        "date": "2024-04-04T14:29:00.000Z",
        "voteCount": 1,
        "content": "It should be A, using BI desktop... and if minimizing implementation was not mentioned then tabular editor."
      },
      {
        "date": "2024-04-04T14:34:00.000Z",
        "voteCount": 2,
        "content": "Although it can be implemented using service as well. if someone can share the reasoning, that would be great! \n\nhttps://powerbi.microsoft.com/en-us/blog/model-explorer-and-calculation-groups-authoring-is-now-available-in-power-bi-service-including-direct-lake-semantic-models/"
      },
      {
        "date": "2024-04-26T02:12:00.000Z",
        "voteCount": 1,
        "content": "The solution should support version control which presumably rules that out"
      },
      {
        "date": "2024-04-07T03:45:00.000Z",
        "voteCount": 4,
        "content": "So what is minimizing here? This can be done by Tabular editor, power bi desktop and now in the power bi service too according to that article. Both Tabular editor and Power bi desktop must do some extra things to publish it. Well in this question the answers are very close"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138020-exam-dp-600-topic-1-question-67-discussion/",
    "body": "HOTSPOT<br> -<br><br><br>Case study<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br><br>Existing Environment<br> -<br><br><br>Identity Environment<br> -<br><br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br><br>Data Environment<br> -<br><br>Contoso has the following data environment:<br><br>\u2022\tThe Sales division uses a Microsoft Power BI Premium capacity.<br>\u2022\tThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>\u2022\tThe Research department uses an on-premises, third-party data warehousing product.<br>\u2022\tFabric is enabled for contoso.com.<br>\u2022\tAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>\u2022\tA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Contoso plans to make the following changes:<br><br>\u2022\tEnable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>\u2022\tMake all the data for the Sales division and the Research division available in Fabric.<br>\u2022\tFor the Research division, create two Fabric workspaces named Productline1ws and Productine2ws.<br>\u2022\tIn Productline1ws, create a lakehouse named Lakehouse1.<br>\u2022\tIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br><br>Data Analytics Requirements<br> -<br><br>Contoso identifies the following data analytics requirements:<br><br>\u2022\tAll the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>\u2022\tThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>\u2022\tThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>\u2022\tAll the semantic models and reports for the Research division must use version control that supports branching.<br><br><br>Data Preparation Requirements<br> -<br><br>Contoso identifies the following data preparation requirements:<br><br>\u2022\tThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>\u2022\tAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br><br>Semantic Model Requirements<br> -<br><br>Contoso identifies the following requirements for implementing and managing semantic models:<br><br>The number of rows added to the Orders table during refreshes must be minimized.<br>\u2022\tThe semantic models in the Research division workspaces must use Direct Lake mode.<br><br><br>General Requirements<br> -<br><br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br><br>\u2022\tFollow the principle of least privilege when applicable.<br>\u2022\tMinimize implementation and maintenance effort when possible.<br><br>Which workspace role assignments should you recommend for ResearchReviewersGroup1 and ResearchReviewersGroup2? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image84.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image85.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-08T23:35:00.000Z",
        "voteCount": 15,
        "content": "correct, you can see in this link: https://learn.microsoft.com/en-us/fabric/get-started/roles-workspaces"
      },
      {
        "date": "2024-05-12T13:42:00.000Z",
        "voteCount": 6,
        "content": "IMHO, correct, Viewer &amp; Contributor\n\nRequirements: \n\u2022 For the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.\n\u2022 For the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.\n\nFrom https://learn.microsoft.com/en-us/fabric/get-started/roles-workspaces\n1) Viewer+: Read Lakehouse and Data warehouse data and shortcuts2 with T-SQL through TDS endpoint.\n\n2) Contributor+: Read Lakehouse data through Lakehouse explorer."
      },
      {
        "date": "2024-08-07T05:59:00.000Z",
        "voteCount": 1,
        "content": "1--&gt; Viewer\n2 --&gt; Member"
      },
      {
        "date": "2024-04-28T01:14:00.000Z",
        "voteCount": 4,
        "content": "Viewer and contributor\n\nRead using sql endpoint -&gt; Min. Viewer\nRead using lakehouse explorer -&gt; Min. Contributor"
      },
      {
        "date": "2024-04-06T02:49:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137925-exam-dp-600-topic-1-question-68-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant.<br><br>You need to configure OneLake security for users shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image86.png\"><br><br>The solution must follow the principle of least privilege.<br><br>Which permission should you assign to each user? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image87.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image88.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-04T14:40:00.000Z",
        "voteCount": 12,
        "content": "Correct! ReadAll and readData\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sharing#sharing-and-permissions"
      },
      {
        "date": "2024-10-07T12:49:00.000Z",
        "voteCount": 1,
        "content": "the selections of ReadAll for User1 and Read for User2 ensure that each user gets the appropriate level of access according to their stated needs while adhering to the principle of least privilege. ReadData may imply limited access and could restrict User2 from reading all SQL endpoint data effectively."
      },
      {
        "date": "2024-05-28T09:58:00.000Z",
        "voteCount": 2,
        "content": "\u2022 User1: Assign ReadAll. This is because User1 needs to read all Spark data, which would require broad access across multiple datasets and objects managed by Spark.\n\u2022 User2: Assign ReadData. This permission is more likely to be specific to data endpoints, including SQL endpoints."
      },
      {
        "date": "2024-05-12T13:45:00.000Z",
        "voteCount": 2,
        "content": "IMHO,\n\nReq: user1 - spark -&gt; **ReadAll**, user 2 - SQL endpoint -&gt; **ReadData**.\n\nRoles: \nReadData permission on SQL endpoint to access data without SQL policy.\nReadAll permission on the lakehouse to access all data using Apache Spark.\nfrom https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-sharing#sharing-and-permissions"
      },
      {
        "date": "2024-04-06T02:47:00.000Z",
        "voteCount": 3,
        "content": "ReadAll and ReadData is Correct\nhttps://blog.fabric.microsoft.com/en/blog/lakehouse-sharing-and-access-permission-management?ft=All"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137945-exam-dp-600-topic-1-question-69-discussion/",
    "body": "You have an Azure Repos repository named Repo1 and a Fabric-enabled Microsoft Power BI Premium capacity. The capacity contains two workspaces named Workspace1 and Workspace2. Git integration is enabled at the workspace level.<br><br>You plan to use Microsoft Power BI Desktop and Workspace1 to make version-controlled changes to a semantic model stored in Repo1. The changes will be built and deployed to Workspace2 by using Azure Pipelines.<br><br>You need to ensure that report and semantic model definitions are saved as individual text files in a folder hierarchy. The solution must minimize development and maintenance effort.<br><br>In which file format should you save the changes?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIP\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIDS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIX"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T10:02:00.000Z",
        "voteCount": 5,
        "content": "PBIP (Power BI Project):\n-&gt; PBIP format is designed to work with version control systems like Azure Repos. It breaks down Power BI artifacts into individual files that can be managed and versioned separately, facilitating better collaboration and change tracking.\n-&gt; Folder Hierarchy: It saves the project structure in a folder hierarchy, where each component of the Power BI project (like datasets, reports, data sources) is stored in separate files.\n-&gt; Text-Based: Being a text-based format, it integrates well with Git repositories and supports diff and merge operations."
      },
      {
        "date": "2024-05-24T05:21:00.000Z",
        "voteCount": 1,
        "content": "A - no-brainer"
      },
      {
        "date": "2024-05-12T13:49:00.000Z",
        "voteCount": 1,
        "content": "IMHO, A) PBIP is correct.\n\nWhy:\nPower BI Desktop introduces a new way to author, collaborate, and save your projects. When you save your work as a Power BI Project (PBIP), report and semantic model item definitions are saved as individual plain text files in a simple, intuitive folder structure.\n\nLink: https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview"
      },
      {
        "date": "2024-04-05T06:47:00.000Z",
        "voteCount": 2,
        "content": "A.PBIP Correct"
      },
      {
        "date": "2024-04-04T22:58:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/developer/projects/projects-overview#:~:text=Power%20BI%20Desktop%20introduces%20a%20new%20way%20to%20author%2C%20collaborate%2C%20and%20save%20your%20projects.%20You%20can%20now%20save%20your%20work%20as%20a%20Power%20BI%20Project%20(PBIP).%20As%20a%20project%2C%20report%20and%20semantic%20model%20item%20definitions%20are%20saved%20as%20individual%20plain%20text%20files%20in%20a%20simple%2C%20intuitive%20folder%20structure."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139155-exam-dp-600-topic-1-question-70-discussion/",
    "body": "DRAG DROP<br> -<br><br>You are implementing a medallion architecture in a single Fabric workspace.<br><br>You have a lakehouse that contains the Bronze and Silver layers and a warehouse that contains the Gold layer.<br><br>You create the items required to populate the layers as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image89.png\"><br><br>You need to ensure that the layers are populated daily in sequential order such that Silver is populated only after Bronze is complete, and Gold is populated only after Silver is complete. The solution must minimize development effort and complexity.<br><br>What should you use to execute each set of items? To answer, drag the appropriate options to the correct items. Each option may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image90.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image91.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-29T09:08:00.000Z",
        "voteCount": 21,
        "content": "Bronze Layer answer is wrong. the Bronze Layer already has pipeline defined with it so you need an invoke pipeline activity to call them"
      },
      {
        "date": "2024-06-05T00:26:00.000Z",
        "voteCount": 7,
        "content": "Should be;\n\nOrchestration pipeline: Schedule\nBronze layer: An invoke pipeline activity (see table -&gt; PIPELINE is already there)\nSilver: Dataflow activity\nGold: Stored procedure activity"
      },
      {
        "date": "2024-10-07T13:04:00.000Z",
        "voteCount": 1,
        "content": "Orchestration pipeline: Use An Invoke pipeline activity to manage the orchestration and invoke other pipelines in the correct sequence.\n\nBronze layer: Use A pipeline Copy activity because the Bronze layer typically involves raw data ingestion, where data is copied into the lakehouse.\n\nSilver layer: Use A pipeline Dataflow activity as this layer involves transforming and cleaning the data, for which Dataflows are typically used.\n\nGold layer: Use A pipeline Stored procedure activity because the Gold layer often involves aggregations and advanced processing, which can be handled using stored procedures in a warehouse."
      },
      {
        "date": "2024-06-02T08:47:00.000Z",
        "voteCount": 3,
        "content": "I also think that bronze is wrong. Since the pipeline with Copy activities is already created, we just need an invoke pipeline to trigger it"
      },
      {
        "date": "2024-06-01T22:57:00.000Z",
        "voteCount": 4,
        "content": "Orchestration: A schedule\nBronze Layer: An invoke pipeline activity (the actual pipeline is already there)\nSilver Layer: A pipeline Dataflow activity\nGold Layer: A pipeline Stored Procedure activity"
      },
      {
        "date": "2024-05-28T10:07:00.000Z",
        "voteCount": 3,
        "content": "\u2022 Orchestration Pipeline: A schedule\n\u2022 Bronze Layer: A pipeline Copy activity\n\u2022 Silver Layer: A pipeline Dataflow activity\n\u2022 Gold Layer: A pipeline Stored procedure activity"
      },
      {
        "date": "2024-04-19T02:03:00.000Z",
        "voteCount": 5,
        "content": "I'd say answer is correct.\nHowever, IMO there is no suitable item for \"orchestration pipeline\". \"A schedule\" seems the most appropriate. But I'd imagine \"data pipeline\" would be a better option since one would use an data pipeline to orchestrate the different activities."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138094-exam-dp-600-topic-1-question-71-discussion/",
    "body": "DRAG DROP<br> -<br><br>You are building a solution by using a Fabric notebook.<br><br>You have a Spark DataFrame assigned to a variable named df. The DataFrame returns four columns.<br><br>You need to change the data type of a string column named Age to integer. The solution must return a DataFrame that includes all the columns.<br><br>How should you complete the code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image92.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image93.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-07T12:52:00.000Z",
        "voteCount": 9,
        "content": "from pyspark.sql.types import IntegerType\nfrom pyspark.sql import functions as F\n# first method\ndf = df.withColumn(\"Age\", df.age.cast(\"int\"))\n# second method\ndf = df.withColumn(\"Age\", df.age.cast(IntegerType()))\n# third method\ndf = df.withColumn(\"Age\", F.col(\"Age\").cast(IntegerType()))\n\nhttps://aporia.com/resources/how-to/change-column-data-types-in-dataframe/\n\nconsidering the 3rd method for the answer"
      },
      {
        "date": "2024-06-05T00:27:00.000Z",
        "voteCount": 2,
        "content": "Correct\n\nwithColumn | col | cast\n\nIt's the very basic syntax of data type allocation"
      },
      {
        "date": "2024-06-02T08:49:00.000Z",
        "voteCount": 1,
        "content": "withColumn &gt; column&gt; cast"
      },
      {
        "date": "2024-05-12T14:01:00.000Z",
        "voteCount": 2,
        "content": "IMHO, withColumn -&gt; col -&gt; cast \nis good!"
      },
      {
        "date": "2024-04-16T06:45:00.000Z",
        "voteCount": 4,
        "content": "correct - withColumn, col, cast"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137977-exam-dp-600-topic-1-question-72-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Lake Storage Gen2 account named storage1 that contains a Parquet file named sales.parquet.<br><br>You have a Fabric tenant that contains a workspace named Workspace1.<br><br>Using a notebook in Workspace1, you need to load the content of the file to the default lakehouse. The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image94.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image95.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T08:38:00.000Z",
        "voteCount": 35,
        "content": "delta\nsales"
      },
      {
        "date": "2024-06-16T03:01:00.000Z",
        "voteCount": 3,
        "content": "thats ok\ndelta is the format needed to creat managed table. \nusing saveastable you don't need to specify the path"
      },
      {
        "date": "2024-04-07T18:25:00.000Z",
        "voteCount": 2,
        "content": "why? please provide some explanation"
      },
      {
        "date": "2024-04-09T15:16:00.000Z",
        "voteCount": 15,
        "content": "I think it should only be sales because if saveastable is used, the argument should only be table name. Link: https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-notebook-load-data."
      },
      {
        "date": "2024-04-09T01:03:00.000Z",
        "voteCount": 8,
        "content": "delta\ntables/sales"
      },
      {
        "date": "2024-06-05T00:29:00.000Z",
        "voteCount": 4,
        "content": "Yep, you're wrong. This would have been right with .save but ALWAYS look carefully: saveAsTable can just be used with the table name"
      },
      {
        "date": "2024-06-03T07:00:00.000Z",
        "voteCount": 1,
        "content": "I think this is wrong since it's usingsaveAsTable"
      },
      {
        "date": "2024-08-04T12:09:00.000Z",
        "voteCount": 1,
        "content": "df.write.mode(\"overwrite\").format(\"delta\").partitionBy(\"Year\",\"Quarter\").save(\"Tables/\" + table_name)"
      },
      {
        "date": "2024-06-01T13:30:00.000Z",
        "voteCount": 3,
        "content": "delta and sales (100%)"
      },
      {
        "date": "2024-05-28T10:26:00.000Z",
        "voteCount": 2,
        "content": "delta - sales : \ndf = spark.read.parquet(\"abfss://fs1@storage1.dfs.core.windows.net/files/sales.parquet\")\ndf.write.mode(\"overwrite\").format(\"delta\").saveAsTable(\"sales\")"
      },
      {
        "date": "2024-05-28T04:29:00.000Z",
        "voteCount": 2,
        "content": "df.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"Sales\"): Writes the DataFrame to the default Lakehouse using the Delta format, and saves it as a table named Sales."
      },
      {
        "date": "2024-05-17T08:51:00.000Z",
        "voteCount": 1,
        "content": "Delta, sales: the code already using \"SaveasTable\" so no need to reference the table again."
      },
      {
        "date": "2024-05-11T20:44:00.000Z",
        "voteCount": 3,
        "content": "DELTA, Sales. This is the correct answer since the destination was to loaded Delta the comand SaveAsTable you only need the table name no path. https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-notebook-load-data"
      },
      {
        "date": "2024-04-29T09:17:00.000Z",
        "voteCount": 1,
        "content": "I gave it a try, it is 'sales' not 'File/sales'"
      },
      {
        "date": "2024-04-19T01:21:00.000Z",
        "voteCount": 3,
        "content": "Saveastable so it is directly the name of the tables don't need to add tables/name"
      },
      {
        "date": "2024-04-16T06:51:00.000Z",
        "voteCount": 4,
        "content": "delta &amp; sales.\nother options do not work in the context of the given code fragments (e.g. files/sales is for external tables but the path-parameter is missing here)"
      },
      {
        "date": "2024-04-09T00:08:00.000Z",
        "voteCount": 4,
        "content": "\"The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer.\" - so only Delta in Tables section, otherwise table won`t be displayed automatically.\nDelta, Tables."
      },
      {
        "date": "2024-04-12T00:13:00.000Z",
        "voteCount": 9,
        "content": "i missed that it has saveastable, so \"Tables/\" is not needed, it`s included to save function.\nSo only \n1.Delta\n2.sales."
      },
      {
        "date": "2024-04-09T00:01:00.000Z",
        "voteCount": 2,
        "content": "A) delta\nB) Table/ sales --&gt; The solution must ensure that the content will display automatically as a table named Sales in Lakehouse explorer"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138811-exam-dp-600-topic-1-question-73-discussion/",
    "body": "You have a Fabric workspace named Workspace1 that contains a lakehouse named Lakehouse1.<br><br>In Workspace1, you create a data pipeline named Pipeline1.<br><br>You have CSV files stored in an Azure Storage account.<br><br>You need to add an activity to Pipeline1 that will copy data from the CSV files to Lakehouse1. The activity must support Power Query M formula language expressions.<br><br>Which type of activity should you add?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflow\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNotebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScript",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy data"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-18T23:53:00.000Z",
        "voteCount": 9,
        "content": "I think Dataflow since dataflow is like Power Query. But question is a bit unclear. Because data flow in itself is more like a whole, well, dataflow but not a singular activity like e.g. Copy Data."
      },
      {
        "date": "2024-04-24T13:24:00.000Z",
        "voteCount": 1,
        "content": "Data flow being \"more like a whole\" doesn't related to Power Query m. Instead, the Copy Data activity, when added within the pipeline, will allow access to Power query m."
      },
      {
        "date": "2024-06-10T09:22:00.000Z",
        "voteCount": 1,
        "content": "You can add dataflow to pipeline as activity: https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-dataflows-gen2-pipeline-activity"
      },
      {
        "date": "2024-06-05T00:32:00.000Z",
        "voteCount": 2,
        "content": "Although you're just copying data, which would result in a 'Copy Data' activity... you would preferably go with a Dataflow in Fabric. Especially because the big take away is that the activity must support Power Query M"
      },
      {
        "date": "2024-05-28T10:32:00.000Z",
        "voteCount": 1,
        "content": "Power Query M Support: Dataflows in Azure Data Factory and Synapse Analytics support the Power Query M formula language, enabling you to perform complex transformations and data manipulations as part of the data ingestion process.\nTransformations: Dataflows allow for a wide range of data transformation capabilities which are especially useful when working with CSV files to cleanse, aggregate, or reshape data before loading it into the destination."
      },
      {
        "date": "2024-05-11T20:54:00.000Z",
        "voteCount": 2,
        "content": "A. Dataflow use powerquery. i don't think you have powerquery with copy data"
      },
      {
        "date": "2024-04-17T15:23:00.000Z",
        "voteCount": 3,
        "content": "The answer is Correct! To copy data from CSV files to Lakehouse1 in Workspace1, you should add a copy activity to Pipeline1. https://learn.microsoft.com/en-us/fabric/data-factory/connector-lakehouse-copy-activity"
      },
      {
        "date": "2024-04-16T01:02:00.000Z",
        "voteCount": 1,
        "content": "You can specify transformations using Power Query M expressions during the data copy process. This ensures that your data is transformed according to your requirements before being loaded into Lakehouse1"
      },
      {
        "date": "2024-04-18T23:54:00.000Z",
        "voteCount": 1,
        "content": "I don't think you can use Power Query M in a copy data process to transform data. Copy activity simply copies data from A to B."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137950-exam-dp-600-topic-1-question-74-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant that contains lakehouse named Lakehouse1. Lakehouse1 contains a Delta table with eight columns.<br><br>You receive new data that contains the same eight columns and two additional columns.<br><br>You create a Spark DataFrame and assign the DataFrame to a variable named df. The DataFrame contains the new data.<br><br>You need to add the new data to the Delta table to meet the following requirements:<br><br>\u2022\tKeep all the existing rows.<br>\u2022\tEnsure that all the new data is added to the table.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image96.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image97.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T01:22:00.000Z",
        "voteCount": 37,
        "content": "append, (\"mergeschema\", \"true\") - schema evolution, same columns and 2 added new columns"
      },
      {
        "date": "2024-04-15T01:42:00.000Z",
        "voteCount": 15,
        "content": "- append\n- (\"mergeschema\", \"true\")\n\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/update-schema\n\nAdd columns with automatic schema update\nColumns that are present in the DataFrame but missing from the table are automatically added as part of a write transaction when:\n\nwrite or writeStream have .option(\"mergeSchema\", \"true\")\nspark.databricks.delta.schema.autoMerge.enabled is true"
      },
      {
        "date": "2024-06-05T00:37:00.000Z",
        "voteCount": 4,
        "content": "1. Append\n--&gt; Because of \"Keep all the existing rows\"\n\n2. mergeSchema, true\n--&gt; Because there will be 2 additional columns, so it's they are not deleted from the new version. If they would have been deleted from the new version, you would have to use \"overwriteSchema, true\". Since that option replaces the existing schema with the schema of the new dataframe. mergeSchema, on the contrary, allows the addition of new columns rather than overwriting the schema."
      },
      {
        "date": "2024-04-25T20:30:00.000Z",
        "voteCount": 5,
        "content": "append\nmergeschema=\"true\"\n\nappend as all existing rows must be kept.\naccording to this: https://learn.microsoft.com/en-us/azure/databricks/delta/update-schema\noverwriteschema is for \"change a column\u2019s type or name or drop a column\"\nmergeschema is for \"Columns that are present in the DataFrame but missing from the table are automatically added\""
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139147-exam-dp-600-topic-1-question-75-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric warehouse that contains a table named Sales.Orders. Sales.Orders contains the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image98.png\"><br><br>You need to write a T-SQL query that will return the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image99.png\"><br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image100.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image101.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-19T00:05:00.000Z",
        "voteCount": 16,
        "content": "IMHO answer is correct:\nhttps://learn.microsoft.com/de-de/sql/t-sql/language-elements/coalesce-transact-sql?view=sql-server-ver16\nhttps://learn.microsoft.com/de-de/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-05-20T11:54:00.000Z",
        "voteCount": 1,
        "content": "strange question. In the second box coalesce is not correct because it doesn't choose the smallest. Least also has problems - it doesn't ignore null:\nIf one or more arguments aren't NULL, then NULL arguments are ignored during comparison. If all arguments are NULL, then LEAST returns NULL.\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16\n\nMIN compares all the rows in one column and choose the smallest, so it is also the wrong choice. \n\nI feel confused."
      },
      {
        "date": "2024-06-14T10:27:00.000Z",
        "voteCount": 2,
        "content": "Its simple, COALESCE returns first non-null value from the parameters passed, see there are three parameters used in the specific order which is asked in the question. MIN is grouping function which is not in the query so LEAST should be used"
      },
      {
        "date": "2024-05-19T21:51:00.000Z",
        "voteCount": 2,
        "content": "IMHO answer is correct: COALESCE, LEAST"
      },
      {
        "date": "2024-05-12T14:30:00.000Z",
        "voteCount": 1,
        "content": "IMHO, Coalesce -&gt; Least is winner.\n\nLeast because of that:\nIf one or more arguments aren't NULL, then NULL arguments are ignored during comparison. If all arguments are NULL, then LEAST returns NULL.\n\nLink: https://learn.microsoft.com/en-us/sql/t-sql/functions/logical-functions-least-transact-sql?view=sql-server-ver16"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139156-exam-dp-600-topic-1-question-76-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse.<br><br>You plan to use a visual query to merge two tables.<br><br>You need to ensure that the query returns all the rows in both tables.<br><br>Which type of join should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tinner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfull outer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tleft outer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tright anti",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tright outer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tleft anti"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-05T00:40:00.000Z",
        "voteCount": 1,
        "content": "Just making it more obvious, but yea... all other options wouldn't return all the rows in both tables but exclude some."
      },
      {
        "date": "2024-05-12T14:31:00.000Z",
        "voteCount": 2,
        "content": "IMHO, 1st grage of SQL school - FULL OUTER JOIN.\n\nBecause - ALL rows, not from left, right, ... table(s)"
      },
      {
        "date": "2024-05-11T21:01:00.000Z",
        "voteCount": 3,
        "content": "B. Left or right will only return one table all rows and other table if exist in the other table. inner will only return if data match in both tables so Full (outer) join will return all data from both tables"
      },
      {
        "date": "2024-04-23T14:31:00.000Z",
        "voteCount": 4,
        "content": "FULL (OUTER) JOIN\n\nhttps://www.w3schools.com/sql/sql_join.asp"
      },
      {
        "date": "2024-04-19T02:07:00.000Z",
        "voteCount": 2,
        "content": "full outer keeps rows from both sides"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137987-exam-dp-600-topic-1-question-77-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table that has one million Parquet files.<br><br>You need to remove files that were NOT referenced by the table during the past 30 days. The solution must ensure that the transaction log remains consistent, and the ACID properties of the table are maintained.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom OneLake file explorer, delete the files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the OPTIMIZE command and specify the Z-order parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the OPTIMIZE command and specify the V-order parameter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the VACUUM command.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T00:13:00.000Z",
        "voteCount": 8,
        "content": "D: https://learn.microsoft.com/en-us/fabric/data-engineering/lakehouse-table-maintenance"
      },
      {
        "date": "2024-06-05T00:41:00.000Z",
        "voteCount": 2,
        "content": "Just think of the aspect of a real VACUUM: Cleanup of dust just lying around"
      },
      {
        "date": "2024-05-12T14:33:00.000Z",
        "voteCount": 2,
        "content": "IMHO, D) Vacuum is good"
      },
      {
        "date": "2024-04-05T11:28:00.000Z",
        "voteCount": 4,
        "content": "Yes D - To remove files from a Delta table that are no longer referenced by the current version of the table, you should run the VACUUM command."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137988-exam-dp-600-topic-1-question-78-discussion/",
    "body": "DRAG DROP<br> -<br><br>You are implementing two dimension tables named Customers and Products in a Fabric warehouse.<br><br>You need to use slowly changing dimension (SCD) to manage the versioning of data. The solution must meet the requirements shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image102.png\"><br><br>Which type of SCD should you use for each table? To answer, drag the appropriate SCD types to the correct tables. Each SCD type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image103.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image104.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-15T01:54:00.000Z",
        "voteCount": 32,
        "content": "Correct. We need more questions like these. People are forgetting the importance of data modeling.\n\nStatic table:\nType 0 \u2013 Fixed Dimension. No changes allowed, dimension never changes.\n\nCommonly used SCD types:\nType 1 \u2013 No History. Update record directly, there is no record of historical values, only current state.\nType 2 \u2013 Row Versioning.\nType 3 \u2013 Previous Value column.\n\nRarely used:\nType 4 \u2013 History Table. \nType 6 \u2013 Hybrid SCD."
      },
      {
        "date": "2024-04-05T11:33:00.000Z",
        "voteCount": 6,
        "content": "seems correct and - Type 0 SCD, we ignore any changes and simply audit them\n\nType 1 SCD, we overwrite existing data with new values. Historical versions are not preserved.\n\nType 2 SCD involves adding new rows to the dimension table when changes occur. It allows us to track historical versions of attributes."
      },
      {
        "date": "2024-04-19T00:15:00.000Z",
        "voteCount": 2,
        "content": "correct, Type 2 (new row) and Type 1 (overwrite)"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138724-exam-dp-600-topic-1-question-79-discussion/",
    "body": "You have a Fabric workspace named Workspace1 and an Azure SQL database.<br><br>You plan to create a dataflow that will read data from the database, and then transform the data by performing an inner join.<br><br>You need to ignore spaces in the values when performing the inner join. The solution must minimize development effort.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend the queries by using fuzzy matching.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge the queries by using fuzzy matching.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend the queries by using a lookup table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge the queries by using a lookup table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-15T01:58:00.000Z",
        "voteCount": 18,
        "content": "Fuzzy Matching: Fuzzy matching allows you to match data even when there are minor differences, such as extra spaces, in the values. This eliminates the need to manually clean or preprocess the data before the join."
      },
      {
        "date": "2024-05-12T14:45:00.000Z",
        "voteCount": 3,
        "content": "IMHO, B is good.\n\nNot append (append is like UNION).\nNot  D, because spaces, and we need minimal efforts"
      },
      {
        "date": "2024-04-25T20:47:00.000Z",
        "voteCount": 3,
        "content": "Merge as the requirement is for inner join. \ndoc on fuzzy join and fuzzy match:\nhttps://learn.microsoft.com/en-us/powerquery-m/table-fuzzyjoin\nhttps://learn.microsoft.com/en-us/power-query/merge-queries-fuzzy-match"
      },
      {
        "date": "2024-04-19T02:09:00.000Z",
        "voteCount": 3,
        "content": "B should be correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138029-exam-dp-600-topic-1-question-80-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse named Warehouse1. Warehouse1 contains two schemas name schema1 and schema2 and a table named schema1.city.<br><br>You need to make a copy of schema1.city in schema2. The solution must minimize the copying of data.<br><br>Which T-SQL statement should you run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tINSERT INTO schema2.city SELECT * FROM schema1.city;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT * INTO schema2.city FROM schema1.city;",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE schema2.city AS CLONE OF schema1.city;\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE TABLE schema2.city AS SELECT * FROM schema1.city;"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T14:43:00.000Z",
        "voteCount": 23,
        "content": "The \u201cCREATE TABLE \u2026 AS CLONE OF\u201d statement is the most efficient way to create a copy of a table in the same Fabric tenant. This statement creates a new table in the specified schema that is an exact clone of the source table, including the table structure, data, and indexes. This approach minimizes the amount of data copied, as it simply creates a reference to the underlying data rather than performing a full table scan and data copy.\n\nOptions A and B, which use INSERT INTO or SELECT INTO, would require scanning the entire source table and copying the data row-by-row, which is less efficient than the CLONE method.\n\nOption D, CREATE TABLE \u2026 AS SELECT, would also work to create a copy of the table, but it would perform a full table scan and data copy, which is less efficient than the CLONE approach."
      },
      {
        "date": "2024-05-24T18:32:00.000Z",
        "voteCount": 4,
        "content": "To clarify, AS CLONE OF does not copy the data. See this documentation: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql?view=fabric\n\nOption C will only copy over the metadata of table schema1.city, so I guess it is minimizing the copying of data by copying no data, lol. Option D is most correct if we want to duplicate the table itself, data and metadata. I'm sure the question is just slightly worded wrong and the intended correct answer is C, i.e. we only want to make a copy of the metadata."
      },
      {
        "date": "2024-08-27T06:44:00.000Z",
        "voteCount": 1,
        "content": "utsuha, the question clearly mentions \"The solution must minimize the copying of data.\"\nThat's why, IMO option C is correct."
      },
      {
        "date": "2024-08-07T01:56:00.000Z",
        "voteCount": 1,
        "content": "utsuha option C is correct please read this\nUpon creation, a table clone is an independent and separate copy of the data from its source.\n\nAny changes made through DML or DDL on the source of the clone table are not reflected in the clone table.\nSimilarly, any changes made through DDL or DML on the table clone are not reflected on the source of the clone table.\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#separate-and-independent"
      },
      {
        "date": "2024-07-22T22:18:00.000Z",
        "voteCount": 1,
        "content": "no copy data but pointing to source data."
      },
      {
        "date": "2024-09-29T07:15:00.000Z",
        "voteCount": 1,
        "content": "should be C, since copying of data should be minimized"
      },
      {
        "date": "2024-08-29T08:34:00.000Z",
        "voteCount": 1,
        "content": "CLONE AS Creates a new table as a zero-copy clone of another table in Warehouse in Microsoft Fabric. Only the metadata of the table is copied. The underlying data of the table, stored as parquet files, is not copied. \n\nA zero-copy clone creates a replica of the table by copying the metadata, while still referencing the same data files in OneLake. The metadata is copied while the underlying data of the table stored as parquet files is not copied. The creation of a clone is similar to creating a table within a Warehouse in Microsoft Fabric."
      },
      {
        "date": "2024-08-29T08:38:00.000Z",
        "voteCount": 1,
        "content": "I stand corrected. Though what I mentioned above is correct, there's a trick in the question. The requirement is not to copy the data, it is to MINIMIZE the copying of data. Which will happen in the case of CLONE AS. Hence, C is the correct answer."
      },
      {
        "date": "2024-06-15T18:35:00.000Z",
        "voteCount": 2,
        "content": "go with D. C is a clone of table's metadata not copy data activity"
      },
      {
        "date": "2024-06-05T00:45:00.000Z",
        "voteCount": 3,
        "content": "AS CLONE basically minimizes the copying of data. That's the biggest requirement."
      },
      {
        "date": "2024-06-02T18:53:00.000Z",
        "voteCount": 2,
        "content": "C. CREATE TABLE schema2.city AS CLONE OF schema1.city;\n\nThis statement creates a new table named city in schema2 that has the same structure as the city table in schema1 without copying any data. It essentially creates a metadata reference to the original table, which minimizes the data copying."
      },
      {
        "date": "2024-05-28T10:53:00.000Z",
        "voteCount": 1,
        "content": "Option A: INSERT INTO schema2.city SELECT * FROM schema1.city;\nThis option assumes that schema2.city already exists. It will insert data into schema2.city but will not create the table.\n\nOption B: SELECT * INTO schema2.city FROM schema1.city;\nThis option copies data from schema1.city to schema2.city but is typically used in SQL Server to create a new table and copy data. However, it doesn't handle schema changes well and might not be the best for large datasets.\n\nOption C: CREATE TABLE schema2.city AS CLONE OF schema1.city;\nThis is not a valid T-SQL syntax for creating tables.\n\nOption D: CREATE TABLE schema2.city AS SELECT * FROM schema1.city;\nThis statement creates a new table schema2.city and copies all data from schema1.city into it (CTAS). This option is efficient for creating a new table with the same schema and data as the original table."
      },
      {
        "date": "2024-05-21T21:34:00.000Z",
        "voteCount": 2,
        "content": "D is the correct answer. \n\n Only the metadata of the table is copied. The underlying data of the table, stored as parquet files, is not copied.\n\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-table-as-clone-of-transact-sql?view=fabric"
      },
      {
        "date": "2024-05-12T14:48:00.000Z",
        "voteCount": 3,
        "content": "IMHO, C is good.\n\nBecause: Within a warehouse, a clone of a table can be created near-instantaneously using simple T-SQL. A clone of a table can be created within or across schemas in a warehouse.\n\nHere: https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#creation-of-a-table-clone"
      },
      {
        "date": "2024-04-28T02:24:00.000Z",
        "voteCount": 1,
        "content": "I would go with C, is the fastest way to replicate the schema."
      },
      {
        "date": "2024-04-19T02:10:00.000Z",
        "voteCount": 2,
        "content": "C is correct - Cloning reduces data movement"
      },
      {
        "date": "2024-04-18T02:57:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2024-04-07T07:28:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table\nMicrosoft Fabric offers the capability to create near-instantaneous zero-copy clones with minimal storage costs."
      },
      {
        "date": "2024-04-06T07:10:00.000Z",
        "voteCount": 3,
        "content": "d is the write answer"
      },
      {
        "date": "2024-04-07T14:34:00.000Z",
        "voteCount": 2,
        "content": "why? any explanation or reference to help learn?"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137972-exam-dp-600-topic-1-question-81-discussion/",
    "body": "You have a Fabric tenant that contains a lakehouse named Lakehouse1.<br><br>You need to prevent new tables added to Lakehouse1 from being added automatically to the default semantic model of the lakehouse.<br><br>What should you configure?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe SQL analytics endpoint settings\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe semantic model settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe workspace settings",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Lakehouse1 settings"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T01:31:00.000Z",
        "voteCount": 19,
        "content": "Read the question carefully: \"You need to prevent new tables added to Lakehouse1 from being added automatically to the default semantic model of the lakehouse.\" The answer is in the article https://learn.microsoft.com/en-us/fabric/data-warehouse/semantic-models but \"Sync the default Power BI semantic model\". The correct answer is there A"
      },
      {
        "date": "2024-06-14T06:08:00.000Z",
        "voteCount": 3,
        "content": "So, pay attention. The Answer is C\n\"Previously we auto added all tables and views in the Warehouse to the default Power BI semantic model. Based on feedback, we have modified the default behavior to not automatically add tables and views to the default Power BI semantic model. This change will ensure the background sync will not get triggered. This will also disable some actions like \"New Measure\", \"Create Report\", \"Analyze in Excel\".\n\nIf you want to change this default behavior, you can:\n\nManually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL analytics endpoint in the workspace. This will restart the background sync that will incur some consumption costs.\""
      },
      {
        "date": "2024-06-26T07:10:00.000Z",
        "voteCount": 2,
        "content": "In your workspace you will have the Warehouse or SQL analytics endpoint and that is where you need to do the setting. So, I will go with A.  If you go with C it is like saying you do the setting in that page but when you say A it points out where exactly in the page you need to do that setting."
      },
      {
        "date": "2024-05-09T02:17:00.000Z",
        "voteCount": 8,
        "content": "I've confirmed it on Fabric, and the correct answer is A.\nTo change this setting, you need to open the settings of either the SQL analytics endpoint object of a Lakehouse or the warehouse object of a warehouse."
      },
      {
        "date": "2024-05-09T02:20:00.000Z",
        "voteCount": 3,
        "content": "A small detail: Fabric has since changed this setting, and now it comes off by default.\nIf anything you would need to turn it on, but the place where you do that is still the same."
      },
      {
        "date": "2024-10-18T02:23:00.000Z",
        "voteCount": 1,
        "content": "Tested in Fabric; go to &gt;  SQL Endpoint &gt; ... &gt; Settings &gt; Default Power BI semantic model &gt; Sync the default Power BI semantic model &gt; On | Off\n\nAdd objects from the lakehouse to the default Power BI semantic model. Also, update the model with any new objects added to the lakehouse. You can use this model to build reports faster with lakehouse data."
      },
      {
        "date": "2024-07-25T02:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct .. SQL analytics endpoint settings.. I checked this option in a workspace I have. It seems off by default. It shows like below:\nSync the default Power BI semantic model\nOff\nAdd objects from the lakehouse to the default Power BI semantic model. Also, update the model with any new objects added to the lakehouse. You can use this model to build reports faster with lakehouse data."
      },
      {
        "date": "2024-07-18T17:18:00.000Z",
        "voteCount": 1,
        "content": "I'm going for B\nit's chat GPT answer LOL\nAccess the Lakehouse Settings:\n\nNavigate to the Lakehouse1 in your Fabric tenant.\nGo to the settings or properties of the Lakehouse1.\nLocate the Semantic Model Configuration:\n\nFind the option related to the semantic model or data modeling settings.\nDisable Automatic Addition:\n\nLook for a setting that mentions the automatic addition of new tables to the semantic model.\nDisable this setting to prevent new tables from being automatically added."
      },
      {
        "date": "2024-07-12T18:50:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2024-06-19T16:15:00.000Z",
        "voteCount": 1,
        "content": "Tested in Fabric. It's A."
      },
      {
        "date": "2024-06-05T00:07:00.000Z",
        "voteCount": 5,
        "content": "To prevent new tables added to Lakehouse1 from being automatically added to the default semantic model of the lakehouse, you need to configure the semantic model settings.\n\nThe semantic model in a Fabric lakehouse represents the logical data model and defines how the data in the lakehouse should be interpreted and used. By default, when new tables are added to the lakehouse, they may be automatically added to the semantic model as well.\n\nTo prevent this behavior and maintain more control over the contents of the semantic model, you need to adjust the semantic model settings. This allows you to configure the lakehouse to not automatically include new tables in the semantic model, requiring you to manually add them if desired."
      },
      {
        "date": "2024-06-16T18:27:00.000Z",
        "voteCount": 1,
        "content": "I'm not sure, you lock the semantic model but you can create new tables in Lakehouse. With option A you'll lock both"
      },
      {
        "date": "2024-06-03T03:25:00.000Z",
        "voteCount": 2,
        "content": "You should configure B. the semantic model settings to prevent new tables from being automatically added to the default semantic model of Lakehouse1"
      },
      {
        "date": "2024-06-02T18:55:00.000Z",
        "voteCount": 3,
        "content": "Open the lakehouse in Microsoft Fabric.\nNavigate to the semantic model section.\nOpen the settings for the default semantic model.\nLook for an option that controls the automatic inclusion of new tables. This might be labeled as \"auto-update\" or something similar.\nDisable or adjust this setting to prevent automatic inclusion"
      },
      {
        "date": "2024-05-28T11:02:00.000Z",
        "voteCount": 3,
        "content": "Explanation: In Microsoft Fabric, the default behavior is to include new tables in the semantic model automatically. To control this behavior and ensure that new tables are not added automatically, you need to adjust the settings related to the semantic model."
      },
      {
        "date": "2024-05-07T11:59:00.000Z",
        "voteCount": 3,
        "content": "correct answer is B\nBy default, the semantic model in a lakehouse is set to automatically include new tables, but you can disable this behavior by modifying the semantic model settings. Specifically, you need to navigate to the semantic model settings for the lakehouse and find the option to disable the automatic inclusion of new tables."
      },
      {
        "date": "2024-04-28T02:30:00.000Z",
        "voteCount": 2,
        "content": "\"Manually enable the Sync the default Power BI semantic model setting for each Warehouse or SQL analytics endpoint in the workspace\""
      },
      {
        "date": "2024-06-11T06:33:00.000Z",
        "voteCount": 1,
        "content": "Sync the default Power BI semantic model : toggle on/off\nAdd objects from the warehouse to the default Power BI semantic model. Also, update the model with any new objects added to the warehouse. You can use this model to build reports faster with warehouse data. (in semantic model settings)"
      },
      {
        "date": "2024-04-19T00:35:00.000Z",
        "voteCount": 5,
        "content": "A - check here: https://learn.microsoft.com/en-us/fabric/data-warehouse/default-power-bi-semantic-model#add-or-remove-objects-to-the-default-power-bi-semantic-model"
      },
      {
        "date": "2024-04-12T04:26:00.000Z",
        "voteCount": 4,
        "content": "it should be SQL end point setting."
      },
      {
        "date": "2024-04-10T22:58:00.000Z",
        "voteCount": 4,
        "content": "A is correct, there is \"Sync the default Power BI semantic model\" setting in sql endpoint."
      },
      {
        "date": "2024-04-07T07:34:00.000Z",
        "voteCount": 2,
        "content": "I think you do that in the Warehouse setting, there is not a Warehouse option but since a warehouse is created when you create the Lakehouse, it should be option D\nhttps://learn.microsoft.com/en-us/fabric/data-warehouse/semantic-models#sync-the-default-power-bi-semantic-model"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138078-exam-dp-600-topic-1-question-82-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant that contains a workspace named Workspace1. Workspace1 contains a lakehouse named Lakehouse1 and a warehouse named Warehouse1.<br><br>You need to create a new table in Warehouse1 named POSCustomers by querying the customer table in Lakehouse1.<br><br>How should you complete the T-SQL statement? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image105.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image106.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-07T07:36:00.000Z",
        "voteCount": 15,
        "content": "Correct answer"
      },
      {
        "date": "2024-04-25T03:10:00.000Z",
        "voteCount": 7,
        "content": "https://learn.microsoft.com/en-us/fabric/data-warehouse/clone-table#limitations\nTable clones across warehouses in a workspace are not currently supported.\n\nANSWER, AS SELECT"
      },
      {
        "date": "2024-07-10T13:31:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-05-28T11:06:00.000Z",
        "voteCount": 4,
        "content": "CREATE TABLE dbo.POSCustomers\nAS\nSELECT DISTINCT customerid, customer, postalcode, category\nFROM lakehouse1.dbo.customer"
      },
      {
        "date": "2024-04-19T00:36:00.000Z",
        "voteCount": 2,
        "content": "answer is correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137992-exam-dp-600-topic-1-question-83-discussion/",
    "body": "You have a Fabric tenant.<br><br>You are creating an Azure Data Factory pipeline.<br><br>You have a stored procedure that returns the number of active customers and their average sales for the current month.<br><br>You need to add an activity that will execute the stored procedure in a warehouse. The returned values must be available to the downstream activities of the pipeline.<br><br>Which type of activity should you add?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend variable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-15T01:46:00.000Z",
        "voteCount": 7,
        "content": "I took the test today, and this question was included. The options were: Switch, Copy Activity, Script, and Stored Procedure. I chose Script."
      },
      {
        "date": "2024-06-17T13:13:00.000Z",
        "voteCount": 1,
        "content": "Script -&gt; https://learn.microsoft.com/en-us/fabric/data-factory/tutorial-preprocess-data-with-stored-procedure-load-into-lakehouse"
      },
      {
        "date": "2024-06-15T18:50:00.000Z",
        "voteCount": 1,
        "content": "Stored proc would be the answer"
      },
      {
        "date": "2024-05-21T02:29:00.000Z",
        "voteCount": 2,
        "content": "Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure. The output can be a singleton value or an array of attributes, which can be consumed in a subsequent copy, transformation, or control flow activities like ForEach activity.\nthe quote from https://learn.microsoft.com/en-us/fabric/data-factory/lookup-activity\n\nso I suppose the answer is correct. But why Append variable isn't correct?"
      },
      {
        "date": "2024-05-09T01:08:00.000Z",
        "voteCount": 4,
        "content": "Lookup activity reads and returns the content of a configuration file or table. It also returns the result of executing a query or stored procedure. The output can be a singleton value or an array of attributes, which can be consumed in a subsequent copy, transformation, or control flow activities like ForEach activity.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/control-flow-lookup-activity"
      },
      {
        "date": "2024-05-08T19:22:00.000Z",
        "voteCount": 1,
        "content": "The correct  answer is D"
      },
      {
        "date": "2024-05-03T23:36:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/tutorial-pipeline-failure-error-handling\n\nUpon Success:\t(Default Pass) Execute this path if the current activity succeeded\nUpon Failure:\tExecute this path if the current activity failed\nUpon Completion:\tExecute this path after the current activity completed, regardless if it succeeded or not\nUpon Skip:\tExecute this path if the activity itself didn't run"
      },
      {
        "date": "2024-04-22T20:27:00.000Z",
        "voteCount": 2,
        "content": "https://pragmaticworks.com/blog/azure-data-factory-lookup-and-stored-procedure"
      },
      {
        "date": "2024-04-19T00:38:00.000Z",
        "voteCount": 2,
        "content": "I'd say D"
      },
      {
        "date": "2024-04-10T23:05:00.000Z",
        "voteCount": 1,
        "content": "it`s strange, because there is activity stored proc in fabric pipes."
      },
      {
        "date": "2024-04-19T00:39:00.000Z",
        "voteCount": 1,
        "content": "not quite sure but I thought these do not return values to a variable which is available for downstream activities as requested in this question."
      },
      {
        "date": "2024-04-07T07:37:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-04-05T12:50:00.000Z",
        "voteCount": 2,
        "content": "ANS D - Lookup"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137991-exam-dp-600-topic-1-question-84-discussion/",
    "body": "You have a Fabric tenant that contains two workspaces named Workspace1 and Workspace2. Workspace1 contains a lakehouse named Lakehouse1. Workspace2 contains a lakehouse named Lakehouse2. Lakehouse1 contains a table named dbo.Sales. Lakehouse2 contains a table named dbo.Customers.<br><br>You need to ensure that you can write queries that reference both dbo.Sales and dbo.Customers in the same SQL query without making additional copies of the tables.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shortcut\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-09T21:34:00.000Z",
        "voteCount": 17,
        "content": "you can not create view directly. you need to create shortcuts first to access table in different workspaces\n.https://community.fabric.microsoft.com/t5/General-Discussion/Cross-Workspace-access/m-p/3744282#:~:text=You%20cannot%20directly%20query%20the,and%20then%20query%20the%20shortcut."
      },
      {
        "date": "2024-06-05T01:01:00.000Z",
        "voteCount": 9,
        "content": "It's a fabric exam and they're really proud of their shortcuts, which are specifically designed for accessing data without making an additional copy of it."
      },
      {
        "date": "2024-09-27T01:28:00.000Z",
        "voteCount": 1,
        "content": "the d option is changed to script, that is the correct one anyway"
      },
      {
        "date": "2024-06-04T04:58:00.000Z",
        "voteCount": 1,
        "content": "My mistake. Please delete my post."
      },
      {
        "date": "2024-06-04T04:56:00.000Z",
        "voteCount": 1,
        "content": "I checked the query below using MS fabric. The below query runs in both lake houses (warehouses under those). As long as they are managed tables, the below query works. \nselect * from Lakehouse1.dbo.Sales\nselect * from Lakehouse2.dbo.Customers"
      },
      {
        "date": "2024-06-04T04:55:00.000Z",
        "voteCount": 1,
        "content": "I checked the query below with MS fabric. The below query runs in both lake houses (warehouses under those). As long as they are tables, the below query works. \nselect * from Lakehouse1.dbo.Sales\nselect * from Lakehouse2.dbo.Customers"
      },
      {
        "date": "2024-06-01T13:48:00.000Z",
        "voteCount": 4,
        "content": "shortcut (100%)"
      },
      {
        "date": "2024-05-28T11:13:00.000Z",
        "voteCount": 2,
        "content": "Shortcuts are specifically designed for cross-lakehouse referencing and provide a seamless and efficient way to query data across different workspaces without data duplication or complex configurations, making them the best choice for this scenario.\nViews typically reference tables within the same database or lakehouse. While you can create views that reference tables in different schemas or databases, they generally do not support cross-workspace references directly. Creating views for cross-workspace data might require additional configuration or data duplication."
      },
      {
        "date": "2024-04-20T13:13:00.000Z",
        "voteCount": 3,
        "content": "Shortcuts enable you to do it in an easy way."
      },
      {
        "date": "2024-04-19T00:42:00.000Z",
        "voteCount": 2,
        "content": "shortcut first"
      },
      {
        "date": "2024-04-18T05:11:00.000Z",
        "voteCount": 1,
        "content": "First create shortcut in order to be able to create view on top"
      },
      {
        "date": "2024-04-05T12:49:00.000Z",
        "voteCount": 3,
        "content": "C. a view\nA view is a virtual table that contains data from one or more tables.\nYou can create a view that references tables from different lakehouses (in this case, dbo.Sales from Lakehouse1 and dbo.Customers from Lakehouse2)."
      },
      {
        "date": "2024-04-19T00:41:00.000Z",
        "voteCount": 3,
        "content": "you cannot reference tables from lakehouses across different workspaces without a shortcut first."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139495-exam-dp-600-topic-1-question-85-discussion/",
    "body": "You have a Fabric tenant that contains a warehouse.<br><br>You are designing a star schema model that will contain a customer dimension. The customer dimension table will be a Type 2 slowly changing dimension (SCD).<br><br>You need to recommend which columns to add to the table. The columns must NOT already exist in the source.<br><br>Which three types of columns should you recommend? Each correct answer presents part of the solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta foreign key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta natural key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan effective end date and time\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta surrogate key\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan effective start date and time\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CDE",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-28T06:19:00.000Z",
        "voteCount": 8,
        "content": "Because the table is set to SCD2 there will be a new row for each change in an existing row with an start and an end date (from when to when the row was valid). therefore the ' old' primary key will be duplicated. That is why a new key, a surrogate key, is introduced which makes each row unique again"
      },
      {
        "date": "2024-05-12T15:15:00.000Z",
        "voteCount": 3,
        "content": "IMHO, CDE,\nbecause:\nstart &amp; end - must\nsurrogate - which is autoincremental id - is must.\n\nLink: https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
      },
      {
        "date": "2024-05-01T01:04:00.000Z",
        "voteCount": 3,
        "content": "To create SCD type 2 one needs to add a surrogate key + start/end date beside the other technical attributes. Therefore CDE."
      },
      {
        "date": "2024-04-25T18:45:00.000Z",
        "voteCount": 1,
        "content": "As per chat GPT: Surrogate keys are typically used in dimension tables rather than fact tables. In a data warehouse, a surrogate key is a unique identifier assigned to each record in a dimension table, usually for internal processing and joining purposes. It provides a stable reference to the dimension record, regardless of any changes in the natural key or other attributes."
      },
      {
        "date": "2024-04-28T03:37:00.000Z",
        "voteCount": 6,
        "content": "chatgpt is not a source"
      },
      {
        "date": "2024-05-01T14:48:00.000Z",
        "voteCount": 1,
        "content": "It is often a better source than some random here..."
      },
      {
        "date": "2024-04-25T02:23:00.000Z",
        "voteCount": 1,
        "content": "B: A natural key would be the Dim tables own Primarykey column not the source Primary key\nC &amp; D: is requires to incorporate SCD Type 2."
      },
      {
        "date": "2024-05-07T07:27:00.000Z",
        "voteCount": 3,
        "content": "A dimension's \"Natural Key\" is usually part of its description elements, like the SSN in the US. That information would already be present in the source table, and it's not the object of the question since it states that \"The columns must NOT already exist in the source.\"\nSince the question states that we are building a star schema, the foreign keys should be present only on the fact table.\n\nThus, the correct answer needs to be CDE."
      },
      {
        "date": "2024-04-25T02:25:00.000Z",
        "voteCount": 1,
        "content": "Sorry there is a type in my comment above it should read C&amp;E \nso the correct answer is BC&amp;E"
      },
      {
        "date": "2024-05-01T01:01:00.000Z",
        "voteCount": 1,
        "content": "I didn't get, why would you create a new \"Dim table own Primarykey\"?"
      },
      {
        "date": "2024-04-24T03:32:00.000Z",
        "voteCount": 2,
        "content": "Let's discard,\n\nA. A FOREIGN KEY in SQL is a key (a column field) that is used to relate two tables. The FOREIGN KEY field is related or linked to the PRIMARY KEY of another database table.\nIt already exists at the origin.\nB. Natural key, likewise, already exists in the origin.\n\nCDE, are the fields that we must create in our ETL to create an SCD2."
      },
      {
        "date": "2024-04-25T02:24:00.000Z",
        "voteCount": 1,
        "content": "Surrogate key is used in Fact Tables not Dim Tables"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138011-exam-dp-600-topic-1-question-86-discussion/",
    "body": "You have a Fabric tenant.<br><br>You plan to create a data pipeline named Pipeline1. Pipeline1 will include two activities that will execute in sequence.<br><br>You need to ensure that a failure of the first activity will NOT block the second activity.<br><br>Which conditional path should you configure between the first activity and the second activity?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon Failure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon Completion\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon Skip",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpon Skip"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-05T01:03:00.000Z",
        "voteCount": 1,
        "content": "A: Will only launch the second one if the first one fails, but it's not given that it will ALWAYS fail\n\nC&amp;D: Upon skip -&gt; activity should be executed"
      },
      {
        "date": "2024-05-14T06:36:00.000Z",
        "voteCount": 3,
        "content": "B: Upon Complete will move to next activities regardless of its success or failure."
      },
      {
        "date": "2024-05-12T15:16:00.000Z",
        "voteCount": 1,
        "content": "IMHO, B - easy game"
      },
      {
        "date": "2024-04-19T00:50:00.000Z",
        "voteCount": 2,
        "content": "completion is correct"
      },
      {
        "date": "2024-04-07T07:40:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-04-05T23:26:00.000Z",
        "voteCount": 2,
        "content": "B. Its correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138102-exam-dp-600-topic-1-question-87-discussion/",
    "body": "You have a Microsoft Power BI semantic model.<br><br>You need to identify any surrogate key columns in the model that have the Summarize By property set to a value other than to None. The solution must minimize effort.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Formatter in DAX Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModel explorer in Microsoft Power BI Desktop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModel view in Microsoft Power BI Desktop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBest Practice Analyzer in Tabular Editor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T00:56:00.000Z",
        "voteCount": 11,
        "content": "Best Practice Analyzer should be able to identify these with some rules"
      },
      {
        "date": "2024-05-28T11:20:00.000Z",
        "voteCount": 1,
        "content": "The Best Practice Analyzer (BPA) in Tabular Editor can be configured to check for specific properties and configurations in your model. This includes identifying columns with certain \"Summarize By\" settings.\nYou can create or use existing BPA rules to quickly identify any surrogate key columns that do not have the \"Summarize By\" property set to \"None,\" thus minimizing the manual effort required."
      },
      {
        "date": "2024-05-16T10:51:00.000Z",
        "voteCount": 1,
        "content": "D (although it can be done it B, but not as efficiently as D)\nTo identify surrogate key columns with the 'Summarize By' property set to a value other than 'None,' the Best Practice Analyzer in Tabular Editor is the most efficient tool. The Best Practice Analyzer can analyze the entire model and provide a report on all columns that do not meet a specified best practice, such as having the 'Summarize By' property set correctly for surrogate key columns. Here's how you would proceed:\nOpen your Power BI model in Tabular Editor.\nGo to the Advanced Scripting window.\nWrite or use an existing script that checks the 'Summarize By' property of each column.\nExecute the script to get a report on the surrogate key columns that do not have their 'Summarize By' property set to 'None'.\n\nYou can then review and adjust the properties of the columns directly within the Tabular Editor."
      },
      {
        "date": "2024-05-12T15:22:00.000Z",
        "voteCount": 1,
        "content": "I go with D) Best Practice Analyzer"
      },
      {
        "date": "2024-04-07T15:05:00.000Z",
        "voteCount": 1,
        "content": "It should be B\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/model-explorer#anatomy-of-model-explorer\n\nhttps://www.purplefrogsystems.com/2021/08/how-to-change-the-summarization-of-multiple-columns-in-power-bi/#:~:text=Once%20you've%20selected%20the,That's%20it!"
      },
      {
        "date": "2024-04-09T18:21:00.000Z",
        "voteCount": 8,
        "content": "The question is about identifying, not changing. I think it should be D."
      },
      {
        "date": "2024-04-19T00:56:00.000Z",
        "voteCount": 1,
        "content": "agree, question is about identifying not changing so probably it's D."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138005-exam-dp-600-topic-1-question-88-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a Fabric tenant that contains a Microsoft Power BI report named Report1.<br><br>Report1 is slow to render. You suspect that an inefficient DAX query is being executed.<br><br>You need to identify the slowest DAX query, and then review how long the query spends in the formula engine as compared to the storage engine.<br><br>Which five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-600/image107.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image108.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T17:16:00.000Z",
        "voteCount": 34,
        "content": "Good Video: https://www.youtube.com/watch?v=C5HBhlLUFsE&amp;t=176s\nFrom performance analyzer capture code. \nSort duration descending by DAX query time.\nCopy the first query over to Dax Studio\nEnable query and server timings and run the query\nView Server timings"
      },
      {
        "date": "2024-06-15T02:01:00.000Z",
        "voteCount": 8,
        "content": "I took the test today, and this question was included. The options 'Sort the Duration (ms) column in descending order' was not there. Hence the answer:\n-From Performance analyzer, capture a recording.\n-Sort the Duration (ms) column in descending order by DAX query time.\n-Copy the first query to Dax Studio.\n-Enable Query Timings and Server Timings. Run the query.\n-View the Server Timings tab."
      },
      {
        "date": "2024-10-08T12:00:00.000Z",
        "voteCount": 1,
        "content": "From Performance analyzer, capture a recording - This will start recording the performance of the report.\nEnable Query Timings and Server Timings. Run the query - This step captures detailed timings of the query execution on both the formula engine and storage engine.\nView the Query Timings tab - Here you can analyze how long the query takes in different phases of execution.\nSort the Duration (ms) column in descending order by DAX query time - This will help identify the slowest DAX query.\nCopy the first query to DAX Studio - Once the slowest query is identified, you can further analyze it using DAX Studio for optimization."
      },
      {
        "date": "2024-05-12T15:51:00.000Z",
        "voteCount": 2,
        "content": "IMHO, \n\n1. Start Recording\n2. Sort in descending order\n3. Copy to DAX studio (the problematic query)\n4. Enable query t and server t (to gather additional info)\n5. View Server Timings (to understand it is storage engine or formula engine)\n\nIt is taken from the YouTube video below."
      },
      {
        "date": "2024-04-22T13:46:00.000Z",
        "voteCount": 2,
        "content": "Here are the steps you should follow to identify the slowest DAX query and review how long the query spends in the formula engine compared to the storage engine:\n\n1.Open the report in Power BI Desktop. Go to the 'View' tab and click on 'Performance Analyzer' and Click on 'Start Recording', then 'Refresh Visuals' \n2.Review the DAX query and duration in the performance analyzer pane. Sort the Duration (ms) column in descending order\n3.Copy the first query to DAX Studio \n4.In DAX Studio, enable Query Timings and Server Timings. Run the query\n5.View the Server Timings tab to see data for the formula engine (FE) and the storage engine (SE)"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139059-exam-dp-600-topic-1-question-89-discussion/",
    "body": "You have a Fabric tenant that contains a semantic model. The model contains 15 tables.<br><br>You need to programmatically change each column that ends in the word Key to meet the following requirements:<br><br>\u2022\tHide the column.<br>\u2022\tSet Nullable to False<br>\u2022\tSet Summarize By to None.<br>\u2022\tSet Available in MDX to False.<br>\u2022\tMark the column as a key column.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Power BI Desktop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALM Toolkit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabular Editor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-12T16:02:00.000Z",
        "voteCount": 5,
        "content": "IMHO, C) Tabulr Editor is good.\n\nHere's why the other options are not ideal for this scenario:\n\n1) Microsoft Power BI Desktop: doesn't offer functionalities for bulk programmatic changes based on naming conventions.\n2) ALM Toolkit: it's not specifically designed for modifying the structure of semantic models within the tool itself.\n3) DAX Studio doesn't provide functionalities for directly modifying model structure or properties en masse."
      },
      {
        "date": "2024-04-21T23:49:00.000Z",
        "voteCount": 3,
        "content": "Tabular Editor seems most correct for me"
      },
      {
        "date": "2024-04-17T23:59:00.000Z",
        "voteCount": 1,
        "content": "Why option A is not correct? You can make all this options by using PBI Desktop... Am I wrong?"
      },
      {
        "date": "2024-04-21T23:50:00.000Z",
        "voteCount": 3,
        "content": "yes, in theory, however, not programmatically, only manually"
      },
      {
        "date": "2024-04-21T16:09:00.000Z",
        "voteCount": 7,
        "content": "Tabular Editor allows you to use C# scripts to automate these changes. In PBI Desktop you can do this only manually"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138007-exam-dp-600-topic-1-question-90-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Microsoft Power BI semantic model.<br><br>You plan to implement calculation groups.<br><br>You need to create a calculation item that will change the context from the selected date to month-to-date (MTD).<br><br>How should you complete the DAX expression? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image109.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image110.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-25T23:03:00.000Z",
        "voteCount": 17,
        "content": "CALCULATE\nSELECTEDMEASURE\nmore info refer to:\nhttps://www.sqlbi.com/articles/using-calculation-groups-to-selectively-replace-measures-in-dax-expressions/"
      },
      {
        "date": "2024-04-05T17:30:00.000Z",
        "voteCount": 6,
        "content": "CALCULATE(Selectedvalue(), DATESMTD('Date'[Date])) combines two DAX functions to achieve a specific result\nSelectedvalue():\nThis function returns the value of the currently selected item in a column.\nIt is commonly used in scenarios where you want to retrieve a single value from a column (such as a slicer selection).\nDATESMTD('Date'[Date]):\nThe DATESMTD function calculates the month-to-date (MTD) total for a given expression.\nIt considers all dates from the beginning of the month up to the current date defined by the filter context.\nIn this case, it operates on the 'Date'[Date] column.\nOverall Purpose:\nThe entire expression calculates the MTD value for the currently selected item (such as a date) based on the date context.\nIt\u2019s useful for creating dynamic MTD calculations that adjust automatically based on user selections.\nFor example, if you have a measure called \u201cSales Amount\u201d and you want to calculate the MTD sales amount based on the selected date, this expression would give you that value."
      },
      {
        "date": "2024-04-21T16:16:00.000Z",
        "voteCount": 28,
        "content": "Don't confuse people, the answer is correct. Calculate  + selectedmeasure() is a standard combination when it comes to calculation items. When Calc. Item is applied dax engine replaces selectedmeasure() with measure reference."
      },
      {
        "date": "2024-05-12T16:06:00.000Z",
        "voteCount": 4,
        "content": "IMHO, Calculate / SELECTEDMEASURE,\n\nSource: https://learn.microsoft.com/en-us/dax/selectedmeasure-function-dax#example"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138103-exam-dp-600-topic-1-question-91-discussion/",
    "body": "You have a Microsoft Power BI report named Report1 that uses a Fabric semantic model.<br><br>Users discover that Report1 renders slowly.<br><br>You open Performance analyzer and identify that a visual named Orders By Date is the slowest to render. The duration breakdown for Orders By Date is shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image111.png\"><br><br>What will provide the greatest reduction in the rendering duration of Report1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable automatic page refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOptimize the DAX query of Orders By Date by using DAX Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the visual type of Orders By Date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of visuals in Report1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T15:20:00.000Z",
        "voteCount": 10,
        "content": "https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-performance-analyzer#display-the-performance-analyzer-pane\n\n\"Other - This is the time required by the visual for preparing queries, waiting for other visuals to complete, or performing other background processing.\""
      },
      {
        "date": "2024-06-20T09:49:00.000Z",
        "voteCount": 1,
        "content": "Chatbot said the answer is c"
      },
      {
        "date": "2024-06-02T02:20:00.000Z",
        "voteCount": 1,
        "content": "Other is referring to the time needed to waiting for other visuals to complete."
      },
      {
        "date": "2024-05-29T06:22:00.000Z",
        "voteCount": 1,
        "content": "While optimizing the DAX query could slightly improve performance, the DAX query duration (27 ms) is already very low. The \"Other\" duration (1047 ms) is significantly higher than the DAX query (27 ms) and the visual display (39 ms) durations combined. This indicates that most of the time is spent on backend processes such as data preparation, transformations, or communication with the data source. By reducing the number of visuals in Report1, you can decrease the overall load on the report rendering process."
      },
      {
        "date": "2024-05-12T16:10:00.000Z",
        "voteCount": 1,
        "content": "IMHO, D) Reducing visuals\n\nbecause according to Microsoft, it is the queries under them.\nLink as from colleagues below."
      },
      {
        "date": "2024-04-22T00:14:00.000Z",
        "voteCount": 4,
        "content": "\"Other is slow\" -&gt; \"Other\" implies e.g. waiting time for other visuals. D is the solutio."
      },
      {
        "date": "2024-04-11T12:26:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer\nFor D. While reducing the number of visuals in Report1 can potentially improve overall performance by reducing the workload on the rendering engine, it may not directly address the specific issue of slow DAX query performance in the \"Orders By Date\" visual. It's better to focus on optimizing the problematic visual first."
      },
      {
        "date": "2024-04-22T04:42:00.000Z",
        "voteCount": 1,
        "content": "The DAX itself it not the problem, as most of the time comes from \"Other\""
      },
      {
        "date": "2024-04-22T00:14:00.000Z",
        "voteCount": 1,
        "content": "\"Other\" is slow, \"DAX query\" performance is ok. \"Other\" implies e.g. waiting time for other visuals. So can't be B. D is the solutio."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138006-exam-dp-600-topic-1-question-92-discussion/",
    "body": "You have a custom Direct Lake semantic model named Model1 that has one billion rows of data.<br>You use Tabular Editor to connect to Model1 by using the XMLA endpoint.<br><br>You need to ensure that when users interact with reports based on Model1, their queries always use Direct Lake mode.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Model, configure the Default Mode option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Partitions, configure the Mode option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Model, configure the Storage Location option.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Model, configure the Direct Lake Behavior option.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-07T15:24:00.000Z",
        "voteCount": 8,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-overview#fallback-behavior\n\nThe DirectLakeBehavior property can be configured by using Tabular Object Model (TOM) or Tabular Model Scripting Language (TMSL).\n\nThe following example specifies all queries use Direct Lake mode only:\n\nC#\n\nCopy\n// Disable fallback to DirectQuery mode.\n//\ndatabase.Model.DirectLakeBehavior = DirectLakeBehavior.DirectLakeOnly = 1;\ndatabase.Model.SaveChanges();"
      },
      {
        "date": "2024-04-05T17:28:00.000Z",
        "voteCount": 5,
        "content": "Click on Semantic model.\nIn the Properties pane, choose the Direct Lake behavior for your custom Direct Lake semantic model:\nAutomatic: This is the default behavior. It allows Direct Lake with fallback to DirectQuery mode if data can\u2019t be efficiently loaded into memory.\nDirect Lake only: This option ensures no fallback to DirectQuery mode\nhttps://powerbi.microsoft.com/en-au/blog/leveraging-pure-direct-lake-mode-for-maximum-query-performance/"
      },
      {
        "date": "2024-06-24T10:22:00.000Z",
        "voteCount": 1,
        "content": "The correct option to ensure queries use Direct Lake mode in Tabular Editor is:\n\nD. From Model, configure the Direct Lake Behavior option.\n\nHere's why the other options are incorrect:\n\nA. Default Mode: This option doesn't exist specifically for Direct Lake mode. It might be related to a different setting within the model.\nB. Partitions: Partitions are used to manage data refresh in large models, not to define the overall query mode.\nC. Storage Location: This option specifies where the model metadata is stored, not the data retrieval method."
      },
      {
        "date": "2024-05-12T16:14:00.000Z",
        "voteCount": 1,
        "content": "IMHO, D, \n\nBecause:\nDirect Lake models include the DirectLakeBehavior property, which has three options:\n...\n**DirectLakeOnly** - Specifies all queries use Direct Lake mode only. Fallback to DirectQuery mode is disabled. If data can't be loaded into memory, an error is returned. Use this setting to determine if DAX queries fail to load data into memory, forcing an error to be returned.\n...\nThe DirectLakeBehavior property can be configured by using Tabular Object Model (TOM) or Tabular Model Scripting Language (TMSL).\n\nLink: https://learn.microsoft.com/en-us/power-bi/enterprise/directlake-overview#fallback-behavior"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138008-exam-dp-600-topic-1-question-93-discussion/",
    "body": "DRAG DROP<br> -<br><br>You create a semantic model by using Microsoft Power BI Desktop. The model contains one security role named SalesRegionManager and the following tables:<br><br>\u2022\tSales<br>\u2022\tSalesRegion<br>\u2022\tSalesAddress<br><br>You need to modify the model to ensure that users assigned the SalesRegionManager role cannot see a column named Address in SalesAddress.<br><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-600/image112.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image113.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T17:42:00.000Z",
        "voteCount": 26,
        "content": "Setting OLS to None:\nWhen you set OLS to None for a table or column:\no\tFor viewers without the appropriate access levels, it\u2019s as if the secured tables or columns don\u2019t exist.\n\u2022\tThe table or column will be hidden from that role.\n\u2022\tUsers won\u2019t be able to see or interact with the secured data.\nHow To:\nhttps://www.insightsarena.com/post/mastering-object-level-security-in-power-bi-fixing-broken-visuals#:~:text=Configure%20Object-Level%20Security%3A%201%20In%20Tabular%20Editor%2C%20navigate,need%20to%20be%20restricted%20for%20a%20particular%20role.\n\u2022\tIn Tabular Editor, navigate to Tables and select the specific column names.\n\u2022\tUnder Object-Level Security, set the security level to \"None\" for the role that needs restrictions.\n\n1 Open the model in Tabular Editor\n2 Select the address column in SalesAddress\n3 Set the Object Level Security to None for the sales manager role."
      },
      {
        "date": "2024-04-30T16:35:00.000Z",
        "voteCount": 5,
        "content": "Answer is correct; ref: https://learn.microsoft.com/en-us/fabric/security/service-admin-object-level-security?tabs=table"
      },
      {
        "date": "2024-05-29T06:29:00.000Z",
        "voteCount": 1,
        "content": "Not 100% sure, but I would:\n1. Open the model in Tabular Editor.\n2. Select the Address column in SalesAddress.\n3. Set the Hidden property to True."
      },
      {
        "date": "2024-05-12T16:19:00.000Z",
        "voteCount": 1,
        "content": "1) Tabular Editor - Open the Model\n2) Select Address\n3) Object Level Security (OLS) = None\n\nBecause: \nTo create roles on Power BI Desktop semantic models, use external tools such as Tabular Editor.\n...\nObject-level security (OLS) enables model authors to secure specific tables or columns from report viewers.\n...\nNone: OLS is enforced and the table or column will be hidden from that role\nRead: The table or column will be visible to that role\n\nLink: https://learn.microsoft.com/en-us/fabric/security/service-admin-object-level-security?tabs=table"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138114-exam-dp-600-topic-1-question-94-discussion/",
    "body": "You have a Microsoft Power BI semantic model that contains measures. The measures use multiple CALCULATE functions and a FILTER function.<br><br>You are evaluating the performance of the measures.<br><br>In which use case will replacing the FILTER function with the KEEPFILTERS function reduce execution time?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twhen the FILTER function uses a nested calculate function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twhen the FILTER function references a measure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twhen the FILTER function references columns from multiple tables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\twhen the FILTER function references a column from a single table that uses Import mode\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-17T02:02:00.000Z",
        "voteCount": 13,
        "content": "Explanation here: https://learn.microsoft.com/en-us/dax/best-practices/dax-avoid-avoid-filter-as-filter-argument\nFILTER returns a table whereas KEEPFILTERS returns a Boolean. So, A, B and C are limitations of uses of Boolean expressions."
      },
      {
        "date": "2024-07-18T20:55:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: B\nReplace FILTER with KEEPFILTERS in measures when you want to retain and respect existing filters in the current context, leading to potentially improved performance by reducing the overhead associated with creating new filter contexts."
      },
      {
        "date": "2024-07-11T14:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2024-06-18T00:03:00.000Z",
        "voteCount": 1,
        "content": "The answer is A we use keep filters for nesting\nhttps://learn.microsoft.com/en-us/dax/keepfilters-function-dax"
      },
      {
        "date": "2024-06-14T13:45:00.000Z",
        "voteCount": 3,
        "content": "The answer is D based on this page: https://learn.microsoft.com/en-us/dax/best-practices/dax-avoid-avoid-filter-as-filter-argument . Check out this Note in the given link \"This article is especially relevant for model calculations that apply filters to Import tables.\". Also, see this section extract in the given link :  \"restrictions that apply to Boolean expressions(KEEP Filters) when they're used as filter arguments. They:\n\n-Cannot reference columns from multiple tables\n-Cannot reference a measure\n-Cannot use nested CALCULATE functions\n-Cannot use functions that scan or return a table"
      },
      {
        "date": "2024-06-02T03:04:00.000Z",
        "voteCount": 1,
        "content": "KEEPFILTERS is a filter modifier that does not delete existing column or table filters in the filter context that conflict with the filters applied by the KEEPFILTERS parameter.\n\nhttps://community.fabric.microsoft.com/t5/Desktop/Conceptual-Question-FILTER-vs-KEEPFILTERS-vs-neither-What-is-the/td-p/1438803"
      },
      {
        "date": "2024-05-29T06:40:00.000Z",
        "voteCount": 2,
        "content": "The use case in which replacing the FILTER function with the KEEPFILTERS function will most likely reduce execution time is when the FILTER function references a column from a single table that uses Import mode. This scenario allows KEEPFILTERS to optimize the filter application process more efficiently than FILTER."
      },
      {
        "date": "2024-05-28T02:11:00.000Z",
        "voteCount": 2,
        "content": "This is because KEEPFILTERS can efficiently maintain the current filter context on the imported table without the need to create a new table through FILTER, thus reducing the computational overhead and improving performance."
      },
      {
        "date": "2024-05-17T02:45:00.000Z",
        "voteCount": 4,
        "content": "D. ref: https://learn.microsoft.com/en-us/dax/best-practices/dax-avoid-avoid-filter-as-filter-argument"
      },
      {
        "date": "2024-04-28T03:56:00.000Z",
        "voteCount": 1,
        "content": "I'm a bit confused, in any case I will go with A because context transition"
      },
      {
        "date": "2024-04-25T03:15:00.000Z",
        "voteCount": 1,
        "content": "This is a very confusing question.\nIf a FILTER references the entire table rather than a single table, then using KEEPFILTER is a better choice, but none of the options actually specify that."
      },
      {
        "date": "2024-04-23T13:07:00.000Z",
        "voteCount": 1,
        "content": "Replacing the FILTER function with the KEEPFILTERS function will reduce execution time in the use case where the FILTER function references columns from multiple tables (Option C). This is because KEEPFILTERS is more efficient when it intersects with an existing filter context, which can occur when conditions involve multiple columns (https://www.sqlbi.com/articles/using-keepfilters-in-dax-updated/)"
      },
      {
        "date": "2024-04-16T02:40:00.000Z",
        "voteCount": 2,
        "content": "When you replace FILTER with KEEPFILTERS, you avoid re-evaluating the entire table for each row. Instead, it leverages the existing context, resulting in better performance."
      },
      {
        "date": "2024-04-12T13:24:00.000Z",
        "voteCount": 3,
        "content": "A is correct because\nOption A (when the FILTER function uses a nested calculate function): The FILTER function in DAX filters a table to the rows that meet the condition provided in its arguments. When FILTER is used within a CALCULATE function, it typically modifies the context in which the calculation is performed. If FILTER uses a nested CALCULATE function, it creates a context transition. Using KEEPFILTERS in this scenario can improve performance by modifying the behavior of context transition to ensure that existing filters on a table are not unintentionally removed or overridden, which can otherwise increase the complexity and computational load of the query."
      },
      {
        "date": "2024-04-12T07:15:00.000Z",
        "voteCount": 4,
        "content": "he KEEPFILTERS function in Power BI is primarily designed to work with single-table or single-column filter contexts. When you use the FILTER function to reference columns from multiple tables, KEEPFILTERS may not work directly because it operates within the context of a single table or column.\n\nA is the ans"
      },
      {
        "date": "2024-04-09T08:01:00.000Z",
        "voteCount": 2,
        "content": "view this --&gt; https://community.fabric.microsoft.com/t5/DAX-Commands-and-Tips/1-Diff-between-Filter-and-KeepFilters/m-p/2694220"
      },
      {
        "date": "2024-04-08T12:55:00.000Z",
        "voteCount": 3,
        "content": "In general, every time you have to preserve the combination of columns coming from different tables, KEEPFILTERS is probably needed. Even though this could be required for columns of the same table, KEEPFILTERS is not required whenever there is a column that represents a cardinality higher than the intersection of the other columns \u2013 such as Date[Calendar Year Month] instead of the intersection of Date[Month] and Date[Calendar Year].\nhttps://www.sqlbi.com/articles/when-to-use-keepfilters-over-iterators/"
      },
      {
        "date": "2024-04-25T03:10:00.000Z",
        "voteCount": 1,
        "content": "You cannot reference columns from multiple tables within the same FILTER"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138009-exam-dp-600-topic-1-question-95-discussion/",
    "body": "You have a semantic model named Model1. Model1 contains five tables that all use Import mode. Model1 contains a dynamic row-level security (RLS) role named HR. The HR role filters employee data so that HR managers only see the data of the department to which they are assigned.<br><br>You publish Model1 to a Fabric tenant and configure RLS role membership. You share the model and related reports to users.<br><br>An HR manager reports that the data they see in a report is incomplete.<br><br>What should you do to validate the data seen by the HR Manager?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Test as role to view the data as the HR role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter the data in the report to match the intended logic of the filter for the HR department.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Test as role to view the report as the HR manager.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the HR manager to open the report in Microsoft Power BI Desktop."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T05:03:00.000Z",
        "voteCount": 10,
        "content": "Although A basically is true as well, C would be \"the most\" correct. For instance, if the HR manager is not part of the HR role, doing as described in A would not help you troubleshoot the issue. Also, if the RLS is set up such that different HR managers have different rows visible, you have to select the person. Thus, C is more correct than A."
      },
      {
        "date": "2024-04-21T08:12:00.000Z",
        "voteCount": 7,
        "content": "Role name is HR not HR manager so the answer is A."
      },
      {
        "date": "2024-10-08T13:07:00.000Z",
        "voteCount": 1,
        "content": "Answer is A\nA. Select Test as role to view the data as the HR role: This option allows you to simulate the RLS settings directly. By testing the role, you can see exactly what data is being presented to users assigned to the HR role, which helps you identify if the filtering logic is correctly implemented and if there are any issues with data visibility.\n\nB. Filter the data in the report to match the intended logic of the filter for the HR department: While this might help in analyzing the report data, it does not address whether the RLS is functioning as intended. The filtering in the report might not accurately represent the RLS logic.\n\nC. Select Test as role to view the report as the HR manager: This option would allow you to see the report from the perspective of the HR manager, but it may not provide clarity on how the RLS is filtering data specifically.\n\nD. Ask the HR manager to open the report in Microsoft Power BI Desktop: This option does not directly help you validate the data as it would not give you insight into the RLS logic being applied."
      },
      {
        "date": "2024-08-01T19:32:00.000Z",
        "voteCount": 1,
        "content": "The \"Test as role\" feature in Power BI allows you to impersonate a role to see the data as a user assigned to that role would see it. This helps in validating the RLS settings and ensuring that the data is being filtered correctly according to the RLS rules."
      },
      {
        "date": "2024-07-12T19:14:00.000Z",
        "voteCount": 1,
        "content": "A is the correct answer"
      },
      {
        "date": "2024-07-12T01:57:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is C., 100%. The key is in the \"dynamic\" row-level security. \nSelecting A. (HR Role only) would work for a static rule for RLS, but as it is dynamic you will need to input as well the HR manager email and make sure the rule is applied correctly."
      },
      {
        "date": "2024-07-11T14:25:00.000Z",
        "voteCount": 1,
        "content": "C View as report is correct"
      },
      {
        "date": "2024-06-23T02:49:00.000Z",
        "voteCount": 1,
        "content": "role name is HR"
      },
      {
        "date": "2024-06-19T16:51:00.000Z",
        "voteCount": 1,
        "content": "I chose A because only role and not a specific user can be tested using \"Test as role\"."
      },
      {
        "date": "2024-06-18T13:39:00.000Z",
        "voteCount": 2,
        "content": "Theres no role called HR MANAGER !!!\nAnswer is A"
      },
      {
        "date": "2024-05-29T06:49:00.000Z",
        "voteCount": 1,
        "content": "Option C (Select \"Test as role\" to view the report as the HR manager) is the best approach as it directly validates what the specific HR manager sees under the dynamic RLS conditions, ensuring the completeness and accuracy of the data.\nA. Select \"Test as role\" to view the data as the HR role: This option is useful, but it doesn't specify viewing the report as the specific HR manager, which is crucial to identify user-specific issues."
      },
      {
        "date": "2024-05-17T02:48:00.000Z",
        "voteCount": 2,
        "content": "A.\nthe question is about how to validate role of a user. https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-rls#validate-the-roles-within-power-bi-desktop"
      },
      {
        "date": "2024-05-14T14:34:00.000Z",
        "voteCount": 1,
        "content": "what is the difference btw see data (A) or see report (C)?"
      },
      {
        "date": "2024-05-12T16:38:00.000Z",
        "voteCount": 1,
        "content": "IMHO, C) HR Manager is the winner.\n\nNo HR role, because each HR assigned to the own department."
      },
      {
        "date": "2024-05-02T23:35:00.000Z",
        "voteCount": 1,
        "content": "You have to check the problem of a particular user. So we should select \"C\". Maybe the user isn't part of the role. So \"A\" would achieve nothing."
      },
      {
        "date": "2024-04-30T08:26:00.000Z",
        "voteCount": 1,
        "content": "A &amp; C: says \"Test\" as role. We do not know what conditions we have in Test.\nTo test these conditions we should select B"
      },
      {
        "date": "2024-05-14T14:39:00.000Z",
        "voteCount": 1,
        "content": "That's what i was thinking and my way to test and do comparison, but haven't done that in fabric so interesting in why would we choose A or C instead"
      },
      {
        "date": "2024-04-15T07:32:00.000Z",
        "voteCount": 3,
        "content": "you can test roles on report view , so you can see the report as \" \" \nanswer is C"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137953-exam-dp-600-topic-1-question-96-discussion/",
    "body": "You have a Microsoft Fabric tenant that contains a dataflow.<br><br>You are exploring a new semantic model.<br><br>From Power Query, you need to view column information as shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-600/image114.png\"><br><br>Which three Data view options should you select? Each correct answer presents part of the solution.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column value distribution\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable details pane",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable column profile\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column quality details\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column profile in details pane"
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-05T10:11:00.000Z",
        "voteCount": 19,
        "content": "C Enable column profile\nA Show column value distribution\nD Show Column quality details"
      },
      {
        "date": "2024-06-13T00:15:00.000Z",
        "voteCount": 5,
        "content": "As per the screenshot, there are only 2 options to show that information: 1st is \"Column Distribution\" and 2nd is \"Column Quality\". I don't know why it is asking for 3 and when we select \"Column profile\" it shows statistics and value distribution."
      },
      {
        "date": "2024-06-18T00:42:00.000Z",
        "voteCount": 1,
        "content": "I agree, column profile is not necesary to show the info in pictures. But question request 3 option, so we have to add it"
      },
      {
        "date": "2024-06-20T11:20:00.000Z",
        "voteCount": 5,
        "content": "In PowerQuery in Dataflow 'Enable column profile' is there and it has to be selected to show other view options. However, PowerQuery in PowerBI does not have this option 'Enable column profile'."
      },
      {
        "date": "2024-06-16T07:58:00.000Z",
        "voteCount": 1,
        "content": "Column Profile is one more checkbox you overlooked"
      },
      {
        "date": "2024-05-29T06:58:00.000Z",
        "voteCount": 2,
        "content": "Show column value distribution: This option provides a visual representation of the distribution of values in each column, which is visible in the exhibit.\n\nEnable column profile: This option displays statistics and other detailed information about each column, including value distribution, which aligns with the data shown in the exhibit.\n\nShow column quality details: This option shows the quality of the data in each column, indicating valid, error, and empty values, as displayed in the exhibit."
      },
      {
        "date": "2024-05-12T16:41:00.000Z",
        "voteCount": 2,
        "content": "IMHO, ACD is good.\n\nThere is no example for details, so B and E are not needed"
      },
      {
        "date": "2024-04-27T08:23:00.000Z",
        "voteCount": 4,
        "content": "ACD for sure. Tested"
      },
      {
        "date": "2024-04-27T08:22:00.000Z",
        "voteCount": 2,
        "content": "ACD for sure"
      },
      {
        "date": "2024-04-16T04:19:00.000Z",
        "voteCount": 3,
        "content": "Tested it with a dataflow in Fabric"
      },
      {
        "date": "2024-04-16T02:54:00.000Z",
        "voteCount": 4,
        "content": "A: Show column value \nC: Enable column profile\nD: Show Column quality"
      },
      {
        "date": "2024-04-05T01:54:00.000Z",
        "voteCount": 2,
        "content": "Hm.Just tested this. I suggest this answers: C \u2013 enable column profile, E \u2013 Show column quality details, A-Show Column Value distribution. Dette er testet"
      },
      {
        "date": "2024-04-07T02:31:00.000Z",
        "voteCount": 3,
        "content": "Sorry misleading it is C A D as taphyoe said"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137979-exam-dp-600-topic-1-question-97-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric warehouse that contains a table named Sales.Products. Sales.Products contains the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image115.png\"><br><br>You need to write a T-SQL query that will return the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image116.png\"><br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image117.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image118.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-07T07:56:00.000Z",
        "voteCount": 21,
        "content": "GREATEST, COALESCE"
      },
      {
        "date": "2024-07-11T05:44:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer \nGREATEST, COALESCE"
      },
      {
        "date": "2024-06-02T23:14:00.000Z",
        "voteCount": 2,
        "content": "SELECT \n    ProductID,\n    GREATEST(ListPrice, WholesalePrice, AgentPrice) AS HighestSellingPrice,\n    COALESCE(AgentPrice, WholesalePrice, ListPrice) AS TradePrice\nFROM \n    Sales.Products;"
      },
      {
        "date": "2024-05-24T09:06:00.000Z",
        "voteCount": 3,
        "content": "I believe that the 2nd box should be COALESCE, but if you read requirements for TradePrice column, you will realize that actually, COALESCE will not work here. Clearly there's a mistake somewhere."
      },
      {
        "date": "2024-04-27T08:56:00.000Z",
        "voteCount": 3,
        "content": "Greatest is ok; however, if we have all of them present, list price will return first if you use coalesce, which is not what the question requests. The question requests AgentPrice first. I am not sure about Coalesce."
      },
      {
        "date": "2024-05-21T04:52:00.000Z",
        "voteCount": 1,
        "content": "Yes, it seems like there is no correct answer for the second box"
      },
      {
        "date": "2024-04-22T00:47:00.000Z",
        "voteCount": 2,
        "content": "answer is correct"
      },
      {
        "date": "2024-04-05T09:13:00.000Z",
        "voteCount": 1,
        "content": "MAX\nCOALESCE"
      },
      {
        "date": "2024-04-07T15:58:00.000Z",
        "voteCount": 3,
        "content": "MAX() accepts one argument; GREATEST() accepts multiple arguments"
      },
      {
        "date": "2024-04-05T21:36:00.000Z",
        "voteCount": 5,
        "content": "correct - GREATEST  and  COALESCE"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138079-exam-dp-600-topic-1-question-98-discussion/",
    "body": "You have a Fabric notebook that has the Python code and output shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-600/image119.png\"><br><br>Which type of analytics are you performing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescriptive\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdiagnostic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tprescriptive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpredictive"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-12T16:49:00.000Z",
        "voteCount": 3,
        "content": "IMHO, definitely A) Descriptive\n\nLink: https://azure.microsoft.com/es-es/blog/answering-whats-happening-whys-happening-and-what-will-happen-with-iot-analytics/\n\nBecause of it is \"What is happening\""
      },
      {
        "date": "2024-04-22T00:48:00.000Z",
        "voteCount": 2,
        "content": "classic descriptive visual"
      },
      {
        "date": "2024-04-07T07:56:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138080-exam-dp-600-topic-1-question-99-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric warehouse that contains a table named Sales.Orders. Sales.Orders contains the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image120.png\"><br><br>You need to write a T-SQL query that will return the following columns.<br><br><img src=\"https://img.examtopics.com/dp-600/image121.png\"><br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image122.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image123.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-05-12T17:01:00.000Z",
        "voteCount": 10,
        "content": "IMHO, datetrunc &amp; weekday\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/datetrunc-transact-sql?view=sql-server-ver16\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/datename-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-05-02T23:40:00.000Z",
        "voteCount": 7,
        "content": "Just echecked it. Date trunc returns the first date of the month and datepart just the month number. So given answer is correct."
      },
      {
        "date": "2024-10-06T07:03:00.000Z",
        "voteCount": 1,
        "content": "Note: The weekday, timezoneoffset, and nanosecond T-SQL dateparts aren't supported for DATETRUNC. https://learn.microsoft.com/en-us/sql/t-sql/functions/datetrunc-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-04-28T04:11:00.000Z",
        "voteCount": 1,
        "content": "The DATETRUNC function returns an input date truncated to a specified datepart.\n answer is correct"
      },
      {
        "date": "2024-04-27T00:16:00.000Z",
        "voteCount": 1,
        "content": "DATEFROMPARTS , weekday are the correct answers"
      },
      {
        "date": "2024-04-22T00:57:00.000Z",
        "voteCount": 4,
        "content": "answer is correct"
      },
      {
        "date": "2024-04-07T07:59:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138081-exam-dp-600-topic-1-question-100-discussion/",
    "body": "You have a Fabric tenant that contains JSON files in OneLake. The files have one billion items.<br><br>You plan to perform time series analysis of the items.<br><br>You need to transform the data, visualize the data to find insights, perform anomaly detection, and share the insights with other business users. The solution must meet the following requirements:<br><br>\u2022\tUse parallel processing.<br>\u2022\tMinimize the duplication of data.<br>\u2022\tMinimize how long it takes to load the data.<br><br>What should you use to transform and visualize the data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe PySpark library in a Fabric notebook\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe pandas library in a Fabric notebook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Power BI report that uses core visuals"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-29T07:16:00.000Z",
        "voteCount": 3,
        "content": "PySpark provides robust capabilities for data transformation and manipulation, making it well-suited for preparing data for time series analysis. You can then use libraries like pandas for further data manipulation if needed and leverage Spark's machine learning capabilities for anomaly detection."
      },
      {
        "date": "2024-05-12T17:03:00.000Z",
        "voteCount": 1,
        "content": "IMHO, A) PySpark is winner.\n\nBecause of parallelization (Pandas is not distributed, PBI - has different doals)"
      },
      {
        "date": "2024-04-07T07:59:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139329-exam-dp-600-topic-1-question-101-discussion/",
    "body": "You have a Fabric tenant that contains customer churn data stored as Parquet files in OneLake. The data contains details about customer demographics and product usage.<br><br>You create a Fabric notebook to read the data into a Spark DataFrame. You then create column charts in the notebook that show the distribution of retained customers as compared to lost customers based on geography, the number of products purchased, age, and customer tenure.<br><br>Which type of analytics are you performing?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdiagnostic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescriptive\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tprescriptive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpredictive"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T13:33:00.000Z",
        "voteCount": 10,
        "content": "Descriptive analytics tells what happened in the past, presenting it as numbers and visuals in reports and dashboards.\nDiagnostic analytics gives the reason why something happened.\nPredictive analytics determines the potential outcomes of present and past actions and trends.\nPrescriptive analytics offers decision support for the best course of action.\nGiven the scenario in the question where data is read into a Spark DataFrame and column charts are created to show the distribution of retained customers compared to lost customers based on various factors, this falls under the definition of descriptive analytics. No future predictions or prescriptions are made, nor are reasons for the past events provided.\n\nFinal Answer:\n\nThe type of analytics being performed in the scenario described is descriptive analytics.\nhttps://www.selecthub.com/business-intelligence/predictive-descriptive-prescriptive-analytics/"
      },
      {
        "date": "2024-09-05T21:31:00.000Z",
        "voteCount": 1,
        "content": "The analysis is primarily descriptive because it summarizes and visualizes the distribution of retained versus lost customers based on various factors. However, it can also be considered diagnostic if you use these visualizations to identify patterns and investigate the reasons behind customer churn, such as higher churn rates in specific age groups or regions. Thus, it serves both to describe the data and to diagnose underlying issues.\nI believe in the exam; it is possible to dispute questions with answers that contain two possible solutions, so don't worry too much about this question."
      },
      {
        "date": "2024-07-18T19:06:00.000Z",
        "voteCount": 3,
        "content": "https://azure.microsoft.com/es-es/blog/answering-whats-happening-whys-happening-and-what-will-happen-with-iot-analytics/ contains text \"Examine data from multiple angles to understand why something is happening.\" for Diagnostic. In this case we are looking at data from multiple angles."
      },
      {
        "date": "2024-06-26T14:07:00.000Z",
        "voteCount": 1,
        "content": "For me this is a diagnostic analysis, since we try to evaluate the root cause of churn (i.e. loosing customers)."
      },
      {
        "date": "2024-06-23T15:55:00.000Z",
        "voteCount": 1,
        "content": "Verified by ChatGPT"
      },
      {
        "date": "2024-05-29T07:21:00.000Z",
        "voteCount": 1,
        "content": "The creation of column charts to show the distribution of retained and lost customers based on various attributes is aimed at summarizing and visualizing historical data, which aligns with the principles of descriptive analytics. This is not enough to determine why something happened. It involves analyzing data to identify causes and correlations (Diagnostic Analytics)."
      },
      {
        "date": "2024-05-08T00:51:00.000Z",
        "voteCount": 1,
        "content": "sounds like B - descriptive analytics to me. comparing one metric against another by different categories. I cannot see that we are saying \"why\" did something happen, which means it would not be A"
      },
      {
        "date": "2024-05-03T03:24:00.000Z",
        "voteCount": 1,
        "content": "the terms as compared in the phrase tell use it is a descriptive comparaison :\nshow the distribution.\nof retained customers as compared to lost customers based on geography,"
      },
      {
        "date": "2024-04-28T04:19:00.000Z",
        "voteCount": 1,
        "content": "I think that this chart tries to show why people are leaving, so I would go with A diagnostic"
      },
      {
        "date": "2024-06-16T06:34:00.000Z",
        "voteCount": 1,
        "content": "No. It's just describing what happened, not explaining why."
      },
      {
        "date": "2024-04-23T11:32:00.000Z",
        "voteCount": 1,
        "content": "B. descriptive"
      },
      {
        "date": "2024-04-22T18:05:00.000Z",
        "voteCount": 3,
        "content": "Descriptive - it's just describing the customers, not saying why they stayed or left"
      },
      {
        "date": "2024-04-22T01:03:00.000Z",
        "voteCount": 1,
        "content": "A, diagnostic - why did it happen"
      },
      {
        "date": "2024-04-21T17:21:00.000Z",
        "voteCount": 2,
        "content": "A. diagnostic"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137946-exam-dp-600-topic-1-question-102-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant that contains a semantic model. The model contains data about retail stores.<br><br>You need to write a DAX query that will be executed by using the XMLA endpoint. The query must return the total amount of sales from the same period last year.<br><br>How should you complete the DAX expression? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image124.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image125.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-05T00:07:00.000Z",
        "voteCount": 41,
        "content": "Should be CALCULATE, not SUMMARIZE"
      },
      {
        "date": "2024-04-05T20:41:00.000Z",
        "voteCount": 10,
        "content": "CALCULATE([Total Sales], SAMEPERIODLASTYEAR('Order'[Order Date]))\nThe entire expression calculates the sales amount for the same period last year based on the current date context.\n\nThe SUMMARIZE function creates a summary table based on grouping and aggregation."
      },
      {
        "date": "2024-09-15T00:09:00.000Z",
        "voteCount": 1,
        "content": "right ans -&gt; CALCULATE\nI tried in DAX.do with Contoso data and return an error (In query, I have changed [Total Sales] by [Sales Amount] and Orders[Orde Date] by Sales[Order Date]):\n\nEVALUATE\nVAR _LYSales = SUMMARIZE ([Sales Amount],SAMEPERIODLASTYEAR (Sales[Order Date]))\nRETURN\n_LYSales\n\nError:\nQuery (2, 42) Function SUMMARIZE expects a column name as argument number 2.\n\nTechnical Details:\nRootActivityId: 435f1c64-89c5-4587-9d79-187541b1e1e3\nDate (UTC): 9/15/2024 8:04:33 AM"
      },
      {
        "date": "2024-06-18T04:54:00.000Z",
        "voteCount": 8,
        "content": "Be careful, EVALUATE in DAX queries always returns the result of a table expression, then _LYSales is a wrong answer (scalar value). \nThe correct answers are then: CALCULATE, {_LYSales}"
      },
      {
        "date": "2024-06-22T14:03:00.000Z",
        "voteCount": 1,
        "content": "No right sequence is : RETURN\n_LYSales"
      },
      {
        "date": "2024-05-29T07:31:00.000Z",
        "voteCount": 3,
        "content": "EVALUATE\nVAR _LYSales =\n    CALCULATE ( [Total Sales], SAMEPERIODLASTYEAR ( 'Orders'[Order Date] ) )\nRETURN\n    _LYSales"
      },
      {
        "date": "2024-05-19T17:16:00.000Z",
        "voteCount": 1,
        "content": "Calculate  &amp; _ LY"
      },
      {
        "date": "2024-04-28T04:21:00.000Z",
        "voteCount": 2,
        "content": "CALCULATE, _LYSales"
      },
      {
        "date": "2024-04-22T01:04:00.000Z",
        "voteCount": 7,
        "content": "CALCULATE &amp; _LYSales"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137981-exam-dp-600-topic-1-question-103-discussion/",
    "body": "You have a Fabric workspace named Workspace1 that contains a dataflow named Dataflow1. Dataflow1 returns 500 rows of data.<br><br>You need to identify the min and max values for each column in the query results.<br><br>Which three Data view options should you select? Each correct answer presents part of the solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column value distribution",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable column profile\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column profile in details pane\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow column quality details",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable details pane\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-05T09:33:00.000Z",
        "voteCount": 11,
        "content": "B Enable column profile\nE Enable details pane\nC Show column profile in details pane"
      },
      {
        "date": "2024-04-07T02:33:00.000Z",
        "voteCount": 5,
        "content": "answer is BEC"
      },
      {
        "date": "2024-05-29T07:39:00.000Z",
        "voteCount": 2,
        "content": "Column quality: % of Valid, Error, Empty\nColumn distribution: Number of distinct and unique values"
      },
      {
        "date": "2024-05-22T11:27:00.000Z",
        "voteCount": 2,
        "content": "I have checked in Fabric. \nB. profile\nE. Enable detail pane\nC Show column profile in details pane."
      },
      {
        "date": "2024-05-12T17:56:00.000Z",
        "voteCount": 1,
        "content": "IMHO, B&amp;C&amp;E looks right.\n\nNot A (what distribution?), nor D (what quantity?)"
      },
      {
        "date": "2024-05-08T00:40:00.000Z",
        "voteCount": 2,
        "content": "B - Enable column profile; this needs to be enabled to be able to select...\nC - Show column profile in details pane\nE - Enable details pane; this needs to be enabled to be able to see the output of (C)"
      },
      {
        "date": "2024-04-27T09:19:00.000Z",
        "voteCount": 1,
        "content": "BCE are the correct answers."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137993-exam-dp-600-topic-1-question-104-discussion/",
    "body": "You have a Fabric tenant that contains a Microsoft Power BI report.<br><br>You are exploring a new semantic model.<br><br>You need to display the following column statistics:<br><br>\u2022\tCount<br>\u2022\tAverage<br>\u2022\tNull count<br>\u2022\tDistinct count<br>\u2022\tStandard deviation<br><br>Which Power Query function should you run?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.schema",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.FuzzyGroup",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable.Profile\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-05T13:36:00.000Z",
        "voteCount": 8,
        "content": "Ans D correct-The Table.Profile function provides an overview of the data in a table, including basic statistics like count, average, null count, distinct count, and standard deviation for numerical columns. T"
      },
      {
        "date": "2024-05-12T17:59:00.000Z",
        "voteCount": 4,
        "content": "IMHO, D\n\nReturns a profile for the columns in table.\n\nBecause: \nTable.Profile(table as table...\n\nThe following information is returned for each column (when applicable):\nminimum\nmaximum\naverage\nstandard deviation\ncount\nnull count\ndistinct count\n\nLink: https://learn.microsoft.com/en-us/powerquery-m/table-profile"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139347-exam-dp-600-topic-1-question-105-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a lakehouse named Lakehouse1. Lakehouse1 contains a Delta table named Customer.<br><br>When you query Customer, you discover that the query is slow to execute. You suspect that maintenance was NOT performed on the table.<br><br>You need to identify whether maintenance tasks were performed on Customer.<br><br>Solution: You run the following Spark SQL statement:<br><br><br>DESCRIBE DETAIL customer -<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-22T01:11:00.000Z",
        "voteCount": 9,
        "content": "should be DESCRIBE HISTORY"
      },
      {
        "date": "2024-05-12T18:02:00.000Z",
        "voteCount": 4,
        "content": "IMHO, No.\n\nDescribe Detail give general info about delta table, not the historical operations.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/table-details"
      },
      {
        "date": "2024-05-08T00:42:00.000Z",
        "voteCount": 1,
        "content": "DESCRIBE HISTORY will show maintenance history of the table."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139460-exam-dp-600-topic-1-question-106-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a new semantic model in OneLake.<br><br>You use a Fabric notebook to read the data into a Spark DataFrame.<br><br>You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.<br><br>Solution: You use the following PySpark expression:<br><br>df.explain().show()<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-12T18:03:00.000Z",
        "voteCount": 2,
        "content": "IMHO, No,\n\nBecause:\ndf.explain gives execution plan.\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.explain.html"
      },
      {
        "date": "2024-05-08T00:44:00.000Z",
        "voteCount": 3,
        "content": "No. describe() and summary() provide the summary statistics."
      },
      {
        "date": "2024-04-23T13:45:00.000Z",
        "voteCount": 2,
        "content": "https://www.datasciencemadesimple.com/descriptive-statistics-or-summary-statistics-of-dataframe-in-pyspark/"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139527-exam-dp-600-topic-1-question-107-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a semantic model named Model1.<br><br>You discover that the following query performs slowly against Model1.<br><br><img src=\"https://img.examtopics.com/dp-600/image126.png\"><br><br>You need to reduce the execution time of the query.<br><br>Solution: You replace line 4 by using the following code:<br><br>ISEMPTY ( RELATEDTABLE ( 'Order Item' ) )<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-15T23:47:00.000Z",
        "voteCount": 2,
        "content": "NOT ISEMPTY IS THE CORRECT."
      },
      {
        "date": "2024-05-12T18:05:00.000Z",
        "voteCount": 1,
        "content": "IMHO, No\n\nIt should be NOT EMPTY, to have same meaning"
      },
      {
        "date": "2024-05-08T00:46:00.000Z",
        "voteCount": 2,
        "content": "No. The suggested logic would show where COUNTROWS = 0, not &gt; 0.\nAnswer is correct"
      },
      {
        "date": "2024-05-02T11:27:00.000Z",
        "voteCount": 2,
        "content": "B is correct (NO) because the suggested change does not meet the goal because it reverses the logic of the query."
      },
      {
        "date": "2024-05-02T08:41:00.000Z",
        "voteCount": 2,
        "content": "Should be NOT ISEMPTY"
      },
      {
        "date": "2024-04-27T02:26:00.000Z",
        "voteCount": 2,
        "content": "Hence the answer B is only correct!!"
      },
      {
        "date": "2024-04-25T07:42:00.000Z",
        "voteCount": 1,
        "content": "The answers is correct"
      },
      {
        "date": "2024-04-25T05:08:00.000Z",
        "voteCount": 1,
        "content": "I think, CALCULATE would take more processing time than using a native FUNCTION like ISEMPTY"
      },
      {
        "date": "2024-04-27T02:25:00.000Z",
        "voteCount": 1,
        "content": "The condition itself is wrong here the. The CALCULATE Logic checks a list of customers who have made at least one sale. While ISEMPTY Logic checks a list of customers who haven't made any sales."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/microsoft/view/138108-exam-dp-600-topic-1-question-108-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a semantic model named Model1.<br><br>You discover that the following query performs slowly against Model1.<br><br><img src=\"https://img.examtopics.com/dp-600/image126.png\"><br><br>You need to reduce the execution time of the query.<br><br>Solution: You replace line 4 by using the following code:<br><br>NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) )<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-05-29T07:49:00.000Z",
        "voteCount": 7,
        "content": "The original query uses the COUNTROWS function inside a CALCULATE function to count the number of rows in the 'Order Item' table. This approach can be inefficient because it involves counting rows even if just one row exists, which might be resource-intensive especially with large datasets.\n\nThe suggested solution NOT ISEMPTY ( CALCULATETABLE ( 'Order Item' ) ) simplifies the logic to check if the 'Order Item' table related to the customer is empty or not. This approach can be faster as it stops as soon as it finds one row, rather than counting all rows."
      },
      {
        "date": "2024-05-12T18:06:00.000Z",
        "voteCount": 1,
        "content": "IMHO, YES, \n\nBecause IS NOT EMPTY replicates the logic"
      },
      {
        "date": "2024-05-08T00:49:00.000Z",
        "voteCount": 3,
        "content": "Yes. Answer is correct.\nCALCULATETABLE will accept the row context for each of the rows returned by VALUES, and in turn NOT ISEMPTY will check if the calculated table has rows. This is like using EXISTS in T-SQL. It will check if any rows exists, but doesn't return rows, thus improving performance."
      },
      {
        "date": "2024-05-02T11:20:00.000Z",
        "voteCount": 1,
        "content": "A \u2014 The proposed solution improves efficiency by reducing the number of calculations required. Instead of counting all the rows for each customer and then checking if the count is greater than zero, it simply checks if there are any rows at all, which requires fewer computational resources and execution time"
      },
      {
        "date": "2024-04-28T04:26:00.000Z",
        "voteCount": 2,
        "content": "It's correct, it returns the ones with values (not isempty)."
      },
      {
        "date": "2024-04-27T18:15:00.000Z",
        "voteCount": 1,
        "content": "what if order item is negative?"
      },
      {
        "date": "2024-04-27T18:16:00.000Z",
        "voteCount": 1,
        "content": "oh nvm it is count LOL Sorry"
      },
      {
        "date": "2024-04-23T13:51:00.000Z",
        "voteCount": 4,
        "content": "Yes, replacing CALCULATE ( COUNTROWS( 'Order Item' ) ) &gt; 0 with NOT ISEMPTY ( CALCULATETABLE ( 'Order Item ' ) ) should reduce the execution time of the query. It is a simpler, more meaningful, and faster way to check if a table is empty. https://www.sqlbi.com/articles/check-empty-table-condition-with-dax/"
      },
      {
        "date": "2024-04-21T17:49:00.000Z",
        "voteCount": 2,
        "content": "answer is correct.  its faster because it only needs to check if at least one row exists that meets the filter criteria, rather than counting all rows that do."
      },
      {
        "date": "2024-04-07T17:08:00.000Z",
        "voteCount": 4,
        "content": "isnt the syntax incorrect? \nNOT(ISEMPTY(Calculate ... \n\nthere should be a ( after NOT"
      },
      {
        "date": "2024-04-19T02:05:00.000Z",
        "voteCount": 1,
        "content": "It's not mandatory, so I'd answer Yes. \nPs: CALCULATETABLE triggers context transition so it's crucial to use it this example."
      },
      {
        "date": "2024-04-07T17:08:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/dax/isempty-function-dax#example"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/microsoft/view/139526-exam-dp-600-topic-1-question-109-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a semantic model named Model1.<br><br>You discover that the following query performs slowly against Model1.<br><br><img src=\"https://img.examtopics.com/dp-600/image126.png\"><br><br>You need to reduce the execution time of the query.<br><br>Solution: You replace line 4 by using the following code:<br><br>CALCULATE ( COUNTROWS ( 'Order Item' ) ) &gt;= 0<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-28T08:15:00.000Z",
        "voteCount": 7,
        "content": "No - Good luck everyone :)"
      },
      {
        "date": "2024-09-10T09:34:00.000Z",
        "voteCount": 1,
        "content": "Well, we made it boys!"
      },
      {
        "date": "2024-06-20T15:20:00.000Z",
        "voteCount": 2,
        "content": "A silly question.."
      },
      {
        "date": "2024-05-29T07:51:00.000Z",
        "voteCount": 3,
        "content": "The provided solution does not meet the goal as it does not correctly filter out customers without orders and does not provide an optimization in terms of performance."
      },
      {
        "date": "2024-05-12T18:07:00.000Z",
        "voteCount": 2,
        "content": "IMHO, NO, as below in comments"
      },
      {
        "date": "2024-05-08T00:41:00.000Z",
        "voteCount": 2,
        "content": "No, adding the = does not improve performance."
      },
      {
        "date": "2024-04-24T11:16:00.000Z",
        "voteCount": 3,
        "content": "No.\nBy adding the = the only thing we achieve is changing the logic, not the performance."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148872-exam-dp-600-topic-1-question-112-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric workspace that uses the default Spark starter pool and runtime version 1.2.<br><br>You plan to read a CSV file named Sales_raw.csv in a lakehouse, select columns, and save the data as a Delta table to the managed area of the lakehouse. Sales_raw.csv contains 12 columns.<br><br>You have the following code.<br><br><img src=\"https://img.examtopics.com/dp-600/image129.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image130.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image131.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-08T07:34:00.000Z",
        "voteCount": 2,
        "content": "1. YES - The .select() function in the code specifies the exact columns ('SalesOrderNumber', 'OrderDate', 'CustomerName', and 'UnitPrice') to be selected. \n              Therefore, only these columns will be read from the CSV.\n2. NO - The withColumn(\"Year\", year(\"OrderDate\")) function adds a new column called \"Year\" by extracting the year from the \"OrderDate\" column. \n             However, it does not replace the \"OrderDate\" column\u2014it only adds the new \"Year\" column.\n3. YES - The inferSchema='true' tells Spark to infer the data types of each column in the CSV, which requires an extra scan of the data to determine these types. \n             This can indeed increase execution time."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148730-exam-dp-600-topic-1-question-113-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a new semantic model in OneLake.<br><br>You use a Fabric notebook to read the data into a Spark DataFrame.<br><br>You need to evaluate the data to calculate the min, max, mean, and standard deviation values for all the string and numeric columns.<br><br>Solution: You use the following PySpark expression:<br><br>df.describe().show()<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-06T01:38:00.000Z",
        "voteCount": 5,
        "content": "I think A is correct https://learn.microsoft.com/en-us/dotnet/api/microsoft.spark.sql.dataframe.describe?view=spark-dotnet"
      },
      {
        "date": "2024-10-13T03:01:00.000Z",
        "voteCount": 1,
        "content": "DataFrame.Describe = Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."
      },
      {
        "date": "2024-10-13T03:15:00.000Z",
        "voteCount": 1,
        "content": "A is correct, confirming after trying the command from Notebook. Displays count as well in addition to min, max, mean, and standard deviation."
      },
      {
        "date": "2024-10-13T03:02:00.000Z",
        "voteCount": 1,
        "content": "DataFrame.Describe = Computes basic statistics for numeric and string columns, including count, mean, stddev, min, and max. If no columns are given, this function computes statistics for all numerical or string columns."
      },
      {
        "date": "2024-10-06T06:32:00.000Z",
        "voteCount": 1,
        "content": "It shows all stat for numeric values but shows only 3 stat for string(count,min and max, it doesnt account for mean and std)"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148965-exam-dp-600-topic-1-question-114-discussion/",
    "body": "You have a Fabric tenant.<br><br>You are creating a Fabric Data Factory pipeline.<br><br>You have a stored procedure that returns the number of active customers and their average sales for the current month.<br><br>You need to add an activity that will execute the stored procedure in a warehouse. The returned values must be available to the downstream activities of the pipeline.<br><br>Which type of activity should you add?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend variable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLookup"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-10T01:10:00.000Z",
        "voteCount": 1,
        "content": "D is correct - A Lookup activity in Fabric Data Factory is specifically designed to execute a query or stored procedure and retrieve data from a data source."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148731-exam-dp-600-topic-1-question-115-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant that contains a semantic model named model1. The two largest columns in model1 are shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image132.png\"><br><br>You need to optimize model1. The solution must meet the following requirements:<br><br>\u2022\tReduce the model size.<br>\u2022\tIncrease refresh performance when using Import mode.<br>\u2022\tEnsure that the datetime value for each sales transaction is available in the model.<br><br>What should you do on each column? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image133.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image134.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-06T01:45:00.000Z",
        "voteCount": 1,
        "content": "Correct: Remove + Split"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148732-exam-dp-600-topic-1-question-116-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a Fabric tenant that contains a data warehouse named DW1. DW1 contains a table named DimCustomer. DimCustomer contains the fields shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-600/image135.png\"><br><br>You need to identify duplicate email addresses in DimCustomer. The solution must return a maximum of 1,000 records.<br><br>Which four T-SQL statements should you run in sequence? To answer, move the appropriate statements from the list of statements to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-600/image136.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image137.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-10T01:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2024-10-06T01:46:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148734-exam-dp-600-topic-1-question-117-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Fabric tenant that contains a warehouse named WH1.<br><br>You run the following T-SQL query against WH1.<br><br><img src=\"https://img.examtopics.com/dp-600/image138.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-600/image139.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-600/image140.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-06T01:49:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is no-no-yes. You couldn't select columns from the return value of a scalar function as done in line 4 and 5 of the SQL code."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148735-exam-dp-600-topic-1-question-118-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Fabric tenant that contains a semantic model named Model1.<br><br>You discover that the following query performs slowly against Model1.<br><br><img src=\"https://img.examtopics.com/dp-600/image141.png\"><br><br>You need to reduce the execution time of the query.<br><br>Solution: You replace line 4 by using the following code:<br><br>NOT ( CALCULATE ( COUNTROWS ( 'Order Item' ) ) &lt; 0)<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-06T01:50:00.000Z",
        "voteCount": 1,
        "content": "No is correct. Performance would be about the same."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/microsoft/view/148668-exam-dp-600-topic-1-question-119-discussion/",
    "body": "Case study -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br>Contoso, Ltd. is a US-based health supplements company. Contoso has two divisions named Sales and Research. The Sales division contains two departments named Online Sales and Retail Sales. The Research division assigns internally developed product lines to individual teams of researchers and analysts.<br><br><br>Existing Environment -<br><br><br>Identity Environment -<br><br>Contoso has a Microsoft Entra tenant named contoso.com. The tenant contains two groups named ResearchReviewersGroup1 and ResearchReviewersGroup2.<br><br><br>Data Environment -<br><br>Contoso has the following data environment:<br><br>\u2022\tThe Sales division uses a Microsoft Power BI Premium capacity.<br>\u2022\tThe semantic model of the Online Sales department includes a fact table named Orders that uses Import made. In the system of origin, the OrderID value represents the sequence in which orders are created.<br>\u2022\tThe Research department uses an on-premises, third-party data warehousing product.<br>\u2022\tFabric is enabled for contoso.com.<br>\u2022\tAn Azure Data Lake Storage Gen2 storage account named storage1 contains Research division data for a product line named Productline1. The data is in the delta format.<br>\u2022\tA Data Lake Storage Gen2 storage account named storage2 contains Research division data for a product line named Productline2. The data is in the CSV format.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Contoso plans to make the following changes:<br><br>\u2022\tEnable support for Fabric in the Power BI Premium capacity used by the Sales division.<br>\u2022\tMake all the data for the Sales division and the Research division available in Fabric.<br>\u2022\tFor the Research division, create two Fabric workspaces named Productline1ws and Productline2ws.<br>\u2022\tIn Productline1ws, create a lakehouse named Lakehouse1.<br>\u2022\tIn Lakehouse1, create a shortcut to storage1 named ResearchProduct.<br><br><br>Data Analytics Requirements -<br><br>Contoso identifies the following data analytics requirements:<br><br>\u2022\tAll the workspaces for the Sales division and the Research division must support all Fabric experiences.<br>\u2022\tThe Research division workspaces must use a dedicated, on-demand capacity that has per-minute billing.<br>\u2022\tThe Research division workspaces must be grouped together logically to support OneLake data hub filtering based on the department name.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup1 must be able to read lakehouse and warehouse data and shortcuts by using SQL endpoints.<br>\u2022\tFor the Research division workspaces, the members of ResearchReviewersGroup2 must be able to read lakehouse data by using Lakehouse explorer.<br>\u2022\tAll the semantic models and reports for the Research division must use version control that supports branching.<br><br><br>Data Preparation Requirements -<br><br>Contoso identifies the following data preparation requirements:<br><br>\u2022\tThe Research division data for Productline1 must be retrieved from Lakehouse1 by using Fabric notebooks.<br>\u2022\tAll the Research division data in the lakehouses must be presented as managed tables in Lakehouse explorer.<br><br><br>Semantic Model Requirements -<br><br>Contoso identifies the following requirements for implementing and managing semantic models:<br><br>\u2022\tThe number of rows added to the Orders table during refreshes must be minimized.<br>\u2022\tThe semantic models in the Research division workspaces must use Direct Lake mode.<br><br><br>General Requirements -<br><br>Contoso identifies the following high-level requirements that must be considered for all solutions:<br><br>\u2022\tFollow the principle of least privilege when applicable.<br>\u2022\tMinimize implementation and maintenance effort when possible.<br><br><br>Which syntax should you use in a notebook to access the Research division data for Productline1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read.format(\u201cdelta\u201d).load(\u201cTables/ResearchProduct\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tspark.read.format(\u201cdelta\u201d).load(\u201cFiles/ResearchProduct\u201d)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texternal_table(\u2018Tables/ResearchProduct)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texternal_table(ResearchProduct)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-10-04T23:35:00.000Z",
        "voteCount": 3,
        "content": "B I think"
      },
      {
        "date": "2024-10-05T09:44:00.000Z",
        "voteCount": 1,
        "content": "I agree. Providing a file format if it's already a table wouldn't make sense."
      },
      {
        "date": "2024-10-06T01:27:00.000Z",
        "voteCount": 3,
        "content": "My bad, A is correct, I found the example: https://learn.microsoft.com/en-us/fabric/onelake/onelake-shortcuts#apache-spark\nAlthough, technically both, A and B, could be correct, looking at the requirements, the datalake should be linked as a table shortcut, not as a file shortcut, and then A is correct."
      }
    ],
    "examNameCode": "dp-600",
    "topicNumber": "1"
  }
]