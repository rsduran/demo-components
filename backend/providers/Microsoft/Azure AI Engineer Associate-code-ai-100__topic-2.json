[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5661-exam-ai-100-topic-2-question-1-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution that will analyze bank transactions in real time. The transactions will be evaluated by using an algorithm and classified into one of five groups. The transaction data will be enriched with information taken from Azure SQL Database before the transactions are sent to the classification process. The enrichment process will require custom code. Data from different banks will require different stored procedures.<br>You need to develop a pipeline for the solution.<br>Which components should you use for data ingestion and data preparation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0005700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0005800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/bs-latn-ba/azure/architecture/example-scenario/data/fraud-detection",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-13T10:15:00.000Z",
        "voteCount": 8,
        "content": "we are not doing Analytics on a streaming data. We will do data enrichment by using queries ( stored procedures). Hence my vote is for Azure Functions."
      },
      {
        "date": "2020-09-28T06:56:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf-methods"
      },
      {
        "date": "2020-05-17T02:06:00.000Z",
        "voteCount": 5,
        "content": "Azure Functions could be an option here, maily for two reasons:\n1. data preparation require custom code\n2. \"An Azure Function integrated with SQL Database provides multiple benefits: ... Doing the bulk of data preparation using SQL Database in-memory processing and native stored procedures was very fast\"\nhttps://azure.microsoft.com/it-it/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/"
      },
      {
        "date": "2020-05-24T07:13:00.000Z",
        "voteCount": 5,
        "content": "Stream Analytics is designed for real-time and can run custom code (Stream Analytics Query Language, which is T-SQL like).\nhttps://docs.microsoft.com/en-us/stream-analytics-query/stream-analytics-query-language-reference"
      },
      {
        "date": "2020-05-24T07:13:00.000Z",
        "voteCount": 1,
        "content": "Therefore Stream Analytics is correct"
      },
      {
        "date": "2020-05-30T02:44:00.000Z",
        "voteCount": 4,
        "content": "Functions are well suited for real-time processing too. Moreover, the question is asking for a component for \"data preparation\", which is a good use case for Functions. Finally, I'm not sure Stream Analytics supports calls for Stored Procedures. Some users were complaining about this on the blogs below:\nhttps://feedback.azure.com/forums/270577-stream-analytics/suggestions/38438074-support-sql-db-stored-procedures-as-asa-output\nhttps://stackoverflow.com/questions/44026676/how-can-i-set-stream-analytics-output-as-stored-procedure"
      },
      {
        "date": "2020-07-27T18:21:00.000Z",
        "voteCount": 3,
        "content": "Because Azure Event Hubs can be an input to Stream Analytics but not to Azure Functions, I would say Stream Analytics is the answer not Azure Functions"
      },
      {
        "date": "2021-02-10T12:40:00.000Z",
        "voteCount": 2,
        "content": "Sure it can:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-reliable-event-processing\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=csharp"
      },
      {
        "date": "2023-06-19T06:55:00.000Z",
        "voteCount": 1,
        "content": "components to be used for data ingestion and data preparation are:\n\nData Ingestion: Azure Event Hubs\nData Preparation: Azure Functions"
      },
      {
        "date": "2021-06-29T06:07:00.000Z",
        "voteCount": 1,
        "content": "Analytics is correct as it will be used to \"process the data\"\nhttps://docs.microsoft.com/en-in/azure/stream-analytics/stream-analytics-with-azure-functions"
      },
      {
        "date": "2021-06-22T01:22:00.000Z",
        "voteCount": 1,
        "content": "According to this quote \"Azure Stream Analytics is an event-processing engine that can analyze high volumes of data streaming from devices and other data sources. It also supports extracting information from data streams to identify patterns and relationships. These patterns can trigger other downstream actions. In this scenario, Stream Analytics transforms the input stream from Event Hubs to identify fraudulent calls.\" IMO the second answer is Stream Analytics. https://docs.microsoft.com/bs-latn-ba/azure/architecture/example-scenario/data/fraud-detection"
      },
      {
        "date": "2021-03-20T12:55:00.000Z",
        "voteCount": 3,
        "content": "This links explains it all: https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-event-hubs-trigger?tabs=csharp you can use a function and the trigger can come from event hubs. Therefore, Event hubs and azure functions 100% sure"
      },
      {
        "date": "2020-10-18T04:28:00.000Z",
        "voteCount": 4,
        "content": "The question says the enrichment process will require custom code, The only option for this is azure Functions.\nAzure stream Analytics can also has custom code But this option has off 09/10/2020 is in Preview \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge-csharp-udf-methods"
      },
      {
        "date": "2020-10-12T20:19:00.000Z",
        "voteCount": 5,
        "content": "According to this article, the solution seems to be: Event hub+Azure Functions\nhttps://azure.microsoft.com/hu-hu/blog/considering-azure-functions-for-a-serverless-data-streaming-scenario/"
      },
      {
        "date": "2020-05-04T03:38:00.000Z",
        "voteCount": 4,
        "content": "My thoughts regarding the 2nd answer: \nAgree azure Stream Analytics is good candidate for real-time analytics and complex event-processing and designed for high volumes of fast streaming data but the question here is about preparing the data and also remember the data enrichment is done at the SQL DB level and there is already an algorithm that does the classification. \nAn azure function could act as an orchestrator, not doing any heavy lifting except loading data payload to azure SQL DB, the stored procedures computes/calculates the new features."
      },
      {
        "date": "2021-11-19T03:12:00.000Z",
        "voteCount": 1,
        "content": "You are representing your thoughts, but why?    Is this Roadies or Splitsvilla?    This Azure -100.... Behave properly....nahale bhara khaibu"
      },
      {
        "date": "2019-09-24T16:27:00.000Z",
        "voteCount": 1,
        "content": "In the first part-&gt; The answer should be Azure IOT Hub...Why Not Event Hub ? Because the IOT Hub contains and Event Hub and hence essentially is an additional features. More important thing is that Event Hub can only receive message,where as IOT hub can send messages to individual devices."
      },
      {
        "date": "2019-09-27T17:24:00.000Z",
        "voteCount": 26,
        "content": "There is no requirement to write back to the devices nor anything around device management.  Therefore I would go with Event Hub"
      },
      {
        "date": "2020-05-24T07:11:00.000Z",
        "voteCount": 3,
        "content": "Event Hub is correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56179-exam-ai-100-topic-2-question-2-discussion/",
    "body": "DRAG DROP -<br>You are designing an Azure Batch AI solution that will be used to train many different Azure Machine Learning models. The solution will perform the following:<br>\u2711 Image recognition<br>\u2711 Deep learning that uses convolutional neural networks.<br>You need to select a compute infrastructure for each model. The solution must minimize the processing time.<br>What should you use for each model? To answer, drag the appropriate compute infrastructures to the correct models. Each compute infrastructure may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0005900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0005900002.png\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/virtual-machines/windows/sizes-gpu",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:56:00.000Z",
        "voteCount": 1,
        "content": "compute infrastructures to be used for each model are:\n\nImage recognition: GPU VM (Virtual Machine)\nDeep learning with convolutional neural networks: GPU VM (Virtual Machine)"
      },
      {
        "date": "2021-06-27T09:35:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/virtual-machines/sizes\nThat is the better link to understand why GPU is the correct answer."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38899-exam-ai-100-topic-2-question-3-discussion/",
    "body": "You design an AI workflow that combines data from multiple data sources for analysis. The data sources are composed of:<br>\u2711 JSON files uploaded to an Azure Storage account<br>\u2711 On-premises Oracle databases<br>\u2711 Azure SQL databases<br>Which service should you use to ingest the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Warehouse",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks"
    ],
    "answer": "A",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/data-factory/introduction",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:58:00.000Z",
        "voteCount": 1,
        "content": "best option for ingesting data from JSON files uploaded to an Azure Storage account, on-premises Oracle databases, and Azure SQL databases is Azure Data Factory (Option A)."
      },
      {
        "date": "2021-07-09T06:30:00.000Z",
        "voteCount": 1,
        "content": "why not wharehouse???"
      },
      {
        "date": "2021-04-12T03:12:00.000Z",
        "voteCount": 1,
        "content": "ADF is correct"
      },
      {
        "date": "2020-12-05T05:07:00.000Z",
        "voteCount": 3,
        "content": "ADF is correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/10707-exam-ai-100-topic-2-question-4-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution that will ingest temperature data from IoT devices, calculate the average temperature, and then take action based on the aggregated data. The solution must meet the following requirements:<br>\u2711 Minimize the amount of uploaded data.<br>\u2711 Take action based on the aggregated data as quickly as possible.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0006100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0006200001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Functions -<br>Azure Function is a (serverless) service to host functions (little piece of code) that can be used for e. g. event driven applications.<br>General rule is always difficult since everything depends on your requirement but if you have to analyze a data stream, you should take a look at Azure Stream<br>Analytics and if you want to implement something like a serverless event driven or timer-based application, you should check Azure Function or Logic Apps.<br>Note: Azure IoT Edge allows you to deploy complex event processing, machine learning, image recognition, and other high value AI without writing it in-house.<br>Azure services like Azure Functions, Azure Stream Analytics, and Azure Machine Learning can all be run on-premises via Azure IoT Edge.<br><br>Box 2: An Azure IoT Edge device -<br>Azure IoT Edge moves cloud analytics and custom business logic to devices so that your organization can focus on business insights instead of data management.<br>References:<br>https://docs.microsoft.com/en-us/azure/iot-edge/about-iot-edge",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-20T01:04:00.000Z",
        "voteCount": 26,
        "content": "I think Stream Analytics on IoT Edge is the correct answer\n\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics"
      },
      {
        "date": "2023-06-19T07:00:00.000Z",
        "voteCount": 1,
        "content": "By combining Azure IoT Hub, Azure Stream Analytics, and Azure Functions, you can create a solution that efficiently ingests temperature data from IoT devices, calculates the average temperature in real-time, and triggers actions based on the aggregated data. This approach minimizes the amount of uploaded data by using lightweight messaging protocols and ensures quick response times for taking actions based on the aggregated data."
      },
      {
        "date": "2021-01-26T11:28:00.000Z",
        "voteCount": 2,
        "content": "Service: Azure Stream Analytics - Minimize the Data Upload and Take Action based on aggregated data (i.e., call an Azure function). Ref. link below.\nLocation: Azure IoT Edge - Doesn't require explanation I guess.\n\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics?view=iotedge-2018-06"
      },
      {
        "date": "2020-12-27T00:23:00.000Z",
        "voteCount": 3,
        "content": "stream  analytics and iot edge"
      },
      {
        "date": "2020-10-21T11:07:00.000Z",
        "voteCount": 2,
        "content": "It is Azure Stream Analytics - find averages \nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics"
      },
      {
        "date": "2020-10-25T00:07:00.000Z",
        "voteCount": 4,
        "content": "correct. your link proves it all. Stream Analytics is what they want us to choose. \n\nThese 3 phrases from the link are all the question needs:\nreduce the amount of uploaded data\nreduce the time it takes to react to actionable insights\ncalculates the average temperature over a rolling 30-second window"
      },
      {
        "date": "2020-06-23T05:54:00.000Z",
        "voteCount": 2,
        "content": "From my perspective : \"calculate the average temperature, and then take action based on the aggregated data.\" should guide us to Stream Analytics."
      },
      {
        "date": "2020-07-27T16:40:00.000Z",
        "voteCount": 4,
        "content": "Solution should require minimum upload which is why it cannot be Stream Analytics and the Azure Functions makes more sense"
      },
      {
        "date": "2020-05-28T09:02:00.000Z",
        "voteCount": 2,
        "content": "A Stream Analytics job at the edge should be a valid option here. This article describes the same use case of the request: analyze temperature data in real-time and take actions (by sending alerts to temperature sensors) if specific conditions are met.\nhttps://docs.microsoft.com/it-it/azure/stream-analytics/stream-analytics-edge"
      },
      {
        "date": "2020-09-12T12:28:00.000Z",
        "voteCount": 2,
        "content": "don't think so. the logic will be on IOT edge. An Azure function will run to calculate average temperature, and if it meets the criteria then it will send it to stream analytics."
      },
      {
        "date": "2020-05-04T20:31:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct since we have to implement some logics"
      },
      {
        "date": "2020-03-07T06:11:00.000Z",
        "voteCount": 3,
        "content": "https://rangv.github.io/AzureIoTEdgeUbuntuLabs/FunctionsAtTheEdge/ -&gt; so functions would be perfectly valid here"
      },
      {
        "date": "2020-01-12T19:40:00.000Z",
        "voteCount": 1,
        "content": "AZURE STREAM ANALYTICS would be right answer"
      },
      {
        "date": "2020-01-12T19:43:00.000Z",
        "voteCount": 4,
        "content": "Sorry, it won't be because, its correct that stream analytics can be used for data aggregation with specific funtion, but if the iot Edge is used, data aggregation would be done on edge and no need for aggregation later,"
      },
      {
        "date": "2020-06-25T01:53:00.000Z",
        "voteCount": 1,
        "content": "ASA is deployed on IoT edge. Then ASA is a good answer for me."
      },
      {
        "date": "2019-12-19T23:40:00.000Z",
        "voteCount": 1,
        "content": "Why would this be Functions and not Stream Analytics?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/44643-exam-ai-100-topic-2-question-5-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an app named App1 that uses the Face API.<br>App1 contains several PersonGroup objects.<br>You discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains<br>10,000 entries.<br>You need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.<br>Solution: You modify the custom time interval for the training phase of App1.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead, use a LargePersonGroup.  LargePersonGroup and LargeFaceList are collectively referred to as large-scale operations. LargePersonGroup can contain up to 1 million persons, each with a maximum of 248 faces. LargeFaceList can contain up to 1 million faces. The large-scale operations are similar to the conventional PersonGroup and FaceList but have some differences because of the new architecture.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T01:45:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nModifying the custom time interval for the training phase of App1 will not resolve the issue of the PersonGroup object for Ben Smith reaching its capacity and being unable to accept additional entries. Changing the training interval will only affect the training process of the Face API, which is unrelated to the capacity of a PersonGroup object. To enable additional entries for Ben Smith, you would need to either increase the capacity of the PersonGroup object or create a new PersonGroup object for Ben Smith with a higher entry limit."
      },
      {
        "date": "2021-08-23T06:51:00.000Z",
        "voteCount": 1,
        "content": "all solutions are wrong except for migration"
      },
      {
        "date": "2021-05-31T00:56:00.000Z",
        "voteCount": 3,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-02-13T18:13:00.000Z",
        "voteCount": 2,
        "content": "Looks like this can be a possible solution too. Besides LargePersonGroup which is also correct.\n\nI'm choosing Yes for Custom Interval and for LargePersonGroup.\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale#step-31-customize-time-interval"
      },
      {
        "date": "2021-02-13T18:19:00.000Z",
        "voteCount": 2,
        "content": "Rethinking because the Customize Time Interval applies only if entries are migrated to LargePersonGroup."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47758-exam-ai-100-topic-2-question-6-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an app named App1 that uses the Face API.<br>App1 contains several PersonGroup objects.<br>You discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains<br>10,000 entries.<br>You need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.<br>Solution: You create a second PersonGroup object for Ben Smith.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead, use a LargePersonGroup.  LargePersonGroup and LargeFaceList are collectively referred to as large-scale operations. LargePersonGroup can contain up to 1 million persons, each with a maximum of 248 faces. LargeFaceList can contain up to 1 million faces. The large-scale operations are similar to the conventional PersonGroup and FaceList but have some differences because of the new architecture.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T01:48:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nTo ensure that additional entries can be added to the PersonGroup object for Ben Smith, you need to convert it to a LargePersonGroup object. A LargePersonGroup object can hold up to 1,000,000 entries\n\nCreating a second PersonGroup object for Ben Smith will not meet the goal because it will not allow you to add more entries to the original PersonGroup object for Ben Smith."
      },
      {
        "date": "2021-05-31T00:57:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-05-20T04:28:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-03-19T13:37:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/41453-exam-ai-100-topic-2-question-7-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an app named App1 that uses the Face API.<br>App1 contains several PersonGroup objects.<br>You discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains<br>10,000 entries.<br>You need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.<br>Solution: You migrate all the entries to the LargePersonGroup object for Ben Smith.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "LargePersonGroup and LargeFaceList are collectively referred to as large-scale operations. LargePersonGroup can contain up to 1 million persons, each with a maximum of 248 faces. LargeFaceList can contain up to 1 million faces. The large-scale operations are similar to the conventional PersonGroup and FaceList but have some differences because of the new architecture.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/how-to-use-large-scale",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T01:50:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nMigrating all the entries to the LargePersonGroup object for Ben Smith would meet the goal of allowing additional entries to be added. The LargePersonGroup object has a higher capacity than the regular PersonGroup object, allowing for more entries to be stored. By migrating all the entries from the existing PersonGroup object to the LargePersonGroup object, you would be able to accommodate additional entries for Ben Smith while ensuring that he can be identified by all the entries."
      },
      {
        "date": "2021-05-31T00:57:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-01-03T22:27:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct. https://westus.dev.cognitive.microsoft.com/docs/services/563879b61984550e40cbbe8d/operations/563879b61984550f30395244"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24530-exam-ai-100-topic-2-question-8-discussion/",
    "body": "Your company plans to develop a mobile app to provide meeting transcripts by using speech-to-text. Audio from the meetings will be streamed to provide real-time transcription.<br>You need to recommend which task each meeting participant must perform to ensure that the transcripts of the meetings can identify all participants.<br>Which task should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecord the meeting as an MP4.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a voice signature.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSign up for Azure Speech Services.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSign up as a guest in Azure Active Directory (Azure AD)"
    ],
    "answer": "B",
    "answerDescription": "The first step is to create voice signatures for the conversation participants. Creating voice signatures is required for efficient speaker identification.<br>Note: In addition to the standard baseline model used by the Speech Services, you can customize models to your needs with available data, to overcome speech recognition barriers such as speaking style, vocabulary and background noise.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/cognitive-services/speech-service/how-to-use-conversation-transcription-service",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:04:00.000Z",
        "voteCount": 1,
        "content": "recommended task to ensure that the transcripts can identify all participants is to create a voice signature (Option B)."
      },
      {
        "date": "2021-05-20T04:28:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-03-23T01:56:00.000Z",
        "voteCount": 3,
        "content": "\"The first step is to create voice signatures for the conversation participants so that they can be identified as unique speakers. The input .wav audio file for creating voice signatures should be 16-bit, 16 kHz sample rate, and single channel (mono) format....\" link: https://docs.microsoft.com/bs-latn-ba/azure/cognitive-services/speech-service/how-to-use-conversation-transcription?pivots=programming-language-javascript"
      },
      {
        "date": "2021-02-13T18:52:00.000Z",
        "voteCount": 1,
        "content": "Wrong IMO. This is what I could infer:\nThe prerequisite (mentioned in the article too) is to \"Sign up for Speech Services\". To use the REST API to create a Voice Signature, you need to sign up first.\nhttps://docs.microsoft.com/bs-latn-ba/azure/cognitive-services/speech-service/how-to-use-conversation-transcription?pivots=programming-language-javascript#prerequisites"
      },
      {
        "date": "2021-02-19T12:48:00.000Z",
        "voteCount": 5,
        "content": "I may be wrong. Retracting. \nThe questions says, \"You need to recommend which task EACH MEETING PARTICIPANT must perform\", and not what YOU would do. YOU would sign up for Speech Services. But wouldn't recommend each participant to sign up. Each participant would Create a Voice Signature.\nGiven answer is correct."
      },
      {
        "date": "2020-07-01T07:22:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/44644-exam-ai-100-topic-2-question-9-discussion/",
    "body": "You need to create a prototype of a bot to demonstrate a user performing a task. The demonstration will use the Bot Framework Emulator.<br>Which botbuilder CLI tool should you use to create the prototype?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChatdown",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQnAMaker",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDispatch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLuDown"
    ],
    "answer": "A",
    "answerDescription": "Use Chatdown to produce prototype mock conversations in markdown and convert the markdown to transcripts you can load and view in the new V4 Bot<br>Framework Emulator.<br>Incorrect Answers:<br>B: QnA Maker is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semi-structured content, including FAQs, manuals, and documents. Answer users' questions with the best answers from the QnAs in your knowledge base\u05d2\u20ac\"automatically. Your knowledge base gets smarter, too, as it continually learns from user behavior.<br>C: Dispatch lets you build language models that allow you to dispatch between disparate components (such as QnA, LUIS and custom code).<br>D: LuDown build LUIS language understanding models using markdown files<br>References:<br>https://github.com/microsoft/botframework/blob/master/README.md",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:04:00.000Z",
        "voteCount": 1,
        "content": "recommended botbuilder CLI tool to use for creating a prototype of a bot to demonstrate a user performing a task in the Bot Framework Emulator is Chatdown (Option A)."
      },
      {
        "date": "2021-06-29T06:27:00.000Z",
        "voteCount": 1,
        "content": "https://github.com/microsoft/botframework-cli/blob/main/packages/chatdown/docs/chatdown-format.md"
      },
      {
        "date": "2021-06-01T16:20:00.000Z",
        "voteCount": 1,
        "content": "Ok - so this was in my exam. I got this wrong. It is chatdown i choose dispatch lol"
      },
      {
        "date": "2021-05-31T00:57:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-02-13T19:08:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56182-exam-ai-100-topic-2-question-10-discussion/",
    "body": "DRAG DROP -<br>You need to create a bot to meet the following requirements:<br>\u2711 The bot must support multiple bot channels including Direct Line.<br>\u2711 Users must be able to sign in to the bot by using a Gmail user account and save activities and preferences.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0006800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0006900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: From the Azure portal, configure an identity provider.<br>The Azure Bot Service and the v4 SDK include new bot authentication capabilities, providing features to make it easier to develop a bot that authenticates users to various identity providers, such as Azure AD (Azure Active Directory), GitHub, Uber, and so on.<br>Step 2: From the Azure portal, create an Azure Active Directory (Azure AD) B2C service.<br>Azure Active Directory B2C provides business-to-customer identity as a service. Your customers use their preferred social, enterprise, or local account identities to get single sign-on access to your applications and APIs.<br>Step 3: From the Azure portal, create a client application<br>You can enable communication between your bot and your own client application by using the Direct Line API.<br>Step 4: From the bot code, add the connection settings and OAuthPrompt<br>Use an OAuth prompt to sign the user in and get a token.<br>Azure AD B2C uses standards-based authentication protocols including OpenID Connect, OAuth 2.0, and SAML.<br>References:<br>https://docs.microsoft.com/en-us/azure/bot-service/bot-builder-authentication?view=azure-bot-service-4.0",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-29T06:30:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/active-directory/develop/quickstart-register-app \ncorrect sequence"
      },
      {
        "date": "2021-06-27T09:56:00.000Z",
        "voteCount": 4,
        "content": "my preferred order would be:\nStep 1: From the Azure portal, create a client application\nStep 2: From the Azure portal, create an Azure Active Directory (Azure AD) B2C service.\nStep 3: From the Azure portal, configure an identity provider.\nStep 4: From the bot code, add the connection settings and OAuthPrompt"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53202-exam-ai-100-topic-2-question-11-discussion/",
    "body": "HOTSPOT -<br>You have an app that uses the Language Understanding (LUIS) API as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/03857/0007000001.png\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0007100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0007200001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: train -<br>Utterances are input from the user that your app needs to interpret. To train LUIS to extract intents and entities from them, it's important to capture a variety of different example utterances for each intent. Active learning, or the process of continuing to train on new utterances, is essential to machine-learned intelligence that LUIS provides.<br><br>Box 2: creating intents -<br>Each intent needs to have example utterances, at least 15. If you have an intent that does not have any example utterances, you will not be able to train LUIS. If you have an intent with one or very few example utterances, LUIS will not accurately predict the intent.<br><br>Box 3: never published -<br>In each iteration of the model, do not add a large quantity of utterances. Add utterances in quantities of 15. Train, publish, and test again.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-utteran3ce",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-15T11:40:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      },
      {
        "date": "2021-05-20T04:29:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/10205-exam-ai-100-topic-2-question-12-discussion/",
    "body": "You are designing an AI solution that will provide feedback to teachers who train students over the Internet. The students will be in classrooms located in remote areas. The solution will capture video and audio data of the students in the classrooms.<br>You need to recommend Azure Cognitive Services for the AI solution to meet the following requirements:<br>\u2711 Alert teachers if a student facial expression indicates the student is angry or scared.<br>\u2711 Identify each student in the classrooms for attendance purposes.<br>\u2711 Allow the teachers to log voice conversations as text.<br>Which Cognitive Services should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFace API and Text Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision and Text Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQnA Maker and Computer Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpeech to Text and Face API"
    ],
    "answer": "D",
    "answerDescription": "Speech-to-text from Azure Speech Services, also known as speech-to-text, enables real-time transcription of audio streams into text that your applications, tools, or devices can consume, display, and take action on as command input.<br>Face detection: Detect one or more human faces in an image and get back face rectangles for where in the image the faces are, along with face attributes which contain machine learning-based predictions of facial features. The face attribute features available are: Age, Emotion, Gender, Pose, Smile, and Facial Hair along with 27 landmarks for each face in the image.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/speech-to-text https://azure.microsoft.com/en-us/services/cognitive-services/face/",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-14T01:25:00.000Z",
        "voteCount": 6,
        "content": "Face API: \"Detect, identify, and analyze faces in images and videos\"\nReference: https://azure.microsoft.com/en-us/services/cognitive-services/face/#demo\n\nSo it seems ok"
      },
      {
        "date": "2021-02-13T19:18:00.000Z",
        "voteCount": 2,
        "content": "Also this article shows the steps to analyze videos in real-time using the Face API. It does use steps to do it though such as to acquire frames from the video source etc.\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/face/face-api-how-to-topics/howtoanalyzevideo_face\n\nAlthough much easier to use the Video Indexer, Face API and Speech-To-Text seem sufficient here.\nThis appears in another question where Video Indexer is also an option along with the two APIs given here."
      },
      {
        "date": "2019-12-11T22:16:00.000Z",
        "voteCount": 5,
        "content": "Video Indexer is also needed"
      },
      {
        "date": "2023-06-19T07:11:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure Cognitive Services for the given requirements are the Face API and Text Analytics (Option A)."
      },
      {
        "date": "2021-05-20T04:29:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2020-04-17T05:50:00.000Z",
        "voteCount": 3,
        "content": "You can easily get a frame of your video and use Face API"
      },
      {
        "date": "2020-02-04T14:38:00.000Z",
        "voteCount": 2,
        "content": "Why is video indexer needed?"
      },
      {
        "date": "2020-02-14T01:15:00.000Z",
        "voteCount": 2,
        "content": "Video Indexer  \u2013 Face Detection in VIDEOS (not images), Object and Activity Detection"
      },
      {
        "date": "2021-04-18T02:03:00.000Z",
        "voteCount": 1,
        "content": "Detect, identify, and analyze faces in images and VIDEDOS\n\nREF: https://azure.microsoft.com/en-us/services/cognitive-services/face/#overview"
      },
      {
        "date": "2020-02-14T01:16:00.000Z",
        "voteCount": 1,
        "content": "Face API \u2013 Face Detection in Images, Person Identification , Emotion Recog in images, Similar Face Recognition"
      },
      {
        "date": "2020-05-04T11:09:00.000Z",
        "voteCount": 1,
        "content": "agree, needed for near real time"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49701-exam-ai-100-topic-2-question-13-discussion/",
    "body": "HOTSPOT -<br>Your company plans to build an app that will perform the following tasks:<br>Match a user's picture to a picture of a celebrity.<br><img src=\"/assets/media/exam-media/03857/0007300004.png\" class=\"in-exam-image\"><br>\u2711 Tag a scene from a movie, and then search for movie scenes by using the tags.<br>You need to recommend which Azure Cognitive Services APIs must be used to perform the tasks.<br>Which Cognitive Services API should you recommend for each task? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0007400002.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0007500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Computer Vision -<br>Azure's Computer Vision service provides developers with access to advanced algorithms that process images and return information.<br>Computer Vision Detect Faces: Detect faces in an image and provide information about each detected face. Computer Vision returns the coordinates, rectangle, gender, and age for each detected face.<br>Computer Vision provides a subset of the Face service functionality. You can use the Face service for more detailed analysis, such as facial identification and pose detection.<br><br>Box 2: Bing Video Search -<br>Search for videos and get comprehensive results<br>With Bing Video Search API v7, find videos across the web. Results provide useful metadata including creator, encoding format, video length, view count, improved &amp; simplified paging, and more.<br>Incorrect Answers:<br>Video Indexer:<br>Automatically extract metadata\u05d2\u20ac\"such as spoken words, written text, faces, speakers, celebrities, emotions, topics, brands, and scenes\u05d2\u20ac\"from video and audio files.<br>Custom Vision:<br>Easily customize your own state-of-the-art computer vision models for your unique use case. Just upload a few labeled images and let Custom Vision Service do the hard work. With just one click, you can export trained models to be run on device or as Docker containers.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/home https://azure.microsoft.com/en-us/services/cognitive-services/bing-video-search-api/",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-24T22:46:00.000Z",
        "voteCount": 9,
        "content": "I think for the second one, video indexer makes more sense..."
      },
      {
        "date": "2023-06-19T07:12:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure Cognitive Services APIs for the specified tasks are:\n\nMatch a user's picture to a picture of a celebrity: Face API\nTag a scene from a movie and search for movie scenes by using the tags: Computer Vision API"
      },
      {
        "date": "2021-05-31T02:41:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-04-09T01:32:00.000Z",
        "voteCount": 4,
        "content": "I think that at least the second answer is incorrect. Bing video search cannot \"Tag a scene from a movie, and then search for movie scenes by using the tags\". Video indexer can."
      },
      {
        "date": "2021-04-10T04:52:00.000Z",
        "voteCount": 2,
        "content": "I agree... question clearly mentions which azure cognitive services api would you choose"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47759-exam-ai-100-topic-2-question-14-discussion/",
    "body": "You need to evaluate trends in fuel prices during a period of 10 years. The solution must identify unusual fluctuations in prices and produce visual representations.<br>Which Azure Cognitive Services API should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnomaly Detector",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBing Autosuggest"
    ],
    "answer": "A",
    "answerDescription": "The Anomaly Detector API enables you to monitor and detect abnormalities in your time series data with machine learning. The Anomaly Detector API adapts by automatically identifying and applying the best-fitting models to your data, regardless of industry, scenario, or data volume. Using your time series data, the API determines boundaries for anomaly detection, expected values, and which data points are anomalies.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/anomaly-detector/overview",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:14:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure Cognitive Services API for evaluating trends in fuel prices and identifying unusual fluctuations over a 10-year period is the Anomaly Detector API (Option A)."
      },
      {
        "date": "2021-05-31T02:42:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-03-19T13:38:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50350-exam-ai-100-topic-2-question-15-discussion/",
    "body": "HOTSPOT -<br>Your company plans to deploy several apps that will use Azure Cognitive Services APIs.<br>You need to recommend which Cognitive Services APIs must be used to meet the following requirements:<br>\u2711 Must be able to identify inappropriate text and profanities in multiple languages.<br>\u2711 Must be able to interpret user requests sent by using text input.<br>\u2711 Must be able to identify named entities in text.<br>Which API should you recommend for each requirement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0007700004.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0007800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Content Moderator -<br>The Azure Content Moderator API is a cognitive service that checks text, image, and video content for material that is potentially offensive, risky, or otherwise undesirable. When such material is found, the service applies appropriate labels (flags) to the content. Your app can then handle flagged content in order to comply with regulations or maintain the intended environment for users.<br>Box 2: Language Understanding (LUIS)<br>Designed to identify valuable information in conversations, LUIS interprets user goals (intents) and distills valuable information from sentences (entities), for a high quality, nuanced language model. LUIS integrates seamlessly with the Azure Bot Service, making it easy to create a sophisticated bot.<br><br>Box 3: Text Analytics -<br>The Text Analytics API is a cloud-based service that provides advanced natural language processing over raw text, and includes four main functions: sentiment analysis, key phrase extraction, named entity recognition, and language detection.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/cognitive-services/content-moderator/overview https://www.luis.ai/home https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:15:00.000Z",
        "voteCount": 1,
        "content": "the recommended Azure Cognitive Services APIs for each requirement are:\n\nMust be able to identify inappropriate text and profanities in multiple languages: Content Moderator API\nMust be able to interpret user requests sent by using text input: Language Understanding (LUIS) API\nMust be able to identify named entities in text: Text Analytics API"
      },
      {
        "date": "2021-06-15T11:42:00.000Z",
        "voteCount": 2,
        "content": "this question was in the exam"
      },
      {
        "date": "2021-05-31T02:42:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-04-17T23:26:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11848-exam-ai-100-topic-2-question-16-discussion/",
    "body": "You plan to perform analytics of the medical records of patients located around the world.<br>You need to recommend a solution that avoids storing and processing data in the cloud.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Text Analytics API that has container support",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Apache Spark cluster that uses MMLSpark"
    ],
    "answer": "D",
    "answerDescription": "The Microsoft Machine Learning Library for Apache Spark (MMLSpark) assists in provisioning scalable machine learning models for large datasets, especially for building deep learning problems. MMLSpark works with SparkML pipelines, including Microsoft CNTK and the OpenCV library, which provide end-to-end support for the ingress and processing of image input data, categorization of images, and text analytics using pre-trained deep learning algorithms.<br>References:<br>https://subscription.packtpub.com/book/big_data_and_business_intelligence/9781789131956/10/ch10lvl1sec61/an-overview-of-the-microsoft-machine-learning- library-for-apache-spark-mmlspark",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-12T13:01:00.000Z",
        "voteCount": 13,
        "content": "B) should be the correct answer here"
      },
      {
        "date": "2020-02-06T03:27:00.000Z",
        "voteCount": 3,
        "content": "I agree. Containers can help store data locally without being stored on Cloud."
      },
      {
        "date": "2020-02-23T00:20:00.000Z",
        "voteCount": 2,
        "content": "Spark cluster will be deployed using HDInsight or used with Databricks and clusters cant be deployed locally therefore all processing and storing will be on cloud. D can't be right answer."
      },
      {
        "date": "2020-04-27T13:11:00.000Z",
        "voteCount": 1,
        "content": "you can deploy a cluster locally: https://medium.com/@joseignaciocastelli92/how-to-make-your-first-local-cluster-with-kubernetes-e15fc4b262e7"
      },
      {
        "date": "2021-04-17T21:57:00.000Z",
        "voteCount": 1,
        "content": "Shoud be B, https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-install-containers?tabs=sentiment"
      },
      {
        "date": "2021-05-27T06:28:00.000Z",
        "voteCount": 1,
        "content": "Correct. Anytime you get a question that suggests wording like \"should not be uploaded to the cloud\" usually suggests deploying a container service. Since text analytics can help perform analytics on medical documents, B is the right answer."
      },
      {
        "date": "2020-04-14T04:48:00.000Z",
        "voteCount": 13,
        "content": "D is correct.\nB is not correct as the text analytics API can't make analytics for medical records it's used in  sentiment analysis, key phrase extraction, and language detection. \nhttps://github.com/Azure/mmlspark"
      },
      {
        "date": "2020-05-28T02:07:00.000Z",
        "voteCount": 4,
        "content": "The question gives you two key pieces of information regarding the solution:\n1 - Analytics on medical records (vague statement) scattered all over the world;\n2 - The data must not be stored nor processed in the cloud;\n\nThe only relevant example in the link you referenced is example 8, which deals with....**drum roll** key phrase extraction, which you also state that Text Analytics API performs. Therefore, Text Analytics API seems like the right choice for me, as it addresses all the requirements of the question."
      },
      {
        "date": "2020-05-31T01:53:00.000Z",
        "voteCount": 7,
        "content": "MMLSpark can be deployed locally via Docker container and it uses Microsoft Cognitive Services to \"tackle problems in Deep Learning, Micro-Service Orchestration, Gradient Boosting, Model Interpretability, and other areas of modern computation\". If you want to make analytics on data you need an environment (while Text Analytics may represent only a piece of the solution), and Spark would be a good choice. So, in my opinion D is correct. Just check out here:\nhttps://mmlspark.blob.core.windows.net/website/index.html"
      },
      {
        "date": "2023-06-19T07:17:00.000Z",
        "voteCount": 1,
        "content": "the most suitable recommendation for avoiding storing and processing data in the cloud is to use the Text Analytics API with container support (Option B)."
      },
      {
        "date": "2021-02-13T19:38:00.000Z",
        "voteCount": 1,
        "content": "\"Medical Records\" most likely contain images. Text Analytics is \"text\" analytics.\n\nAzure Machine Learning works on an Edge Node... more seamlessly too, and is native to Azure. But I think bulk of the data would still need to get stored on the Cloud. Eliminating this option.\n\nMML Spark does run on Edge. But can it store all the date locally?\n\nI think AML is a better option here because it can work off of local data.\nhttps://docs.microsoft.com/en-us/azure/architecture/hybrid/deploy-ai-ml-azure-stack-edge\n\nThat said, I still don't have a strong opinion about the answer unfortunately."
      },
      {
        "date": "2020-10-07T12:33:00.000Z",
        "voteCount": 5,
        "content": "B) is correct answer to me. This allows to store data locally. In addition, Text Analytics for Health can be used for medical record processing."
      },
      {
        "date": "2020-08-04T05:24:00.000Z",
        "voteCount": 2,
        "content": "Though Text Analytics for health is in preview, but the use case on this link is an answer for this question:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-for-health"
      },
      {
        "date": "2021-03-27T14:02:00.000Z",
        "voteCount": 1,
        "content": "You never use something in Preview in production environment."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31296-exam-ai-100-topic-2-question-17-discussion/",
    "body": "Your company has an on-premises datacenter.<br>You plan to publish an app that will recognize a set of individuals by using the Face API. The model is trained.<br>You need to ensure that all images are processed in the on-premises datacenter.<br>What should you deploy to host the Face API?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Docker container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure File Sync",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Application Gateway",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Box Edge"
    ],
    "answer": "A",
    "answerDescription": "A container is a standard unit of software that packages up code and all its dependencies so the application runs quickly and reliably from one computing environment to another. A Docker container image is a lightweight, standalone, executable package of software that includes everything needed to run an application: code, runtime, system tools, system libraries and settings.<br>Incorrect Answers:<br>D: Azure Data Box Edge is an AI-enabled edge computing device with network data transfer capabilities. This article provides you an overview of the Data Box<br>Edge solution, benefits, key capabilities, and the scenarios where you can deploy this device.<br>Data Box Edge is a Hardware-as-a-service solution. Microsoft ships you a cloud-managed device with a built-in Field Programmable Gate Array (FPGA) that enables accelerated AI-inferencing and has all the capabilities of a storage gateway.<br>References:<br>https://www.docker.com/resources/what-container",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:17:00.000Z",
        "voteCount": 1,
        "content": "to ensure that all images are processed in the on-premises datacenter, the recommended approach is to deploy the Face API within a Docker container (Option A)."
      },
      {
        "date": "2021-05-31T02:42:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-03-23T02:09:00.000Z",
        "voteCount": 1,
        "content": "There is no question in this case that is a container. The other options seem quite irrelevant in my opinion.\nFor the ones saying databox: https://azure.microsoft.com/en-gb/blog/expanding-the-azure-data-box-family/ it's a product to shop data to the cloud. Does the question we need to send data from on premises to the cloud? in this case you could say azure import/export service which doesn't make a lot of sense."
      },
      {
        "date": "2021-03-16T10:58:00.000Z",
        "voteCount": 1,
        "content": "Azure stack Edge and Azure data box Edge are separate products. Azure stack you can use for model inferencing, data box edge is mainly for moving data. Container is the right answer in my opinion - https://azure.microsoft.com/en-us/services/databox/"
      },
      {
        "date": "2021-02-13T19:55:00.000Z",
        "voteCount": 1,
        "content": "I think it could be both Data Box Edge and Docker. Since the model is trained, you could deploy it into Docker. However, Data Box Edge can do the preprocessing etc. on the on-premises (edge) data center. Check out 1:06 to 1:42:\nhttps://youtu.be/UyJIexkErJw"
      },
      {
        "date": "2020-09-14T15:27:00.000Z",
        "voteCount": 1,
        "content": "It should be option D Azure Stack Edge (data box edge) https://docs.microsoft.com/en-us/azure/databox-online/azure-stack-edge-overview"
      },
      {
        "date": "2020-09-16T04:01:00.000Z",
        "voteCount": 4,
        "content": "But a Stack Edge is, (as stated in the link you provide) a hardware-as-a-service solution: \"Azure Stack Edge is a Hardware-as-a-service solution. Microsoft ships you a cloud-managed device with a built-in Field Programmable Gate Array (FPGA) that enables accelerated AI-inferencing and has all the capabilities of a network storage gateway.\"\n, so I should vote for the Docker container"
      },
      {
        "date": "2021-02-10T09:32:00.000Z",
        "voteCount": 2,
        "content": "The given answer is correct, Azure Data Box is not a hosting option https://azure.microsoft.com/en-us/services/databox/"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47760-exam-ai-100-topic-2-question-18-discussion/",
    "body": "You have a Bing Search service that is used to query a product catalog.<br>You need to identify the following information:<br>\u2711 The locale of the query<br>\u2711 The top 50 query strings<br>\u2711 The number of calls to the service<br>\u2711 The top geographical regions of the service<br>What should you implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBing Statistics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure API Management (APIM)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Monitor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Application Insights"
    ],
    "answer": "A",
    "answerDescription": "The Bing Statistics add-in provides metrics such as call volume, top queries, API response, code distribution, and market distribution. The rich slicing-and-dicing capability lets you gather deeper understanding of your users and their usage to inform your business strategy.<br>References:<br>https://www.bingapistatistics.com/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:18:00.000Z",
        "voteCount": 1,
        "content": "the recommended option for capturing the required information is to implement Azure Application Insights (Option D)."
      },
      {
        "date": "2021-06-24T02:20:00.000Z",
        "voteCount": 1,
        "content": "should not be in AI exam"
      },
      {
        "date": "2021-07-26T07:33:00.000Z",
        "voteCount": 1,
        "content": "Agree, this is more about Human Intelligence."
      },
      {
        "date": "2021-04-09T01:10:00.000Z",
        "voteCount": 3,
        "content": "how is this related to AI?"
      },
      {
        "date": "2021-03-19T13:40:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47761-exam-ai-100-topic-2-question-19-discussion/",
    "body": "You have a Face API solution that updates in real time. A pilot of the solution runs successfully on a small dataset.<br>When you attempt to use the solution on a larger dataset that continually changes, the performance degrades, slowing how long it takes to recognize existing faces.<br>You need to recommend changes to reduce the time it takes to recognize existing faces without increasing costs.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the solution to use the Computer Vision API instead of the Face API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeparate training into an independent pipeline and schedule the pipeline to run daily.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the solution to use the Bing Image Search API instead of the Face API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDistribute the face recognition inference process across many Azure Cognitive Services instances."
    ],
    "answer": "B",
    "answerDescription": "Incorrect Answers:<br>A: The purpose of Computer Vision is to inspects each image associated with an incoming article to (1) scrape out written words from the image and (2) determine what types of objects are present in the image.<br>C: The Bing API provides an experience similar to Bing.com/search by returning search results that Bing determines are relevant to a user's query. The results include Web pages and may also include images, videos, and more.<br>D: That would increase cost.<br>References:<br>https://github.com/Azure/cognitive-services",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:34:00.000Z",
        "voteCount": 1,
        "content": "recommended approach to reduce the time it takes to recognize existing faces without increasing costs is to distribute the face recognition inference process across multiple Azure Cognitive Services instances (Option D)."
      },
      {
        "date": "2021-05-31T02:42:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-03-19T13:40:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53203-exam-ai-100-topic-2-question-20-discussion/",
    "body": "HOTSPOT -<br>You plan to create a bot that will support five languages. The bot will be used by users located in three different countries. The bot will answer common customer questions. The bot will use Language Understanding (LUIS) to identify which skill to use and to detect the language of the customer.<br>You need to identify the minimum number of Azure resources that must be created for the planned bot.<br>How many QnA Maker, LUIS and Text Analytics instances should you create? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0008300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0008400001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "QnA Maker: 5 -<br>If the user plans to support multiple languages, they need to have a new QnA Maker resource for each language.<br><br>LUIS: 5 -<br>If you need a multi-language LUIS client application such as a chatbot, you have a few options. If LUIS supports all the languages, you develop a LUIS app for each language. Each LUIS app has a unique app ID, and endpoint log. If you need to provide language understanding for a language LUIS does not support, you can use Microsoft Translator API to translate the utterance into a supported language, submit the utterance to the LUIS endpoint, and receive the resulting scores.<br><br>Language detection: 1 -<br>The Language Detection feature of the Azure Text Analytics REST API evaluates text input for each document and returns language identifiers with a score that indicates the strength of the analysis.<br>This capability is useful for content stores that collect arbitrary text, where language is unknown. You can parse the results of this analysis to determine which language is used in the input document. The response also returns a score that reflects the confidence of the model. The score value is between 0 and 1.<br>The Language Detection feature can detect a wide range of languages, variants, dialects, and some regional or cultural languages. The exact list of languages for this feature isn't published.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/overview/language-support https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-language-support https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-language-detection",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-27T10:23:00.000Z",
        "voteCount": 5,
        "content": "1. QnA Maker supports knowledge base content in many languages. However, each QnA Maker service should be reserved for a single language. therefore the answer is 5\n2. for LUIS supports all the languages, you develop a LUIS app for each language. therefore 5\n3. to detect what language is required, you need only one service.\n\nTherefore, the provided answer is correct."
      },
      {
        "date": "2023-06-19T07:36:00.000Z",
        "voteCount": 1,
        "content": "the minimum number of Azure resources that should be created are 5 QnA Maker instances, 1 LUIS instance, and 1 Text Analytics instance."
      },
      {
        "date": "2021-06-05T00:14:00.000Z",
        "voteCount": 3,
        "content": "i think it should be 3,3,1"
      },
      {
        "date": "2021-05-20T04:29:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17831-exam-ai-100-topic-2-question-21-discussion/",
    "body": "You have a database that contains sales data.<br>You plan to process the sales data by using two data streams named Stream1 and Stream2. Stream1 will be used for purchase order data. Stream2 will be used for reference data.<br>The reference data is stored in CSV files.<br>You need to recommend an ingestion solution for each data stream.<br>What two solutions should you recommend? Each correct answer is a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure event hub for Stream1 and Azure Blob storage for Stream2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage for Stream1 and Stream2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure event hub for Stream1 and Stream2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage for Stream1 and Azure Cosmos DB for Stream2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB for Stream1 and an Azure event hub for Stream2"
    ],
    "answer": "AB",
    "answerDescription": "Stream1 - Azure Event -<br><br>Stream2 - Blob Storage -<br>Azure Event Hubs is a highly scalable data streaming platform and event ingestion service, capable of receiving and processing millions of events per second.<br>Event Hubs can process and store events, data, or telemetry produced by distributed software and devices. Data sent to an event hub can be transformed and stored using any real-time analytics provider or batching/storage adapters. Event Hubs provides publish-subscribe capabilities with low latency at massive scale, which makes it appropriate for big data scenarios.<br><br>Stream1, Stream2 - Blob Storage -<br>Stream Analytics has first-class integration with Azure data streams as inputs from three kinds of resources:<br><br>Azure Event Hubs -<br><br>Azure IoT Hub -<br><br>Azure Blob storage -<br>These input resources can live in the same Azure subscription as your Stream Analytics job or a different subscription.<br>References:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/real-time-ingestion",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:15:00.000Z",
        "voteCount": 1,
        "content": "For Stream1, you should use an Azure Event Hub. For Stream2, you should use Azure Blob storage."
      },
      {
        "date": "2021-02-09T12:52:00.000Z",
        "voteCount": 4,
        "content": "I think the trick here is that Stream2 only has reference data which needs to be persisted. Not sure, but I think that's why Stream2 needs a storage solution and cannot be just Event Hubs.\nGoing with given answer."
      },
      {
        "date": "2020-12-27T00:29:00.000Z",
        "voteCount": 1,
        "content": "A and C"
      },
      {
        "date": "2020-10-10T04:10:00.000Z",
        "voteCount": 1,
        "content": "I think the answer could be A and C"
      },
      {
        "date": "2020-09-28T12:12:00.000Z",
        "voteCount": 1,
        "content": "I think the answers are correct. blob data can be processed as a data stream by Stream Analytics. https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs"
      },
      {
        "date": "2020-12-11T16:46:00.000Z",
        "voteCount": 1,
        "content": "but each correct answer represents a complete solution, and stream analytics is not mentioned"
      },
      {
        "date": "2020-09-10T23:26:00.000Z",
        "voteCount": 1,
        "content": "Where is it said in the question that real time stream ingestion is required? The data already exists in a database right"
      },
      {
        "date": "2020-05-23T06:59:00.000Z",
        "voteCount": 1,
        "content": "As shown in the link, ingestion is done either with IoT Hub, Event Hubs or Kafka/HDInsight. Therefore the answer should be C\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/real-time-ingestion"
      },
      {
        "date": "2020-07-27T16:43:00.000Z",
        "voteCount": 1,
        "content": "Stream2 has to be by reference so that's why its Blob and event hub for ingestion for Stream1"
      },
      {
        "date": "2020-04-03T12:06:00.000Z",
        "voteCount": 3,
        "content": "Why is Azure Blob Storage an answer for a question asking about an ingestion solution?"
      },
      {
        "date": "2020-04-17T07:21:00.000Z",
        "voteCount": 1,
        "content": "Maybe using Hot Path?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30300-exam-ai-100-topic-2-question-22-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing an application that uses an Azure Kubernetes Service (AKS) cluster.<br>You are troubleshooting a node issue.<br>You need to connect to an AKS node by using SSH.<br>Solution: You create a managed identity for AKS, and then you create an SSH connection.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead add an SSH key to the node, and then you create an SSH connection.<br>References:<br>https://docs.microsoft.com/en-us/azure/aks/ssh",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:41:00.000Z",
        "voteCount": 1,
        "content": "B. No\nthe given solution of creating a managed identity for AKS and then creating an SSH connection does not meet the goal of connecting to an AKS node using SSH."
      },
      {
        "date": "2021-08-23T06:53:00.000Z",
        "voteCount": 1,
        "content": "all are wrong except for add SSH to the node then make SSH connection"
      },
      {
        "date": "2021-06-16T03:31:00.000Z",
        "voteCount": 1,
        "content": "We need to add an SSH key to the node, and then create an SSH connection, so answer is NO"
      },
      {
        "date": "2021-03-14T09:44:00.000Z",
        "voteCount": 1,
        "content": "I think answer is correct. As per link below with managed identity we can only Sign into Azure CLI or PowerShell with the identity.  https://docs.microsoft.com/en-us/azure/container-registry/container-registry-authentication-managed-identity"
      },
      {
        "date": "2021-02-24T16:46:00.000Z",
        "voteCount": 2,
        "content": "The reason why using SSH to AKS node is to check maintenance or troubleshoot. So here we want to generate SSH keys and added to the AKS cluster. And then create SSH connection to AKS node. So clearly what the question does is the first part, answer is B."
      },
      {
        "date": "2020-09-01T18:33:00.000Z",
        "voteCount": 2,
        "content": "Is this correct?"
      },
      {
        "date": "2020-09-25T03:43:00.000Z",
        "voteCount": 2,
        "content": "Yes, one of the correct ways to achieve this is in the explanation provided"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40951-exam-ai-100-topic-2-question-23-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing an application that uses an Azure Kubernetes Service (AKS) cluster.<br>You are troubleshooting a node issue.<br>You need to connect to an AKS node by using SSH.<br>Solution: You change the permissions of the AKS resource group, and then you create an SSH connection.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead add an SSH key to the node, and then you create an SSH connection.<br>References:<br>https://docs.microsoft.com/en-us/azure/aks/ssh",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:41:00.000Z",
        "voteCount": 1,
        "content": "B. No\n the given solution of changing the permissions of the AKS resource group and then creating an SSH connection does not meet the goal of connecting to an AKS node using SSH."
      },
      {
        "date": "2020-12-28T23:55:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/40952-exam-ai-100-topic-2-question-24-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing an application that uses an Azure Kubernetes Service (AKS) cluster.<br>You are troubleshooting a node issue.<br>You need to connect to an AKS node by using SSH.<br>Solution: You add an SSH key to the node, and then you create an SSH connection.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "By default, SSH keys are generated when you create an AKS cluster. If you did not specify your own SSH keys when you created your AKS cluster, add your public SSH keys to the AKS nodes.<br>You also need to create an SSH connection to the AKS node.<br>References:<br>https://docs.microsoft.com/en-us/azure/aks/ssh",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:43:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nAdding an SSH key to the AKS node and then creating an SSH connection can meet the goal of connecting to an AKS node using SSH. By adding the SSH key to the node, you establish the necessary credentials for SSH authentication. Once the key is added, you can use it to authenticate and establish an SSH connection to the AKS node.\n\nThis solution assumes that you have access to the AKS node and have the necessary privileges to add the SSH key. Additionally, make sure that the SSH key is correctly configured and that the SSH port (default: port 22) is accessible for the AKS node.\n\nConnecting to an AKS node using SSH can be useful for troubleshooting purposes, but it's important to follow security best practices and limit SSH access only to authorized users or when necessary."
      },
      {
        "date": "2020-12-28T23:55:00.000Z",
        "voteCount": 2,
        "content": "This answer is correct."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31157-exam-ai-100-topic-2-question-25-discussion/",
    "body": "You are developing a Computer Vision application.<br>You plan to use a workflow that will load data from an on-premises database to Azure Blob storage, and then connect to an Azure Machine Learning service.<br>What should you use to orchestrate the workflow?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Kubernetes Service (AKS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Container Instances"
    ],
    "answer": "C",
    "answerDescription": "With Azure Data Factory you can use workflows to orchestrate data integration and data transformation processes at scale.<br>Build data integration, and easily transform and integrate big data processing and machine learning with the visual interface.<br>References:<br>https://azure.microsoft.com/en-us/services/data-factory/",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-15T06:00:00.000Z",
        "voteCount": 10,
        "content": "The comment by Nova077 must not be approved untill it has some source or reference . Such comments creates confusions,,,,"
      },
      {
        "date": "2023-06-19T13:17:00.000Z",
        "voteCount": 1,
        "content": "To orchestrate the workflow, you should use Azure Data Factory"
      },
      {
        "date": "2021-07-26T07:44:00.000Z",
        "voteCount": 1,
        "content": "Seems \"orchestrate' always end up with Azure Data Factory..."
      },
      {
        "date": "2021-06-19T12:38:00.000Z",
        "voteCount": 1,
        "content": "you can do copy activity but read through the docs\nhttps://docs.microsoft.com/en-us/azure/data-factory/copy-activity-overview"
      },
      {
        "date": "2021-03-23T07:58:00.000Z",
        "voteCount": 1,
        "content": "Why not azure pipeline?"
      },
      {
        "date": "2020-09-12T12:57:00.000Z",
        "voteCount": 1,
        "content": "You can't use ADF to transfer on premise data though!"
      },
      {
        "date": "2020-09-13T17:46:00.000Z",
        "voteCount": 4,
        "content": "You can.\n https://marlonribunal.com/copy-data-from-on-premise-sql-server-to-azure-database-using-azure-data-factory/"
      },
      {
        "date": "2020-09-22T21:44:00.000Z",
        "voteCount": 2,
        "content": "Yes. answer is correct. This link shows how it can be done https://docs.microsoft.com/en-us/azure/data-factory/tutorial-hybrid-copy-portal"
      },
      {
        "date": "2021-03-09T21:23:00.000Z",
        "voteCount": 8,
        "content": "Yes it can be done, i use ADF on day-to-day basis. Please be careful with your comments. Community is dependent upon discussions, as they clarify lot of confusions."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38871-exam-ai-100-topic-2-question-26-discussion/",
    "body": "DRAG DROP -<br>You are designing an AI solution that will use IoT devices to gather data from conference attendees, and then later analyze the data. The IoT devices will connect to an Azure IoT hub.<br>You need to design a solution to anonymize the data before the data is sent to the IoT hub.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0009000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0009000002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create a storage container<br>ASA Edge jobs run in containers deployed to Azure IoT Edge devices.<br>Step 2: Create an Azure Stream Analytics Edge Job<br>Azure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data.<br>Scenario overview:<br><img src=\"/assets/media/exam-media/03857/0009100001.jpg\" class=\"in-exam-image\"><br>Step 3: Add the job to the IoT devices in IoT<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-05T01:24:00.000Z",
        "voteCount": 8,
        "content": "This is correct"
      },
      {
        "date": "2021-05-10T09:36:00.000Z",
        "voteCount": 2,
        "content": "Storage container or docker container?!"
      },
      {
        "date": "2021-06-20T10:10:00.000Z",
        "voteCount": 1,
        "content": "Firstly create Storage Account and container https://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics?view=iotedge-2020-11"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19624-exam-ai-100-topic-2-question-27-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution that will ingest data from an Azure IoT Edge device, preprocess the data in Azure Machine Learning, and then move the data to<br>Azure HDInsight for further processing.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0009200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0009300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Export Data -<br>The Export data to Hive option in the Export Data module in Azure Machine Learning Studio. This option is useful when you are working with very large datasets, and want to save your machine learning experiment data to a Hadoop cluster or HDInsight distributed storage.<br><br>Box 2: Apache Hive -<br>Apache Hive is a data warehouse system for Apache Hadoop. Hive enables data summarization, querying, and analysis of data. Hive queries are written in<br>HiveQL, which is a query language similar to SQL.<br><br>Box 3: Azure Data Lake -<br>Default storage for the HDFS file system of HDInsight clusters can be associated with either an Azure Storage account or an Azure Data Lake Storage.<br>References:<br>https://docs.microsoft.com/en-us/azure/machine-learning/studio-module-reference/export-to-hive-query https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/hdinsight-use-hive",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-04T21:15:00.000Z",
        "voteCount": 6,
        "content": "The answer is right, Apache Hive is a better choice since Apache Spark is more for in-memory data processing\n https://www.quora.com/What-is-the-difference-between-Apache-Hive-and-Apache-Spark"
      },
      {
        "date": "2020-06-19T22:28:00.000Z",
        "voteCount": 1,
        "content": "But, there is no mention of cost implication. By this, Apache Spark is more validated."
      },
      {
        "date": "2020-06-25T05:59:00.000Z",
        "voteCount": 5,
        "content": "From my perspective, Output should be HDFS, since it gonna used by HDInsight."
      },
      {
        "date": "2020-08-05T00:26:00.000Z",
        "voteCount": 1,
        "content": "HDInsight storage options are listed here:\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-compare-storage-options\nBased on this, the correct answer should be Azure Data Lake"
      },
      {
        "date": "2020-09-12T13:14:00.000Z",
        "voteCount": 2,
        "content": "I think HDFS is not necessarily a storage option. Its a data file structure within Hadoop. The  storage should be something like Cosmos DB or Data Lake. Datalake is a better option as its IOT data."
      },
      {
        "date": "2020-09-22T23:23:00.000Z",
        "voteCount": 1,
        "content": "Here is the link for the last one https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-hadoop-use-blob-storage"
      },
      {
        "date": "2020-09-22T22:32:00.000Z",
        "voteCount": 4,
        "content": "Is this question still relevant? The question and the solution links direct us to the old azure ml studio(classic). However the current version has no mention of exporting data to Apache Hive https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/export-data\nThese are the ones supported now:\n    Azure Blob Container\n    Azure File Share\n    Azure Data Lake Storage Gen1\n    Azure Data Lake Storage Gen2\n    Azure SQL database\nSo can this question still come or is it outdated?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47764-exam-ai-100-topic-2-question-28-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You need to create an IoT solution that performs the following tasks:<br>\u2711 Identifies hazards<br>\u2711 Provides a real-time online dashboard<br>\u2711 Takes images of an area every minute<br>\u2711 Counts the number of people in an area every minute<br>Solution: You implement Azure Cognitive Services containers on the IoT devices, and then you configure results to be sent to an Azure IoT hub. You configure<br>Microsoft Power BI to connect to the IoT hub by using Azure Stream Analytics.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "There is support for running Azure Cognitive Services containers for Text Analytics and Language Understanding containers on edge devices with Azure IoT<br>Edge. This means that all your workloads can be run locally where your data is being generated while keeping the simplicity of the cloud to manage them remotely, securely and at scale.<br>You would have to set up an IoT Edge device and its IoT Hub.<br>Note: Azure Stream Analytics enables you to take advantage of one of the leading business intelligence tools, Microsoft Power BI.<br>Get your IoT hub ready for data access by adding a consumer group.<br>Create, configure, and run a Stream Analytics job for data transfer from your IoT hub to your Power BI account.<br>Create and publish a Power BI report to visualize the data.<br>References:<br>https://azure.microsoft.com/es-es/blog/running-cognitive-services-on-iot-edge/ https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:22:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\nYou can use Azure Blob storage to store the images of an area every minute"
      },
      {
        "date": "2021-03-19T13:44:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/44712-exam-ai-100-topic-2-question-29-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You need to create an IoT solution that performs the following tasks:<br>\u2711 Identifies hazards<br>\u2711 Provides a real-time online dashboard<br>\u2711 Takes images of an area every minute<br>\u2711 Counts the number of people in an area every minute<br>Solution: You configure the IoT devices to send the images to an Azure IoT hub, and then you configure an Azure Functions call to Azure Cognitive Services that sends the results to an Azure event hub. You configure Microsoft Power BI to connect to the event hub by using Azure Stream Analytics.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead use Cognitive Services containers on the IoT devices.<br>References:<br>https://azure.microsoft.com/es-es/blog/running-cognitive-services-on-iot-edge/ https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T02:44:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nThe provided solution meets the stated goals of the IoT solution:\n\nIdentifies hazards: By configuring Azure Functions to call Azure Cognitive Services, the solution can analyze the images received from the IoT devices and identify hazards or objects of interest.\nProvides a real-time online dashboard: By connecting Microsoft Power BI to the Azure event hub using Azure Stream Analytics, the solution can stream the analyzed results to Power BI for real-time visualization and dashboard creation.\nTakes images of an area every minute: By configuring the IoT devices to send the images to an Azure IoT hub, the solution ensures that images are captured from the area at regular intervals.\nCounts the number of people in an area every minute: Although the exact method for counting people in the area is not specified in the solution, it can be achieved by utilizing Azure Cognitive Services or other image processing techniques within the Azure Functions."
      },
      {
        "date": "2021-03-16T13:47:00.000Z",
        "voteCount": 1,
        "content": "It says that it identifies hazards. from that I understand that it does that by its own without the need for cloud. So the solution needs to be imbedded inside, thus No"
      },
      {
        "date": "2021-03-16T01:39:00.000Z",
        "voteCount": 3,
        "content": "Isn't this exactly the example shown here: https://docs.microsoft.com/en-us/learn/modules/build-ml-model-with-azure-stream-analytics/ ?"
      },
      {
        "date": "2021-03-16T13:45:00.000Z",
        "voteCount": 1,
        "content": "very similar indeed"
      },
      {
        "date": "2021-02-22T18:32:00.000Z",
        "voteCount": 1,
        "content": "I don't think it is wrong!"
      },
      {
        "date": "2021-02-22T18:33:00.000Z",
        "voteCount": 1,
        "content": "It doesn't have to be in the edge all the time!"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33475-exam-ai-100-topic-2-question-30-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You need to create an IoT solution that performs the following tasks:<br>\u2711 Identifies hazards<br>\u2711 Provides a real-time online dashboard<br>\u2711 Takes images of an area every minute<br>\u2711 Counts the number of people in an area every minute<br>Solution: You configure the IoT devices to send the images to an Azure IoT hub, and then you configure an Azure Automation call to Azure Cognitive Services that sends the results to an Azure event hub. You configure Microsoft Power BI to connect to the event hub by using Azure Stream Analytics.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Instead use Cognitive Services containers on the IoT devices.<br>References:<br>https://azure.microsoft.com/es-es/blog/running-cognitive-services-on-iot-edge/ https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:25:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\nTo provide a real-time online dashboard, you can configure Microsoft Power BI to connect to the IoT hub by using Azure Stream Analytics[1]. Therefore, the proposed solution meets the goal."
      },
      {
        "date": "2020-10-02T19:59:00.000Z",
        "voteCount": 2,
        "content": "Because IOT device need to detect the hazards. So the solution does not match"
      },
      {
        "date": "2021-02-14T12:34:00.000Z",
        "voteCount": 1,
        "content": "I don't think that is the primary reason for why this is not a match. I think the culprit here is the Azure Automation. As long as the IoT Hub gets the images, and as long as Cog Svcs are in the picture and as long as the integrations are possible between the components listed in the solution, it would work.\nThat's the reason I think that using Functions and Event Hubs will also serve the purpose.\nIn this case though, Az Automation is a misfit."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47762-exam-ai-100-topic-2-question-31-discussion/",
    "body": "You plan to deploy Azure IoT Edge devices. Each device will store more than 10,000 images locally. Each image is approximately 5 MB.<br>You need to ensure that the images persist on the devices for 14 days.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics on the IoT Edge devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Database for Postgres SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage on the IoT Edge devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server on the IoT Edge devices"
    ],
    "answer": "C",
    "answerDescription": "Azure Blob Storage on IoT Edge provides a block blob and append blob storage solution at the edge. A blob storage module on your IoT Edge device behaves like an Azure blob service, except the blobs are stored locally on your IoT Edge device.<br>This e is useful where data needs to be stored locally until it can be processed or transferred to the cloud. This data can be videos, images, finance data, hospital data, or any other unstructured data.<br>References:<br>https://docs.microsoft.com/en-us/azure/iot-edge/how-to-store-data-blob",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:27:00.000Z",
        "voteCount": 1,
        "content": "To store the images locally on the IoT Edge devices, you can use Azure Blob Storage on IoT Edge. You can configure the time-to-live feature to ensure that the images persist on the devices for 14 days. Therefore, you should use Azure Blob storage on the IoT Edge devices to store the images."
      },
      {
        "date": "2021-03-19T13:42:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2021-08-21T03:39:00.000Z",
        "voteCount": 1,
        "content": "I think you need to share more insights on why you think an answer is correct so you would help people gain more understanding. Just saying correct with no basis is not enough. Thanks."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47763-exam-ai-100-topic-2-question-32-discussion/",
    "body": "You have an Azure Machine Learning experiment.<br>You need to validate that the experiment meets GDPR regulation requirements and stores documentation about the experiment.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompliance Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Log Analytics workspace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Security Center"
    ],
    "answer": "A",
    "answerDescription": "Compliance Manager for Azure helps you assess and manage GDPR compliance. Compliance Manager is a free, Microsoft cloud services solution designed to help organizations meet complex compliance obligations, including the GDPR, ISO 27001, ISO 27018, and NIST 800-53. Generally available today for Azure customers, the Compliance Manager GDPR dashboard enables you to assign, track, and record your GDPR compliance activities so you can collaborate across teams and manage your documents for creating audit reports more easily.<br>References:<br>https://azure.microsoft.com/en-us/blog/new-capabilities-to-enable-robust-gdpr-compliance/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:28:00.000Z",
        "voteCount": 1,
        "content": "To store the documentation about the experiment, you can use Azure Table storage. To validate that the experiment meets GDPR regulation requirements, you can use Compliance Manager"
      },
      {
        "date": "2021-03-19T13:43:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17705-exam-ai-100-topic-2-question-33-discussion/",
    "body": "You are designing a solution that will integrate the Bing Web Search API and will return a JSON response. The development team at your company uses C# as its primary development language.<br>You provide developers with the Bing endpoint.<br>Which additional component do the developers need to prepare and to retrieve data by using an API call?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe subscription ID",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe API key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe resource group ID"
    ],
    "answer": "C",
    "answerDescription": "The Bing Web Search SDK makes it easy to integrate Bing Web Search into your C# application. You instantiate a client, send a request, and receive a response.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/web-search-sdk-quickstart",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-30T20:56:00.000Z",
        "voteCount": 23,
        "content": "Shouldnt answer be API Key ?"
      },
      {
        "date": "2020-04-23T09:26:00.000Z",
        "voteCount": 11,
        "content": "I can't believe that the answer is not an API key."
      },
      {
        "date": "2023-06-19T13:29:00.000Z",
        "voteCount": 1,
        "content": "Based on the retrieved documents, you should use the API key to retrieve data by using an API call"
      },
      {
        "date": "2021-06-29T08:17:00.000Z",
        "voteCount": 1,
        "content": "For the uriBase value, you can use the global endpoint in the following code, or use the custom subdomain endpoint displayed in the Azure portal for your resource.\n\nConfirm that uriBase is valid and replace the accessKey value with a subscription key from your Azure account.\n\nOptionally, customize the search query by replacing the value for searchTerm.\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/quickstarts/csharp"
      },
      {
        "date": "2021-07-26T17:53:00.000Z",
        "voteCount": 1,
        "content": "Here only options are \"Subscription ID\" and \"API Key\"o_0"
      },
      {
        "date": "2021-06-26T18:45:00.000Z",
        "voteCount": 1,
        "content": "This should be API key. \nhttps://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/quickstarts/csharp"
      },
      {
        "date": "2021-01-31T11:59:00.000Z",
        "voteCount": 2,
        "content": "Subscription ID could be a better answer"
      },
      {
        "date": "2021-02-14T12:57:00.000Z",
        "voteCount": 3,
        "content": "It's not the Azure Subscription ID. I'm sure of it because the Subscription ID will do nothing for the retrieval of anything in a Cognitive Svc.\nThe Subscription Key on the other hand is required to get hold of the API. It allows to authenticate to the API using a basic API Key mechanism.\nHowever, the question says \"prepare\" and \"retrieve\" data by using an API call. So while 'query' comes close to preparing, I'm certain that the expected answer is the \"API Key\"."
      },
      {
        "date": "2021-01-31T11:57:00.000Z",
        "voteCount": 3,
        "content": "Bing is a public search engine, no authentication needed. \nSo, I think C is the correct answer."
      },
      {
        "date": "2021-01-12T23:49:00.000Z",
        "voteCount": 1,
        "content": "I suppose need to understand the ask here \"additional component developer need to prepare and to retrieve data by using an API call\". Developer need to prepare the query to retrieve the data."
      },
      {
        "date": "2020-06-24T23:30:00.000Z",
        "voteCount": 6,
        "content": "Answer is \"C\"  \n,According to me its correct only. The developer is going to prepare only the query not a API key\n \"Which additional component do the developers need to prepare and to retrieve data\"."
      },
      {
        "date": "2021-05-25T13:04:00.000Z",
        "voteCount": 2,
        "content": "You didn't read the full question - it's asking for components that you are giving to the developers. In order to intiate anything on the web, you will need the endpoint URL and the key. Hence in this case, the answer is the API key."
      },
      {
        "date": "2020-06-18T01:42:00.000Z",
        "voteCount": 2,
        "content": "According to MS Documentation it should be subscriptionKey to call the Bing Web Search API.\n\"In this application, the main method includes code that instantiates the client, validates the subscriptionKey, and calls WebResults. Make sure that you enter a valid subscription key for your Azure account before continuing.\"\nSrc:\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/bing-web-search/quickstarts/client-libraries?pivots=programming-language-csharp"
      },
      {
        "date": "2020-06-21T20:21:00.000Z",
        "voteCount": 1,
        "content": "i think there is no such thing as Sub key, only Sub ID.\nTo all other cognitive services, they all use API key aka Ocp-Apim-Subscription-Key.\nYou can give it a try at https://dev.cognitive.microsoft.com/"
      },
      {
        "date": "2020-10-21T05:41:00.000Z",
        "voteCount": 1,
        "content": "the link provided by the person does talk about a subscription key. however there is no such option...only subscription id"
      },
      {
        "date": "2020-10-21T05:49:00.000Z",
        "voteCount": 1,
        "content": "however the class to which the subscription key is passed is called ApiKeyServiceClientCredentials\n\nvar client = new WebSearchClient(new ApiKeyServiceClientCredentials(\"YOUR_SUBSCRIPTION_KEY\"));\n\nI will go with the option subscription_id for this question because of this line above the code\n\"Make sure that you enter a valid subscription key for your Azure account before continuing.\"\nIt seems like the subscription key is something users have at a account level..not specific to the api.\nThe wording of the question is confusing. It says \"prepare\""
      },
      {
        "date": "2020-05-07T17:17:00.000Z",
        "voteCount": 2,
        "content": "answer has to be API Key, you have the URL endpoint and now you need credential to use it\nhttps://docs.microsoft.com/en-gb/azure/cognitive-services/bing-web-search/quickstarts/client-libraries?pivots=programming-language-csharp"
      },
      {
        "date": "2020-04-24T02:35:00.000Z",
        "voteCount": 7,
        "content": "static void Main(string[] args)\n{\n    var client = new WebSearchClient(new ApiKeyServiceClientCredentials(\"YOUR_SUBSCRIPTION_KEY\"));\n\n    WebResults(client);\n\n    Console.WriteLine(\"Press any key to exit...\");\n    Console.ReadKey();\n}"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/38872-exam-ai-100-topic-2-question-34-discussion/",
    "body": "DRAG DROP -<br>You need to build an AI solution that will be shared between several developers and customers.<br>You plan to write code, host code, and document the runtime all within a single user experience.<br>You build the environment to host the solution.<br>Which three actions should you perform in sequence next? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0009900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0010000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an Azure Machine Learning Studio workspace<br><br>Step 2: Create a notebook -<br>You can manage notebooks using the UI, the CLI, and by invoking the Workspace API.<br><br>To create a notebook -<br>1. Click the Workspace button Workspace Icon or the Home button Home Icon in the sidebar. Do one of the following:<br>Next to any folder, click the Menu Dropdown on the right side of the text and select Create &gt; Notebook. Create Notebook<br>In the Workspace or a user folder, click Down Caret and select Create &gt; Notebook.<br>2.  In the Create Notebook dialog, enter a name and select the notebook's primary language.<br>3.  If there are running clusters, the Cluster drop-down displays. Select the cluster to attach the notebook to.<br>4.  Click Create.<br><br>Step 3: Create a new experiment -<br>Create a new experiment by clicking +NEW at the bottom of the Machine Learning Studio window. Select EXPERIMENT &gt; Blank Experiment.<br>References:<br>https://docs.azuredatabricks.net/user-guide/notebooks/notebook-manage.html https://docs.microsoft.com/en-us/azure/machine-learning/service/quickstart-run-cloud-notebook",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-05T01:36:00.000Z",
        "voteCount": 5,
        "content": "The order is correct"
      },
      {
        "date": "2021-01-24T04:04:00.000Z",
        "voteCount": 3,
        "content": "1 Create an Azure Machine Learning Studio workspace\n2 Create a new notebook\n3 Build an experiment"
      },
      {
        "date": "2021-02-09T17:20:00.000Z",
        "voteCount": 4,
        "content": "Couldn't find anything in the documentation that says \"Build\" an experiment. Rather it's \"Run\" an experiment in some docs. But mostly found \"Create\" an experiment to be more appropriate. Tricky verbs.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/tutorial-1st-experiment-sdk-train#environment"
      },
      {
        "date": "2021-01-15T01:52:00.000Z",
        "voteCount": 1,
        "content": "I believe the correct answer is:\n1. Create an Azure Analytics Service\n2. Create stream inputs and outputs\n3. Implement Transact-SQL for the stream query\n\nReference: \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-quick-create-portal"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5784-exam-ai-100-topic-2-question-35-discussion/",
    "body": "Your company has a data team of Transact-SQL experts.<br>You plan to ingest data from multiple sources into Azure Event Hubs.<br>You need to recommend which technology the data team should use to move and query data from Event Hubs to Azure Storage. The solution must leverage the data team's existing skills.<br>What is the best recommendation to achieve the goal? More than one answer choice may achieve the goal.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Notification Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Grid",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka streams",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics"
    ],
    "answer": "B",
    "answerDescription": "Event Hubs Capture is the easiest way to automatically deliver streamed data in Event Hubs to an Azure Blob storage or Azure Data Lake store. You can subsequently process and deliver the data to any other storage destinations of your choice, such as SQL Data Warehouse or Cosmos DB.<br>You to capture data from your event hub into a SQL data warehouse by using an Azure function triggered by an event grid.<br>Example:<br><img src=\"/assets/media/exam-media/03857/0010200001.jpg\" class=\"in-exam-image\"><br>First, you create an event hub with the Capture feature enabled and set an Azure blob storage as the destination. Data generated by WindTurbineGenerator is streamed into the event hub and is automatically captured into Azure Storage as Avro files.<br>Next, you create an Azure Event Grid subscription with the Event Hubs namespace as its source and the Azure Function endpoint as its destination.<br>Whenever a new Avro file is delivered to the Azure Storage blob by the Event Hubs Capture feature, Event Grid notifies the Azure Function with the blob URI. The<br>Function then migrates data from the blob to a SQL data warehouse.<br>References:<br>https://docs.microsoft.com/en-us/azure/event-hubs/store-captured-data-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-27T17:11:00.000Z",
        "voteCount": 35,
        "content": "This answer is wrong, it is Stream Analytics...  https://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/intelligent-apps-image-processing"
      },
      {
        "date": "2019-12-12T21:04:00.000Z",
        "voteCount": 7,
        "content": "I agree. Stream analytics used T-SQL"
      },
      {
        "date": "2020-01-12T00:07:00.000Z",
        "voteCount": 7,
        "content": "I concur , because stream analytics uses a T-SQL API"
      },
      {
        "date": "2023-06-19T13:31:00.000Z",
        "voteCount": 1,
        "content": "D. Azure Stream Analytics\n\nAzure Stream Analytics is a real-time analytics and complex event processing engine that is well-suited for ingesting, processing, and storing data from various sources, including Azure Event Hubs. It provides a familiar SQL-like language (Transact-SQL) for querying and transforming data, making it a suitable choice for a data team of Transact-SQL experts. With Azure Stream Analytics, the data team can easily define queries and transformations to move and query data from Event Hubs to Azure Storage."
      },
      {
        "date": "2021-08-23T06:55:00.000Z",
        "voteCount": 2,
        "content": "DDDDDDDDDDDDDDDDDDDDDDDDdd"
      },
      {
        "date": "2021-02-09T17:50:00.000Z",
        "voteCount": 1,
        "content": "Stream Analytics for sure.\nAdditionally, if I can overthink this, you could use the Azure Data Explorer to have the SQL guys write KQL to flow it through Event Grid too.\nhttps://docs.microsoft.com/en-us/azure/data-explorer/data-explorer-overview"
      },
      {
        "date": "2020-08-05T02:50:00.000Z",
        "voteCount": 2,
        "content": "Event Grid do not use SQL, while Stream Analytics does. Both could do the job, but as it is required to use T-SQL knowledge, ASA is the best solution: https://docs.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics"
      },
      {
        "date": "2020-06-23T05:54:00.000Z",
        "voteCount": 3,
        "content": "\" More than one answer choice may achieve the goal.\", so Stream Analytics and Event Grid could achieve the goal"
      },
      {
        "date": "2020-03-29T10:50:00.000Z",
        "voteCount": 6,
        "content": "Azure Stream Analytics is the answer"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55503-exam-ai-100-topic-2-question-36-discussion/",
    "body": "You are designing a Computer Vision AI application.<br>You need to recommend a deployment solution for the application. The solution must ensure that costs scale linearly without any upfront costs.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta containerized Computer Vision API on Azure Kubernetes Service (AKS) that has autoscaling configured",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Computer Vision API as a single resource",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Container Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta containerized Computer Vision API on Azure Kubernetes Service (AKS) that has virtual nodes configured"
    ],
    "answer": "A",
    "answerDescription": "Containers enable you to run the Computer Vision APIs in your own environment.<br>Note: The host is a x64-based computer that runs the Docker container. It can be a computer on your premises or a Docker hosting service in Azure, such as:<br>\u2711 Azure Container Instances.<br>\u2711 Azure Kubernetes Service.<br>\u2711 A Kubernetes cluster deployed to Azure Stack.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/computer-vision/computer-vision-how-to-install-containers",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T03:00:00.000Z",
        "voteCount": 1,
        "content": "D. a containerized Computer Vision API on Azure Kubernetes Service (AKS) that has virtual nodes configured\n\nTo deploy a Computer Vision AI application that scales linearly without any upfront costs, you should use a containerized Computer Vision API on Azure Kubernetes Service (AKS) that has virtual nodes configured"
      },
      {
        "date": "2021-06-17T06:10:00.000Z",
        "voteCount": 1,
        "content": "This was in my exam 15th of June, 2021."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/44724-exam-ai-100-topic-2-question-37-discussion/",
    "body": "You are implementing the Language Understanding (LUIS) API and are building a GDPR-compliant bot by using the Bot Framework.<br>You need to recommend a solution to ensure that the implementation of LUIS is GDPR-compliant.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable active learning for the bot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the bot to send the active learning preference of a user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the utterances from Review endpoint utterances."
    ],
    "answer": "C",
    "answerDescription": "Deleting personal data from the device or service and can be used to support your obligations under the GDPR.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/cognitive-services/luis/luis-user-privacy",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-22T18:09:00.000Z",
        "voteCount": 6,
        "content": "Answer should be \"B\"\nhttps://romsbotframeworkarr.azurewebsites.net/2018/04/23/general-data-protection-regulation-gdpr/\n\nGDPR is not deleting the data. It is giving the user the option to view, export and or delete user data."
      },
      {
        "date": "2021-07-26T18:12:00.000Z",
        "voteCount": 1,
        "content": "But deleting data also surely works."
      },
      {
        "date": "2023-06-19T13:35:00.000Z",
        "voteCount": 1,
        "content": "C. Delete the utterances from Review endpoint utterances.\n\nTo ensure GDPR compliance with the Language Understanding (LUIS) API in the context of a Bot Framework implementation, it is important to handle user data appropriately. Deleting the utterances from the Review endpoint utterances helps to remove any personal data that may have been collected during the interaction with the bot. This step ensures that user data is not stored or retained longer than necessary, aligning with GDPR principles of data minimization and data retention."
      },
      {
        "date": "2021-02-14T18:46:00.000Z",
        "voteCount": 2,
        "content": "Correct. Another option is to *Disable* Active Learning."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/45456-exam-ai-100-topic-2-question-38-discussion/",
    "body": "You need to build a reputation monitoring solution that reviews Twitter activity about your company. The solution must identify negative tweets and tweets that contain inappropriate images.<br>You plan to use Azure Logic Apps to build the solution.<br>Which two additional Azure services should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComputer Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blueprint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContent Moderator",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning Service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForm Recognizer"
    ],
    "answer": "CD",
    "answerDescription": "C: You can filter your tweets using Azure Logic Apps &amp; Content Moderation. Azure Content Moderator is a cognitive service that checks text, image, and video content for material that is potentially offensive, risky, or otherwise undesirable. When this material is found, the service applies appropriate labels (flags) to the content. Your app can then handle flagged content in order to comply with regulations or maintain the intended environment for users.<br>D: You can write an application so that when a user tweets with configured Twitter Hashtag, Logic App gets triggered and passed to Cognitive Text Analytics<br>Connector for detecting the sentiments of the tweet (text). If the tweeted text is found to be harsh or with bad or abusive language, the tweet can be handled appropriately.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/content-moderator/overview https://www.c-sharpcorner.com/article/role-of-text-analytics-service-as-a-connector-in-azure-logic-apps/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:39:00.000Z",
        "voteCount": 1,
        "content": "To identify negative tweets and tweets that contain inappropriate images, you can use Azure Content Moderator. Azure Content Moderator is a cognitive service that checks text, image, and video content for material that is potentially offensive, risky, or otherwise undesirable. You can use the Text Analytics connector in Azure Logic Apps to analyze the sentiment of tweets[1]. You can then use Azure Content Moderator to check the images in the tweets for inappropriate content. Therefore, you should include Azure Content Moderator and Text Analytics in the solution."
      },
      {
        "date": "2021-04-10T04:25:00.000Z",
        "voteCount": 2,
        "content": "For inappropriate images we need computer vision, I would go with text analytics along with it"
      },
      {
        "date": "2021-05-10T10:59:00.000Z",
        "voteCount": 3,
        "content": "It is definitely Content Moderator and not Computer Vision"
      },
      {
        "date": "2021-05-24T05:06:00.000Z",
        "voteCount": 3,
        "content": "inappropriate images are captured via COntent Moderator\nhttps://azure.microsoft.com/en-us/services/cognitive-services/content-moderator/"
      },
      {
        "date": "2021-02-22T17:30:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47766-exam-ai-100-topic-2-question-39-discussion/",
    "body": "Your company uses an internal blog to share news with employees.<br>You use the Translator Text API to translate the text in the blog from English to several other languages used by the employees.<br>Several employees report that the translations are often inaccurate.<br>You need to improve the accuracy of the translations.<br>What should you add to the translation solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLanguage Understanding (LUIS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Media Services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustom Translator"
    ],
    "answer": "D",
    "answerDescription": "Custom Translator is a feature of the Microsoft Translator service. With Custom Translator, enterprises, app developers, and language service providers can build neural translation systems that understand the terminology used in their own business and industry. The customized translation system will then seamlessly integrate into existing applications, workflows and websites.<br>Custom Translator allows users to customize Microsoft Translator's advanced neural machine translation for Translator's supported neural translation languages.<br>Custom Translator can be used for customizing text when using the Microsoft Translator Text API , and speech translation using the Microsoft Speech services.<br>References:<br>https://www.microsoft.com/en-us/translator/business/customization/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:40:00.000Z",
        "voteCount": 1,
        "content": "D. Custom Translator\n\nTo improve the accuracy of translations in your scenario, you should add the Custom Translator service to your translation solution. Custom Translator allows you to customize and fine-tune the translation models according to your specific domain and language requirements. By training the translation models with your company-specific terminology and style, you can enhance the accuracy of translations and ensure that the translations align with the context and preferences of your employees."
      },
      {
        "date": "2021-03-19T13:45:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47767-exam-ai-100-topic-2-question-40-discussion/",
    "body": "You plan to develop a bot that tracks communications between the employees at your company.<br>You need to identify which channel the bot must use to monitor reactions to messages by employees.<br>What should you identify?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Cortana",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Outlook",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Teams"
    ],
    "answer": "C",
    "answerDescription": "Bots in Microsoft Teams can be part of a one-to-one conversation, a group chat, or a channel in a Team.<br>Note: In Microsoft Teams, teams are groups of people brought together for work, projects, or common interests. Teams are made up of channels. Each channel is built around a topic, like \u05d2\u20acTeam Events,\u05d2\u20ac a department name, or just for fun. Channels are where you hold meetings, have conversations, and work on files together.<br>References:<br>https://docs.microsoft.com/en-us/microsoftteams/platform/bots/what-are-bots",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:43:00.000Z",
        "voteCount": 1,
        "content": "You should identify Microsoft Teams as the channel to monitor reactions to messages by employees. Microsoft Teams is a collaboration platform that allows employees to communicate and collaborate in real-time. It provides features such as chat, video conferencing, and file sharing, which makes it an ideal platform for monitoring employee communications"
      },
      {
        "date": "2021-03-19T13:46:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55385-exam-ai-100-topic-2-question-41-discussion/",
    "body": "You plan to implement a bot that will require user authentication.<br>You need to recommend a secure solution that provides encryption for the authentication of the bot.<br>Which two security solutions should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNTLM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON Web Token (JWT)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAPI keys",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsmart cards",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSSL/TLS"
    ],
    "answer": "BE",
    "answerDescription": "Your bot communicates with the Bot Connector service using HTTP over a secured channel (SSL/TLS).<br>JSON Web Tokens are used to encode tokens that are sent to and from the bot.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/bot-service/rest-api/bot-framework-rest-connector-authentication",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T13:44:00.000Z",
        "voteCount": 1,
        "content": "To provide encryption for the authentication of the bot, you should include the following security solutions in the recommendation:\n\nB. JSON Web Token (JWT): JWT is a secure and compact method for representing claims between two parties. It can be used for authentication and authorization purposes, providing a secure way to transmit and verify user identity and permissions.\n\nE. SSL/TLS: SSL (Secure Sockets Layer) and its successor TLS (Transport Layer Security) are cryptographic protocols that provide secure communication over the internet. By implementing SSL/TLS, you can ensure that the communication between the bot and the authentication system is encrypted, protecting the confidentiality and integrity of the data transmitted."
      },
      {
        "date": "2021-06-26T16:18:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2021-06-15T11:48:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3400-exam-ai-100-topic-2-question-42-discussion/",
    "body": "HOTSPOT -<br>You are developing an application that will perform clickstream analysis. The application will ingest and analyze millions of messages in the real time.<br>You need to ensure that communication between the application and devices is bidirectional.<br>What should you use for data ingestion and stream processing? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0010700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0010800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure IoT Hub -<br>Azure IoT Hub is the cloud gateway that connects IoT devices to gather data and drive business insights and automation. In addition, IoT Hub includes features that enrich the relationship between your devices and your backend systems. Bi-directional communication capabilities mean that while you receive data from devices you can also send commands and policies back to devices.<br>Note on why not Azure Event Hubs: An Azure IoT Hub contains an Event Hub and hence essentially is an Event Hub plus additional features. An important additional feature is that an Event Hub can only receive messages, whereas an IoT Hub additionally can also send messages to individual devices. Further, an<br>Event Hub has access security on hub level, whereas an IoT Hub is aware of the individual devices and can grand and revoke access on device level.<br>Box 2: Azure Hdinsight with Azure Machine Learning service<br>References:<br>https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-machine-learning-overview",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-09T09:39:00.000Z",
        "voteCount": 47,
        "content": "Second answer should be HDInsight with Apache Storm"
      },
      {
        "date": "2019-08-30T22:09:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2019-12-31T22:17:00.000Z",
        "voteCount": 5,
        "content": "Storm for real time analytic. https://www.edureka.co/blog/introduction-to-real-time-analytics-with-apache-storm/"
      },
      {
        "date": "2020-04-24T02:11:00.000Z",
        "voteCount": 6,
        "content": "Azure Machine Learning services include Spark and it is recommended as a good choice to perform Clickstream analysis. So the answer is correct, from my point of view. https://azure.microsoft.com/en-in/blog/streaming-analytics-use-cases-with-spark-on-azure/"
      },
      {
        "date": "2020-05-20T11:03:00.000Z",
        "voteCount": 1,
        "content": "I dont think so. I setted up an Machine Learning Enviroment and Azure ML is not including it from scratch, but there are scenarios in which you can use Spark with Azure. But in my understanding its not included."
      },
      {
        "date": "2020-05-29T00:05:00.000Z",
        "voteCount": 7,
        "content": "The question is talking about ML Services in HDInsight, not Azure Machine Learning as itself, which is different (https://docs.microsoft.com/it-it/azure/machine-learning/overview-what-is-azure-ml). ML Services in HDInsight include Spark compute context, just check out here:\nhttps://docs.microsoft.com/it-it/azure/hdinsight/r-server/r-server-overview\nhttps://docs.microsoft.com/it-it/azure/hdinsight/r-server/r-server-compute-contexts"
      },
      {
        "date": "2020-06-18T04:45:00.000Z",
        "voteCount": 2,
        "content": "Ah I see, thank you!"
      },
      {
        "date": "2021-02-24T17:43:00.000Z",
        "voteCount": 3,
        "content": "HDInsight with Storm is the answer: link here\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-streaming-at-scale-overview\nHDInsight with ML is more for gaining insights and analytics. Question here specifically saying process streaming data in REAL-TIME. Hope the answer gets corrected since it is the consensus here in the comments"
      },
      {
        "date": "2020-11-20T20:35:00.000Z",
        "voteCount": 1,
        "content": "IoT hub or Event hub?\nhttps://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-compare-event-hubs"
      },
      {
        "date": "2021-01-03T06:22:00.000Z",
        "voteCount": 7,
        "content": "IoT hub is correct because it is bidirectional"
      },
      {
        "date": "2020-09-19T06:17:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct. https://techcommunity.microsoft.com/t5/azure-global/azure-data-architecture-guide-blog-5-clickstream-analysis/ba-p/306630"
      },
      {
        "date": "2020-10-24T21:46:00.000Z",
        "voteCount": 4,
        "content": "no. machine learning service is not a stream processing service. here are the stream processing technologies    https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-09-11T06:36:00.000Z",
        "voteCount": 2,
        "content": "data ingestion should be event hubs. check this: https://techcommunity.microsoft.com/t5/azure-global/azure-data-architecture-guide-blog-5-clickstream-analysis/ba-p/306630"
      },
      {
        "date": "2020-09-23T01:11:00.000Z",
        "voteCount": 2,
        "content": "and stream processing is storm like mentioned above"
      },
      {
        "date": "2020-10-24T21:45:00.000Z",
        "voteCount": 4,
        "content": "here are the stream processing technologies supported by Azure. storm is one of them. machine learning service is NOT one of them.                         \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-11-05T14:27:00.000Z",
        "voteCount": 8,
        "content": "event hub doesn't allow bidirectional communication\nit's IoT Edge just because it does allow it"
      },
      {
        "date": "2019-08-26T18:12:00.000Z",
        "voteCount": 5,
        "content": "Thanks for posting your solution.  I agree totally."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3401-exam-ai-100-topic-2-question-43-discussion/",
    "body": "HOTSPOT -<br>You are designing an Azure infrastructure to support an Azure Machine Learning solution that will have multiple phases. The solution must meet the following requirements:<br>\u2711 Securely query an on-premises database once a week to update product lists.<br>\u2711 Access the data without using a gateway.<br>\u2711 Orchestrate the separate phases.<br>What should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0011000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "Incorrect Answer: The VPN connection solution both use gateways.",
    "answerDescription": "Box 1: Azure App Service Hybrid Connections<br>With Hybrid Connections, Azure websites and mobile services can access on-premises resources as if they were located on the same private network. Application admins thus have the flexibility to simply lift-and-shift specific most front-end tiers to Azure with minimal configuration changes, extending their enterprise apps for hybrid scenarios.<br>Box 2: Machine Learning pipelines<br>Typically when running machine learning algorithms, it involves a sequence of tasks including pre-processing, feature extraction, model fitting, and validation stages. For example, when classifying text documents might involve text segmentation and cleaning, extracting features, and training a classification model with cross-validation. Though there are many libraries we can use for each stage, connecting the dots is not as easy as it may look, especially with large-scale datasets. Most ML libraries are not designed for distributed computation or they do not provide native support for pipeline creation and tuning.<br><br>Box 3: Azure Databricks -<br>References:<br>https://azure.microsoft.com/is-is/blog/hybrid-connections-preview/ https://databricks.com/glossary/what-are-ml-pipelines",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-26T18:19:00.000Z",
        "voteCount": 8,
        "content": "If you agree that the second box is pipelines then the third box makes sense.  Look up the second URL give which points to the Databricks documentation for pipelines"
      },
      {
        "date": "2021-05-13T01:44:00.000Z",
        "voteCount": 2,
        "content": "The third answer should be azure automation. Control orchestrating using Azure Automation - which ties into an MLOps/DevOps/Continous Integration/Continous delivery approach."
      },
      {
        "date": "2020-03-22T09:08:00.000Z",
        "voteCount": 1,
        "content": "it should be automation, Azure Automation provides complete control during deployment,\noperations, and decommissioning of workloads and resources\n\nhttps://opdhsblobprod01.blob.core.windows.net/contents/4a6d75bb3af747de838e6ccc97c5d978/c04062585e59c060681bb6b6b0606102?sv=2015-04-05&amp;sr=b&amp;si=ReadPolicy&amp;sig=Nd4YY3g5M9bQ940ineEqNQcir0hS%2Bz8umM6bAoOlDso%3D&amp;st=2020-03-22T16%3A53%3A36Z&amp;se=2020-03-23T17%3A03%3A36Z"
      },
      {
        "date": "2021-02-09T18:39:00.000Z",
        "voteCount": 5,
        "content": "You need to understand \"orchestrate multiple phases\" of what! Read up what Azure Automation can do and you'll know what I mean. Agree with Bharat's comment.\nSummary: Databricks controls the orchestrations through ML Pipelines.\nGiven answer is correct.\nGiven answer is correct."
      },
      {
        "date": "2019-08-09T09:51:00.000Z",
        "voteCount": 2,
        "content": "Third box should be automation I believe"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13452-exam-ai-100-topic-2-question-44-discussion/",
    "body": "You plan to design a solution for an AI implementation that uses data from IoT devices.<br>You need to recommend a data storage solution for the IoT devices that meets the following requirements:<br>\u2711 Allow data to be queried in real-time as it streams into the solution.<br>\u2711 Provide the lowest amount of latency for loading data into the solution.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Azure Table Storage solution",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Azure HDInsight R Server cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Azure HDInsight Hadoop cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Azure SQL database that has In-Memory OLTP enabled"
    ],
    "answer": "C",
    "answerDescription": "You can use HDInsight to process streaming data that's received in real time from a variety of devices.<br>Internet of Things (IoT)<br>You can use HDInsight to build applications that extract critical insights from data. You can also use Azure Machine Learning on top of that to predict future trends for your business.<br>By combining enterprise-scale R analytics software with the power of Apache Hadoop and Apache Spark, Microsoft R Server for HDInsight gives you the scale and performance you need. Multi-threaded math libraries and transparent parallelization in R Server handle up to 1000x more data and up to 50x faster speeds than open-source R, which helps you to train more accurate models for better predictions.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-16T04:59:00.000Z",
        "voteCount": 14,
        "content": "The question ask for storage solution not processing solution, so we can use hadoop cluster as storage (HDFS) and spark for processing.\n https://www.edureka.co/blog/5-reasons-when-to-use-and-not-to-use-hadoop/"
      },
      {
        "date": "2021-06-28T12:01:00.000Z",
        "voteCount": 1,
        "content": "If you want to do some Real Time Analytics, where you are expecting result quickly, Hadoop should not be used directly. It is because Hadoop works on batch processing, hence response time is high.\nThis excerpt is from the same link."
      },
      {
        "date": "2020-09-23T01:50:00.000Z",
        "voteCount": 13,
        "content": "The answer is D\n\nIn-Memory OLTP increases number of transactions per second and reduces latency for transaction processing. Scenarios that benefit from In-Memory OLTP are: high-throughput transaction processing such as trading and gaming, data ingestion from events or IoT devices, caching, data load, and temporary table and table variable scenarios.\n\nLink for the above para: https://docs.microsoft.com/en-us/azure/azure-sql/in-memory-oltp-overview#overview"
      },
      {
        "date": "2020-12-14T04:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with this"
      },
      {
        "date": "2020-12-29T08:49:00.000Z",
        "voteCount": 2,
        "content": "Is OLTP considered \"real-time as it streams into the server\"? I thought it was real-time but you could only query it after it was streamed into the server specifically because it's transactional and the transaction has to finish before you can query it."
      },
      {
        "date": "2023-06-20T01:07:00.000Z",
        "voteCount": 1,
        "content": "D. a Microsoft Azure SQL database that has In-Memory OLTP enabled\n\nExplanation:\n\nA Microsoft Azure SQL database with In-Memory OLTP enabled would be the best choice for this scenario. In-Memory OLTP, also known as Hekaton, is an in-memory processing technology in Azure SQL Database that provides high-performance and low-latency access to data.\n\nBy utilizing In-Memory OLTP, the IoT data can be stored and queried in real-time as it streams into the solution. This technology enables the database to keep the entire dataset in memory, resulting in significantly reduced latency for loading and accessing the data."
      },
      {
        "date": "2021-08-23T06:56:00.000Z",
        "voteCount": 2,
        "content": "B Definitely"
      },
      {
        "date": "2020-10-18T03:04:00.000Z",
        "voteCount": 5,
        "content": "Answer is D a \"Microsoft Azure SQL database that has In-Memory OLTP enabled\"\nCommon application patterns are:\nIngesting sensor readings and events, and allow notifications as well as historical analysis.\nManaging batch updates, even from multiple sources, while minimizing the impact on the concurrent read workload.\n\nhttps://docs.microsoft.com/en-us/sql/relational-databases/in-memory-oltp/overview-and-usage-scenarios?view=sql-server-ver15#data-ingestion-including-iot-internet-of-things"
      },
      {
        "date": "2020-04-17T07:29:00.000Z",
        "voteCount": 8,
        "content": "Should be R Server:\n\nBy combining enterprise-scale R analytics software with the power of Apache Hadoop and Apache Spark, Microsoft R Server for HDInsight gives you the scale and performance you need. Multi-threaded math libraries and transparent parallelization in R Server handle up to 1000x more data and up to 50x faster speeds than open-source R, which helps you to train more accurate models for better predictions."
      },
      {
        "date": "2020-05-30T01:26:00.000Z",
        "voteCount": 1,
        "content": "Agree. RevoScaleR compute contexts (ex. Spark) enable faster processing than Hadoop, with the same storage\nhttps://docs.microsoft.com/it-it/azure/hdinsight/r-server/r-server-compute-contexts\nhttps://docs.microsoft.com/it-it/azure/hdinsight/r-server/r-server-storage"
      },
      {
        "date": "2020-09-12T13:53:00.000Z",
        "voteCount": 3,
        "content": "R server is not a storage solution. So why is this an answer?"
      },
      {
        "date": "2021-05-26T03:47:00.000Z",
        "voteCount": 1,
        "content": "You're not evaluating a processing solution, you are recommending a data storage solution. Why is this an answer?"
      },
      {
        "date": "2020-04-07T11:22:00.000Z",
        "voteCount": 2,
        "content": "Hadoop is more for batch processing, so I wouldn't recommend this solution."
      },
      {
        "date": "2020-04-07T11:21:00.000Z",
        "voteCount": 7,
        "content": "As we need to query data in real-time, I believe that the answer should be Microsoft Azure HDInsight R Server cluster."
      },
      {
        "date": "2020-02-05T08:10:00.000Z",
        "voteCount": 4,
        "content": "Shouldn't this be the R server?"
      },
      {
        "date": "2020-02-28T23:18:00.000Z",
        "voteCount": 14,
        "content": "justify your statement please, don't just throw options around"
      },
      {
        "date": "2021-01-15T21:05:00.000Z",
        "voteCount": 1,
        "content": "somewhere moderator is responsible for this . Why he approves such comments who dont have any justification."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3402-exam-ai-100-topic-2-question-45-discussion/",
    "body": "Your company has factories in 10 countries. Each factory contains several thousand IoT devices.<br>The devices present status and trending data on a dashboard.<br>You need to ingest the data from the IoT devices into a data warehouse.<br>Which two Microsoft Azure technologies should you use? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure HDInsight cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Batch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake"
    ],
    "answer": "CE",
    "answerDescription": "With Azure Data Lake Store (ADLS) serving as the hyper-scale storage layer and HDInsight serving as the Hadoop-based compute engine services. It can be used for prepping large amounts of data for insertion into a Data Warehouse<br>References:<br>https://www.blue-granite.com/blog/azure-data-lake-analytics-holds-a-unique-spot-in-the-modern-data-architecture",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-27T17:17:00.000Z",
        "voteCount": 19,
        "content": "I think it is Stream Analytics and Data Lake"
      },
      {
        "date": "2019-09-27T17:18:00.000Z",
        "voteCount": 5,
        "content": "Here is the supporting argument for that... https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs"
      },
      {
        "date": "2020-10-18T03:23:00.000Z",
        "voteCount": 8,
        "content": "The solution should ingest the data from the IoT devices into a data warehouse\n\u2022\tAzure Stream Analytics cannot ingest IoT Data on its own it need Iot Hub to do it .\n\u2022\tAzure HDInsight cluster with kafka will work.\n\u2022\tAzure data Factory will work if IoT devices are storing data local on prem data store and  ADF can pull data form on prem data store and ingest to a data warehouse\n\u2022\tAzure Batch cannot ingest IoT Data on its own.\n\u2022\tAzure Data Lake is a scalable data storage and analytics service need some help to ingest IoT Data."
      },
      {
        "date": "2021-02-09T19:10:00.000Z",
        "voteCount": 5,
        "content": "Good stuff. But here's the issue:\n1. Question says \"Each correct answer presents part of the solution\", while you are looking at each solution to independently solve the case.\n2. I don't understand what this entails: \"The devices present status and trending data on a dashboard\". Should the solution account for a tech that can output to a dashboard such as Power BI? Unclear.\n\nIn fact, \"ingesting data\" from IoT Devices is not possible with any of these. It needs IoT Hub, Event Hub etc. or Streaming services such as Kafka or Akka.\n\nLooks like this question was framed by a 6-yr old!"
      },
      {
        "date": "2023-06-20T01:11:00.000Z",
        "voteCount": 1,
        "content": "A. Azure Stream Analytics\nB. Azure Data Factory\n\nExplanation:\n\nA. Azure Stream Analytics:\nAzure Stream Analytics is a real-time analytics and event processing service in Azure. It is designed to handle streaming data from various sources, such as IoT devices, and process it in real-time. You can use Azure Stream Analytics to ingest data from the IoT devices as it streams in, perform real-time analytics, filtering, aggregation, and transformation, and then load the processed data into a data warehouse.\n\nB. Azure Data Factory:\nAzure Data Factory is a cloud-based data integration service that enables you to orchestrate and automate the movement and transformation of data across various data sources and destinations. With Azure Data Factory, you can create data pipelines to ingest data from different sources, including IoT devices, and then load it into your data warehouse. It provides"
      },
      {
        "date": "2021-03-08T22:30:00.000Z",
        "voteCount": 1,
        "content": "I think ans is correct.\n\nWith Azure Data Lake Store (ADLS) serving as the hyper-scale storage layer and HDInsight serving as the Hadoop-based compute engine services. It can be used for prepping large amounts of data for insertion into a Data Warehouse"
      },
      {
        "date": "2020-12-05T02:17:00.000Z",
        "voteCount": 1,
        "content": "It' A &amp; E as below.\nhttps://docs.microsoft.com/en-us/azure/architecture/reference-architectures/iot"
      },
      {
        "date": "2020-08-14T16:49:00.000Z",
        "voteCount": 3,
        "content": "should be A and E since the data trends gonna show on a dashboard but HDInsight can not output to powerbi. refer:https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-06-17T03:06:00.000Z",
        "voteCount": 2,
        "content": "I also believe the answer is Stream Analytics and Data Lake. https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs"
      },
      {
        "date": "2020-05-04T01:23:00.000Z",
        "voteCount": 1,
        "content": "i think this solution requires a lambda architecture\ndashboard to monitor devices mean we need real time processing i.e. Azure Stream Analytics, however not all data requires real time processing ,some data may need further processing, that's when we need Azure Data Lake to store data for further processing  or just initial batch processing"
      },
      {
        "date": "2020-04-21T03:18:00.000Z",
        "voteCount": 5,
        "content": "A and E"
      },
      {
        "date": "2020-03-22T13:15:00.000Z",
        "voteCount": 1,
        "content": "You can use Hive to output data to a variety of targets including:\nA relational database, such as SQL Server or AzureSQL Database.\nA data warehouse, such as AzureSQL Data Warehouse.\nso, also correct that Azure HDInsight is included"
      },
      {
        "date": "2019-08-09T10:06:00.000Z",
        "voteCount": 1,
        "content": "I believe it should be stream analytics and an HDInsights cluster."
      },
      {
        "date": "2019-09-01T02:17:00.000Z",
        "voteCount": 7,
        "content": "Stream Analytics would be the answer IF there was any pre-processing involved.  There is none that can be inferred from the Question statement, hence ADL might be more correct."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/45416-exam-ai-100-topic-2-question-46-discussion/",
    "body": "You plan to deploy a bot that will use the following Azure Cognitive Services:<br>\u2711 Language Understanding (LUIS)<br>\u2711 Text Analytics<br>Your company's compliance policy states that all data used by the bot must be stored in the on-premises network.<br>You need to recommend a compute solution to support the planned bot.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Databricks cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Docker container",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Machine Learning Server",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Azure Machine Learning service"
    ],
    "answer": "B",
    "answerDescription": "You can deploy LUIS on-premise as Docker Image in a container.<br>Note: Azure Cognitive LUIS service can be deployed on any hardware or on any host (Linus, Windows and IOS). This feature allows enterprises to quickly train the LUIS model on the cloud and deploy it anywhere which makes Cognitive services to be available truly to every person and every Organization -<br>\u05d2\u20acDemocratizing AI\u05d2\u20ac.<br>Reference:<br>https://www.linkedin.com/pulse/deploying-microsoft-azure-cognitive-luis-service-on-premise-s",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T01:12:00.000Z",
        "voteCount": 1,
        "content": "To comply with your company's policy of storing all data used by the bot in the on-premises network, the most suitable recommendation for a compute solution to support the planned bot would be:\n\nB. a Docker container\n\nExplanation:\n\nA Docker container provides a lightweight and portable runtime environment that can be deployed on-premises. By packaging the bot and its dependencies into a Docker container, you can ensure that the bot's compute resources are isolated and can be deployed and run within your on-premises network."
      },
      {
        "date": "2021-02-22T05:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct. \nFor More information https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-container-support"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56079-exam-ai-100-topic-2-question-47-discussion/",
    "body": "HOTSPOT -<br>Your company is building a cinema chatbot by using the Bot Framework and Language Understanding (LUIS).<br>You are designing of the intents and the entities for LUIS.<br>The following are utterances that customers might provide:<br>\u2711 Which movies are playing on December 8?<br>\u2711 What time is the performance of Movie1?<br>\u2711 I would like to purchase two adult tickets in the balcony section for Movie2.<br>You need to identify which entity types to use. The solution must minimize development effort.<br>Which entry type should you use for each entity? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0011500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0011600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Prebuilt -<br>Datetime is prebuilt.<br>Language Understanding (LUIS) provides prebuilt entities. When a prebuilt entity is included in your application, LUIS includes the corresponding entity prediction in the endpoint response.<br><br>Box 2: Simple -<br><br>Box 3: Composite -<br>A composite entity is made up of other entities, such as prebuilt entities, simple, regular expression, and list entities. The separate entities form a whole entity.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-reference-prebuilt-entities https://docs.microsoft.com/en-us/azure/cognitive-services/luis/reference-entity-composite",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-25T13:04:00.000Z",
        "voteCount": 2,
        "content": "Looks right.\nFirst uses in-built Date entity. Second is a list of movies and hence Simple entity. The last one consists of multiple entities (tickets, location), hence composite."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47769-exam-ai-100-topic-2-question-48-discussion/",
    "body": "Your company uses several bots. The bots use Azure Bot Service.<br>Several users report that some of the bots fail to return the expected results.<br>You plan to view the service health of the bot service.<br>You need to request the appropriate role to access the service health of the bot service. The solution must use the principle of least privilege.<br>Which role should you request?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Contributor role on the Azure subscription",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Reader role on the bot service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Owner role on the bot service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Reader role on the Azure subscription"
    ],
    "answer": "B",
    "answerDescription": "Use the Reader role on the bot service to limit access and scope.<br>Note: Access management for cloud resources is a critical function for any organization that is using the cloud. Azure role-based access control (Azure RBAC) helps you manage who has access to Azure resources, what they can do with those resources, and what areas they have access to.<br>Azure includes several built-in roles that you can use. The Reader Role can view existing Azure resources.<br>Scope is the set of resources that the access applies to. When you assign a role, you can further limit the actions allowed by defining a scope. In Azure, you can specify a scope at multiple levels: management group, subscription, resource group, or resource.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/role-based-access-control/overview",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T01:31:00.000Z",
        "voteCount": 1,
        "content": "To access the service health of the Azure Bot Service while following the principle of least privilege, you should request the following role:\n\nB. The Reader role on the bot service\n\nExplanation:\n\nThe Reader role on the bot service provides read-only access to the specific Azure Bot Service instance. This role allows you to view the service health and related information without the ability to make any changes or modifications to the service."
      },
      {
        "date": "2021-03-19T13:47:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/27906-exam-ai-100-topic-2-question-49-discussion/",
    "body": "You build an internal application that uses the Computer Vision API.<br>You need to ensure that only specific employees can access the application.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta single-service subscription key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tuser principals in Azure Active Directory (Azure AD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tservice principals in Azure Active Directory (Azure AD)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta multi-service subscription key"
    ],
    "answer": "C",
    "answerDescription": "The app requires an Azure service principal account to deploy services to your Azure subscription. A service principal lets you delegate specific permissions to an app using role-based access control.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/logo-detector-mobile",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-10T04:29:00.000Z",
        "voteCount": 12,
        "content": "Why wouldn't the answer be B? The question is asking about granting user access to the application. The Service Principal is used to deploy the application. I'm not granting access to the SP."
      },
      {
        "date": "2020-08-19T10:17:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer is B as well."
      },
      {
        "date": "2023-06-20T01:35:00.000Z",
        "voteCount": 1,
        "content": "To ensure that only specific employees can access the internal application that uses the Computer Vision API, you should include the following in the solution:\n\nB. User principals in Azure Active Directory (Azure AD)\n\nExplanation:\n\nUser principals in Azure Active Directory (Azure AD) provide a way to authenticate and authorize specific employees to access resources and applications. By configuring the internal application to use Azure AD authentication, you can enforce access control based on user identities."
      },
      {
        "date": "2021-05-26T04:32:00.000Z",
        "voteCount": 4,
        "content": "Service principles is CORRECT - \n\"An application that needs to deploy or configure resources through Azure Resource Manager must be represented by its own identity. Just as a user is represented by a security principal called a user principal, an app is represented by a service principal.\n\nThe service principal identity allows you to delegate only the necessary permissions to the app. For example, a configuration management app might use Azure Resource Manager to inventory Azure resources. In this scenario, you would register the app in your directory, grant the \"reader\" role to the app's service principal, and limit the configuration management app to read-only access.\"\n\nhttps://docs.microsoft.com/en-us/azure-stack/operator/azure-stack-create-service-principals?view=azs-2102&amp;tabs=az1%2Caz2&amp;pivots=state-connected"
      },
      {
        "date": "2021-05-15T19:00:00.000Z",
        "voteCount": 1,
        "content": "Well - I am not sure now. I selected the service principle and I think it should have been B"
      },
      {
        "date": "2021-02-17T04:36:00.000Z",
        "voteCount": 2,
        "content": "The service principal object defines what the app can actually do in the specific tenant, who can access the app, and what resources the app can access."
      },
      {
        "date": "2020-10-20T06:16:00.000Z",
        "voteCount": 3,
        "content": "The service principal object defines what the app can actually do in the specific tenant,    \"who can access the app\",     and what resources the app can access.\n\nhttps://docs.microsoft.com/en-us/azure/active-directory/develop/app-objects-and-service-principals#service-principal-object"
      },
      {
        "date": "2020-09-26T06:40:00.000Z",
        "voteCount": 3,
        "content": "I think  service principal is the correct answer, since it is an internal application, it needs to have  service principal created to register to Azure, then assign RBAC to the registered service, the answer link also shows that."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31321-exam-ai-100-topic-2-question-50-discussion/",
    "body": "You plan to deploy two AI applications named AI1 and AI2. The data for the applications will be stored in a relational database.<br>You need to ensure that the users of AI1 and AI2 can see only data in each user's respective geographic region. The solution must be enforced at the database level by using row-level security.<br>Which database solution should you use to store the application data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server on a Microsoft Azure virtual machine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Database for MySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Data Lake Store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Azure Cosmos DB"
    ],
    "answer": "A",
    "answerDescription": "Row-level security is supported by SQL Server, Azure SQL Database, and Azure SQL Data Warehouse.<br>References:<br>https://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-2017",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-09T19:21:00.000Z",
        "voteCount": 7,
        "content": "I think the answer being looked for is SQL on a VM because of the line solution must be enforced at the database level... which kinda warrants the view-workaround for MySQL too but hey.\n\nAnother poorly constructed question.\n\nAlso, how is this related to AI??? Like many others."
      },
      {
        "date": "2020-10-18T03:30:00.000Z",
        "voteCount": 5,
        "content": "Azure database for MySQL still doesn't support row-level security"
      },
      {
        "date": "2023-06-20T01:37:00.000Z",
        "voteCount": 1,
        "content": "To ensure that the users of AI1 and AI2 can see only data in their respective geographic regions and enforce row-level security at the database level, the most suitable database solution to store the application data would be:\n\nA. Microsoft SQL Server on a Microsoft Azure virtual machine\n\nExplanation:\n\nMicrosoft SQL Server on a Microsoft Azure virtual machine provides a fully managed relational database solution. It offers robust security features, including row-level security (RLS), which allows you to restrict data access at the row level based on specific conditions.\n\nWith SQL Server on Azure virtual machines, you can implement RLS to enforce data access restrictions based on the user's geographic region. By defining appropriate security predicates, you can ensure that users of AI1 and AI2 can only see the data that belongs to their respective regions."
      },
      {
        "date": "2020-09-15T02:26:00.000Z",
        "voteCount": 1,
        "content": "Tricky question since both sql on a vm and mysql support row-level security, but since the criteria is the region, azure mysql offers better integration with azure metadata that enable to distinguish the region, so I should go for MySql"
      },
      {
        "date": "2020-10-11T17:35:00.000Z",
        "voteCount": 2,
        "content": "It says in this article: \"Mysql doesn't natively support row level security on tables. However, you can sort of implement it with views. So, just create a view on your table that exposes only the rows you want a given client to see. Then, only provide that client access to those views, and not the underlying tables.\"https://stackoverflow.com/questions/5527129/mysql-how-to-do-row-level-security-like-oracles-virtual-private-database"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31333-exam-ai-100-topic-2-question-51-discussion/",
    "body": "You are designing an AI workflow that will aggregate data stored in Azure as JSON documents.<br>You expect to store more than 2 TB of new data daily.<br>You need to choose the data storage service for the data. The solution must minimize costs.<br>Which data storage service should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Manage Disks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure File Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage"
    ],
    "answer": "B",
    "answerDescription": "Generally, Data Lake will be a bit more expensive although they are in close range of each other. Blob storage has more options for pricing depending upon things like how frequently you need to access your data (cold vs hot storage). Data Lake is priced on volume, so it will go up as you reach certain tiers of volume.<br>References:<br>http://blog.pragmaticworks.com/azure-data-lake-vs-azure-blob-storage-in-data-warehousing",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-15T04:13:00.000Z",
        "voteCount": 9,
        "content": "Problem with blob is that blob container size is limited to 500 Tb whereas Data Lake is not, so given the ratio at which new data is appended, I think Data Lake is the right choice"
      },
      {
        "date": "2020-12-07T23:03:00.000Z",
        "voteCount": 3,
        "content": "Question mentions \u2018Minimise Cost\u2019 which can be addressed by Blob storage ."
      },
      {
        "date": "2023-06-20T01:39:00.000Z",
        "voteCount": 1,
        "content": "To store more than 2 TB of new data daily and minimize costs, the most suitable data storage service would be:\n\nD. Azure Data Lake Storage\n\nExplanation:\n\nAzure Data Lake Storage is designed to handle large volumes of data, making it a suitable choice for storing over 2 TB of new data daily. It provides scalable storage capacity and is optimized for big data workloads. Additionally, it supports storing and processing data in various formats, including JSON."
      },
      {
        "date": "2021-02-09T19:30:00.000Z",
        "voteCount": 2,
        "content": "Costs are the same for blob and data lake storage. I'll go with the JSON storage being better with Lake."
      },
      {
        "date": "2021-02-17T18:05:00.000Z",
        "voteCount": 2,
        "content": "Going back on my statement a little bit because although ADL is based on Blobs, there would be additional costs for ADL because it is more feature-rich than Blobs.\nActually, it is just wrong to give both Blob and Data Lake as options with just this much info in the question."
      },
      {
        "date": "2021-05-26T04:44:00.000Z",
        "voteCount": 1,
        "content": "That is why it's a trick question. Both ADL and blob are in there to confuse you. The key is in the requirement \"Minimise cost\". Since ADL builds off the blob concept, it will be cheaper than the data lake and so blob storage is the answer."
      },
      {
        "date": "2020-12-29T17:13:00.000Z",
        "voteCount": 1,
        "content": "wrong answer, this should be data lake"
      },
      {
        "date": "2021-06-06T02:59:00.000Z",
        "voteCount": 1,
        "content": "@aceking... kindly also justify your answer"
      },
      {
        "date": "2021-01-08T18:45:00.000Z",
        "voteCount": 1,
        "content": "For storing json data, azure data lake is the best choice. But, low-cost is given, so, it should be azure blob storage."
      },
      {
        "date": "2020-09-17T09:59:00.000Z",
        "voteCount": 1,
        "content": "Azure data lake benefit from the same storage tier as a blob (the article you are referring to is 2 years old)"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30302-exam-ai-100-topic-2-question-52-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are developing an application that uses an Azure Kubernetes Service (AKS) cluster.<br>You are troubleshooting a node issue.<br>You need to connect to an AKS node by using SSH.<br>Solution: You run the kubect1 command, and then you create an SSH connection.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-15T17:24:00.000Z",
        "voteCount": 5,
        "content": "Throughout the lifecycle of your Azure Kubernetes Service (AKS) cluster, you may need to access an AKS node. This access could be for maintenance, log collection, or other troubleshooting operations. You can access AKS nodes using SSH, including Windows Server nodes. You can also connect to Windows Server nodes using remote desktop protocol (RDP) connections. For security purposes, the AKS nodes aren't exposed to the internet. To SSH to the AKS nodes, you use the private IP address.\nDetails:https://docs.microsoft.com/en-us/azure/aks/ssh"
      },
      {
        "date": "2021-06-13T05:43:00.000Z",
        "voteCount": 3,
        "content": "The last sentence on your link says \"For security purposes, the AKS nodes aren't exposed to the internet. To SSH to the AKS nodes, you use kubectl debug or the private IP address.\" so maybe the answer to this question should be yes."
      },
      {
        "date": "2023-06-20T01:43:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nExplanation:\nThe solution does not meet the goal. Running the \"kubect1\" command (which seems to have a typo and should be \"kubectl\") does not directly establish an SSH connection to an AKS node. The \"kubectl\" command is used to interact with the Kubernetes cluster and manage its resources, but it does not provide a built-in way to establish an SSH connection to the nodes."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51042-exam-ai-100-topic-2-question-53-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an app named App1 that uses the Face API.<br>App1 contains several PersonGroup objects.<br>You discover that a PersonGroup object for an individual named Ben Smith cannot accept additional entries. The PersonGroup object for Ben Smith contains<br>10,000 entries.<br>You need to ensure that additional entries can be added to the PersonGroup object for Ben Smith. The solution must ensure that Ben Smith can be identified by all the entries.<br>Solution: You delete 1,000 entries from the PersonGroup object for Ben Smith.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T01:44:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Deleting 1,000 entries from the PersonGroup object for Ben Smith will not make room for additional entries because the limit for a PersonGroup in the Face API is 10,000 entries. Even if you delete some entries, the PersonGroup will still be at its maximum capacity."
      },
      {
        "date": "2021-05-17T23:38:00.000Z",
        "voteCount": 3,
        "content": "It is \"No\" because Person group can only hold up to 10000 persons. To hold more than this , we would need to create a LargePersonGroup, which holds up to 1000000 people.\nhttps://docs.microsoft.com/en-us/rest/api/faceapi/persongroup"
      },
      {
        "date": "2021-05-20T05:50:00.000Z",
        "voteCount": 2,
        "content": "It's possible to delete faces entries for a person.  However the question states \"The solution must ensure that Ben Smith can be identified by all the entries.\" This seems like you need to keep the others so moving to a LargePersonGroup is required to keep the existing 10k.  If you think it doesn't require keeping the 10k and just making space for new entries the answer is Yes.\n\nDELETE {Endpoint}/face/v1.0/persongroups/{personGroupId}/persons/{personId}/persistedfaces/{persistedFaceId}"
      },
      {
        "date": "2021-05-15T19:07:00.000Z",
        "voteCount": 1,
        "content": "I had this in the exam - I selected NO but this was confusing. If anyone can explain.  I will come back and check."
      },
      {
        "date": "2021-04-26T21:13:00.000Z",
        "voteCount": 1,
        "content": "I don't understand the answer, can anyone share your opinion?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50475-exam-ai-100-topic-2-question-54-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.<br>You need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.<br>Solution: You create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Using the Batch Execution Activity in an Azure Data Factory pipeline, you can invoke an Azure Machine Learning Studio (classic) web service to make predictions on the data in batch<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-19T08:55:00.000Z",
        "voteCount": 7,
        "content": "Question54 &amp; 58, same series, solution only 1 word diff, need to take care \n\nYou create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity\n\nYou create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning"
      },
      {
        "date": "2023-06-20T01:46:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Although Azure Data Factory (ADF) can be used for orchestrating data movement and transformation workflows, it does not directly support the execution of Azure Machine Learning Studio (classic) APIs through its built-in activities.\n\nTo ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account, you would need a different approach."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39022-exam-ai-100-topic-2-question-55-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.<br>You need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.<br>Solution: You create a scheduled Jupyter Notebook in Azure Databricks.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "We need to schedule the job in Azure Data Factory.",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T08:55:00.000Z",
        "voteCount": 5,
        "content": "The other two options were something along the lines of\n- You should use Azure Data Factory with a Jupyter Notebook.\n- Use Azure Data Factory with Machine Learning ML something."
      },
      {
        "date": "2023-06-20T01:48:00.000Z",
        "voteCount": 1,
        "content": "A. Yes\n\nExplanation:\n\nThe given solution meets the goal. By creating a scheduled Jupyter Notebook in Azure Databricks, you can implement the necessary steps to ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account.\n\nAzure Databricks provides a collaborative environment for developing and running Apache Spark-based analytics workloads. With a Jupyter Notebook, you can write Python or Scala code to interact with different data sources, including the Azure SQL database, and utilize the Azure Machine Learning Studio API for scoring."
      },
      {
        "date": "2021-05-17T23:46:00.000Z",
        "voteCount": 3,
        "content": "I Believe this is a yes, as we can create a scheduled notebook inside of the Databricks.\nAlso, I think that it is possible to make API calls inside of the notebook.\nhttps://docs.microsoft.com/en-us/azure/databricks/notebooks/notebooks-manage"
      },
      {
        "date": "2021-05-15T19:13:00.000Z",
        "voteCount": 1,
        "content": "I selected NO in the exam. But I have a feeling it should have been Yes."
      },
      {
        "date": "2021-02-22T05:00:00.000Z",
        "voteCount": 1,
        "content": "Why can't this be true?"
      },
      {
        "date": "2021-02-14T20:24:00.000Z",
        "voteCount": 3,
        "content": "Found the exact text for the other two options:\n- You create an Azure Data Factory pipeline that contains a Jupyter notebook activity.\n- You create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity.\n\nThe site that I found this in marks both as NO. If ADF is the right choice, then which one is it?"
      },
      {
        "date": "2021-02-14T20:18:00.000Z",
        "voteCount": 1,
        "content": "Agree ADF can do this. And thanks for the other options littleaznman. But wouldn't Databricks (SQL Analytics) be able to do this too? And which one of the ADF answers is right: Jupyter Notebook or the ML something option?"
      },
      {
        "date": "2021-01-28T19:05:00.000Z",
        "voteCount": 1,
        "content": "Yes, ADF is the best choice!"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/112953-exam-ai-100-topic-2-question-56-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.<br>You need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.<br>Solution: You create an Azure Data Factory pipeline that contains a Jupyter Notebook activity.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-22T08:19:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nThe solution mentioned does not meet the goal of ingesting data from the Azure SQL database, scoring each row using the API, and writing the data to the Azure Data Lake Storage Gen2 account."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/58740-exam-ai-100-topic-2-question-57-discussion/",
    "body": "HOTSPOT -<br>You are designing a Custom Vision Service solution to identify perishable products in grocery stores. The solution will be deployed as part of a mobile app.<br>You need to recommend the configurations for the Custom Vision API. The solution must minimize the size of the mobile app.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0012400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0012500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1:<br><br>Project type: Classification -<br>Box 2:<br>Domain: Food (compact)<br>If you want to classify photographs of individual fruits or vegetables, use the Food domain. Compact domains are optimized for the constraints of real-time classification on mobile devices.<br>Box 3:<br>Export capability: Vision AI Dev Kit<br>Custom Vision Service supports the following exports:<br>\u2711 Tensorflow for Android.<br>\u2711 CoreML for iOS11.<br>\u2711 ONNX for Windows ML.<br>\u2711 Vision AI Developer Kit.<br>\u2711 A Docker container for Windows, Linux, or ARM architecture.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/select-domain https://docs.microsoft.com/en-us/azure/cognitive-services/custom-vision-service/export-your-model",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-23T07:06:00.000Z",
        "voteCount": 1,
        "content": "box3 is basic platform"
      },
      {
        "date": "2021-07-26T19:03:00.000Z",
        "voteCount": 1,
        "content": "\"For Vision AI Dev Kit, the project must be created with the General (Compact) domain.\""
      },
      {
        "date": "2021-07-26T19:01:00.000Z",
        "voteCount": 1,
        "content": "\"perishable products in grocery stores\" means fruit?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50476-exam-ai-100-topic-2-question-58-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure SQL database, an Azure Data Lake Storage Gen 2 account, and an API developed by using Azure Machine Learning Studio.<br>You need to ingest data once daily from the database, score each row by using the API, and write the data to the storage account.<br>Solution: You create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "The Machine Learning Execute Pipeline activity enables batch prediction scenarios such as identifying possible loan defaults, determining sentiment, and analyzing customer behavior patterns.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T01:59:00.000Z",
        "voteCount": 1,
        "content": "B. No\n\nExplanation:\n\nThe given solution does not meet the goal. Azure Data Factory (ADF) does not provide a built-in activity called \"Machine Learning Execute Pipeline\" that can be used to directly invoke Azure Machine Learning Studio (classic) APIs.\n\nTo ingest data from the Azure SQL database, score each row using the Azure Machine Learning Studio API, and write the data to the Azure Data Lake Storage Gen2 account, you would need a different approach."
      },
      {
        "date": "2021-04-19T08:55:00.000Z",
        "voteCount": 4,
        "content": "Question54 &amp; 58, same series, solution only 1 word diff, need to take care \n\nYou create an Azure Data Factory pipeline that contains the Machine Learning Batch Execution activity\n\nYou create an Azure Data Factory pipeline that contains a Machine Learning Execute Pipeline activity\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-machine-learning"
      },
      {
        "date": "2021-05-18T02:35:00.000Z",
        "voteCount": 1,
        "content": "What should be the right question ? This is tricky...Batch Execution Activity can invoke studio (classic) and make preditions....In this case, we are not talking about the classic version( I would say).\nIn other hands Machine Learning Execute Pipeline activity enables batch prediction also, but I didn't found anything specifying \"prediction on Studio\"....\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service\nhttps://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-azure-ml-batch-execution-activity"
      },
      {
        "date": "2021-05-20T06:25:00.000Z",
        "voteCount": 1,
        "content": "This is a confusing one.  These both can be used but the batch execute is the older way (MLS Classic) and has a specific API exposed as a web service.  Whereas the execute pipeline is a reference to the experiment setup in the ADF UX.  IMO is seems like question 54 is a Yes, and question 58 is a No.  Thanks mingchieh2 for calling this out."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5288-exam-ai-100-topic-2-question-59-discussion/",
    "body": "Your company has a data team of Scala and R experts.<br>You plan to ingest data from multiple Apache Kafka streams.<br>You need to recommend a processing technology to broker messages at scale from Kafka streams to Azure Storage.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Apache Storm",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Microsoft Machine Learning Server"
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-streaming-at-scale-overview?toc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%<br>2Fhdinsight%2Fhadoop%2FTOC.json&amp;bc=https%3A%2F%2Fdocs.microsoft.com%2Fen-us%2Fazure%2Fbread%2Ftoc.json",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-17T00:15:00.000Z",
        "voteCount": 18,
        "content": "I think it is Databricks because of Scala and R support"
      },
      {
        "date": "2020-05-30T01:58:00.000Z",
        "voteCount": 3,
        "content": "Although it could be a valid option, I found that Storm supports multi-language:\nhttps://storm.apache.org/about/multi-language.html\n\nHard to say what's the best answer..."
      },
      {
        "date": "2021-01-13T22:38:00.000Z",
        "voteCount": 1,
        "content": "Scala and R support is also there with HDinsight .  Your logic does not holds."
      },
      {
        "date": "2021-07-20T05:37:00.000Z",
        "voteCount": 1,
        "content": "Not sure that HDinsight supports R (for Scala it does) : https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-overview"
      },
      {
        "date": "2020-05-04T02:40:00.000Z",
        "voteCount": 6,
        "content": "Storm supports C# &amp; Java\nDatabricks supports C#/F#, Java, Python, R, Scala\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2020-06-29T09:09:00.000Z",
        "voteCount": 3,
        "content": "Databricks, as Storm do not support either scala or R \nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview"
      },
      {
        "date": "2020-08-04T23:11:00.000Z",
        "voteCount": 2,
        "content": "Storm supports scala and other languages \nhttps://www.educba.com/apache-storm-vs-apache-spark/\nso HDInsight with Apache Storm is the correct answer"
      },
      {
        "date": "2023-06-20T08:15:00.000Z",
        "voteCount": 1,
        "content": "C. Azure HDInsight with Apache Storm\n\nExplanation:\nTo broker messages at scale from Kafka streams to Azure Storage, you can recommend using Azure HDInsight with Apache Storm.\n\nApache Storm is a distributed real-time stream processing system that can handle high-volume data streams. It provides fault-tolerant processing and can integrate well with Apache Kafka for data ingestion."
      },
      {
        "date": "2021-05-18T05:09:00.000Z",
        "voteCount": 1,
        "content": "For real-time Kafka data streams - HD Insight with Apache Storm is best. The key context here is 'streams'."
      },
      {
        "date": "2021-06-14T06:38:00.000Z",
        "voteCount": 1,
        "content": "Kafka as an input is not supported in Storm, but it is in Databricks. So I would say Databricks is the answer\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2021-06-14T06:40:00.000Z",
        "voteCount": 1,
        "content": "You can also see that in the Sinks or Outputs, Azure Storage is supported in Databricks while it's not in Storm"
      },
      {
        "date": "2021-05-18T05:07:00.000Z",
        "voteCount": 1,
        "content": "HD Insight with apache Strom is preferred - for processing real-time data streams."
      },
      {
        "date": "2021-02-01T14:44:00.000Z",
        "voteCount": 2,
        "content": "C is the correct Ans, because question does NOT mention utilization of your data team R and Scala expertise (believe they are meant to distract you).  Question is to suggest technology to \u201cbroker\u201d messages and Apache Storm is the right choice since it is meant for ingestion / streaming. DataBricks is an \u201cData Analytics\u201d platform requiring ingestion thru Data factory / Kafka / Event Hub, etc.. so DataBricks is incorrect"
      },
      {
        "date": "2021-02-10T11:59:00.000Z",
        "voteCount": 1,
        "content": "Disagree. There are just 3 lines in the question to make the distinction and you suggest we ignore the 1st line?\nAbout your 2nd comment on brokering messages, it means \"integration capabilities\". Having the capability to Input and/or Sink.\n\nBoth are Real-time stream processing techs to consume messages from queue etc.\n\nBoth Databricks (on Spark) and HDInsight with Storm can \"Sink\" Kafka while only Databricks has a direct input integration capability.\n\nHDInsight with Storm does not support Scala and R... I think this does need consideration.\n\nDatabricks is the answer IMO\n\nRead again in detail:\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2021-02-20T09:59:00.000Z",
        "voteCount": 2,
        "content": "Going back on my inference. Databricks won't work here as it is a Spark-based analytics solution that allows you to create big data pipelines. Moreover, it runs on Spark clusters and not Hadoop clusters.\nAnother reliable &amp; verified practice test states HDInsight as the solution. It discards ML Service but the option doesn't combine it with HDInsight.\nGiven this, I'll go with HDInsight + Storm"
      },
      {
        "date": "2021-01-13T22:35:00.000Z",
        "voteCount": 1,
        "content": "I think C and A both could be possible answers,, but since here big data is not mentioned, so we can eliminate A. So answer is correct : C"
      },
      {
        "date": "2020-10-18T03:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is Azure Databricks which supports R and Scala , which is not supported by Apache Storm"
      },
      {
        "date": "2020-10-12T04:02:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is A: Azure Databricks, according to this article from Microsoft: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing\nApache Spark in Azure Databricks: C#/F#, Java, Python, R, Scala\nAzure Functions:C#, F#, Java, Node.js, Python\nHDInsight with Storm: C#, Java"
      },
      {
        "date": "2020-09-23T06:26:00.000Z",
        "voteCount": 1,
        "content": "Apache Storm with Apache Kafka on HDInsight seems correct as per https://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-storm-with-kafka\nAnd yes Apache Spark supports all languages mentioned in question https://spark.apache.org/\n\nHowever Azure Databricks can also be a answer because of:\nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/kafka\nhttps://docs.databricks.com/spark/latest/sparkr/index.html\nhttps://docs.databricks.com/languages/scala.html\n\nSo not sure what to pick here"
      },
      {
        "date": "2020-09-13T10:04:00.000Z",
        "voteCount": 1,
        "content": "1. Databricks can interface with Kafka. It supports R and Scala\nhttps://docs.databricks.com/spark/latest/structured-streaming/kafka.html\nhttps://databricks.com/r-programming\n\n\n2. Storm and Kafka works together. HD insights also supports R and Scala\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-apache-storm-with-kafka\nCan't find any article which specifically says that storm supports both R and Scala.\n\n\n3. Ruling out Machine learning R server. This is primarily used if someone has need to do things on R\n\nSo my vote is for Databricks"
      },
      {
        "date": "2020-04-22T12:26:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/databricks/spark/latest/structured-streaming/kafka"
      },
      {
        "date": "2020-03-09T06:51:00.000Z",
        "voteCount": 1,
        "content": "Apache Spark in Azure Databricks\nhttps://docs.microsoft.com/en-us/azure/azure-databricks/what-is-azure-databricks"
      },
      {
        "date": "2020-01-07T16:40:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/hdinsight/kafka/apache-kafka-introduction"
      },
      {
        "date": "2020-01-06T03:12:00.000Z",
        "voteCount": 4,
        "content": "Option C,HDInsight with Apache Storm is the right answer as has been shown in the solution - correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3403-exam-ai-100-topic-2-question-60-discussion/",
    "body": "You are designing an AI application that will use an azure Machine Learning Studio experiment.<br>The source data contains more than 200 TB of relational tables. The experiment will run once a month.<br>You need to identify a data storage solution for the application. The solution must minimize compute costs.<br>Which data storage solution should you identify?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Database for MySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Data Warehouse"
    ],
    "answer": "A",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/mysql/overview",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-09T10:39:00.000Z",
        "voteCount": 25,
        "content": "I believe this should be data warehouse. Since the experiment will only run once a month, querying power is not needed. Warehouse is also able to store much more data and would minimize cost for 200TB"
      },
      {
        "date": "2019-08-26T18:52:00.000Z",
        "voteCount": 7,
        "content": "True.  Moreover the provided link shows that the upper limit for SQL Database is 4 TB"
      },
      {
        "date": "2020-04-07T12:57:00.000Z",
        "voteCount": 7,
        "content": "SQL database can go up to 100TB on hyper-scale, but it's still not sufficient. I agree with the data warehouse."
      },
      {
        "date": "2020-09-23T06:44:00.000Z",
        "voteCount": 2,
        "content": "SQL Database can go more than 200 TB. Check Gen5 here https://azure.microsoft.com/en-in/pricing/details/sql-database/single/"
      },
      {
        "date": "2023-06-20T08:18:00.000Z",
        "voteCount": 1,
        "content": "C. Azure SQL Data Warehouse\n\nExplanation:\nTo store more than 200 TB of relational tables and minimize compute costs for the AI application that will use Azure Machine Learning Studio experiment, the recommended data storage solution is Azure SQL Data Warehouse.\n\nAzure SQL Data Warehouse is a cloud-based, fully managed data warehouse service that provides high-performance analytics and scalability. It is designed to handle large volumes of data and allows you to scale compute resources independently of storage. This flexibility enables you to allocate the necessary compute resources for the monthly experiment while minimizing costs during idle periods."
      },
      {
        "date": "2021-08-23T07:07:00.000Z",
        "voteCount": 1,
        "content": "Difinetely warehouse"
      },
      {
        "date": "2021-06-28T20:48:00.000Z",
        "voteCount": 1,
        "content": "For all those in favor of 'C', please note that a Warehouse has low storage costs and high computational costs (OLAP).\nA database has high storage costs but comparatively lower computational costs. Here we must minimize compute costs."
      },
      {
        "date": "2021-01-31T14:59:00.000Z",
        "voteCount": 4,
        "content": "My pick would be C due to the following \n1) Storage limits\n2) Ability to pause DW computes when it is not used \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-compute-overview\nhttps://stackify.com/azure-sql-database-vs-warehouse/"
      },
      {
        "date": "2021-02-10T12:23:00.000Z",
        "voteCount": 1,
        "content": "Agree. Although I wouldn't choose DWH over SQL DB for the storage limits but for the pause feature to reduce costs. And also because of the Massive Parallel Processing which is required for an AI ML project such as this one.\nThis is also why the question is relevant in an AI exam because it is important to pick a DWH instead of a DB even if it doesn't detail requirements about having datamarts or slice &amp; dice of a DWH."
      },
      {
        "date": "2021-01-15T21:21:00.000Z",
        "voteCount": 3,
        "content": "Can someone tell me How this question is relevant to AI-100 exam,,, ??"
      },
      {
        "date": "2021-01-28T09:35:00.000Z",
        "voteCount": 5,
        "content": "Bruhhh..... Just learn it though. We'll ask questions after we have gotten the certificates"
      },
      {
        "date": "2021-05-26T05:11:00.000Z",
        "voteCount": 1,
        "content": "AI-100/AI-102 focusses on becoming an Azure AI architect. By nature of what it has to offer, there will be questions where it will focus on AI storage and retrieval and hence database options will be one of them."
      },
      {
        "date": "2020-10-29T15:58:00.000Z",
        "voteCount": 6,
        "content": "I think it's Azure SQL Data Warehouse, which is now called Azure Synapse. Gen 1 has max size of 240 TB with an ability to grow up to 1 PB with clustered columnstore compression. Plus, with the pause and scale feature, compute costs can be greatly reduced which is what we want here.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-best-practices"
      },
      {
        "date": "2020-09-16T22:37:00.000Z",
        "voteCount": 1,
        "content": "So it says the source data is 200TB. Are they not asking where to store the output data? SQL Database could be sufficient then?"
      },
      {
        "date": "2020-09-23T06:53:00.000Z",
        "voteCount": 1,
        "content": "The next thing they say is it should minimize compute costs. Now if you check the 40 core 204 GB option for sql database under gen 5 and the 32 core 320 GB option for \"database for mysql\" then it seems that \"database for mysql\" should be the answer"
      },
      {
        "date": "2020-09-23T06:53:00.000Z",
        "voteCount": 1,
        "content": "relevant links:\nhttps://azure.microsoft.com/en-in/pricing/details/sql-database/single/\nhttps://azure.microsoft.com/en-us/pricing/details/mysql/server/"
      },
      {
        "date": "2021-01-31T14:46:00.000Z",
        "voteCount": 1,
        "content": "But doesnt Azure DB for MySQL have a 16TB storage limit"
      },
      {
        "date": "2021-02-10T12:16:00.000Z",
        "voteCount": 2,
        "content": "Correct. I wouldn't choose MySQL here.\nhttps://docs.microsoft.com/en-us/azure/mysql/concepts-pricing-tiers"
      },
      {
        "date": "2020-10-10T06:10:00.000Z",
        "voteCount": 1,
        "content": "The answer could be \"A. Azure Database for MySQL\""
      },
      {
        "date": "2020-09-15T09:46:00.000Z",
        "voteCount": 1,
        "content": "I think SQL Data Warehouse is correct answer due to 200 TB size of data. It should be C."
      },
      {
        "date": "2020-09-23T06:42:00.000Z",
        "voteCount": 1,
        "content": "check Gen5 here. it can very well support it https://azure.microsoft.com/en-in/pricing/details/sql-database/single/"
      },
      {
        "date": "2020-08-02T21:06:00.000Z",
        "voteCount": 1,
        "content": "I just spent some time reading through several articles on this topic, and I'm certain that SQL DB cannot be the solution because of the amount of data is capped around 10 TB. It's cheaper initially but cannot support the premises of this question. The correct answer is Data Warehouse"
      },
      {
        "date": "2020-09-23T06:46:00.000Z",
        "voteCount": 1,
        "content": "Check this https://azure.microsoft.com/en-in/pricing/details/sql-database/single/  It can go more than 200 TB. See under Gen5"
      },
      {
        "date": "2020-04-21T03:20:00.000Z",
        "voteCount": 1,
        "content": "C is the right ans"
      },
      {
        "date": "2020-03-22T20:50:00.000Z",
        "voteCount": 2,
        "content": "Max instance storage size (reserved)\t- 2 TB for 4 vCores (Gen5 only)\n- 8 TB for other sizes\tGen4: 1 TB\nGen5:\n- 1 TB for 4, 8, 16 vCores\n- 2 TB for 24 vCores\n- 4 TB for 32, 40, 64, 80 vCores\nThese are the max instance for SQL database, the answer should be SQL data warehouse"
      },
      {
        "date": "2020-03-22T20:58:00.000Z",
        "voteCount": 3,
        "content": "https://stackify.com/azure-sql-database-vs-warehouse/\nAzure SQL Database\tAzure SQL Data Warehouse\nData type\tRelational\tRelational"
      },
      {
        "date": "2020-09-23T06:46:00.000Z",
        "voteCount": 1,
        "content": "No. See Gen5 tiers here for SQL Database here. https://azure.microsoft.com/en-in/pricing/details/sql-database/single/ It can go more than 200 TB"
      },
      {
        "date": "2020-01-07T16:51:00.000Z",
        "voteCount": 1,
        "content": "I believe keyword was 'relational', hence the Azure SQL DB response (and low cost)."
      },
      {
        "date": "2020-01-09T07:11:00.000Z",
        "voteCount": 4,
        "content": "Relational data type is relevant for both SQL DB and SQL Warehouse"
      },
      {
        "date": "2019-12-12T03:39:00.000Z",
        "voteCount": 4,
        "content": "Correct answer should be Azure SQL Data Warehouse"
      },
      {
        "date": "2019-12-31T22:39:00.000Z",
        "voteCount": 5,
        "content": "Only Azure SQL Data Warehouse can support 200TB DB https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-service-capacity-limits"
      },
      {
        "date": "2020-09-23T06:43:00.000Z",
        "voteCount": 1,
        "content": "why? Check Gen5 for sql database here. it can support more than 200TB https://azure.microsoft.com/en-in/pricing/details/sql-database/single/"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50017-exam-ai-100-topic-2-question-61-discussion/",
    "body": "You have an SAP production landscape.<br>You plan to use Azure Machine Learning to develop a fraud detection API. The API will identify potentially fraudulent transactions in the SAP production landscape in near real time.<br>You need to recommend a workflow for the API.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Factory pipeline that uses an SAP table connector and a Machine Learning Execute Pipeline activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Microsoft Excel workbook that imports SAP transactions and uses the Excel add-in for web services to score the transactions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure logic app triggered by an SAP message that calls the API and sends an email based on the results",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta scheduled Jupyter Notebook in Azure Databricks that connects to SAP HANA"
    ],
    "answer": "A",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-pipelines-activities https://docs.microsoft.com/en-us/azure/data-factory/transform-data-machine-learning-service",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-13T05:51:00.000Z",
        "voteCount": 6,
        "content": "https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-overview\nAzure Logic Apps is a cloud service that helps you schedule, automate, and orchestrate tasks, business processes, and workflows when you need to integrate apps, data, systems, and services across enterprises or organizations\n\nFrom here I would say that C is correct as \"near real-time\" in the question, and logic apps do have triggers for that. \nI am not sure that this \"near real-time\" condition can be achieved  with ADF."
      },
      {
        "date": "2023-06-20T08:21:00.000Z",
        "voteCount": 1,
        "content": "A. An Azure Data Factory pipeline that uses an SAP table connector and a Machine Learning Execute Pipeline activity.\n\nExplanation:\nTo develop a fraud detection API using Azure Machine Learning and identify potentially fraudulent transactions in the SAP production landscape in near real-time, the recommended workflow is to use an Azure Data Factory pipeline that utilizes an SAP table connector and a Machine Learning Execute Pipeline activity.\n\nAzure Data Factory is a cloud-based data integration service that allows you to create pipelines to orchestrate and automate data movement and data transformation. By using the SAP table connector, you can extract data from the SAP production landscape and pass it to the Machine Learning Execute Pipeline activity."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "2"
  }
]