[
  {
    "topic": 12,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14836-exam-az-203-topic-12-question-1-discussion/",
    "body": "You need to resolve the log capacity issue.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a LogCategoryFilter during startup.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Application Insights Telemetry Filter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the minimum log level in the host.json file for the function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Application Insights Sampling."
    ],
    "answer": "D",
    "answerDescription": "Scenario, the log capacity issue: Developers report that the number of log message in the trace output for the processor is too high, resulting in lost log messages.<br>Sampling is a feature in Azure Application Insights. It is the recommended way to reduce telemetry traffic and storage, while preserving a statistically correct analysis of application data. The filter selects items that are related, so that you can navigate between items when you are doing diagnostic investigations. When metric counts are presented to you in the portal, they are renormalized to take account of the sampling, to minimize any effect on the statistics.<br>Sampling reduces traffic and data costs, and helps you avoid throttling.<br>References:<br>https://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-17T09:22:00.000Z",
        "voteCount": 11,
        "content": "should be c in the requirements \"Application Insights must always contain all log\nmessages.\""
      },
      {
        "date": "2020-05-20T13:22:00.000Z",
        "voteCount": 3,
        "content": "I agreed the answer should be C, but with a different reason.\n\nOption B &amp; D are both wrong because sampling is only useful to reduce telemetry data (CPU %, # of requests, etc); the issue in this question however is about trace output (User created!  File uploaded!  Permission denied! etc), not telemetry."
      },
      {
        "date": "2020-12-19T11:38:00.000Z",
        "voteCount": 1,
        "content": "Capacity issue -\nDuring busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\n\nbusy hours and long delays doesn't impose CPU and therefore telemetry? \nim confused"
      },
      {
        "date": "2020-12-25T12:23:00.000Z",
        "voteCount": 1,
        "content": "This is not a capacity issue question but Logging. One of the requirements is that App Insights must always contain all log messages. The issue is lost log msgs.\nI'll go with Sampling."
      },
      {
        "date": "2020-08-21T16:17:00.000Z",
        "voteCount": 7,
        "content": "I think the answer is correct it is D:\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/sampling\nAdaptive sampling\nAdaptive sampling affects the volume of telemetry sent from your web server app to the Application Insights service endpoint."
      },
      {
        "date": "2021-01-12T07:23:00.000Z",
        "voteCount": 7,
        "content": "Alot of ppl just come here to confuse themselves darm it"
      },
      {
        "date": "2020-12-19T11:40:00.000Z",
        "voteCount": 1,
        "content": "I think it's correct \nhttps://docs.microsoft.com/en-us/azure/azure-monitor/learn/tutorial-performance\n\nFind and diagnose performance issues with Azure Application Insights\nIdentify the performance of server-side operations\nAnalyze server operations to determine the root cause of slow performance\nIdentify slowest client-side operations\nAnalyze details of page views using query language"
      },
      {
        "date": "2020-08-28T19:29:00.000Z",
        "voteCount": 2,
        "content": "public async static Task&lt;SqlDataReader&gt; ExecuteReaderWithRetryAsync(this SqlCommand command)\n{\n    GuardConnectionIsNotNull(command);\n\n    var policy = Policy.Handle&lt;Exception&gt;().WaitAndRetryAsync(\n        retryCount: 3, // Retry 3 times\n        sleepDurationProvider: attempt =&gt; TimeSpan.FromMilliseconds(200 * Math.Pow(2, attempt - 1)), // Exponential backoff based on an initial 200 ms delay.\n        onRetry: (exception, attempt) =&gt;\n        {\n            // Capture some information for logging/telemetry.\n            logger.LogWarn($\"ExecuteReaderWithRetryAsync: Retry {attempt} due to {exception}.\");\n        });\n\n    // Retry the following call according to the policy.\n    await policy.ExecuteAsync&lt;SqlDataReader&gt;(async token =&gt;\n    {\n        // This code is executed within the Policy\n\n        if (conn.State != System.Data.ConnectionState.Open) await conn.OpenAsync(token);\n        return await command.ExecuteReaderAsync(System.Data.CommandBehavior.Default, token);\n\n    }, cancellationToken);\n}"
      },
      {
        "date": "2020-02-25T00:43:00.000Z",
        "voteCount": 5,
        "content": "maybe change minumum log level?"
      }
    ],
    "examNameCode": "az-203",
    "topicNumber": "12"
  },
  {
    "topic": 12,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11870-exam-az-203-topic-12-question-2-discussion/",
    "body": "You need to resolve the capacity issue.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the Azure Function to a dedicated App Service Plan.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the trigger on the Azure Function to a File Trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the consumption plan is configured correctly to allow for scaling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the loop starting on line PC09 to process items in parallel."
    ],
    "answer": "D",
    "answerDescription": "If you want to read the files in parallel, you cannot use forEach. Each of the async callback function calls does return a promise. You can await the array of promises that you'll get with Promise.all.<br>Scenario: Capacity issue: During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.<br><img src=\"/assets/media/exam-media/02838/0018400001.png\" class=\"in-exam-image\"><br>References:<br>https://stackoverflow.com/questions/37576685/using-async-await-with-a-foreach-loop",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-11T01:13:00.000Z",
        "voteCount": 15,
        "content": "A scaling in this case brings nothing. This Function is called once in 5 min and get a huge number of files to be processed. \nIt\u2019 possible to convert the loop to call an parallel processing of files or just convert a function to be triggered from files. I think both B and D can be an answer."
      },
      {
        "date": "2020-08-05T21:09:00.000Z",
        "voteCount": 5,
        "content": "A is the answer. You can scale out and up with a dedicate App Service Plan.\n\"You need more CPU or memory options than what is provided on the Consumption plan.\"\nhttp://twocents.nl/?p=2078\n\nD is not the answer for this reason:\n\"... will depend on the number of cores the machine has, and you can't control that when you're deployed to the consumption plan.\"\nhttps://github.com/Azure/Azure-Functions/issues/815\n\nC is also not the answer because auto-scaling a default for consumption plans. You don't need to configuration anything, so it is already on for this scenario:\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#consumption-plan\n\nB is also not the answer, as there is not file share trigger for Azure Functions.\nhttps://stackoverflow.com/questions/50872265/is-there-any-trigger-for-azure-file-share-in-azure-functions-or-azure-logic-app"
      },
      {
        "date": "2022-01-23T10:57:00.000Z",
        "voteCount": 1,
        "content": "Cleared AZ-204 today, the question appeared, the option \"D\" was not there"
      },
      {
        "date": "2020-10-24T23:17:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. A is incorrect. TimerTrigger does not get faster when you change the plan."
      },
      {
        "date": "2021-01-17T14:12:00.000Z",
        "voteCount": 3,
        "content": "OK, so the correct one is D:\n- A (incorrect): TimerTrigger does not get faster when you change the plan.\n- C (incorrect): Current situation, so, same reason as before (A).\n- B (Incorrect): There is not file share trigger for Azure Functions"
      },
      {
        "date": "2020-10-11T23:36:00.000Z",
        "voteCount": 2,
        "content": "Read the explanation:\nIf you want to read the files in parallel, you cannot use forEach. \n\nThen read at stackoverflow:\nIt says you can not use foreach - you need to use for\n\nHence correct answer D - switch your loop from foreach to a for loop."
      },
      {
        "date": "2020-07-13T07:39:00.000Z",
        "voteCount": 2,
        "content": "It's C.\n\nBase on the fact \"During busy periods, employees report long delays between the time they upload the receipt and when it appears in the web application.\", the capacity issue doesn't come from the code algorithm, the root cause is underlying infrastructure of Azure function.\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-scale#hosting-plans-comparison"
      },
      {
        "date": "2020-05-17T02:43:00.000Z",
        "voteCount": 2,
        "content": "The only answer I am sure is not right is D. Why? Look into requirements:\nReceipt processing -\nConcurrent processing of a receipt must be prevented.\nDoes anyone know which answer is correct?"
      },
      {
        "date": "2020-06-18T20:19:00.000Z",
        "voteCount": 6,
        "content": "Concurrent != Parallel"
      },
      {
        "date": "2020-07-23T07:31:00.000Z",
        "voteCount": 4,
        "content": "I think that means 2 parallel processing on the same receipt."
      },
      {
        "date": "2020-05-02T18:05:00.000Z",
        "voteCount": 4,
        "content": "It doesn't look like there is azure fileshare triggers, there is triggers for blob storage\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-triggers-bindings\nhttps://stackoverflow.com/questions/50872265/is-there-any-trigger-for-azure-file-share-in-azure-functions-or-azure-logic-app\n\nAlso if azure fileshare trigger is supported and many files got dropped at once it would trigger many instances of the function which could have constraints.\n\nIt seems it would be easier to run the function once every x minutes and process whats there in parallel\n\nAlso this issue happens \"During busy periods\" so on non busy periods the user is fine to wait the 5 minutes that it could take for the timer to trigger to process the file"
      },
      {
        "date": "2020-04-03T01:54:00.000Z",
        "voteCount": 2,
        "content": "Can be rewritten to someting like:\n\npublic static Task Run([TimeTrigger(\"0 /5 * * * *\")] TimerInfo timer, ILogger log)\n{\n    var container = await GetCloudBlobContainer();\n    async Task ProcessFile(ListFileItem fileItem)\n    {\n        var file = new CloudFile(fileItem.StorageUri.PrimaryUri)\n        var ms = new MemoryStream();\n        await file.DownloadToStramAsync(ms)\n        var blob = container.GetBlockBlobReference(fileItem.Uri.ToString());\n        await blob.UploadFromStreamAsync(ms)\n    }\n    var fileItems = await ListFiles();\n    return Task.WhenAll(fileItems.AsParallel().Select(fi =&gt; ProcessFile(fi)));"
      },
      {
        "date": "2020-03-03T09:13:00.000Z",
        "voteCount": 3,
        "content": "Given answer D is Correct"
      },
      {
        "date": "2020-02-24T07:47:00.000Z",
        "voteCount": 2,
        "content": "C should be the correct solution since it allows scaling"
      },
      {
        "date": "2020-01-12T22:36:00.000Z",
        "voteCount": 2,
        "content": "How is this a solution, the stackoverflow link and the promise library are javascript solutions.."
      },
      {
        "date": "2020-02-09T03:02:00.000Z",
        "voteCount": 5,
        "content": "The given C# code is also async ... I think in real life switching to a dedicated app service plan with always on would be best"
      },
      {
        "date": "2020-07-23T07:38:00.000Z",
        "voteCount": 1,
        "content": "The C# version:\n1. https://docs.microsoft.com/en-us/dotnet/standard/parallel-programming/how-to-write-a-simple-parallel-foreach-loop\n2. https://stackoverflow.com/questions/15136542/parallel-foreach-with-asynchronous-lambda"
      }
    ],
    "examNameCode": "az-203",
    "topicNumber": "12"
  },
  {
    "topic": 12,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/8263-exam-az-203-topic-12-question-3-discussion/",
    "body": "You need to ensure receipt processing occurs correctly.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob metadata to prevent concurrency problems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob SnapshotTime to prevent concurrency problems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob leases to prevent concurrency problems.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse blob properties to prevent concurrency problems."
    ],
    "answer": "B",
    "answerDescription": "You can create a snapshot of a blob. A snapshot is a read-only version of a blob that's taken at a point in time. Once a snapshot has been created, it can be read, copied, or deleted, but not modified. Snapshots provide a way to back up a blob as it appears at a moment in time.<br>Scenario: Processing is performed by an Azure Function that uses version 2 of the Azure Function runtime. Once processing is completed, results are stored in<br>Azure Blob Storage and an Azure SQL database. Then, an email summary is sent to the user with a link to the processing report. The link to the report must remain valid if the email is forwarded to another user.<br>References:<br>https://docs.microsoft.com/en-us/rest/api/storageservices/creating-a-snapshot-of-a-blob",
    "votes": [],
    "comments": [
      {
        "date": "2019-11-15T04:37:00.000Z",
        "voteCount": 49,
        "content": "answer should be C"
      },
      {
        "date": "2022-05-27T13:54:00.000Z",
        "voteCount": 1,
        "content": "A point-in-time snapshot is a copy of a storage volume, file or database as they appeared at a given point in time and are used as method of data protection. In the event of a failure, users can restore their data from the most recent snapshot before the failure. Many point-in-time snapshots are read-only.\n\nSo C is not correct"
      },
      {
        "date": "2019-12-10T13:27:00.000Z",
        "voteCount": 15,
        "content": "I agree, answer should be C, using Lease"
      },
      {
        "date": "2020-08-05T21:21:00.000Z",
        "voteCount": 9,
        "content": "B is correct.\n\"The link to the report must remain valid if the email is forwarded to another user.\"\nWith a snapshot, it can never be updated. With a lease, someone could release the lease and modify the blob. Then the URL would return not return the original BLOB"
      },
      {
        "date": "2021-02-07T23:59:00.000Z",
        "voteCount": 3,
        "content": "Question is about \"processing\" the receipt. Email is only sent after the processing."
      },
      {
        "date": "2020-06-23T16:23:00.000Z",
        "voteCount": 4,
        "content": "\"C\" Leases should be answer\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-concurrency"
      },
      {
        "date": "2020-02-24T07:54:00.000Z",
        "voteCount": 5,
        "content": "should be lease"
      },
      {
        "date": "2020-02-23T12:14:00.000Z",
        "voteCount": 5,
        "content": "Agree , it is mentioned concurrent processing of receipt should be prevented"
      },
      {
        "date": "2019-11-15T04:36:00.000Z",
        "voteCount": 8,
        "content": "Concurrency can be achieved by either using Lease or using ETag(or last updated time) ."
      }
    ],
    "examNameCode": "az-203",
    "topicNumber": "12"
  }
]