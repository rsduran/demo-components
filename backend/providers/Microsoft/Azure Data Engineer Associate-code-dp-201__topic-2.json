[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/33411-exam-dp-201-topic-2-question-1-discussion/",
    "body": "You are designing an Azure Databricks interactive cluster. The cluster will be used infrequently and will be configured for auto-termination.<br>You need to ensure that the cluster configuration is retained indefinitely after the cluster is terminated. The solution must minimize costs.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClone the cluster after it is terminated.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTerminate the cluster manually when processing completes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure runbook that starts the cluster every 90 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPin the cluster."
    ],
    "answer": "D",
    "answerDescription": "To keep an interactive cluster configuration even after it has been terminated for more than 30 days, an administrator can pin a cluster to the cluster list.<br>Reference:<br>https://docs.azuredatabricks.net/clusters/clusters-manage.html#automatic-termination",
    "votes": [],
    "comments": [
      {
        "date": "2020-12-06T02:11:00.000Z",
        "voteCount": 12,
        "content": "This is same as Topic 1 Qn 12\nAnswer is D"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47938-exam-dp-201-topic-2-question-2-discussion/",
    "body": "HOTSPOT -<br>You are planning a design pattern based on the Kappa architecture as shown in the exhibit.<br><img src=\"/assets/media/exam-media/03774/0014500001.png\" class=\"in-exam-image\"><br>Which Azure service should you use for each layer? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0014600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0014700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Layer 1: Azure Data Factory -<br><br>Layer 2: Azure Databricks -<br>Azure Databricks is fully integrated with Azure Data Factory .<br><img src=\"/assets/media/exam-media/03774/0014800001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-24T02:42:00.000Z",
        "voteCount": 71,
        "content": "Speed layer: Cosmos DB, Long-term store: Synapse"
      },
      {
        "date": "2021-05-24T01:52:00.000Z",
        "voteCount": 5,
        "content": "Agree with this solution"
      },
      {
        "date": "2021-06-07T19:18:00.000Z",
        "voteCount": 4,
        "content": "Agree. I saw same question in another sample test .. Answer mention above is right."
      },
      {
        "date": "2021-05-20T11:46:00.000Z",
        "voteCount": 6,
        "content": "Speed layer: Cosmos , Long-term : Synapse"
      },
      {
        "date": "2021-08-12T21:51:00.000Z",
        "voteCount": 1,
        "content": "Cosmos DB + ASA fit appropriately for this question I think."
      },
      {
        "date": "2021-04-23T11:49:00.000Z",
        "voteCount": 1,
        "content": "The kappa architecture was proposed by Jay Kreps as an alternative to the lambda architecture. It has the same basic goals as the lambda architecture, but with an important distinction: All data flows through a single path, using a stream processing system ,Analytical data store\nAzure Synapse Analytics, Azure Data Explorer, HBase, Spark, or Hive. Processed real-time data can be stored in a relational database such Synapse Analytics, Azure Data Explorer, a NoSQL store such as HBase, or as files in distributed storage over which Spark or Hive tables can be defined and queried.\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing"
      },
      {
        "date": "2021-03-28T13:11:00.000Z",
        "voteCount": 1,
        "content": "speed layer should be Cosmos DB, based on following article. Long term is correct. Data stored in Parquets. \nhttps://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/"
      },
      {
        "date": "2021-03-26T23:45:00.000Z",
        "voteCount": 1,
        "content": "There must be a reason why you said that"
      },
      {
        "date": "2021-03-22T04:49:00.000Z",
        "voteCount": 3,
        "content": "i don't think this's the right answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48907-exam-dp-201-topic-2-question-3-discussion/",
    "body": "You need to design a telemetry data solution that supports the analysis of log files in real time.<br>Which two Azure services should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2"
    ],
    "answer": "AC",
    "answerDescription": "You connect a data ingestion system with Azure Databricks to stream data into an Apache Spark cluster in near real-time. You set up data ingestion system using<br>Azure Event Hubs and then connect it to Azure Databricks to process the messages coming through.<br>Note: Azure Event Hubs is a highly scalable data streaming platform and event ingestion service, capable of receiving and processing millions of events per second. Event Hubs can process and store events, data, or telemetry produced by distributed software and devices. Data sent to an event hub can be transformed and stored using any real-time analytics provider or batching/storage adapters.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-stream-from-eventhubs",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-06T01:03:00.000Z",
        "voteCount": 11,
        "content": "Correct answer."
      },
      {
        "date": "2021-05-24T01:56:00.000Z",
        "voteCount": 2,
        "content": "Propose solution is correct"
      },
      {
        "date": "2021-04-03T03:08:00.000Z",
        "voteCount": 1,
        "content": "A: for realtime analysis.\nB: for stream injection."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48909-exam-dp-201-topic-2-question-4-discussion/",
    "body": "You are planning a design pattern based on the Lambda architecture as shown in the exhibit.<br><img src=\"/assets/media/exam-media/03774/0014900001.png\" class=\"in-exam-image\"><br>Which Azure service should you use for the hot path?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics"
    ],
    "answer": "A",
    "answerDescription": "In Azure, all of the following data stores will meet the core requirements supporting real-time processing:<br>\u2711 Apache Spark in Azure Databricks<br>\u2711 Azure Stream Analytics<br>\u2711 HDInsight with Spark Streaming<br>\u2711 HDInsight with Storm<br>\u2711 Azure Functions<br>\u2711 Azure App Service WebJobs<br>Note: Lambda architectures use batch-processing, stream-processing, and a serving layer to minimize the latency involved in querying big data.<br><img src=\"/assets/media/exam-media/03774/0015000007.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T04:45:00.000Z",
        "voteCount": 3,
        "content": "Why not Azure Stream Analytics? A speed layer (hot path) analyzes data in real time. Databricks is for batch processing.\n\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/#lambda-architecture"
      },
      {
        "date": "2021-05-23T04:47:00.000Z",
        "voteCount": 4,
        "content": "Sorry, I see now that Azure Stream Analytics is not a proposed solution, so Databricks is the best option among others."
      },
      {
        "date": "2021-04-03T03:10:00.000Z",
        "voteCount": 1,
        "content": "Streaming vs Batching..."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48910-exam-dp-201-topic-2-question-5-discussion/",
    "body": "You are designing an audit strategy for an Azure SQL Database environment.<br>You need to recommend a solution to provide real-time notifications for potential security breaches. The solution must minimize development effort.<br>Which destination should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Log Analytics"
    ],
    "answer": "D",
    "answerDescription": "Auditing for Azure SQL Database and SQL Data Warehouse tracks database events and writes them to an audit log in your Azure storage account, Log Analytics workspace or Event Hubs.<br>Alerts in Azure Monitor can identify important information in your Log Analytics repository. They are created by alert rules that automatically run log searches at regular intervals, and if results of the log search match particular criteria, then an alert record is created and it can be configured to perform an automated response.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-database/sql-database-auditing https://docs.microsoft.com/en-us/azure/azure-monitor/learn/tutorial-response",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-03T03:21:00.000Z",
        "voteCount": 14,
        "content": "Not C: Solution must minimize development effort.\nD: Alerts in Azure Monitor can identify important information in your Log Analytics repository."
      },
      {
        "date": "2021-05-24T02:00:00.000Z",
        "voteCount": 2,
        "content": "Appropriate answer is azure log analytics as the it can be configured easily by sending the logs into the component"
      },
      {
        "date": "2021-04-06T01:41:00.000Z",
        "voteCount": 3,
        "content": "Destination should be Azure storage because it\u2019s the place where the logs are saved when you enable auditing."
      },
      {
        "date": "2021-04-06T02:01:00.000Z",
        "voteCount": 3,
        "content": "Sorry. D is correct. Bdloko is right."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46913-exam-dp-201-topic-2-question-6-discussion/",
    "body": "You need to design a real-time stream solution that uses Azure Functions to process data uploaded to Azure Blob Storage.<br>The solution must meet the following requirements:<br>Support up to 1 million blobs.<br><img src=\"/assets/media/exam-media/03774/0015100001.png\" class=\"in-exam-image\"><br>\u2711 Scaling must occur automatically.<br>\u2711 Costs must be minimized.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in an App Service plan and use a Blob trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in a Consumption plan and use an Event Grid trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in a Consumption plan and use a Blob trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in an App Service plan and use an Event Grid trigger."
    ],
    "answer": "C",
    "answerDescription": "Create a function, with the help of a blob trigger template, which is triggered when files are uploaded to or updated in Azure Blob storage.<br>You use a consumption plan, which is a hosting plan that defines how resources are allocated to your function app. In the default Consumption Plan, resources are added dynamically as required by your functions. In this serverless hosting, you only pay for the time your functions run. When you run in an App Service plan, you must manage the scaling of your function app.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-13T09:55:00.000Z",
        "voteCount": 33,
        "content": "The solution is B - Deploy the Azure Function in a Consumption plan and use an Event Grid trigger. 1M blobs will cripple the ability of blob trigger to provide the events.\nThe EventGrid trigger is instantaneous, so it depends on your needs."
      },
      {
        "date": "2021-06-20T11:45:00.000Z",
        "voteCount": 1,
        "content": "High-scale: High scale can be loosely defined as containers that have more than 100,000 blobs in them or storage accounts that have more than 100 blob updates per second."
      },
      {
        "date": "2021-03-15T05:43:00.000Z",
        "voteCount": 1,
        "content": "you're right\nIn addition, storage logs are created on a \"best effort\" basis. There's no guarantee that all events are captured. Under some conditions, logs may be missed.\n\nIf you require faster or more reliable blob processing, consider creating a queue message when you create the blob. Then use a queue trigger instead of a blob trigger to process the blob. Another option is to use Event Grid; see the tutorial Automate resizing uploaded images using Event Grid.\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp"
      },
      {
        "date": "2021-03-22T05:08:00.000Z",
        "voteCount": 2,
        "content": "Because it's a real time processing, it has to be appservice plan THE CORRECT ANSWER IS D"
      },
      {
        "date": "2021-03-22T23:28:00.000Z",
        "voteCount": 3,
        "content": "I would say it is B as because of thee High number of blobs that can land it has to be definitely an Event Grid Trigger (See https://docs.microsoft.com/es-es/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp#alternatives). Also, due to cost minimized, it should be a consumption plan. Even being a consumption plan the trigger delay will be minimum and fit to the conditions. However if you have a service plan you have to had the resources needed allocated even if you are using them or not."
      },
      {
        "date": "2021-04-20T11:08:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp#alternatives"
      },
      {
        "date": "2021-05-24T02:07:00.000Z",
        "voteCount": 2,
        "content": "The requirement is leading to the answer below as it was focusing to minimize cost.\nB. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger.\n\nReference: https://docs.microsoft.com/en-us/azure/azure-functions/functions-scale"
      },
      {
        "date": "2021-04-29T04:46:00.000Z",
        "voteCount": 3,
        "content": "B. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger."
      },
      {
        "date": "2021-03-28T04:36:00.000Z",
        "voteCount": 1,
        "content": "It doesn't say you have to minimize latency."
      },
      {
        "date": "2021-06-28T10:16:00.000Z",
        "voteCount": 3,
        "content": "real-time = minimize latency"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/24057-exam-dp-201-topic-2-question-7-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics cloud job using Azure PowerShell",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Microsoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "Stream Analytics is a cost-effective event processing engine that helps uncover real-time insights from devices, sensors, infrastructure, applications and data quickly and easily.<br>Monitor and manage Stream Analytics resources with Azure PowerShell cmdlets and powershell scripting that execute basic Stream Analytics tasks.<br>Note: Visual Studio 2019 and Visual Studio 2017 also support Stream Analytics Tools.<br>Reference:<br>https://cloudblogs.microsoft.com/sqlserver/2014/10/29/microsoft-adds-iot-streaming-analytics-data-production-and-workflow-services-to-azure/",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-25T11:44:00.000Z",
        "voteCount": 21,
        "content": "I agree with answer C. But shoudn't be Azure Stream Analytics edge job ?"
      },
      {
        "date": "2021-03-01T04:49:00.000Z",
        "voteCount": 4,
        "content": "yes, it should be this option"
      },
      {
        "date": "2020-10-24T00:55:00.000Z",
        "voteCount": 16,
        "content": "In the exam, the answer is \"Azure Stream Analytics with Azure Portal\", not PowerShell. That is the correct answer."
      },
      {
        "date": "2021-08-31T09:49:00.000Z",
        "voteCount": 1,
        "content": "on 31st August, it was Azure Stream Analytics Edge Application using Microsoft Visual Studio"
      },
      {
        "date": "2020-12-07T01:04:00.000Z",
        "voteCount": 4,
        "content": "It can only be stream analytics"
      },
      {
        "date": "2020-08-21T06:21:00.000Z",
        "voteCount": 6,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17771-exam-dp-201-topic-2-question-8-discussion/",
    "body": "HOTSPOT -<br>You plan to create a real-time monitoring app that alerts users when a device travels more than 200 meters away from a designated location.<br>You need to design an Azure Stream Analytics job to process the data for the planned app. The solution must minimize the amount of code developed and the number of technologies used.<br>What should you include in the Stream Analytics job? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0015400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0015500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Input type: Stream -<br>You can process real-time IoT data streams with Azure Stream Analytics.<br><br>Input source: Azure IoT Hub -<br>In a real-world scenario, you could have hundreds of these sensors generating events as a stream. Ideally, a gateway device would run code to push these events to Azure Event Hubs or Azure IoT Hubs.<br><br>Function: Geospatial -<br>With built-in geospatial functions, you can use Azure Stream Analytics to build applications for scenarios such as fleet management, ride sharing, connected cars, and asset tracking.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-get-started-with-azure-stream-analytics-to-process-data-from-iot-devices https://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-28T02:15:00.000Z",
        "voteCount": 60,
        "content": "Just finished my test, there is no second drop box for input source, only Input Type and Function. Maybe Event Hub and IoT hub both are correct now, so they removed the question."
      },
      {
        "date": "2020-05-15T05:52:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2020-04-16T10:34:00.000Z",
        "voteCount": 14,
        "content": "It says that the app needs to alert users"
      },
      {
        "date": "2021-04-07T08:06:00.000Z",
        "voteCount": 2,
        "content": "How about box 1? Is the input a stream or a reference?"
      },
      {
        "date": "2021-04-18T05:12:00.000Z",
        "voteCount": 3,
        "content": "stream"
      },
      {
        "date": "2020-08-22T02:00:00.000Z",
        "voteCount": 3,
        "content": "yes both are correct event hub and Iot Hub"
      },
      {
        "date": "2020-08-21T06:23:00.000Z",
        "voteCount": 3,
        "content": "correct answers"
      },
      {
        "date": "2020-06-25T12:02:00.000Z",
        "voteCount": 7,
        "content": "I thinks Answer = IoT, as per that link of Geospatial function\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/geospatial-scenarios"
      },
      {
        "date": "2021-05-24T02:21:00.000Z",
        "voteCount": 1,
        "content": "good reference"
      },
      {
        "date": "2020-06-21T04:52:00.000Z",
        "voteCount": 2,
        "content": "I guess it is IoT hub because IoT has the ability to integrate with Edge devices"
      },
      {
        "date": "2020-04-20T02:32:00.000Z",
        "voteCount": 2,
        "content": "I think bothe Event Hub and IoT hub work in this scenario."
      },
      {
        "date": "2020-04-01T10:58:00.000Z",
        "voteCount": 4,
        "content": "Why IoT Hub? I don't see a requirement for bi-directional..."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48067-exam-dp-201-topic-2-question-9-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using the Azure portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Edge application using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using the Azure portal"
    ],
    "answer": "C",
    "answerDescription": "Azure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-24T02:18:00.000Z",
        "voteCount": 8,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-edge-jobs"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46620-exam-dp-201-topic-2-question-10-discussion/",
    "body": "You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics.<br>You need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON"
    ],
    "answer": "A",
    "answerDescription": "The Avro format is great for data and message preservation.<br>Avro schema with its support for evolution is essential for making the data robust for streaming architectures like Kafka, and with the metadata that schema provides, you can reason on the data. Having a schema provides robustness in providing meta-data about the data stored in Avro records which are self- documenting the data.<br>References:<br>http://cloudurable.com/blog/avro/index.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-11T20:19:00.000Z",
        "voteCount": 52,
        "content": "I think this Answer is wrong since polybase does not support Avro.\nI will pick Parquet"
      },
      {
        "date": "2021-03-26T23:14:00.000Z",
        "voteCount": 15,
        "content": "I understand that Databricks and Polybase will consume the data independently ... So, based on that premise the selected output format from Synapse Stream Analytics should be a format compatible with both. Since, we need the file format to be a distributed file format for speed up the queries, the only possible solutions are AVRO and Parquet. As, AVRO is no a valid solution as Polybase doesn't support this format, the only possible answer is PARQUET"
      },
      {
        "date": "2021-11-18T06:14:00.000Z",
        "voteCount": 1,
        "content": "for me the correct answer is parquet"
      },
      {
        "date": "2021-06-07T22:00:00.000Z",
        "voteCount": 3,
        "content": "Parquet is correct answer I verify"
      },
      {
        "date": "2021-05-27T19:59:00.000Z",
        "voteCount": 2,
        "content": "Agreed with Parquet"
      },
      {
        "date": "2021-05-24T02:27:00.000Z",
        "voteCount": 2,
        "content": "Both services uses CSV and parquet as input files though parquet is the candidate for this requirement as it is the recommended file format for azure databricks and is also supported by polybase"
      },
      {
        "date": "2021-04-29T05:17:00.000Z",
        "voteCount": 3,
        "content": "C. Parquet"
      },
      {
        "date": "2021-04-06T01:35:00.000Z",
        "voteCount": 7,
        "content": "JSON and CSV don't define the types strongly and we need to preserve the data types, so those 2 are exuded.\nParquet is better optimized for read, avro is for write and requirement is to make queries fast, so parquet.\nhttps://www.datanami.com/2018/05/16/big-data-file-formats-demystified/"
      },
      {
        "date": "2021-03-24T02:29:00.000Z",
        "voteCount": 2,
        "content": "its Parquet file format"
      },
      {
        "date": "2021-03-23T15:26:00.000Z",
        "voteCount": 1,
        "content": "Polybase support requirement eliminates Avro. Not sure what the right answer is."
      },
      {
        "date": "2021-03-15T05:58:00.000Z",
        "voteCount": 1,
        "content": "avro is not supported by polybase, but why not CSV"
      },
      {
        "date": "2021-03-15T06:00:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs\nit's PARKET"
      },
      {
        "date": "2021-03-13T03:17:00.000Z",
        "voteCount": 1,
        "content": "I think Parquet is the right Answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16875-exam-dp-201-topic-2-question-11-discussion/",
    "body": "HOTSPOT -<br>The following code segment is used to create an Azure Databricks cluster.<br><img src=\"/assets/media/exam-media/03774/0015800001.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0015900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0016000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Yes -<br><br>Box 2: No -<br>autotermination_minutes: Automatically terminates the cluster after it is inactive for this time in minutes. If not set, this cluster will not be automatically terminated.<br>If specified, the threshold must be between 10 and 10000 minutes. You can also set this value to 0 to explicitly disable automatic termination.<br><br>Box 3: Yes -<br>References:<br>https://docs.databricks.com/dev-tools/api/latest/clusters.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-14T04:43:00.000Z",
        "voteCount": 30,
        "content": "1. Yes\nA cluster mode of \u2018High Concurrency\u2019 is selected, unlike all the others which are \u2018Standard\u2019. This results in a worker type of Standard_DS13_v2.\nref: https://adatis.co.uk/databricks-cluster-sizing/\n\n2. NO\nrecommended: New Job Cluster.\nWhen you run a job on a new cluster, the job is treated as a data engineering (job) workload subject to the job workload pricing. When you run a job on an existing cluster, the job is treated as a data analytics (all-purpose) workload subject to all-purpose workload pricing.\nref: https://docs.microsoft.com/en-us/azure/databricks/jobs\nScheduled batch workload- Launch new cluster via job\nref: https://docs.databricks.com/administration-guide/capacity-planning/cmbp.html#plan-capacity-and-control-cost\n\n3.YES\nDelta Lake on Databricks allows you to configure Delta Lake based on your workload patterns.\nref: https://docs.databricks.com/delta/index.html"
      },
      {
        "date": "2021-05-24T19:56:00.000Z",
        "voteCount": 4,
        "content": "This explanation is entirely correct. \n\nthe first item is referencing 'high concurrency' and one could check this while creating an interactive cluster. \n\nsecond item, a new job cluster should be created for job purposes as the existing all purpose cluster has different pricing. refer to the url provided at the bottom\n\nlastly, delta lake is configurable in the mentioned cluster version\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/jobs#cluster-config-tips"
      },
      {
        "date": "2021-05-24T19:57:00.000Z",
        "voteCount": 6,
        "content": "btw, first item hint is when you see 'serverless' it automatically indicates the 'high concurrency' cluster mode"
      },
      {
        "date": "2020-04-30T10:15:00.000Z",
        "voteCount": 17,
        "content": "My take on it:\nYes to multiple users - fits to support high concurrency since no scala support\nYes to efficiency - autostop and autoscale\nYes to the delta store - elastic disk (not 100% sure about that)"
      },
      {
        "date": "2020-10-11T08:44:00.000Z",
        "voteCount": 2,
        "content": "Auto termination is not configured for high concurrency clusters. so this cluster does not support high concurrency. So the answer should be\nNo\nYes\nNo\nrefer \nhttps://docs.databricks.com/clusters/clusters-manage.html#automatic-termination"
      },
      {
        "date": "2020-10-24T04:15:00.000Z",
        "voteCount": 5,
        "content": "Auto termination is not configured for high concurrency clusters BY DEFAULT, yet you can still enable and configure it."
      },
      {
        "date": "2021-01-19T01:12:00.000Z",
        "voteCount": 1,
        "content": "exactly"
      },
      {
        "date": "2021-04-29T08:08:00.000Z",
        "voteCount": 3,
        "content": "it seems serverless corresponds to \"high concurrency\" as per this blogpost - https://databricks.com/blog/2017/06/07/databricks-serverless-next-generation-resource-management-for-apache-spark.html"
      },
      {
        "date": "2021-01-16T12:47:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct. I am able to create a High Concurrency cluster as per given json config."
      },
      {
        "date": "2021-01-17T03:04:00.000Z",
        "voteCount": 2,
        "content": "Cluster Mode - High Concurrency\nDatabricks Runtime Version\n7.4 (includes Apache Spark 3.0.1, Scala 2.12)\nNewThis Runtime version supports only Python 3.\nAutopilot Options\n\nEnable autoscaling\n\nTerminate after \n120\n minutes of inactivity\nWorker Type\nStandard_DS13_v2\n56.0 GB Memory, 8 Cores, 2 DBU\nMin Workers\n2\nMax Workers\n8\nDriver Type\nStandard_DS13_v2\n56.0 GB Memory, 8 Cores, 2 DBU"
      },
      {
        "date": "2021-01-17T03:04:00.000Z",
        "voteCount": 1,
        "content": "{\n    \"autoscale\": {\n        \"min_workers\": 2,\n        \"max_workers\": 8\n    },\n    \"cluster_name\": \"cluster2\",\n    \"spark_version\": \"7.4.x-scala2.12\",\n    \"spark_conf\": {\n        \"spark.databricks.repl.allowedLanguages\": \"sql,python,r\",\n        \"spark.databricks.cluster.profile\": \"serverless\"\n    },\n    \"node_type_id\": \"Standard_DS13_v2\",\n    \"driver_node_type_id\": \"Standard_DS13_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {\n        \"ResourceClass\": \"Serverless\"\n    },\n    \"spark_env_vars\": {\n        \"PYSPARK_PYTHON\": \"/databricks/python3/bin/python3\"\n    },\n    \"autotermination_minutes\": 120,\n    \"enable_elastic_disk\": true,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"cluster_id\": \"0116-203628-tins636\"\n}"
      },
      {
        "date": "2021-02-03T10:58:00.000Z",
        "voteCount": 2,
        "content": "As per below link, High Concurrency clusters are configured to not terminate automatically. But while configuring High Concurrency, I am able to set the autotermination_minutes=120\n\nhttps://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2021-01-14T09:28:00.000Z",
        "voteCount": 4,
        "content": "1. YES \n2. NO (use job custer to reduce cost rather than high concurency)\n3. NO (we can use Delta lake starting from spark 2.4.2  based on scala 2.12.x. In this example the cluster definition is based on scala 2.11)"
      },
      {
        "date": "2020-12-09T02:54:00.000Z",
        "voteCount": 5,
        "content": "allowed languages are R SQL and Python -&gt; High concurrency cluster\nautoscaling is enabled as seen by min and max nodes -&gt; minimise cost definitely\nno CREATE TABLE syntax -&gt; no Delta Lake table\nYes Yes No"
      },
      {
        "date": "2020-11-12T11:15:00.000Z",
        "voteCount": 1,
        "content": "1. High Concurrency \"Yes\" because of following config:\n\"spark_conf\": {\n        \"spark.databricks.cluster.profile\": \"serverless\",\n        \"spark.databricks.repl.allowedLanguages\": \"sql,python,r\"\n    },\n2. minimise cost \"No\", because there is no auto scale config as below:\n    \"autoscale\": {\n        \"min_workers\": 2,\n        \"max_workers\": 8\n    },"
      },
      {
        "date": "2020-11-12T11:19:00.000Z",
        "voteCount": 1,
        "content": "sorry, ignore the second point."
      },
      {
        "date": "2020-08-08T04:44:00.000Z",
        "voteCount": 1,
        "content": "I think for part 2 of question \"NO\" is the right answer. Let's say we have three scheduled jobs with a difference of 180 minutes each that had to be run throughout the day. Since we have set the auto-termination to 90 minutes the cluster after executing the first schedule job remains active for 90 minutes so we'll have to pay for it. Which in turn doesn't minimize cost."
      },
      {
        "date": "2020-07-27T03:37:00.000Z",
        "voteCount": 2,
        "content": "Data Lakes Support All Data Types\nA data lake holds big data from many sources in a raw, granular format. It can store structured, semi-structured, or unstructured data, which means data can be kept in a more flexible format so we can transform it when we\u2019re ready to use . I stick with the default answer"
      },
      {
        "date": "2020-07-12T00:53:00.000Z",
        "voteCount": 3,
        "content": "I think Answer is\nYes\nNo\nNo\n\nThe given Configuration is for Interactive Cluster -(My Sample Interactive Cluster with Delta Enabled)\n{\n    \"autoscale\": {\n        \"min_workers\": 2,\n        \"max_workers\": 8\n    },\n    \"cluster_name\": \"dev_work\",\n    \"spark_version\": \"6.6.x-scala2.11\",\n    \"spark_conf\": {\n        \"spark.databricks.delta.preview.enabled\": \"true\"\n    },\n    \"node_type_id\": \"Standard_DS3_v2\",\n    \"driver_node_type_id\": \"Standard_DS3_v2\",\n    \"ssh_public_keys\": [],\n    \"custom_tags\": {},\n    \"spark_env_vars\": {},\n    \"autotermination_minutes\": 120,\n    \"enable_elastic_disk\": true,\n    \"cluster_source\": \"UI\",\n    \"init_scripts\": [],\n    \"cluster_id\": \"0529-111838-patch496\"\n}"
      },
      {
        "date": "2020-12-05T04:20:00.000Z",
        "voteCount": 1,
        "content": "But it says: The Databricks cluster supports the creation of a Delta Lake table.\nIt is a spark cluster and it \"supports\" if it is needed. So I would say Yes."
      },
      {
        "date": "2020-07-11T04:03:00.000Z",
        "voteCount": 4,
        "content": "High Concurrency does not support Auto termination; Auto-scaling minimizes the cost. So, No, Yes, Yes"
      },
      {
        "date": "2020-07-10T02:44:00.000Z",
        "voteCount": 1,
        "content": "First - True \nOptimized to run concurrent SQL, Phyton and R workloads\" Doesn't support Scala. Previously known as SERVERLESS"
      },
      {
        "date": "2020-06-25T13:26:00.000Z",
        "voteCount": 6,
        "content": "This link shows that standard for single user, so i think High concurrency clusters for concurrency : https://docs.microsoft.com/en-us/azure/databricks/clusters/configure\nStandard clusters\n---------------------------\nStandard clusters are recommended for a single user. Standard can run workloads developed in any language: Python, R, Scala, and SQL.\n1) No\n2) Yes  :autoscale enabled and auto-termination was decreased from 120 default to 90\n3) Yes"
      },
      {
        "date": "2020-10-30T01:06:00.000Z",
        "voteCount": 2,
        "content": "Standard_DS13_v2 is a High Concurrency Cluster Mode, if I select High Concurrency the Worker Type defaults to Standard_DS13_v2"
      },
      {
        "date": "2020-06-21T05:32:00.000Z",
        "voteCount": 2,
        "content": "Yes - Standard_DS13_V2 is cluster mode for High concurrency \nNo- It's an interactive cluster\nYes - I'm not sure, it seems like it is default setting when SQL API is chosen."
      },
      {
        "date": "2020-03-17T09:40:00.000Z",
        "voteCount": 2,
        "content": "In part 2 of the question, I have a confusion, in the datbricks config, the auto termination is set to 90 mins, and hence there is a provision of automatically getting the cluster down and minimizing cost. Had it been 0, it would to be auto termination disabled.\n\nAny thoughtS?"
      },
      {
        "date": "2020-03-19T02:33:00.000Z",
        "voteCount": 3,
        "content": "I think it talks about running a job on a job cluster instead of an interactive cluster. Not sure.."
      },
      {
        "date": "2020-04-20T11:22:00.000Z",
        "voteCount": 4,
        "content": "I think part 2 should be yes"
      },
      {
        "date": "2020-05-27T07:37:00.000Z",
        "voteCount": 21,
        "content": "To minimize  the cost, it shoud be set to the lower value = 10. Since it is set to 90, it means the cluster can run for nothing during the next 90 minutes after the last schedule job which is not cost-efficient so the answer \"NO\" is correct for this one.\nYES/NO/YES seams to be the correct answer."
      },
      {
        "date": "2020-12-30T04:10:00.000Z",
        "voteCount": 1,
        "content": "High Concurrency clusters are configured to not terminate automatically. https://docs.microsoft.com/en-us/azure/databricks/clusters/configure"
      },
      {
        "date": "2020-12-30T04:11:00.000Z",
        "voteCount": 1,
        "content": "ignore it. it's not set by default"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23499-exam-dp-201-topic-2-question-12-discussion/",
    "body": "HOTSPOT -<br>A company stores large datasets in Azure, including sales transactions and customer account information.<br>You must design a solution to analyze the data. You plan to create the following HDInsight clusters:<br><img src=\"/assets/media/exam-media/03774/0016100001.jpg\" class=\"in-exam-image\"><br>You need to ensure that the clusters support the query requirements.<br>Which cluster types should you recommend? To answer, select the appropriate configuration in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0016200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0016300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Interactive Query -<br>Choose Interactive Query cluster type to optimize for ad hoc, interactive queries.<br><br>Box 2: Hadoop -<br>Choose Apache Hadoop cluster type to optimize for Hive queries used as a batch process.<br>Note: In Azure HDInsight, there are several cluster types and technologies that can run Apache Hive queries. When you create your HDInsight cluster, choose the appropriate cluster type to help optimize performance for your workload needs.<br>For example, choose Interactive Query cluster type to optimize for ad hoc, interactive queries. Choose Apache Hadoop cluster type to optimize for Hive queries used as a batch process. Spark and HBase cluster types can also run Hive queries.<br>Reference:<br>https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-hadoop-optimize-hive-query?toc=%2Fko-kr%2Fazure%2Fhdinsight%2Finteractive-query%<br>2FTOC.json&amp;bc=%2Fbs-latn-ba%2Fazure%2Fbread%2Ftoc.json",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-23T06:38:00.000Z",
        "voteCount": 10,
        "content": "HDInsight is not covered in the exam any more."
      },
      {
        "date": "2020-06-19T04:19:00.000Z",
        "voteCount": 6,
        "content": "Cluster types in HDInsight\nhttps://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-overview#cluster-types-in-hdinsight"
      },
      {
        "date": "2021-03-22T05:18:00.000Z",
        "voteCount": 1,
        "content": "NOT ANY MORE IN THE DP-201"
      },
      {
        "date": "2021-02-27T04:14:00.000Z",
        "voteCount": 2,
        "content": "The given soln is correct\nInteractive qury - In-memory caching for interactive and faster Hive queries\nhadoop - A framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\nWe're receiving qns on this topic until dp-201 is removed"
      },
      {
        "date": "2020-12-19T01:24:00.000Z",
        "voteCount": 6,
        "content": "Answer is correct.\n\nSales: Interactive Queries\nIn-memory caching for interactive and faster Hive queries. \n\nAccounts: Hadoop\nA framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\n\nSource: https://docs.microsoft.com/bs-latn-ba/azure/hdinsight/hdinsight-overview#cluster-types-in-hdinsight"
      },
      {
        "date": "2020-12-09T01:23:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/39912-exam-dp-201-topic-2-question-13-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of<br>Contoso.<br>You need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to<br>Microsoft Power BI in near real-time.<br>Solution: You use Azure Data Factory and an event trigger to detect when new blobs are created. You use mapping data flows in Azure Data Factory to aggregate and filter the data, and then send the data to an Azure SQL database. You consume the data in Power BI by using DirectQuery mode.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-24T20:03:00.000Z",
        "voteCount": 11,
        "content": "Answer is NO. The propose solution to utilize ADF doesn't have real-time capability. Instead use of Azure Stream Analytics as it can count the data with the use of window function and output the data directly to the PowerBI dataset."
      },
      {
        "date": "2021-03-07T09:24:00.000Z",
        "voteCount": 8,
        "content": "scenario: You need to count how many times .. appear in the same post every 30 seconds. \nSolution: You use Azure Data Factory and an event trigger \nAnswer: No - you don't need an event trigger - you need a schedule trigger each 30 seconds"
      },
      {
        "date": "2021-05-01T05:40:00.000Z",
        "voteCount": 4,
        "content": "Answer: No - you don't need an event trigger - you need a schedule trigger each 30 seconds"
      },
      {
        "date": "2021-04-06T05:21:00.000Z",
        "voteCount": 4,
        "content": "Mapping data flow needs few minutes to start up spark cluster, it's good for batch ETL, but not suitable for real time stream processing."
      },
      {
        "date": "2021-04-29T21:57:00.000Z",
        "voteCount": 1,
        "content": "reply from maciejt seems only logical to me for \"No\" to this question i.e. since mapping dataflow needs few mins to spark up the cluster."
      },
      {
        "date": "2021-01-14T14:42:00.000Z",
        "voteCount": 1,
        "content": "correction: in my previous post, forget the 30-word distance. But the \"break a post into words\" remains the reason why the proposed solution does not fully meet the requirements IMHO"
      },
      {
        "date": "2021-01-14T14:40:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct in my opinion. The proposed approach is missing a step where you break out the social media post into words and investigate the 30-word \"distance\"."
      },
      {
        "date": "2021-01-02T11:56:00.000Z",
        "voteCount": 5,
        "content": "ADF is not a suitable solution for realtime feeds.When Streaming analytics can do the job directly no need of triggers to identify new blobs.So the answer looks correct to me."
      },
      {
        "date": "2020-12-15T06:34:00.000Z",
        "voteCount": 2,
        "content": "Why ? There should be an explanation. Solution provided seems reasonable"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48071-exam-dp-201-topic-2-question-14-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of<br>Contoso.<br>You need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to<br>Microsoft Power BI in near real-time.<br>Solution: You create an Azure Stream Analytics job that uses an input from Event Hubs to count the posts that have the specified keywords, and then send the data to an Azure SQL database. You consume the data in Power BI by using DirectQuery mode.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/power-bi/service-real-time-streaming https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-12T03:03:00.000Z",
        "voteCount": 1,
        "content": "Unless Power BI connects to SQL DB thru Import mode and schedule a automatic refresh, Data can never be available in near real-time."
      },
      {
        "date": "2021-05-28T19:23:00.000Z",
        "voteCount": 1,
        "content": "What I think is the proposed solution is correct, as we can generate the report directly through power BI by connecting it to ASA but not through direct query mode, I am not able to find the source as ASA while referring to DirectQuery. Refer the link: https://docs.microsoft.com/en-us/power-bi/connect-data/power-bi-data-sources\n\nSo we have to use sql db if we use directquery mode. Thanks."
      },
      {
        "date": "2021-05-24T20:10:00.000Z",
        "voteCount": 1,
        "content": "The propose solution is feasible as the data can be stored in Azure SQL DB then use of direct mode from Power BI retrieves the latest data while 'import' connectivity mode requires schedule to refresh the dataset. And as mentioned in the requirement, it states 'near real time' unless it is explicitly label as 'real time' then Azure Stream Analytics is the most suited solution."
      },
      {
        "date": "2021-03-24T02:46:00.000Z",
        "voteCount": 3,
        "content": "You can add Power BI as an output within Azure Stream Analytics (ASA), and then visualize those data streams in the Power BI service in real time\n\nhttps://docs.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming#pushing-data-to-datasets"
      },
      {
        "date": "2021-03-24T02:43:00.000Z",
        "voteCount": 3,
        "content": "Answer should be No why we output data from Stream to SQL db we can direct output to power BI"
      },
      {
        "date": "2021-04-09T20:47:00.000Z",
        "voteCount": 3,
        "content": "but they have not mentioned real-time, it is near real-time so it should be year"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/35815-exam-dp-201-topic-2-question-15-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of<br>Contoso.<br>You need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to<br>Microsoft Power BI in near real-time.<br>Solution: You use Azure Databricks to create a Scala notebook. You use a Structured Streaming job to connect to the event hub that counts the posts that have the specified keywords, and then writes the data to a Delta table. You consume the data in Power BI by using DirectQuery mode.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-02T13:23:00.000Z",
        "voteCount": 14,
        "content": "this question is outdated. this should be perfectly possible."
      },
      {
        "date": "2021-05-24T21:56:00.000Z",
        "voteCount": 3,
        "content": "The propose solution is feasible as the PowerBI can integrate to Azure Databricks. Unless the requirement changes to 'real time' then Azure Stream Analytics is suited service.\n\nReference: https://azure.microsoft.com/nl-nl/blog/structured-streaming-with-databricks-into-power-bi-cosmos-db/"
      },
      {
        "date": "2021-01-14T14:46:00.000Z",
        "voteCount": 2,
        "content": "https://databricks.com/blog/2020/10/30/announcing-azure-databricks-power-bi-connector-public-preview.html"
      },
      {
        "date": "2021-01-18T05:21:00.000Z",
        "voteCount": 2,
        "content": "So, the answer is Yes, the solution meets the goal."
      },
      {
        "date": "2021-01-18T05:42:00.000Z",
        "voteCount": 5,
        "content": "I mean, it is possible, but not near real-time. The answer is No."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47189-exam-dp-201-topic-2-question-16-discussion/",
    "body": "You are planning a design pattern based on the Lambda architecture as shown in the exhibit.<br><img src=\"/assets/media/exam-media/03774/0016600001.png\" class=\"in-exam-image\"><br>Which Azure service should you use for the hot path?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Catalog"
    ],
    "answer": "C",
    "answerDescription": "In Azure, all of the following data stores will meet the core requirements supporting real-time processing:<br>\u2711 Apache Spark in Azure Databricks<br>\u2711 Azure Stream Analytics<br>\u2711 HDInsight with Spark Streaming<br>\u2711 HDInsight with Storm<br>\u2711 Azure Functions<br>\u2711 Azure App Service WebJobs<br>Note: Lambda architectures use batch-processing, stream-processing, and a serving layer to minimize the latency involved in querying big data.<br><img src=\"/assets/media/exam-media/03774/0016800001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing https://docs.microsoft.com/en-us/azure/cosmos-db/lambda-architecture",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-15T07:09:00.000Z",
        "voteCount": 8,
        "content": "https://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/"
      },
      {
        "date": "2021-04-06T05:56:00.000Z",
        "voteCount": 1,
        "content": "Why not synapse? It also can use spark databricks notebooks..."
      },
      {
        "date": "2021-05-13T21:21:00.000Z",
        "voteCount": 8,
        "content": "synapse is cold. it's basicaly a warehouse"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47835-exam-dp-201-topic-2-question-17-discussion/",
    "body": "You are designing an enterprise data warehouse in Azure Synapse Analytics. You plan to load millions of rows of data into the data warehouse each day.<br>You must ensure that staging tables are optimized for data loading.<br>You need to design the staging tables.<br>What type of tables should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRound-robin distributed table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHash-distributed table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplicated table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExternal table"
    ],
    "answer": "A",
    "answerDescription": "To achieve the fastest loading speed for moving data into a data warehouse table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option.<br>Incorrect:<br>Not B: Consider that loading is usually a two-step process in which you first load to a staging table and then insert the data into a production data warehouse table. If the production table uses a hash distribution, the total time to load and insert might be faster if you define the staging table with the hash distribution.<br>Loading to the staging table takes longer, but the second step of inserting the rows to the production table does not incur data movement across the distributions.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/guidance-for-loading-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-20T16:04:00.000Z",
        "voteCount": 8,
        "content": "correct"
      },
      {
        "date": "2021-05-22T10:54:00.000Z",
        "voteCount": 3,
        "content": "keyword: staging table"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47112-exam-dp-201-topic-2-question-18-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have streaming data that is received by Azure Event Hubs and stored in Azure Blob storage. The data contains social media posts that relate to a keyword of<br>Contoso.<br>You need to count how many times the Contoso keyword and a keyword of Litware appear in the same post every 30 seconds. The data must be available to<br>Microsoft Power BI in near real-time.<br>Solution: You create an Azure Stream Analytics job that uses an input from Event Hubs to count the posts that have the specified keywords, and then send the data directly to Power BI.<br>Does the solution meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "DirectQuery mode is required for automatic page refresh.<br>Reference:<br>https://docs.microsoft.com/en-us/power-bi/service-real-time-streaming https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-14T15:23:00.000Z",
        "voteCount": 32,
        "content": "Should be yes, Stream analytics can directly send the output to Power BI\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard"
      },
      {
        "date": "2021-04-30T15:43:00.000Z",
        "voteCount": 2,
        "content": "Link is for real-time, questions says \"The data must be available to Microsoft Power BI in near real-time.\" So is, B-No"
      },
      {
        "date": "2021-05-22T02:38:00.000Z",
        "voteCount": 1,
        "content": "MS documentations confirms this point:\n\nThe analysis of the streaming data is performed with Stream Analytics itself. A Stream Analytics job is created that contains the Event Hubs as input streams, with a query that performs stream processing of the two event hubs to correlate the records in the two data streams. An output is defined to Cosmos DB that stores the correlated results written as JSON documents to a Cosmos DB document database.\n\nPower BI then uses Cosmos DB as a source for a dashboard of the correlated records. You could also have Power BI point \ndirectly from Stream Analytics; however, this data would not be persisted in a data store."
      },
      {
        "date": "2021-05-24T23:15:00.000Z",
        "voteCount": 2,
        "content": "The link already stated that the setup is feasible in 'REAL TIME'"
      },
      {
        "date": "2023-12-26T06:41:00.000Z",
        "voteCount": 1,
        "content": "chat gpt says \nBased on the information provided, it seems like the solution has the potential to meet the goal, but there are some details that need clarification and adjustments. Therefore, I would say it's a \"Yes, with caveats.\" Ensure that you address the specific details mentioned in the evaluation to make the solution more robust and aligned with the goal of counting keyword occurrences and sending the data to Power BI in near real-time."
      },
      {
        "date": "2021-09-12T06:20:00.000Z",
        "voteCount": 1,
        "content": "the question says \"The data must be available to Microsoft Power BI in near real-time\", but ASA +Power BI is real-time combination as per: https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard\n\nAnd, there is difference between real time and near real time. So, I feel the answer is 'NO'\ncan someone pls confirm?"
      },
      {
        "date": "2021-06-09T19:44:00.000Z",
        "voteCount": 2,
        "content": "I think the key phrase is \"...then send the data directly to Power BI\". Power BI is NOT a data store and does not \"accept\" data, it goes and gets it. Therefore, I believe the answer to be NO."
      },
      {
        "date": "2021-06-07T19:52:00.000Z",
        "voteCount": 2,
        "content": "You have to use power Bi in direct query mode"
      },
      {
        "date": "2021-06-06T07:14:00.000Z",
        "voteCount": 2,
        "content": "Should be YES. Another doc showing it is possible - BI can only query every 1 second - but that is near real time:\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/power-bi-output"
      },
      {
        "date": "2021-03-24T02:49:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/power-bi/connect-data/service-real-time-streaming#pushing-data-to-datasets\nAnswer is Yes"
      },
      {
        "date": "2021-03-16T22:56:00.000Z",
        "voteCount": 2,
        "content": "You create an Azure Stream Analytics job that uses an input from Event Hubs to count the posts that have the specified keywords, and then send the data to an Azure SQL database. You consume the data in Power BI by using DirectQuery mode."
      },
      {
        "date": "2021-03-16T22:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is No."
      },
      {
        "date": "2021-03-23T03:04:00.000Z",
        "voteCount": 7,
        "content": "You can aggregate and send directly to Power BI with Azure stream analytics if you want to achieve near-real time reporting. Using a intermediary SQL Database will be a waste of cost &amp; management and won't allow you to achieve near real-time reporting. The Answer is YES."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29213-exam-dp-201-topic-2-question-19-discussion/",
    "body": "A company has an application that uses Azure SQL Database as the data store.<br>The application experiences a large increase in activity during the last month of each year.<br>You need to manually scale the Azure SQL Database instance to account for the increase in data write operations.<br>Which scaling method should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale up by using elastic pools to distribute resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale out by sharding the data across databases.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale up by increasing the database throughput units."
    ],
    "answer": "C",
    "answerDescription": "As of now, the cost of running an Azure SQL database instance is based on the number of Database Throughput Units (DTUs) allocated for the database. When determining the number of units to allocate for the solution, a major contributing factor is to identify what processing power is needed to handle the volume of expected requests.<br>Running the statement to upgrade/downgrade your database takes a matter of seconds.<br>Incorrect Answers:<br>A: Elastic pools is used if there are two or more databases.<br>Reference:<br>https://www.skylinetechnologies.com/Blog/Skyline-Blog/August_2017/dynamically-scale-azure-sql-database",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-21T04:46:00.000Z",
        "voteCount": 16,
        "content": "correct answer"
      },
      {
        "date": "2021-02-27T04:42:00.000Z",
        "voteCount": 1,
        "content": "When we say increase it' s Scale up  by increasing the DTUs of Azure SQL DB"
      },
      {
        "date": "2020-12-19T01:26:00.000Z",
        "voteCount": 2,
        "content": "C. Scale up by increasing the database throughput units."
      },
      {
        "date": "2020-12-06T00:06:00.000Z",
        "voteCount": 2,
        "content": "Single database so C is the answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28837-exam-dp-201-topic-2-question-20-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "If you need to transform data in a way that is not supported by Data Factory, you can create a custom activity with your own data processing logic and use the activity in the pipeline. You can create a custom activity to run R scripts on your HDInsight cluster with R installed.<br>Reference:<br>https://docs.microsoft.com/en-US/azure/data-factory/transform-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-17T05:25:00.000Z",
        "voteCount": 16,
        "content": "The proposed solution seems to let the R function do the loading into Synapse. The answer then should be 'no', but more likely the description seems again to be incomplete."
      },
      {
        "date": "2021-03-23T06:44:00.000Z",
        "voteCount": 2,
        "content": "I agree with you. The insert operation into the DWH should be come after the R script process."
      },
      {
        "date": "2021-05-24T23:32:00.000Z",
        "voteCount": 5,
        "content": "Agree with the statement and as of this year, the Azure Synapse doesn't support R language unless it is executed against Azure SQL Manage Instance"
      },
      {
        "date": "2021-06-14T10:26:00.000Z",
        "voteCount": 2,
        "content": "Looks like executing R is possible(sp_execute_external_script), please review the link.\nhttps://docs.microsoft.com/en-us/sql/relational-databases/system-stored-procedures/sp-execute-external-script-transact-sql?view=sql-server-ver15"
      },
      {
        "date": "2021-06-14T10:29:00.000Z",
        "voteCount": 1,
        "content": "My bad it only applies to SQL Server 2016 &amp; Azure SQL Managed Instance, Moderator please dont post this."
      },
      {
        "date": "2021-06-27T01:36:00.000Z",
        "voteCount": 1,
        "content": "there is no moderator human"
      },
      {
        "date": "2021-05-09T03:26:00.000Z",
        "voteCount": 1,
        "content": "https://www.mssqltips.com/sqlservertip/6622/stored-procedure-in-sql-server-with-r-code/\nThe R function can be used inside stored procedure activity, so answer makes sense to me."
      },
      {
        "date": "2021-05-30T06:50:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-US/azure/data-factory/transform-data#custom-activity"
      },
      {
        "date": "2021-06-05T15:50:00.000Z",
        "voteCount": 1,
        "content": "yes R function can be used within the Stored procedure for SQL Server or Azure SQL Managed instance , however the statement states that the data is loaded using SQL Synapse which does not support R at this time"
      },
      {
        "date": "2021-03-16T23:10:00.000Z",
        "voteCount": 1,
        "content": "The answer should be No. Because MS always supply options for users so it won't engage in one specific programming language such as R."
      },
      {
        "date": "2021-03-16T03:45:00.000Z",
        "voteCount": 1,
        "content": "I am also not cleared with this answer"
      },
      {
        "date": "2021-01-09T20:47:00.000Z",
        "voteCount": 1,
        "content": "I am still not clear as to what is the correct answer"
      },
      {
        "date": "2020-12-08T08:19:00.000Z",
        "voteCount": 1,
        "content": "Solution proposed is on data pipeline and orchestration so I would say yes"
      },
      {
        "date": "2020-09-20T22:42:00.000Z",
        "voteCount": 2,
        "content": "Should use a tumbling window trigger in ADF for incremental loading.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"
      },
      {
        "date": "2020-11-05T16:30:00.000Z",
        "voteCount": 1,
        "content": "You can do incremental load using schedule trigger. Does not have to be Tumbling window"
      },
      {
        "date": "2020-08-22T19:20:00.000Z",
        "voteCount": 2,
        "content": "can we run a stored procedure to execute the R script ??  I don't think so."
      },
      {
        "date": "2020-12-17T14:04:00.000Z",
        "voteCount": 1,
        "content": "possible"
      },
      {
        "date": "2020-08-22T19:25:00.000Z",
        "voteCount": 6,
        "content": "https://docs.microsoft.com/en-us/sql/machine-learning/tutorials/quickstart-r-create-script?view=sql-server-ver15\ni believe this answers the question . Answer should be 'yes'"
      },
      {
        "date": "2020-11-10T08:09:00.000Z",
        "voteCount": 4,
        "content": "That is for sql server and managed instances and the question is about Azure Synapse Analytics"
      },
      {
        "date": "2020-08-17T05:48:00.000Z",
        "voteCount": 2,
        "content": "The explanation of the answer contains R on an HDInsight-Cluster. This kind of solution is stated to be incorrect in another questions explanation - in favor of an Azure function."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/30870-exam-dp-201-topic-2-question-21-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You schedule an Azure Databricks job that executes an R notebook, and then inserts the data into the data warehouse.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "You should use an Azure Data Factory, not an Azure Databricks job.<br>Reference:<br>https://docs.microsoft.com/en-US/azure/data-factory/transform-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-08T08:09:00.000Z",
        "voteCount": 20,
        "content": "But I can do with this too!"
      },
      {
        "date": "2021-01-04T04:48:00.000Z",
        "voteCount": 3,
        "content": "it is possible, but first you need to ingest data from staging source"
      },
      {
        "date": "2021-02-17T13:16:00.000Z",
        "voteCount": 4,
        "content": "with a mount in DBFS, we can ingest data from ADLS"
      },
      {
        "date": "2020-11-02T13:48:00.000Z",
        "voteCount": 17,
        "content": "answer should be yes."
      },
      {
        "date": "2021-05-25T00:00:00.000Z",
        "voteCount": 2,
        "content": "This requirement is possible with the use R script in Azure Databricks job. Therefore, answer should be 'Yes'"
      },
      {
        "date": "2021-01-14T15:09:00.000Z",
        "voteCount": 4,
        "content": "A scheduled daily Databricks job does the trick. Data Factory isn't the only tool that can bring data from one place to another... Answer should be yes."
      },
      {
        "date": "2020-12-05T14:24:00.000Z",
        "voteCount": 3,
        "content": "Who is gonna stop me from using Databricks. There seems to be no technical limitation in this approach"
      },
      {
        "date": "2020-11-08T22:44:00.000Z",
        "voteCount": 9,
        "content": "I think it should be YES.\n\nhttps://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/28839-exam-dp-201-topic-2-question-22-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Data Lake Storage account that contains a staging zone.<br>You need to design a daily process to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics.<br>Solution: You use an Azure Data Factory schedule trigger to execute a pipeline that executes an Azure Databricks notebook, and then inserts the data into the data warehouse.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "Use a stored procedure, not an Azure Databricks notebook to invoke the R script.<br>Reference:<br>https://docs.microsoft.com/en-US/azure/data-factory/transform-data",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-17T06:00:00.000Z",
        "voteCount": 27,
        "content": "This should be the correct answer."
      },
      {
        "date": "2021-01-04T04:49:00.000Z",
        "voteCount": 1,
        "content": "first step is to ingest data.."
      },
      {
        "date": "2021-04-06T05:30:00.000Z",
        "voteCount": 2,
        "content": "I think notebooks are only interactive. It should be a job cluster. Any opinions?"
      },
      {
        "date": "2021-08-18T11:31:00.000Z",
        "voteCount": 4,
        "content": "Now your comment is ambiguous. Do you mean correct answer provided in that case 'NO' is answer or the Solution provided is correct and it will do the Job, in this case 'Yes' will be the answer..."
      },
      {
        "date": "2023-05-25T06:34:00.000Z",
        "voteCount": 1,
        "content": "Yes, this solution meets the goal. You can use an Azure Data Factory schedule trigger to execute a pipeline that copies the data to a staging table in the data warehouse, and then uses a stored procedure to execute the R script. This will allow you to ingest incremental data from the staging zone, transform the data by executing an R script, and then insert the transformed data into a data warehouse in Azure Synapse Analytics on a daily basis."
      },
      {
        "date": "2021-09-12T06:44:00.000Z",
        "voteCount": 1,
        "content": "The answer should be NO because:\n1. we can't assume that the Azure Databricks notebook will execute/run the transform R script, it is not mentioned that Azure Databricks notebook will run the R script\n2. for incremental loads in ADF, I think a tumbling trigger should be used.\ncan someone pls confirm?"
      },
      {
        "date": "2021-06-06T07:35:00.000Z",
        "voteCount": 3,
        "content": "Answer should be YES:  ADF can trigger a Databricks notebook (not required to be user-driven):\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-databricks-notebook"
      },
      {
        "date": "2021-05-25T00:19:00.000Z",
        "voteCount": 1,
        "content": "The answer is Yes. R script is executed in the azure databricks notebook and once the transformation is completed then the mount the Azure Synapse to load the data.\n\nReference: https://docs.microsoft.com/en-us/azure/databricks/scenarios/databricks-extract-load-sql-data-warehouse"
      },
      {
        "date": "2020-09-20T22:43:00.000Z",
        "voteCount": 2,
        "content": "Should use a tumbling window trigger in ADF for incremental loading.\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/solution-template-copy-new-files-lastmodifieddate"
      },
      {
        "date": "2020-12-10T13:37:00.000Z",
        "voteCount": 1,
        "content": "Don't have to, can just use a regular schedule no problem."
      },
      {
        "date": "2020-09-08T08:17:00.000Z",
        "voteCount": 1,
        "content": "I'm surprised as I ran R in Azure Databrick"
      },
      {
        "date": "2020-08-23T10:56:00.000Z",
        "voteCount": 3,
        "content": "The solution template mentioned by Bob123456 does not fit, as --- per description --- the R script is to be run when the data is still located in the data lake. After the R based transformation, the result is to be loaded to the DWH. This type of processing would need polybase for accessing the data lake, which is not mentioned here."
      },
      {
        "date": "2020-10-01T15:05:00.000Z",
        "voteCount": 2,
        "content": "Databricks notebook can use mount to access data lake. Notebook is correct answer"
      },
      {
        "date": "2020-08-22T19:24:00.000Z",
        "voteCount": 1,
        "content": "this is incorrect \nhttps://docs.microsoft.com/en-us/sql/machine-learning/tutorials/quickstart-r-create-script?view=sql-server-ver15"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/56163-exam-dp-201-topic-2-question-23-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an Azure IoT Hub to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Edge application using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Microsoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "Azure Stream Analytics (ASA) on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data.<br>You can use Visual Studio plugin to create an ASA Edge job.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-13T02:05:00.000Z",
        "voteCount": 1,
        "content": "It's a duplicate Question"
      },
      {
        "date": "2021-06-27T04:40:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/36509-exam-dp-201-topic-2-question-24-discussion/",
    "body": "A company has a real-time data analysis solution that is hosted on Microsoft Azure. The solution uses Azure Event Hub to ingest data and an Azure Stream<br>Analytics cloud job to analyze the data. The cloud job is configured to use 120 Streaming Units (SU).<br>You need to optimize performance for the Azure Stream Analytics job.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement event ordering",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale the SU count for the job up",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Azure Stream Analytics user-defined functions (UDF)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale the SU count for the job down",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement query parallelization by partitioning the data output",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement query parallelization by partitioning the data output"
    ],
    "answer": "BF",
    "answerDescription": "Scale out the query by allowing the system to process each input partition separately.<br>F: A Stream Analytics job definition includes inputs, a query, and output. Inputs are where the job reads the data stream from.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-08T14:19:00.000Z",
        "voteCount": 25,
        "content": "Answer is correct. F should specify \"Input\""
      },
      {
        "date": "2021-03-24T03:50:00.000Z",
        "voteCount": 2,
        "content": "yep Input need to be partitioned not output"
      },
      {
        "date": "2020-11-10T08:22:00.000Z",
        "voteCount": 6,
        "content": "I have seen this question and answer before but I don't think it is correct as it specifically mentions optimize performance.\n\nIn Microsoft's documentation, it specifies partitioning input and output to leverage parallelization, so I think E and F should be the answer.\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"
      },
      {
        "date": "2021-06-27T01:43:00.000Z",
        "voteCount": 1,
        "content": "\"E and F should be the answer\"?  What is the different between E and F when it is copy pasted?"
      },
      {
        "date": "2021-04-18T06:42:00.000Z",
        "voteCount": 1,
        "content": "Isn't the input already partitioned?"
      },
      {
        "date": "2021-06-26T20:45:00.000Z",
        "voteCount": 2,
        "content": "This question came in my DP 200 exam\nI gave on 24th June 2021"
      },
      {
        "date": "2021-06-03T01:01:00.000Z",
        "voteCount": 5,
        "content": "The correct answer: B &amp; F \n\nIn F: you can change the latest word ( output &gt;&gt;&gt;&gt; input )! \nA. Implement event ordering\nB. Scale the SU count for the job up\nC. Implement Azure Stream Analytics user-defined functions (UDF)\nD. Scale the SU count for the job down\nE. Implement query parallelization by partitioning the data output\nF. Implement query parallelization by partitioning the data input"
      },
      {
        "date": "2021-05-25T00:28:00.000Z",
        "voteCount": 1,
        "content": "Answer are Scale up the Streaming Units then include partition on 'Input' (done in the query)"
      },
      {
        "date": "2021-03-16T23:21:00.000Z",
        "voteCount": 2,
        "content": "E and F are exactly same."
      },
      {
        "date": "2020-12-17T18:14:00.000Z",
        "voteCount": 2,
        "content": "i saw this question in dp-200, but option E,F wasnt there either"
      },
      {
        "date": "2021-04-18T06:41:00.000Z",
        "voteCount": 3,
        "content": "I had exactly the same question on dp-200"
      },
      {
        "date": "2020-12-17T18:12:00.000Z",
        "voteCount": 4,
        "content": "DP-200 question not 201"
      },
      {
        "date": "2020-12-08T08:28:00.000Z",
        "voteCount": 2,
        "content": "Scaling the SU count is correct\npartition the output not input\nso B is correct\nEither E or F is right since there is a typo of output twice"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/42500-exam-dp-201-topic-2-question-25-discussion/",
    "body": "You manage a process that performs analysis of daily web traffic logs on an HDInsight cluster. Each of the 250 web servers generates approximately<br>10megabytes (MB) of log data each day. All log data is stored in a single folder in Microsoft Azure Data Lake Storage Gen 2.<br>You need to improve the performance of the process.<br>Which two changes should you make? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCombine the daily log files for all servers into one file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the value of the mapreduce.map.memory parameter",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the log files into folders so that each day's logs are in their own folder",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of worker nodes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the value of the hive.tez.container.size parameter"
    ],
    "answer": "AC",
    "answerDescription": "A: Typically, analytics engines such as HDInsight and Azure Data Lake Analytics has a per-five overhead. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256MB to 100GB in size). Some engines and applications might have trouble efficiently processing files that are greater than 100GB in size.<br>C: For Hive workloads, partition pruning of time-series data can help some queries read only a subset of the data which improves performance.<br>Those pipelines that ingest time-series data, often place their files with a very structured naming for files and folders. Below is a very common example we see for data is structured by date:<br>\\DataSet\\YYYY\\MM\\DD\\datafile_YYYY_MM_DD.tsv<br>Notice that the datetime information appears both as folders and in the filename.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-15T20:05:00.000Z",
        "voteCount": 13,
        "content": "This question is also in the DP-200 exam. Same with the previous question."
      },
      {
        "date": "2021-06-14T10:37:00.000Z",
        "voteCount": 1,
        "content": "Agreed this is a question from DP-200 but wondering if this is part of DP-201 as well?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50396-exam-dp-201-topic-2-question-26-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics Edge application using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Microsoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "Stream Analytics is a cost-effective event processing engine that helps uncover real-time insights from devices, sensors, infrastructure, applications and data quickly and easily.<br>Visual Studio 2019 and Visual Studio 2017 support Stream Analytics Tools.<br>Note: You can also monitor and manage Stream Analytics resources with Azure PowerShell cmdlets and powershell scripting that execute basic Stream Analytics tasks.<br>Reference:<br>https://cloudblogs.microsoft.com/sqlserver/2014/10/29/microsoft-adds-iot-streaming-analytics-data-production-and-workflow-services-to-azure/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-tools-for-visual-studio-install",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-22T06:00:00.000Z",
        "voteCount": 6,
        "content": "3 times DAMN"
      },
      {
        "date": "2023-01-02T09:43:00.000Z",
        "voteCount": 2,
        "content": "Now I cannot miss this question in exam . Lol"
      },
      {
        "date": "2021-05-22T09:10:00.000Z",
        "voteCount": 1,
        "content": "It's a repeated question. Already appeared once before."
      },
      {
        "date": "2021-05-30T07:02:00.000Z",
        "voteCount": 7,
        "content": "even twice"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48316-exam-dp-201-topic-2-question-27-discussion/",
    "body": "You are designing an anomaly detection solution for streaming data from an Azure IoT hub. The solution must meet the following requirements:<br>\u2711 Send the output to Azure Synapse.<br>\u2711 Identify spikes and dips in time series data.<br>\u2711 Minimize development and configuration effort<br>Which should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "B",
    "answerDescription": "You can identify anomalies by routing data via IoT Hub to a built-in ML model in Azure Stream Analytics.<br>Reference:<br>https://docs.microsoft.com/en-us/learn/modules/data-anomaly-detection-using-azure-iot-hub/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-25T01:01:00.000Z",
        "voteCount": 4,
        "content": "Propose solution is correct and better to perform hands-on to identify the other possible destination of data using Azure Stream Analytics"
      },
      {
        "date": "2021-04-07T11:49:00.000Z",
        "voteCount": 2,
        "content": "I am not sure if this is the correct answer. Never heard of Azure streaming analytics output being sent to Azure synapse/Data warehouse"
      },
      {
        "date": "2021-04-09T10:18:00.000Z",
        "voteCount": 4,
        "content": "It is possible.\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/azure-synapse-analytics-output#:~:text=Azure%20Stream%20Analytics%20jobs%20can,rates%20up%20to%20200MB%2Fsec.&amp;text=To%20use%20Azure%20Synapse%20as,have%20the%20storage%20account%20configured."
      },
      {
        "date": "2021-03-27T10:35:00.000Z",
        "voteCount": 4,
        "content": "answer is correct.\nhttps://azure.github.io/iot-workshop-asset-tracking/step-003-anomaly-detection/"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11871-exam-dp-201-topic-2-question-28-discussion/",
    "body": "You are designing an Azure Data Factory pipeline for processing data. The pipeline will process data that is stored in general-purpose standard Azure storage.<br>You need to ensure that the compute environment is created on-demand and removed when the process is completed.<br>Which type of activity should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Python activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Lake Analytics U-SQL activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDInsight Pig activity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDatabricks Jar activity"
    ],
    "answer": "C",
    "answerDescription": "The HDInsight Pig activity in a Data Factory pipeline executes Pig queries on your own or on-demand HDInsight cluster.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/transform-data-using-hadoop-pig",
    "votes": [],
    "comments": [
      {
        "date": "2020-02-22T22:32:00.000Z",
        "voteCount": 53,
        "content": "According to Microsoft documentation: https://docs.microsoft.com/en-us/azure/data-factory/transform-data only 4 external transformations can be executed on-demand: HDInsight MapReduce Activity, HDInsight Hive Activity, HDInsight Pig Activity and HDInsight Streaming Activity. On-demand means that the computing environment is automatically created by the Data Factory service before a job is submitted to process data and removed when the job is completed. Therefore, the correct answer is C."
      },
      {
        "date": "2020-01-12T23:06:00.000Z",
        "voteCount": 7,
        "content": "I agree with the solution C : \"With on-demand HDInsight linked service, a HDInsight cluster is created every time a slice needs to be processed unless there is an existing live cluster (timeToLive) and is deleted when the processing is done.\" But why are the others false ?"
      },
      {
        "date": "2021-03-15T10:09:00.000Z",
        "voteCount": 6,
        "content": "NOT IN THE DP-201 ANY MORE"
      },
      {
        "date": "2021-02-27T04:57:00.000Z",
        "voteCount": 1,
        "content": "I would go with HDInsight Pig activity - rather than option A as per the given condition in the question where we're using ADLS n data bricks is ideally used during ADLS Gen2"
      },
      {
        "date": "2020-12-06T02:40:00.000Z",
        "voteCount": 1,
        "content": "I would agree with the answer\nFrom https://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-compute-linked-services#:~:text=When%20the%20job%20is%20finished,cluster%20management%2C%20and%20bootstrapping%20actions.:\n\"Data Factory automatically creates the compute environment before a job is submitted for processing data. When the job is finished, Data Factory removes the compute environment.\"\n\"The Azure Storage linked service to be used by the on-demand cluster for storing and processing data. The HDInsight cluster is created in the same region as this storage account.\nCurrently, you can't create an on-demand HDInsight cluster that uses Azure Data Lake Store as the storage. If you want to store the result data from HDInsight processing in Data Lake Store, use Copy Activity to copy the data from Blob storage to Data Lake Store.\""
      },
      {
        "date": "2020-11-28T04:12:00.000Z",
        "voteCount": 2,
        "content": "HDinsight is not in dp201 anymore"
      },
      {
        "date": "2020-06-21T01:42:00.000Z",
        "voteCount": 2,
        "content": "Azure Databricks also supports on-demand. when running from Az Datafactory, Databricks cluster gets created as an Automated cluster and destroyed after completion. The question is ambiguous."
      },
      {
        "date": "2020-06-10T13:46:00.000Z",
        "voteCount": 1,
        "content": "The HDInsight Pig activity in a Data Factory pipeline executes Pig queries on your own or on-demand Windows/Linux-based HDInsight cluster. See Pig activity article for details about this activity.\n\nSame as Mapreduce , streaming and hive activity - mentioned explicitly  \"on your own or on-demand\" and based on on demand \"On-Demand: In this case, the computing environment is fully managed by Data Factory. It is automatically created by the Data Factory service before a job is submitted to process data and removed when the job is completed. You can configure and control granular settings of the on-demand compute environment for job execution, cluster management, and bootstrapping actions.\" However, python or jar activities doesn't do any on-demand process. So answer is C."
      },
      {
        "date": "2020-04-28T06:52:00.000Z",
        "voteCount": 3,
        "content": "It's the strange question. Every one of them could answer the demand."
      },
      {
        "date": "2020-05-16T23:28:00.000Z",
        "voteCount": 1,
        "content": "The Azure Databricks Python Activity in a Data Factory pipeline runs a Python file in your Azure Databricks cluster. This article builds on the data transformation activities article, which presents a general overview of data transformation and the supported transformation activities. Azure Databricks is a managed platform for running Apache Spark."
      },
      {
        "date": "2020-02-10T06:10:00.000Z",
        "voteCount": 2,
        "content": "A is also correct answer."
      },
      {
        "date": "2020-01-14T01:53:00.000Z",
        "voteCount": 1,
        "content": "A and D are correct too, u can use automatic created cluster option in linked services"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18937-exam-dp-201-topic-2-question-29-discussion/",
    "body": "A company installs IoT devices to monitor its fleet of delivery vehicles. Data from devices is collected from Azure Event Hub.<br>The data must be transmitted to Power BI for real-time data visualizations.<br>You need to recommend a solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Spark Streaming",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Spark in Azure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight with Storm"
    ],
    "answer": "C",
    "answerDescription": "Step 1: Get your IoT hub ready for data access by adding a consumer group.<br>Step 2: Create, configure, and run a Stream Analytics job for data transfer from your IoT hub to your Power BI account.<br>Step 3: Create and publish a Power BI report to visualize the data.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/iot-hub/iot-hub-live-data-visualization-in-power-bi",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-08T23:56:00.000Z",
        "voteCount": 33,
        "content": "data is already collected from event hub as per question, hence stream analytics is correct"
      },
      {
        "date": "2021-02-28T08:38:00.000Z",
        "voteCount": 2,
        "content": "It's ASA - Azure Stream Analytics where it's fastest way to view real time data visualizations"
      },
      {
        "date": "2020-12-19T01:27:00.000Z",
        "voteCount": 2,
        "content": "C. Azure Stream Analytics\n\nIt is most efficient with Event Hubs"
      },
      {
        "date": "2020-12-08T07:16:00.000Z",
        "voteCount": 1,
        "content": "It can only be stream analytics"
      },
      {
        "date": "2020-05-22T06:17:00.000Z",
        "voteCount": 2,
        "content": "Is 'B' wrong/no good because it says Apache Spark with Databricks?  If 'Databricks' was by itself, wouldn't that be an acceptable answer?  Databricks can model and serve to BI."
      },
      {
        "date": "2020-06-21T01:50:00.000Z",
        "voteCount": 5,
        "content": "Azure Databricks is suitable for complex analysis. With Strem analytics,  one can query event data and perform the required analysis on it. upon the data can directly send to Power BI without any other interface between, SA has Power BI as an output stream."
      },
      {
        "date": "2020-04-22T12:05:00.000Z",
        "voteCount": 2,
        "content": "The given reference is incorrect. Should use Event Hub.\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-tutorial-visualize-anomalies"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25694-exam-dp-201-topic-2-question-30-discussion/",
    "body": "You have a Windows-based solution that analyzes scientific data. You are designing a cloud-based solution that performs real-time analysis of the data.<br>You need to design the logical flow for the solution.<br>Which two actions should you recommend? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend data from the application to an Azure Stream Analytics job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Azure Stream Analytics job on an edge device. Ingress data from an Azure Data Factory instance and build queries that output to Power BI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Azure Stream Analytics job in the cloud. Ingress data from the Azure Event Hub instance and build queries that output to Power BI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Azure Stream Analytics job in the cloud. Ingress data from an Azure Event Hub instance and build queries that output to Azure Data Lake Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend data from the application to Azure Data Lake Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend data from the application to an Azure Event Hub instance."
    ],
    "answer": "CF",
    "answerDescription": "Stream Analytics has first-class integration with Azure data streams as inputs from three kinds of resources:<br>\u2711 Azure Event Hubs<br>\u2711 Azure IoT Hub<br>\u2711 Azure Blob storage<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-inputs",
    "votes": [],
    "comments": [
      {
        "date": "2020-08-04T03:38:00.000Z",
        "voteCount": 36,
        "content": "CF is the correct one, no confusion"
      },
      {
        "date": "2020-08-21T04:50:00.000Z",
        "voteCount": 15,
        "content": "CF correct"
      },
      {
        "date": "2021-05-25T01:09:00.000Z",
        "voteCount": 1,
        "content": "As stated C and F are the correct solutions though if it is needed to configure it sequentially then F and C are the appropriate steps to accomplish it.\n\nEvent Hub &gt; Azure Stream Analytics (Cloud) &gt; Power BI (output for real time analysis)"
      },
      {
        "date": "2021-02-28T05:23:00.000Z",
        "voteCount": 1,
        "content": "we can't o/p azure data lake storage as we have a better option Power BI as we can run Power BI directly on ASA"
      },
      {
        "date": "2021-02-28T05:20:00.000Z",
        "voteCount": 1,
        "content": "The 1st step would be read option F &amp; then option C in order to understand the logical flow \nD cannot be included as per the given scenario."
      },
      {
        "date": "2021-02-17T11:54:00.000Z",
        "voteCount": 1,
        "content": "Why not CD? and how do you meet \"performs real-time analysis of the data.\" through event hub?"
      },
      {
        "date": "2021-02-17T11:57:00.000Z",
        "voteCount": 1,
        "content": "I guess these are two actions or steps part of same solution. If so CF is correct. First action is F and then perform C."
      },
      {
        "date": "2021-02-23T04:14:00.000Z",
        "voteCount": 1,
        "content": "Yes for real time analysis input through event hub and output to BI"
      },
      {
        "date": "2021-01-12T14:16:00.000Z",
        "voteCount": 5,
        "content": "Why is the output to DLake v2 option wrong? Thanks"
      },
      {
        "date": "2021-04-07T02:16:00.000Z",
        "voteCount": 1,
        "content": "I think that data lake cannot be tread as last step to consume data, there would be needed a third step to consume data from DL. PowerBI visualization can be threated as final data consumption"
      },
      {
        "date": "2020-07-24T23:08:00.000Z",
        "voteCount": 1,
        "content": "The answer should be DF"
      },
      {
        "date": "2020-07-26T02:08:00.000Z",
        "voteCount": 40,
        "content": "Ignore this. CF is correct"
      },
      {
        "date": "2020-07-14T03:41:00.000Z",
        "voteCount": 2,
        "content": "Could it be D? Can you run Power BI directly on Streaming Analytics? Or do you need direct query for real time?"
      },
      {
        "date": "2020-07-24T23:10:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-power-bi-dashboard  It seems you can..."
      },
      {
        "date": "2020-10-08T12:47:00.000Z",
        "voteCount": 2,
        "content": "Direct Query works in real time when you have Power BI Premium subcription"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46705-exam-dp-201-topic-2-question-31-discussion/",
    "body": "DRAG DROP -<br>You are designing a Spark job that performs batch processing of daily web log traffic.<br>When you deploy the job in the production environment, it must meet the following requirements:<br>\u2711 Run once a day.<br>\u2711 Display status information on the company intranet as the job runs.<br>You need to recommend technologies for triggering and monitoring jobs.<br>Which technologies should you recommend? To answer, drag the appropriate technologies to the correct locations. Each technology may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0017900003.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0018000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Livy -<br>You can use Livy to run interactive Spark shells or submit batch jobs to be run on Spark.<br><br>Box 2: Beeline -<br>Apache Beeline can be used to run Apache Hive queries on HDInsight. You can use Beeline with Apache Spark.<br>Note: Beeline is a Hive client that is included on the head nodes of your HDInsight cluster. Beeline uses JDBC to connect to HiveServer2, a service hosted on your<br>HDInsight cluster. You can also use Beeline to access Hive on HDInsight remotely over the internet.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/hdinsight/spark/apache-spark-livy-rest-interface https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-use-hive-beeline",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-15T10:24:00.000Z",
        "voteCount": 6,
        "content": "NOT ANY MORE IN DP-201"
      },
      {
        "date": "2021-06-05T09:30:00.000Z",
        "voteCount": 1,
        "content": "are you sure? how do you know?"
      },
      {
        "date": "2021-03-12T06:13:00.000Z",
        "voteCount": 5,
        "content": "Beeline used for running hive queries not for monitoring"
      },
      {
        "date": "2023-12-26T07:38:00.000Z",
        "voteCount": 1,
        "content": "Beeline or Livy for triggering the Spark job.\nAzure Logic Apps for monitoring and updating status on the company intranet."
      },
      {
        "date": "2021-06-26T04:34:00.000Z",
        "voteCount": 1,
        "content": "Guys, then which is the answer ?"
      },
      {
        "date": "2021-03-17T02:46:00.000Z",
        "voteCount": 2,
        "content": "answer is right https://livy.incubator.apache.org/"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51620-exam-dp-201-topic-2-question-32-discussion/",
    "body": "You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1.<br>You need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade workspace1 to the Premium pricing tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pool in workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a global init script for workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster policy in workspace1."
    ],
    "answer": "B",
    "answerDescription": "Databricks Pools increase the productivity of both Data Engineers and Data Analysts. With Pools, Databricks customers eliminate slow cluster start and auto- scaling times. Data Engineers can reduce the time it takes to run short jobs in their data pipeline, thereby providing better SLAs to their downstream teams.<br>Reference:<br>https://databricks.com/blog/2019/11/11/databricks-pools-speed-up-data-pipelines.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-02T17:25:00.000Z",
        "voteCount": 7,
        "content": "Correct."
      },
      {
        "date": "2021-10-30T22:04:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47947-exam-dp-201-topic-2-question-33-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution to process data from multiple Azure event hubs in near real-time.<br>Once processed, the data will be written to an Azure SQL database.<br>The solution must meet the following requirements:<br>\u2711 Support the auditing of resource and data changes.<br>\u2711 Support data versioning and rollback.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0018200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0018300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Stream Analytics -<br>Users can now ingest, process, view, and analyze real-time streaming data into a table directly from a database in Azure SQL Database. They do so in the Azure portal using Azure Stream Analytics.<br>In the Azure portal, you can select an events source (Event Hub/IoT Hub), view incoming real-time events, and select a table to store events.<br>Stream Analytics leverages versioning of reference data to augment streaming data with the reference data that was valid at the time the event was generated.<br>This ensures repeatability of results.<br><br>Box 2: Replay -<br>Reference data is versioned, enabling to always get the same results, even when we \u05d2\u20acreplay\u05d2\u20ac the stream.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/stream-data-stream-analytics-integration https://azure.microsoft.com/en-us/updates/additional-support-for-managed-identity-and-new-features-in-azure-stream-analytics/",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-22T07:16:00.000Z",
        "voteCount": 38,
        "content": "Azure streaming analytics and replay are about job recovery.\nI would definitely go for Azure Databricks and Delta.\n\nhttps://databricks.com/blog/2019/02/04/introducing-delta-time-travel-for-large-scale-data-lakes.html"
      },
      {
        "date": "2021-06-08T04:07:00.000Z",
        "voteCount": 1,
        "content": "Also azure stream analytics does not support multiple inputs"
      },
      {
        "date": "2021-05-25T01:31:00.000Z",
        "voteCount": 2,
        "content": "Agree with this solution. Better to compare the delta table of Azure Databricks against the Azure Stream Analytics"
      },
      {
        "date": "2021-04-25T03:29:00.000Z",
        "voteCount": 8,
        "content": "Data is to be written to Azure SQL Database, Azure Databricks doesn't support it as a sink - https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing. Therefore ASA is the only otpion left along with Replay feature. Replay may not be the best method but since ASA has to be included this will do. Also Azure SQL Database can handle rollbacks and auditing by itself. So given solution is correct."
      },
      {
        "date": "2023-12-26T09:27:00.000Z",
        "voteCount": 1,
        "content": "Azure synpse analytics and delta\n\nAzure Service to Use: Azure Synapse Analytics (formerly SQL Data Warehouse):\n\nAzure Synapse Analytics is a comprehensive analytics service that brings together big data and data warehousing. It supports near real-time analytics and integrates with Azure SQL Database.\nFeature to Use: Delta:\n\nDelta is a feature associated with Azure Synapse Analytics that provides capabilities like versioning, change tracking, and rollback options. It is designed to handle data versioning and changes efficiently."
      },
      {
        "date": "2022-05-17T11:48:00.000Z",
        "voteCount": 1,
        "content": "Job Replay is simply about recovering your stream to its prior state in the case of a service upgrade or any manual intervention in which you stop the stream. It has nothing to do with versioning or rolling back to a specific version. That is Delta."
      },
      {
        "date": "2021-09-21T05:59:00.000Z",
        "voteCount": 1,
        "content": "azure databricks and delta tables. Databricks can use delta feature to load required snapshot. ADB can load data to sql db."
      },
      {
        "date": "2021-07-13T01:31:00.000Z",
        "voteCount": 2,
        "content": "Azure databricks and Delta \nWe can push data from databricks to azure sql db using jdbc driver."
      },
      {
        "date": "2021-07-29T05:00:00.000Z",
        "voteCount": 1,
        "content": "https://www.sqlshack.com/load-data-into-azure-sql-database-from-azure-databricks/"
      },
      {
        "date": "2021-05-02T17:31:00.000Z",
        "voteCount": 3,
        "content": "Not sure what is the confusion.. refrence links are already given in and looks correct ..\neference:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/stream-data-stream-analytics-integration https://azure.microsoft.com/en-us/updates/additional-support-for-managed-identity-and-new-features-in-azure-stream-analytics/"
      },
      {
        "date": "2021-04-17T02:26:00.000Z",
        "voteCount": 2,
        "content": "The given answer, I think i correct\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-concepts-checkpoint-replay"
      },
      {
        "date": "2021-04-18T20:25:00.000Z",
        "voteCount": 2,
        "content": "I think the questions talks about data versioning and auditing data and date related changes and not about the job recovery , and hence I think Azure Databricks and Delta happens to be logically correct with corelation to the question asked for Data changes and not job related changes or recovery"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46706-exam-dp-201-topic-2-question-34-discussion/",
    "body": "DRAG DROP -<br>You are designing a real-time processing solution for maintenance work requests that are received via email. The solution will perform the following actions:<br>\u2711 Store all email messages in an archive.<br>\u2711 Access weather forecast data by using the Python SDK for Azure Open Datasets.<br>\u2711 Identify high priority requests that will be affected by poor weather conditions and store the requests in an Azure SQL database.<br>The solution must minimize costs.<br>How should you complete the solution? To answer, drag the appropriate services to the correct locations. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0018500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0018500002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Storage -<br>Azure Event Hubs enables you to automatically capture the streaming data in Event Hubs in an Azure Blob storage or Azure Data Lake Storage Gen 1 or Gen 2 account of your choice, with the added flexibility of specifying a time or size interval. Setting up Capture is fast, there are no administrative costs to run it, and it scales automatically with Event Hubs throughput units. Event Hubs Capture is the easiest way to load streaming data into Azure, and enables you to focus on data processing rather than on data capture.<br><br>Box 2: Azure Logic Apps -<br>You can monitor and manage events sent to Azure Event Hubs from inside a logic app with the Azure Event Hubs connector. That way, you can create logic apps that automate tasks and workflows for checking, sending, and receiving events from your Event Hub.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview https://docs.microsoft.com/en-us/azure/connectors/connectors-create-api-azure-event-hubs",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-14T12:42:00.000Z",
        "voteCount": 36,
        "content": "I believe it is Databricks in the second box due to the Python - Azure Open Datasets."
      },
      {
        "date": "2021-06-20T12:59:00.000Z",
        "voteCount": 1,
        "content": "Databrick is the correct answer"
      },
      {
        "date": "2021-03-15T13:12:00.000Z",
        "voteCount": 5,
        "content": "it's for sure azure databricks"
      },
      {
        "date": "2021-03-23T19:25:00.000Z",
        "voteCount": 3,
        "content": "How? The processing unit has to pick up incoming events and then process. ASA is best fit for it"
      },
      {
        "date": "2021-05-25T01:38:00.000Z",
        "voteCount": 1,
        "content": "ASA doesn't have Python SDK on it"
      },
      {
        "date": "2021-03-29T05:10:00.000Z",
        "voteCount": 6,
        "content": "Jony is right. \nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets"
      },
      {
        "date": "2021-06-20T12:59:00.000Z",
        "voteCount": 1,
        "content": "Valid link"
      },
      {
        "date": "2021-03-12T06:14:00.000Z",
        "voteCount": 8,
        "content": "Can we use Stream Analytics instead of the Logic app?"
      },
      {
        "date": "2021-06-06T09:46:00.000Z",
        "voteCount": 1,
        "content": "The NOAA data options only show Databricks or Synapse, which was not an option in the question, and Azure Notebooks which was in preview and no longer available (and also not an option in the question):\n\nhttps://azure.microsoft.com/en-us/services/open-datasets/catalog/noaa-integrated-surface-data/#AzureDatabricks"
      },
      {
        "date": "2021-04-24T02:47:00.000Z",
        "voteCount": 4,
        "content": "The key here is 'Identify high priority requests that will be affected by poor weather conditions..' - This can be done efficiently by Logic app."
      },
      {
        "date": "2021-05-26T18:29:00.000Z",
        "voteCount": 1,
        "content": "check this link\nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets"
      },
      {
        "date": "2021-03-26T04:40:00.000Z",
        "voteCount": 3,
        "content": "Azure open dataset can be accessed from databricks or any Python environment with or without Spark. Hence, the second box should be Databricks.\nhttps://docs.microsoft.com/en-us/azure/open-datasets/overview-what-are-open-datasets"
      },
      {
        "date": "2021-03-20T04:37:00.000Z",
        "voteCount": 1,
        "content": "Function Apps would be the way to go for any custom code needed in logic apps.\nPython in Azure Functions is still in Preview, so it's not recommended for production use.\nYou can refer to this for more information on creating a new python function app.\nhttps://social.msdn.microsoft.com/Forums/en-US/1b46dcb2-2832-4861-a214-e49e85247d53/how-can-i-run-a-python-script-in-logic-apps"
      },
      {
        "date": "2021-03-21T10:32:00.000Z",
        "voteCount": 1,
        "content": "I think they are not in preview right now, this thread is from 2018, so it is not up to date"
      },
      {
        "date": "2021-03-22T06:00:00.000Z",
        "voteCount": 2,
        "content": "yet databricks is good condidate"
      },
      {
        "date": "2021-04-06T10:23:00.000Z",
        "voteCount": 1,
        "content": "I think Logic App (in which you can use Python by mean of function App) o stream analytics are better."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47546-exam-dp-201-topic-2-question-35-discussion/",
    "body": "You have a large amount of sensor data stored in an Azure Data Lake Storage Gen2 account. The files are in the Parquet file format.<br>New sensor data will be published to Azure Event Hubs.<br>You need to recommend a solution to add the new sensor data to the existing sensor data in real-time. The solution must support the interactive querying of the entire dataset.<br>Which type of server should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks"
    ],
    "answer": "C",
    "answerDescription": "Azure Stream Analytics is a fully managed PaaS offering that enables real-time analytics and complex event processing on fast moving data streams.<br>By outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-26T05:24:00.000Z",
        "voteCount": 5,
        "content": "As per below link the answer is correct.\nhttps://azure.microsoft.com/en-in/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/"
      },
      {
        "date": "2021-05-25T02:08:00.000Z",
        "voteCount": 5,
        "content": "Both Azure Databricks and Azure Stream analytics can output data to parquet format and have interactive queries as well. For simplicity, I'll choose Azure Stream Analytics"
      },
      {
        "date": "2021-06-09T19:16:00.000Z",
        "voteCount": 2,
        "content": "Opt to choose Azure Databricks instead of Azure Streaming Analytics due to the keywork 'large dataset' \n\nReference: https://techcommunity.microsoft.com/t5/analytics-on-azure/azure-stream-analytics-real-time-analytics-for-big-data-made/ba-p/549621"
      },
      {
        "date": "2021-05-26T18:32:00.000Z",
        "voteCount": 2,
        "content": "ASA doesn't support Parquet  format.!"
      },
      {
        "date": "2021-05-26T18:34:00.000Z",
        "voteCount": 1,
        "content": "I was wrong, it supports now \n\nhttps://azure.microsoft.com/en-us/updates/stream-analytics-offers-native-support-for-parquet-format/#:~:text=Azure%20Stream%20Analytics%20now%20offers,in%20the%20Big%20Data%20ecosystems."
      },
      {
        "date": "2021-05-29T02:56:00.000Z",
        "voteCount": 1,
        "content": "One of the requirements is to be able to interactively query the whole (possibly very large) dataset according to the scenario. This requirement alone is a perfect fit for Spark. I highly doubt there is a sensible way to achieve this with ASA. Therefore I vote for Databricks."
      },
      {
        "date": "2021-07-20T02:25:00.000Z",
        "voteCount": 1,
        "content": "By outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data.\n\nSoure: https://azure.microsoft.com/en-in/blog/new-capabilities-in-stream-analytics-reduce-development-time-for-big-data-apps/"
      },
      {
        "date": "2021-08-10T09:47:00.000Z",
        "voteCount": 1,
        "content": "What this quote says is that ASA can output parquet format to blob storage, so that another tool can then run interactive queries on the data. ASA itself can't do interactive queries on parquet in blob storage, which is what is required here. I'd go with databricks."
      },
      {
        "date": "2021-05-02T17:36:00.000Z",
        "voteCount": 3,
        "content": "Native support for egress in Apache parquet format into Azure Blob Storage is now generally available. Parquet is a columnar format enabling efficient big data processing. By outputting data in parquet format into a blob store or a data lake, you can take advantage of Azure Stream Analytics to power large scale streaming extract, transfer, and load (ETL), to run batch processing, to train machine learning algorithms, or to run interactive queries on your historical data. We are now announcing general availability of this feature for egress to Azure Blob Storage."
      },
      {
        "date": "2021-03-27T00:13:00.000Z",
        "voteCount": 2,
        "content": "I think that Databrick is a good answer. I'm not sure if Azure Stream Analytics is another right answer but maybe there are two possibilities"
      },
      {
        "date": "2021-04-18T08:17:00.000Z",
        "voteCount": 3,
        "content": "interactive querying eliminates ASA"
      },
      {
        "date": "2021-04-26T04:48:00.000Z",
        "voteCount": 2,
        "content": "Azure Stream Analytics does not support Parquet data format.\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-stream-analytics-query-patterns"
      },
      {
        "date": "2021-05-05T04:02:00.000Z",
        "voteCount": 2,
        "content": "It does as of July 2019 https://azure.microsoft.com/en-us/updates/stream-analytics-offers-native-support-for-parquet-format/"
      },
      {
        "date": "2021-03-26T05:23:00.000Z",
        "voteCount": 1,
        "content": "As per below link the answer is correct.\nnew-capabilities-in-stream-analytics-reduce-development-time-for-big-data-app"
      },
      {
        "date": "2021-03-17T07:41:00.000Z",
        "voteCount": 2,
        "content": "is C really the correct answer pls?"
      },
      {
        "date": "2021-03-22T06:04:00.000Z",
        "voteCount": 7,
        "content": "i think it's D because the interactive querying of the entire dataset.\nentire dataset/interative isn't possible with A.stream"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48245-exam-dp-201-topic-2-question-36-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Storage account that generates 200,000 new files daily. The file names have a format of {YYYY}/{MM}/{DD}/{HH}/{CustomerID}.csv.<br>You need to design an Azure Data Factory solution that will load new data from the storage account to an Azure Data Lake once hourly. The solution must minimize load times and costs.<br>How should you configure the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0018800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0018900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Incremental load -<br>When you start to build the end to end data integration flow the first challenge is to extract data from different data stores, where incrementally (or delta) loading data after an initial full load is widely used at this stage. Now, ADF provides a new capability for you to incrementally copy new or changed files only by<br>LastModifiedDate from a file-based store. By using this new feature, you do not need to partition the data by time-based folder or file name. The new or changed file will be automatically selected by its metadata LastModifiedDate and copied to the destination store.<br><br>Box 2: Tumbling window -<br>Tumbling window triggers are a type of trigger that fires at a periodic time interval from a specified start time, while retaining state. Tumbling windows are a series of fixed-sized, non-overlapping, and contiguous time intervals. A tumbling window trigger has a one-to-one relationship with a pipeline and can only reference a singular pipeline.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/incrementally-copy-new-files-by-lastmodifieddate-with-azure-data-factory/ https://docs.microsoft.com/en-us/azure/data-factory/how-to-create-tumbling-window-trigger",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-16T02:26:00.000Z",
        "voteCount": 1,
        "content": "I believe it can be tumbling as well as fixed schedule as they both can do fixed hour"
      },
      {
        "date": "2021-06-09T00:35:00.000Z",
        "voteCount": 3,
        "content": "According the MS documentation, incremental loads are used together with tumbling window. Tumbling window is used in both of these examples where we are performing an incremental load from Blob Storage. \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-lastmodified-copy-data-tool\n\nand \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-incremental-copy-partitioned-file-name-copy-data-tool"
      },
      {
        "date": "2021-06-06T10:36:00.000Z",
        "voteCount": 1,
        "content": "Tumbling Window trigger is a \"smarter\" run - what if the pipeline takes longer than an hour to run? \n\nhttps://docs.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"
      },
      {
        "date": "2021-06-04T18:54:00.000Z",
        "voteCount": 2,
        "content": "The appropriate solutions are 'incremental load' and schedule is mentioned as once hourly  which is 'Tumbling window' the answer is correct!"
      },
      {
        "date": "2021-05-25T02:19:00.000Z",
        "voteCount": 3,
        "content": "The appropriate solutions are 'incremental load' and 'fixed schedule' as the basis is 1 hour trigger and the use of tumbling window requires further configuration than the mentioned schedule earlier. It would be better if there is an option to use 'storage events' as the ADF will trigger if a blob is created or deleted.\n\nReference: https://www.mssqltips.com/sqlservertip/6061/create-tumbling-window-trigger-in-azure-data-factory-adf/"
      },
      {
        "date": "2021-06-15T19:33:00.000Z",
        "voteCount": 1,
        "content": "In the event the requirement requires to take in consideration the load processing time then tumbling window is the appropriate configuration as the trigger won't overlap."
      },
      {
        "date": "2021-05-14T06:26:00.000Z",
        "voteCount": 1,
        "content": "tumbling window will be used for stream analytics..."
      },
      {
        "date": "2021-04-29T10:46:00.000Z",
        "voteCount": 4,
        "content": "But schedule is mentioned as once hourly , why would it be Tumbling window ?"
      },
      {
        "date": "2021-03-26T05:12:00.000Z",
        "voteCount": 1,
        "content": "If its tumbling window then why not new individual file as they arrive? Tumbling window works only when a new event occurs"
      },
      {
        "date": "2021-03-27T00:59:00.000Z",
        "voteCount": 4,
        "content": "ignore. The answer is correct"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46709-exam-dp-201-topic-2-question-37-discussion/",
    "body": "You are designing a solution that will copy Parquet files stored in an Azure Blob storage account to an Azure Data Lake Storage Gen2 account.<br>The data will be loaded daily to the data lake and will use a folder structure of {Year}/{Month}/{Day}/.<br>You need to design a daily Azure Data Factory data load to minimize the data transfer between the two accounts.<br>Which two configurations should you include in the design? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the files in the destination before loading new data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter by the last modified date of the source files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the source files after they are copied.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a file naming pattern for the destination."
    ],
    "answer": "BC",
    "answerDescription": "B: To copy a subset of files under a folder, specify folderPath with a folder part and fileName with a wildcard filter.<br>C: After completion: Choose to do nothing with the source file after the data flow runs, delete the source file, or move the source file. The paths for the move are relative.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-07T18:17:00.000Z",
        "voteCount": 12,
        "content": "If you choose C. Delete the source files after they are copied, why do you choose B. Filter by the last modified date of the source files? I prefer BD."
      },
      {
        "date": "2021-09-30T09:06:00.000Z",
        "voteCount": 2,
        "content": "This is a basic question. Copy data from one place to another. The requirements are : 1- need to minimize transfert and 2- need to adapte data to the destination folder structure. Filter on LastModifiedDate will copy everything that have changed since the latest load while minimizing the data transfert. Specifying the file naming pattern allows to copy data at the right place to the destination Data Lake. The answer is BD"
      },
      {
        "date": "2021-08-18T22:21:00.000Z",
        "voteCount": 1,
        "content": "How naming pattern gonna minimize the Data Transfer? BC should be correct answer."
      },
      {
        "date": "2021-05-13T12:29:00.000Z",
        "voteCount": 10,
        "content": "Correct answer is BC.\nIn the source option of copy activities. There are three choices: 1. No Action 2. Delete Source files 3. Move"
      },
      {
        "date": "2021-06-11T10:54:00.000Z",
        "voteCount": 4,
        "content": "A is obviously out and you're are not going to do both B and C so D is in by default. Your only choice at that point is B or C to go along with D. In my experience, you cannot rely 100% on any job to run every single day (assuming this process is daily). Therefore, if the job does not run for one or more days, if you were to choose B you would only copy over the most recent files and there would be files left in the storage account. Therefore, my choice would be to not filter and load everything that is in the storage account and then delete the files once they have been copied. So, C and D are my choices."
      },
      {
        "date": "2021-09-14T07:10:00.000Z",
        "voteCount": 1,
        "content": "B ensures minimized data transfer. If it copies everything every time, then data transfer is not minimized."
      },
      {
        "date": "2021-04-20T18:51:00.000Z",
        "voteCount": 3,
        "content": "I would like to choose CD."
      },
      {
        "date": "2021-04-08T07:11:00.000Z",
        "voteCount": 3,
        "content": "The was no requirement what to do with original files, so why i the world anwer C - delete them???"
      },
      {
        "date": "2021-06-04T18:56:00.000Z",
        "voteCount": 1,
        "content": "I guess to make sure you dont read the file again!"
      },
      {
        "date": "2021-03-24T04:41:00.000Z",
        "voteCount": 2,
        "content": "C seems not correcct as to deletion you can do life cycle mgmt in storage, so D should be second answer."
      },
      {
        "date": "2021-03-12T06:27:00.000Z",
        "voteCount": 1,
        "content": "thought it's the only logical choice but they said copy activity not moving files"
      },
      {
        "date": "2021-03-15T13:17:00.000Z",
        "voteCount": 22,
        "content": "I think it\"s BD"
      },
      {
        "date": "2021-03-19T19:44:00.000Z",
        "voteCount": 1,
        "content": "Wildcard path: Using a wildcard pattern will instruct ADF to loop through each matching folder and file in a single Source transformation. This is an effective way to process multiple files within a single flow. Add multiple wildcard matching patterns with the + sign that appears when hovering over your existing wildcard pattern.\n\nFrom your source container, choose a series of files that match a pattern. Only container can be specified in the dataset. Your wildcard path must therefore also include your folder path from the root folder."
      },
      {
        "date": "2021-03-19T19:46:00.000Z",
        "voteCount": 4,
        "content": "yes BD..  i think you are right"
      },
      {
        "date": "2021-04-08T07:10:00.000Z",
        "voteCount": 2,
        "content": "but this applies to finding a source files and D was about destintion file naming pattern... which there were no requirement to change the file name"
      },
      {
        "date": "2021-05-25T19:31:00.000Z",
        "voteCount": 3,
        "content": "Agree with the answer B and D as this kind of setup doesn't perform any deletion from both storages which lessen the processing."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49608-exam-dp-201-topic-2-question-38-discussion/",
    "body": "You have a C# application that process data from an Azure IoT hub and performs complex transformations.<br>You need to replace the application with a real-time solution. The solution must reuse as much code as possible from the existing application.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Grid",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory"
    ],
    "answer": "C",
    "answerDescription": "Azure Stream Analytics on IoT Edge empowers developers to deploy near-real-time analytical intelligence closer to IoT devices so that they can unlock the full value of device-generated data. UDF are available in C# for IoT Edge jobs<br>Azure Stream Analytics on IoT Edge runs within the Azure IoT Edge framework. Once the job is created in Stream Analytics, you can deploy and manage it using<br>IoT Hub.<br>References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-edge",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-13T12:10:00.000Z",
        "voteCount": 12,
        "content": "Correct answer is C"
      },
      {
        "date": "2021-08-18T22:25:00.000Z",
        "voteCount": 2,
        "content": "Nope It has to be Databricks."
      },
      {
        "date": "2021-05-04T05:21:00.000Z",
        "voteCount": 10,
        "content": "Ans : A\nApache Spark in Azure Databricks supports C#.\nRef: https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing"
      },
      {
        "date": "2021-05-30T23:32:00.000Z",
        "voteCount": 2,
        "content": "This seems to be correct answer as Databricks, as from the available options only Databricks have the support for C#. so the question says maximum reusability of the code, hence Databricks must be the correct answer. Thanks."
      },
      {
        "date": "2021-06-15T15:01:00.000Z",
        "voteCount": 1,
        "content": "ASA supports C#: https://azure.microsoft.com/en-us/blog/supercharge-your-azure-stream-analytics-query-with-c-code/"
      },
      {
        "date": "2021-06-28T22:50:00.000Z",
        "voteCount": 2,
        "content": "That is limited only to IoT Edge devices....Az Databricks looks more relevant"
      },
      {
        "date": "2023-12-26T09:56:00.000Z",
        "voteCount": 1,
        "content": "Azure synapse analytics \n\nAzure Databricks is a big data analytics platform. While it's powerful, it may not be the most straightforward choice if the goal is to reuse existing C# code for real-time processing.\n\nGiven the requirements, Azure Stream Analytics is likely the most appropriate option for a real-time solution with the potential for code reuse.     \n\nchat gpt"
      },
      {
        "date": "2021-08-16T02:57:00.000Z",
        "voteCount": 1,
        "content": "question says complex transformation, I think ASA is not the right fit for that kind calculation. I would go with databricks"
      },
      {
        "date": "2021-04-08T07:24:00.000Z",
        "voteCount": 5,
        "content": "why not databricks? can also utilize c# code and process data in real time"
      },
      {
        "date": "2021-04-27T19:37:00.000Z",
        "voteCount": 5,
        "content": "SPARK does real time processing, not Databricks\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing"
      },
      {
        "date": "2021-06-28T22:51:00.000Z",
        "voteCount": 1,
        "content": "yeah, but spark is core of Databricks solution"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50404-exam-dp-201-topic-2-question-39-discussion/",
    "body": "HOTSPOT -<br>You are designing an Azure Data Factory solution that will download up to 5 TB of data from several REST APIs.<br>The solution must meet the following staging requirements:<br>\u2711 Ensure that the data can be landed quickly and in parallel to a staging area.<br>\u2711 Minimize the need to return to the API sources to retrieve the data again should a later activity in the pipeline fail.<br>The solution must meet the following analysis requirements:<br>\u2711 Ensure that the data can be loaded in parallel.<br>\u2711 Ensure that users and applications can query the data without requiring an additional compute engine.<br>What should you include in the solution to meet the requirements? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0019200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0019300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Blob storage -<br>When you activate the staging feature, first the data is copied from the source data store to the staging storage (bring your own Azure Blob or Azure Data Lake<br>Storage Gen2).<br><br>Box 2: Azure Synapse Analytics -<br>The Azure Synapse Analytics connector in copy activity provides built-in data partitioning to copy data in parallel.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-data-warehouse",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-30T09:59:00.000Z",
        "voteCount": 2,
        "content": "Look at this: https://docs.microsoft.com/en-us/azure/data-factory/copy-activity-performance-features\n\nWhen you activate the staging feature, first the data is copied from the source data store to the staging storage (bring your own Azure Blob or Azure Data Lake Storage Gen2). Next, the data is copied from the staging to the sink data store. The copy activity automatically manages the two-stage flow for you, and also cleans up temporary data from the staging storage after the data movement is complete."
      },
      {
        "date": "2021-04-18T08:41:00.000Z",
        "voteCount": 2,
        "content": "correct, but the explanation for synapse is that ASA allows querying"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50405-exam-dp-201-topic-2-question-40-discussion/",
    "body": "A company purchases IoT devices to monitor manufacturing machinery. The company uses an IoT appliance to communicate with the IoT devices.<br>The company must be able to monitor the devices in real-time.<br>You need to design the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Analysis Services using Microsoft Visual Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics cloud job using Azure Portal",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory instance using Azure Portal"
    ],
    "answer": "C",
    "answerDescription": "The Stream Analytics query language allows to perform CEP (Complex Event Processing) by offering a wide array of functions for analyzing streaming data. This query language supports simple data manipulation, aggregation and analytics functions, geospatial functions, pattern matching and anomaly detection. You can edit queries in the portal or using our development tools, and test them using sample data that is extracted from a live stream.<br>Note: Stream Analytics is a cost-effective event processing engine that helps uncover real-time insights from devices, sensors, infrastructure, applications and data quickly and easily.<br>Monitor and manage Stream Analytics resources with Azure PowerShell cmdlets and powershell scripting that execute basic Stream Analytics tasks.<br>Reference:<br>https://cloudblogs.microsoft.com/sqlserver/2014/10/29/microsoft-adds-iot-streaming-analytics-data-production-and-workflow-services-to-azure/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-19T01:41:00.000Z",
        "voteCount": 6,
        "content": "appears fourth time"
      },
      {
        "date": "2021-06-22T06:22:00.000Z",
        "voteCount": 2,
        "content": "to much duplicated"
      },
      {
        "date": "2021-05-11T12:51:00.000Z",
        "voteCount": 1,
        "content": "a and d are same answer"
      },
      {
        "date": "2021-04-18T08:42:00.000Z",
        "voteCount": 3,
        "content": "c but edge job"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46914-exam-dp-201-topic-2-question-41-discussion/",
    "body": "You are designing a real-time stream solution based on Azure Functions. The solution will process data uploaded to Azure Blob Storage.<br>The solution requirements are as follows:<br>\u2711 Support up to 1 million blobs.<br>\u2711 Scaling must occur automatically.<br>\u2711 Costs must be minimized.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in an App Service plan and use a Blob trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in a Consumption plan and use an Event Grid trigger.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in a Consumption plan and use a Blob trigger.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Azure Function in an App Service plan and use an Event Grid trigger."
    ],
    "answer": "C",
    "answerDescription": "Create a function, with the help of a blob trigger template, which is triggered when files are uploaded to or updated in Azure Blob storage.<br>You use a consumption plan, which is a hosting plan that defines how resources are allocated to your function app. In the default Consumption Plan, resources are added dynamically as required by your functions. In this serverless hosting, you only pay for the time your functions run. When you run in an App Service plan, you must manage the scaling of your function app.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-13T09:56:00.000Z",
        "voteCount": 39,
        "content": "The solution is B - Deploy the Azure Function in a Consumption plan and use an Event Grid trigger. 1M blobs will cripple the ability of blob trigger to provide the events.\nThe EventGrid trigger is instantaneous, so it depends on your needs."
      },
      {
        "date": "2021-05-13T11:53:00.000Z",
        "voteCount": 8,
        "content": "It repeated question"
      },
      {
        "date": "2023-12-26T10:11:00.000Z",
        "voteCount": 1,
        "content": "B. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger.\n\nConsumption Plan: Azure Functions in a Consumption Plan automatically scales based on the number of incoming events. It is a serverless option where you only pay for the actual execution of functions.\n\nEvent Grid Trigger: Using an Event Grid trigger allows you to respond to events in Azure Blob Storage, such as new blobs being created. This is a more event-driven and scalable approach compared to polling for changes. \n\nchat gpt"
      },
      {
        "date": "2021-10-22T04:22:00.000Z",
        "voteCount": 2,
        "content": "repeated . Answer B"
      },
      {
        "date": "2021-04-29T08:20:00.000Z",
        "voteCount": 4,
        "content": "C. Deploy the Azure Function in a Consumption plan and use a Blob trigger."
      },
      {
        "date": "2021-04-29T08:19:00.000Z",
        "voteCount": 2,
        "content": "B. Deploy the Azure Function in a Consumption plan and use an Event Grid trigger."
      },
      {
        "date": "2021-04-27T23:19:00.000Z",
        "voteCount": 4,
        "content": "\"B\" is correct because https://docs.microsoft.com/en-us/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp#event-grid-trigger"
      },
      {
        "date": "2021-05-25T05:02:00.000Z",
        "voteCount": 1,
        "content": "Agree with the propose solution as the url states the scenario to utilize event trigger"
      },
      {
        "date": "2021-04-14T05:46:00.000Z",
        "voteCount": 2,
        "content": "The solution is D - Deploy the Azure Function in an App Service plan and use an Event Grid trigger.\nApp Service plan  -  ... If you need low latency in your blob triggered functions, consider running your function app in an App Service plan.  SOURCE -&gt; https://docs.microsoft.com/en-us/azure/azure-functions/functions-create-storage-blob-triggered-function\n Event Grid trigger - ... The Event Grid trigger also has built-in support for blob events. Use Event Grid instead of the Blob storage trigger for the following scenarios:\n...\nHigh-scale: High scale can be loosely defined as containers that have more than 100,000 blobs in them or storage accounts that have more than 100 blob updates per second.\nSOURCE - &gt; https://docs.microsoft.com/en-gb/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp"
      },
      {
        "date": "2021-04-27T05:09:00.000Z",
        "voteCount": 2,
        "content": "Since minimizing costs is a requirement, and low latency is not, the correct answer should be a \"consumption plan\". Otherwise I agree with Event Grid Trigger, as this should be used if there are more than 100 000 blobs."
      },
      {
        "date": "2021-03-15T03:41:00.000Z",
        "voteCount": 2,
        "content": "When your function app runs in the default Consumption plan, there may be a delay of up to several minutes between the blob being added or updated and the function being triggered. If you need low latency in your blob triggered functions, consider running your function app in an App Service plan."
      },
      {
        "date": "2021-03-24T10:36:00.000Z",
        "voteCount": 1,
        "content": "\"You can also use an Event Grid trigger with your Blob storage account.\"\nhttps://docs.microsoft.com/en-gb/azure/azure-functions/functions-bindings-storage-blob-trigger?tabs=csharp"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/21521-exam-dp-201-topic-2-question-42-discussion/",
    "body": "You plan to migrate data to Azure SQL Database.<br>The database must remain synchronized with updates to Microsoft Azure and SQL Server.<br>You need to set up the database as a subscriber.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Data Tools",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Migration Assistant",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Agent for SQL Server 2017 or later\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Management Studio 17.9.1 or later"
    ],
    "answer": "E",
    "answerDescription": "To set up the database as a subscriber we need to configure database replication. You can use SQL Server Management Studio to configure replication. Use the latest versions of SQL Server Management Studio in order to be able to use all the features of Azure SQL Database.<br>Reference:<br>https://www.sqlshack.com/sql-server-database-migration-to-azure-sql-database-using-sql-server-transactional-replication/",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-08-13T22:32:00.000Z",
        "voteCount": 29,
        "content": "The answer E is correct. It's not asking how to migrate but what tool to use for setting up a transactional replication. \"Replication can be configured by using SQL Server Management Studio\". https://docs.microsoft.com/en-us/azure/azure-sql/database/replication-to-sql-database"
      },
      {
        "date": "2021-05-25T22:08:00.000Z",
        "voteCount": 1,
        "content": "This is the correct answer as it needs configuration on the SSMS side to perform the replication\n\nReference: https://www.sqlshack.com/sql-server-database-migration-to-azure-sql-database-using-sql-server-transactional-replication/"
      },
      {
        "date": "2020-06-25T09:52:00.000Z",
        "voteCount": 5,
        "content": "I think he means the updated versions of the services and tools not the data"
      },
      {
        "date": "2020-07-03T11:16:00.000Z",
        "voteCount": 2,
        "content": "Exactly"
      },
      {
        "date": "2023-12-26T10:15:00.000Z",
        "voteCount": 1,
        "content": "D. SQL Server Agent for SQL Server 2017 or later\n\nExplanation:\n\nSQL Server Agent: SQL Server Agent is a component of SQL Server that enables the scheduling and automation of administrative tasks, including replication. You can use SQL Server Agent to set up and manage replication subscriptions.\nAzure SQL Database supports various types of replication, such as transactional replication or snapshot replication, depending on your synchronization requirements. SQL Server Agent is commonly used for managing replication in SQL Server environments."
      },
      {
        "date": "2020-12-08T07:35:00.000Z",
        "voteCount": 3,
        "content": "E is the best answer though I would say SQL Data Sync is the right solution for this question"
      },
      {
        "date": "2020-08-22T13:27:00.000Z",
        "voteCount": 3,
        "content": "The given answer \"E: SQL Server Management Studio 17.9.1 or later\" is correct based on the remarks from: https://docs.microsoft.com/en-us/azure/azure-sql/database/replication-to-sql-database.\n\n 1. Replication can be configured by using SQL Server Management Studio or by executing Transact-SQL statements on the publisher. You cannot configure replication by using the Azure portal.\n2. Replication can only use SQL Server authentication logins to connect to Azure SQL Database."
      },
      {
        "date": "2020-06-17T05:39:00.000Z",
        "voteCount": 3,
        "content": "I Believe the answer should be SQL Data Sync which is not there in the options"
      },
      {
        "date": "2020-05-28T07:43:00.000Z",
        "voteCount": 3,
        "content": "I believe the answer should be B (SQL Data sync tool)\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-data-sync-data-sql-server-sql-database"
      },
      {
        "date": "2020-06-10T17:39:00.000Z",
        "voteCount": 3,
        "content": "That's now what B says but were it an option I agree SQL Data Sync would be correct"
      },
      {
        "date": "2020-08-13T22:20:00.000Z",
        "voteCount": 3,
        "content": "SQL Server Data Tools (SSDT) allows Visual Studio to create a SQL Server database project. It is nothing to do with migration/replication. It's a development tool."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/17755-exam-dp-201-topic-2-question-43-discussion/",
    "body": "HOTSPOT -<br>You are designing a solution for a company. You plan to use Azure Databricks.<br>You need to recommend workloads and tiers to meet the following requirements:<br>\u2711 Provide managed clusters for running production jobs.<br>\u2711 Provide persistent clusters that support auto-scaling for analytics processes.<br>\u2711 Provide role-based access control (RBAC) support for Notebooks.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0019700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0019700002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Data Engineering Only -<br>Box 2: Data Engineering and Data Analytics<br><br>Box 3: Standard -<br><br>Box 4: Data Analytics only -<br><br>Box 5: Premium -<br>Premium required for RBAC. Data Analytics Premium Tier provide interactive workloads to analyze data collaboratively with notebooks<br>Reference:<br>https://azure.microsoft.com/en-us/pricing/details/databricks/",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-23T02:30:00.000Z",
        "voteCount": 125,
        "content": "Based on MS documentation (https://azure.microsoft.com/en-us/pricing/details/databricks/) the answers should be: Box 1 = Data Engineering and Data Analytics, Box 2 = Data Analytics only, Box 3 = Standard, Box 4 = Data Engineering and Data Analytics, Box 5 = Premium"
      },
      {
        "date": "2020-05-13T18:17:00.000Z",
        "voteCount": 4,
        "content": "It's perfect."
      },
      {
        "date": "2020-05-13T18:18:00.000Z",
        "voteCount": 2,
        "content": "Filippo - It's perfect."
      },
      {
        "date": "2020-07-21T07:33:00.000Z",
        "voteCount": 2,
        "content": "I did validate too. it is perfect."
      },
      {
        "date": "2020-10-20T03:51:00.000Z",
        "voteCount": 2,
        "content": "Yes, perfect!"
      },
      {
        "date": "2021-05-25T22:31:00.000Z",
        "voteCount": 1,
        "content": "Perfect answer with citation of reference. Cheers!"
      },
      {
        "date": "2021-06-09T22:31:00.000Z",
        "voteCount": 2,
        "content": "To justify this solution feel free to check out the url below\n\nReference: https://www.azure.cn/en-us/pricing/details/databricks/"
      },
      {
        "date": "2020-06-30T18:46:00.000Z",
        "voteCount": 20,
        "content": "After spending too much time on examtopics, I'm a robot!"
      },
      {
        "date": "2021-09-05T13:29:00.000Z",
        "voteCount": 1,
        "content": "i thought Autoscaling requires premium??"
      },
      {
        "date": "2021-08-18T23:10:00.000Z",
        "voteCount": 2,
        "content": "The pricing plans have different names now and many of the features has been combined. So this question is not valid today."
      },
      {
        "date": "2021-07-15T03:34:00.000Z",
        "voteCount": 1,
        "content": "so many updates for same question so which answer is correct one ?"
      },
      {
        "date": "2021-06-20T14:12:00.000Z",
        "voteCount": 1,
        "content": "https://sql-stack.com/2018/11/29/azure-databricks-workloads-and-job-scheduling/\n\n1. Production Job (scheduled operations) - Data Engineering Only, Standard Tier\n2.  Persistent for analytics, auto-scaling - Data Analytics Only, Standard Tier\n3. Role-based access - For both Data Engineering and Data Analytics, Premium Tier"
      },
      {
        "date": "2021-03-19T03:20:00.000Z",
        "voteCount": 1,
        "content": "https://azure.microsoft.com/en-us/pricing/details/databricks/\nPersistent clusters for analytics is only available for All Purpose(data Analyst) Tier"
      },
      {
        "date": "2021-01-18T11:15:00.000Z",
        "voteCount": 7,
        "content": "Best article I have found that explains this: https://sql-stack.com/2018/11/29/azure-databricks-workloads-and-job-scheduling/#:~:text=The%20Data%20Engineering%20workload%20is,the%20duration%20of%20the%20job.&amp;text=This%20workload%20is%20also%20designed,the%20administrator%20in%20the%20workload.\n\n1. Data engineering and standard (data engineering is meant for jobs, will tear itself down)\n2. Data analytics and standard (is meant for ad-hoc analysis, will stay up)\n3. Data engineer and analytics and premium (RBAC is a premium feature)"
      },
      {
        "date": "2021-04-09T06:41:00.000Z",
        "voteCount": 1,
        "content": "perfect"
      },
      {
        "date": "2020-12-26T09:02:00.000Z",
        "voteCount": 5,
        "content": "Box 1: Data Engineering (Jobs Compute) and Data Analytics (All-Purpose Compute)\nBox 2: Data Engineering (Jobs Compute) and Data Analytics (All-Purpose Compute)\nBox 3: Standard\nBox 4: Data Engineering (Jobs Compute) and Data Analytics (All-Purpose Compute)\nBox 5: Premium"
      },
      {
        "date": "2020-12-22T10:30:00.000Z",
        "voteCount": 2,
        "content": "the original answer is correct"
      },
      {
        "date": "2020-12-09T01:33:00.000Z",
        "voteCount": 1,
        "content": "https://azure.microsoft.com/en-us/pricing/details/databricks/\nCredit to Filippo for the link and mapping of answers to the details in the link"
      },
      {
        "date": "2020-12-07T18:34:00.000Z",
        "voteCount": 1,
        "content": "from an efficiency point of view why cant we need Box 1 be  Data Engineering only (Jobs Light Compute), as its less price and does the production job."
      },
      {
        "date": "2020-08-21T05:09:00.000Z",
        "voteCount": 1,
        "content": "Box1 = Data Analytics &amp; Data Engineering\n\nBox2 = Data Analytics &amp; Data Engineering\nBox4 = Standard\n\nBox3 = Data Analytics &amp; Data Engineering\nBox5 = Premium"
      },
      {
        "date": "2020-08-21T05:22:00.000Z",
        "voteCount": 1,
        "content": "ignore this.\nBox1 = Data Engineering\n\nBox2 = Data Analytics only\nBox4 = Standard\n\nBox3 = Data Analytics only\nBox5 = Premium"
      },
      {
        "date": "2020-10-23T07:52:00.000Z",
        "voteCount": 6,
        "content": "ignore this!\nBox 1: Data Engineering and Data Analytics\nBox 2: Data Analytics only\nBox 3: Standard\nBox 4: Data Engineering and Data Analytics\nBox 5: Premium\n\nsource: https://azure.microsoft.com/en-us/pricing/details/databricks/"
      },
      {
        "date": "2020-10-20T02:05:00.000Z",
        "voteCount": 1,
        "content": "True source : https://databricks.com/fr/product/azure-pricing"
      },
      {
        "date": "2020-07-21T01:08:00.000Z",
        "voteCount": 3,
        "content": "Phew . these original  answers are messed up and confusing . even though we have correct answers in the link. the answers provided are totally different"
      },
      {
        "date": "2020-11-09T07:15:00.000Z",
        "voteCount": 5,
        "content": "https://azure.microsoft.com/en-us/pricing/details/databricks/\n\nReading this link, it looks like Data Analytics has been rebranded to \"All purpose Compute\", and Data Engineering to \"Jobs Compute\". In which case, the answers given by wak are correct."
      },
      {
        "date": "2020-07-16T18:49:00.000Z",
        "voteCount": 11,
        "content": "Box1 = Data Analytics &amp; Data Engineering\nBox2 = Data Analytics\nBox3 = Data Analytics &amp; Data Engineering\nBox4 = Standard\nBox5 = Premium\nAs per https://azure.microsoft.com/en-us/pricing/details/databricks/"
      },
      {
        "date": "2020-06-22T04:47:00.000Z",
        "voteCount": 1,
        "content": "As per https://azure.microsoft.com/en-us/pricing/details/databricks/ box 1,2,4 should be Data Engineering and Data Analytics, 5,4=Premium"
      },
      {
        "date": "2020-06-25T10:24:00.000Z",
        "voteCount": 1,
        "content": "I think Box 4 = Standard  as mentioned in the table and per @Filippo \nTable header is Standard tier features in the below link\n===============================================\nhttps://azure.microsoft.com/en-us/pricing/details/databricks/"
      },
      {
        "date": "2020-08-09T06:39:00.000Z",
        "voteCount": 1,
        "content": "Persistent clusters for analytics is a standard tier (box 4) feature for data analytics (box 3) workload. Filippo has the provided the correct answer."
      },
      {
        "date": "2020-06-10T17:43:00.000Z",
        "voteCount": 1,
        "content": "https://databricks.com/product/azure-pricing"
      },
      {
        "date": "2020-06-11T13:38:00.000Z",
        "voteCount": 2,
        "content": "Box 2 should be premium for - Optimized autoscaling of compute"
      },
      {
        "date": "2021-01-13T01:32:00.000Z",
        "voteCount": 1,
        "content": "Box 3 can be Standard. See the \"Autopilot\" option in the Standard plan on this link: https://databricks.com/product/azure-pricing"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51623-exam-dp-201-topic-2-question-44-discussion/",
    "body": "You design data engineering solutions for a company.<br>A project requires analytics and visualization of large set of data. The project has the following requirements:<br>\u2711 Notebook scheduling<br>\u2711 Cluster automation<br>\u2711 Power BI Visualization<br>You need to recommend the appropriate Azure service. Your solution must minimize the number of services required.<br>Which Azure service should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Batch",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight"
    ],
    "answer": "C",
    "answerDescription": "A databrick job is a way of running a notebook or JAR either immediately or on a scheduled basis.<br>Azure Databricks has two types of clusters: interactive and job. Interactive clusters are used to analyze data collaboratively with interactive notebooks. Job clusters are used to run fast and robust automated workloads using the UI or API.<br>You can visualize Data with Azure Databricks and Power BI Desktop.<br>Reference:<br>https://docs.azuredatabricks.net/user-guide/clusters/index.html https://docs.azuredatabricks.net/user-guide/jobs.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-02T18:05:00.000Z",
        "voteCount": 9,
        "content": "No doubts.. Databricks.. Correct answer!!!"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47165-exam-dp-201-topic-2-question-45-discussion/",
    "body": "HOTSPOT -<br>You design data engineering solutions for a company.<br>You must integrate on-premises SQL Server data into an Azure solution that performs Extract-Transform-Load (ETL) operations have the following requirements:<br>\u2711 Develop a pipeline that can integrate data and run notebooks.<br>\u2711 Develop notebooks to transform the data.<br>\u2711 Load the data into a massively parallel processing database for later analysis.<br>You need to recommend a solution.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0020000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0020100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-17T09:58:00.000Z",
        "voteCount": 29,
        "content": "I would rather have\nIntegrate on premises data to Cloud : ADF\nDevelop notebooks to Transform Data : DataBricks\nRun Notebooks  : ADF (Azure Databricks notebooks can be run within an ADF pipeline)\nLoad the Data : Use ADF to load the Data\nStore the Transformed Data: Azure Synapse Analyses"
      },
      {
        "date": "2021-04-09T03:57:00.000Z",
        "voteCount": 1,
        "content": "Exactly that was my take before seeing the solution."
      },
      {
        "date": "2021-05-25T22:53:00.000Z",
        "voteCount": 2,
        "content": "Azure databricks can handle the loading of data from the notebook to the external tables of Azure Synapse unless the requirement is explicitly to export the file to another storage then use of ADF is the appropriate"
      },
      {
        "date": "2021-05-13T14:10:00.000Z",
        "voteCount": 8,
        "content": "Given answer is right.\nRemember requirement: Load the data into a massively parallel processing database for later analysis.\nADF and Batch can work together.\nref: https://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-data-processing-using-batch"
      },
      {
        "date": "2021-06-05T14:37:00.000Z",
        "voteCount": 1,
        "content": "I am agree with you."
      },
      {
        "date": "2021-08-18T23:34:00.000Z",
        "voteCount": 2,
        "content": "Given Solution is 100% Correct.\n\nDo not confuse people with absurd arguments. I can do all the activities through Synapse Analysis also. That doesn't mean I will choose 5 times Synapse Analyses."
      },
      {
        "date": "2021-06-27T10:47:00.000Z",
        "voteCount": 1,
        "content": "Just one change Run notebook is better done from ADF as we can orchestrate the sequence better. When run from databricks, it may not know the time of data retrieveal and also the next step, Azure Batch cannot be called from ADB"
      },
      {
        "date": "2021-05-26T20:28:00.000Z",
        "voteCount": 3,
        "content": "Why note using Databricks to load the data? When the notebook finishes the process, it also can load the data into Synapse. Databricks can easily uploads results to Synapse, Azure SQL, and Azure Cosmos DB."
      },
      {
        "date": "2021-05-02T18:10:00.000Z",
        "voteCount": 4,
        "content": "Given Solution is correct.. no confusions..\nwhy anyone will use ADB to develop notebook and then use ADF to run them unless it is specifically specified ?"
      },
      {
        "date": "2021-11-26T14:21:00.000Z",
        "voteCount": 1,
        "content": "Because they were asking for a Data Engineering solution and having everything handled within one orchestration/etl tool makes definitely sense."
      },
      {
        "date": "2021-04-29T08:58:00.000Z",
        "voteCount": 3,
        "content": "Load the data - Azure data factory\ntransformed data-azure sql data warehouse"
      },
      {
        "date": "2021-04-25T08:05:00.000Z",
        "voteCount": 2,
        "content": "Shouldn't Load the data (Box 4) be Azure Synapse Analytics ? It's the only one with a MPP engine, which is exactly what is mentioned in the question"
      },
      {
        "date": "2021-04-09T03:59:00.000Z",
        "voteCount": 1,
        "content": "Why Azure Batch is better than ADF to load data?\nADF could be used to: Integrate from on-prem to azure, invoke notebook (developed in data bricks), then load data into warehouse, all within one pipeline."
      },
      {
        "date": "2021-06-05T14:36:00.000Z",
        "voteCount": 1,
        "content": "I guess for loading the data into a massively parallel processing database , azure data batch is the better solution.\nhttps://docs.microsoft.com/en-us/azure/data-factory/v1/data-factory-data-processing-using-batch"
      },
      {
        "date": "2021-03-15T10:17:00.000Z",
        "voteCount": 3,
        "content": "Regarding loading the data, I think Azure Data Factory could also be an appropriate answer."
      },
      {
        "date": "2021-03-15T04:15:00.000Z",
        "voteCount": 3,
        "content": "azure data factory could be used to load the data too"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49711-exam-dp-201-topic-2-question-46-discussion/",
    "body": "A company plans to use Apache Spark Analytics to analyze intrusion detection data.<br>You need to recommend a solution to analyze network and system activities for malicious activities and policy violations. The solution must minimize administrative efforts.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight"
    ],
    "answer": "D",
    "answerDescription": "With Azure HDInsight you can set up Azure Monitor alerts that will trigger when the value of a metric or the results of a query meet certain conditions. You can condition on a query returning a record with a value that is greater than or less than a certain threshold, or even on the number of results returned by a query. For example, you could create an alert to send an email if a Spark job fails or if a Kafka disk usage becomes over 90 percent full.<br>Reference:<br>https://azure.microsoft.com/en-us/blog/monitoring-on-azure-hdinsight-part-4-workload-metrics-and-logs/",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-04-09T06:56:00.000Z",
        "voteCount": 28,
        "content": "this is typical use case for Azure databricks that can use spark analytics paltform. so the answer should be databricks"
      },
      {
        "date": "2021-04-10T14:03:00.000Z",
        "voteCount": 4,
        "content": "I agree: https://azure.microsoft.com/es-es/blog/three-critical-analytics-use-cases-with-microsoft-azure-databricks/"
      },
      {
        "date": "2021-05-25T23:33:00.000Z",
        "voteCount": 1,
        "content": "Second agree with the propose solution"
      },
      {
        "date": "2023-12-26T10:29:00.000Z",
        "voteCount": 1,
        "content": "Azure Databricks: Azure Databricks is a fast, easy, and collaborative Apache Spark-based analytics platform. It simplifies the deployment and management of Apache Spark clusters, making it an excellent choice for large-scale data processing, including analyzing network and system activities for security purposes. It integrates with various Azure services and provides collaborative notebooks for data scientists and analysts."
      },
      {
        "date": "2021-08-18T23:38:00.000Z",
        "voteCount": 2,
        "content": "HDInsight is not in syllabus. So anytime you have a confusion with Databricks and HDInsight better go with Databricks."
      },
      {
        "date": "2021-08-17T00:06:00.000Z",
        "voteCount": 2,
        "content": "I will go with the databricks for the analytics of intrusion data!"
      },
      {
        "date": "2021-06-05T14:42:00.000Z",
        "voteCount": 2,
        "content": "Answer should be Azure databricks.\nIntrusion detection is one of use case for Azure databricks. \nhttps://azure.microsoft.com/es-es/blog/three-critical-analytics-use-cases-with-microsoft-azure-databricks/"
      },
      {
        "date": "2021-05-23T06:24:00.000Z",
        "voteCount": 1,
        "content": "The answer provided is correct."
      },
      {
        "date": "2021-05-27T19:30:00.000Z",
        "voteCount": 1,
        "content": "are you sure?"
      },
      {
        "date": "2021-06-02T07:23:00.000Z",
        "voteCount": 2,
        "content": "nope.. it's Databricks for sure"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49487-exam-dp-201-topic-2-question-47-discussion/",
    "body": "You are planning a streaming data solution that will use Azure Databricks. The solution will stream sales transaction data from an online store. The solution has the following specifications:<br>\u2711 The output data will contain items purchased, quantity, line total sales amount, and line total tax amount.<br>\u2711 Line total sales amount and line total tax amount will be aggregated in Databricks.<br>\u2711 Sales transactions will never be updated. Instead, new rows will be added to adjust a sale.<br>You need to recommend an output mode for the dataset that will be processed by using Structured Streaming. The solution must minimize duplicate data.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAppend",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tComplete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate"
    ],
    "answer": "A",
    "answerDescription": "Append Mode: Only new rows appended in the result table since the last trigger are written to external storage. This is applicable only for the queries where existing rows in the Result Table are not expected to change.<br>Incorrect Answers:<br>B: Complete Mode: The entire updated result table is written to external storage. It is up to the storage connector to decide how to handle the writing of the entire table.<br>C: Update Mode: Only the rows that were updated in the result table since the last trigger are written to external storage. This is different from Complete Mode in that Update Mode outputs only the rows that have changed since the last trigger. If the query doesn't contain aggregations, it is equivalent to Append mode.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/databricks/getting-started/spark/streaming",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-22T05:48:00.000Z",
        "voteCount": 5,
        "content": "Same question in DP 200"
      },
      {
        "date": "2022-08-08T06:36:00.000Z",
        "voteCount": 1,
        "content": "there will be a new line for each updated transaction (record). So 'append' is correct"
      },
      {
        "date": "2022-05-18T11:01:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is correct because it says data will be aggregated using Databricks. As far as the streaming mode The data is only being appended. Aggregations will be calculated separately."
      },
      {
        "date": "2021-06-27T05:23:00.000Z",
        "voteCount": 1,
        "content": "Sales transactions will never be updated --&gt; No updates meaning, no need to perform merge operation or updates, Hence append is the correct answer"
      },
      {
        "date": "2021-06-18T10:28:00.000Z",
        "voteCount": 2,
        "content": "The required conditions are confusing:\ncondition2-&gt; update\ncondition3-&gt; append"
      },
      {
        "date": "2021-04-07T05:02:00.000Z",
        "voteCount": 1,
        "content": "I think it should be update because of the possible new additions of new data to already copied rows. Any opinions?"
      },
      {
        "date": "2021-04-07T05:04:00.000Z",
        "voteCount": 5,
        "content": "sorry, I haven't read third condition. I think answer is correct."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53426-exam-dp-201-topic-2-question-48-discussion/",
    "body": "HOTSPOT -<br>You have an Azure event hub named retailhub that has 16 partitions. Transactions are posted to retailhub. Each transaction includes the transaction ID, the individual line items, and the payment details. The transaction ID is used as the partition key.<br>You are designing an Azure Stream Analytics job to identify potentially fraudulent transactions at a retail store. The job will use retailhub as the input. The job will output the transaction ID, the individual line items, the payment details, a fraud score, and a fraud indicator.<br>You plan to send the output to an Azure event hub named fraudhub.<br>You need to ensure that the fraud detection solution is highly scalable and processes transactions as quickly as possible.<br>How should you structure the output of the Stream Analytics job? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0020400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0020500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 16 -<br>For Event Hubs you need to set the partition key explicitly.<br>An embarrassingly parallel job is the most scalable scenario in Azure Stream Analytics. It connects one partition of the input to one instance of the query to one partition of the output.<br><br>Box 2: Transaction ID -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-features#partitions",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T09:24:00.000Z",
        "voteCount": 9,
        "content": "Given answer and explanation are correct."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49409-exam-dp-201-topic-2-question-49-discussion/",
    "body": "DRAG DROP -<br>You have a CSV file in Azure Blob storage. The file does NOT have a header row.<br>You need to use Azure Data Factory to copy the file to an Azure SQL database. The solution must minimize how long it takes to copy the file.<br>How should you configure the copy process? To answer, drag the appropriate components to the correct locations. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0020600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0020700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Input: A delimited text dataset that has a comma a column delimiter columnDelimiter: The character(s) used to separate columns in a file.<br>The default value is comma ,. When the column delimiter is defined as empty string, which means no delimiter, the whole line is taken as a single column.<br>Pipeline: A data flow activity that has a general purpose compute type<br>When you're transforming data in mapping data flows, you can read and write files from Azure Blob storage.<br>Output: A copy activity that has an explicit schema mapping<br>Use Copy Activity in Azure Data Factory to copy data from and to Azure SQL Database, and use Data Flow to transform data in Azure SQL Database.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/format-delimited-text https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-sql-database",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-06T07:20:00.000Z",
        "voteCount": 62,
        "content": "2n box: copy activity\n3rd box: azure sqldb dataset"
      },
      {
        "date": "2021-06-01T12:24:00.000Z",
        "voteCount": 20,
        "content": "Input: A delimited text dataset...\nOutput: Azure SQL DB dataset...\nPipeline: Copy Activity ...."
      },
      {
        "date": "2021-06-02T08:47:00.000Z",
        "voteCount": 9,
        "content": "I have performed this activity during my course. I am 200% sure that second one is copy activity an third one is sql db with fixed schema"
      },
      {
        "date": "2021-05-25T23:56:00.000Z",
        "voteCount": 1,
        "content": "First layer corresponds to linked services while the second layer is for the dataset and lastly the pipeline level. Therefore second layer is CSV and Azure SQL Database then copy activity."
      },
      {
        "date": "2021-04-09T06:53:00.000Z",
        "voteCount": 7,
        "content": "This is completely wrong. Both middle boxes are an abstraction layer, so if left box is a dataset of input, then right box is a dataset for output. There is no requirement to transform the data, only copy, so pipeline consists only of copy activity. If we were using data flow to copy, then we would not need copy activity, because data flow could copy directly to sql database, but requirement is performance and data flow need to start up the cluster that it is run at, while copy activity works instantly."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48070-exam-dp-201-topic-2-question-50-discussion/",
    "body": "DRAG DROP -<br>You are planning a design pattern based on the Lambda architecture as shown in the exhibit.<br><img src=\"/assets/media/exam-media/03774/0020800001.png\" class=\"in-exam-image\"><br>Which Azure services should you use for the cold path? To answer, drag the appropriate services to the correct layers. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0020900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0020900002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Layer 2: Azure Data Lake Storage Gen2<br>Layer 3: Azure Synapse Analytics<br>Azure Synapse Analytics can be used for batch processing.<br>Note: Layer 1 = speed layer, layer 2 = batch layer, layer 3 = serving layer<br>Note 2: Lambda architectures use batch-processing, stream-processing, and a serving layer to minimize the latency involved in querying big data.<br><img src=\"/assets/media/exam-media/03774/0021000001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://azure.microsoft.com/en-us/blog/lambda-architecture-using-azure-cosmosdb-faster-performance-low-tco-low-devops/ https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/batch-processing",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-24T02:40:00.000Z",
        "voteCount": 8,
        "content": "Correct answer"
      },
      {
        "date": "2021-05-25T23:57:00.000Z",
        "voteCount": 2,
        "content": "Entirely correct"
      },
      {
        "date": "2021-06-10T18:46:00.000Z",
        "voteCount": 2,
        "content": "Layer 2 : Azure data lake GEN2\nLayer 3: ASA"
      },
      {
        "date": "2021-06-10T18:48:00.000Z",
        "voteCount": 1,
        "content": "Sorry ASA : Azure Synapse Analytics"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53387-exam-dp-201-topic-2-question-51-discussion/",
    "body": "You need to recommend an Azure Cosmos DB solution that meets the following requirements:<br>\u2711 All data that was NOT modified during the last 30 days must be purged automatically.<br>\u2711 The solution must NOT affect ongoing user requests.<br>What should you recommend using to purge the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Cosmos DB stored procedure executed by an Azure logic app",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Cosmos DB REST API Delete Document operation called by an Azure function",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTime To Live (TTL) setting in Azure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Cosmos DB change feed queried by an Azure function"
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/time-to-live",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T01:43:00.000Z",
        "voteCount": 7,
        "content": "answer is CORRECT"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53388-exam-dp-201-topic-2-question-52-discussion/",
    "body": "You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java.<br>Which service should you recommend using to process the streaming data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-23T01:48:00.000Z",
        "voteCount": 5,
        "content": "answer is CORRECT\n--&gt;&gt; Apache Spark in Azure Databricks"
      },
      {
        "date": "2022-03-26T19:20:00.000Z",
        "voteCount": 2,
        "content": "Stream analytics does not integrate with Kafka. Look at the documentation provided. Therefore, the answer is Databricks"
      },
      {
        "date": "2021-09-21T06:42:00.000Z",
        "voteCount": 1,
        "content": "is it due to fact that jar can be uploaded and used in adb. Question mentions about java developers. think that is reason to pick adb instead of stream analytics"
      },
      {
        "date": "2021-09-14T08:06:00.000Z",
        "voteCount": 1,
        "content": "Why not Azure Stream Analytics?"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55598-exam-dp-201-topic-2-question-53-discussion/",
    "body": "DRAG DROP -<br>Your company has a custom human resources (HR) app named App1 that is deployed to mobile devices.<br>You are designing a solution that will use real-time metrics to indicate how employees use App1. The solution must meet the following requirements:<br>\u2711 Use hierarchy data exported monthly from the company's HR enterprise application.<br>\u2711 Process approximately 1 GB of telemetry data per day.<br>\u2711 Minimize costs.<br>You need to recommend which Azure services to use to meet the requirements.<br>Which two services should you recommend? To answer, drag the appropriate services to the correct targets. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03774/0021200004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0021200005.png\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-18T21:36:00.000Z",
        "voteCount": 6,
        "content": "the proposed answer makes sense"
      },
      {
        "date": "2022-03-26T19:23:00.000Z",
        "voteCount": 1,
        "content": "Why PowerBI and not Synapse Analytics. The question makes no mention of dashboards. Dashboards is the last step. You have to analyze the data before you can produce dashboards. So it should be Stream + Synapse"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52104-exam-dp-201-topic-2-question-54-discussion/",
    "body": "You are planning an Azure solution that will aggregate streaming data.<br>The input data will be retrieved from tab-separated values (TSV) files in Azure Blob storage.<br>You need to output the maximum value from a specific column for every two-minute period in near real-time. The output must be written to Blob storage as a<br>Parquet file.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory and mapping data flows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory and wrangling data flows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics window functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks and Apache Spark SQL window functions"
    ],
    "answer": "C",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-define-outputs#parquet-output-batching-window-properties",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-26T00:09:00.000Z",
        "voteCount": 13,
        "content": "Appropriate answer is C as the Azure Stream Analytics can perform aggregation syntax using MAX then output it in blob storage/ADLS Gen2"
      },
      {
        "date": "2021-06-18T21:38:00.000Z",
        "voteCount": 1,
        "content": "The Databricks spark SQL by default works with batch mode. It won't fulfill the near real-time requirement"
      },
      {
        "date": "2021-05-28T12:24:00.000Z",
        "voteCount": 1,
        "content": "Both ASA and Databricks should be able to do it now"
      },
      {
        "date": "2021-05-07T16:07:00.000Z",
        "voteCount": 3,
        "content": "I think correct answer is D"
      },
      {
        "date": "2021-06-14T22:12:00.000Z",
        "voteCount": 1,
        "content": "Make no SENSE"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52113-exam-dp-201-topic-2-question-55-discussion/",
    "body": "You are designing a statistical analysis solution that will use custom proprietary Python functions on near real-time data from Azure Event Hubs.<br>You need to recommend which Azure service to use to perform the statistical analysis. The solution must minimize latency.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "B",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/event-hubs/process-data-azure-stream-analytics<br>Design for data security and compliance",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-07T21:01:00.000Z",
        "voteCount": 32,
        "content": "Shouldn't be C (Databricks)? I think that the keyword is Python."
      },
      {
        "date": "2021-05-26T00:12:00.000Z",
        "voteCount": 3,
        "content": "Provided with the limited option, Azure Databricks is the appropriate solution as it can accommodate Python script"
      },
      {
        "date": "2021-05-22T09:23:00.000Z",
        "voteCount": 14,
        "content": "https://docs.microsoft.com/en-us/azure/stream-analytics/functions-overview\n\nAzure Stream Analytics supports the following four function types:\n\nJavaScript user-defined functions\nJavaScript user-defined aggregates\nC# user-defined functions (using Visual Studio)\nAzure Machine Learning\n\nPython is not on list so Databricks must be the right choice \n\nAnswer: C"
      },
      {
        "date": "2021-11-11T06:57:00.000Z",
        "voteCount": 2,
        "content": "why not A)Azure Synapse Analytics? support the python and nearl real time instead databricks is batch, or not?"
      },
      {
        "date": "2021-10-11T21:06:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is B\nhttps://docs.microsoft.com/en-us/azure/event-hubs/event-hubs-python-get-started-send"
      },
      {
        "date": "2021-08-19T13:17:00.000Z",
        "voteCount": 1,
        "content": "I agree it should be Databricks. Nowhere in Microsoft documentation does it say Azre Stream Analytics supports python. \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/stream-processing\n\nPython Reference document to show all integrated Azure services also does not list Azure Stream Analytics. \nhttps://docs.microsoft.com/en-us/python/api/overview/azure/?view=azure-python\n\nIf someone finds something updated, please post!"
      },
      {
        "date": "2021-06-18T21:39:00.000Z",
        "voteCount": 1,
        "content": "Should be the Databricks which has better support for UDF in python"
      },
      {
        "date": "2021-06-09T19:39:00.000Z",
        "voteCount": 1,
        "content": "You have to perform statistical analysis that what question says. Means what ever date you get on that you need to do it. I have found the link https://stackoverflow.com/questions/58097539/execute-azure-steaming-analytics-queries-from-a-python-script which talks about executing queries using python. The answer seems correct."
      },
      {
        "date": "2021-06-02T09:34:00.000Z",
        "voteCount": 1,
        "content": "Custom proprietary functions are written manually which need to be packaged and imported so yes databricks"
      },
      {
        "date": "2021-06-02T08:29:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct. ASA supports python https://docs.microsoft.com/en-us/python/api/overview/azure/mgmt-streamanalytics-readme?view=azure-python-preview"
      },
      {
        "date": "2021-05-25T17:03:00.000Z",
        "voteCount": 2,
        "content": "Agree, Databricks should be the answer"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "2"
  }
]