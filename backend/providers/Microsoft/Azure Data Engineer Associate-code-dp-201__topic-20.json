[
  {
    "topic": 20,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/20482-exam-dp-201-topic-20-question-1-discussion/",
    "body": "You need to design a solution that meets the business requirements of Health Insights.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB that uses the Gremlin API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB that uses the SQL API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks"
    ],
    "answer": "D",
    "answerDescription": "Azure Synapse Analytics is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use SQL Data Warehouse as a key component of a big data solution.<br>You can access Azure Synapse Analytics (SQL DW) from Databricks using the SQL Data Warehouse connector (referred to as the SQL DW connector), a data source implementation for Apache Spark that uses Azure Blob Storage, and PolyBase in SQL DW to transfer large volumes of data efficiently between a<br>Databricks cluster and a SQL DW instance.<br>Scenario: ADatum identifies the following requirements for the Health Insights application:<br>\u2711 The new Health Insights application must be built on a massively parallel processing (MPP) architecture that will support the high performance of joins on large fact tables<br>Reference:<br>https://docs.databricks.com/data/data-sources/azure/sql-data-warehouse.html",
    "votes": [],
    "comments": [
      {
        "date": "2020-05-13T08:02:00.000Z",
        "voteCount": 12,
        "content": "It should be azure data warehouse (or actually synapse analytics now)"
      },
      {
        "date": "2020-05-17T01:26:00.000Z",
        "voteCount": 5,
        "content": "True\nSince that option is not available, next best option would be Azure Databricks which can access Azure Synapse Anlytics through Azure Synapse Connector I guess"
      },
      {
        "date": "2020-12-04T11:39:00.000Z",
        "voteCount": 1,
        "content": "I think question is asking for design solution for storing the data .. so Databricks should be fine i guess ..."
      },
      {
        "date": "2020-06-17T19:11:00.000Z",
        "voteCount": 10,
        "content": "isn't the answer B - ADF can be used to make data available into SQLDW as per requirement\"The data from Health Interface and Health Review must be available in Health Insights within 15 minutes of being committed.\""
      },
      {
        "date": "2021-01-16T10:39:00.000Z",
        "voteCount": 5,
        "content": "I pick databricks over ADF because it need to grab both streaming (the interface data) and batch data."
      },
      {
        "date": "2021-01-14T05:20:00.000Z",
        "voteCount": 2,
        "content": "I had this question in my exam. I believe Databricks is the best answer."
      },
      {
        "date": "2020-12-22T17:29:00.000Z",
        "voteCount": 1,
        "content": "\"Minimize the number of services required to perform data processing, development, scheduling, monitoring, and the operationalizing of pipelines\" - ADF best fits this requirement so my answer is B"
      },
      {
        "date": "2020-12-26T19:22:00.000Z",
        "voteCount": 1,
        "content": "Can I use ADF only for solution of both Health Insights and Health Interface?"
      },
      {
        "date": "2021-01-16T10:38:00.000Z",
        "voteCount": 2,
        "content": "databricks fits that description as well."
      },
      {
        "date": "2020-12-08T03:07:00.000Z",
        "voteCount": 2,
        "content": "From https://docs.databricks.com/data/data-sources/azure/synapse-analytics.html:\n\"You can access Azure Synapse from Databricks using the Azure Synapse connector\"\nDatabricks it is then"
      },
      {
        "date": "2020-10-27T03:42:00.000Z",
        "voteCount": 2,
        "content": "Still not convinced why ADF is not the answer instead of ADB? I think both can be used as part of the solution. -- Data is ingested from Health Interface which should support scalable batch processing (ADB) and the data should be transferred to the Insight in 15 minutes intervals (ADF). So either both answers are correct or question is faulty and has been changed by now."
      },
      {
        "date": "2020-08-22T14:57:00.000Z",
        "voteCount": 3,
        "content": "How about Cosmos with Sql Api option C ??"
      },
      {
        "date": "2020-09-28T01:40:00.000Z",
        "voteCount": 4,
        "content": "Cosmos DB supports only self joins. But health insight app shold support joins on fact tables. Hence cosmos db is ruled out"
      },
      {
        "date": "2020-08-15T01:59:00.000Z",
        "voteCount": 1,
        "content": "From \"The data from Health Interface and Health Review must be available in Health Insights within 15 minutes of being committed.\" we have two options: Azure Data Factory and Azure Databricks. \nThen we need to be able to analyze events, so Azure Databricks is the answer."
      },
      {
        "date": "2020-08-02T04:05:00.000Z",
        "voteCount": 1,
        "content": "Answer B? \n\nAs described by Health Insights Requirements: the solution shall provide a data warehouse and the ingestion into the data warehouse. ADF might be the solution for the ingestion past."
      },
      {
        "date": "2020-12-26T19:15:00.000Z",
        "voteCount": 1,
        "content": "I cannot see the requirement of ingestion into the data warehouse."
      },
      {
        "date": "2020-06-21T11:33:00.000Z",
        "voteCount": 2,
        "content": "where is the option for synapse? the answer should be synapse"
      },
      {
        "date": "2020-06-05T13:45:00.000Z",
        "voteCount": 1,
        "content": "massively parallel processing (MPP) architecture -&gt; SQL DW but its not part of the answer options here"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46877-exam-dp-201-topic-20-question-2-discussion/",
    "body": "HOTSPOT -<br>Which Azure data storage solution should you recommend for each application? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0002900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0003000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Health Review: Azure SQL Database<br>Scenario: ADatum identifies the following requirements for the Health Review application:<br>Ensure that sensitive health data is encrypted at rest and in transit.<br><img src=\"/assets/media/exam-media/03774/0003000002.png\" class=\"in-exam-image\"><br>\u2711 Tag all the sensitive health data in Health Review. The data will be used for auditing.<br>Health Interface:  Azure Cosmos DB<br>ADatum identifies the following requirements for the Health Interface application:<br>\u2711 Upgrade to a data storage solution that will provide flexible schemas and increased throughput for writing data. Data must be regionally located close to each hospital, and reads must display be the most recent committed version of an item.<br>\u2711 Reduce the amount of time it takes to add data from new hospitals to Health Interface.<br>\u2711 Support a more scalable batch processing solution in Azure.<br>\u2711 Reduce the amount of development effort to rewrite existing SQL queries.<br>Health Insights: Azure Synapse Analytics<br>Azure Synapse Analytics is a cloud-based enterprise data warehouse that leverages massively parallel processing (MPP) to quickly run complex queries across petabytes of data. Use SQL Data Warehouse as a key component of a big data solution.<br>You can access Azure Synapse Analytics (SQL DW) from Databricks using the SQL Data Warehouse connector (referred to as the SQL DW connector), a data source implementation for Apache Spark that uses Azure Blob Storage, and PolyBase in SQL DW to transfer large volumes of data efficiently between a<br>Databricks cluster and a SQL DW instance.<br>Scenario: ADatum identifies the following requirements for the Health Insights application:<br>\u2711 The new Health Insights application must be built on a massively parallel processing (MPP) architecture that will support the high performance of joins on large fact tables<br>Reference:<br>https://docs.databricks.com/data/data-sources/azure/sql-data-warehouse.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-05T03:24:00.000Z",
        "voteCount": 12,
        "content": "Do not read other comments(save time), the answer is correct."
      },
      {
        "date": "2021-03-13T06:21:00.000Z",
        "voteCount": 4,
        "content": "It should be cosmosdb for health review as well because of encyption at rest and transit"
      },
      {
        "date": "2021-03-23T08:15:00.000Z",
        "voteCount": 1,
        "content": "oltp synaps .?"
      },
      {
        "date": "2021-04-13T05:47:00.000Z",
        "voteCount": 1,
        "content": "Nah, always encrypted will do the trick"
      },
      {
        "date": "2021-04-20T04:40:00.000Z",
        "voteCount": 3,
        "content": "No, refer - https://docs.microsoft.com/en-us/azure/security/fundamentals/encryption-atrest#:~:text=Azure%20SQL%20Database%20currently%20supports,feature%20called%20Transparent%20Data%20Encryption."
      },
      {
        "date": "2021-05-09T08:07:00.000Z",
        "voteCount": 2,
        "content": "you can enable always encrypted on SQL DB."
      },
      {
        "date": "2021-05-28T19:35:00.000Z",
        "voteCount": 1,
        "content": "Well, Azure SQL is also encrypted in transit without using Always Encrypted."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53452-exam-dp-201-topic-20-question-3-discussion/",
    "body": "You need to recommend a solution that meets the data platform requirements of Health Interface. The solution must minimize redevelopment efforts for the application.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB that uses the SQL API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB that uses the Table API"
    ],
    "answer": "C",
    "answerDescription": "Scenario: ADatum identifies the following requirements for the Health Interface application:<br>\u2711 Reduce the amount of development effort to rewrite existing SQL queries.<br>\u2711 Upgrade to a data storage solution that will provide flexible schemas and increased throughput for writing data. Data must be regionally located close to each hospital, and reads must display be the most recent committed version of an item.<br>\u2711 Reduce the amount of time it takes to add data from new hospitals to Health Interface.<br>\u2711 Support a more scalable batch processing solution in Azure.",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-13T08:04:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/cosmos-db/how-to-provision-database-throughput?tabs=dotnetv2"
      },
      {
        "date": "2021-06-13T08:03:00.000Z",
        "voteCount": 2,
        "content": "100 % CORRECT"
      },
      {
        "date": "2021-05-23T19:55:00.000Z",
        "voteCount": 2,
        "content": "%100 CORRECT"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/35290-exam-dp-201-topic-20-question-4-discussion/",
    "body": "Which consistency level should you use for Health Interface?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsistent Prefix",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSession",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBounded Staleness",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStrong"
    ],
    "answer": "D",
    "answerDescription": "Scenario: ADatum identifies the following requirements for the Health Interface application:<br>\u2711 ..reads must display be the most recent committed version of an item.<br>Azure Cosmos DB consistency levels include:<br>Strong: Strong consistency offers a linearizability guarantee. Linearizability refers to serving requests concurrently. The reads are guaranteed to return the most recent committed version of an item. A client never sees an uncommitted or partial write. Users are always guaranteed to read the latest committed write.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-01T07:38:00.000Z",
        "voteCount": 13,
        "content": "\"and reads must display be the most recent committed version of an item\" --&gt; Strong"
      },
      {
        "date": "2021-06-06T09:39:00.000Z",
        "voteCount": 1,
        "content": "surely strong should be the consistency level"
      },
      {
        "date": "2021-04-09T01:27:00.000Z",
        "voteCount": 2,
        "content": "I think answer is correct if we assume all data is written only to Los Angeles (where are located all its apps) and the other regions only read from it. In this case (single writer/multiple readers), it may be. The problem is that is unclear."
      },
      {
        "date": "2021-02-24T06:15:00.000Z",
        "voteCount": 3,
        "content": "The following requirements make sure that the answer must be Session: \"reads must display be the most recent committed version of an item.\" and \"reduce the amount of time it takes to add data from new hospitals to Health Interface\" \nWith Session consistency, reads will never be out of order and it minimizes the latency"
      },
      {
        "date": "2021-01-20T01:03:00.000Z",
        "voteCount": 2,
        "content": "We cant use Strong on Dallas, New York and Los Angeles at the same time so D is wrong. A is obviously wrong cause it doesn't talk about low write latency and we only have to choose between B and C. Although C gives higher write latency, the read operation is not exactly 15 minutes and depends upon the number of updates which can change from 5 minutes to 1 day where Session gives us the RPO of less than 15 minutes guarantee which is reliable accordingly to the requirements. So the answer is C."
      },
      {
        "date": "2021-01-17T09:33:00.000Z",
        "voteCount": 2,
        "content": "Definitely Strong"
      },
      {
        "date": "2020-12-08T02:56:00.000Z",
        "voteCount": 1,
        "content": "I would pick Bounded staleness\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/consistency-levels\nFor multiple write regions, Bounded staleness is recommended; you can find the table towards the end"
      },
      {
        "date": "2020-12-08T03:00:00.000Z",
        "voteCount": 3,
        "content": "My bad please disregard; Strong is the answer"
      },
      {
        "date": "2020-12-01T05:17:00.000Z",
        "voteCount": 4,
        "content": "You can't have multi region writes and Strong Consistancy"
      },
      {
        "date": "2021-04-11T05:56:00.000Z",
        "voteCount": 1,
        "content": "Never was said all region were writers. If you replicate data to all regions (one writer - multiple readers) you can use strong consistency."
      },
      {
        "date": "2021-04-11T06:02:00.000Z",
        "voteCount": 1,
        "content": "... but \u201cincreased throughput for writing data\u201d may suggest multiple writers. In this case, I\u2019ll get session consistency because is better than Bounded staleness."
      },
      {
        "date": "2020-10-27T03:49:00.000Z",
        "voteCount": 2,
        "content": "And \"reduce the amount of time it takes to add data from new hospitals to Health Interface\" by configuring multiple write-regions."
      },
      {
        "date": "2020-11-29T01:58:00.000Z",
        "voteCount": 4,
        "content": "The given answer for this question is correct as the most recent committed version of an item so it is Strong Consistency.\nBut the reason you are providing is not. This sentence \"reduce the amount of time it takes to add data from new hospitals to Health Interface\" is a sign to use Cosmo DB as a database solution. This means that new hospital can be with variant columns and format and so we need to provide a flexible schema solution (JSON Document etc) and so Cosmo DB with SQL API is the answer. Don't confuse it with multiple write-region."
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "20"
  },
  {
    "topic": 20,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/46737-exam-dp-201-topic-20-question-5-discussion/",
    "body": "HOTSPOT -<br>You need to design the storage for the Health Insights data platform.<br>Which types of tables should you include in the design? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03774/0003300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03774/0003400001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash-distributed tables -<br>The new Health Insights application must be built on a massively parallel processing (MPP) architecture that will support the high performance of joins on large fact tables.<br>Hash-distributed tables improve query performance on large fact tables.<br>Box 2: Round-robin distributed tables<br>A round-robin distributed table distributes table rows evenly across all distributions. The assignment of rows to distributions is random.<br>Scenario:<br>ADatum identifies the following requirements for the Health Insights application:<br>\u2711 The new Health Insights application must be built on a massively parallel processing (MPP) architecture that will support the high performance of joins on large fact tables.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-distribute<br>Design Azure data storage solutions",
    "votes": [],
    "comments": [
      {
        "date": "2021-03-12T09:12:00.000Z",
        "voteCount": 28,
        "content": "Data Dimension should be replicated tables"
      },
      {
        "date": "2021-04-09T01:35:00.000Z",
        "voteCount": 2,
        "content": "Round robin improve data loading, which is needed. Data from Interface and Review must be loading in less than 15 minutes."
      },
      {
        "date": "2021-05-03T06:43:00.000Z",
        "voteCount": 7,
        "content": "Round Robin is used to increase staging performance. It doesn''t make sense to use this on Dimension tables who tend to be relatively small and don't change often. So load performance is less of an issue. Query performance however will benefit from replicated tables. Therefore I would go for replicated tables as well"
      },
      {
        "date": "2021-06-24T05:58:00.000Z",
        "voteCount": 1,
        "content": "If the table size is &lt; 2 GB, we should always go with Replicated Tables. As this is cached in all the nodes therefore reduces data movement across nodes."
      },
      {
        "date": "2021-06-06T09:41:00.000Z",
        "voteCount": 1,
        "content": "Round Robin is bad option for Dim tables. Idea is to use replicated tables for dimensions to improve performance"
      },
      {
        "date": "2021-03-12T17:59:00.000Z",
        "voteCount": 2,
        "content": "I agree, unless the part says used with multiple fact tables' joins means something else"
      }
    ],
    "examNameCode": "dp-201",
    "topicNumber": "20"
  }
]