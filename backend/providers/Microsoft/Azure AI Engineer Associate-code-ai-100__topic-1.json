[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51486-exam-ai-100-topic-1-question-1-discussion/",
    "body": "HOTSPOT -<br>You are designing an application to parse images of business forms and upload the data to a database. The upload process will occur once a week.<br>You need to recommend which services to use for the application. The solution must minimize infrastructure costs.<br>Which services should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0000200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0000300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Cognitive Services -<br>Azure Cognitive Services include image-processing algorithms to smartly identify, caption, index, and moderate your pictures and videos.<br>Not: Azure Linguistic Analytics API, which provides advanced natural language processing over raw text.<br><br>Box 2: Azure Data Factory -<br>The Azure Data Factory (ADF) is a service designed to allow developers to integrate disparate data sources.  It is a platform somewhat like SSIS in the cloud to manage the data you have both on-prem and in the cloud.<br>It provides access to on-premises data in SQL Server and cloud data in Azure Storage (Blob and Tables) and Azure SQL Database.<br>Reference:<br>https://azure.microsoft.com/en-us/services/cognitive-services/ https://www.jamesserra.com/archive/2014/11/what-is-azure-data-factory/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-01T23:08:00.000Z",
        "voteCount": 5,
        "content": "Azure Cognitive Service and Azure Functions"
      },
      {
        "date": "2021-05-11T20:30:00.000Z",
        "voteCount": 2,
        "content": "Same, why would you not use Data Factory?"
      },
      {
        "date": "2021-05-09T13:38:00.000Z",
        "voteCount": 1,
        "content": "why not Data Factory?"
      },
      {
        "date": "2021-05-25T03:49:00.000Z",
        "voteCount": 2,
        "content": "You cannot use azure functions to store data in. It's not a database, rather it's serverless code to help applications running.\n\nhttps://docs.microsoft.com/en-us/azure/azure-functions/functions-overview"
      },
      {
        "date": "2021-06-23T23:09:00.000Z",
        "voteCount": 1,
        "content": "Beceause it says: \"Upload the data to the database\" and not \"The database to store the image in\". Soooo... what would you probably use to \"upload\" the data. I think Wesley0312 is right."
      },
      {
        "date": "2023-01-15T06:30:00.000Z",
        "voteCount": 1,
        "content": "The criteria is to reduce infrastructure costs. So functions app would be the choice."
      },
      {
        "date": "2021-05-20T04:22:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/5157-exam-ai-100-topic-1-question-2-discussion/",
    "body": "HOTSPOT -<br>You plan to deploy an Azure Data Factory pipeline that will perform the following:<br>\u2711 Move data from on-premises to the cloud.<br>\u2711 Consume Azure Cognitive Services APIs.<br>You need to recommend which technologies the pipeline should use. The solution must minimize custom code.<br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0000400003.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0000500001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Self-hosted Integration Runtime<br>A self-hosted IR is capable of running copy activity between a cloud data stores and a data store in private network.<br>Not Azure-SSIS Integration Runtime, as you would need to write custom code.<br><br>Box 2: Azure Logic Apps -<br>Azure Logic Apps helps you orchestrate and integrate different services by providing 100+ ready-to-use connectors, ranging from on-premises SQL Server or SAP to Microsoft Cognitive Services.<br>Incorrect:<br>Not Azure API Management: Use Azure API Management as a turnkey solution for publishing APIs to external and internal customers.<br>References:<br>https://docs.microsoft.com/en-us/azure/data-factory/concepts-integration-runtime https://docs.microsoft.com/en-us/azure/logic-apps/logic-apps-examples-and-scenarios",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-15T06:39:00.000Z",
        "voteCount": 1,
        "content": "Logic apps is low/no code solution"
      },
      {
        "date": "2021-04-14T10:05:00.000Z",
        "voteCount": 2,
        "content": "1. Self-Hosted Integration Runtime\n2. The solution must minimize custom code based on this I would go with Logic Apps"
      },
      {
        "date": "2020-10-05T10:31:00.000Z",
        "voteCount": 4,
        "content": "proof for second https://docs.microsoft.com/en-us/azure/azure-functions/functions-twitter-email"
      },
      {
        "date": "2020-09-30T20:27:00.000Z",
        "voteCount": 2,
        "content": "Self-Hosted Integration Runtime (SHIR) service to connect on-premises and Azure data sources. \nFor Cognitive services, Azure Webjobs is appropriate answer (per my understanding)"
      },
      {
        "date": "2021-01-11T10:45:00.000Z",
        "voteCount": 2,
        "content": "WebJobs need more custom code than Logic Apps."
      },
      {
        "date": "2020-04-07T08:24:00.000Z",
        "voteCount": 2,
        "content": "Yes, self-hosted integration runtime is the correct answer here."
      },
      {
        "date": "2019-09-14T03:29:00.000Z",
        "voteCount": 3,
        "content": "Correct answer\nADF leverages a Self-Hosted Integration Runtime (SHIR) service to connect on-premises and Azure data sources. SHIR can run copy activities between a cloud data store and a data store in a private network, and it can dispatch transform activities against compute resources in an on-premises network or an Azure virtual network."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25532-exam-ai-100-topic-1-question-3-discussion/",
    "body": "HOTSPOT -<br>You need to build an interactive website that will accept uploaded images, and then ask a series of predefined questions based on each image.<br>Which services should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0000600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0000700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Bot Service -<br><br>Box 2: Computer Vision -<br>The Computer Vision Analyze an image feature, returns information about visual content found in an image. Use tagging, domain-specific models, and descriptions in four languages to identify content and label it with confidence. Use Object Detection to get location of thousands of objects within an image. Apply the adult/racy settings to help you detect potential adult content. Identify image types and color schemes in pictures.<br>References:<br>https://azure.microsoft.com/en-us/services/cognitive-services/computer-vision/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-31T00:48:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-01-09T09:32:00.000Z",
        "voteCount": 3,
        "content": "correct. Verified,"
      },
      {
        "date": "2020-12-14T03:19:00.000Z",
        "voteCount": 1,
        "content": "The Answer is Correct"
      },
      {
        "date": "2020-07-12T16:55:00.000Z",
        "voteCount": 3,
        "content": "Seems to be correct."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23620-exam-ai-100-topic-1-question-4-discussion/",
    "body": "You are designing an AI solution that will analyze millions of pictures.<br>You need to recommend a solution for storing the pictures. The solution must minimize costs.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Data Lake store",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure File Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage"
    ],
    "answer": "C",
    "answerDescription": "Data Lake will be a bit more expensive although they are in close range of each other. Blob storage has more options for pricing depending upon things like how frequently you need to access your data (cold vs hot storage).<br>Reference:<br>http://blog.pragmaticworks.com/azure-data-lake-vs-azure-blob-storage-in-data-warehousing",
    "votes": [],
    "comments": [
      {
        "date": "2021-02-24T10:43:00.000Z",
        "voteCount": 11,
        "content": "Cleared the AI-100 yesterday. This question appeared, but they replaced \"Blob storage\" with \"queue storage\".  So, answer is clearly ADL."
      },
      {
        "date": "2023-06-19T07:49:00.000Z",
        "voteCount": 1,
        "content": "recommended solution for storing the pictures and minimizing costs is Azure Blob storage (Option C)."
      },
      {
        "date": "2021-07-10T19:19:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2021-05-31T00:49:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-02-15T18:07:00.000Z",
        "voteCount": 2,
        "content": "My bet is on Blob. ADL is built on blob but has additional features (not required in this case) which obviously costs more than simply Blob storage."
      },
      {
        "date": "2021-01-06T20:24:00.000Z",
        "voteCount": 1,
        "content": "Azure Blob Storage seems like the appropriate storage for images given the numbers"
      },
      {
        "date": "2020-09-29T07:50:00.000Z",
        "voteCount": 2,
        "content": "Azure Blob Storage \"helps you create data lakes for your analytics needs\". So data lake is build on Blob strage. Blob storage seems more appropriate as images/files are usually stored here and its meant to be cost effective too."
      },
      {
        "date": "2020-07-05T13:29:00.000Z",
        "voteCount": 3,
        "content": "Data Lake store will be costly. But it will be good if you need scalable feature. Most importantly Data Lake might not be available in all the regions."
      },
      {
        "date": "2020-06-21T01:03:00.000Z",
        "voteCount": 1,
        "content": "For millions of pictures, will data lake store be better?"
      },
      {
        "date": "2020-12-28T12:15:00.000Z",
        "voteCount": 4,
        "content": "Data Lake is basically blob storage with extra features. Since you won't be doing any analytics on these photos directly in the storage but rather using ML to analyze something, blob storage is cheaper. The keywords are \"minimize costs\". If you were in the future planning on doing analytics in the storage location, then Data Lake is the way to go."
      },
      {
        "date": "2021-02-24T10:47:00.000Z",
        "voteCount": 2,
        "content": "The questions says: \"...solution that will *analyze* millions of pictures...\"."
      },
      {
        "date": "2021-07-10T10:42:00.000Z",
        "voteCount": 1,
        "content": "Yes it does say that... But there's a full stop and then it says you need to recommend a solution for \"storing\" the pictures."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/19017-exam-ai-100-topic-1-question-5-discussion/",
    "body": "You are configuring data persistence for a Microsoft Bot Framework application. The application requires a structured NoSQL cloud data store.<br>You need to identify a storage solution for the application. The solution must minimize costs.<br>What should you identify?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure HDInsight",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage"
    ],
    "answer": "D",
    "answerDescription": "Table Storage is a NoSQL key-value store for rapid development using massive semi-structured datasets<br>You can develop applications on Cosmos DB using popular NoSQL APIs.<br>Both services have a different scenario and pricing model.<br>While Azure Storage Tables is aimed at high capacity on a single region (optional secondary read only region but no failover), indexing by PK/RK and storage- optimized pricing; Azure Cosmos DB Tables aims for high throughput (single-digit millisecond latency), global distribution (multiple failover), SLA-backed predictive performance with automatic indexing of each attribute/property and a pricing model focused on throughput.<br>References:<br>https://db-engines.com/en/system/Microsoft+Azure+Cosmos+DB%3BMicrosoft+Azure+Table+Storage",
    "votes": [],
    "comments": [
      {
        "date": "2021-01-06T20:31:00.000Z",
        "voteCount": 14,
        "content": "\"Table API in Azure Cosmos DB Vs Azure Table storage:\nWhere is Table API not identical with Azure Table storage behavior?\nThere are some behavior differences that users coming from Azure Table storage who want to create tables with the Azure Cosmos DB Table API should be aware of:\n\nAzure Cosmos DB Table API uses a reserved capacity model in order to ensure guaranteed performance but this means that one pays for the capacity as soon as the table is created, even if the capacity isn't being used. With Azure Table storage one only pays for capacity that's used. This helps to explain why Table API can offer a 10 ms read and 15 ms write SLA at the 99th percentile while Azure Table storage offers a 10-second SLA. But as a consequence, with Table API tables, even empty tables without any requests, cost money in order to ensure the capacity is available to handle any requests to them at the SLA offered by Azure Cosmos DB.\"\n\nHence, Table storage is more appropriate here than Azure Cosmos DB Table API. \nAs for why not Azure Blob Storage, enough said already by others here."
      },
      {
        "date": "2021-01-11T07:54:00.000Z",
        "voteCount": 2,
        "content": "Nice Explanation."
      },
      {
        "date": "2023-01-15T07:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is simple. Price basis Table storage is cheaper. Please don't mix Table API of Cosmos DB which requires storage in Cosmos DB which occurs very high cost."
      },
      {
        "date": "2023-06-19T07:50:00.000Z",
        "voteCount": 1,
        "content": "For a structured NoSQL cloud data store with cost optimization in mind, the most suitable option among the provided choices would be B. Azure Cosmos DB."
      },
      {
        "date": "2021-03-22T08:29:00.000Z",
        "voteCount": 4,
        "content": "What does this question have to do with AI solutions? It's a question about data and pricing. A Cosmos DB solution would be an acceptable solution"
      },
      {
        "date": "2021-01-04T10:13:00.000Z",
        "voteCount": 1,
        "content": "Azure Table storage is a cloud-based NoSQL datastore you can use to store large amounts of structured, non-relational data. Azure Table offers a schemaless design, which enables you to store a collection of entities in one table"
      },
      {
        "date": "2020-12-27T08:48:00.000Z",
        "voteCount": 1,
        "content": "Azure Table storage is a service that stores structured NoSQL data in the cloud, providing a key/attribute store with a schemaless design.\n\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/table-storage-overview"
      },
      {
        "date": "2020-11-27T01:24:00.000Z",
        "voteCount": 2,
        "content": "This is an interesting one, as the table storage api for CosmosDB is now recommended over table storage for all use cases, check the wording in the link at the top:-\n\nhttps://azure.microsoft.com/en-gb/services/storage/tables/"
      },
      {
        "date": "2020-04-24T06:58:00.000Z",
        "voteCount": 1,
        "content": "Not sure why it is not Azure Blob storage"
      },
      {
        "date": "2020-05-21T04:53:00.000Z",
        "voteCount": 6,
        "content": "Not Azure Blob Storage because it's not NoSQL DB"
      },
      {
        "date": "2020-11-22T21:59:00.000Z",
        "voteCount": 6,
        "content": "It asked for a \"structured\" NoSQL DB and blob is specifically unstructured."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25533-exam-ai-100-topic-1-question-6-discussion/",
    "body": "You have an Azure Machine Learning model that is deployed to a web service.<br>You plan to publish the web service by using the name ml.contoso.com.<br>You need to recommend a solution to ensure that access to the web service is encrypted.<br>Which three actions should you recommend? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGenerate a shared access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tObtain an SSL certificate",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a deployment slot",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the web service",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate DNS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Key Vault"
    ],
    "answer": "BDE",
    "answerDescription": "The process of securing a new web service or an existing one is as follows:<br>1. Get a domain name.<br>2. Get a digital certificate.<br>3. Deploy or update the web service with the SSL setting enabled.<br>4. Update your DNS to point to the web service.<br>Note: To deploy (or re-deploy) the service with SSL enabled, set the ssl_enabled parameter to True, wherever applicable. Set the ssl_certificate parameter to the value of the certificate file and the ssl_key to the value of the key file.<br>References:<br>https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-secure-web-service",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T07:51:00.000Z",
        "voteCount": 1,
        "content": "recommended actions to ensure encrypted access to the web service are B. Obtain an SSL certificate, D. Update the web service, and E. Update DNS."
      },
      {
        "date": "2021-04-14T20:03:00.000Z",
        "voteCount": 3,
        "content": "This is the general process to secure a web service:\nGet a domain name.\nGet a digital certificate.\nDeploy or update the web service with TLS enabled.\nUpdate your DNS to point to the web service."
      },
      {
        "date": "2021-03-07T16:46:00.000Z",
        "voteCount": 1,
        "content": "Why Azure Key Vault is not an option? We could store the certificate on Azure Key Vault."
      },
      {
        "date": "2021-05-09T13:58:00.000Z",
        "voteCount": 2,
        "content": "I believe it is because, even though you can use Azure Key Vault, it is not a must for configuring the web service encryption"
      },
      {
        "date": "2023-01-15T07:51:00.000Z",
        "voteCount": 1,
        "content": "we might use if we deploy it through CD pipeline"
      },
      {
        "date": "2021-01-06T20:33:00.000Z",
        "voteCount": 1,
        "content": "Yes it looks correct."
      },
      {
        "date": "2020-07-12T16:58:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23178-exam-ai-100-topic-1-question-7-discussion/",
    "body": "Your company recently deployed several hardware devices that contain sensors.<br>The sensors generate new data on an hourly basis. The data generated is stored on-premises and retained for several years.<br>During the past two months, the sensors generated 300 GB of data.<br>You plan to move the data to Azure and then perform advanced analytics on the data.<br>You need to recommend an Azure storage solution for the data.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Queue storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "C",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-21T03:48:00.000Z",
        "voteCount": 9,
        "content": "Since the data is collected from sensors, maybe it's indicating that the greater part of the data is unstructured and thus, blob maybe the safer option"
      },
      {
        "date": "2021-01-06T20:36:00.000Z",
        "voteCount": 6,
        "content": "No where it says the sensors generated structured tabular data. Sensors usually produce logs of data or files of data that could contain JSON, XML, etc. Azure BLOB seems to be the appropriate storage option here."
      },
      {
        "date": "2021-01-11T12:15:00.000Z",
        "voteCount": 1,
        "content": "The question is vague. Sensors generate several types of data. Could very well be streaming data that is structured and stored in a SQL database. Agree it doesn't say structured tabular data... it doesn't say it is unstructured either. BLOB or Cosmos or even SQL can be used as the target store.\nI'll go with Blob, but the question is definitely vague."
      },
      {
        "date": "2023-06-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "the most suitable Azure storage solution for storing and analyzing the sensor data would be C. Azure Blob storage."
      },
      {
        "date": "2023-01-15T08:05:00.000Z",
        "voteCount": 1,
        "content": "I will go with blob storage and then do analytics in databricks. I can implement lambda architecture for performing a typical advance analytics solution. \nCosmosDB is good for high throughput data ingestion but again we need to move data to more cheaper storage which will be again blob storage.\nSQL Server is not good for live analytics. Queue storage is funny option which can be ignored."
      },
      {
        "date": "2021-06-27T12:00:00.000Z",
        "voteCount": 1,
        "content": "Azure Blob Storage - as the data should be retained for several years."
      },
      {
        "date": "2021-03-25T17:48:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B in terms of analytics that requires NoSQL DB."
      },
      {
        "date": "2021-01-02T09:23:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct\nBlob storage is ideal for:\n\nServing images or documents directly to a browser.\nStoring files for distributed access.\nStreaming video and audio.\nStoring data for backup and restore, disaster recovery, and archiving.\nStoring data for analysis by an on-premises or Azure-hosted service.\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-introduction"
      },
      {
        "date": "2021-02-16T07:13:00.000Z",
        "voteCount": 2,
        "content": "So, nearly everything, except what is specified as the only clear requirement: \"perform advanced analytics on the data\". \nBlob/DataLake2 do not cater for real time analytics / advanced querying.\nThe answer should be Cosmos DB."
      },
      {
        "date": "2021-04-15T09:25:00.000Z",
        "voteCount": 1,
        "content": "the question never says it needs real time analysis"
      },
      {
        "date": "2020-08-31T04:52:00.000Z",
        "voteCount": 2,
        "content": "Strange!! The url states that Azure Blob is not optimized for Analytics.'\nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/technology-choices/data-storage"
      },
      {
        "date": "2020-12-11T17:29:00.000Z",
        "voteCount": 2,
        "content": "data lake is blob storage"
      },
      {
        "date": "2023-01-15T07:59:00.000Z",
        "voteCount": 1,
        "content": "blob storage is not data lake storage"
      },
      {
        "date": "2020-08-05T22:11:00.000Z",
        "voteCount": 1,
        "content": "Data Lake would be better choice, but in this case it's Azure storage because it's optimized for analytics (\"... you plan to do advanced analytics on the data.\"), see the links."
      },
      {
        "date": "2023-01-15T07:59:00.000Z",
        "voteCount": 1,
        "content": "data lake storage is not even in the option."
      },
      {
        "date": "2020-06-29T07:24:00.000Z",
        "voteCount": 2,
        "content": "This question is very vague, it does not mention any requirements other than size. If it said to minimize cost, then Blob storage is correct. But like this, all the answers are technically correct..."
      },
      {
        "date": "2020-06-23T00:22:00.000Z",
        "voteCount": 2,
        "content": "I suppose they mention Blob Storage because Azure Data Lake Storage Gen2 is based on Blob Storage"
      },
      {
        "date": "2020-06-15T02:57:00.000Z",
        "voteCount": 3,
        "content": "Why blob? At best should be Data Lake. I think here should be Cosmos DB or Azure SQL Database."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3058-exam-ai-100-topic-1-question-8-discussion/",
    "body": "You plan to design an application that will use data from Azure Data Lake and perform sentiment analysis by using Azure Machine Learning algorithms.<br>The developers of the application use a mix of Windows- and Linux-based environments. The developers contribute to shared GitHub repositories.<br>You need all the developers to use the same tool to develop the application.<br>What is the best tool to use? More than one answer choice may achieve the goal.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Visual Studio Code",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Notebooks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Machine Learning Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft Visual Studio"
    ],
    "answer": "C",
    "answerDescription": "References:<br>https://github.com/MicrosoftDocs/azure-docs/blob/master/articles/machine-learning/studio/algorithm-choice.md",
    "votes": [],
    "comments": [
      {
        "date": "2019-12-25T03:45:00.000Z",
        "voteCount": 19,
        "content": "Azure Machine Learning Studio is the correct answer.\nAzure Machine Learning Studio (classic) can read data directly from Azure Data Lake Store and then be used to create and deploy models. This approach uses a Hive table that points at the Azure Data Lake Store. This requires that a separate Azure HDInsight cluster be provisioned, on which the Hive table is created. \n Refer the following url: \n https://docs.microsoft.com/en-us/azure/machine-learning/team-data-science-process/data-lake-walkthrough"
      },
      {
        "date": "2020-01-01T02:27:00.000Z",
        "voteCount": 1,
        "content": "How can you contribute to GitHub using the Azure ML?"
      },
      {
        "date": "2020-02-15T23:59:00.000Z",
        "voteCount": 3,
        "content": "technically, you have a Download button for every notebook you create in ML studio. the text only mentions that the notebooks need to be shared, not through which mechanism. so although VS code could also be an option, this would require them to also install it accordingly. and because the questions states at the end that more than one choice can achieve the goal, i think it's safe to end the debate by choosing either VScode or MLS as only viable options from the 4, and i think you will be scored correct on either one"
      },
      {
        "date": "2019-10-03T15:52:00.000Z",
        "voteCount": 13,
        "content": "I think that the answer is Visual Studio Code.  https://code.visualstudio.com/docs"
      },
      {
        "date": "2020-12-28T12:20:00.000Z",
        "voteCount": 1,
        "content": "VS Code is also available for both Windows and Linux."
      },
      {
        "date": "2023-06-19T07:52:00.000Z",
        "voteCount": 1,
        "content": "To ensure that all developers, regardless of their operating system, can collaborate on developing the application and perform sentiment analysis using Azure Machine Learning algorithms, the best tool to use would be A. Microsoft Visual Studio Code."
      },
      {
        "date": "2022-07-19T19:20:00.000Z",
        "voteCount": 1,
        "content": "Visual Studio Code as it's talking about GitHub"
      },
      {
        "date": "2021-03-15T13:14:00.000Z",
        "voteCount": 2,
        "content": "It is indeed a tricky question but AZ ML Studio is the correct option because it provide a common development place where you can find all the required ML development resources e.g., Notebook, Compute and Data Connectivity to different data sources."
      },
      {
        "date": "2021-02-21T20:51:00.000Z",
        "voteCount": 2,
        "content": "I think ML Studion is Correct."
      },
      {
        "date": "2021-01-06T22:10:00.000Z",
        "voteCount": 2,
        "content": "Everyone focuses on the sharing the notebook on GitHub, missing rest of the details. The important detail here is to use existing ML Algorithms and connecting to Azure Data Lake for data, which is also available. I don't know why people said it is not available. \n\nCheck this link: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-access-data"
      },
      {
        "date": "2020-11-12T22:32:00.000Z",
        "voteCount": 1,
        "content": "Answer is D \n\nVisual Studio Code Tools for AI: This service is an extension of Visual Studio Code (VS Code) \u2014 a desktop source code editor for Windows, macOS and Linux \u2014 that helps developers create scripts and gather metrics for Azure Machine Learning experiments.\nAzure Machine Learning Studio: This is a visual, drag-and-drop tool designed to help users build and deploy predictive analysis models with no coding required.\n\nhttps://medium.com/@ahmedkhemiri24/microsoft-azure-machine-learning-d148478e867c"
      },
      {
        "date": "2020-12-27T22:00:00.000Z",
        "voteCount": 2,
        "content": "So based on your explanation, ans shouuld be A, correct?"
      },
      {
        "date": "2020-09-21T22:58:00.000Z",
        "voteCount": 1,
        "content": "currently Azure ML Studio supports import from Data Lake also using the designer   https://docs.microsoft.com/en-us/azure/machine-learning/algorithm-module-reference/import-data"
      },
      {
        "date": "2020-09-06T06:08:00.000Z",
        "voteCount": 2,
        "content": "the url in the answer is obviously broken"
      },
      {
        "date": "2020-07-05T14:19:00.000Z",
        "voteCount": 1,
        "content": "Why not Azure Notebooks?\nI think the perfect answer is VS Code. Apart from that we can use AZURE ML Studio and Azure Notebooks as well."
      },
      {
        "date": "2020-06-14T04:08:00.000Z",
        "voteCount": 5,
        "content": "Clearly the question says \"More than one answer choice may achieve the goal\" - It would be either MLS or VS."
      },
      {
        "date": "2019-12-12T21:31:00.000Z",
        "voteCount": 7,
        "content": "Machine learning studio doesn't make any sense because the scenario is about sharing code and ML studio doesn't have any code or sharing with GitHub. Visual Studio is not an option because of Windows dependency. Azure Notebooks and VS Code are two suitable options. I am a little biased towards VSCode"
      },
      {
        "date": "2020-01-01T02:28:00.000Z",
        "voteCount": 1,
        "content": "I think it's VS Code. You are right."
      },
      {
        "date": "2019-07-31T04:41:00.000Z",
        "voteCount": 3,
        "content": "Wouldn't Azure Notebooks make more sense as Azure Machine Learning Studio does not have an import data function for Data Lake yet..."
      },
      {
        "date": "2019-08-30T20:49:00.000Z",
        "voteCount": 1,
        "content": "I think u r correct.... as Machine Learning Studio does not have an import data function for Data Lake \nhttps://docs.microsoft.com/en-us/azure/machine-learning/studio/what-is-ml-studio"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/6082-exam-ai-100-topic-1-question-9-discussion/",
    "body": "You have several AI applications that use an Azure Kubernetes Service (AKS) cluster. The cluster supports a maximum of 32 nodes.<br>You discover that occasionally and unpredictably, the application requires more than 32 nodes.<br>You need to recommend a solution to handle the unpredictable application load.<br>Which scaling method should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thorizontal pod autoscaler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcluster autoscaler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmanual scaling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Container Instances"
    ],
    "answer": "B",
    "answerDescription": "B: To keep up with application demands in Azure Kubernetes Service (AKS), you may need to adjust the number of nodes that run your workloads. The cluster autoscaler component can watch for pods in your cluster that can't be scheduled because of resource constraints. When issues are detected, the number of nodes is increased to meet the application demand. Nodes are also regularly checked for a lack of running pods, with the number of nodes then decreased as needed. This ability to automatically scale up or down the number of nodes in your AKS cluster lets you run an efficient, cost-effective cluster.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler",
    "votes": [],
    "comments": [
      {
        "date": "2020-03-10T07:17:00.000Z",
        "voteCount": 13,
        "content": "Is this part of AI-100? I don't remember learning anything related to this..."
      },
      {
        "date": "2021-06-14T14:36:00.000Z",
        "voteCount": 1,
        "content": "Simpler parts of data engineering, database, and SQL questions are included in AI-100. This type of question was also on the data engineering exam."
      },
      {
        "date": "2021-01-02T14:46:00.000Z",
        "voteCount": 9,
        "content": "The question asks about nodes so the answer is Cluster Auto-scaler. If it asks about pods then the answer will be  horizontal pod auto-scaler"
      },
      {
        "date": "2021-01-06T22:15:00.000Z",
        "voteCount": 2,
        "content": "Simple and elegant! Yes Cluster Auto-Scaler is the answer"
      },
      {
        "date": "2023-06-19T07:54:00.000Z",
        "voteCount": 1,
        "content": "To handle the unpredictable application load in an Azure Kubernetes Service (AKS) cluster that occasionally requires more than the maximum of 32 nodes, the recommended scaling method would be B. cluster autoscaler."
      },
      {
        "date": "2023-01-15T08:24:00.000Z",
        "voteCount": 1,
        "content": "ACI: D is answer.\nhttps://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/scale-using-aks-with-aci\n \"Use the AKS virtual node to provision pods inside ACI that start in seconds. This enables AKS to run with just enough capacity for your average workload. As you run out of capacity in your AKS cluster, scale out additional pods in ACI, without any additional servers to manage\""
      },
      {
        "date": "2023-01-15T08:12:00.000Z",
        "voteCount": 1,
        "content": "autoscaler is to scale the cluster up/down. So as per the question it can't go beyond 32 nodes and the question is what you do when we need more than 32 nodes? surely autoscaler won't do this job."
      },
      {
        "date": "2021-04-20T09:34:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is cluster autoscaler.\n\nRef : https://docs.microsoft.com/en-us/azure/aks/cluster-autoscaler#:~:text=cluster%20autoscaler%20settings.-,Update%20an%20existing%20AKS%20cluster%20to%20enable%20the%20cluster%20autoscaler,count%20and%20%2D%2Dmax%2Dcount%20."
      },
      {
        "date": "2020-12-20T22:30:00.000Z",
        "voteCount": 1,
        "content": "can the answer be A?"
      },
      {
        "date": "2020-12-08T06:36:00.000Z",
        "voteCount": 4,
        "content": "The ans is (D)Azure container Instance:\nTo rapidly scale your AKS cluster, you can integrate with Azure Container Instances (ACI).\nhttps://docs.microsoft.com/en-us/azure/aks/concepts-scale"
      },
      {
        "date": "2020-11-29T08:53:00.000Z",
        "voteCount": 3,
        "content": "My answer would be D. Azure Container Instances, since maximum of the cluster is 32 and \"occasionally and unpredictably\" we need resources above that value.\nProof: https://azure.microsoft.com/en-us/services/container-instances/#overview\nElastic bursting with AKS\nACI provides fast, isolated compute to meet traffic that comes in spikes, without the need to manage servers. For example, Azure Kubernetes Service (AKS) can use the Virtual Kubelet to provision pods inside ACI that start in seconds. This enables AKS to run with just enough capacity for your average workload. As you run out of capacity in your AKS cluster, scale out additional pods in ACI without any additional servers to manage."
      },
      {
        "date": "2020-10-15T13:09:00.000Z",
        "voteCount": 1,
        "content": "In the question: \"The cluster supports a maximum of 32 nodes.\nYou discover that occasionally and unpredictably, the application requires more than 32 nodes.\"\nTo me, this is a clear indication an additional cluster is needed, therefore B."
      },
      {
        "date": "2020-08-26T19:12:00.000Z",
        "voteCount": 1,
        "content": "\u201cCluster autoscaler is typically used alongside the horizontal pod autoscaler. When combined, the horizontal pod autoscaler increases or decreases the number of pods based on application demand, and the cluster autoscaler adjusts the number of nodes as needed to run those additional pods accordingly.\u201d"
      },
      {
        "date": "2020-05-26T00:36:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is B. If you check https://docs.microsoft.com/en-us/azure/aks/concepts-scale, particularly the section \"Cluster Auto-scaler\", you'll find the following excerpt:\n\"If the cluster autoscale determines that a change is required, the number of nodes in your AKS cluster is increased or decreased accordingly\"\n\nOn the reason why A. Horizontal pod auto-scaler, it becomes clear on the same web page that this scaler is limited by the number of nodes in the cluster, therefore, not acceptable"
      },
      {
        "date": "2020-04-07T08:59:00.000Z",
        "voteCount": 2,
        "content": "I think the correct answers are \"A\" and \"B\". The horizontal pod auto-scaler  increase/ decrease the number of replicas of an app that has been deployed, while the cluster auto-scaler increase/ decrease the number of nodes in the cluster. Any of these two options could fit well in the required scenario."
      },
      {
        "date": "2019-10-03T15:52:00.000Z",
        "voteCount": 1,
        "content": "Please ignore, this was for the previous question"
      },
      {
        "date": "2019-10-03T15:51:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is Visual Studio Code...  https://code.visualstudio.com/docs"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/22547-exam-ai-100-topic-1-question-10-discussion/",
    "body": "You deploy an infrastructure for a big data workload.<br>You need to run Azure HDInsight and Microsoft Machine Learning Server. You plan to set the RevoScaleR compute contexts to run rx function calls in parallel.<br>What are three compute contexts that you can use for Machine Learning Server? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlocal parallel",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHBase",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlocal sequential"
    ],
    "answer": "ABC",
    "answerDescription": "Remote computing is available for specific data sources on selected platforms. The following tables document the supported combinations.<br>\u2711 RxInSqlServer, sqlserver: Remote compute context. Target server is a single database node (SQL Server 2016 R Services or SQL Server 2017 Machine<br>Learning Services). Computation is parallel, but not distributed.<br>\u2711 RxSpark, spark: Remote compute context. Target is a Spark cluster on Hadoop.<br>\u2711 RxLocalParallel, localpar: Compute context is often used to enable controlled, distributed computations relying on instructions you provide rather than a built-in scheduler on Hadoop. You can use compute context for manual distributed computing.<br>References:<br>https://docs.microsoft.com/en-us/machine-learning-server/r/concept-what-is-compute-context",
    "votes": [],
    "comments": [
      {
        "date": "2020-06-15T02:15:00.000Z",
        "voteCount": 7,
        "content": "I think A,B,C is correct. In this link https://docs.microsoft.com/en-us/machine-learning-server/r/concept-what-is-compute-context  , it is indicated which RevoSvaleR compute context are available, which are local, spark, sqlserver, localpar, dopar.  If you are going to run R script from an Edge, the possible values of the context are: local sequential, local parallel, Map Reduce and Spark. https://docs.microsoft.com/en-us/azure/hdinsight/r-server/r-server-compute-contexts."
      },
      {
        "date": "2020-12-19T23:40:00.000Z",
        "voteCount": 2,
        "content": "if based on your last link given https://docs.microsoft.com/en-us/azure/hdinsight/r-server/r-server-compute-contexts , the correct option should be Spark, Local Parallel &amp; Local Sequential . Answer should be B, C, E right?"
      },
      {
        "date": "2023-06-19T07:55:00.000Z",
        "voteCount": 1,
        "content": "the three compute contexts you can use for Machine Learning Server to run rx function calls in parallel are B. Spark, C. local parallel, and E. local sequential."
      },
      {
        "date": "2021-08-24T06:26:00.000Z",
        "voteCount": 1,
        "content": "The correct one is B,C and E because he is processing BIg data on HD insight. https://docs.microsoft.com/en-us/azure/hdinsight/r-server/r-server-compute-contexts"
      },
      {
        "date": "2021-01-06T22:18:00.000Z",
        "voteCount": 2,
        "content": "\"You plan to set the RevoScaleR compute contexts to run rx function calls in PARALLEL.\"\nYet people Say E instead of A"
      },
      {
        "date": "2021-01-14T17:12:00.000Z",
        "voteCount": 4,
        "content": "That's because the documentation for compute context for an edge node says Local sequential does parallelized execution. However, I found this: https://docs.microsoft.com/en-us/machine-learning-server/r-reference/revoscaler/revoscaler#7-compute-context-functions\n\nNo one posted this link but I think it is the right one to refer here for RevoScaleR.\n\nIt does list Spark and Parallel clearly. But doesn't say Parallel execution for SQL and LocalSeq. Since LocalSeq does say \"sequential computations\" so I'm going with given answer:\nSQL, Local parallel and Spark.\nA, B, C"
      },
      {
        "date": "2021-01-02T15:38:00.000Z",
        "voteCount": 2,
        "content": "The Answer is wrong. The correct one is B,C and E because he is processing BIg data on HD insight. https://docs.microsoft.com/en-us/azure/hdinsight/r-server/r-server-compute-contexts\nBut if he was working on small data then the answer will be A, B, and C( although I am not sure about C in the small data case)\nhttps://docs.microsoft.com/en-us/machine-learning-server/r/tutorial-rxexecby"
      },
      {
        "date": "2020-11-12T02:57:00.000Z",
        "voteCount": 1,
        "content": "ABC is correct"
      },
      {
        "date": "2020-07-12T17:02:00.000Z",
        "voteCount": 3,
        "content": "correct answer is ABC"
      },
      {
        "date": "2020-06-07T22:17:00.000Z",
        "voteCount": 1,
        "content": "B,C,E seems right"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3345-exam-ai-100-topic-1-question-11-discussion/",
    "body": "Your company has 1,000 AI developers who are responsible for provisioning environments in Azure.<br>You need to control the type, size, and location of the resources that the developers can provision.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Key Vault",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure service principals",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure managed identities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Security Center",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Policy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "When an application needs access to deploy or configure resources through Azure Resource Manager in Azure Stack, you create a service principal, which is a credential for your application. You can then delegate only the necessary permissions to that service principal.<br>References:<br>https://docs.microsoft.com/en-us/azure/azure-stack/azure-stack-create-service-principals",
    "votes": [
      {
        "answer": "E",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2019-08-07T12:53:00.000Z",
        "voteCount": 51,
        "content": "Should be policy. This is what allows you to restrict resources that are created in groups."
      },
      {
        "date": "2019-08-30T20:57:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2020-07-05T11:10:00.000Z",
        "voteCount": 5,
        "content": "It is a tricky question !! I think the answer is service Principle WHY? Because the question says to control location and size ..ETC. so first you need to create a service account for developers then you hse azure policy to apply rules on this account."
      },
      {
        "date": "2021-01-18T11:37:00.000Z",
        "voteCount": 1,
        "content": "Eh no, location etc can also be controlled by Azure Policy."
      },
      {
        "date": "2023-06-19T07:56:00.000Z",
        "voteCount": 1,
        "content": "To control the type, size, and location of the resources that AI developers can provision in Azure, you should use E. Azure Policy."
      },
      {
        "date": "2022-06-08T07:28:00.000Z",
        "voteCount": 1,
        "content": "azure policy"
      },
      {
        "date": "2021-03-25T17:56:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is E in terms of \"control\""
      },
      {
        "date": "2021-02-19T11:57:00.000Z",
        "voteCount": 1,
        "content": "A service principal must be created in each tenant where the application is used, enabling it to establish an identity for sign-in a... You need to provision resources to a large number- Azure Policy is the best answer"
      },
      {
        "date": "2021-01-19T22:19:00.000Z",
        "voteCount": 1,
        "content": "This must be Azure Policy. Overview of Azure Policy clearly talks about all three:\nhttps://docs.microsoft.com/en-us/azure/governance/policy/overview"
      },
      {
        "date": "2021-01-14T17:27:00.000Z",
        "voteCount": 1,
        "content": "The question is clear and it speaks about provisioning the infra and not about the access related to infra. \"You need to control the type, size, and location of the resources that the developers can provision.\" It speaks about controlling the type, size and location of the resources that developers can provision. Azure policies can control the type size and location for an infra that will be provisioned. This can be provisioned by any user, but what control the infra attributes are the policies. The service principles are only for authentication purpose and they don't enforce such policies. Hence the answer is Azure Policy."
      },
      {
        "date": "2021-01-13T23:49:00.000Z",
        "voteCount": 1,
        "content": "This is tricky,,  Def Policy comes in mind at first,  But hold on,,,, \n For specific  only 1000 AI deveoplers ,, since all of them will be having same set of permissions.,,, \n\nSo all of those who are saying policy,,,, where are you going to attach that policy (JSON )..???\n\nSo, Therefore the best way is to create Azure service principal to which role is assigned using which every AI developer has to login...and on that Role,, policy can be attached.  \n\nSo correct answer is B."
      },
      {
        "date": "2021-01-06T22:27:00.000Z",
        "voteCount": 1,
        "content": "Azure Policy seems to be the correct answer here. The answer solution doesn't seem to be aligned with the question. \n\nIn Azure Policy, we offer several built-in policies that are available by default. For example:\n\nAllowed Storage Account SKUs (Deny): Determines if a storage account being deployed is within a set of SKU sizes. Its effect is to deny all storage accounts that don't adhere to the set of defined SKU sizes.\n\"Allowed Resource Type (Deny): Defines the resource types that you can deploy. Its effect is to deny all resources that aren't part of this defined list.\nAllowed Locations (Deny): Restricts the available locations for new resources. Its effect is used to enforce your geo-compliance requirements.\nAllowed Virtual Machine SKUs (Deny): Specifies a set of virtual machine SKUs that you can deploy.\nAdd a tag to resources (Modify): Applies a required tag and its default value if it's not specified by the deploy request.\"\nCheck this link and watch the 23 min video on Azure Policy: \nhttps://docs.microsoft.com/en-us/azure/governance/policy/overview"
      },
      {
        "date": "2021-01-06T22:25:00.000Z",
        "voteCount": 1,
        "content": "Azure Policy seems to be the correct answer here. The answer solution doesn't seem to be aligned with the question. \nCheck this link and watch the 23 min video on Azure Policy: https://docs.microsoft.com/en-us/azure/governance/policy/overview"
      },
      {
        "date": "2020-12-24T08:12:00.000Z",
        "voteCount": 2,
        "content": "AZURE POLICY is the correct answer."
      },
      {
        "date": "2020-11-12T03:01:00.000Z",
        "voteCount": 1,
        "content": "Should be policy"
      },
      {
        "date": "2020-11-08T00:35:00.000Z",
        "voteCount": 2,
        "content": "It seems that Azure service principals has such capabilities.\n\nWhy are people saying it should only be policy if based here (https://docs.microsoft.com/en-us/azure-stack/operator/azure-stack-create-service-principals?view=azs-2005&amp;pivots=state-disconnected):\n\n\" Just as a user is represented by a security principal called a user principal, an app is represented by a service principal. The service principal provides an identity for your app, allowing you to delegate only the necessary permissions to the app.\n\nAs an example, you may have a configuration management app that uses Azure Resource Manager to inventory Azure resources. In this scenario, you can create a service principal, grant the \"reader\" role to that service principal, and limit the configuration management app to read-only access.\""
      },
      {
        "date": "2020-09-22T00:03:00.000Z",
        "voteCount": 1,
        "content": "Azure Policy https://docs.microsoft.com/en-us/azure/governance/policy/overview#policy-definition"
      },
      {
        "date": "2020-09-06T06:50:00.000Z",
        "voteCount": 1,
        "content": "This should be policy."
      },
      {
        "date": "2020-06-23T01:30:00.000Z",
        "voteCount": 1,
        "content": "No doubt. It is Azure Policy."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/16113-exam-ai-100-topic-1-question-12-discussion/",
    "body": "You are designing an AI solution in Azure that will perform image classification.<br>You need to identify which processing platform will provide you with the ability to update the logic over time. The solution must have the lowest latency for inferencing without having to batch.<br>Which compute target should you identify?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgraphics processing units (GPUs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfield-programmable gate arrays (FPGAs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcentral processing units (CPUs)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tapplication-specific integrated circuits (ASICs)"
    ],
    "answer": "B",
    "answerDescription": "FPGAs, such as those available on Azure, provide performance close to ASICs. They are also flexible and reconfigurable over time, to implement new logic.<br>Incorrect Answers:<br>D: ASICs are custom circuits, such as Google's TensorFlow Processor Units (TPU), provide the highest efficiency. They can't be reconfigured as your needs change.<br>References:<br>https://docs.microsoft.com/en-us/azure/machine-learning/service/concept-accelerate-with-fpgas",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-02T09:23:00.000Z",
        "voteCount": 6,
        "content": "FPGAs is correct because of the following line from the docs Piraat linked:\n\n\"FPGAs make it possible to achieve low latency for real-time inference (or model scoring) requests. Asynchronous requests (batching) aren't needed. Batching can cause latency, because more data needs to be processed. Implementations of neural processing units don't require batching; therefore the latency can be many times lower, compared to CPU and GPU processors.\""
      },
      {
        "date": "2020-03-10T07:28:00.000Z",
        "voteCount": 5,
        "content": "relevant: https://docs.microsoft.com/en-us/azure/machine-learning/how-to-deploy-fpga-web-service"
      },
      {
        "date": "2023-06-19T07:57:00.000Z",
        "voteCount": 1,
        "content": "To ensure the ability to update the logic over time, while maintaining low latency for inferencing without the need for batching in an image classification AI solution, the compute target you should identify is C. central processing units (CPUs)."
      },
      {
        "date": "2020-09-06T06:57:00.000Z",
        "voteCount": 1,
        "content": "https://www.aldec.com/en/company/blog/167--fpgas-vs-gpus-for-machine-learning-applications-which-one-is-better#:~:text=Efficiency%20and%20Power%3A%20FPGAs%20are,times%20better%20in%20power%20consumption.&amp;text=This%20feature%20allows%20GPUs%20to%20be%20more%20power%20efficient%20than%20CPUs.\n\nThis document suggests that FPGAs are mostly used where functional safety plays a very important role such as automation, avionics and defense. \nGPUs are originally designed for graphics and high-performance computing systems where safety is not a necessity. GPU is also made for Graphics. It's true that FPGAs are a bit more powerful than GPU, since its graphics and image classification, I wonder if the answer will be GPU"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/13711-exam-ai-100-topic-1-question-13-discussion/",
    "body": "You have a solution that runs on a five-node Azure Kubernetes Service (AKS) cluster. The cluster uses an N-series virtual machine.<br>An Azure Batch AI process runs once a day and rarely on demand.<br>You need to recommend a solution to maintain the cluster configuration when the cluster is not in use. The solution must not incur any compute costs.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownscale the cluster to one node",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDownscale the cluster to zero nodes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the cluster"
    ],
    "answer": "A",
    "answerDescription": "An AKS cluster has one or more nodes.<br>References:<br>https://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-15T15:11:00.000Z",
        "voteCount": 9,
        "content": "A seems to be correct. See here - https://github.com/Azure/AKS/issues/52"
      },
      {
        "date": "2023-06-19T07:59:00.000Z",
        "voteCount": 1,
        "content": "To maintain the cluster configuration and avoid incurring compute costs when the Azure Kubernetes Service (AKS) cluster is not in use, the recommended solution would be to B. \nDownscale the cluster to zero nodes.\n\nBy downscaling the cluster to zero nodes, you effectively stop all the virtual machines (VMs) in the cluster. This allows you to maintain the cluster configuration and associated resources (such as networking, storage, and security settings) without incurring any compute costs.\n\nWhen the cluster is not in use, scaling it down to zero nodes ensures that no resources are actively consuming compute capacity, which helps optimize costs. This approach allows you to easily bring the cluster back online when needed without the need to recreate or reconfigure it."
      },
      {
        "date": "2021-06-20T01:41:00.000Z",
        "voteCount": 1,
        "content": "System pools must contain at least one node, and user node pools may contain zero or more nodes. See https://docs.microsoft.com/en-us/azure/aks/use-multiple-node-pools"
      },
      {
        "date": "2021-01-21T22:56:00.000Z",
        "voteCount": 1,
        "content": "Correct hai"
      },
      {
        "date": "2021-01-06T22:34:00.000Z",
        "voteCount": 1,
        "content": "Azure Batch AI is retired. \nhttps://docs.microsoft.com/en-us/previous-versions/azure/batch-ai/overview-what-happened-batch-ai\n\nAs for the answer, Downscaling to 0 would probably remove the node configuration, whereas we have to keep the node configuration."
      },
      {
        "date": "2020-11-05T13:42:00.000Z",
        "voteCount": 2,
        "content": "Azure Batch AI is retired since early 2019, I doubt this question still applies"
      },
      {
        "date": "2020-09-09T22:41:00.000Z",
        "voteCount": 3,
        "content": "downscaling(not autoscaling) to 0 nodes is now supported in aks. See this: https://github.com/Azure/AKS/issues/52#issuecomment-660897690"
      },
      {
        "date": "2020-10-25T19:22:00.000Z",
        "voteCount": 1,
        "content": "steps to do it     https://docs.microsoft.com/en-us/azure/aks/scale-cluster#scale-user-node-pools-to-0"
      },
      {
        "date": "2020-08-28T06:16:00.000Z",
        "voteCount": 1,
        "content": "Currently AKS supports at least 1 system node and 0 or more user nodes. So, what should be the correct answer?"
      },
      {
        "date": "2020-03-10T23:55:00.000Z",
        "voteCount": 2,
        "content": "With the source provided, shouldn't it be 0 nodes? Or is the controle plane counted as a node?"
      },
      {
        "date": "2020-02-09T22:06:00.000Z",
        "voteCount": 3,
        "content": "Seems to be correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11218-exam-ai-100-topic-1-question-14-discussion/",
    "body": "HOTSPOT -<br>You are designing an AI solution that will be used to find buildings in aerial pictures.<br>Users will upload the pictures to an Azure Storage account. A separate JSON document will contain for the pictures.<br>The solution must meet the following requirements:<br>\u2711 Store metadata for the pictures in a data store.<br>\u2711 Run a custom vision Azure Machine Learning module to identify the buildings in a picture and the position of the buildings' edges.<br>\u2711 Run a custom mathematical module to calculate the dimensions of the buildings in a picture based on the metadata and data from the vision module.<br>You need to identify which Azure infrastructure services are used for each component of the AI workflow. The solution must execute as quickly as possible.<br>What should you identify? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0001500004.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0001600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Blob Storage -<br>Containers and blobs support custom metadata, represented as HTTP headers.<br><br>Box 2: NV -<br>The NV-series enables powerful remote visualisation workloads and other graphics-intensive applications backed by the NVIDIA Tesla M60 GPU.<br>Note: The N-series is a family of Azure Virtual Machines with GPU capabilities. GPUs are ideal for compute and graphics-intensive workloads, helping customers to fuel innovation through scenarios like high-end remote visualisation, deep learning and predictive analytics.<br><br>Box 3: F -<br>F-series VMs feature a higher CPU-to-memory ratio. Example use cases include batch processing, web servers, analytics and gaming.<br>Incorrect:<br>A-series VMs have CPU performance and memory configurations best suited for entry level workloads like development and test.<br>References:<br>https://azure.microsoft.com/en-in/pricing/details/virtual-machines/series/",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-01T15:20:00.000Z",
        "voteCount": 7,
        "content": "Why not Cosmos DB for JSON documents?"
      },
      {
        "date": "2020-01-21T18:10:00.000Z",
        "voteCount": 1,
        "content": "It costs higher than Blob."
      },
      {
        "date": "2020-02-17T11:57:00.000Z",
        "voteCount": 15,
        "content": "it costs, but the ask on the scenario is to execute as fast as possible, which makes cosmos a better candidate"
      },
      {
        "date": "2020-04-07T09:30:00.000Z",
        "voteCount": 1,
        "content": "It depends on which consistency level you choose for cosmos DB. Doesn't necessarily need to be faster than blob storage."
      },
      {
        "date": "2021-05-09T23:19:00.000Z",
        "voteCount": 3,
        "content": "I think it should be Cosmos DB as the requirement is to have the fastest possible solution without worrying about cost"
      },
      {
        "date": "2021-05-20T14:42:00.000Z",
        "voteCount": 1,
        "content": "Yes. I am also wondering why the answer is not File storage, they are only talking about JSOn meta data, not the actual pictures."
      },
      {
        "date": "2021-01-06T22:37:00.000Z",
        "voteCount": 2,
        "content": "The answer seem correct in my opinion. Azure Blob is the recommended storage for HD images and similar large size file storage."
      },
      {
        "date": "2020-11-29T18:36:00.000Z",
        "voteCount": 3,
        "content": "This might be wrong, but I'm actually going to say file storage. There is metadata for every picture, and it would probably help to keep it organized. File storage would be quickly available, and would probably make analysis easier. The question doesn't ask about cost, so I didn't consider that"
      },
      {
        "date": "2021-02-16T11:21:00.000Z",
        "voteCount": 2,
        "content": "Store metadata only. Not files."
      },
      {
        "date": "2020-06-10T23:35:00.000Z",
        "voteCount": 3,
        "content": "\"Run a custom mathematical module to calculate the dimensions of the buildings in a picture based on the metadata and data from the vision module.\" - Since you are doing custom math module - why not create one that will take advantage of GPU acceleration - hence use NV series?"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3346-exam-ai-100-topic-1-question-15-discussion/",
    "body": "Your company has recently deployed 5,000 Internet-connected sensors for a planned AI solution.<br>You need to recommend a computing solution to perform a real-time analysis of the data generated by the sensors.<br>Which computing solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure HDInsight Storm cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Notification Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure HDInsight Hadoop cluster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure HDInsight R cluster"
    ],
    "answer": "C",
    "answerDescription": "Azure HDInsight makes it easy, fast, and cost-effective to process massive amounts of data.<br>You can use HDInsight to process streaming data that's received in real time from a variety of devices.<br>References:<br>https://docs.microsoft.com/en-us/azure/hdinsight/hadoop/apache-hadoop-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-07T13:43:00.000Z",
        "voteCount": 31,
        "content": "Should be a Storm Cluster. This allows for real-time event processing."
      },
      {
        "date": "2020-07-30T12:18:00.000Z",
        "voteCount": 1,
        "content": "Hadoop comes with Spark"
      },
      {
        "date": "2019-12-31T19:38:00.000Z",
        "voteCount": 10,
        "content": "I agree the correct answer should be Storm Cluster for real time analytic. https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview"
      },
      {
        "date": "2023-06-19T08:02:00.000Z",
        "voteCount": 1,
        "content": "To perform real-time analysis of data generated by 5,000 Internet-connected sensors, the recommended computing solution would be A. an Azure HDInsight Storm cluster"
      },
      {
        "date": "2021-05-26T00:54:00.000Z",
        "voteCount": 2,
        "content": "Storm cluster since when you instantiate a HDINsights service you select which type of cluster you need. (Hadoop, Spark, Storm..) Storm is the best suitable tech to the task and Haddop does not have realtime capabillities by itself"
      },
      {
        "date": "2021-04-20T10:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is A.\n\nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview"
      },
      {
        "date": "2021-01-21T23:01:00.000Z",
        "voteCount": 3,
        "content": "Strom is correct"
      },
      {
        "date": "2021-01-19T23:10:00.000Z",
        "voteCount": 2,
        "content": "Apache Hadoop: A framework that uses HDFS, YARN resource management, and a simple MapReduce programming model to process and analyze batch data in parallel.\nApache Storm: A distributed, real-time computation system for processing large streams of data fast. Storm is offered as a managed cluster in HDInsight.\n\nAs per above definition Storm Cluster fits better, as question says real-time."
      },
      {
        "date": "2021-01-06T22:40:00.000Z",
        "voteCount": 2,
        "content": "\"Apache Storm is a distributed, fault-tolerant, open-source computation system. You can use Storm to process streams of data in real time with Apache Hadoop. Storm solutions can also provide guaranteed processing of data, with the ability to replay data that wasn't successfully processed the first time.\"\nsource: https://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview\n\nMy verdict is HDInsight Storm Cluster, hence A"
      },
      {
        "date": "2020-12-26T01:54:00.000Z",
        "voteCount": 1,
        "content": "Answer  is an Azure HDInsight Storm cluster"
      },
      {
        "date": "2020-09-09T23:12:00.000Z",
        "voteCount": 4,
        "content": "This link says it all: https://storm.apache.org/                                                                                                Apache Storm is a free and open source distributed realtime computation system. Apache Storm makes it easy to reliably process unbounded streams of data, doing for realtime processing what Hadoop did for batch processing."
      },
      {
        "date": "2020-09-03T04:15:00.000Z",
        "voteCount": 3,
        "content": "It should be Storm \nhttps://docs.microsoft.com/en-us/azure/hdinsight/storm/apache-storm-overview#apache-storm-use-cases\n\nThe following are some common scenarios for which you might use Storm on HDInsight:\n\nInternet of Things (IoT)\nFraud detection\nSocial analytics\nExtraction, transformation, and loading (ETL)\nNetwork monitoring\nSearch\nMobile engagement"
      },
      {
        "date": "2020-08-30T06:50:00.000Z",
        "voteCount": 2,
        "content": "Real time - Apache Storm\nApache Storm is a distributed, fault-tolerant, open-source computation system. You can use Storm to process streams of data in real time with Apache Hadoop. Storm solutions can also provide guaranteed processing of data, with the ability to replay data that wasn't successfully processed the first time."
      },
      {
        "date": "2020-04-24T05:03:00.000Z",
        "voteCount": 1,
        "content": "A tricky question. Apache Storm is a Hadoop component available with HDInsight. So, generally speaking, it could be HDInsight Hadoop as well...I'm a bit confused.\nhttps://docs.microsoft.com/en-us/azure/hdinsight/hdinsight-component-versioning"
      },
      {
        "date": "2020-05-29T06:01:00.000Z",
        "voteCount": 6,
        "content": "Sorry, I think my statement is wrong. Storm is an HDInsight component, not an Apache Hadoop one. Both are HDInsight components. Hadoop modules consist in Hadoop Common, HDFS, YARN, MapReduce, Ozone, as stated here: https://hadoop.apache.org/\nSo, yes, Azure HDInsight Storm should be the correct answer."
      },
      {
        "date": "2020-04-07T09:42:00.000Z",
        "voteCount": 4,
        "content": "Azure HDInsight Storm cluster fits better for this scenario as we need a real-time event processing. Hadoop is for batch processing and would introduce higher latency than Storm."
      },
      {
        "date": "2020-01-15T06:55:00.000Z",
        "voteCount": 2,
        "content": "It should be Storm"
      },
      {
        "date": "2020-01-12T01:08:00.000Z",
        "voteCount": 2,
        "content": "definitely storm cluster"
      },
      {
        "date": "2019-08-26T11:44:00.000Z",
        "voteCount": 5,
        "content": "I agree.  Here is the link: https://storm.apache.org/"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/11304-exam-ai-100-topic-1-question-16-discussion/",
    "body": "HOTSPOT -<br>Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You plan to deploy an application that will perform image recognition. The application will store image data in two Azure Blob storage stores named Blob1 and<br>Blob2.<br>You need to recommend a security solution that meets the following requirements:<br>\u2711 Access to Blob1 must be controlled by using a role.<br>\u2711 Access to Blob2 must be time-limited and constrained to specific operations.<br>What should you recommend using to control access to each blob store? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0001900001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0002000001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-auth",
    "votes": [],
    "comments": [
      {
        "date": "2020-01-02T22:31:00.000Z",
        "voteCount": 33,
        "content": "Answer is right"
      },
      {
        "date": "2020-02-06T02:41:00.000Z",
        "voteCount": 3,
        "content": "Thanks for letting us know!! :D"
      },
      {
        "date": "2020-02-17T22:44:00.000Z",
        "voteCount": 21,
        "content": ":D But Vatan's comment was helpful to me as many answers are wrong, an additional confirmation adds up more reliably."
      },
      {
        "date": "2021-06-13T09:54:00.000Z",
        "voteCount": 2,
        "content": "Answer seems right. Thanks everyone for all the discussions."
      },
      {
        "date": "2021-06-10T08:50:00.000Z",
        "voteCount": 1,
        "content": "But I don\u2019t think Azure blobs support Azure Active Directory\u2026"
      },
      {
        "date": "2021-05-09T20:08:00.000Z",
        "voteCount": 1,
        "content": "Answer's right, and basically, \"Role\" == \"Azure AD\"."
      },
      {
        "date": "2021-01-06T22:42:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2020-04-05T08:34:00.000Z",
        "voteCount": 1,
        "content": "thanks guys :)"
      },
      {
        "date": "2020-02-09T03:16:00.000Z",
        "voteCount": 2,
        "content": "@SunnyS lol :D"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25535-exam-ai-100-topic-1-question-17-discussion/",
    "body": "You deploy an application that performs sentiment analysis on the data stored in Azure Cosmos DB.<br>Recently, you loaded a large amount of data to the database. The data was for a customer named Contoso, Ltd.<br>You discover that queries for the Contoso data are slow to complete, and the queries slow the entire application.<br>You need to reduce the amount of time it takes for the queries to complete. The solution must minimize costs.<br>What is the best way to achieve the goal? More than one answer choice may achieve the goal. Select the BEST answer.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the request units.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the partitioning strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the transaction isolation level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data to the Cosmos DB database."
    ],
    "answer": "B",
    "answerDescription": "Throughput provisioned for a container is divided evenly among physical partitions.<br>Incorrect:<br>Not A: Increasing request units would also improve throughput, but at a cost.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning",
    "votes": [],
    "comments": [
      {
        "date": "2020-11-08T22:57:00.000Z",
        "voteCount": 6,
        "content": "More than one answer choice may achieve the goal. \n\nA. Change the request units.\n Request unit is a performance currency abstracting the system resources such as CPU, IOPS, and memory that are required to perform the database operations supported by Azure Cosmos DB.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/request-units\n\nB. Change the partitioning strategy.\nImprove performance. Data access operations on each partition take place over a smaller volume of data. Correctly done, partitioning can make your system more efficient. Operations that affect more than one partition can run in parallel.\nhttps://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning"
      },
      {
        "date": "2020-07-12T17:33:00.000Z",
        "voteCount": 5,
        "content": "Seems to be correct"
      },
      {
        "date": "2023-06-19T08:03:00.000Z",
        "voteCount": 1,
        "content": "To reduce the amount of time it takes for queries to complete on the Contoso data in Azure Cosmos DB, while minimizing costs, the BEST way to achieve the goal is B. Change the partitioning strategy."
      },
      {
        "date": "2021-02-21T21:21:00.000Z",
        "voteCount": 2,
        "content": "Question Say More than one answer choice may achieve the goal but Choose the Best Answer, it would be B and if it says to choose two then we can go with A &amp; B."
      },
      {
        "date": "2021-05-25T04:39:00.000Z",
        "voteCount": 1,
        "content": "It's a trick question - The question does have provision to give multiple answers but it also says to choose the BEST answer. So B is indeed the right answer for this question."
      },
      {
        "date": "2021-02-15T18:40:00.000Z",
        "voteCount": 1,
        "content": "This is about provisioning throughput in Azure Cosmos DB.\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/set-throughput\nBoth A and B serve the purpose. RU's impact the cost directly, so it makes Partitioning Strategy (B) a lower cost option. \nSince the question asks for more than one answer, both A and B are correct... if this is a multichoice question."
      },
      {
        "date": "2020-11-22T22:24:00.000Z",
        "voteCount": 2,
        "content": "A and B both work but as you want to minimize cost, B becomes the better choice."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/4404-exam-ai-100-topic-1-question-18-discussion/",
    "body": "You have an AI application that uses keys in Azure Key Vault.<br>Recently, a key used by the application was deleted accidentally and was unrecoverable.<br>You need to ensure that if a key is deleted, it is retained in the key vault for 90 days.<br>Which two features should you configure? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe expiration date on the keys",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSoft delete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPurge protection",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuditors",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe activation date on the keys"
    ],
    "answer": "BC",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning",
    "votes": [],
    "comments": [
      {
        "date": "2019-10-31T05:17:00.000Z",
        "voteCount": 19,
        "content": "Answer is BC - but the link reference should be this : https://docs.microsoft.com/en-us/azure/key-vault/key-vault-ovw-soft-delete"
      },
      {
        "date": "2019-09-14T04:28:00.000Z",
        "voteCount": 10,
        "content": "Soft-delete behavior\nWith this feature, the DELETE operation on a key vault or key vault object is a soft-delete, effectively holding the resources for a given retention period (90 days), while giving the appearance that the object is deleted. The service further provides a mechanism for recovering the deleted object, essentially undoing the deletion.\n\nSoft-delete is an optional Key Vault behavior and is not enabled by default in this release. It can be turned on via CLI or Powershell.\n\nPurge protection\nWhen purge protection is on, a vault or an object in deleted state cannot be purged until the retention period of 90 days has passed. These vaults and objects can still be recovered, assuring customers that the retention policy will be followed.\n\nPurge protection is an optional Key Vault behavior and is not enabled by default. It can be turned on via CLI or Powershell."
      },
      {
        "date": "2023-06-19T08:05:00.000Z",
        "voteCount": 1,
        "content": "two features that should be configured to ensure that deleted keys are retained in Azure Key Vault for 90 days are B. Soft delete and C. Purge protection."
      },
      {
        "date": "2019-09-14T04:14:00.000Z",
        "voteCount": 3,
        "content": "Soft Delete is correct answer\n\u201cSoft delete\u201d is a Key Vault feature that may be enabled on a vault. When this is true, if a Key Vault is deleted, it is recoverable for 90 days. It disappears from the Azure portal and it looks like the Key Vault has been completely deleted, like any other resource or service in Azure. This isn\u2019t the case, however. It is held by Azure for 90 days and can be restored for any reason. Because of this precaution, a new Key Vault with the same name cannot be added to the Azure subscription until the \u201csoft deleted\u201d vault is truly deleted."
      },
      {
        "date": "2019-08-30T21:11:00.000Z",
        "voteCount": 1,
        "content": "Can it be The expiration date on the keys and The activation date on the keys."
      },
      {
        "date": "2020-11-25T12:03:00.000Z",
        "voteCount": 1,
        "content": "no, but you corrected yourself later. has to be B and C"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3367-exam-ai-100-topic-1-question-19-discussion/",
    "body": "DRAG DROP -<br>You are designing an AI solution that will analyze media data. The data will be stored in Azure Blob storage.<br>You need to ensure that the storage account is encrypted by using a key generated by the hardware security module (HSM) of your company.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0002300001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0002400001.png\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-encryption-keys-portal https://docs.microsoft.com/en-us/azure/key-vault/key-vault-hsm-protected-keys",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-08T08:39:00.000Z",
        "voteCount": 45,
        "content": "I believe the answer is: 1: generate an encryption key 2: upload a key to key vault 3: enable customer encryption keys"
      },
      {
        "date": "2019-08-30T21:38:00.000Z",
        "voteCount": 2,
        "content": "can you please describe why u think that..."
      },
      {
        "date": "2019-09-01T01:50:00.000Z",
        "voteCount": 5,
        "content": "It is a user generated key.  You don't want it to be lost, hence store it in the Key Vault.  Also, the service using encryption will try to find it in the Key Vault by default since that is the recommended best practice."
      },
      {
        "date": "2021-01-31T13:40:00.000Z",
        "voteCount": 2,
        "content": "I don't agree because key will be generated and stored in HSM."
      },
      {
        "date": "2020-12-26T01:58:00.000Z",
        "voteCount": 6,
        "content": "answer is: 1: generate an encryption key 2: upload a key to key vault 3: enable customer encryption keys"
      },
      {
        "date": "2021-02-09T10:33:00.000Z",
        "voteCount": 1,
        "content": "Why does it say upload \"A\" key to key vault and not \"The\" key (that was encrypted)? Typo? I think not! The links shared in the comments here provide descriptions of how things get done but is it precisely addressing the given scenario?"
      },
      {
        "date": "2021-07-27T17:03:00.000Z",
        "voteCount": 1,
        "content": "First, query for the key vault URI by calling az keyvault show, and for the key version by calling az keyvault key list-versions. Then call az storage account update to update the storage account's encryption settings to use the new version of the key, as shown in the previous example.\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/customer-managed-keys-configure-key-vault-hsm"
      },
      {
        "date": "2021-07-14T10:16:00.000Z",
        "voteCount": 1,
        "content": "from this link here https://docs.microsoft.com/en-us/azure/storage/common/storage-encryption-keys-portal I think we create the vault which is the storage endpoint, upload the keys same as add keys and then enable encryption or Configure encryption with customer-managed keys."
      },
      {
        "date": "2021-03-16T08:16:00.000Z",
        "voteCount": 4,
        "content": "If you go to Azure Portal and select your storage account then select Encryption option from Settings this how you will get option in sequence:\n1. Encryption Type: Microsoft-managed keys or Customer-managed keys\n     Once you select \"Customer-managed keys\" you will get add option called 'Key Selection'\n2. Key Selection  has two options for \"Encryption Key\"\n    1. Select from 'key vault' ( where you need to identity your key vault &amp; Encryption key it \n        will allow you to create a new 'key vault' &amp; 'Encryption Key' or upload existing key '\n     2. Enter key URI ( If you already have URI for key vault &amp; keys).\n      Summary : 1. Enable Customer Encryption Keys 2. Generate Encryption Key 3. Upload \n                         key to Azure vault."
      },
      {
        "date": "2020-10-14T06:09:00.000Z",
        "voteCount": 5,
        "content": "https://docs.microsoft.com/en-us/azure/storage/common/customer-managed-keys-configure-key-vault-hsm contains same answer as mentioned in the question - HSM is the clue here"
      },
      {
        "date": "2020-09-10T00:59:00.000Z",
        "voteCount": 2,
        "content": "Link provided in solution should be updated to do this: https://docs.microsoft.com/en-us/azure/key-vault/keys/hsm-protected-keys   as the previous link doesn't work anymore"
      },
      {
        "date": "2020-06-25T01:25:00.000Z",
        "voteCount": 4,
        "content": "agreed with exam taker5. Proof is there : https://docs.microsoft.com/en-us/azure/storage/common/storage-encryption-keys-portal"
      },
      {
        "date": "2020-10-08T02:32:00.000Z",
        "voteCount": 2,
        "content": "yes this proves exam_taker5's answer"
      },
      {
        "date": "2021-02-16T07:05:00.000Z",
        "voteCount": 1,
        "content": "agree with you"
      },
      {
        "date": "2020-04-16T01:24:00.000Z",
        "voteCount": 1,
        "content": "Agree to Bharat, key needs to be uploaded to key vault as it's user generated"
      },
      {
        "date": "2019-08-27T12:36:00.000Z",
        "voteCount": 2,
        "content": "Agreed again"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3368-exam-ai-100-topic-1-question-20-discussion/",
    "body": "You plan to implement a new data warehouse for a planned AI solution.<br>You have the following information regarding the data warehouse:<br>\u2711 The data files will be available in one week.<br>\u2711 Most queries that will be executed against the data warehouse will be ad-hoc queries.<br>\u2711 The schemas of data files that will be loaded to the data warehouse will change often.<br>\u2711 One month after the planned implementation, the data warehouse will contain 15 TB of data.<br>You need to recommend a database solution to support the planned implementation.<br>What two solutions should you include in the recommendation? Each correct answer is a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Hadoop",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Spark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Microsoft Azure SQL database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn Azure virtual machine that runs Microsoft SQL Server"
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-08T09:08:00.000Z",
        "voteCount": 22,
        "content": "I believe the answer should be Hadoop and Spark. Both of these are intended for unstructured data, and the question specifies that the schema will be changing constantly. Both also excel with big data (4TB over the first month qualifies)"
      },
      {
        "date": "2019-08-27T01:43:00.000Z",
        "voteCount": 5,
        "content": "I think Hadoop... is correct... not SQl not Spark.... as Spark is for analytics not for storage"
      },
      {
        "date": "2020-03-11T05:33:00.000Z",
        "voteCount": 1,
        "content": "But Hadoop nor Spark are database systems right? So it doesn't answer the question"
      },
      {
        "date": "2019-10-28T03:59:00.000Z",
        "voteCount": 12,
        "content": "You ask for two solutions but the answer is only SQL."
      },
      {
        "date": "2023-06-19T08:08:00.000Z",
        "voteCount": 1,
        "content": "two recommended solutions for supporting the planned data warehouse implementation are B. Apache Spark and C. A Microsoft Azure SQL database."
      },
      {
        "date": "2021-07-14T10:58:00.000Z",
        "voteCount": 1,
        "content": "With Azure SQL Database, you can create a highly available and high-performance data storage layer for the applications and solutions in Azure. SQL Database can be the right choice for a variety of modern cloud applications because it enables you to process both relational data and non-relational structures, such as graphs, JSON, spatial, and XML.\n\nThe hyperscale service tier for single databases enables you to scale to 100 TB, with fast backup and restore capabilities.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview"
      },
      {
        "date": "2021-06-27T06:28:00.000Z",
        "voteCount": 1,
        "content": "the correct answer is B and D"
      },
      {
        "date": "2021-02-09T10:47:00.000Z",
        "voteCount": 2,
        "content": "By asking for a Database Solution, the question asks for a combination given the requirements. Part of the requirements can be addressed by SQL and part by No SQL or Big Data. Spark is only for Analytics but can still be used here. However, I would go with Hadoop and Azure SQL as a combined solution to address the 4 requirements.\nI may be wrong but this is just how I see the \"solution\" since none of these offerings/services/products can cater solely to all requirements.\nAlso, I am wondering why it says Azure SQL DB and not Azure SQL DWH (or Synapse, but this an old question before Synapse was born)."
      },
      {
        "date": "2021-02-09T10:54:00.000Z",
        "voteCount": 3,
        "content": "Update: Then again, looking at the Spark connector available for SQL Server and Azure SQL DB, I'm thinking Spark is still the right one instead of Hadoop.\n\nhttps://docs.microsoft.com/en-us/sql/connect/spark/connector?view=sql-server-ver15\n\nCheck the link. This is kind of what the question is asking, about ad-hoc etc.\n\nHowever, Hyperscale service tier of Azure SQL DB can grow to 100TB. SQL Server on a VM can do it easily and the Spark connector is available for that too.\n\nChanging my answer to Spark... unsure about SQL DB or SQL Server VM. Think I'll go with SQL DB."
      },
      {
        "date": "2021-05-27T05:47:00.000Z",
        "voteCount": 1,
        "content": "It can't be SQL as the schema of the data keeps changing. Hadoop and spark have data storage systems that cater to the requirement of the question."
      },
      {
        "date": "2021-02-08T08:03:00.000Z",
        "voteCount": 1,
        "content": "AB is correct"
      },
      {
        "date": "2020-10-05T14:35:00.000Z",
        "voteCount": 2,
        "content": "I think A&amp;C.\nFirst two requirement is for SQL. Because data is ad-hoc and available 1 week. So not much storage here\nThe remaining of requirement is data warehouse. It is Hadoop: 15TB"
      },
      {
        "date": "2020-09-12T11:24:00.000Z",
        "voteCount": 5,
        "content": "\" The schemas of data files that will be loaded to the data warehouse will change often\"\nThis rules out SQL. It should be a No SQL option. Hadoop stores data in HDFS files system. Spark also has its storage. So it will be spark and hadoop"
      },
      {
        "date": "2020-11-04T15:13:00.000Z",
        "voteCount": 1,
        "content": "schema of the data files will change often, but not the data warehouse, these two are separated"
      },
      {
        "date": "2020-09-10T06:50:00.000Z",
        "voteCount": 1,
        "content": "C is a correct option based on the link provided. Also hyperscale option in sql database supports upto 100 TB. Don't know what the other answer might be."
      },
      {
        "date": "2020-08-17T03:20:00.000Z",
        "voteCount": 5,
        "content": "Is there any consensus on this??"
      },
      {
        "date": "2021-01-07T17:11:00.000Z",
        "voteCount": 1,
        "content": "I am wondering the same thing!!! Seems like no agreement yet"
      },
      {
        "date": "2020-07-09T12:36:00.000Z",
        "voteCount": 9,
        "content": "The question is to \u201crecommend a DATABASE solution\u201d, Spark and Hadoop are not databases, which means both SQL answers (C&amp;D) would be the correct choices.  Please correct me if I am wrong."
      },
      {
        "date": "2020-11-04T15:16:00.000Z",
        "voteCount": 2,
        "content": "I agree on both C&amp;D should be the answer, either Spark or Hadoop are databases or at least spark is not. SQL Azure Hyperscale supports up to 100 TB"
      },
      {
        "date": "2020-07-03T12:01:00.000Z",
        "voteCount": 4,
        "content": "I think the original answer provided is correct based on the solution reported by the link https://docs.microsoft.com/en-us/azure/sql-database/saas-multitenantdb-adhoc-reporting. Since it use elastic query there is only one answer: SQL database."
      },
      {
        "date": "2020-05-04T13:04:00.000Z",
        "voteCount": 1,
        "content": "Azure Hadoop in HDinsight could be a right answer \nhttps://azure.microsoft.com/en-in/blog/azure-hdinsight-interactive-query-simplifying-big-data-analytics-architecture-and-operations/"
      },
      {
        "date": "2020-04-21T02:57:00.000Z",
        "voteCount": 3,
        "content": "A and B"
      },
      {
        "date": "2020-04-05T08:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is AB"
      },
      {
        "date": "2020-03-29T09:22:00.000Z",
        "voteCount": 3,
        "content": "Azure SQL can support more than 15TB\nhttps://docs.microsoft.com/en-us/azure/sql-database/sql-database-service-tier-hyperscale"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/4933-exam-ai-100-topic-1-question-21-discussion/",
    "body": "You need to build a solution to monitor Twitter. The solution must meet the following requirements:<br>\u2711 Send an email message to the marketing department when negative Twitter messages are detected.<br>\u2711 Run sentiment analysis on Twitter messages that mention specific tags.<br>\u2711 Use the least amount of custom code possible.<br>Which two services should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cognitive Services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Logic Apps"
    ],
    "answer": "BE",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/streaming-technologies https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-09T01:00:00.000Z",
        "voteCount": 51,
        "content": "Should be Logic Apps (Azure has a dedicated Twitter connector) and Cognitive Services to score for sentiment."
      },
      {
        "date": "2019-12-12T03:25:00.000Z",
        "voteCount": 2,
        "content": "What about the custom code requirement. ?"
      },
      {
        "date": "2020-01-01T02:54:00.000Z",
        "voteCount": 5,
        "content": "Logic App is the answer for that."
      },
      {
        "date": "2020-02-08T11:02:00.000Z",
        "voteCount": 1,
        "content": "I agree"
      },
      {
        "date": "2021-02-09T08:56:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2020-02-17T20:47:00.000Z",
        "voteCount": 12,
        "content": "both logic apps and stream analytics are here are mechanisms for capturing and routing the data. noe of them are actually doing the analysis, tht is done by the cognitive services. so with request asking for 2 services and least ammount of code, i would say logic apps+cognitive"
      },
      {
        "date": "2020-05-29T07:54:00.000Z",
        "voteCount": 2,
        "content": "It's a valid theory, but you'd need an azure function to classify sentiment scores, which means custom code:\nhttps://docs.microsoft.com/it-it/azure/azure-functions/functions-twitter-email\n\nUsing a twitter client application instead, you could get negative tweets through a simple SQL-like query, in Stream Analytics, and then send the output to Logic Apps (via Event Hubs) for email sending:\nhttps://github.com/uglide/azure-content/blob/master/articles/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends.md\n\nA very tricky question :) I think all depends on what it is meant for \"least amount of custom code possible\""
      },
      {
        "date": "2023-06-19T08:10:00.000Z",
        "voteCount": 1,
        "content": "two services that should be included in the solution to monitor Twitter and meet the given requirements are B. Azure Stream Analytics and D. Azure Cognitive Services."
      },
      {
        "date": "2021-05-31T00:51:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-06-05T14:00:00.000Z",
        "voteCount": 2,
        "content": "Hope you did well in the exam. \nI was wondering, are most of the questions in the exam from these questions? or were there many new questions?"
      },
      {
        "date": "2021-05-25T05:34:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct - \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends\n\n\"Real-time Twitter trend analysis is a great example of an analytics tool because the hashtag subscription model enables you to listen to specific keywords (hashtags) and develop sentiment analysis of the feed.\""
      },
      {
        "date": "2021-05-11T02:10:00.000Z",
        "voteCount": 1,
        "content": "I think it should be Text Analytics ( cognitive services ) and Logic APP....I was dome tweets analysis but I used the stream analytics and event hub that send the data to the blob storage.....Logic App has its connector already"
      },
      {
        "date": "2021-04-22T23:53:00.000Z",
        "voteCount": 1,
        "content": "I feel it is Azure logic apps and cognitive services"
      },
      {
        "date": "2021-04-06T04:11:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/azure-functions/functions-twitter-email\n use Azure Functions with Logic Apps and Cognitive Services on Azure to run sentiment analysis from Twitter posts. An HTTP trigger function categorizes tweets as green, yellow, or red based on the sentiment score. An email is sent when poor sentiment is detected.\n\nsteps:\nCreate a Cognitive Services API Resource.\nCreate a function that categorizes tweet sentiment.\nCreate a logic app that connects to Twitter.\nAdd sentiment detection to the logic app.\nConnect the logic app to the function.\nSend an email based on the response from the function."
      },
      {
        "date": "2021-07-15T03:54:00.000Z",
        "voteCount": 1,
        "content": "I think a key diff in this link and the one in the solution is that this one ticks the send notification to email and the one in the solution doesn't really show... But this link indicates that an outlook mail for notification would be needed."
      },
      {
        "date": "2021-03-27T07:30:00.000Z",
        "voteCount": 2,
        "content": "Given Answer is right! https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends"
      },
      {
        "date": "2021-03-07T19:06:00.000Z",
        "voteCount": 1,
        "content": "I'd go with Cognitive Services (Text Analytics) and Logic Apps"
      },
      {
        "date": "2021-02-10T12:53:00.000Z",
        "voteCount": 3,
        "content": "as the consensus is quite clear and well proven with sources (logicapps and cognitive services), could one of the mods change the correct answers?"
      },
      {
        "date": "2021-01-11T11:56:00.000Z",
        "voteCount": 2,
        "content": "https://www.linkedin.com/pulse/7-steps-twitter-sentiment-analysis-using-azure-services-sujit-kadam/\nLogic Apps and Cognitive Services"
      },
      {
        "date": "2021-01-11T11:16:00.000Z",
        "voteCount": 1,
        "content": "1. Need Cognitive services for Sentiment analysis and LUIS...\n\n2. Need Logic app which will trigger stuffs reg twitter. \n From my aspect,, below flow helps to understand\nTwitter -&gt; Bot -&gt; api gateway -&gt; logic app -&gt; cognitiveServices"
      },
      {
        "date": "2020-12-26T02:04:00.000Z",
        "voteCount": 2,
        "content": "Logic Apps  and Cognitive Services"
      },
      {
        "date": "2020-11-20T01:15:00.000Z",
        "voteCount": 1,
        "content": "I think that both methods are work fine. \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-sentiment-analysis\nIt's looks like that Cognitive Services is less customer code."
      },
      {
        "date": "2021-02-09T11:16:00.000Z",
        "voteCount": 1,
        "content": "Agree. Cognitive Services is lesser code as compared to the 1-2 lines of the SQL-like query to put into Stream Analytics. saswata30 has a good reference link too... although not an official MS documentation.\nI'm going with Logic Apps and Cognitive Services"
      },
      {
        "date": "2020-09-18T05:21:00.000Z",
        "voteCount": 1,
        "content": "D &amp; E\nReferences:\n\nhttps://mscloud.be/azure/real-time-twitter-sentiment-analysis-with-azure-cognitive-services/\n\nhttps://medium.com/windows-developer/develop-a-mind-reading-twitter-client-with-azure-cognitive-services-f73f0c937671\n\nhttps://www.youtube.com/watch?v=LIIDiNGNEio"
      },
      {
        "date": "2020-09-10T07:17:00.000Z",
        "voteCount": 4,
        "content": "The answer provided is correct. \nhttps://medium.com/@dhruvkinger813/social-media-sentimental-analysis-using-azure-logic-apps-5dc8147c42ee \nThe link provided above shows clearly that we can easily send emails when negative tweet is detected. \nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-twitter-sentiment-analysis-trends\nThe above link shows that it is possible to run sentiment analysis on tweets having specific tags easily using stream analytics with few simple SQL queries, no code.\nhttps://towardsdatascience.com/using-azure-cognitive-services-for-sentiment-analysis-of-trumps-tweets-part-1-f42d68c7e40a\nThe above link shows that using azure cognitive services requires lot of code for this usecase."
      },
      {
        "date": "2021-01-19T12:31:00.000Z",
        "voteCount": 1,
        "content": "In your first link it is clearly written that Text Analytics (which is a Service on the Cognitive Services) will be used. So it should be D &amp; E"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/3369-exam-ai-100-topic-1-question-22-discussion/",
    "body": "HOTSPOT -<br>You need to configure security for an Azure Machine Learning service used by groups of data scientists. The groups must have access to only their own experiments and must be able to grant permissions to the members of their team.<br>What should you do? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0002700001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0002800001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/machine-learning-server/operationalize/configure-roles#how-are-roles-assigned https://docs.microsoft.com/en-us/azure/machine-learning/service/how-to-assign-roles",
    "votes": [],
    "comments": [
      {
        "date": "2019-08-08T10:05:00.000Z",
        "voteCount": 29,
        "content": "I believe the second answer should be owner. They need to be able to change role assignments for other members of their team."
      },
      {
        "date": "2019-08-27T01:42:00.000Z",
        "voteCount": 10,
        "content": "I believe the answer is correct \"Contributor\". as the question says only their own experiment. adding to that contributor can give permission to there projects. \" The groups must have access to only their own experiments and must be able to grant permissions to the members of their team\""
      },
      {
        "date": "2020-01-01T02:58:00.000Z",
        "voteCount": 16,
        "content": "According to this: https://docs.microsoft.com/en-us/azure/role-based-access-control/overview\n\nContributor can't grant the access."
      },
      {
        "date": "2020-07-27T16:03:00.000Z",
        "voteCount": 1,
        "content": "Contributor can't grant access to resources yes but the question is in reference to a workspace not a resource"
      },
      {
        "date": "2020-09-22T06:50:00.000Z",
        "voteCount": 1,
        "content": "In the second link provided in the explanation is this line: \"An Azure Machine Learning workspace is an Azure resource. Like other Azure resources, when a new Azure Machine Learning workspace is created, it comes with three default roles.\""
      },
      {
        "date": "2019-09-16T23:00:00.000Z",
        "voteCount": 20,
        "content": "Owner\tLets you manage everything, including access to resources.\nContributor\tLets you manage everything except granting access to resources.\n\nSecond answer should be owner"
      },
      {
        "date": "2019-12-31T21:44:00.000Z",
        "voteCount": 3,
        "content": "Agreed with Karl. https://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles"
      },
      {
        "date": "2021-06-29T07:12:00.000Z",
        "voteCount": 1,
        "content": "Contributor can grant access; \nGrants full access to manage all resources, but does not allow you to assign roles in Azure RBAC, manage assignments in Azure Blueprints, or share image galleries (From Microsoft Site)"
      },
      {
        "date": "2021-05-31T14:23:00.000Z",
        "voteCount": 1,
        "content": "Owner - Has full access to all resources including the right to delegate access to others. Contributor - Can create and manage all types of Azure resources but can't grant access to others.\n\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-steps"
      },
      {
        "date": "2021-03-07T19:15:00.000Z",
        "voteCount": 1,
        "content": "Workspace and owner"
      },
      {
        "date": "2021-02-24T13:34:00.000Z",
        "voteCount": 1,
        "content": "Thinks the second answer should be \"Contributor\". Contributor can manages their published services (which in the configuration it is \"owner\") so the contributor will maintain some level of functionality like granting access to others on the service they published (pure speculation). \nAnother reasoning is in the link here with a reference for different personas:https://docs.microsoft.com/en-us/machine-learning-server/operationalize/configure-roles#how-are-roles-assigned\nThe data scientists mentioned in this question will be equivalent \"R programmer\" or \"Python Programmer\" personas where they will only focuses on the experiments they are working on."
      },
      {
        "date": "2021-03-20T12:55:00.000Z",
        "voteCount": 1,
        "content": "\"Contributor - Can create and manage all types of Azure resources but can't grant access to others.\"\nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/role-assignments-steps"
      },
      {
        "date": "2021-02-19T23:55:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/role-based-access-control/rbac-and-directory-admin-roles : Contributor\t\nCreate and manage all of types of Azure resources\nCreate a new tenant in Azure Active Directory\nCannot grant access to others :  contributor cannot grant access/permissions to others.\nOwner is th right answer"
      },
      {
        "date": "2021-02-09T11:28:00.000Z",
        "voteCount": 2,
        "content": "Owner!\nOwner is scoped for the Workspace here and cannot access the RG anyway.\nGiving only Contributor cannot do squat for members of the team."
      },
      {
        "date": "2021-02-07T03:45:00.000Z",
        "voteCount": 1,
        "content": "Second answer needs to be Owner, as the mentioned link clearly states:\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles\nManage workspace access\nIf you're an owner of a workspace, you can add and remove roles for the workspace. Y"
      },
      {
        "date": "2021-01-14T04:28:00.000Z",
        "voteCount": 3,
        "content": "hold on.. let me put my thoughts//\n\n1. The groups must have access to only their own experiments and \n2.must be able to grant permissions to the members of their team.\n\nHere point is clear. \nPoint 2 meaning means  user to authenticate to workspace . Isn;t it...[[[ it doesn't mean to change role assignments. ]]]\n\nSo Contributor is Fine."
      },
      {
        "date": "2021-02-10T12:56:00.000Z",
        "voteCount": 1,
        "content": "\"must be able to grant access to members of their team\" clearly sounds like adding/changing role assignments to me"
      },
      {
        "date": "2021-01-11T11:26:00.000Z",
        "voteCount": 1,
        "content": "Manage workspace access\nIf you're an owner of a workspace, you can add and remove roles for the workspace. You can also assign roles to users. Use the following links to discover how to manage access:\n\nAzure portal UI\nPowerShell\nAzure CLI\nREST API\nAzure Resource Manager templates\nIf you have installed the Azure Machine Learning CLI, you can use CLI commands to assign roles to users:\n\nAzure CLI\n\nCopy\n\nTry It\naz ml workspace share -w &lt;workspace_name&gt;"
      },
      {
        "date": "2020-11-20T07:37:00.000Z",
        "voteCount": 1,
        "content": "I just read this in Azure Portal when I try to add new Machine Learning Studio (classic) Workspace:\n \"As the owner of a Workspace, you can invite other users to share the Workspace so you can collaborate with them on predictive analytics solutions.\""
      },
      {
        "date": "2020-11-09T00:07:00.000Z",
        "voteCount": 2,
        "content": "answer Workplace and Owner\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles"
      },
      {
        "date": "2020-10-13T02:30:00.000Z",
        "voteCount": 1,
        "content": "Owner--&gt;\tFull access to the workspace, including the ability to view, create, edit, or delete (where applicable) assets in a workspace. Additionally, you can change role assignments.\nhttps://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#default-roles"
      },
      {
        "date": "2020-10-04T23:56:00.000Z",
        "voteCount": 2,
        "content": "Contributor is correct. \nhttps://docs.microsoft.com/en-us/azure/role-based-access-control/built-in-roles#contributor"
      },
      {
        "date": "2020-09-22T06:53:00.000Z",
        "voteCount": 1,
        "content": "nowhere could I find that contributor can grant permissions to other members. second one has to be owner like most have said here"
      },
      {
        "date": "2020-09-07T20:44:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/machine-learning/how-to-assign-roles#default-roles\nWorkspace &amp; Owner"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/4918-exam-ai-100-topic-1-question-23-discussion/",
    "body": "You plan to build an application that will perform predictive analytics. Users will be able to consume the application data by using Microsoft Power BI or a custom website.<br>You need to ensure that you can audit application usage.<br>Which auditing solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Storage Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Application Insights",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure diagnostics logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) reporting"
    ],
    "answer": "D",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-audit-logs",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-08T14:05:00.000Z",
        "voteCount": 36,
        "content": "This answer is wrong. Application usage is performed by Application Insights"
      },
      {
        "date": "2020-07-30T12:44:00.000Z",
        "voteCount": 1,
        "content": "App Insights is more for monitoring to improve performance. AD logs all system activities, its audit-log concept is for this specific reason"
      },
      {
        "date": "2020-01-06T18:37:00.000Z",
        "voteCount": 14,
        "content": "App Insights monitors apps THEMSELVES, while Azure AD monitors application usage. This exam is built to trick us. Not a fan."
      },
      {
        "date": "2020-02-17T21:00:00.000Z",
        "voteCount": 3,
        "content": "Azure AD can audit managed applications, which is a totally different beast than the one in this requirement. it is true that the word \"usage\" is very shady here, but an app usage can also be inferred by counting number of requests, unique or not, etc. so App insights is the winner here"
      },
      {
        "date": "2020-02-18T00:09:00.000Z",
        "voteCount": 1,
        "content": "As question provides two possible services, insight cannot be used."
      },
      {
        "date": "2020-03-11T06:46:00.000Z",
        "voteCount": 2,
        "content": "Looking at Azure AD, it seems that usage in terms of the number of requests etc. is not covered by this solution (rather creation and deletion of elements). Hence, most logically Insights should be correct."
      },
      {
        "date": "2023-06-19T08:14:00.000Z",
        "voteCount": 1,
        "content": "To ensure that you can audit the usage of your predictive analytics application, the recommended auditing solution is B. Azure Application Insights."
      },
      {
        "date": "2023-06-19T08:15:00.000Z",
        "voteCount": 1,
        "content": "Azure Application Insights is a comprehensive application performance monitoring and diagnostics service provided by Microsoft. It offers rich telemetry data, including request and dependency tracking, exceptions, performance counters, and custom events, which can be used for auditing purposes. Application Insights provides insights into how users interact with your application, including the ability to track user sessions, page views, and custom events.\n\nBy integrating Azure Application Insights into your application, you can capture and analyze usage patterns, monitor performance metrics, and track user interactions. This allows you to audit application usage, gain insights into user behavior, and identify any issues or bottlenecks."
      },
      {
        "date": "2023-06-19T06:14:00.000Z",
        "voteCount": 1,
        "content": "Azure Application Insights is the auditing solution that you should use.\nhttps://github.com/MicrosoftDocs/azure-docs/blob/main/articles/security/develop/threat-modeling-tool-auditing-and-logging.md"
      },
      {
        "date": "2021-03-16T10:10:00.000Z",
        "voteCount": 2,
        "content": "Azure AD reporting is correct choice;\nhttps://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-usage-insights-report"
      },
      {
        "date": "2021-03-15T22:49:00.000Z",
        "voteCount": 1,
        "content": "It's Azure AD Reports --&gt; Azure Active Directory (Azure AD) reports provide a comprehensive view of activity in your environment. The provided data enables you to:\nDetermine how your apps and services are utilized by your users\n\nhttps://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/overview-reports"
      },
      {
        "date": "2021-02-17T19:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct\nIf you want to review audit data related to your applications, you can find a filtered view under Audit logs in the Activity section of the Enterprise applications blade. This entry point has Enterprise applications preselected as the Application Type."
      },
      {
        "date": "2021-02-16T08:04:00.000Z",
        "voteCount": 1,
        "content": "I choose Azure Application Insights, which is used to monitor live application. With Azure Active Directory (Azure AD) reports, you can get the information you need to determine how your environment is doing. It\u2019s just audit log, which is related to changing resource, not related to app usage."
      },
      {
        "date": "2020-09-27T22:25:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-usage-insights-report\nDepending on the meaning of \"usage\". If it means how many times of using PowerBI vs how many times of using the custom website, I will go for the given answer - A AD Reporting."
      },
      {
        "date": "2021-02-09T11:44:00.000Z",
        "voteCount": 3,
        "content": "Question asks to audit \"Application Usage\". The link you shared measures Sign-in activity. It is a trick question, but App Insights look more accurate (and broader usage metrics than AAD) because it can even tell how many times the app was used... which is the only relevant insight that AAD provides."
      },
      {
        "date": "2020-09-10T09:47:00.000Z",
        "voteCount": 3,
        "content": "Read these 2 links:\nhttps://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/concept-audit-logs#enterprise-applications-audit-logs\nhttps://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview\nI feel it is app insights. the diagram provided in the link also shows its integration with apps and power BI. Also we want logs about application usage here not the type of scenarios the first link shows. So the phrases audit logs were meant to trick us into thinking it is AD"
      },
      {
        "date": "2020-10-25T06:16:00.000Z",
        "voteCount": 1,
        "content": "In the first link you can see the type of things you get to know with aad. That is not what the question means by \"application usage\". So the word audit has been used to mislead people towards aad."
      },
      {
        "date": "2020-09-03T10:40:00.000Z",
        "voteCount": 1,
        "content": "Application insights is the correct solution only if want usage analysis https://docs.microsoft.com/en-us/azure/azure-monitor/app/usage-overview, as it can show in general 5 users did such and such activity, but not which 5 users. If you want to audit application usage ( as the questions asks - Which audit solution ) , then Azure AD is the correct answer. In this case Azure AD is the correct answer."
      },
      {
        "date": "2020-08-27T12:41:00.000Z",
        "voteCount": 5,
        "content": "The answer Azure AD report is wrong. Azure AD report log is for auditing activity of users for security reasons. For auditing and analyzing application usage (telemetry) is Application Insights."
      },
      {
        "date": "2020-08-27T09:14:00.000Z",
        "voteCount": 1,
        "content": "not a fan"
      },
      {
        "date": "2020-08-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "Its App Insights, Refer: \nhttps://azuredevopslabs.com/labs/azuredevops/appinsights/"
      },
      {
        "date": "2020-08-08T23:19:00.000Z",
        "voteCount": 1,
        "content": "Its App Insights, Refer: \nhttps://azuredevopslabs.com/labs/azuredevops/appinsights/"
      },
      {
        "date": "2020-07-30T05:32:00.000Z",
        "voteCount": 2,
        "content": "You can use the sign-ins report to view details about application usage, by filtering on user name or application name. In this article, you learn how to find Azure Active Directory (Azure AD) user activity reports in the Azure portal. \nhttps://docs.microsoft.com/en-us/azure/active-directory/reports-monitoring/howto-find-activity-reports"
      },
      {
        "date": "2020-05-24T05:59:00.000Z",
        "voteCount": 2,
        "content": "Right Answer is Azure Application Insight- https://docs.microsoft.com/en-us/azure/azure-monitor/app/app-insights-overview.. AD does at who is login kind of stuff . Azure insight does at application usage level what pages have more hit .. where the performance issue due to large hits etc.."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/48672-exam-ai-100-topic-1-question-24-discussion/",
    "body": "HOTSPOT -<br>You need to build a sentiment analysis solution that will use input data from JSON documents and PDF documents. The JSON documents must be processed in batches and aggregated.<br>Which storage type should you use for each file type? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0003000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0003100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Blob Storage -<br>The following technologies are recommended choices for batch processing solutions in Azure.<br><br>Data storage -<br>\u2711 Azure Storage Blob Containers. Many existing Azure business processes already use Azure blob storage, making this a good choice for a big data store.<br>\u2711 Azure Data Lake Store. Azure Data Lake Store offers virtually unlimited storage for any size of file, and extensive security options, making it a good choice for extremely large-scale big data solutions that require a centralized store for data in heterogeneous formats.<br><br>Box 2: Azure Blob Storage -<br>References:<br>https://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/batch-processing https://docs.microsoft.com/bs-latn-ba/azure/storage/blobs/storage-blobs-introduction",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-01T10:31:00.000Z",
        "voteCount": 7,
        "content": "Why not CosmosDB for JSON?"
      },
      {
        "date": "2021-04-19T22:01:00.000Z",
        "voteCount": 4,
        "content": "Agreed... CosmosDB is the right one"
      },
      {
        "date": "2023-06-19T06:17:00.000Z",
        "voteCount": 1,
        "content": "By utilizing Azure Blob Storage for both JSON and PDF documents, you can have a unified storage solution that enables easy management, processing, and retrieval of your input data. Additionally, Azure Blob Storage offers various features such as access controls, durability, and high availability, which are essential for storing and processing data in a reliable and secure manner."
      },
      {
        "date": "2021-04-29T01:00:00.000Z",
        "voteCount": 2,
        "content": "CosmosDB for JSON"
      },
      {
        "date": "2021-04-21T03:10:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct Blob.\nThe link \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/batch-processing \nprovide the answer:\n\"Technology choices\nThe following technologies are recommended choices for batch processing solutions in Azure.\n\nData storage\nAzure Storage Blob Containers. Many existing Azure business processes already use Azure blob storage, making this a good choice for a big data store.\nAzure Data Lake Store. Azure Data Lake Store offers virtually unlimited storage for any size of file, and extensive security options, making it a good choice for extremely large-scale big data solutions that require a centralized store for data in heterogeneous formats.\""
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/4123-exam-ai-100-topic-1-question-25-discussion/",
    "body": "You are developing a mobile application that will perform optical character recognition (OCR) from photos.<br>The application will annotate the photos by using metadata, store the photos in Azure Blob storage, and then score the photos by using an Azure Machine<br>Learning model.<br>What should you use to process the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Batch"
    ],
    "answer": "B",
    "answerDescription": "By using Azure services such as the Computer Vision API and Azure Functions, companies can eliminate the need to manage individual servers, while reducing costs and leveraging the expertise that Microsoft has already developed around processing images with Cognitive Services.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/intelligent-apps-image-processing",
    "votes": [],
    "comments": [
      {
        "date": "2019-09-16T23:43:00.000Z",
        "voteCount": 50,
        "content": "I think it should be Azure functions. https://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/intelligent-apps-image-processing"
      },
      {
        "date": "2019-08-26T13:35:00.000Z",
        "voteCount": 12,
        "content": "I believe that this question is incorrect.  At least the answer.  It cannot be Event Hub since Event Hub is pub-sub service and high ingress is its strength.  Would it not be Stream Analytics?"
      },
      {
        "date": "2023-06-19T08:16:00.000Z",
        "voteCount": 1,
        "content": "To process the data in the scenario described, you should use B. Azure Functions.\n\nAzure Functions is a serverless compute service in Azure that allows you to run your code in a serverless environment, responding to events and executing code on-demand. It is an ideal choice for processing data in a scalable and event-driven manner.\n\nIn the given scenario, the mobile application captures photos and sends them for processing. Azure Functions can be set up as an event-driven trigger to process the photos as soon as they are uploaded to Azure Blob storage. You can configure a Blob storage trigger in Azure Functions, which will automatically execute your code whenever a new photo is added to the storage."
      },
      {
        "date": "2023-06-19T06:18:00.000Z",
        "voteCount": 1,
        "content": "Azure Functions (option B) is the most suitable choice for processing the data in this mobile application scenario, as it provides a serverless and event-driven platform to implement the necessary logic for OCR, metadata annotation, photo storage, and scoring using Azure Machine Learning."
      },
      {
        "date": "2021-06-04T23:21:00.000Z",
        "voteCount": 1,
        "content": "Refer #Q25 it has different answer."
      },
      {
        "date": "2021-05-20T04:26:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-05-11T15:49:00.000Z",
        "voteCount": 2,
        "content": "I think it's viable through both Functions and Logic Apps, but Functions calling are much, much cheaper (.2 USD per million calls). And Logic Apps have designer-oriented interface, so if there's a low-code requirement.... This question needs more information."
      },
      {
        "date": "2021-05-07T07:40:00.000Z",
        "voteCount": 1,
        "content": "Logic Apps looks right for this question.\nLogic Apps. If you don't need to react in real-time on added files to a blob, you might consider using Logic Apps. A logic app which can check if a file was added might be start by the recurrence trigger or sliding windows trigger.\nhttps://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/intelligent-apps-image-processing"
      },
      {
        "date": "2021-04-21T00:27:00.000Z",
        "voteCount": 3,
        "content": "Answer is Azure Functions.\n\nRefer Question 25 Topic 1"
      },
      {
        "date": "2021-03-25T18:50:00.000Z",
        "voteCount": 1,
        "content": "Azure Stream Analytics in terms of photo analysis and score."
      },
      {
        "date": "2021-03-25T03:58:00.000Z",
        "voteCount": 1,
        "content": "I think the correct is Azure Stream Analytics because it is able to analyze and score the photos by using an Azure ML model: https://docs.microsoft.com/en-us/azure/stream-analytics/machine-learning-udf"
      },
      {
        "date": "2021-03-07T19:27:00.000Z",
        "voteCount": 1,
        "content": "I'd go with Computer Vision API. Since this option is not available. I'd choose Azure Functions"
      },
      {
        "date": "2021-02-17T14:19:00.000Z",
        "voteCount": 2,
        "content": "Functions is the right answer.\nBut I wish they make the questions a bit more detailed. For instance, I can very well do this with Logic Apps. I can write inline code in Logic Apps, or call Functions from Logic Apps. More graphical and granular way to design the solution as opposed to a whole lot of code to maintain as with Functions.\nWriting Inline code will make Logic Apps an independent solution too."
      },
      {
        "date": "2021-03-20T21:09:00.000Z",
        "voteCount": 1,
        "content": "so why you still prefer Functions as the right answer? any idea that is more preferable."
      },
      {
        "date": "2021-01-28T03:04:00.000Z",
        "voteCount": 2,
        "content": "Azure function is close sha"
      },
      {
        "date": "2021-02-11T07:29:00.000Z",
        "voteCount": 1,
        "content": "How do you mean \"close\"?"
      },
      {
        "date": "2020-12-26T02:27:00.000Z",
        "voteCount": 1,
        "content": "Azure functions"
      },
      {
        "date": "2020-09-12T11:54:00.000Z",
        "voteCount": 10,
        "content": "the answer is here :\nhttps://docs.microsoft.com/en-us/azure/architecture/example-scenario/ai/intelligent-apps-image-processing\nI wish we could fix these answers."
      },
      {
        "date": "2020-11-14T04:22:00.000Z",
        "voteCount": 13,
        "content": "To save others the stress of opening and confirming. It is Azure functions. (B)"
      },
      {
        "date": "2020-08-28T06:06:00.000Z",
        "voteCount": 1,
        "content": "Correct ans is B i.e. Azure functions...\nGiven answer is wrong"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49968-exam-ai-100-topic-1-question-26-discussion/",
    "body": "You create an Azure Cognitive Services resource.<br>A data scientist needs to make API calls to the Cognitive Services resource.<br>Which two values should you provide to the data scientist? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEndpoint URL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResource name",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tResource group name",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubscription ID"
    ],
    "answer": "AC",
    "answerDescription": "Reference:<br>https://www.c-sharpcorner.com/article/using-cognitive-service-face-api-with-azure-logic-app/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T08:17:00.000Z",
        "voteCount": 1,
        "content": "To enable the data scientist to make API calls to the Azure Cognitive Services resource, you should provide the following two values:\n\nA. Endpoint URL: The endpoint URL is the address that the data scientist needs to use to access the Cognitive Services APIs. It is specific to the Cognitive Services resource you created and typically follows the format https://&lt;region&gt;.api.cognitive.microsoft.com/. The data scientist will need to use this URL when making API calls.\n\nC. Access key: The access key is a security credential that grants access to the Cognitive Services resource. It acts as an authentication token and should be kept confidential. The data scientist will need to include the access key in the API calls to authenticate and authorize access to the Cognitive Services resource."
      },
      {
        "date": "2023-06-19T06:19:00.000Z",
        "voteCount": 1,
        "content": "A. Endpoint URL: The data scientist needs to know the Endpoint URL of the Azure Cognitive Services resource. This URL specifies the location and endpoint of the API that the data scientist will use to make the API calls. The Endpoint URL is unique to each Cognitive Services resource and provides the entry point for accessing the services.\n\nC. Access key: The data scientist requires the Access key for authentication and authorization when making API calls to the Cognitive Services resource. The Access key acts as a security credential that grants access to the resource. It is used to authenticate the data scientist's API calls and ensure that only authorized users can interact with the resource."
      },
      {
        "date": "2021-05-20T04:24:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-04-24T21:19:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      },
      {
        "date": "2021-04-12T07:27:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26229-exam-ai-100-topic-1-question-27-discussion/",
    "body": "You plan to deploy an AI solution that tracks the behavior of 10 custom mobile apps. Each mobile app has several thousand users.<br>You need to recommend a solution for real-time data ingestion for the data originating from the mobile app users.<br>Which Microsoft Azure service should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Service Bus queries",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Service Bus topics and subscriptions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Storm on Azure HDInsight"
    ],
    "answer": "A",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-in/azure/event-hubs/event-hubs-about",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-20T06:24:00.000Z",
        "voteCount": 13,
        "content": "I agree with the chosen answer"
      },
      {
        "date": "2023-06-19T06:20:00.000Z",
        "voteCount": 1,
        "content": "Azure Event Hubs (option A) is the most appropriate recommendation for real-time data ingestion from mobile app users, considering its scalability, real-time processing capabilities, and event-driven architecture."
      },
      {
        "date": "2021-03-16T10:48:00.000Z",
        "voteCount": 3,
        "content": "The question ask for a solution for real-time \"data ingestion\" not data/stream processing which make it confusing. The correct answer is \"Event Hubs\" which serve real time data ingestion. \nhttps://docs.microsoft.com/en-us/azure/architecture/data-guide/big-data/real-time-processing"
      },
      {
        "date": "2021-02-16T08:31:00.000Z",
        "voteCount": 2,
        "content": "Real-time data ingestion, it's Event Hubs\nhttps://azure.microsoft.com/en-us/services/event-hubs/#features"
      },
      {
        "date": "2020-11-07T22:50:00.000Z",
        "voteCount": 2,
        "content": "D. Apache Storm on Azure HDInsight"
      },
      {
        "date": "2021-06-04T05:56:00.000Z",
        "voteCount": 4,
        "content": "Plz be aware you are responsible for referencing your answer."
      },
      {
        "date": "2020-11-11T07:36:00.000Z",
        "voteCount": 2,
        "content": "Reference?"
      },
      {
        "date": "2021-01-14T22:31:00.000Z",
        "voteCount": 10,
        "content": "Reason. ???. Pls just dont throw options in the air... pls validate too... \n\n@MODERATOR ::   I wish moderator must not approve such comments who dont have reference source .&gt;&gt;&gt;"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/26181-exam-ai-100-topic-1-question-28-discussion/",
    "body": "You plan to deploy Azure IoT Edge devices that will each store more than 10,000 images locally and classify the images by using a Custom Vision Service classifier.<br>Each image is approximately 5 MB.<br>You need to ensure that the images persist on the devices for 14 days.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe device cache",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob storage on the IoT Edge devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics on the IoT Esge devices",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server on the IoT Edge devices"
    ],
    "answer": "B",
    "answerDescription": "References:<br>https://docs.microsoft.com/en-us/azure/iot-edge/how-to-store-data-blob",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:21:00.000Z",
        "voteCount": 1,
        "content": "Azure Blob storage on the IoT Edge devices (option B) is the recommended solution to store the images locally for 14 days, considering its scalability, durability, and suitability for large unstructured data such as images."
      },
      {
        "date": "2020-07-19T22:35:00.000Z",
        "voteCount": 1,
        "content": "why not Azure stream analytics which moves data from iot hub to blob storage"
      },
      {
        "date": "2020-07-27T16:35:00.000Z",
        "voteCount": 7,
        "content": "Because of the requirement for persistence the answer is Blob"
      },
      {
        "date": "2021-02-09T12:32:00.000Z",
        "voteCount": 2,
        "content": "Correct. Additionally, ASA is not native to the IoT Edge device whereas Storage can be. ASA can only reduce the amount of uploaded data to reduce time for actions. Check the diagram in this tutorial to ascertain why ASA is not the answer.\nhttps://docs.microsoft.com/en-us/azure/iot-edge/tutorial-deploy-stream-analytics?view=iotedge-2018-06\nThe device cache is not an answer either given the image size times no. of images."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/14075-exam-ai-100-topic-1-question-29-discussion/",
    "body": "Your company is building custom models that integrate into microservices architecture on Azure Kubernetes Services (AKS).<br>The model is built by using Python and published to AKS.<br>You need to update the model and enable Azure Application Insights for the model.<br>What should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Azure CLI",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMLNET Model Builder",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Azure Machine Learning SDK",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Azure portal"
    ],
    "answer": "C",
    "answerDescription": "You can set up Azure Application Insights for Azure Machine Learning. Application Insights gives you the opportunity to monitor:<br>\u2711 Request rates, response times, and failure rates.<br>\u2711 Dependency rates, response times, and failure rates.<br>\u2711 Exceptions.<br>Requirements include an Azure Machine Learning workspace, a local directory that contains your scripts, and the Azure Machine Learning SDK for Python installed.<br>References:<br>https://docs.microsoft.com/bs-latn-ba/azure/machine-learning/service/how-to-enable-app-insights",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-17T05:08:00.000Z",
        "voteCount": 13,
        "content": "You can not update your model using Azure Portal"
      },
      {
        "date": "2021-03-16T14:30:00.000Z",
        "voteCount": 1,
        "content": "Just to make somethings clearer. you can't update the AKS to use APP insights using the portal after the AKS is deployed."
      },
      {
        "date": "2023-06-19T06:23:00.000Z",
        "voteCount": 1,
        "content": "Azure portal (option D) is the recommended choice for updating the model and enabling Azure Application Insights in this scenario, as it provides an intuitive and user-friendly interface for managing AKS resources and configuring services."
      },
      {
        "date": "2023-06-21T11:52:00.000Z",
        "voteCount": 1,
        "content": "To update the model and enable Azure Application Insights for the model in a microservices architecture on Azure Kubernetes Services (AKS), you should use:\n\nC. the Azure Machine Learning SDK\n\nThe Azure Machine Learning SDK provides comprehensive tools and libraries for managing and deploying machine learning models on Azure. It allows you to create, update, and deploy models using Python, making it suitable for your scenario."
      },
      {
        "date": "2021-04-30T10:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. Azure Machine Learning SDK. \nfrom azureml.core.webservice import Webservice\naks_service= Webservice(ws, \"my-service-name\") \nthen you Update your service and enable Azure Application Insights\naks_service.update(enable_app_insights=True)\n\nPortal does not offer any option like such neither az ml cli.\n\nNote: You can enable data collection at the time of deployment using Studio i.e., ml.azure.com but once published then SDK is only solution."
      },
      {
        "date": "2021-03-23T01:47:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/learn/modules/monitor-models-with-azure-machine-learning/2-enable-application-insights given answer is 100% correct"
      },
      {
        "date": "2020-02-13T12:16:00.000Z",
        "voteCount": 2,
        "content": "Azure Portal and SDK both seem correct to me."
      },
      {
        "date": "2020-05-28T01:50:00.000Z",
        "voteCount": 9,
        "content": "When the model has been deployed, you need to use the SDK to enable App Insights"
      },
      {
        "date": "2021-02-13T17:48:00.000Z",
        "voteCount": 2,
        "content": "ML SDK and the ML Studio can be used to enable APP Insights. ML SDK is the correct answer here since ML Studio is not an option... and because only one answer is expected. Reference: documentation referenced in the answer."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52525-exam-ai-100-topic-1-question-30-discussion/",
    "body": "You are designing an AI solution that will analyze millions of pictures by using Azure HDInsight Hadoop cluster.<br>You need to recommend a solution for storing the pictures. The solution must minimize costs.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure File Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen1"
    ],
    "answer": "D",
    "answerDescription": "Azure Data Lake Storage Gen1 is adequate and less expensive compared to Gen2.<br>References:<br>https://visualbi.com/blogs/microsoft/introduction-azure-data-lake-gen2/",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-27T12:51:00.000Z",
        "voteCount": 7,
        "content": "The same reference article cited quotes as follows: When we compare the Azure Data Lake Gen 2 pricing with Gen 1, Gen 2 pricing will be half the price of Gen 1; so answer should be Gen 2"
      },
      {
        "date": "2023-06-19T06:24:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Lake Storage Gen2 (option C) is the recommended solution for storing millions of pictures while minimizing costs, considering its scalability, cost-effectiveness, Hadoop integration, and security features."
      },
      {
        "date": "2021-11-04T10:31:00.000Z",
        "voteCount": 1,
        "content": "Gen 1 is more expensive than Gen 2 \nhttps://azure.microsoft.com/en-us/pricing/details/data-lake-storage-gen1/   \nhttps://azure.microsoft.com/en-us/pricing/details/storage/data-lake/"
      },
      {
        "date": "2021-08-12T12:25:00.000Z",
        "voteCount": 1,
        "content": "Gen 2 its correct, because Gen 1 is disable"
      },
      {
        "date": "2021-06-27T07:02:00.000Z",
        "voteCount": 2,
        "content": "Tricky, the Azure documentation is not clear about the pricing, but I found many websites that suggest that ADLS GEN2 is actually the cheaper option. e.g. https://stackoverflow.com/questions/51782502/azure-data-lake-gen-1-vs-gen-2"
      },
      {
        "date": "2021-05-12T05:08:00.000Z",
        "voteCount": 2,
        "content": "Azure Data Lake Storage Gen1 - built on the Azure Blob Platform. Legacy, yes, but cheaper than Data Lake Storage Gen2"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50752-exam-ai-100-topic-1-question-31-discussion/",
    "body": "You deploy an application that performs sentiment analysis on the data stored in Azure Cosmos DB.<br>Recently, you loaded a large amount of data to the database. The data was for a customer named Contoso, Ltd.<br>You discover that queries for the Contoso data are slow to complete, and the queries slow the entire application.<br>You need to reduce the amount of time it takes for the queries to complete. The solution must minimize costs.<br>What should you do? More than one answer choice may achieve the goal. (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the request units.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the partitioning strategy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the transaction isolation level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the data to the Cosmos DB database."
    ],
    "answer": "AB",
    "answerDescription": "Increasing request units would improve throughput, but at a cost.<br>Throughput provisioned for a container is divided evenly among physical partitions.<br>References:<br>https://docs.microsoft.com/en-us/azure/architecture/best-practices/data-partitioning",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:25:00.000Z",
        "voteCount": 1,
        "content": "recommended options to improve query performance on the Contoso data in Azure Cosmos DB while minimizing costs are:\n\nB. Change the partitioning strategy.\nD. Migrate the data to the Cosmos DB database."
      },
      {
        "date": "2023-06-21T12:04:00.000Z",
        "voteCount": 1,
        "content": "The most effective options to address slow query performance and minimize costs are A (changing the request units) and B (changing the partitioning strategy). These options focus on optimizing the resource allocation and distribution of data to improve query performance within the existing Cosmos DB database."
      },
      {
        "date": "2021-04-24T21:23:00.000Z",
        "voteCount": 2,
        "content": "To minimize the cost, B seems the best choice"
      },
      {
        "date": "2021-04-22T01:55:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/18599-exam-ai-100-topic-1-question-32-discussion/",
    "body": "Your company has several AI solutions and bots.<br>You need to implement a solution to monitor the utilization of the bots. The solution must ensure that analysts at the company can generate dashboards to review the utilization.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Application Insights",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Logic Apps",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Monitor"
    ],
    "answer": "A",
    "answerDescription": "Bot Analytics.<br>Analytics is an extension of Application Insights. Application Insights provides service-level and instrumentation data like traffic, latency, and integrations. Analytics provides conversation-level reporting on user, message, and channel data.<br>References:<br>https://docs.microsoft.com/en-us/azure/bot-service/bot-service-manage-analytics",
    "votes": [],
    "comments": [
      {
        "date": "2020-04-25T09:02:00.000Z",
        "voteCount": 14,
        "content": "Indeed, Bot Analytics is an extension of Application Insights.\nhttps://docs.microsoft.com/it-it/azure/bot-service/bot-service-manage-analytics?view=azure-bot-service-4.0"
      },
      {
        "date": "2023-06-19T06:28:00.000Z",
        "voteCount": 1,
        "content": "recommended options for implementing a solution to monitor the utilization of bots and enable analysts to generate dashboards are:\n\nA. Azure Application Insights\nD. Azure Monitor"
      },
      {
        "date": "2021-01-04T20:26:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct"
      },
      {
        "date": "2020-04-17T06:27:00.000Z",
        "voteCount": 2,
        "content": "Something is wrong here. The correct should be Bot Analytics"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/45294-exam-ai-100-topic-1-question-33-discussion/",
    "body": "Your plan to design a bot that will be hosted by using Azure Bot Service.<br>Your company identifies the following compliance requirements for the bot:<br>\u2711 Payment Card Industry Data Security Standards (PCI DSS)<br>\u2711 General Data Protection Regulation (GDPR)<br>\u2711 ISO 27001<br>You need to identify which compliance requirements are met by hosting the bot in the bot service.<br>What should you identify?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPCI DSS only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPCI DSS, ISO 27001, and GDPR",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tISO 27001 only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGDPR only"
    ],
    "answer": "B",
    "answerDescription": "Azure Bot service is compliant with ISO 27001:2013, ISO 27019:2014, SOC 1 and 2, Payment Card Industry Data Security Standard (PCI DSS), and Health<br>Insurance Portability and Accountability Act Business Associate Agreement (HIPAA BAA).<br>Microsoft products and services, including Azure Bot Service, are available today to help you meet the GDPR requirements.<br>References:<br>https://docs.microsoft.com/en-us/azure/bot-service/bot-service-compliance https://blog.botframework.com/2018/04/23/general-data-protection-regulation-gdpr/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:29:00.000Z",
        "voteCount": 1,
        "content": "hosting the bot in Azure Bot Service can address the compliance requirements of PCI DSS, GDPR, and ISO 27001, making option B the correct choice."
      },
      {
        "date": "2021-02-26T08:38:00.000Z",
        "voteCount": 1,
        "content": "I think, This is a repeated question"
      },
      {
        "date": "2021-02-20T09:20:00.000Z",
        "voteCount": 2,
        "content": "A custom practice test states that Microsoft has not yet obtained compliance with GDPR for Azure Bot Service. The question in the other (more reliable but *maybe* not up to date) site stated GDPR as an option but says it's the wrong answer.\nIt selects HIPAA, SOC and PCI DSS and eliminates GDPR and CCPA.\n\nNotice that the compliance page too doesn't mention GDPR.\n\nI'm hoping there was an update to this in the recent exam."
      },
      {
        "date": "2021-02-23T05:43:00.000Z",
        "voteCount": 3,
        "content": "It is compliant.\nPlease check this url\nhttps://azure.microsoft.com/en-us/resources/knowledge-center/what-is-gdpr/"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52416-exam-ai-100-topic-1-question-34-discussion/",
    "body": "HOTSPOT -<br>You plan to use Azure Cognitive Services to provide the development team at your company with the ability to create intelligent apps without having direct AI or data science skills.<br>The company identifies the following requirements for the planned Cognitive Services deployment:<br>\u2711 Provide support for the following languages: English, Portuguese, and German.<br>\u2711 Perform text analytics to derive a sentiment score.<br>Which Cognitive Service service should you deploy for each requirement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0003800003.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0003900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Text  Analytics -<br>The Language Detection feature of the Azure Text Analytics REST API evaluates text input for each document and returns language identifiers with a score that indicates the strength of the analysis.<br><br>Box 2: Language API -<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-language-detection https://docs.microsoft.com/en-us/azure/azure-databricks/databricks-sentiment-analysis-cognitive-services",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:31:00.000Z",
        "voteCount": 1,
        "content": "appropriate Cognitive Services to deploy for each requirement are:\nLanguage Support: Azure Text Analytics\nSentiment Analysis: Azure Text Analytics"
      },
      {
        "date": "2021-06-15T11:29:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam, June 2021"
      },
      {
        "date": "2021-05-20T04:25:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-05-11T05:09:00.000Z",
        "voteCount": 2,
        "content": "Is this a real question? I couldn't find any thing about \"Language APIs\" on Azure docs"
      },
      {
        "date": "2021-05-24T02:24:00.000Z",
        "voteCount": 2,
        "content": "probably referring to LUIS"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53701-exam-ai-100-topic-1-question-35-discussion/",
    "body": "HOTSPOT -<br>You plan to deploy the Text Analytics and Computer Vision services. The Azure Cognitive Services will be deployed to the West US and East Europe Azure regions.<br>You need to identify the minimum number of service endpoints and API keys required for the planned deployment.<br>What should you identify? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0004000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0004100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 2 -<br>After creating a Cognitive Service resource in the Azure portal, you'll get an endpoint and a key for authenticating your applications. You can access Azure<br>Cognitive Services through two different resources: A multi-service resource, or a single-service one.<br>Multi-service resource: Access multiple Azure Cognitive Services with a single key and endpoint.<br>Note: You need a key and endpoint for a Text Analytics resource. Azure Cognitive Services are represented by Azure resources that you subscribe to.<br>Each request must include your access key and an HTTP endpoint. The endpoint specifies the region you chose during sign up, the service URL, and a resource used on the request<br><br>Box 2: 2 -<br>You need at least one key per region.<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/cognitive-services-apis-create-account",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:32:00.000Z",
        "voteCount": 1,
        "content": "minimum number of service endpoints and API keys required for the planned deployment would be:\nService Endpoints: 2\nAPI Keys: 2"
      },
      {
        "date": "2021-06-19T21:04:00.000Z",
        "voteCount": 1,
        "content": "I had this in the exam not sure if it was right."
      },
      {
        "date": "2021-05-31T00:53:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-05-28T00:44:00.000Z",
        "voteCount": 2,
        "content": "isn't it 4 keys all together if we have 2 endpoints,  1endpoints one per service. and then each end point has two keys one per region."
      },
      {
        "date": "2021-06-04T06:19:00.000Z",
        "voteCount": 1,
        "content": "Yes, if you are accessing the service through Single-service resource. Multi-service resource allows to access multiple Azure Cognitive Services with a single key and endpoint."
      },
      {
        "date": "2021-06-11T05:42:00.000Z",
        "voteCount": 1,
        "content": "The question says the minimum required, so one key is required at minimum to access."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/31804-exam-ai-100-topic-1-question-36-discussion/",
    "body": "Your company plans to create a mobile app that will be used by employees to query the employee handbook.<br>You need to ensure that the employees can query the handbook by typing or by using speech.<br>Which core component should you use for the app?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLanguage Understanding (LUIS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQnA Maker",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Search"
    ],
    "answer": "D",
    "answerDescription": "Azure Cognitive Search (formerly known as \"Azure Search\") is a search-as-a-service cloud solution that gives developers APIs and tools for adding a rich search experience over private, heterogeneous content in web, mobile, and enterprise applications. Your code or a tool invokes data ingestion (indexing) to create and load an index. Optionally, you can add cognitive skills to apply AI processes during indexing. Doing so can add new information and structures useful for search and other scenarios.<br>Incorrect Answres:<br>B: QnA Maker is a cloud-based API service that lets you create a conversational question-and-answer layer over your existing data. Use it to build a knowledge base by extracting questions and answers from your semi-structured content, including FAQs, manuals, and documents. Answer users' questions with the best answers from the QnAs in your knowledge base\u05d2\u20ac\"automatically.<br>References:<br>https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search",
    "votes": [],
    "comments": [
      {
        "date": "2020-10-02T19:55:00.000Z",
        "voteCount": 11,
        "content": "It should be A.\n\"Together, Language Understanding and Azure Bot Service enable developers to create conversational interfaces for various scenarios like banking, travel, and entertainment. For example, a hotel\u2019s concierge can use a bot to enhance traditional e-mail and phone call interactions by validating a customer via Azure Active Directory and using Cognitive Services to better contextually process customer requests using text and voice. The Speech recognition service can be added to support voice commands\"\nhttps://www.luis.ai/"
      },
      {
        "date": "2021-05-27T13:28:00.000Z",
        "voteCount": 1,
        "content": "Agree. \nCognitive Search is used for extracting text from knowledge bases, \nText analytics recognizes up to 120 languages and can perform text to speech and vice versa. \nHowever LUIS determines intent and entities from utterances hence is most suited for this use case"
      },
      {
        "date": "2021-05-12T23:52:00.000Z",
        "voteCount": 10,
        "content": "I would go with \"Azure Search\", as it is used for searching and can include AI processing during indexing. So I would say that it is possible to process speech/text with AI enrichment, and then perform the search...\n\nhttps://docs.microsoft.com/en-us/azure/search/search-features-list"
      },
      {
        "date": "2021-06-04T06:32:00.000Z",
        "voteCount": 3,
        "content": "I would also go with 'Azure Search' as question talks about a Mobile App that queries an Employee Handbook, not creating a Bot for QnA or Text Analytics. Azure Cognitive search serves the purpose. https://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search"
      },
      {
        "date": "2023-06-19T06:33:00.000Z",
        "voteCount": 1,
        "content": "recommended core component to use for the mobile app to enable employees to query the employee handbook by typing or using speech is:\nA. Language Understanding (LUIS)"
      },
      {
        "date": "2021-05-11T08:59:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct because you can add LU in the azure search"
      },
      {
        "date": "2021-05-10T00:31:00.000Z",
        "voteCount": 1,
        "content": "I would go with A, as it states that also voice input is needed."
      },
      {
        "date": "2021-02-23T05:50:00.000Z",
        "voteCount": 4,
        "content": "They haven't mention about creating any form of a bot. Please look at \nhttps://docs.microsoft.com/en-us/azure/search/search-what-is-azure-search"
      },
      {
        "date": "2021-02-14T12:20:00.000Z",
        "voteCount": 2,
        "content": "Assuming Handbook is like an FAQ document, QnA maker would be the answer. LUIS could still be a \"core component\" or a secondary component. Azure Search can mine more than one datasource, so it would be overkill but possible to achieve the solution.\nI think the \"expected\" answer for core component is QnA maker."
      },
      {
        "date": "2020-12-08T06:14:00.000Z",
        "voteCount": 4,
        "content": "It's QnA maker, because we need to search the handbook which is nothing but the knowledge base"
      },
      {
        "date": "2020-12-11T05:41:00.000Z",
        "voteCount": 1,
        "content": "I also think it is probably QnA maker, which can handle text and speech"
      },
      {
        "date": "2020-12-11T05:45:00.000Z",
        "voteCount": 1,
        "content": "I've seen several instances of microsoft saying manuals are a possible input for QnA maker. https://docs.microsoft.com/en-us/azure/cognitive-services/QnAMaker/reference-document-format-guidelines\nif A is used in this case, it would be supplementary"
      },
      {
        "date": "2020-09-20T22:26:00.000Z",
        "voteCount": 8,
        "content": "Ans: A"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/23640-exam-ai-100-topic-1-question-37-discussion/",
    "body": "You have an existing Language Understanding (LUIS) model for an internal bot.<br>You need to recommend a solution to add a meeting reminder functionality to the bot by using a prebuilt model. The solution must minimize the size of the model.<br>Which component of LUIS should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdomain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tintents",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tentities"
    ],
    "answer": "C",
    "answerDescription": "LUIS includes a set of prebuilt entities for recognizing common types of information, like dates, times, numbers, measurements, and currency. Prebuilt entity support varies by the culture of your LUIS app.<br>Note: LUIS provides three types of prebuilt models. Each model can be added to your app at any time.<br><br>Model type: Includes -<br>\u2711 Domain: Intents, utterances, entities<br>\u2711 Intents: Intents, utterances<br>\u2711 Entities: Entities only<br>References:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-prebuilt-model",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-14T04:30:00.000Z",
        "voteCount": 10,
        "content": "Seems domain (see decription of calendar domain in screen shot here https://docs.microsoft.com/en-us/azure/cognitive-services/LUIS/howto-add-prebuilt-models)"
      },
      {
        "date": "2020-09-25T02:26:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-05-25T09:06:00.000Z",
        "voteCount": 1,
        "content": "How is it Domain? Surely Domains increase the size of the model compared to adding entities?"
      },
      {
        "date": "2020-06-26T13:29:00.000Z",
        "voteCount": 6,
        "content": "Answeris Domain"
      },
      {
        "date": "2023-06-19T06:34:00.000Z",
        "voteCount": 1,
        "content": "recommended component of LUIS to use for adding a meeting reminder functionality while minimizing the size of the model is:\nC. entities"
      },
      {
        "date": "2021-06-26T14:36:00.000Z",
        "voteCount": 1,
        "content": "The answer is intent. We need to add Calendar.Add intent from prebuilt Calendar. See the example in the below:\nhttps://www.codemag.com/Article/1809021/Natural-Language-Understanding-with-LUIS"
      },
      {
        "date": "2021-06-15T11:32:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      },
      {
        "date": "2021-05-27T13:31:00.000Z",
        "voteCount": 2,
        "content": "Intent is for tasks or actions\nEntities is for object - names, dates, times, numbers, measurements, currency\nHence answer is intent"
      },
      {
        "date": "2021-05-20T04:25:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-03-24T00:39:00.000Z",
        "voteCount": 1,
        "content": "Response: Entity. Explanation: It's a tricky question for 2 factors: \n- reduce the size of the model\n- select the category\n\\https://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-intent\nIntent it's an action whereas entities are parameters of the action. One intent can have several entities. In the example, a meeting reminder, this is clearly an intent. \nAbout the size: you can use a prebuilt one through the LUIS portal (it doesn't require storage as far as I know as the intents come out of the box)."
      },
      {
        "date": "2021-04-16T09:52:00.000Z",
        "voteCount": 3,
        "content": "I don't get it.... suppose you only add entities by calendar.xxxxx of prebuild calendar domain, you will need to mark facet in the intent level or model will not know \"what is \"meeting\"\". in this case, you can not guarantee the size will be minimized.\n\nfrom my view points, we does not need whole prebuild calendar domain, but we need some of sub-domains (i.e. intents) from it such as calendar.StartTime, calendar.StartDate.\n\nso, to me, I will say we need to add in the intent level"
      },
      {
        "date": "2021-03-12T08:33:00.000Z",
        "voteCount": 2,
        "content": "Given answer is correct - \nhttps://docs.microsoft.com/en-us/azure/cognitive-services/luis/howto-add-prebuilt-models"
      },
      {
        "date": "2021-02-14T14:25:00.000Z",
        "voteCount": 2,
        "content": "How can there be a distinction? Most of the LUIS documentation suggests these as process &amp; actions of adding prebuilt domains, prebuilt intents &amp; entities. I don't understand how to choose one of the other while the documentation says to do them all.\nVague!\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-get-started-create-app"
      },
      {
        "date": "2021-01-15T02:41:00.000Z",
        "voteCount": 1,
        "content": "But the solution must minimize the size of the model, doesn`t it mean that it should be entity?"
      },
      {
        "date": "2021-02-14T14:18:00.000Z",
        "voteCount": 1,
        "content": "What determines size of the model?"
      },
      {
        "date": "2020-12-05T15:07:00.000Z",
        "voteCount": 4,
        "content": "Intents\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/luis/luis-concept-enterprise"
      },
      {
        "date": "2020-11-05T21:24:00.000Z",
        "voteCount": 3,
        "content": "I think B (Intent) should be the answer. If we add an entity, we have to build the rest of the flow. We don't need the whole domain of meetings. And we can add only the meeting reminder intent in a specific domain\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/LUIS/howto-add-prebuilt-models"
      },
      {
        "date": "2020-07-14T05:13:00.000Z",
        "voteCount": 4,
        "content": "Should be intent as we have a goal. As Microsoft describes: \"An intent represents a task or action the user wants to perform. In addition to intents that you define, you can use prebuilt intents from one of the prebuilt domains.\"\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/LUIS/howto-add-prebuilt-models"
      },
      {
        "date": "2020-07-06T22:45:00.000Z",
        "voteCount": 1,
        "content": "Answer should be \"Domain\""
      },
      {
        "date": "2020-06-24T23:58:00.000Z",
        "voteCount": 2,
        "content": "The important word is prebuilt. I've checked in luis.ai, I am not able to find a PREBUILT intents for reminders. For entities, you can use DateTime prebuilt entities"
      },
      {
        "date": "2020-06-21T08:45:00.000Z",
        "voteCount": 5,
        "content": "I think it is \"intent\" - like creating a meeting reminder, \n\"entity\" is more like time, date, of the meeting"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/25700-exam-ai-100-topic-1-question-38-discussion/",
    "body": "You have an on-premises repository that contains 5,000 videos. The videos feature demonstrations of the products sold by your company.<br>The company's customers plan to search the videos by using the name of the product demonstrated in each video.<br>You need to build a custom search tool for the customers.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy an Azure Media Services resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Storage account and a blob container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Search resource.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Custom Vision API service."
    ],
    "answer": "A",
    "answerDescription": "Azure Media Services can be used to encode and package content, stream videos on-demand, broadcast live, analyze your videos with Media Services v3.<br>You can snalyze recorded videos or audio content. For example, to achieve higher customer satisfaction, organizations can extract speech-to-text and build search indexes and dashboards. Then, they can extract intelligence around common complaints, sources of complaints, and other relevant data.<br>References:<br>https://docs.microsoft.com/en-us/azure/media-services/latest/media-services-overview",
    "votes": [],
    "comments": [
      {
        "date": "2020-07-14T04:37:00.000Z",
        "voteCount": 6,
        "content": "Why not Azure Search (question is about searching)."
      },
      {
        "date": "2020-10-05T18:14:00.000Z",
        "voteCount": 3,
        "content": "We need to analyze the product in video. So, it is Azure MediaService"
      },
      {
        "date": "2021-02-14T14:29:00.000Z",
        "voteCount": 3,
        "content": "Product IN the video. Moreover the questions asks \"what should you do first?\". First you Deploy a Media Services Resource."
      },
      {
        "date": "2023-06-19T06:35:00.000Z",
        "voteCount": 1,
        "content": "recommended first step for building a custom search tool for customers to search the videos by product names is to:\nC. Create an Azure Search resource."
      },
      {
        "date": "2021-04-30T00:15:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/47768-exam-ai-100-topic-1-question-39-discussion/",
    "body": "Your company manages a sports team.<br>The company sets up a video booth to record messages for the team.<br>Before replaying the messages on a video screen, you need to generate captions for the messages and check the sentiment of the video to ensure that only positive messages are played.<br>Which Azure Cognitive Services service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLanguage Understanding (LUIS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpeaker Recognition",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustom Vision",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVideo Indexer"
    ],
    "answer": "D",
    "answerDescription": "Video Indexer includes Audio transcription: Converts speech to text in 12 languages and allows extensions. Supported languages include English, Spanish,<br>French, German, Italian, Mandarin Chinese, Japanese, Arabic, Russian, Portuguese, Hindi, and Korean.<br>When indexing by one channel, partial result for those models will be available, such as sentiment analysis: Identifies positive, negative, and neutral sentiments from speech and visual text.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/media-services/video-indexer/video-indexer-overview",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:36:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure Cognitive Services service to use for generating captions and analyzing sentiment in the recorded messages is:\nD. Video Indexer"
      },
      {
        "date": "2021-05-31T00:53:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-03-19T13:46:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49693-exam-ai-100-topic-1-question-40-discussion/",
    "body": "HOTSPOT -<br>You plan to build an app that will provide users with the ability to dictate messages and convert the messages into text.<br>You need to recommend a solution to meet the following requirements for the app:<br>\u2711 Must be able to transcribe streaming dictated messages that are longer than 15 seconds.<br>\u2711 Must be able to upload existing recordings to Azure Blob storage to be transcribed later.<br>Which solution should you recommend for each requirement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0004500003.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0004600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: The Speech SDK -<br>The Speech SDK is not limited to 15 seconds.<br><br>Box 2: Batch Transcription API -<br>Batch transcription is a set of REST API operations that enables you to transcribe a large amount of audio in storage. You can point to audio files with a shared access signature (SAS) URI and asynchronously receive transcription results. With the new v3.0 API, you have the choice of transcribing one or more audio files, or process a whole storage container.<br>Asynchronous speech-to-text transcription is just one of the features.<br>Reference:<br>https://github.com/Azure-Samples/cognitive-services-speech-sdk/issues/13 https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/batch-transcription",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:41:00.000Z",
        "voteCount": 1,
        "content": "recommended solution for each requirement is:\n\u2711 Must be able to transcribe streaming dictated messages that are longer than 15 seconds:\nAzure Cognitive Services - Speech to Text\n\n\u2711 Must be able to upload existing recordings to Azure Blob storage to be transcribed later:\nAzure Media Services - Video Indexer"
      },
      {
        "date": "2021-06-19T21:02:00.000Z",
        "voteCount": 1,
        "content": "It maybe an old question and things have changed but AI-100 would still have the old information. @Y2Data"
      },
      {
        "date": "2021-05-31T00:53:00.000Z",
        "voteCount": 2,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-05-20T04:25:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-05-11T15:29:00.000Z",
        "voteCount": 2,
        "content": "This is an old question, Speech API now supports up to 60 seconds, so the 15 seconds limitation is no longer an issue here."
      },
      {
        "date": "2021-04-08T23:44:00.000Z",
        "voteCount": 3,
        "content": "A should be Speech to Text API"
      },
      {
        "date": "2021-04-16T10:30:00.000Z",
        "voteCount": 2,
        "content": "Speech to Text API 2.0 can only support the audio which is &lt;=60 seconds.\nFor long audio (i.e. &gt;60 secs) , use speech SDK or Speech to Text API 3.0 but I don't find how long is supported for 3.0...\n\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/rest-speech-to-text#speech-to-text-rest-api-v30"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/29006-exam-ai-100-topic-1-question-41-discussion/",
    "body": "Your company plans to monitor twitter hashtags, and then to build a graph of connected people and places that contains the associated sentiment.<br>The monitored hashtags use several languages, but the graph will be displayed in English.<br>You need to recommend the required Azure Cognitive Services endpoints for the planned graph.<br>Which Cognitive Services endpoints should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLanguage Detection, Content Moderator, and Key Phrase Extraction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTranslator Text, Content Moderator, and Key Phrase Extraction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLanguage Detection, Sentiment Analysis, and Key Phase Extraction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTranslator Text, Sentiment Analysis, and Named Entity Recognition"
    ],
    "answer": "C",
    "answerDescription": "Sentiment analysis, which is also called opinion mining, uses social media analytics tools to determine attitudes toward a product or idea.<br>Translator Text: Translate text in real time across more than 60 languages, powered by the latest innovations in machine translation.<br>The Key Phrase Extraction skill evaluates unstructured text, and for each record, returns a list of key phrases. This skill uses the machine learning models provided by Text Analytics in Cognitive Services.<br>This capability is useful if you need to quickly identify the main talking points in the record. For example, given input text \"The food was delicious and there were wonderful staff\", the service returns \"food\" and \"wonderful staff\".<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking https://docs.microsoft.com/en-us/azure/search/cognitive-search-skill-keyphrases",
    "votes": [],
    "comments": [
      {
        "date": "2020-09-10T01:54:00.000Z",
        "voteCount": 20,
        "content": "It is definitely Translator Text, Sentiment Analysis, and Named Entity Recognition. Translator Text can show the results in English. Sentiment Analysis is determines the sentiments of the tweets. Named Entity Recognition identifies the people mentioned in the tweets."
      },
      {
        "date": "2021-01-03T22:09:00.000Z",
        "voteCount": 2,
        "content": "I agree"
      },
      {
        "date": "2021-02-22T05:36:00.000Z",
        "voteCount": 1,
        "content": "I agree"
      },
      {
        "date": "2020-08-18T10:46:00.000Z",
        "voteCount": 6,
        "content": "Why Language Detection and why not Translator Text? The graph must be built in english and Language Detection doesn't perform any translation."
      },
      {
        "date": "2023-06-19T06:44:00.000Z",
        "voteCount": 1,
        "content": "recommended Cognitive Services endpoints for the planned graph are:\nD. Translator Text, Sentiment Analysis, and Named Entity Recognition."
      },
      {
        "date": "2023-06-21T12:24:00.000Z",
        "voteCount": 1,
        "content": "C. Language Detection, Sentiment Analysis, and Key Phrase Extraction\n\nTranslator Text and Named Entity Recognition are not necessary in this scenario as the graph is planned to be displayed in English. Therefore, the translation of different languages is not required, and Named Entity Recognition, which identifies named entities like people, organizations, and locations, is not explicitly mentioned as a requirement."
      },
      {
        "date": "2021-08-23T06:48:00.000Z",
        "voteCount": 1,
        "content": "wrong, it's D"
      },
      {
        "date": "2021-05-31T00:54:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-04-08T23:48:00.000Z",
        "voteCount": 1,
        "content": "ANY ANSWER WITH translator text and Sentiment analysis is correct, i do not see any mention for the use of key phrase extraction or entity recognition"
      },
      {
        "date": "2021-02-14T19:51:00.000Z",
        "voteCount": 3,
        "content": "Named Entity Recognition is the key differentiator to choose D (besides the other 2 right services). Because NER is the ability to identify different entities in text and categorize them into pre-defined classes or types such as: person, location, event, product, and organization.\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking?tabs=version-3-preview#named-entity-recognition-ner\nHence D."
      },
      {
        "date": "2020-12-21T22:11:00.000Z",
        "voteCount": 4,
        "content": "Answer is D: \nTranslator Text, Sentiment Analysis, Named Entity Recognition. \n\nNER : https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-entity-linking?tabs=version-3-preview\n\nKey Phase Extraction: https://docs.microsoft.com/en-us/azure/cognitive-services/text-analytics/how-tos/text-analytics-how-to-keyword-extraction"
      },
      {
        "date": "2020-09-09T04:21:00.000Z",
        "voteCount": 4,
        "content": "Why not Named Entity Recognition. \nSeems D as correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49694-exam-ai-100-topic-1-question-42-discussion/",
    "body": "HOTSPOT -<br>You plan to create an intelligent bot to handle internal user chats to the help desk of your company. The bot has the following requirements:<br>\u2711 Must be able to interpret what a user means.<br>\u2711 Must be able to perform multiple tasks for a user.<br>Must be able to answer questions from an existing knowledge base.<br><img src=\"/assets/media/exam-media/03857/0004700003.png\" class=\"in-exam-image\"><br>You need to recommend which solutions meet the requirements.<br>Which solution should you recommend for each requirement? To answer, drag the appropriate solutions to the correct requirements. Each solution may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/03857/0004800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0004900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: The Language Understanding (LUIS) service<br>Language Understanding (LUIS) is a cloud-based API service that applies custom machine-learning intelligence to a user's conversational, natural language text to predict overall meaning, and pull out relevant, detailed information.<br><br>Box 2: Text Analytics API -<br>The Text Analytics API is a cloud-based service that provides advanced natural language processing over raw text, and includes four main functions: sentiment analysis, key phrase extraction, named entity recognition, and language detection.<br><br>Box 3: The QnA Maker service -<br>QnA Maker is a cloud-based Natural Language Processing (NLP) service that easily creates a natural conversational layer over your data. It can be used to find the most appropriate answer for any given natural language input, from your custom knowledge base (KB) of information.<br>Incorrect Answers:<br>Dispatch tool library:<br>If a bot uses multiple LUIS models and QnA Maker knowledge bases (knowledge bases), you can use Dispatch tool to determine which LUIS model or QnA Maker knowledge base best matches the user input. The dispatch tool does this by creating a single LUIS app to route user input to the correct model.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/bot-service/bot-builder-tutorial-dispatch https://docs.microsoft.com/en-us/azure/cognitive-services/qnamaker/overview/overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-08T23:50:00.000Z",
        "voteCount": 10,
        "content": "Answer for 2 should be dispatch tool library and not text Analytics API"
      },
      {
        "date": "2023-06-19T06:46:00.000Z",
        "voteCount": 1,
        "content": "recommended solutions for each requirement are:\n\n\u2711 Must be able to interpret what a user means:\n\nAzure Language Understanding (LUIS)\nAzure QnA Maker\n\u2711 Must be able to perform multiple tasks for a user:\n\nAzure Bot Service (Bot Framework)\n\u2711 Must be able to answer questions from an existing knowledge base:\n\nAzure QnA Maker"
      },
      {
        "date": "2021-06-27T08:47:00.000Z",
        "voteCount": 1,
        "content": "I think it should be: 1. user intention is LUIS. 2. perform multiple actions should be the dispatch tool \"does this by creating a single LUIS app to route user input to the correct model.\" and 3 QnA Maker service"
      },
      {
        "date": "2021-05-31T00:54:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-04-12T08:15:00.000Z",
        "voteCount": 4,
        "content": "Text analytics is right. As you can use Dispatch tool to determine which LUIS model or QnA Maker knowledge base best matches the user input. \nSo, Option A for 2nd :D"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/49691-exam-ai-100-topic-1-question-43-discussion/",
    "body": "You are developing a mobile application that will perform optical character recognition (OCR) from photos.<br>The application will annotate the photos by using metadata, store the photos in Azure Blob storage, and then score the photos by using an Azure Machine<br>Learning model.<br>What should you use to process the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Logic Apps"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2021-04-21T03:16:00.000Z",
        "voteCount": 5,
        "content": "Correct Azure Function"
      },
      {
        "date": "2023-06-19T06:49:00.000Z",
        "voteCount": 1,
        "content": "recommended services to process the data in the described scenario are:\n\nB. Azure Functions\nD. Azure Logic Apps"
      },
      {
        "date": "2021-11-19T03:09:00.000Z",
        "voteCount": 1,
        "content": "Right answer is  . . . . . . . . . ."
      },
      {
        "date": "2021-07-21T20:18:00.000Z",
        "voteCount": 2,
        "content": "repeat of question 25 . The answer there is azure functions and explanation given"
      },
      {
        "date": "2021-11-19T03:08:00.000Z",
        "voteCount": 1,
        "content": "You have good memorization power. Hats off Bro\nLets go to MS and then have fun"
      },
      {
        "date": "2021-06-19T17:31:00.000Z",
        "voteCount": 1,
        "content": "I did another search from another site and they wrote Event Hubs. This was in the exam in May"
      },
      {
        "date": "2021-06-15T11:26:00.000Z",
        "voteCount": 2,
        "content": "this question was in the exam, june 2021"
      },
      {
        "date": "2021-05-20T04:24:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-05-11T01:13:00.000Z",
        "voteCount": 2,
        "content": "since Azure Functions roughly equals to AWS Lambda, I'm gonna say it's the best option here."
      },
      {
        "date": "2021-11-19T03:05:00.000Z",
        "voteCount": 2,
        "content": "BAHUT KATHA OUT OF THE BOX KAHUCHU NAI RE BABULA.......KHALI THARE SHYAM BHAI PAKHAKU AASE.....BHARA KHAIBU TAPARE"
      },
      {
        "date": "2021-05-10T02:19:00.000Z",
        "voteCount": 2,
        "content": "Another version of this question has \"Azure Logic Apps\" as one of the answer options. In this case, would it be \"Azure Functions\" or \"Azure Logic Apps\"?"
      },
      {
        "date": "2021-04-08T23:11:00.000Z",
        "voteCount": 2,
        "content": "Why is this not Event hubs??"
      },
      {
        "date": "2021-11-19T03:07:00.000Z",
        "voteCount": 2,
        "content": "Why you are asking this question. .  . It will be better if you follow the videos of                     Mr. Chandrasekhar Thakur on Youtube, he is making great learning sessions of Azure on Youtube."
      },
      {
        "date": "2021-05-09T11:33:00.000Z",
        "voteCount": 3,
        "content": "Event hub cannot process data only transfer"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52083-exam-ai-100-topic-1-question-44-discussion/",
    "body": "You are designing an AI solution that will analyze millions of pictures by using Azure HDInsight Hadoop cluster.<br>You need to recommend a solution for storing the pictures. The solution must minimize costs.<br>Which storage solution should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure File Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks File System"
    ],
    "answer": "C",
    "answerDescription": "Azure Data Lake Store is optimized for storing large amounts of data for reporting and analytical and is geared towards storing data in its native format, making it a great store for non-relational data.<br>Reference:<br>https://stackify.com/store-data-azure-understand-azure-data-storage-options/",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:50:00.000Z",
        "voteCount": 1,
        "content": "recommended storage solution is:\nC. Azure Data Lake Storage Gen2."
      },
      {
        "date": "2021-05-07T07:42:00.000Z",
        "voteCount": 3,
        "content": "If Azure Data Lake Storage Gen1 then it is,  else Azure Data Lake Storage Gen2"
      },
      {
        "date": "2021-06-27T08:51:00.000Z",
        "voteCount": 4,
        "content": "ADLK Gen1 is more expensive than Gen2. So even if Gen1 would be an option, it would still be Gen2."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/44732-exam-ai-100-topic-1-question-45-discussion/",
    "body": "You are designing a real-time speech-to-text AI feature for an Android mobile app. The feature will stream data to the Speech service.<br>You need to recommend which audio format to use to serialize the audio. The solution must minimize the amount of data transferred to the cloud.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMP3",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWAV/PCM",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMP4a"
    ],
    "answer": "B",
    "answerDescription": "Currently, only the following configuration is supported:<br>Audio samples in PCM format, one channel, 16 bits per sample, 8000 or 16000 samples per second (16000 or 32000 bytes per second), two block align (16 bit including padding for a sample).<br>Reference:<br>https://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-use-audio-input-streams",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:50:00.000Z",
        "voteCount": 1,
        "content": "the recommended audio format to use for serializing the audio in order to minimize data transfer is:\nA. MP3."
      },
      {
        "date": "2021-05-31T00:55:00.000Z",
        "voteCount": 1,
        "content": "this was in the AI-100 exam i took today, May 31"
      },
      {
        "date": "2021-04-12T08:20:00.000Z",
        "voteCount": 2,
        "content": "Now, Speech-to-Text supports \"audio/wav\" and \"audio/mp3\" as output formats via the format parameter."
      },
      {
        "date": "2021-06-11T07:05:00.000Z",
        "voteCount": 2,
        "content": "Compressed audio such as mp3 is only supported through GStreamer, which decompresses the files before sending them so it won't be of any benefit.\nhttps://docs.microsoft.com/en-us/azure/cognitive-services/speech-service/how-to-use-codec-compressed-audio-input-streams?tabs=debian&amp;pivots=programming-language-csharp#speech-sdk-version-required-for-compressed-audio-input\n\"The Speech SDK and Speech CLI can accept compressed audio formats using GStreamer. GStreamer decompresses the audio before it is sent over the wire to the Speech service as raw PCM.\""
      },
      {
        "date": "2021-02-14T20:36:00.000Z",
        "voteCount": 3,
        "content": "Correct. Although it says to minimize amount of data which means that a compressed audio format such as mp3 is preferred, only PCM (WAV) is supported by Azure Speech Services.\nhttps://dejanstojanovic.net/aspnet/2019/january/mp3-to-text-using-azure-cognitive-services/"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/50755-exam-ai-100-topic-1-question-46-discussion/",
    "body": "DRAG DROP -<br>You are developing an application for photo classification. Users of the application will include minors. The users will upload photos to the application. The photos will be stored for model training purposes. All the photos must be considered appropriate for minors.<br>You need to recommend an architecture for the application.<br>Which Azure services should you recommend using in the architecture? To answer, drag the appropriate services to the correct targets. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/03857/0005200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/03857/0005300001.png\" class=\"in-exam-image\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:52:00.000Z",
        "voteCount": 1,
        "content": "recommended Azure services for the architecture are:\n\nAzure Blob storage: Target for storing uploaded photos.\n\nAzure Content Moderator: Target for analyzing and moderating the content of the uploaded photos.\n\nAzure Machine Learning: Target for training a custom machine learning model for photo classification."
      },
      {
        "date": "2021-06-21T04:21:00.000Z",
        "voteCount": 1,
        "content": "Why App Service? \nI think that the right one is Logic App, because we need something to orchestrate the pipeline"
      },
      {
        "date": "2021-06-15T11:36:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      },
      {
        "date": "2021-05-20T04:26:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      },
      {
        "date": "2021-04-22T02:20:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53201-exam-ai-100-topic-1-question-47-discussion/",
    "body": "Your company creates a popular mobile game.<br>The company tracks usage patterns of the game.<br>You need to provide special offers to users when there is a significant change in the usage patterns.<br>Which Azure Cognitive Services service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tForm Recognizer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBing Autosuggest",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tText Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnomaly Detector"
    ],
    "answer": "D",
    "answerDescription": "Reference:<br>https://azure.microsoft.com/en-gb/services/cognitive-services/anomaly-detector/#features",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-19T06:54:00.000Z",
        "voteCount": 1,
        "content": "most appropriate Azure Cognitive Services service for your scenario is:\n\nD. Anomaly Detector."
      },
      {
        "date": "2021-06-15T11:36:00.000Z",
        "voteCount": 1,
        "content": "this question was in the exam"
      },
      {
        "date": "2021-05-20T04:26:00.000Z",
        "voteCount": 1,
        "content": "This question was in the exam."
      }
    ],
    "examNameCode": "ai-100",
    "topicNumber": "1"
  }
]