[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90896-exam-dp-500-topic-1-question-1-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to identify the root cause of the data refresh issue.<br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Usage Metrics Report in powerbi.com",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQuery Diagnostics in Power Query Editor\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerformance analyzer in Power BI Desktop"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-19T01:12:00.000Z",
        "voteCount": 7,
        "content": "Query Diagnostics will do the trick. \"...you can use it to understand what sort of queries you're emitting, what slowdowns you might run into during authoring refresh...\" https://learn.microsoft.com/en-us/power-query/query-diagnostics. \nSo, B."
      },
      {
        "date": "2022-12-12T11:58:00.000Z",
        "voteCount": 6,
        "content": "Correct,\nQuery Diagnostics helps in understanding what Power Query is doing at authoring and at refresh time in Power BI Desktop."
      },
      {
        "date": "2023-09-13T22:57:00.000Z",
        "voteCount": 1,
        "content": "i would also say B since its data refresh, \nperformance analyzer is time of the visuals to refresh (and update)"
      },
      {
        "date": "2023-08-24T10:52:00.000Z",
        "voteCount": 1,
        "content": "will give all stats in Query diagnostics."
      },
      {
        "date": "2023-07-26T10:30:00.000Z",
        "voteCount": 1,
        "content": "Data refresh is the equivalent of data load, which is more upfront than loading visuals. Hence we need to use Query Diagnostics (answer B)"
      },
      {
        "date": "2023-06-13T21:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct answer. \nThe question is asking why does the refresh fail not why does the report take a long time to load. \nThe PA will analyse where the bottleneck is. \nPower Query Editor will tell you why the refresh is failing."
      },
      {
        "date": "2023-06-07T23:46:00.000Z",
        "voteCount": 3,
        "content": "When you encounter slow rendering specifically during filter selections in Power BI reports, the Performance Analyzer tool is more suitable for identifying the root causes and optimizing performance. The Performance Analyzer helps you analyze the rendering time of visuals and components in your report, allowing you to pinpoint the specific areas that are causing the slowdown during filter selections. By using the Performance Analyzer, you can focus on optimizing those visuals or components to improve rendering performance.\n\nOn the other hand, Query Diagnostics in Power Query Editor is designed to analyze and optimize the performance of data loading and transformations within the Power Query Editor. It is more relevant when you want to troubleshoot and optimize the data loading process, rather than the rendering performance during filter selections.\n\nso in the context of slow rendering during filter selections, the Performance Analyzer in Power BI Desktop is the recommended tool to use for identifying the root causes and optimizing performance."
      },
      {
        "date": "2023-08-31T03:47:00.000Z",
        "voteCount": 1,
        "content": "Question mentioned data refresh issues so your explanation for Query Diagnostics in Power Query Editor suits"
      },
      {
        "date": "2023-04-14T12:31:00.000Z",
        "voteCount": 1,
        "content": "I prefere C, B doesnt show info about interactions and metrics"
      },
      {
        "date": "2023-02-08T02:53:00.000Z",
        "voteCount": 2,
        "content": "Monitor report performance in Power BI Desktop using the Performance Analyzer. Monitoring will help you learn where the bottlenecks are, and how you can improve report performance.\n\nMonitoring performance is relevant in the following situations:\n\nYour Import data model refresh is slow.\nYour DirectQuery or Live Connection reports are slow.\nYour model calculations are slow."
      },
      {
        "date": "2023-02-08T02:56:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/guidance/monitor-report-performance"
      },
      {
        "date": "2023-05-24T06:27:00.000Z",
        "voteCount": 1,
        "content": "C cannot be used to monitor Premium per user activities or capacity so will go with B"
      },
      {
        "date": "2023-01-08T22:09:00.000Z",
        "voteCount": 4,
        "content": "Should be C - Performance Analyzer\nhttps://learn.microsoft.com/en-us/power-bi/guidance/monitor-report-performance"
      },
      {
        "date": "2023-02-01T08:58:00.000Z",
        "voteCount": 1,
        "content": "'It is asking for the 'root cause of the data refresh issue.'  Not the visuals\u2026 so answer is not Performance Analyzer."
      },
      {
        "date": "2022-12-10T01:08:00.000Z",
        "voteCount": 2,
        "content": "Got this on 12/08/2022"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91265-exam-dp-500-topic-1-question-2-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>Which two possible tools can you use to identify what causes the report to render slowly? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSynapse Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Studio",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerformance analyzer in Power BI Desktop\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T12:00:00.000Z",
        "voteCount": 12,
        "content": "Correct,\nPerformance Analyzer is built into Power BI Desktop which is used first followed by DAX Studio which is an external tool to further dig deep down."
      },
      {
        "date": "2024-05-06T14:11:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-12-10T09:26:00.000Z",
        "voteCount": 1,
        "content": "Performance Analyzer provides information about the time each part of the report takes to finish, from rendering visuals to calculate DAX measures. DAX Studio allows you to go deeper in the analysis with the query you get from the Performance Analyzer tool."
      },
      {
        "date": "2023-06-13T21:47:00.000Z",
        "voteCount": 3,
        "content": "B and D are correct answers."
      },
      {
        "date": "2023-01-08T22:11:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91267-exam-dp-500-topic-1-question-3-discussion/",
    "body": "DRAG DROP -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to integrate the external data source to support the planned changes.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image1.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image2.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-12T12:04:00.000Z",
        "voteCount": 5,
        "content": "I believe this is the correct answer. \nFor more info check this: https://learn.microsoft.com/en-us/power-query/connectors/json#load-from-the-web"
      },
      {
        "date": "2023-06-15T00:31:00.000Z",
        "voteCount": 1,
        "content": "Create\nExpand\nPublish"
      },
      {
        "date": "2023-06-15T00:31:00.000Z",
        "voteCount": 2,
        "content": "* Create a web source"
      },
      {
        "date": "2023-04-14T12:38:00.000Z",
        "voteCount": 1,
        "content": "is ok ok ok"
      },
      {
        "date": "2023-01-12T12:12:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91268-exam-dp-500-topic-1-question-4-discussion/",
    "body": "DRAG DROP -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics  dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to create Power BI reports that will display data based on the customers\u2019 subscription level.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image3.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image4.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-12T12:13:00.000Z",
        "voteCount": 11,
        "content": "this is correct"
      },
      {
        "date": "2023-04-14T12:45:00.000Z",
        "voteCount": 2,
        "content": "role\nadd members\ndax"
      },
      {
        "date": "2023-05-24T23:35:00.000Z",
        "voteCount": 5,
        "content": "no no, DAX expression here is for the RLS filter, not the report yet, so it has to go after you click create a Role, the answer is - role - dax to filter - members"
      },
      {
        "date": "2023-05-24T23:37:00.000Z",
        "voteCount": 1,
        "content": "step 6 from here - https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-rls"
      },
      {
        "date": "2023-01-24T02:35:00.000Z",
        "voteCount": 4,
        "content": "Correct, bi directionnal is not needed"
      },
      {
        "date": "2023-01-08T22:15:00.000Z",
        "voteCount": 1,
        "content": "Create row-level security (RLS) roles\nCreate a DAX expression\nEnable bi-directional filtering"
      },
      {
        "date": "2023-03-08T00:52:00.000Z",
        "voteCount": 2,
        "content": "Why the heck do you want to add bi-directional filtering on this? This is something you do on data modeling...."
      },
      {
        "date": "2024-01-05T16:38:00.000Z",
        "voteCount": 1,
        "content": "I think bi-directional filtering may be used for dynamic RLS (the one that uses USERPRINCIPALNAME), but it isn't needed in this case since I assume it's a static RLS."
      },
      {
        "date": "2024-01-05T16:40:00.000Z",
        "voteCount": 1,
        "content": "Another reason is you don't need to make roles in dynamic RLS since they're already set up in the data."
      },
      {
        "date": "2023-01-05T01:27:00.000Z",
        "voteCount": 3,
        "content": "The last step should be enabling bi-directional filtering.\nSince we are going to implement RLS based on customers' subscription level, we will use username() or userprincipalname(). After this there is no need to add members, the data will be filtered dynamically. Now we should enable apply bi-directional filtering."
      },
      {
        "date": "2023-01-23T04:57:00.000Z",
        "voteCount": 1,
        "content": "I disagree. As this link shows, even with dynamic roles for RLS it is still necessary to assign members/security group to the role: https://www.acuitytraining.co.uk/news-tips/dynamic-row-level-security-in-power-bi/"
      },
      {
        "date": "2023-02-14T22:29:00.000Z",
        "voteCount": 3,
        "content": "When it says \"Implement subscription levels for the customers\", I think that it's related to static RLS, so we have to create one role for each level (for ex.: customer, manager, etc...), if this is the case then we don't have to enable the bi-directional filtering but we must to add members to each role.\nIf you implement this solution using dynamic RLS, then we should enable it such as says this link:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-rls\n\n\"Select this option when you've also implemented dynamic row-level security at the server level, where row-level security is based on username or login ID.\""
      },
      {
        "date": "2022-12-12T12:09:00.000Z",
        "voteCount": 2,
        "content": "I think this may be correct. Although bi-directional filtering can be applied before adding members, I am pretty sure that is not a mandatory requirement.\nCheck out this link: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-rls#define-roles-and-rules-in-power-bi-desktop"
      },
      {
        "date": "2022-12-28T07:04:00.000Z",
        "voteCount": 1,
        "content": "I think the last step should be bi-directional filtering. AFAIK, adding members can only be on Power BI Service and not Desktop. Will be good to get other views/answers from the community too."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91269-exam-dp-500-topic-1-question-5-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to recommend a solution to add new fields to the financial data Power BI dataset with data from the Microsoft SQL Server data warehouse.<br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Purview",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan XMLA endpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSite-to-Site VPN",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe on-premises data gateway\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-02T01:38:00.000Z",
        "voteCount": 9,
        "content": "Since the data is located in Microsoft SQL Server DWH, we need on-premises data gateway."
      },
      {
        "date": "2022-12-16T00:44:00.000Z",
        "voteCount": 9,
        "content": "D is correct.\nIt seems that you need the gateway, as stated in this article:\nhttps://community.powerbi.com/t5/Power-Query/SQL-Server-hosted-on-Azure-VM-Do-I-need-a-Gateway/td-p/489761"
      },
      {
        "date": "2023-07-26T11:30:00.000Z",
        "voteCount": 1,
        "content": "On-prem gateway. A SQL Server database on Azure Virtual Machine is considered on-premises and needs a gateway, check it out here : https://radacad.com/the-power-bi-gateway-all-you-need-to-know"
      },
      {
        "date": "2023-06-15T00:36:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2023-05-08T05:43:00.000Z",
        "voteCount": 2,
        "content": "The source talks about a gateway, not an on-prem gateway. Azure VMs run in the cloud, what you would need is a gateway but not an on-prem gateway.\n\nYou use an On-prem gateway to transfer data between servers that are not in the cloud and servers that are. In this case, both are in the cloud.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/service-gateway-onprem?WT.mc_id=DP-MVP-5003635"
      },
      {
        "date": "2023-04-14T12:59:00.000Z",
        "voteCount": 1,
        "content": "D assuming that DWH or sql pool is on-premise and not cloud"
      },
      {
        "date": "2023-02-05T15:04:00.000Z",
        "voteCount": 1,
        "content": "D is correct because dedicated SQL pool is also called data-warehouse so, on-premise gateway will be used to build power BI dataset."
      },
      {
        "date": "2022-12-12T12:14:00.000Z",
        "voteCount": 6,
        "content": "I believe the correct answer is the XMLA endpoint.\nQuestion mentions - 'Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines (imp)'. Since it's on Azure, we won't need an on-prem gateway. We can use a PowerShell cmdlet script to update the dataset."
      },
      {
        "date": "2023-01-23T05:02:00.000Z",
        "voteCount": 1,
        "content": "I would normally agree if we look at things black vs white i.e. on-prem vs. cloud, here Azure, but as the link from Az301301X states, nothing is black and white and we do need a gateway."
      },
      {
        "date": "2023-01-23T05:03:00.000Z",
        "voteCount": 1,
        "content": "Another confirmation (albeit from a completely different angle) can be found here as well: https://community.powerbi.com/t5/Service/How-to-Install-on-premise-gateway-on-azure-virtual-machine/m-p/2419686"
      },
      {
        "date": "2023-01-30T03:33:00.000Z",
        "voteCount": 1,
        "content": "Apologies for constantly adding comments, but this just crossed my mind as well: XMLA endpoint is connected to a Premium capacity and the question scenario does not mention Premium at all."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91270-exam-dp-500-topic-1-question-6-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to recommend a solution to resolve the query issue of the serverless SQL pool. The solution must minimize impact on the users.<br>What should you in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the statistics for the serverless SQL pool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data from the serverless SQL pool to a dedicated Apache Spark pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute the sp_set_process_data_limit stored procedure for the serverless SQL pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the data from the serverless SQL pool to a dedicated SQL pool."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T12:17:00.000Z",
        "voteCount": 10,
        "content": "Correct"
      },
      {
        "date": "2024-01-08T04:52:00.000Z",
        "voteCount": 1,
        "content": "i would go with option D as well, because updating statics is not a certain solution as we don't know if there is any change on tables as statistics changes require"
      },
      {
        "date": "2023-11-10T20:33:00.000Z",
        "voteCount": 3,
        "content": "Using a dedicated SQL Pool or Apache Spark Pool will probably resolve the issue but they would have a huge impact on the user, you should consider that part of the question otherwise you are ignoring it completely. Updating the statistics might not be as miraculous as a dedicated SQL pool but it is done easily and has a minor if not no impact on the user."
      },
      {
        "date": "2024-01-05T17:22:00.000Z",
        "voteCount": 2,
        "content": "I might agree with this one."
      },
      {
        "date": "2023-10-13T02:44:00.000Z",
        "voteCount": 1,
        "content": "Updating statistics is required when there is a material change in the distribution of values for columns"
      },
      {
        "date": "2023-09-11T18:39:00.000Z",
        "voteCount": 2,
        "content": "Dedicated SQL Pool"
      },
      {
        "date": "2023-08-18T14:57:00.000Z",
        "voteCount": 1,
        "content": "Dedicated SQL pool supports large amounts of data and complex queries."
      },
      {
        "date": "2023-08-15T01:05:00.000Z",
        "voteCount": 2,
        "content": "ChatPGT say Answer C:\nC. Execute the sp_set_process_data_limit stored procedure for the serverless SQL pool.\n\nThe sp_set_process_data_limit stored procedure allows you to set a data limit for a single query. This can help prevent queries from exceeding the available resources and causing tempdb or other resource-related issues. By setting appropriate data limits, you can manage query execution and ensure more stable performance for the serverless SQL pool."
      },
      {
        "date": "2023-08-05T04:11:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is D - Move to dedicated SQL Pool.\n\nQuestion says to minimize impact. There will definitely be impact on running processes in Stat is updated. \n\nRead and see comments: https://learn.microsoft.com/en-us/answers/questions/1340374/serverless-sql-is-not-clearing-the-tempdb-after-qu"
      },
      {
        "date": "2023-07-11T02:19:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT: To resolve the query issue of the serverless SQL pool in Azure Synapse Analytics while minimizing the impact on users, you should recommend option A: Update the statistics for the serverless SQL pool."
      },
      {
        "date": "2023-06-30T12:10:00.000Z",
        "voteCount": 1,
        "content": "D. Move the data from the serverless SQL pool to a dedicated SQL pool."
      },
      {
        "date": "2023-05-02T08:00:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-statistics#statistics-in-serverless-sql-pool"
      },
      {
        "date": "2023-04-15T05:28:00.000Z",
        "voteCount": 2,
        "content": "I select A: change the source is not needed. I don't know the sp in C. So updating statistics is the best selection (for example sp_drop_statistics)"
      },
      {
        "date": "2023-03-29T01:48:00.000Z",
        "voteCount": 2,
        "content": "Option C is the correct solution to resolve the query issue of the serverless SQL pool while minimizing the impact on the users by ChatGPT. By executing the sp_set_process_data_limit stored procedure, the size of tempdb can be controlled, preventing it from being exceeded and causing queries to fail occasionally. This stored procedure limits the amount of data that can be processed by each query, and it can be set at the query level or the session level. This solution is more efficient and cost-effective compared to moving the data from the serverless SQL pool to a dedicated SQL pool or Apache Spark pool. Updating statistics may improve query performance but it is not directly related to the tempdb size issue."
      },
      {
        "date": "2023-03-27T09:26:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT recommends:\n1. Check the size of tempdb\n2. Optimize queries\n3. Monitor resource usage\n4. Increase pool size\n5. Consider dedicated SQL pool: If your workload requires more resources than the serverless SQL pool can provide, you may want to consider using a dedicated SQL pool."
      },
      {
        "date": "2023-03-25T08:30:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A but this is the correct link wih explanations referring to serverless sql pool: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-statistics#statistics-in-serverless-sql-pool"
      },
      {
        "date": "2023-02-19T14:10:00.000Z",
        "voteCount": 1,
        "content": "shouldn't answer be A?\n\"How to understand the Tempdb Usage and what are ways to fix this :\n    Ensure table statistics are created and kept up to date. \"\nhttps://www.linkedin.com/pulse/what-making-azure-synapse-dw-dedicated-pool-tempdb-reaching-mishra/"
      },
      {
        "date": "2023-04-06T10:07:00.000Z",
        "voteCount": 1,
        "content": "Read question carefully. They are asking about serverless pool, not dedicated."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91272-exam-dp-500-topic-1-question-7-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to recommend a solution for the customer workspace to support the planned changes.<br>Which two configurations should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the financial data to the web.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the FinData workspace to use a Power BI Premium capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Build permission for the financial data to each customer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet Use datasets across workspaces to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T12:21:00.000Z",
        "voteCount": 12,
        "content": "I think correct answer is C,D\nQuestion mentions- 'Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.' Since customers will create their own reports, they will need access to the underlying dataset, so Build permissions will have to be provided."
      },
      {
        "date": "2023-05-25T00:01:00.000Z",
        "voteCount": 2,
        "content": "Agree - CD - \"When you create a report in Power BI Desktop, the data in that report is stored in a data model. When you publish a report to the Power BI service, the data model is also published to the service as a dataset at the same time. When you share the report with others, you can give them Build permission for the dataset that the report is built on, so they can discover and reuse it for their own reports, dashboards, etc.\""
      },
      {
        "date": "2023-04-30T08:31:00.000Z",
        "voteCount": 1,
        "content": "A and B\nRequirement is to make findata available for everyone."
      },
      {
        "date": "2023-04-16T04:28:00.000Z",
        "voteCount": 1,
        "content": "correct answer is C,D"
      },
      {
        "date": "2023-03-27T09:49:00.000Z",
        "voteCount": 2,
        "content": "I think it is B and D.\nDon't they all need at least a power bi pro licence in order to read the data / and share it?\nIn order to share it with users with free licenses, designers need to publish that content to a group workspace backed by a Premium capacity. Premium capacity provides the benefit of unlimited content sharing.\nIs it really possible to \"set use datasets across workspaces to Enabled\"? Where is that option."
      },
      {
        "date": "2023-04-29T16:41:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-admin-across-workspaces"
      },
      {
        "date": "2023-02-01T03:13:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is BD\n\n\"You discover that the refresh process of the Power BI model occasionally times out\" \nFor Power BI Pro workspaces, data refreshes must complete in less than 2 hours. On Premium, the limit is 5 hours so B would a solution.\n\nThey are now going to provide customers access to their own workspace (meaning they haven't already) Hence D - \"Use Datasets across workspaces\" needs to be enabled.\n\nPeople do not build permission, but looking at the question, they seem to already have this access so it isn't something that would be included in the solution which is what the question is asking."
      },
      {
        "date": "2023-02-01T03:14:00.000Z",
        "voteCount": 1,
        "content": "People do need* build permissions"
      },
      {
        "date": "2023-01-24T03:05:00.000Z",
        "voteCount": 3,
        "content": "correct answer is C,D"
      },
      {
        "date": "2023-01-06T00:55:00.000Z",
        "voteCount": 2,
        "content": "I agree. I would go for C and D."
      },
      {
        "date": "2023-01-05T20:59:00.000Z",
        "voteCount": 3,
        "content": "it should be C,D"
      },
      {
        "date": "2023-01-05T20:58:00.000Z",
        "voteCount": 2,
        "content": "it should be C,D"
      },
      {
        "date": "2022-12-21T12:28:00.000Z",
        "voteCount": 4,
        "content": "I believe it should be C,D, \nIn PBI Admin portal, Tenant settings --&gt; Workspace settings --&gt; Use datasets across workspaces\nit requires users have the required Build permission when enabling \"use datasets across workspaces\""
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91273-exam-dp-500-topic-1-question-8-discussion/",
    "body": "HOTSPOT -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br>Contoso, Ltd. is a company that sells enriched financial data to a variety of external customers.<br>Contoso has a main office in Los Angeles and two branch offices in New York and Seattle.<br><br>Existing Environment -<br><br>Data Infrastructure -<br>Contoso has a 50-TB data warehouse that uses an instance of SQL Server on Azure Virtual Machines.<br>The data warehouse populates an Azure Synapse Analytics workspace that is accessed by the external customers. Currently, the customers can access all the data.<br>Contoso has one Power BI workspace named FinData that contains a single dataset. The dataset contains financial data from around the world. The workspace is used by 10 internal users and one external customer. The dataset has the following two data sources: the data warehouse and the Synapse Analytics serverless SQL pool.<br>Users frequently query the Synapse Analytics workspace by using Transact-SQL.<br><br>User Problems -<br>Contoso identifies the following user issues:<br>Some users indicate that the visuals in Power BI reports are slow to render when making filter selections.<br>Users indicate that queries against the serverless SQL pool fail occasionally because the size of tempdb has been exceeded.<br>Users indicate that the data in Power BI reports is stale. You discover that the refresh process of the Power BI model occasionally times out.<br><br>Planned Changes -<br>Contoso plans to implement the following changes:<br>Into the existing Power BI dataset, integrate an external data source in JSON that is accessible by using the REST API.<br>Build a new dataset in the FinData workspace by using data from the Synapse Analytics dedicated SQL pool.<br>Provide all the customers with their own Power BI workspace to create their own reports. Each workspace will use the new dataset in the FinData workspace.<br>Implement subscription levels for the customers. Each subscription level will provide access to specific rows of financial data.<br>Deploy prebuilt datasets to Power BI to simplify the query experience of the customers.<br>Provide internal users with the ability to incorporate machine learning models loaded to the dedicated SQL pool.<br>You need to build a Transact-SQL query to implement the planned changes for the internal users.<br>How should you complete the Transact-SQL query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image6.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image7.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-12T12:23:00.000Z",
        "voteCount": 8,
        "content": "Correct"
      },
      {
        "date": "2023-07-05T10:40:00.000Z",
        "voteCount": 3,
        "content": "there is no d in the query, and d.* should be ml.* see here https://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2023-04-30T08:34:00.000Z",
        "voteCount": 1,
        "content": "that Go keyword is mispaced it seems"
      },
      {
        "date": "2023-04-16T04:35:00.000Z",
        "voteCount": 1,
        "content": "coooooorect!"
      },
      {
        "date": "2023-01-24T13:11:00.000Z",
        "voteCount": 3,
        "content": "Predict.,with"
      },
      {
        "date": "2023-01-23T05:07:00.000Z",
        "voteCount": 1,
        "content": "Good association: WITH --&gt; determining data type or other ways to define data scheme"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90981-exam-dp-500-topic-1-question-9-discussion/",
    "body": "DRAG DROP -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to create the customized Power BI usage reporting. The Usage Metrics Report dataset has already been created. The solution must minimize development and administrative effort.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image10.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image11.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-26T16:16:00.000Z",
        "voteCount": 12,
        "content": "Pretty sure you can't create a measure in PBI Service yeah?"
      },
      {
        "date": "2022-12-14T23:29:00.000Z",
        "voteCount": 9,
        "content": "should use Power BI Desktop to create measure"
      },
      {
        "date": "2023-01-26T05:38:00.000Z",
        "voteCount": 2,
        "content": "Additionally, if the first step is done in PBI service and so are the consequent two, than the last step to publish it would make no sense considering you never downloaded it to PBI Desktop and made any changes there. In short, the first step is definetly done in PBI Desktop."
      },
      {
        "date": "2023-09-13T23:15:00.000Z",
        "voteCount": 1,
        "content": "seems correct:\nLitware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.\nso measure needs to be created (can do that only in desktop)\n\nadd to visuals \npublish"
      },
      {
        "date": "2023-08-18T22:58:00.000Z",
        "voteCount": 8,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n-  From Power BI Desktop, open the Usage Metrics Report dataset in the Sales Analytics workspace\n- Add a report measure\n- Add visuals to the report\n- Publish the report to the Sales Analytics workspace"
      },
      {
        "date": "2023-06-27T06:23:00.000Z",
        "voteCount": 3,
        "content": "PBI Desktop\nAdd measure\nAdd visuals\nPublish"
      },
      {
        "date": "2023-06-08T12:06:00.000Z",
        "voteCount": 2,
        "content": "We need to start with Power BI Desktop. We can check this in the second option of this documentation. https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-usage-metrics#about-the-usage-metrics-report\n\"Connect to the dataset from Power BI Desktop.\""
      },
      {
        "date": "2023-04-16T07:12:00.000Z",
        "voteCount": 2,
        "content": "it said the metrics has already done, in fact is not necesary create it. but with these options:\nfrom PBI descktop\nadd report measures (it already exist, is not necesary)\nadd visuals \npublish"
      },
      {
        "date": "2023-02-21T07:48:00.000Z",
        "voteCount": 3,
        "content": "it was on my exam, there is only the Power Bi desktop option"
      },
      {
        "date": "2023-01-24T03:28:00.000Z",
        "voteCount": 4,
        "content": "For me\n1. from PBI Desktop open the dataset 2. add measure 3. add visuals 4. publish\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-usage-metrics\nConnect to the dataset from Power BI Desktop. For every workspace, the dataset has the name \"Report Usage Metrics Model.\" See Establish a connection to a published dataset for details."
      },
      {
        "date": "2023-01-23T05:14:00.000Z",
        "voteCount": 1,
        "content": "Based on other comments re. creating measure exclusively in PBID (which are correct) the answer sequence should be 1. from PBI Desktop open the dataset 2. add measure 3. add visuals 4. publish"
      },
      {
        "date": "2023-01-04T11:10:00.000Z",
        "voteCount": 4,
        "content": "A measure can not be created in Power Bi service. https://community.powerbi.com/t5/Service/How-to-create-a-new-measure-in-Power-bi-service/td-p/2249943"
      },
      {
        "date": "2022-12-10T22:55:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91274-exam-dp-500-topic-1-question-10-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to configure the Sales Analytics workspace to meet the ad hoc reporting requirements.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the sales managers the Build permission for the existing Power BI datasets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the sales managers admin access to the existing Power BI workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a deployment pipeline and grant the sales managers access to the pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a PBIT file and distribute the file to the sales managers."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-14T23:33:00.000Z",
        "voteCount": 12,
        "content": "A is Correct, Template file will require data loading and will lead to separate Power BI dataset and reports. To minmize the effort, should give users build access to existing dataset."
      },
      {
        "date": "2023-01-02T01:35:00.000Z",
        "voteCount": 3,
        "content": "Totally agree"
      },
      {
        "date": "2023-01-12T12:37:00.000Z",
        "voteCount": 8,
        "content": "I think it should be \"A\". Build permission for the existing Power BI datasets. This will reduce the effort for managers, more secure than template which will lead to different dataset/refresh cycle, etc. They can create their own reports from same dataset and have nothing to do with allowing them to change the dataset or any other reports."
      },
      {
        "date": "2023-10-26T07:28:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says A"
      },
      {
        "date": "2023-08-18T22:56:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was \"Grant the sales managers the Build permission for the existing Power BI datasets.\""
      },
      {
        "date": "2023-07-05T11:06:00.000Z",
        "voteCount": 1,
        "content": "I think A is the correct one and the next step is azure synapse is https://techcommunity.microsoft.com/t5/educator-developer-blog/how-to-connect-azure-synapse-to-power-bi-for-data-visualization/ba-p/3614555"
      },
      {
        "date": "2023-06-30T15:40:00.000Z",
        "voteCount": 1,
        "content": "By creating a deployment pipeline, you can establish a controlled and standardized process for promoting Power BI content from the development environment to the test and production workspaces. Granting the sales managers access to the pipeline allows them to deploy their own ad hoc reports while adhering to the development process requirements. This ensures that the reports go through the proper testing and production stages."
      },
      {
        "date": "2023-05-17T07:22:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer :\nC. Create a deployment pipeline and grant the sales managers access to the pipeline.\n\nExplanation:\nCreating a deployment pipeline is a recommended approach to deploy Power BI content to different environments, such as the test and production workspaces. By setting up a deployment pipeline, you can ensure a standardized and automated process for deploying Power BI reports and dashboards. Granting sales managers access to the pipeline allows them to initiate and manage the deployment of Power BI content themselves, enabling ad hoc reporting with minimal effort. This empowers sales managers to access the necessary reports and perform ad hoc analysis as per their requirements, without needing direct access to the development or production workspaces."
      },
      {
        "date": "2023-04-16T09:23:00.000Z",
        "voteCount": 1,
        "content": "I will set Admin role to Manager and readers for the rest to avoid other edit or modify reports"
      },
      {
        "date": "2023-04-16T09:24:00.000Z",
        "voteCount": 2,
        "content": "Sorry answer B"
      },
      {
        "date": "2023-04-14T21:38:00.000Z",
        "voteCount": 1,
        "content": "I agree that A is the correct answer"
      },
      {
        "date": "2023-03-29T21:51:00.000Z",
        "voteCount": 3,
        "content": "The requirement is regarding to the \"workspace\"  so, it clearly the \"A\". Building permission allow them to create reports without affecting the previous one and you could avoid additional effort. \nLink :   https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-build-permissions"
      },
      {
        "date": "2023-02-28T07:02:00.000Z",
        "voteCount": 2,
        "content": "I would choose A"
      },
      {
        "date": "2023-01-04T01:03:00.000Z",
        "voteCount": 6,
        "content": "It says: \"Sales managers must be prevented from modifying reports created by other users.\" So, a PBIT (...a template) would do the trick. So, D."
      },
      {
        "date": "2023-02-06T23:02:00.000Z",
        "voteCount": 2,
        "content": "1. Using the Build feature, users would still be creating their own separate reports and individual reports will be unaffected.\n2. Business requirements says a single imported dataset will be used and refreshed every 30 minutes.\n3. For ad-hoc reporting, it is not recommended to distribute pbit files among all a long lisy of users.\n\nHence, Build permissions is the most feasible answer I can think of."
      },
      {
        "date": "2022-12-16T04:57:00.000Z",
        "voteCount": 3,
        "content": "Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.\nSales managers must be prevented from modifying reports created by other users.\nThe feasibility Says PBIT to comply with 1st point"
      },
      {
        "date": "2022-12-12T12:39:00.000Z",
        "voteCount": 4,
        "content": "Correct\nCheck this link for more info: https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-templates"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91275-exam-dp-500-topic-1-question-11-discussion/",
    "body": "DRAG DROP -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to implement object-level security (OLS) in the Power BI dataset for the sales associates.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image12.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image13.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-12T12:42:00.000Z",
        "voteCount": 15,
        "content": "Correct\nFor more info check: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-ols?tabs=table"
      },
      {
        "date": "2024-01-19T03:06:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt-4 says :\n\n1. Create a role\n2. Add a table or filter\n3. Publish"
      },
      {
        "date": "2023-08-15T12:20:00.000Z",
        "voteCount": 2,
        "content": "It should be-\n1.Create a role\n2. Add a table and filter\n3.Set OLS to None for Customer Email column\nWhy are we skipping the add table and filter stage?"
      },
      {
        "date": "2023-04-16T09:35:00.000Z",
        "voteCount": 2,
        "content": "correctcorrect"
      },
      {
        "date": "2023-04-16T09:36:00.000Z",
        "voteCount": 1,
        "content": "but I will publish the report, not the dataset"
      },
      {
        "date": "2023-03-29T22:01:00.000Z",
        "voteCount": 1,
        "content": "it is OK :)"
      },
      {
        "date": "2023-03-24T01:07:00.000Z",
        "voteCount": 1,
        "content": "It is right !"
      },
      {
        "date": "2023-03-08T04:20:00.000Z",
        "voteCount": 1,
        "content": "It's the only logical sequence to perform that action. The answer is correct."
      },
      {
        "date": "2023-01-23T05:18:00.000Z",
        "voteCount": 2,
        "content": "correct. For more info check: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-ols?tabs=table"
      },
      {
        "date": "2024-05-22T06:32:00.000Z",
        "voteCount": 1,
        "content": "Read, correct and thanks for the link"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91276-exam-dp-500-topic-1-question-12-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to recommend a solution to ensure that sensitivity labels are applied. The solution must minimize administrative effort.<br>Which three actions should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin portal, set Allow users to apply sensitivity labels for Power BI content to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin portal, set Apply sensitivity labels from data sources to their data in Power BI to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn SQLDW, apply sensitivity labels to the columns in the Customer and CustomersWithProductScore tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Power BI datasets, apply sensitivity labels to the columns in the Customer and CustomersWithProductScore tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin portal, set Make certified content discoverable to Enabled."
    ],
    "answer": "ABC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABC",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-31T05:15:00.000Z",
        "voteCount": 13,
        "content": "ABC is correct.\nD - you can't apply sensitivity labels to columns in power bi dataset\nE - certification has nothing to do with sensitivity"
      },
      {
        "date": "2023-04-16T09:57:00.000Z",
        "voteCount": 3,
        "content": "c - sensitive label cannot be applied in SQL"
      },
      {
        "date": "2023-04-23T11:42:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL provides a feature called \"Data Classification\" that allows you to apply sensitivity labels to your data. Data Classification allows you to discover, classify, label, and report on sensitive data in your Azure SQL databases.\n\nTo apply sensitivity labels in Azure SQL, you need to follow these steps:\n\n    Enable the Data Discovery &amp; Classification feature in Azure SQL.\n\n    Create a sensitivity label or use an existing one that aligns with your data classification requirements.\n\n    Apply the sensitivity label to the database or tables in the Azure SQL instance.\n\n    Use the sensitivity labels to control access to sensitive data, monitor and audit access to sensitive data, and apply data protection policies.\n\nBy applying sensitivity labels to your data in Azure SQL, you can ensure that sensitive data is identified, classified, and protected. This can help you comply with various data protection regulations and protect your organization from data breaches.\n\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql"
      },
      {
        "date": "2023-07-05T11:46:00.000Z",
        "voteCount": 2,
        "content": "In the Power BI service, sensitivity labels can be applied to datasets, reports, dashboards, and dataflow. https://learn.microsoft.com/en-us/power-bi/enterprise/service-security-sensitivity-label-overview#introduction"
      },
      {
        "date": "2022-12-24T00:06:00.000Z",
        "voteCount": 6,
        "content": "D cannot be true, as Power BI allows to apply just one label to the entire dataset/report. We can't apply a label to each table inside the PBI dataset"
      },
      {
        "date": "2022-12-24T00:09:00.000Z",
        "voteCount": 6,
        "content": "It should be A, B, C"
      },
      {
        "date": "2023-08-18T23:00:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- From the Power BI Admin portal, set Allow users to apply sensitivity labels for Power BI content to Enabled.\n- From the Power BI Admin portal, set Apply sensitivity labels from data sources to their data in Power BI to Enabled.\n- In SQLDW, apply sensitivity labels to the columns in the Customer and CustomersWithProductScore tables."
      },
      {
        "date": "2023-06-08T05:50:00.000Z",
        "voteCount": 1,
        "content": "A- https://learn.microsoft.com/pt-br/power-bi/enterprise/service-security-enable-data-sensitivity-labels#enable-sensitivity-labels\nB- https://learn.microsoft.com/pt-br/power-bi/admin/service-admin-portal-information-protection#apply-sensitivity-labels-from-data-sources-to-their-data-in-power-bi-preview\nC- https://learn.microsoft.com/pt-br/azure/purview/create-sensitivity-label#benefits-of-labeling-in-microsoft-purview"
      },
      {
        "date": "2023-03-17T04:59:00.000Z",
        "voteCount": 1,
        "content": "A. From the Power BI Admin portal, set Allow users to apply sensitivity labels for Power BI content to Enabled. This will allow users to apply sensitivity labels to their reports and datasets.\nB. From the Power BI Admin portal, set Apply sensitivity labels from data sources to their data in Power BI to Enabled. This will ensure that any sensitivity labels applied to the data source will be carried over to the Power BI content.\nD. In the Power BI datasets, apply sensitivity labels to the columns in the Customer and CustomersWithProductScore tables. This will ensure that the sensitivity labels are applied to the specific columns within the Power BI dataset.\n\nTherefore, the correct actions to include in the recommendation are A, B, and D.\n\nOption C is incorrect because sensitivity labels should be applied in Power BI, not in SQLDW. Option E is also incorrect as it is not related to sensitivity labels."
      },
      {
        "date": "2023-04-06T23:08:00.000Z",
        "voteCount": 2,
        "content": "You can't apply sensitivity labels to columns"
      },
      {
        "date": "2023-02-28T07:19:00.000Z",
        "voteCount": 1,
        "content": "A, B, D. \nC is not correct because sensitive label cannot be applied in SQL db\nE is for certify datasets not for labeling."
      },
      {
        "date": "2023-04-23T11:32:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/sql/t-sql/statements/add-sensitivity-classification-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2023-02-14T02:58:00.000Z",
        "voteCount": 1,
        "content": "A, B and E is the correct answer. You cannot apply labels to columns."
      },
      {
        "date": "2023-02-06T23:20:00.000Z",
        "voteCount": 1,
        "content": "A, D and E"
      },
      {
        "date": "2023-02-09T15:51:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind it's A, B and D \n\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-security-apply-data-sensitivity-labels"
      },
      {
        "date": "2022-12-12T12:50:00.000Z",
        "voteCount": 4,
        "content": "Not sure.\nSince the requirement mentions reducing administrative efforts, I would have thought that inheritance from data sources makes more sense. I would have selected B,C,E maybe.\nCheck here: https://learn.microsoft.com/en-us/power-bi/enterprise/service-security-sensitivity-label-inheritance-from-data-sources"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91277-exam-dp-500-topic-1-question-13-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>How should you configure the Power BI dataset refresh for the dbo.SalesTransactions table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan incremental refresh of Product where the ModifiedDate value is during the last three days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan incremental refresh of dbo.SalesTransactions where the SalesDate value is during the last three days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta full refresh of all the tables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan incremental refresh of dbo.SalesTransactions where the SalesDate value is during the last hour."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-12T12:52:00.000Z",
        "voteCount": 7,
        "content": "Correct\nCompany requirements stated 'Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.'"
      },
      {
        "date": "2023-08-18T23:01:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was \"an incremental refresh of dbo.SalesTransactions where the SalesDate value is during the last three days.\""
      },
      {
        "date": "2023-04-16T11:41:00.000Z",
        "voteCount": 2,
        "content": "it is BBB"
      },
      {
        "date": "2023-02-05T08:08:00.000Z",
        "voteCount": 1,
        "content": "I would agree. But the easiest sol"
      },
      {
        "date": "2023-02-05T08:09:00.000Z",
        "voteCount": 1,
        "content": "ution would be to do the full-load"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91278-exam-dp-500-topic-1-question-14-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>What should you configure in the deployment pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta selective deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tauto-binding",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta backward deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta data source rule\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-13T03:23:00.000Z",
        "voteCount": 1,
        "content": "D is the proper answear"
      },
      {
        "date": "2023-08-18T23:01:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was \"a data source rule\""
      },
      {
        "date": "2023-07-03T13:20:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-06-06T11:39:00.000Z",
        "voteCount": 4,
        "content": "D is correct. As it is mentioned in the case study ALL entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace. --&gt; we need to set up data source rule in order for data source entity not stay the same as in development environment but changed to the production environment.   Step 5 - create deployment rules https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started#step-5---create-deployment-rules-optional"
      },
      {
        "date": "2023-04-25T11:12:00.000Z",
        "voteCount": 1,
        "content": "None of these are correct. \nWe need a linked service in Synapse Studio to actually develop reports there"
      },
      {
        "date": "2023-04-16T11:43:00.000Z",
        "voteCount": 1,
        "content": "is DDDD"
      },
      {
        "date": "2023-01-04T11:23:00.000Z",
        "voteCount": 1,
        "content": "I read the article, why is it not selective deployment, where we can pick which ones we want to deploy?"
      },
      {
        "date": "2023-01-05T07:13:00.000Z",
        "voteCount": 4,
        "content": "Because the requirement states that: Once feature development is complete, ALL entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace. --&gt; if all will be promoted, selective deployment would not make sense"
      },
      {
        "date": "2022-12-12T12:57:00.000Z",
        "voteCount": 3,
        "content": "I think it's correct\nCheck this for more info: https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started#step-4---create-deployment-rules"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91638-exam-dp-500-topic-1-question-15-discussion/",
    "body": "HOTSPOT -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to populate the CustomersWithProductScore table.<br>How should you complete the stored procedure? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image14.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image15.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-15T00:05:00.000Z",
        "voteCount": 27,
        "content": "VARBINARY(MAX)\nCustomPurchases\n\nTo generate predict with model, the input data should come from customer purchase history.\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?preserve-view=true&amp;view=azure-sqldw-latest"
      },
      {
        "date": "2023-05-02T07:50:00.000Z",
        "voteCount": 1,
        "content": "Just a typo, its CustomerPurchases (its a view as per the question)"
      },
      {
        "date": "2023-03-09T03:09:00.000Z",
        "voteCount": 1,
        "content": "I'm agree"
      },
      {
        "date": "2023-08-18T23:02:00.000Z",
        "voteCount": 6,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- VARBINARY(max)\n- dbo.CustomerPurchases"
      },
      {
        "date": "2023-03-30T01:56:00.000Z",
        "voteCount": 4,
        "content": "VARBINARY(MAX) : this data type holds variable-length binary data. Use this type when the data is expected to vary in size. \nCustomPurchases:  It is the only with the data required and you can not insert data in a table from the same table, it doesnt have sense\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2023-02-04T08:43:00.000Z",
        "voteCount": 4,
        "content": "The column Score is comming from the model, not the SQL View.\nSo the view is CustomPurchases. \nBy the way CustomersWithProductScore is populated in the SQL statement, so better not to read from it as well."
      },
      {
        "date": "2023-01-25T07:43:00.000Z",
        "voteCount": 6,
        "content": "VARBINARY(MAX) as models are stored as binary: https://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16#combining-predict-with-an-insert-statement\nCustomersWithProductScore: the only table that contains all columns listed as 'd'"
      },
      {
        "date": "2023-05-25T01:35:00.000Z",
        "voteCount": 4,
        "content": "there is a view with all these 4 columns under D alias - \"SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName]\""
      },
      {
        "date": "2023-01-24T03:44:00.000Z",
        "voteCount": 4,
        "content": "VARBINARY(MAX)\nCustomPurchases"
      },
      {
        "date": "2023-01-06T20:46:00.000Z",
        "voteCount": 3,
        "content": "I think the answer is correct. \nThe ML Model output is float, \nand the field listed for customerID,Email, productName and score are listed in the case study under dbo.CustomersWithProductScore"
      },
      {
        "date": "2023-01-05T00:06:00.000Z",
        "voteCount": 1,
        "content": "PLease anyone confirm"
      },
      {
        "date": "2023-01-04T01:47:00.000Z",
        "voteCount": 1,
        "content": "Please comment"
      },
      {
        "date": "2023-01-01T22:00:00.000Z",
        "voteCount": 3,
        "content": "why custompurchases?It should be productscore"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93912-exam-dp-500-topic-1-question-16-discussion/",
    "body": "HOTSPOT -<br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br>Overview -<br><br>Existing environment -<br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br>Azure Resources -<br>Litware has the following Azure resources:<br>An Azure Synapse Analytics workspace named synapseworkspace1<br>An Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>A Synapse Analytics dedicated SQL pool named SQLDW<br><br>Dedicated SQL Pool -<br>SQLDW contains a dimensional model that contains the following tables.<br><img src=\"https://img.examtopics.com/dp-500/image8.png\"><br>SQLDW contains the following additional tables.<br><img src=\"https://img.examtopics.com/dp-500/image9.png\"><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br>Power BI -<br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br>All users have Power BI Premium per user licenses.<br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br>Requirements -<br><br>Analytics Goals -<br>Litware identifies the following analytics goals:<br>Provide historical reporting of sales by product and channel over time.<br>Allow sales managers to perform ad hoc sales reporting with minimal effort.<br>Perform market basket analysis to understand which products are commonly purchased in the same transaction.<br>Identify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br>Security Requirements -<br>Litware identifies the following security requirements for the analytics environment:<br>All the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>Customer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>Sales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>Sales managers must be prevented from modifying reports created by other users.<br>Development Process Requirements<br>Litware identifies the following development process requirements:<br>SQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>Power BI content must be deployed to test and production by using deployment pipelines.<br>All SQL scripts must be stored in Azure Repos.<br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br>You need to create a measure to count orders for the market basket analysis.<br>How should you complete the DAX expression? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image16.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image17.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-09T08:38:00.000Z",
        "voteCount": 10,
        "content": "This is a very similar example. https://www.daxpatterns.com/basket-analysis/#:~:text=The%20Basket%20analysis%20pattern%20builds,the%20name%20of%20this%20pattern."
      },
      {
        "date": "2023-01-23T05:48:00.000Z",
        "voteCount": 3,
        "content": "and it confirms the that the provided answer is correct. Thanks for the link!"
      },
      {
        "date": "2023-07-31T11:33:00.000Z",
        "voteCount": 1,
        "content": "Correct. I have a little doubt though as per the last variable RemoveEmpty, shouldn't be 'NOT IS EMPTY' in lieu of 'ISEMPTY' ? so when it returns True, the variable will return the [BasketsWithBothProducts]"
      },
      {
        "date": "2023-04-29T13:46:00.000Z",
        "voteCount": 2,
        "content": "is RemoveEmpty really necessary?"
      },
      {
        "date": "2023-03-09T03:19:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-01-05T01:54:00.000Z",
        "voteCount": 3,
        "content": "Is this correct  ?"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90929-exam-dp-500-topic-1-question-17-discussion/",
    "body": "You have a Power BI tenant.<br>You plan to register the tenant in an Azure Purview account.<br>You need to ensure that you can scan the tenant by using Azure Purview.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Microsoft 365 admin center, create a Microsoft 365 group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin center, set Allow live connections to Enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin center, set Allow service principals to use read-only Power BI admin APIs to Enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure Active Directory admin center, create a security group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin center, set Share content with external users to Enabled."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T06:45:00.000Z",
        "voteCount": 12,
        "content": "Answer is correct. Reference: https://learn.microsoft.com/en-us/azure/purview/register-scan-power-bi-tenant?tabs=Scenario1"
      },
      {
        "date": "2023-01-25T07:52:00.000Z",
        "voteCount": 6,
        "content": "Correct"
      },
      {
        "date": "2023-05-30T05:56:00.000Z",
        "voteCount": 1,
        "content": "C and D are correct. \nhttps://learn.microsoft.com/en-us/azure/purview/register-scan-power-bi-tenant?tabs=Scenario1#scan-same-tenant-power-bi-using-azure-ir-and-managed-identity-in-public-network"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91283-exam-dp-500-topic-1-question-18-discussion/",
    "body": "You have a Power BI workspace named Workspace1 that contains five dataflows.<br>You need to configure Workspace1 to store the dataflows in an Azure Data Lake Storage Gen2 account.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Power BI Admin portal, enable tenant-level storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable load for all dataflow queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the dataflow queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Data source settings in the dataflow queries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-06T23:22:00.000Z",
        "voteCount": 6,
        "content": "The question says to configure the Workspace1....\n\nAnswer is C.\nFinally, you can connect to any ADLS Gen 2 from the Admin portal, but if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-08-31T21:53:00.000Z",
        "voteCount": 2,
        "content": "says \"Finally\" so enabling tenant-level storage comes first"
      },
      {
        "date": "2024-04-29T03:15:00.000Z",
        "voteCount": 1,
        "content": "pre req is to ensure there is no dataflows in the workspace"
      },
      {
        "date": "2024-01-20T01:28:00.000Z",
        "voteCount": 1,
        "content": "It seems like everyone is missing the word \"First\". It's A."
      },
      {
        "date": "2024-01-17T05:07:00.000Z",
        "voteCount": 2,
        "content": "First in the admin portal the setting has to be enabled"
      },
      {
        "date": "2023-12-16T02:10:00.000Z",
        "voteCount": 1,
        "content": "tenant-level storage is not mandatory for this case, as question is mentioning workspace connection to ADLS2.\nhowever, workspace must be empty of dataflows before connecting to the ADLS2 account \n\n\"if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\n\"if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-09-13T23:29:00.000Z",
        "voteCount": 1,
        "content": "i'm a bit lost here, but my guess is also b or C since A is tenant level and you need to config a workspace"
      },
      {
        "date": "2023-09-13T23:33:00.000Z",
        "voteCount": 1,
        "content": "ok sorry, chatgpt; Tenant-level storage allows you to specify a default storage location for dataflows at the workspace level. &gt; so that's what i FIRST would do"
      },
      {
        "date": "2023-09-03T04:03:00.000Z",
        "voteCount": 2,
        "content": "No116 is same question\uff0cwhy 116 answer is  A. Change the Data source settings in the dataflow queries.\nIf in No18,  \"A. From the Power BI Admin portal...\" is the best answer ?"
      },
      {
        "date": "2023-08-17T22:10:00.000Z",
        "voteCount": 2,
        "content": "It looks like that A is correct."
      },
      {
        "date": "2023-08-01T08:58:00.000Z",
        "voteCount": 3,
        "content": "Answer is C. Won't be able to create the connection while dataflows exist in the workspace -&gt; https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration -&gt; \"you must first ensure there are no dataflows in the workspace before connecting\""
      },
      {
        "date": "2023-07-10T09:46:00.000Z",
        "voteCount": 1,
        "content": "100% its A"
      },
      {
        "date": "2023-07-06T10:46:00.000Z",
        "voteCount": 1,
        "content": "AS MENTIONED EARLIER, SEE THIS https://learn.microsoft.com/en-us/azure/purview/register-scan-power-bi-tenant?tabs=Scenario1"
      },
      {
        "date": "2023-06-29T13:15:00.000Z",
        "voteCount": 2,
        "content": "According to ChatGPT the correct answer is A: Enabling tenant-level storage allows you to configure the default storage location for dataflows in Power BI. By enabling this feature and setting up the Azure Data Lake Storage Gen2 account as the default storage location, any new dataflows created within Workspace1 will automatically store their data in the specified Azure Data Lake Storage Gen2 account."
      },
      {
        "date": "2023-06-26T03:36:00.000Z",
        "voteCount": 2,
        "content": "MS make questions simple. Maybe this has not been transcribed fully from the exam with all the info needed. \nI would put money on MS asking for the first (logical) step to add in an ADLS, which typically requires enabling of Azure storage.  Happy to be proven wrong. \n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-06-25T07:00:00.000Z",
        "voteCount": 2,
        "content": "The answer is C, the existing dataflows shall be deleted from a workspace before the workspace can be connected to a storage account. (check the 'Connect Power BI with your dataflow storage' video by Curbal on YouTube)"
      },
      {
        "date": "2023-05-26T13:27:00.000Z",
        "voteCount": 2,
        "content": "Looks like it is C, MS guide is saying that there should be no DFs if we're trying to use ADLSGen2 ACCOUNT(!), and check out this discussion as well - https://community.fabric.microsoft.com/t5/Service/Configuring-dataflow-storage-to-use-Azure-Data-Lake-Gen-2-when/m-p/2214368/highlight/true"
      },
      {
        "date": "2023-05-12T04:43:00.000Z",
        "voteCount": 3,
        "content": "If we go with the prerequisite - should we not delete the dataflows in the workspace before we connect it to ADLS? Option C ?"
      },
      {
        "date": "2023-05-02T08:01:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration\n\n\"This feature essentially allows you to \"bring your own storage\" to Power BI dataflows, and establish a connection at the tenant or workspace level.\""
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91284-exam-dp-500-topic-1-question-19-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br>You need to ensure that the SQL pool is scanned by Azure Purview.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data share connection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch the data catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister a data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T06:41:00.000Z",
        "voteCount": 8,
        "content": "Source: https://learn.microsoft.com/en-us/azure/purview/register-scan-synapse-workspace?tabs=MI#steps-to-register"
      },
      {
        "date": "2023-08-24T11:53:00.000Z",
        "voteCount": 1,
        "content": "best answer"
      },
      {
        "date": "2023-08-18T23:03:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was \"Register a data source.\""
      },
      {
        "date": "2023-05-30T06:03:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2022-12-12T13:26:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91285-exam-dp-500-topic-1-question-20-discussion/",
    "body": "You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use import storage mode.<br>A database administrator reports a drastic increase in the number of queries sent from the Power BI service to an Azure SQL database since the creation of the deployment pipeline.<br>An investigation into the issue identifies the following:<br>One of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.<br>When publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.<br>You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.<br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off auto refresh when publishing the dataset changes to the Power BI service.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the dataset, change the fact table from an import table to a hybrid table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the large dataset storage format for workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset parameter to reduce the fact table row count in the development and test pipelines."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-30T15:58:00.000Z",
        "voteCount": 8,
        "content": "B is correct: By changing the fact table from an import table to a hybrid table, you can leverage the benefits of DirectQuery storage mode for the fact table. With DirectQuery, the data remains in the Azure SQL database, and Power BI sends queries directly to the database when users interact with the report. This approach can significantly reduce the amount of data transferred between Power BI and the Azure SQL database."
      },
      {
        "date": "2023-10-26T05:00:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT agrees with you. So I'll go with you and ChatGPT."
      },
      {
        "date": "2022-12-12T13:30:00.000Z",
        "voteCount": 5,
        "content": "Correct\nFor more info, check: https://powerbi.microsoft.com/en-za/blog/announcing-public-preview-of-hybrid-tables-in-power-bi-premium/"
      },
      {
        "date": "2023-01-05T23:20:00.000Z",
        "voteCount": 2,
        "content": "I also agree with B mostly bc with D I don't see how we justify working with lesw rows in development and production. Usually you want more data at these stages, not less, to make sure what yu've developed primarily applies to more/different data."
      },
      {
        "date": "2023-01-05T23:21:00.000Z",
        "voteCount": 3,
        "content": "*test and production, sorry"
      },
      {
        "date": "2024-01-17T05:12:00.000Z",
        "voteCount": 1,
        "content": "B is the simple and correct answer"
      },
      {
        "date": "2023-12-27T23:29:00.000Z",
        "voteCount": 1,
        "content": "I think it's B as well; as DirectQuery option would be best for such a huge table. D option does not cover solution for production environment"
      },
      {
        "date": "2023-09-27T08:01:00.000Z",
        "voteCount": 1,
        "content": "You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.\n\nPRODUCTION being the key word here.\n\nD. Create a dataset parameter to reduce the fact table row count in the development and test pipelines. -  does NOT reduce queries for the production environment.\n\nCorrect answer is B"
      },
      {
        "date": "2023-09-21T00:16:00.000Z",
        "voteCount": 1,
        "content": "With DirectQuery for Fact table"
      },
      {
        "date": "2023-09-13T23:39:00.000Z",
        "voteCount": 1,
        "content": "d\napplied this to clients already\nIf you publish a report with large dataset it will take forever.\nlimit the tables to for example top 5 rows with a parameter \ndeploy\nand in the service turn the parameter off (set to false) and the data is refreshed in the service and you wont publish a large dataset"
      },
      {
        "date": "2023-08-08T01:29:00.000Z",
        "voteCount": 1,
        "content": "By creating a dataset parameter that can dynamically filter or reduce the fact table's row count during development and testing pipelines, you can limit the amount of data that needs to be refreshed and processed. This can help in reducing the load on the database during these pipeline runs and optimize the performance."
      },
      {
        "date": "2023-08-05T11:15:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer : B\n\nhttps://data-mozart.com/hybrid-tables-in-power-bi-the-ultimate-guide/"
      },
      {
        "date": "2023-05-02T08:36:00.000Z",
        "voteCount": 2,
        "content": "issue identified after the deployment, so this solution is wrong which is only applicable to development and testing. Given answer is correct"
      },
      {
        "date": "2023-05-02T05:54:00.000Z",
        "voteCount": 3,
        "content": "the answer is D, pls read the question carefully \n\"reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production\""
      },
      {
        "date": "2023-05-26T13:54:00.000Z",
        "voteCount": 2,
        "content": "Agree, the only option to REDUCE SIZE OF QUERIES is to limit records (there is OR  in \"are published to development, test, or production\", so in any of that), hybrid table will get you the same 500million rows"
      },
      {
        "date": "2023-12-20T00:34:00.000Z",
        "voteCount": 1,
        "content": "please the part you have pasted and the answer, you will find the missing part is that answer does not apply to production so therefore it is incorrect"
      },
      {
        "date": "2023-03-29T22:07:00.000Z",
        "voteCount": 5,
        "content": "The issue of a drastic increase in the number of queries sent to an Azure SQL database can be addressed by reducing the amount of data transferred during a dataset refresh. Given that one of the datasets is larger than 1 GB and has a fact table with over 500 million rows, a full refresh of the entire dataset can cause a significant increase in the number of queries sent to the database.\n\nTherefore, to reduce the size of queries sent to the database during dataset refreshes, I would recommend creating a dataset parameter to filter the fact table rows based on a condition. This will enable the development and test pipelines to refresh the dataset with a smaller subset of data, reducing the overall size of the queries sent to the database."
      },
      {
        "date": "2023-03-29T22:07:00.000Z",
        "voteCount": 1,
        "content": "Answer given by Chat GPT"
      },
      {
        "date": "2023-04-13T21:38:00.000Z",
        "voteCount": 1,
        "content": "I disagree. Data in the test environment should be the same as in production, hence you can't reduce number of rows.\nFor me correct answer is B"
      },
      {
        "date": "2023-04-16T12:58:00.000Z",
        "voteCount": 1,
        "content": "I prefere B but, the needed but the requirement does not talk about same date in production and test/develop. D could be a solution but we have the issue in producction"
      },
      {
        "date": "2023-05-02T08:35:00.000Z",
        "voteCount": 1,
        "content": "issue identified after the deployment, so this solution is wrong which is only applicable to development and testing. Given answer is correct"
      },
      {
        "date": "2023-03-29T02:15:00.000Z",
        "voteCount": 1,
        "content": "D is correct. \n\nIt says that a refresh is always triggered when publishing from one stage to the next, hence changing the refresh mode will not help the situation"
      },
      {
        "date": "2023-03-24T03:15:00.000Z",
        "voteCount": 2,
        "content": "B is correct . You could see this video to know more about Hybrid table benefits \nhttps://www.youtube.com/watch?v=HckuKYlx8kk"
      },
      {
        "date": "2023-03-20T04:54:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-03-15T06:10:00.000Z",
        "voteCount": 1,
        "content": "I believe B is correct since the volume of data going into the test environment should be as close a possible to what will be experienced in production"
      },
      {
        "date": "2022-12-31T12:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90897-exam-dp-500-topic-1-question-21-discussion/",
    "body": "You have a Power BI Premium capacity.<br>You need to increase the number of virtual cores associated to the capacity.<br>Which role do you need?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI workspace admin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcapacity admin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower Platform admin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI admin\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T03:05:00.000Z",
        "voteCount": 20,
        "content": "\"Power BI admins and global administrators can change Power BI Premium capacity. Capacity admins who are not a Power BI admin or global administrator don't have this option.\"\n\nReference: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage"
      },
      {
        "date": "2022-12-10T01:38:00.000Z",
        "voteCount": 7,
        "content": "To increase the number of virtual cores associated with a Power BI Premium capacity, you need to have the capacity admin role. The capacity admin is responsible for managing the capacity settings, including the number of virtual cores, memory, and other resources allocated to the capacity."
      },
      {
        "date": "2023-07-07T10:23:00.000Z",
        "voteCount": 1,
        "content": "I guess Capacity admin is the correct one https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#setting-up-a-new-capacity-power-bi-premium"
      },
      {
        "date": "2023-07-25T04:30:00.000Z",
        "voteCount": 2,
        "content": "From your link:\nPower BI admins and global administrators can change Power BI Premium capacity. Capacity admins who aren't a Power BI admin or global administrator don't have this option.\n\nSo correct is D"
      },
      {
        "date": "2024-02-19T21:19:00.000Z",
        "voteCount": 1,
        "content": "Its D.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#manage-capacity\nPower BI admins and global administrators can change Power BI Premium capacity. Capacity admins who aren't a Power BI admin or global administrator don't have this option."
      },
      {
        "date": "2023-07-07T10:24:00.000Z",
        "voteCount": 1,
        "content": "Sorry I meant B and NOT C\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#setting-up-a-new-capacity-power-bi-premium"
      },
      {
        "date": "2023-07-07T10:23:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#setting-up-a-new-capacity-power-bi-premium"
      },
      {
        "date": "2023-06-25T07:45:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nAfter you have purchased capacity nodes in Microsoft 365, you set up the capacity in the Power BI admin portal. You manage Power BI Premium capacities in the Capacity settings section of the portal.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#manage-capacity"
      },
      {
        "date": "2023-05-30T06:10:00.000Z",
        "voteCount": 2,
        "content": "D is correct.  \nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage#change-capacity-size"
      },
      {
        "date": "2023-03-12T14:36:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says \"capacity admin\" or \"Power BI Premium capacity admin.\""
      },
      {
        "date": "2023-04-16T13:00:00.000Z",
        "voteCount": 1,
        "content": "do not trust a technology that has just been launched"
      },
      {
        "date": "2023-02-12T09:17:00.000Z",
        "voteCount": 1,
        "content": "\"Capacity Admins don't have permissions to create new capacities or scale existing capacities\" \nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-capacity-manage-gen2"
      },
      {
        "date": "2023-01-24T04:06:00.000Z",
        "voteCount": 2,
        "content": "Capacity Admin\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-capacity-manage-gen2"
      },
      {
        "date": "2022-12-29T09:44:00.000Z",
        "voteCount": 3,
        "content": "Ramiel has already explained"
      },
      {
        "date": "2022-12-21T06:48:00.000Z",
        "voteCount": 4,
        "content": "Power BI admin"
      },
      {
        "date": "2022-12-12T13:38:00.000Z",
        "voteCount": 1,
        "content": "B is correct as AT96 mentioned"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92630-exam-dp-500-topic-1-question-22-discussion/",
    "body": "You are attempting to configure certification for a Power BI dataset and discover that the certification setting for the dataset is unavailable.<br>What are two possible causes of the issue? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe workspace is in shared capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou have insufficient permissions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataset certification is disabled for the Power BI tenant.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe sensitivity level for the dataset is set to Highly Confidential.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRow-level security (RLS) is missing from the dataset."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-03-24T03:31:00.000Z",
        "voteCount": 6,
        "content": "BC are ok! For more info : https://learn.microsoft.com/en-us/power-bi/admin/service-admin-setup-certification"
      },
      {
        "date": "2023-08-18T23:04:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- You have insufficient permissions.\n- Dataset certification is disabled for the Power BI tenant."
      },
      {
        "date": "2023-03-20T06:14:00.000Z",
        "voteCount": 2,
        "content": "\"...Certification is available only if a Power BI administrator has enabled and configured it for your organization...\"\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-endorsement-overview#endorsement-overview"
      },
      {
        "date": "2023-03-15T06:43:00.000Z",
        "voteCount": 2,
        "content": "Tried it in a shared capacity, it worked after enabling certification in Admin tenant settings"
      },
      {
        "date": "2023-03-12T14:41:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says:\nThe possible causes of the issue described in the question are:\n\nA. The workspace is in shared capacity.\nB. You have insufficient permissions.\n\nTo configure certification for a Power BI dataset, the workspace must be in Power BI Premium capacity. If the workspace is in shared capacity, certification settings will not be available.\n\nAlso, to configure certification, you need to have the necessary permissions to create and manage datasets in the workspace. If you have insufficient permissions, certification settings will be unavailable.\n\nHere are the Microsoft documentation links for more information:\n\nA. Configuring dataset certification in Power BI: https://docs.microsoft.com/en-us/power-bi/certify/datasets\nB. Manage workspace access in Power BI: https://docs.microsoft.com/en-us/power-bi/collaborate-share/service-manage-workspaces\n\nThe other options mentioned in the question, C, D, and E, are not related to the issue of certification settings being unavailable."
      },
      {
        "date": "2022-12-29T09:46:00.000Z",
        "voteCount": 3,
        "content": "BC for sure!"
      },
      {
        "date": "2022-12-24T05:47:00.000Z",
        "voteCount": 1,
        "content": "B and C looks correct to me. \nRef: https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-endorsement-overview"
      },
      {
        "date": "2022-12-24T05:21:00.000Z",
        "voteCount": 2,
        "content": "You can read more about it here\nhttps://www.fourmoo.com/2019/06/25/how-to-setup-and-get-a-certified-dataset-in-powerbi/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93284-exam-dp-500-topic-1-question-23-discussion/",
    "body": "Your company is migrating its current, custom-built reporting solution to Power BI.<br>The Power BI tenant must support the following scenarios:<br>40 reports that will be embedded in external websites. The websites control their own security. The reports will be consumed by 50 users monthly.<br>Forty-five users that require access to the workspaces and apps in the Power BI Admin portal. Ten of the users must publish and consume datasets that are larger than 1 GB.<br>Ten developers that require Text Analytics transformations and paginated reports for datasets. An additional 15 users will consume the reports.<br>You need to recommend a licensing solution for the company. The solution must minimize costs.<br>Which two Power BI license options should you include in the recommendation? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t70 Premium per user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tone Premium\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t70 Pro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tone Embedded",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t35 Pro\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t35 Premium per user"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-04T11:06:00.000Z",
        "voteCount": 11,
        "content": "My answer is One Premium and 35 Pro.\n\nSince we need 45 users to have access to the workspaces and apps, 35 Premium Per user would not work because for the Premium Per User workspace content to be accessed we need that all users have Premium Per User. So even if we go for Premium Per User, then we need to purchase it for 70 users.\n\nAlso, since we want to publish reports to web and not embed to the app, I would not consider Embedded capacity.\n\nAnother important thing is to minimize the cost. We can use Premium capacity for publishing the reports to web, supporting Text Analytics transformations and paginated reports and having dataset more than 1GB. \nFor the users who will be actively busy with development we can purchase 35 pro licenses (IMO 20 pro account is enough)."
      },
      {
        "date": "2023-03-14T03:06:00.000Z",
        "voteCount": 6,
        "content": "I took the exam the other day, and there is one change to this question as it want's three answers. \nI went with \n1 Premium\n35 Pro\nEmbedded\n\nI have to say that I am not sure about embedded. But just keep in mind that it is 3 answers and not just 2 in the actual exam\n.\nGood luck!"
      },
      {
        "date": "2023-06-29T04:49:00.000Z",
        "voteCount": 2,
        "content": "Agreed - this is asking for 3 answers."
      },
      {
        "date": "2024-01-17T05:25:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-18T23:06:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nThe questions required three licenses. My answer was:\n- one Premium\n- one Embedded\n- 35 Pro"
      },
      {
        "date": "2023-07-07T11:15:00.000Z",
        "voteCount": 1,
        "content": "For embeding in external we need to have one embded.\n\nRegarding the rest:\n\"With Power BI Pro, you can use all sharing methods except Power BI Embedded (which comes through different licensing options). You can use Simple Sharing, Workspaces, Power BI Apps, and Embed in SharePoint Online. If you have a Power BI Pro license, you can share the reports internally or externally (if allowed by the Power BI Administrator). However, if you share a report with someone else, they must also have a paid Power BI license to view the reports. If you have a pro license and share a report with a free user, they cannot see it unless they upgrade to the pro user or premium per user\"\nhttps://radacad.com/power-bi-licensing-walk-through-guide"
      },
      {
        "date": "2023-07-07T11:17:00.000Z",
        "voteCount": 1,
        "content": "please delete my comment admin, sorry for the inconvenience,"
      },
      {
        "date": "2023-07-01T03:18:00.000Z",
        "voteCount": 1,
        "content": "There were three in the actual exam. I selected One Pro"
      },
      {
        "date": "2023-06-29T13:23:00.000Z",
        "voteCount": 1,
        "content": "As per ChatGPT:\nTo minimize costs and meet the requirements of the different user scenarios, the recommended Power BI license options would be:\n\nC. 70 Pro: This license option is suitable for the 45 users who require access to workspaces and apps in the Power BI Admin portal. The Pro license provides the necessary capabilities for consuming and interacting with reports and dashboards.\n\nD. one Embedded: This license option is appropriate for the 40 reports that will be embedded in external websites. The websites control their own security, so the Embedded license allows you to embed Power BI content securely within these external sites. With one Embedded license, you can distribute and share the reports across multiple websites.\n\nBy combining these two license options, you can cover all the user scenarios while minimizing costs. The 10 developers who require Text Analytics transformations and paginated reports can utilize the Pro licenses, and the additional 15 users who consume the reports can also use the Pro licenses assigned to them.\n\nTherefore, the correct answers are:\nC. 70 Pro\nD. one Embedded"
      },
      {
        "date": "2023-03-24T06:38:00.000Z",
        "voteCount": 3,
        "content": "D and F \nPower BI Embedded - This license is designed for users who need to embed reports in external websites or applications. It allows the company to embed up to 40 reports in external websites, and it also supports user-based licensing, which is ideal for the 50 monthly users who will consume the embedded reports.\n10 of the users must publish and consume datasets that are larger than 1 GB \n+ 10 developers that require Text Analytics transformations and paginated reports for datasets \n+Additional 15 users will consume the reports  (if they will work with)\n--&gt; They need PPU making it a 35 Premium Per User (PPU) license\nOthers\nFree License) As long as your colleagues use Premium capacity workspaces to share content, free users can view and interact with that content. When Free is shown, you can only create content in My Workspace and consume content that is hosted in a Premium workspace.\nhttps://learn.microsoft.com/en-us/power-bi/consumer/end-user-features"
      },
      {
        "date": "2023-03-27T08:04:00.000Z",
        "voteCount": 1,
        "content": "and \"Forty-five users that require access to the workspaces and apps in the Power BI Admin portal\", you arent giving them a solution no???"
      },
      {
        "date": "2023-04-05T00:23:00.000Z",
        "voteCount": 1,
        "content": "People with the licenses could share the reports and apps, and the 45 could access without license"
      },
      {
        "date": "2023-04-05T00:26:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/consumer/end-user-features read it please and obtain your own conclusions ;)"
      },
      {
        "date": "2023-04-05T00:24:00.000Z",
        "voteCount": 1,
        "content": "\u201c\u202645 users require access to the workspaces and apps in the Power BI Admin portal. \n-&gt;(Free License) As long as your colleagues use Premium capacity workspaces to share content, free users can view and interact with that content. When Free is shown, you can only create content in My Workspace and consume content that is hosted in a Premium workspace."
      },
      {
        "date": "2023-03-15T06:56:00.000Z",
        "voteCount": 1,
        "content": "1 premium, 1 embedded, 20 pro least cost"
      },
      {
        "date": "2023-03-12T14:45:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says:\nThe recommended Power BI license options for the given scenarios are:\n\nB. One Premium license for the developers who require Text Analytics transformations and paginated reports, and for the ten users who must publish and consume datasets that are larger than 1 GB.\n\nD. One Embedded license for the 40 reports that will be embedded in external websites. This license provides the necessary capacity for embedding the reports and can be controlled by the websites' security.\n\nBoth options are cost-effective as they provide the necessary features and capacities while minimizing costs.\n\nHere are the Microsoft documentation links for more information:\n\n\nB. Power BI Premium: https://docs.microsoft.com/en-us/power-bi/admin/service-premium-what-is\n\nD. Power BI Embedded: https://docs.microsoft.com/en-us/power-bi/developer/embedded/embedded-faq"
      },
      {
        "date": "2023-03-07T02:40:00.000Z",
        "voteCount": 1,
        "content": "My answer is One premium and 35 PRO (actually 20 PRO would have been enough)\n\nTen of the users must publish and consume datasets that are larger than 1 GB. (Premium)\nTen developers that require Text Analytics transformations and paginated reports for datasets. (Pro license for developpers) \nAn additional 15 users will consume the reports. (not relevant if we have Premium capacity)"
      },
      {
        "date": "2023-02-21T08:17:00.000Z",
        "voteCount": 3,
        "content": "should be, one premium, 35 Pro and 35 Premium per user."
      },
      {
        "date": "2023-05-25T01:19:00.000Z",
        "voteCount": 1,
        "content": "if we consider that Users with PPU capacity subscriptions can't share content with users who have a Pro or free license  BF would be correct"
      },
      {
        "date": "2023-02-21T08:15:00.000Z",
        "voteCount": 3,
        "content": "There is an error, the exam says THREE answers, so all the discussion is INCORRECT"
      },
      {
        "date": "2023-02-17T05:14:00.000Z",
        "voteCount": 3,
        "content": "I sat for this exam today but when I saw this question I was asked to select three answers and not two.\n\nPlease review the question and answers"
      },
      {
        "date": "2023-01-31T17:13:00.000Z",
        "voteCount": 1,
        "content": "premium vs embedded -\nhttps://www.bluegranite.com/blog/choosing-between-pbi-premium-or-embedded"
      },
      {
        "date": "2023-01-11T03:11:00.000Z",
        "voteCount": 1,
        "content": "I think Embedded capacity will be enough (there is no restriction 1GB). \nUsers with Power BI Embedded capacity can access the workspaces and apps that they have been granted access to by the administrator of the tenant or the workspace.\n\nWith Power BI Embedded capacity you can give your end users access to report, dashboards and tiles that are embedded in your app. The access to the workspaces and apps depends on the way the capacity is being used and the permissions being granted by the administrator.\n\nFor example, if the Power BI Embedded capacity is being used to create an ISV (Independent Software Vendor) app, the app creator may have access to a specific set of workspaces and apps, and their customers may have access to a different set of workspaces and apps. Additionally, the administrator of the tenant can control the access to the workspaces and apps and can assign roles to the users with different level of permissions, such as view, edit and admin access.\nAnd also it's much more cheaper then Premium."
      },
      {
        "date": "2023-01-09T17:22:00.000Z",
        "voteCount": 1,
        "content": "I think Pro is necessary only if we are assuming that there are premium workspaces set up. However, the language is that \"10 of the users must publish and consume datasets larger than 1 GB\" which refers to capabilities without configuration."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90898-exam-dp-500-topic-1-question-24-discussion/",
    "body": "You have two Power BI reports named Report1 and Report2.<br>Report1 connects to a shared dataset named Dataset1.<br>Report2 connects to a local dataset that has the same structure as Dataset1. Report2 contains several calculated tables and parameters.<br>You need to prepare Report2 to use Dataset1.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the data source permissions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete all the Power Query Editor objects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the source of each query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate all the parameter values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete all the calculated tables."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 11,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T01:50:00.000Z",
        "voteCount": 12,
        "content": "CORRECT ANSWER IS: CE\n\nModify the source of each query: You need to modify the source of each query in Report2 to use Dataset1 as the data source, instead of the local dataset. This will ensure that Report2 uses the data from Dataset1, rather than the data from the local dataset.\nDelete all the calculated tables: Report2 contains several calculated tables that were based on the local dataset. These calculated tables will no longer be valid when using Dataset1 as the data source, so you should delete them."
      },
      {
        "date": "2023-01-05T23:48:00.000Z",
        "voteCount": 10,
        "content": "I agree with C and your explanation for it but I would be hesitant to choose E bc calculated columns absolutely can work if the dataset has the same structure meaning same column names and data types. The calculation will work on the dataset 1 data just as it worked on dataset2 if al elements of the formula for the calculated column stay the same. Therefore I would go with D as the second answer."
      },
      {
        "date": "2022-12-29T10:55:00.000Z",
        "voteCount": 8,
        "content": "You just need to remove all the queries in the PQ and delete calculated tables. You are not able to modify queries to connect Power BI Dataset. Also, when you connect to Power BI Dataset and after that go to Power Query, there is 0 queries, nothing is displayed. Plus, in the question there is no indication that we need the calculated tables and parameters, so no problem with deleting them. We just need to move to dataset1"
      },
      {
        "date": "2024-01-17T05:28:00.000Z",
        "voteCount": 1,
        "content": "Correct in exam perspective"
      },
      {
        "date": "2023-09-07T01:41:00.000Z",
        "voteCount": 2,
        "content": "A. Incorrect: As removing dataset permissions would generate errors while refreshing data. Local dataset eneds to be removed from Report2\n\tB. Correct: As you need to prepare report for shared dataset which would work as live connection so all pwer query objects should be deleted\n\tC. As we need to delete the power query objects and in shared dataset there is a live connection with all the tables in that dataset modifying the source of eact PQ object/query is irrelevant\n\tD. Incorrect: There is no mention of power query parameters so seems irrelevant with scenario posted in the querstion\n Correct: When you are building a report based of shared Dataset you can not use calculated tables (6:35 of video https://www.youtube.com/watch?v=ENZm_7jWU4Q) so they would need to be deleted. Also I tried it myself and with shared dataset under modelling new table option is disabled"
      },
      {
        "date": "2023-08-18T23:07:00.000Z",
        "voteCount": 5,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Modify the source of each query.\n- Update all the parameter values"
      },
      {
        "date": "2023-08-08T01:57:00.000Z",
        "voteCount": 3,
        "content": "Option C: This involves updating the source of each query in Report2 to point to the shared dataset Dataset1\nOption D: If Report2 has parameters that are specific to its local dataset, you should update these parameters to match the parameters of Dataset1."
      },
      {
        "date": "2023-07-07T11:43:00.000Z",
        "voteCount": 2,
        "content": "I think the question has three correct answers, We need to change source this is  a fact, Also PQ and calculated columns are not available in shared dataset.\n\nThe connection to a Power BI dataset won't be shown in Power Query.\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/desktop-composite-models\n\nalso see here regarding above:\nhttps://community.fabric.microsoft.com/t5/Desktop/Can-not-use-Power-Query-with-a-shared-dataset/td-p/2090909\n\n\nWhile you're using Power Bi Shared dataset, you can create measures but you will not be able to create a new column or do any such changes in your report as it derives it's properties and data from the source dataset.\nhttps://community.fabric.microsoft.com/t5/Service/Help-needed-with-shared-datasets-and-limitations-of-live/m-p/2386845#:~:text=While%20you're%20using%20Power,data%20from%20the%20source%20dataset.\n\nSame thing here: video time 6:35\nhttps://radacad.com/power-bi-licensing-walk-through-guide"
      },
      {
        "date": "2023-06-18T22:36:00.000Z",
        "voteCount": 1,
        "content": "How can you connect to the new data sources without modifying the existing sources"
      },
      {
        "date": "2023-04-21T09:41:00.000Z",
        "voteCount": 1,
        "content": "You connect to a shared dataset using a live connection. Therefore you can not use PQ."
      },
      {
        "date": "2023-04-19T05:33:00.000Z",
        "voteCount": 1,
        "content": "You can modify the source for each query and the calculated tables are automatically updated if the underlying source is changed provided the structure is still the same in the new data source."
      },
      {
        "date": "2023-03-25T05:30:00.000Z",
        "voteCount": 2,
        "content": "C and D are correct"
      },
      {
        "date": "2023-03-24T06:40:00.000Z",
        "voteCount": 2,
        "content": "C and D are OK"
      },
      {
        "date": "2023-03-22T23:03:00.000Z",
        "voteCount": 3,
        "content": "The correct answers are C and D.\nExplanation:\nTo prepare Report2 to use Dataset1, you need to modify the queries' source to point to Dataset1 and update the parameter values accordingly. Therefore, you should perform the following actions:\nC. Modify the source of each query: You need to update the source of each query in Report2 to point to Dataset1 instead of the local dataset. To do this, open Power Query Editor in Report2, select each query, and then update the data source to use Dataset1.\nD. Update all the parameter values: If Report2 contains any parameters, you need to update their values to match the parameters in Report1 that connect to Dataset1. To do this, go to the \"Manage Parameters\" dialog box in Report2 and update the parameter values to match those in Report1."
      },
      {
        "date": "2023-03-22T23:04:00.000Z",
        "voteCount": 2,
        "content": "A, B, and E are incorrect:\n\nA. Removing data source permissions will not help prepare Report2 to use Dataset1.\n\nB. Deleting all the Power Query Editor objects would also not be useful as you would need to recreate the objects anyway.\n\nE. Deleting all calculated tables is not necessary. Instead, you should update the calculated tables' source to point to Dataset1, just like you did with the queries."
      },
      {
        "date": "2023-03-22T23:04:00.000Z",
        "voteCount": 1,
        "content": "BTW: Answers given by CHAT GPT"
      },
      {
        "date": "2023-09-07T01:20:00.000Z",
        "voteCount": 1,
        "content": "Agree with A\n\nDisagree with B. \nShared dataset there is no object in power query it is a live connection to published dataset and question says you need to prepare Report2 to use shared Dataset1 so you need to delete all the power query editor objects\n\nDisagree with C\nWhen you are building a report of shared Dataset you can not use calculated tables (6:35 of video https://www.youtube.com/watch?v=ENZm_7jWU4Q) so they would need to be deleted."
      },
      {
        "date": "2023-03-07T02:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is CORRECT (C&amp; D)\nYou need to prepare Report2 to use Dataset1. It never says you need to make Report 1 to be the same as Report1. You do not need to delete the calculated tables or parameters, you just need to update the values."
      },
      {
        "date": "2023-01-25T08:13:00.000Z",
        "voteCount": 1,
        "content": "I would go for B and E based on this blog:\nhttps://technologyblog.rsmus.com/data-analytics/power-bi-datasets-changing-your-data-source/"
      },
      {
        "date": "2022-12-21T07:22:00.000Z",
        "voteCount": 1,
        "content": "I choose AC"
      },
      {
        "date": "2022-12-15T17:19:00.000Z",
        "voteCount": 4,
        "content": "BE,\nIf you connect to a Power BI dataset, you cannot connect other sources at the same time. To connect a Power BI dataset, you must first remove all existing connections in Power Query and Calculated Table &amp; Measure"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93285-exam-dp-500-topic-1-question-25-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse notebook.<br>You need to create the visual shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image21.png\"><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image22.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image23.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-02T14:38:00.000Z",
        "voteCount": 9,
        "content": "I ran the code locally. The answer is correct."
      },
      {
        "date": "2023-10-27T00:34:00.000Z",
        "voteCount": 2,
        "content": "Tried the difference between plotting fill and fill_between.\nfill_between will get the plot as shown in the question, while using fill will get a plot with y axis starts from 2.\nSo, the answer is fill_between and suptitle."
      },
      {
        "date": "2023-08-17T22:50:00.000Z",
        "voteCount": 1,
        "content": "plt.fill_between(x,y)\nplt.suptitle('chart 1', fontweight = 'bold')\nThe answer is correct. I have tested in Python."
      },
      {
        "date": "2023-06-25T11:07:00.000Z",
        "voteCount": 2,
        "content": "https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.fill.html\nNotes\nUse fill_between() if you would like to fill the region between two curves.\n\nAnswer is fill not fill_between"
      },
      {
        "date": "2023-03-25T05:40:00.000Z",
        "voteCount": 3,
        "content": "is ok .fill_between &amp; suptitle"
      },
      {
        "date": "2023-01-10T04:04:00.000Z",
        "voteCount": 4,
        "content": "Correct\nimport matplotlib.pyplot as plt\nx = [0,1,2,3,4,5]\ny = [1.5,3,5.3,6,10,2]\nplt.plot(x,y,'-o', color = 'red')\nplt.fill_between(x,y)\nplt.suptitle('chart 1', fontweight = 'bold')\nplt.show()"
      },
      {
        "date": "2022-12-30T09:11:00.000Z",
        "voteCount": 2,
        "content": "Please confirm the correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90900-exam-dp-500-topic-1-question-26-discussion/",
    "body": "You use an Apache Spark notebook in Azure Synapse Analytics to filter and transform data.<br>You need to review statistics for a DataFrame that includes:<br><br>The column name -<br><br>The column type -<br><br>The number of distinct values -<br>Whether the column has missing values<br>Which function should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdisplayHTML()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdisplay(df, summary=true)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%configure",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdisplay(df)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%lsmagic"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T02:04:00.000Z",
        "voteCount": 7,
        "content": "Correct answer"
      },
      {
        "date": "2022-12-24T06:35:00.000Z",
        "voteCount": 5,
        "content": "True, B is correct\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-data-visualization"
      },
      {
        "date": "2023-11-16T22:40:00.000Z",
        "voteCount": 3,
        "content": "Today I took this exam and there was a question with some similarities to this one. The question was something like this:\nWhat information does this function give us: display(df, summary=true)?\nThere were 6 choices and we had to choose three, these are the correct answers:\nType, Missing &amp;  Unique\nThe only other thing that this function gives us is the column's name.\nhttps://learn.microsoft.com/en-us/fabric/data-engineering/notebook-visualization#displaydf-summary-view"
      },
      {
        "date": "2023-08-17T22:52:00.000Z",
        "voteCount": 1,
        "content": "Correct answer.\nYou can use display(df, summary = true) to check the statistics summary of a given Apache Spark DataFrame that include the column name, column type, unique values, and missing values for each column. You can also select on specific column to see its minimum value, maximum value, mean value and standard deviation."
      },
      {
        "date": "2023-05-25T02:18:00.000Z",
        "voteCount": 1,
        "content": "i retract my previous answer sns.pairplot(iris, hue=\"species\")"
      },
      {
        "date": "2023-04-16T13:20:00.000Z",
        "voteCount": 2,
        "content": "B Correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91290-exam-dp-500-topic-1-question-27-discussion/",
    "body": "HOTSPOT -<br>You are using an Azure Synapse notebook to create a Python visual.<br>You run the following code cell to import a dataset named Iris.<br><img src=\"https://img.examtopics.com/dp-500/image24.png\"><br>A sample of the data is shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image25.png\"><br>You need to create the visual shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"https://img.examtopics.com/dp-500/image26.png\"><br>How should you complete the Python code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image27.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image28.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-12T14:03:00.000Z",
        "voteCount": 27,
        "content": "pairplot and species"
      },
      {
        "date": "2023-05-25T02:15:00.000Z",
        "voteCount": 1,
        "content": "There are 2 species in the table so if we use hue = species then we should get 2 different colors , to get 3 different colors we have to use hue=sepal_width  for the 3 different colors"
      },
      {
        "date": "2023-06-18T18:13:00.000Z",
        "voteCount": 4,
        "content": "Look at the visual... there are 3 species listed on the right"
      },
      {
        "date": "2022-12-23T03:24:00.000Z",
        "voteCount": 10,
        "content": "Pairplot\nSpecies\n\nReference: https://seaborn.pydata.org/generated/seaborn.pairplot.html"
      },
      {
        "date": "2023-12-28T00:55:00.000Z",
        "voteCount": 1,
        "content": "pair_plot and species"
      },
      {
        "date": "2023-08-18T23:07:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- pairplot\n- species"
      },
      {
        "date": "2023-06-26T03:48:00.000Z",
        "voteCount": 1,
        "content": "Pairplot\nSpecies"
      },
      {
        "date": "2023-05-31T11:07:00.000Z",
        "voteCount": 1,
        "content": "Species not width"
      },
      {
        "date": "2023-03-26T22:01:00.000Z",
        "voteCount": 3,
        "content": "Pairplot and Species\nhttps://seaborn.pydata.org/generated/seaborn.pairplot.html"
      },
      {
        "date": "2023-03-25T06:35:00.000Z",
        "voteCount": 1,
        "content": "yes I think is pairplot and species"
      },
      {
        "date": "2023-02-17T03:00:00.000Z",
        "voteCount": 2,
        "content": "pairplot and species"
      },
      {
        "date": "2023-01-25T08:22:00.000Z",
        "voteCount": 5,
        "content": "pairplot and species.\nSpecies because 'hue' specifies to split on color per species."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90901-exam-dp-500-topic-1-question-28-discussion/",
    "body": "You are creating a Power BI single-page report.<br>Some users will navigate the report by using a keyboard, and some users will navigate the report by using a screen reader.<br>You need to ensure that the users can consume content on a report page in a logical order.<br>What should you configure on the report page?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe X position",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe bookmark order",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe layer order",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe tab order\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T02:13:00.000Z",
        "voteCount": 10,
        "content": "correct answer"
      },
      {
        "date": "2024-03-25T20:09:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-08-18T23:07:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- the tab order"
      },
      {
        "date": "2023-06-26T03:48:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2023-06-06T22:34:00.000Z",
        "voteCount": 3,
        "content": "D is correct \nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports#tab-order"
      },
      {
        "date": "2023-04-16T13:23:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90902-exam-dp-500-topic-1-question-29-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse workspace named Workspace1.<br>You need to use PySpark in a notebook to read data from a SQL pool as an Apache Spark DataFrame and display the top five rows.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image30.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image31.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-24T07:50:00.000Z",
        "voteCount": 22,
        "content": "spark.read.sqlanalytics() and spark_read.show().\nYou can see it from the following example notebook https://github.com/Azure-Samples/Synapse/blob/main/Notebooks/Scala/03%20Read%20and%20write%20from%20SQL%20pool%20table.ipynb"
      },
      {
        "date": "2022-12-10T02:17:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is .load and .show"
      },
      {
        "date": "2022-12-16T01:35:00.000Z",
        "voteCount": 2,
        "content": "honest question: why spark.read.sqlanalytics is wrong?"
      },
      {
        "date": "2022-12-25T06:25:00.000Z",
        "voteCount": 1,
        "content": "the question pointing to more of a general thing, they did not specify synapse analytics...? thus use the general function"
      },
      {
        "date": "2023-01-04T01:30:00.000Z",
        "voteCount": 2,
        "content": "But it says: You have an \"Azure Synapse\" workspace named Workspace1.\nYou need to use PySpark in a notebook to read data from a \"SQL pool\" as an Apache Spark DataFrame and display the top five rows. So, .sqlanalytics should work. Thoughts?"
      },
      {
        "date": "2023-01-06T00:52:00.000Z",
        "voteCount": 3,
        "content": "considering the link KWaleed provided, my thought is that the correct answer is definitely spark.read.sqlanalytics() and spark_read.show(). The fact that the question text emphasizes the Azure Synapse Workspace removes any doubts in my opinion."
      },
      {
        "date": "2023-05-02T22:25:00.000Z",
        "voteCount": 1,
        "content": "indeed the code snipped is loading from managed table. so, no load but sqlanalytics"
      },
      {
        "date": "2023-05-02T22:24:00.000Z",
        "voteCount": 2,
        "content": "load is wrong. its for reading directly from storage account."
      },
      {
        "date": "2023-09-15T17:23:00.000Z",
        "voteCount": 1,
        "content": "read.sqlanalytics - to read dataframe from SQL\nread.load - to read dataframe from files such as parquet"
      },
      {
        "date": "2023-08-17T22:59:00.000Z",
        "voteCount": 1,
        "content": "// Read  the table we just created in the sql pool as a Spark dataframe\nval spark_read = spark.read.\n    sqlanalytics(s\"$sql_pool_name.dbo.PublicHoliday\")\nspark_read.show(5, truncate = false)"
      },
      {
        "date": "2023-03-25T11:26:00.000Z",
        "voteCount": 1,
        "content": "is load and show, I understand sqlanalytics is a complement wiht tools and constants"
      },
      {
        "date": "2022-12-22T03:41:00.000Z",
        "voteCount": 2,
        "content": "%%pyspark\ndf = spark.read.load\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-spark"
      },
      {
        "date": "2022-12-15T17:48:00.000Z",
        "voteCount": 3,
        "content": ".sqlanalytics or synapsesql\n.show"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91105-exam-dp-500-topic-1-question-30-discussion/",
    "body": "You have a Power BI report that contains the table shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image33.png\"><br>The table contains conditional formatting that shows which stores are above, near, or below the monthly quota for returns.<br>You need to ensure that the table is accessible to consumers of reports who have color vision deficiency.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd alt text to explain the information that each color conveys.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the icons to use a different shape for each color.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the conditional formatting icons to a tooltip report.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the icons and use red, yellow, and green background colors instead."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T19:34:00.000Z",
        "voteCount": 17,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-24T07:10:00.000Z",
        "voteCount": 6,
        "content": "I think A is correct.\nRef = https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports"
      },
      {
        "date": "2022-12-25T06:27:00.000Z",
        "voteCount": 2,
        "content": "Correct, option B is too generic and can be confusing, everyone can have diff perspective of what the shape supposed to me"
      },
      {
        "date": "2024-05-28T03:33:00.000Z",
        "voteCount": 1,
        "content": "I will go for A, because B: While changing icons to different shapes can help differentiate between categories, it doesn't specifically address the accessibility needs of users with color vision deficiency."
      },
      {
        "date": "2024-02-07T06:59:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-12-28T01:08:00.000Z",
        "voteCount": 1,
        "content": "A is meaningless -- if you have color deficiency and you apply solution on A; then user need to check each row if it is below or up the target -- b option would be nicer and easy to check"
      },
      {
        "date": "2023-09-19T03:40:00.000Z",
        "voteCount": 2,
        "content": "i would go for B, since A is only applicable to screen readers. it's not mentioned that the customers are only screen reader users ... also in the ms page: use text or icons ... my ideal answer would be A &amp; B but solo I would take B"
      },
      {
        "date": "2023-08-08T02:28:00.000Z",
        "voteCount": 2,
        "content": "If you change the icon shape to a unique shape for each color, you still need to explain what does that shape mean. This means that you still need the alt text.\nSo, Alt text would be a better option to choose."
      },
      {
        "date": "2023-07-31T08:45:00.000Z",
        "voteCount": 2,
        "content": "A and B seem right in combination. However B won't work without A - therefor A"
      },
      {
        "date": "2023-07-04T01:06:00.000Z",
        "voteCount": 3,
        "content": "The answer is A; it's on the accessibility Doc no brainer"
      },
      {
        "date": "2023-07-04T01:05:00.000Z",
        "voteCount": 3,
        "content": "The answer is A; it's on the accessibility Doc no brainer"
      },
      {
        "date": "2023-06-29T13:34:00.000Z",
        "voteCount": 2,
        "content": "Adding alt text provides an alternative textual description for the content that is conveyed through colors. By describing the meaning of each color used in the conditional formatting, users with color vision deficiency can understand the information presented in the table.\n\nAlt text can be added to the table or the individual cells to describe the significance of colors, such as \"above quota,\" \"near quota,\" or \"below quota.\"\n\nTherefore, the correct answer is A. Add alt text to explain the information that each color conveys."
      },
      {
        "date": "2023-12-05T05:31:00.000Z",
        "voteCount": 2,
        "content": "A says \"explain the information that EACH COLOR conveys\" means that the person should be able to see the color first !!"
      },
      {
        "date": "2023-06-26T03:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer."
      },
      {
        "date": "2023-06-25T11:30:00.000Z",
        "voteCount": 1,
        "content": "B it should be"
      },
      {
        "date": "2023-06-18T22:43:00.000Z",
        "voteCount": 2,
        "content": "It's A as they did focus in their models that Alt text is the way of helping color blind users"
      },
      {
        "date": "2023-05-18T09:31:00.000Z",
        "voteCount": 1,
        "content": "So if we change the shape we will still have the same colour! A is better by far"
      },
      {
        "date": "2023-05-02T23:04:00.000Z",
        "voteCount": 1,
        "content": "This is very tricky. Except D, all are possible solutions.\nIf User is using screen reader then Alt text is the solution\nHowever, if we change the icon set such as Up/Down/Neutral icons then same message can be easily conveyed.\nA tool tip also can be useful but it needs user to move hover.\nLooking at all options B and A should be combined so it can be used by as many people as possible including complete blind persons. But if I have to choose only one then I will choose B"
      },
      {
        "date": "2023-04-25T17:29:00.000Z",
        "voteCount": 2,
        "content": "Changing shapes alone would not be enough, you would also need to describe what the shapes mean, while colors are self-descriptive). I am thus leaning towards A as B is not a complete answer"
      },
      {
        "date": "2023-05-02T22:49:00.000Z",
        "voteCount": 1,
        "content": "shape with color the option says so its correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91417-exam-dp-500-topic-1-question-31-discussion/",
    "body": "You have a Power BI dataset that has only the necessary fields visible for report development.<br>You need to ensure that end users see only 25 specific fields that they can use to personalize visuals.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tabular Editor, create a new role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure object-level security (OLS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tabular Editor, create a new perspective.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHide all the fields in the dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T06:05:00.000Z",
        "voteCount": 9,
        "content": "Correct,\nCheck this link: https://data-marc.com/2020/08/18/power-bi-visual-customization-using-perspectives"
      },
      {
        "date": "2022-12-25T06:33:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2024-02-07T07:03:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. Perspectives is wrong answer"
      },
      {
        "date": "2023-06-26T03:50:00.000Z",
        "voteCount": 1,
        "content": "C appears to be the logical answer."
      },
      {
        "date": "2023-01-11T03:00:00.000Z",
        "voteCount": 3,
        "content": "https://byobi.wordpress.com/2017/08/09/object-level-security-for-analysis-services-tabular/#:~:text=What%20is%20Object%2DLevel%20Security,hide%20tables%2Fcolumns%20from%20users."
      },
      {
        "date": "2022-12-20T01:38:00.000Z",
        "voteCount": 1,
        "content": "should the answer not be OLS? because even if you allow personalization, users still would be able to use all fields not just 25, no?"
      },
      {
        "date": "2023-01-06T00:59:00.000Z",
        "voteCount": 2,
        "content": "yes, but that is exactly why you use perspectives --&gt; to limit the fields that can be used. nbagchi's link explains it very well"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91419-exam-dp-500-topic-1-question-32-discussion/",
    "body": "HOTSPOT -<br>You are using Azure Synapse Studio to explore a dataset that contains data about taxi trips.<br>You need to create a chart that will show the total trip distance according to the number of passengers as shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image34.png\"><br>How should you configure the chart? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image35.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image36.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T06:08:00.000Z",
        "voteCount": 7,
        "content": "Correct"
      },
      {
        "date": "2023-08-18T23:08:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- PassengerCount\n- SumTripDistance"
      },
      {
        "date": "2023-06-26T03:51:00.000Z",
        "voteCount": 1,
        "content": "PassengerCount\nSumTripMiles"
      },
      {
        "date": "2023-06-26T03:51:00.000Z",
        "voteCount": 1,
        "content": "*SumTripDistance"
      },
      {
        "date": "2023-05-09T22:11:00.000Z",
        "voteCount": 1,
        "content": "Correct, correct"
      },
      {
        "date": "2023-01-25T14:58:00.000Z",
        "voteCount": 2,
        "content": "Correct. I don't think an average trip distance can be over 150k :)"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90942-exam-dp-500-topic-1-question-33-discussion/",
    "body": "You have a Power BI report that contains the visual shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image37.png\"><br>You need to make the visual more accessible to users who have color vision deficiency.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd additional measures to the table values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the red background color to orange.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd icons to represent the sales status of each product.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the font color of values in the Sales column to white."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T06:11:00.000Z",
        "voteCount": 19,
        "content": "Answer will be C\nFor color-blind people, the green and red combo is bad, even if you change red to orange, it won't matter much. The best option here is C. As for D, changing the font color to white has nothing to do with color blindness"
      },
      {
        "date": "2022-12-24T07:25:00.000Z",
        "voteCount": 7,
        "content": "I will go for C. \nSee the Checklist section for All Visuals at\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports"
      },
      {
        "date": "2024-02-19T23:40:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C\nAvoid using color as the only means of conveying information. Use text or icons to supplement or replace the color.\n\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports"
      },
      {
        "date": "2023-10-02T21:45:00.000Z",
        "voteCount": 1,
        "content": "Now that I think about it, \n\nmake the visual more accessible to viewers with color deficiency.\nblack and green as mentioned do not go well together (https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports) \nI would go for D"
      },
      {
        "date": "2023-09-29T18:09:00.000Z",
        "voteCount": 1,
        "content": "I will go for C"
      },
      {
        "date": "2023-09-19T03:43:00.000Z",
        "voteCount": 1,
        "content": "same issue as before, make more accessible &gt; icons or text go for C"
      },
      {
        "date": "2023-10-02T21:45:00.000Z",
        "voteCount": 1,
        "content": "sorry would go for D"
      },
      {
        "date": "2023-09-17T10:50:00.000Z",
        "voteCount": 1,
        "content": "D \nPorque torna o texto mais leg\u00edvel para pessoas com defici\u00eancia visual de n\u00facleos, especialmente se o cor de fundo \u00e9 vermelho, que pode tornar dif\u00edcil a leitura do texto caso o cor do texto seja muito pr\u00f3ximo ao cor de fundo. Alterando o cor da fonte para branco, voc\u00ea aumenta o contraste entre o texto e o fundo, o que facilita a leitura para todos os usu\u00e1rios, independentemente da capacidade de distinguir n\u00facleos."
      },
      {
        "date": "2023-06-30T20:26:00.000Z",
        "voteCount": 1,
        "content": "To make the visual more accessible to users with color vision deficiency, you should choose option C: Add icons to represent the sales status of each product.\n\nBy adding icons, you provide an additional visual cue that can be easily distinguishable by users with color vision deficiency. This helps convey information without relying solely on color differentiation, which may be challenging for some individuals.\n\nUsing icons to represent different sales statuses allows users to quickly identify and understand the sales performance of each product, regardless of their color vision capabilities. This enhances the accessibility of the visual and ensures that users with color vision deficiency can effectively interpret the information presented."
      },
      {
        "date": "2023-06-26T03:52:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2023-06-02T01:18:00.000Z",
        "voteCount": 1,
        "content": "Avoid Reliance on Color Alone: Avoid conveying important information solely through color. Include additional visual cues such as labels, patterns, or shapes to provide context and differentiation for users who may have difficulty perceiving color differences."
      },
      {
        "date": "2023-05-30T07:05:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2023-05-22T03:07:00.000Z",
        "voteCount": 1,
        "content": "C shape would be visible, color is subjective"
      },
      {
        "date": "2023-04-16T13:48:00.000Z",
        "voteCount": 1,
        "content": "D  d  d  d"
      },
      {
        "date": "2023-05-02T06:13:00.000Z",
        "voteCount": 1,
        "content": "Serious\uff1f"
      },
      {
        "date": "2023-01-11T03:19:00.000Z",
        "voteCount": 3,
        "content": "Definitely it cant be D!"
      },
      {
        "date": "2023-01-04T23:16:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-12-16T08:02:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2022-12-16T01:39:00.000Z",
        "voteCount": 4,
        "content": "im going for C as well."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90909-exam-dp-500-topic-1-question-34-discussion/",
    "body": "You plan to generate a line chart to visualize and compare the last six months of sales data for two departments.<br>You need to increase the accessibility of the visual.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a unique marker for each series.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace long text with abbreviations and acronyms.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove important information to a tooltip.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a distinct color for each series."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-24T07:29:00.000Z",
        "voteCount": 10,
        "content": "I will go for A. See Markers at\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports"
      },
      {
        "date": "2022-12-13T06:15:00.000Z",
        "voteCount": 8,
        "content": "Both A and D are good options. However if you think carefully, even with distinct colors, it is possible to still not be accessible for color blind people. The safest option to make it accessible is to use different markers to make a clear distinction"
      },
      {
        "date": "2023-09-19T03:44:00.000Z",
        "voteCount": 1,
        "content": "A avoid depending on colors to make something clear, so different colors of the lines is to be avoided to make your CASE &gt;&gt;&gt; icons or text &gt;&gt; option A"
      },
      {
        "date": "2023-08-18T23:09:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Configure a unique marker for each series"
      },
      {
        "date": "2023-07-31T09:57:00.000Z",
        "voteCount": 1,
        "content": "There is not noted that markers are visible in the chart - so safest option to take is A"
      },
      {
        "date": "2023-07-31T03:31:00.000Z",
        "voteCount": 1,
        "content": "Tooltips are an important accessibility feature in Power BI that allows users to access additional information associated with data points on visualizations. By moving important information to a tooltip, users with various abilities, including those using screen readers or navigating the report by keyboard, can easily access and understand the data without cluttering the main visualization.\nBy placing relevant information in tooltips, users can hover or click on data points to view details without overwhelming the visual with too much information. This approach ensures that critical data is readily available to all users and enhances the accessibility of the report."
      },
      {
        "date": "2023-05-30T07:06:00.000Z",
        "voteCount": 2,
        "content": "A is correct answer."
      },
      {
        "date": "2023-01-13T02:41:00.000Z",
        "voteCount": 5,
        "content": "A is the right answer"
      },
      {
        "date": "2022-12-21T09:45:00.000Z",
        "voteCount": 5,
        "content": "Agreed, safest option is A"
      },
      {
        "date": "2022-12-10T03:56:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90910-exam-dp-500-topic-1-question-35-discussion/",
    "body": "You have the following Python code in an Apache Spark notebook.<br><img src=\"https://img.examtopics.com/dp-500/image38.png\"><br>Which type of chart will the code produce?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta stacked bar chart",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan area chart\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta bar chart",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta pie chart"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-04T23:20:00.000Z",
        "voteCount": 9,
        "content": "Fill_between is the key.\nThe fill_between() function\n\nhttps://www.python-graph-gallery.com/area-plot/"
      },
      {
        "date": "2023-09-29T18:11:00.000Z",
        "voteCount": 1,
        "content": "fill_between = area chart"
      },
      {
        "date": "2023-06-26T03:54:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer."
      },
      {
        "date": "2023-03-25T16:39:00.000Z",
        "voteCount": 2,
        "content": "b is correct answer"
      },
      {
        "date": "2022-12-10T03:58:00.000Z",
        "voteCount": 4,
        "content": "correct answer"
      },
      {
        "date": "2022-12-25T06:51:00.000Z",
        "voteCount": 1,
        "content": "specifically line chart. tested"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90992-exam-dp-500-topic-1-question-36-discussion/",
    "body": "You are creating a Python visual in Power BI Desktop.<br>You need to retrieve the value of a column named Unit Price from a DataFrame.<br>How should you reference the Unit Price column in the Python code?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdataset[\u2018Unit Price\u2019]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t(\u2018Unit Price\u2019)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpandas.DataFrame(\u2018Unit Price\u2019)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata = [Unit Price]"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T01:58:00.000Z",
        "voteCount": 15,
        "content": "I think answer should be A"
      },
      {
        "date": "2023-01-15T20:03:00.000Z",
        "voteCount": 8,
        "content": "it should be A.\n\ndataset[\u2018Unit Price\u2019], =&gt;  (return a column)\npandas.DataFrame(\u2018Unit Price\u2019), =&gt; (show the entire table)"
      },
      {
        "date": "2024-04-02T03:55:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-python-visuals"
      },
      {
        "date": "2023-09-29T18:12:00.000Z",
        "voteCount": 1,
        "content": "definitely a is the answer"
      },
      {
        "date": "2023-06-30T20:33:00.000Z",
        "voteCount": 1,
        "content": "dataset[\u2018Unit Price\u2019] is correct answer"
      },
      {
        "date": "2023-06-29T21:16:00.000Z",
        "voteCount": 1,
        "content": "This syntax accesses the Unit Price column by specifying the column name within square brackets after the dataset object. Using dataset['Unit Price'] retrieves the values from the specified column for further processing or visualization in the Python visual."
      },
      {
        "date": "2023-06-26T03:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2023-06-25T12:09:00.000Z",
        "voteCount": 2,
        "content": "Should be A"
      },
      {
        "date": "2023-06-22T01:11:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer, been developing python scripts for years now and i only used dataset['column name']"
      },
      {
        "date": "2023-06-20T04:51:00.000Z",
        "voteCount": 2,
        "content": "From ChatGPT:\nThe correct answer is A. dataset[\u2018Unit Price\u2019]. This is the correct way to reference the Unit Price column in the Python code."
      },
      {
        "date": "2023-05-02T23:57:00.000Z",
        "voteCount": 3,
        "content": "C is WRONG!"
      },
      {
        "date": "2023-03-25T16:46:00.000Z",
        "voteCount": 4,
        "content": "Is correct A\nhttps://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html"
      },
      {
        "date": "2023-02-19T02:34:00.000Z",
        "voteCount": 3,
        "content": "A is correct since it's how you select a column in pandas dataframe.\nhttps://pandas.pydata.org/docs/getting_started/intro_tutorials/03_subset_data.html\nC is to initialize a new dataframe. In this case it will initialize a dataframe with each character as a column value (string is an iterable in Python)\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
      },
      {
        "date": "2023-02-13T12:42:00.000Z",
        "voteCount": 3,
        "content": "A. Sure"
      },
      {
        "date": "2023-02-16T04:08:00.000Z",
        "voteCount": 1,
        "content": "do you have any example or reference?"
      },
      {
        "date": "2023-01-24T04:55:00.000Z",
        "voteCount": 3,
        "content": "Question is so bad but the forst param for a dataframe is data and \n a string cannot be dataset column just an index So for me A\nhttps://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html"
      },
      {
        "date": "2023-01-10T05:26:00.000Z",
        "voteCount": 2,
        "content": "I think A, because:\nYou can access columns in the dataset by using their names. For example, you can code dataset[\"Age\"] in your Python script to access the age field."
      },
      {
        "date": "2023-01-09T17:12:00.000Z",
        "voteCount": 3,
        "content": "I think A works"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91431-exam-dp-500-topic-1-question-37-discussion/",
    "body": "You open a Power BI Desktop report that contains an imported data model and a single report page.<br>You open Performance analyzer, start recording, and refresh the visuals on the page. The recording produces the results shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image39.png\"><br>What can you identify from the results?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen all the visuals refresh simultaneously, the visuals spend most of the time waiting on other processes to finish.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Actual/Forecast Billable Hrs YTD visual displays the most data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnoptimized DAX queries cause the page to load slowly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Actual/Forecast Hours by Type visual takes a long time to render on the report page when the data is cross-filtered."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T06:40:00.000Z",
        "voteCount": 13,
        "content": "Correct. The high value in 'other' generally represents that there are too many visuals on the page."
      },
      {
        "date": "2023-01-04T23:25:00.000Z",
        "voteCount": 5,
        "content": "Other: this is the time spent waiting for other operations to complete. This is typically synchronization time between different visuals in the same page and should not be considered as a bottleneck of the visual being analyzed. It is usually waiting time caused by other pending operations performed by other visuals.\n\nhttps://community.powerbi.com/t5/Desktop/Performance-Analyzer-quot-Other-quot/m-p/717848#:~:text=Other%3A%20this%20is%20the%20time,operations%20performed%20by%20other%20visuals."
      },
      {
        "date": "2023-08-18T23:09:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- When all the visuals refresh simultaneously, the visuals spend most of the time waiting on other processes to finish."
      },
      {
        "date": "2023-05-30T23:52:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92654-exam-dp-500-topic-1-question-38-discussion/",
    "body": "HOTSPOT -<br>You use Vertipaq Analyzer to analyze a model.<br>The Relationships tab contains the results shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image40.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image41.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image42.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-24T08:59:00.000Z",
        "voteCount": 13,
        "content": "I think the answer for 2nd drop down should be 1,804+6,577 = 8,381\n\nThere is a very good video explaining the concept on https://www.youtube.com/watch?v=XvZyo9Kgyio&amp;list=PLU6II7MW-aiIREc1XdCYviQEj4Ui2pNDS&amp;index=3"
      },
      {
        "date": "2022-12-25T07:02:00.000Z",
        "voteCount": 3,
        "content": "Correct , we need to add them"
      },
      {
        "date": "2022-12-25T07:05:00.000Z",
        "voteCount": 1,
        "content": "**1804"
      },
      {
        "date": "2023-12-28T01:43:00.000Z",
        "voteCount": 1,
        "content": "Customer and 8381"
      },
      {
        "date": "2023-08-18T23:10:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Customer\n- 8381"
      },
      {
        "date": "2023-06-26T03:57:00.000Z",
        "voteCount": 1,
        "content": "Customer\n8381"
      },
      {
        "date": "2023-06-25T12:20:00.000Z",
        "voteCount": 3,
        "content": "Customer &amp; 22 is correct, only missing values due to dimensional relationship should be considered"
      },
      {
        "date": "2023-03-26T22:41:00.000Z",
        "voteCount": 1,
        "content": "Customer and 8381 as a result of wrong relationships (1804 + 6577)"
      },
      {
        "date": "2023-03-25T00:45:00.000Z",
        "voteCount": 1,
        "content": "still stand for Customer and 22\nAlthough invalid value will turn to blank, it is not from missing dem.. and if so, it should count 1804+6577+22 = 8,403"
      },
      {
        "date": "2023-03-25T00:55:00.000Z",
        "voteCount": 1,
        "content": "change my mind as missing key &lt;&gt; missing data, then should 8,381"
      },
      {
        "date": "2023-01-10T05:48:00.000Z",
        "voteCount": 2,
        "content": "Customer and 1,804+6,577 = 8,381"
      },
      {
        "date": "2023-01-06T01:15:00.000Z",
        "voteCount": 2,
        "content": "Customer and 8381"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91435-exam-dp-500-topic-1-question-39-discussion/",
    "body": "You use the Vertipaq Analyzer to analyze tables in a dataset as shown in the Tables exhibit. (Click the Tables tab.)<br><img src=\"https://img.examtopics.com/dp-500/image43.png\"><br>The table relationships for the dataset are shown in the Relationships exhibit. (Click the Relationships tab.)<br><img src=\"https://img.examtopics.com/dp-500/image44.png\"><br>You need to reduce the model size by eliminating invalid relationships.<br>Which column should you remove?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSales[Sales Amount]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSales[Row ID]\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSales[Sales ID]",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlan[Row ID]"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T06:56:00.000Z",
        "voteCount": 8,
        "content": "Correct, Row ID has 858,786 missing keys"
      },
      {
        "date": "2023-12-20T10:37:00.000Z",
        "voteCount": 1,
        "content": "agree with ivanb94"
      },
      {
        "date": "2023-05-31T00:03:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer - 858,756 missing keys!"
      },
      {
        "date": "2023-03-25T17:29:00.000Z",
        "voteCount": 1,
        "content": "B is the only one listed in  2\u00b0 table"
      },
      {
        "date": "2023-01-06T01:46:00.000Z",
        "voteCount": 3,
        "content": "To be precise RowID from the Sales table, so answer B, not the other provided potential answer with Plan table. P.S. Plan table seems to be made up considering that it is not mentioned in either of the pictures --&gt; only Plan Header Details is. Another Microsoft confusion device -.-"
      },
      {
        "date": "2022-12-31T13:04:00.000Z",
        "voteCount": 2,
        "content": "858,786"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90912-exam-dp-500-topic-1-question-40-discussion/",
    "body": "You are optimizing a Power BI data model by using DAX Studio.<br>You need to capture the query events generated by a Power BI Desktop report.<br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe DMV list",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Server Timings trace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan All Queries trace\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Query Plan trace"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T01:46:00.000Z",
        "voteCount": 6,
        "content": "yep, I would go for C. https://www.sqlbi.com/articles/capturing-power-bi-queries-using-dax-studio/"
      },
      {
        "date": "2023-06-26T03:58:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2022-12-25T07:08:00.000Z",
        "voteCount": 3,
        "content": "C = Correct"
      },
      {
        "date": "2023-01-13T00:39:00.000Z",
        "voteCount": 2,
        "content": "yup, the first sentence says it all: https://daxstudio.org/docs/features/traces/all-queries-trace/"
      },
      {
        "date": "2022-12-13T07:05:00.000Z",
        "voteCount": 4,
        "content": "I believe C is correct\nCheck out this link: https://www.sqlbi.com/articles/capturing-power-bi-queries-using-dax-studio/"
      },
      {
        "date": "2022-12-10T04:04:00.000Z",
        "voteCount": 1,
        "content": "incorrect answer\nTo capture the query events generated by a Power BI Desktop report, you can use a Server Timings trace in DAX Studio. A Server Timings trace is a diagnostic tool that records the time taken by each query event in the Power BI Desktop report, allowing you to identify any bottlenecks and optimize the data model accordingly."
      },
      {
        "date": "2023-01-06T01:51:00.000Z",
        "voteCount": 3,
        "content": "I don't see why we would use Server Timings trace considering the main purpose of that is to check the time needed for a query (as the name suggests), while the question says nothing about the time element and only gives the requirement to capture the query events. That you can definitely do with C alone."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90946-exam-dp-500-topic-1-question-41-discussion/",
    "body": "You have a sales report as shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image45.png\"><br>The sales report has the following characteristics:<br>The measures are optimized.<br>The dataset uses import storage mode.<br>Data points, hierarchies, and fields cannot be removed or filtered from the report page.<br>From powerbi.com, users experience slow load times when viewing the report.<br>You need to reduce how long it takes for the report to load without affecting the data displayed in the report.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the report theme to monochromatic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the single-value cards with a multi-row card.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the product category charts with a bar chart for sales and a hierarchy of Category and Sub Category on the axis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace all the filters on the Filters pane with visual slicers on the report page."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-06T01:56:00.000Z",
        "voteCount": 11,
        "content": "I would go with BC too bc both actions are actually just reducing the number of visuals in the report by providing the same amount of information with a single chart instead of multiple. Generally it is a best practice to decrease the number of visuals to improve performance e.g. speed up rendering times so the answer is actually straight forward but Microsoft had to make it sound complicated"
      },
      {
        "date": "2022-12-13T07:16:00.000Z",
        "voteCount": 10,
        "content": "Correct answer"
      },
      {
        "date": "2023-06-26T03:59:00.000Z",
        "voteCount": 1,
        "content": "B and C are correct answers."
      },
      {
        "date": "2023-01-09T17:24:00.000Z",
        "voteCount": 2,
        "content": "great explanation by ivanb94"
      },
      {
        "date": "2022-12-10T11:27:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C,D"
      },
      {
        "date": "2022-12-10T11:29:00.000Z",
        "voteCount": 5,
        "content": "Forget it, Filter pane actually helps improve the performance of report so it should be C,B"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90915-exam-dp-500-topic-1-question-42-discussion/",
    "body": "You are building a Power BI dataset that will use two data sources.<br>The dataset has a query that uses a web data source. The web data source uses anonymous authentication.<br>You need to ensure that the query can be used by all the other queries in the dataset.<br>Which privacy level should you select for the data source?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganizational",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublic\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPrivate"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T07:23:00.000Z",
        "voteCount": 11,
        "content": "Answer is correct.\nIf the report has only one data source, there is no need to change this privacy level (None is the default) because there is no risk of exposing data between various sources. However, as soon as a second data source is created in the report, Power BI asks for the privacy level to be determined for both data sources because it cannot be kept to None now. So choosing it to Public is the correct option."
      },
      {
        "date": "2022-12-28T14:14:00.000Z",
        "voteCount": 5,
        "content": "Check this out Ref: https://learn.microsoft.com/en-us/power-bi/enterprise/desktop-privacy-levels"
      },
      {
        "date": "2023-07-31T07:56:00.000Z",
        "voteCount": 1,
        "content": "Option C: Public is not the correct privacy level to select in this scenario. Choosing the \"Public\" privacy level would allow the data source query to be shared with other users, making the data accessible to them. However, using the \"Public\" privacy level for a web data source with anonymous authentication can potentially expose sensitive information to unauthorized users.\nIn this case, selecting the \"Private\" privacy level is the appropriate choice because it allows the data source query to access the web data source with anonymous authentication without any privacy restrictions, ensuring that other queries in the dataset can use the data returned by this web data source query."
      },
      {
        "date": "2023-05-31T00:07:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2022-12-10T04:08:00.000Z",
        "voteCount": 2,
        "content": "correct answer is A.\nTo ensure that a query that uses a web data source with anonymous authentication can be used by all the other queries in a Power BI dataset, you should select the None privacy level for the data source. The None privacy level allows all the queries in the dataset to access the data from the web data source, regardless of whether the data source uses anonymous authentication."
      },
      {
        "date": "2023-01-13T00:53:00.000Z",
        "voteCount": 1,
        "content": "it cannot be None because this level stops being a viable option as soon as the PBI desktop report has more than one source which is explicitly emphasized in the question (probably exactly to avoid people answering with None)"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91438-exam-dp-500-topic-1-question-43-discussion/",
    "body": "DRAG DROP -<br>You manage a Power BI dataset that queries a fact table named SalesDetails. SalesDetails contains three date columns named OrderDate, CreatedOnDate, and ModifiedDate.<br>You need to implement an incremental refresh of SalesDetails. The solution must ensure that OrderDate starts on or after the beginning of the prior year.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br><img src=\"https://img.examtopics.com/dp-500/image46.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image47.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-06T01:57:00.000Z",
        "voteCount": 1,
        "content": "it's required to exclude data before start of prior year, so, best practice is to filter it out first.\nif we are in June 2023, then all data we have is 1 year and 6 months. \ndata will be split into 1 year of incremental refresh and 6 months will be archived. we can't use the option \"configure IR to archive data 2 years before refresh date\" otherwise, IR will have no sense!\nsolution is :\n1. filter prior year\n2. create parameters\n3. create custom filter for date\n4. configure incremental refresh ONE YEAR BEFORE REFRESH DATE"
      },
      {
        "date": "2023-12-20T11:00:00.000Z",
        "voteCount": 1,
        "content": "small adjustment: 1 year will be archived and 6 months will be incrementally refreshed"
      },
      {
        "date": "2023-09-14T01:22:00.000Z",
        "voteCount": 1,
        "content": "create\nfilter based on start and end\nrefresh 2 years, start of previous needs to be included"
      },
      {
        "date": "2023-06-25T13:13:00.000Z",
        "voteCount": 4,
        "content": "I feel there's an error in the answers.\nIt should be (create parameters, filter orderdata on parameters and then)\nconfigure incr refresh to archive data 2 years before refresh date\nconfigure incr refresh to incrementally refresh (instead of archive) data since 1 year before refresh date"
      },
      {
        "date": "2023-08-16T08:37:00.000Z",
        "voteCount": 1,
        "content": "I think the answer provided is correct, because we have to consider a refresh date that occurs in june 2023 for example, which means we need to refresh from Jan 2022 onwards, and that cannot be done by \"configure incr refresh to incrementally refresh (instead of archive) data since 1 year before refresh date\" . After the first 2 steps, we need to first archive the last 2 years and then filter OrderDate to the start of the prior year."
      },
      {
        "date": "2023-03-26T05:28:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-configure\nis ok"
      },
      {
        "date": "2023-03-26T05:35:00.000Z",
        "voteCount": 2,
        "content": "I mean, I not sure why archive 2years before. I prefere 1 year before"
      },
      {
        "date": "2023-04-16T10:49:00.000Z",
        "voteCount": 1,
        "content": "yeah it's weird, but if it has to be 4 steps..."
      },
      {
        "date": "2023-09-19T03:59:00.000Z",
        "voteCount": 1,
        "content": "2 years, so you always include start of prior year ?"
      },
      {
        "date": "2023-01-13T01:11:00.000Z",
        "voteCount": 2,
        "content": "I don't understand why would we need four steps. I would say that the first 3 steps should be 1. create RangeStart and RangeEnd parameters 2. filter the OrderDate column using a customer filter based on these wo parameters 3. configure an incremental refresh to archive data before the last two years. But after that you should arguably publish the report and do a manual refresh as explained here https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-overview. So what is this 4th step supposed to do?"
      },
      {
        "date": "2023-01-23T06:00:00.000Z",
        "voteCount": 3,
        "content": "Just realized that filtering OrderDate to start of previous year is also needed (but does not have to be the 4th step, can even be the first thing) gives us the subset of data that is relevant since the question more/less states that nothing before that matters. Also this type of filtering determines that the incremental policy should not affect the last two years but only the previous year."
      },
      {
        "date": "2023-04-16T10:48:00.000Z",
        "voteCount": 3,
        "content": "Let's suppose we're in 2023 and the solution requires us to have OrderDate starting on or after the beginning of the prior year (so 2022 onward):\n1. Create RangeStart RangeEnd - obviously\n2. Add an applied step to filter our dates between these two params - obviously\n3. Create an incremental refresh to archive data that starts ONE YEAR BEFORE THE REFRESH DATE.\nNow what that third step does is all we need for the solution - it creates a yearly partition for the entire 2022, and the quarterly/monthly partitions in the current 2023 year. We don't need any additional filtering.\n\nThe same can be achieved in 4 steps, with last 2 years being archived (2021, 2022) and then by filtering OrderDate to the start of the prior year.\n\nSo two approaches are possible in here, but we should select the 4-steps one.\n\nhttps://youtu.be/Kui_1G6kQIQ?t=466\n\nCorrect me If I'm wrong"
      },
      {
        "date": "2022-12-13T07:33:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93992-exam-dp-500-topic-1-question-44-discussion/",
    "body": "DRAG DROP -<br>You plan to create a Power BI report that will use an OData feed as the data source. You will retrieve all the entities from two different collections by using the same service root.<br>The OData feed is still in development. The location of the feed will change once development is complete.<br>The report will be published before the OData feed development is complete.<br>You need to minimize development effort to change the data source once the location changes.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image48.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image49.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-06T02:52:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-07-14T05:54:00.000Z",
        "voteCount": 1,
        "content": "I would consider populating LAST part of the query with parameter as we are going to use the same service root, and as per link service root is the first part of the query\nhttps://learn.microsoft.com/en-us/odata/concepts/url-components"
      },
      {
        "date": "2023-07-20T00:19:00.000Z",
        "voteCount": 3,
        "content": "Changed my mind, this parameter should replace first part of the URL"
      },
      {
        "date": "2023-06-26T04:04:00.000Z",
        "voteCount": 4,
        "content": "Create\nGet data\nDuplicate"
      },
      {
        "date": "2023-03-26T05:46:00.000Z",
        "voteCount": 2,
        "content": "is ok, the last step (duplicate) is because it needed a 2do collection"
      },
      {
        "date": "2023-01-24T07:40:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-06T02:21:00.000Z",
        "voteCount": 4,
        "content": "I don't see why the last step would be duplicating the query from the Advanced Editor so I would go with so I would go with changing the resource path as the last step"
      },
      {
        "date": "2023-02-09T00:25:00.000Z",
        "voteCount": 2,
        "content": "You need two collections, that's why you need to duplicate the query and change the resource path in the URL to the second collection once you've duplicated the query. The answer is correct."
      },
      {
        "date": "2023-01-05T02:00:00.000Z",
        "voteCount": 1,
        "content": "Please confirm the answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91443-exam-dp-500-topic-1-question-45-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Synapse Analytics serverless SQL pool.<br>You need to return a list of files and the number of rows in each file.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image50.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image51.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-30T04:53:00.000Z",
        "voteCount": 10,
        "content": "Count Big\nOpenRowSet"
      },
      {
        "date": "2023-12-06T06:10:00.000Z",
        "voteCount": 1,
        "content": "COUNT_BIG\nOPENROWSET"
      },
      {
        "date": "2023-08-18T23:11:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- COUNT_BIG\n- OPENROWSET"
      },
      {
        "date": "2023-06-26T04:05:00.000Z",
        "voteCount": 1,
        "content": "Count_Big\nOpenRowSet"
      },
      {
        "date": "2023-03-27T19:24:00.000Z",
        "voteCount": 3,
        "content": "COUNT_BIG use *, \nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/count-big-transact-sql?view=sql-server-ver16\nand OpenRowSet"
      },
      {
        "date": "2023-01-21T17:28:00.000Z",
        "voteCount": 3,
        "content": "Since parquet file can be very big, the APPROX_COUNT_DISTINCT makes sense because it's designed for use in big data scenarios. Also, APPROX_COUNT_DISTINCT requires less memory than an exhaustive COUNT DISTINCT operation. With very big file an approximate row count will suffice as opposed to an exact count. https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-01-15T23:19:00.000Z",
        "voteCount": 1,
        "content": "If this was true, group by would not have been used."
      },
      {
        "date": "2023-01-04T12:42:00.000Z",
        "voteCount": 4,
        "content": "I can find no example of using approx_count_distinct with an asterisk, while I know it can be used with count_big."
      },
      {
        "date": "2023-01-06T02:26:00.000Z",
        "voteCount": 8,
        "content": "Not only that but the count_big actually counts rows (check here https://learn.microsoft.com/en-us/sql/t-sql/functions/count-big-transact-sql?view=sql-server-ver16) which is what the Q asks us to do instead of counting non-null values that can repeat across multiple rows (check here https://learn.microsoft.com/en-us/sql/t-sql/functions/approx-count-distinct-transact-sql?view=sql-server-ver16) making approx_count_distinct the wrong answer. Not to mention the approximation element of it all that has no place in the given question."
      },
      {
        "date": "2023-01-11T06:40:00.000Z",
        "voteCount": 3,
        "content": "Your comment makes sense from the links provided"
      },
      {
        "date": "2022-12-13T07:57:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90933-exam-dp-500-topic-1-question-46-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics serverless SQL pool and an Azure Data Lake Storage Gen2 account.<br>You need to query all the files in the \u2018csv/taxi/\u2019 folder and all its subfolders. All the files are in CSV format and have a header row.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image52.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image53.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T01:00:00.000Z",
        "voteCount": 18,
        "content": "it says: query all folders and subfolders, hence /** .\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files#traverse-folders-recursively"
      },
      {
        "date": "2023-01-26T01:05:00.000Z",
        "voteCount": 15,
        "content": "Correct answer is\nBULK 'csv/taxi/**' : although 'csv/taxi/*.csv' also works (tested it), the file extension is not specified. It can be the case that the files are .txt files but still in CSV format.\nand FIRSTROW=2: you need to skip the headers (tested it). FIRSTROW=1 is the default value and will return the data including the headers."
      },
      {
        "date": "2023-03-27T19:30:00.000Z",
        "voteCount": 1,
        "content": "thats correct"
      },
      {
        "date": "2023-06-26T04:07:00.000Z",
        "voteCount": 4,
        "content": "BULK 'csv/taxi/**'\nFIRSTROW=2"
      },
      {
        "date": "2023-01-24T18:49:00.000Z",
        "voteCount": 3,
        "content": "1) https://mydatalake.blob.core.windows.net/data/files/**: All files in the files folder, and recursively its subfolders.\n2) FIRSTROW=1 The FIRSTROW attribute isn't intended to skip column headers. Skipping headers isn't supported by the BULK INSERT statement. If you choose to skip rows, the SQL Server Database Engine looks only at the field terminators, and doesn't validate the data in the fields of skipped rows.\n\nhttps://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/bulk-insert-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2023-01-31T11:27:00.000Z",
        "voteCount": 1,
        "content": "Sorry pals, I'd like to change my opinion, there's no FIRST ROW = TRUE in the code, so the FIRSTROW = 2 in this particular case"
      },
      {
        "date": "2023-01-31T11:28:00.000Z",
        "voteCount": 1,
        "content": "Sorry again, no HEADER = TRUE"
      },
      {
        "date": "2022-12-31T13:44:00.000Z",
        "voteCount": 2,
        "content": "Serverless SQL pool can recursively traverse folders if you specify /** at the end of path. The following query will read all files from all folders and subfolders located in the csv/taxi folder.\n\nThe answer is correct."
      },
      {
        "date": "2023-01-06T02:33:00.000Z",
        "voteCount": 3,
        "content": "How can it be correct when the given solution does not have /** at the end? You probably meant to say that the solution with /** is correct. I agree, obviously."
      },
      {
        "date": "2022-12-30T02:33:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct:\nhttps://mydatalake.blob.core.windows.net/data/files/file*.csv: All .csv files in the files folder with names that start with \"file\".\n\nSource: https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files\n\nAnd it should be FIRSTROW = 2, because it has headers. \nIn the example (follow the link below), you can see that when the file has now header then FIRSTROW = 1.\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#read-specific-columns-from-csv-file\nIn Source"
      },
      {
        "date": "2022-12-13T07:58:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is \nBULK 'csv/taxi/**' and FIRSTROW=1"
      },
      {
        "date": "2022-12-16T01:14:00.000Z",
        "voteCount": 5,
        "content": "FIRSTROW=2 , as first row is header , Reference  : https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files?ns-enrollment-type=learningpath&amp;ns-enrollment-id=learn.wwl.model-query-explore-data-for-azure-synapse"
      },
      {
        "date": "2022-12-16T01:52:00.000Z",
        "voteCount": 8,
        "content": "FIRSTROW=2, remember the \"headers\"."
      },
      {
        "date": "2022-12-10T07:14:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is BULK 'adl://&lt;your_adls_account_name&gt;.dfs.core.windows.net/csv/taxi/',"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92771-exam-dp-500-topic-1-question-47-discussion/",
    "body": "You have a group of data scientists who must create machine learning models and run periodic experiments on a large dataset.<br>You need to recommend an Azure Synapse Analytics pool for the data scientists. The solution must minimize costs.<br>Which type of pool should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Data Explorer pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Apache Spark pool\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dedicated SQL pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta serverless SQL pool"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-24T14:47:00.000Z",
        "voteCount": 1,
        "content": "an Apache Spark pool"
      },
      {
        "date": "2023-08-20T18:34:00.000Z",
        "voteCount": 1,
        "content": "Machine learning models can be trained with help from various algorithms and libraries. Spark MLlib offers scalable machine learning algorithms that can help solving most classical machine learning problems"
      },
      {
        "date": "2023-03-27T19:59:00.000Z",
        "voteCount": 1,
        "content": "b is correct"
      },
      {
        "date": "2022-12-25T07:23:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-01-06T02:35:00.000Z",
        "voteCount": 2,
        "content": "I would agree based on this https://learn.microsoft.com/en-us/azure/synapse-analytics/machine-learning/what-is-machine-learning"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91444-exam-dp-500-topic-1-question-48-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Lake Storage Gen 2 container that stores more than 300,000 files representing hourly telemetry data. The data is organized in folders by the year, month, and day according to when the telemetry was captured.<br>You have the following query in Power Query Editor.<br><img src=\"https://img.examtopics.com/dp-500/image54.png\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image55.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image56.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-26T01:18:00.000Z",
        "voteCount": 19,
        "content": "1. No: the endpoint for HNS is dfs not blob. https://learn.microsoft.com/en-us/azure/storage/blobs/upgrade-to-data-lake-storage-gen2-how-to?tabs=azure-portal#migrate-data-workloads-and-applications\n2. No\n3. Yes"
      },
      {
        "date": "2022-12-13T08:04:00.000Z",
        "voteCount": 10,
        "content": "I think the answers are correct.\nCheck this: https://learn.microsoft.com/en-us/powerquery-m/azurestorage-datalake"
      },
      {
        "date": "2022-12-25T07:23:00.000Z",
        "voteCount": 3,
        "content": "it is correct"
      },
      {
        "date": "2023-12-17T02:59:00.000Z",
        "voteCount": 1,
        "content": "1. Yes: I use ADLS Gen 2 blobs at work at it doest allow hierarchical namespaces.\n2. No\n3. Yes"
      },
      {
        "date": "2023-08-28T00:04:00.000Z",
        "voteCount": 1,
        "content": "I think answer is\nNo\nYes\nNo"
      },
      {
        "date": "2023-08-28T00:06:00.000Z",
        "voteCount": 1,
        "content": "For in Editor uses \"Table.AddColumn\" and \"TransformFiles\" to load file data"
      },
      {
        "date": "2023-08-28T00:07:00.000Z",
        "voteCount": 1,
        "content": "Update\nNo\nYes\nYes"
      },
      {
        "date": "2023-08-28T00:15:00.000Z",
        "voteCount": 1,
        "content": "if there is no funtion name in the script?"
      },
      {
        "date": "2023-08-28T00:24:00.000Z",
        "voteCount": 1,
        "content": "Sorry, Update\nNo\nNo\nYes"
      },
      {
        "date": "2023-08-18T23:12:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- No\n- No\n- Yes"
      },
      {
        "date": "2023-06-08T12:41:00.000Z",
        "voteCount": 6,
        "content": "I think the first one is \"No\" because HierarchicalNavigation is not an option for \"AzureStorage.Blobs\", see here: https://learn.microsoft.com/en-us/powerquery-m/azurestorage-datalake\nHierarchicalNavigation is an option for AzureStorage.Datalake: https://learn.microsoft.com/en-us/powerquery-m/azurestorage-datalake"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91445-exam-dp-500-topic-1-question-49-discussion/",
    "body": "HOTSPOT -<br>You manage a dataset that contains the two data sources as shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image57.png\"><br>When you attempt to refresh the dataset in powerbi.com, you receive the following error message: \u201c[Unable to combine data] Add Columns is accessing data sources that have privacy levels which cannot be used together. Please rebuild this data combination.\u201d<br>You discover that the dataset contains queries that fold data from the SharePoint folder to the Azure SQL database.<br>You need to resolve the error. The solution must provide the highest privacy possible.<br>Which privacy level should you select for each data source? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image58.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image59.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-28T23:55:00.000Z",
        "voteCount": 12,
        "content": "The answers ie Private and Organisational are correct. \nRef: https://learn.microsoft.com/en-us/power-bi/enterprise/desktop-privacy-levels"
      },
      {
        "date": "2022-12-28T23:56:00.000Z",
        "voteCount": 2,
        "content": "*Organisational = Organizational"
      },
      {
        "date": "2023-06-05T03:16:00.000Z",
        "voteCount": 1,
        "content": "Agreed - Private and Organizational."
      },
      {
        "date": "2023-01-30T00:58:00.000Z",
        "voteCount": 1,
        "content": "Sensitive data --&gt; must be private. The other dataset could theoretically be public but why would it be when a higher privacy level is organizational and it seems appropriate for organizational data. The name says it all."
      },
      {
        "date": "2022-12-25T07:25:00.000Z",
        "voteCount": 2,
        "content": "Answer correct, data type"
      },
      {
        "date": "2022-12-13T08:05:00.000Z",
        "voteCount": 4,
        "content": "I would have selected the same"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91446-exam-dp-500-topic-1-question-50-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Power BI dataset named Dataset1.<br>In Dataset1, you currently have 50 measures that use the same time intelligence logic.<br>You need to reduce the number of measures, while maintaining the current functionality.<br>Solution: From Power BI Desktop, you group the measures in a display folder.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-17T18:45:00.000Z",
        "voteCount": 2,
        "content": "may be calculation groups would help reduce the number of measures"
      },
      {
        "date": "2023-06-05T03:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer."
      },
      {
        "date": "2023-03-28T03:44:00.000Z",
        "voteCount": 2,
        "content": "b is correct"
      },
      {
        "date": "2023-01-06T03:10:00.000Z",
        "voteCount": 3,
        "content": "putting them in the same folder is just a cosmetic change that does not impact any underlying problems"
      },
      {
        "date": "2022-12-25T07:26:00.000Z",
        "voteCount": 1,
        "content": "correct, use dax grouping instead"
      },
      {
        "date": "2023-04-26T04:58:00.000Z",
        "voteCount": 1,
        "content": "DAX grouping does not exist, you can GROUPBY as a DAX function."
      },
      {
        "date": "2022-12-13T08:06:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90947-exam-dp-500-topic-1-question-51-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Power BI dataset named Dataset1.<br>In Dataset1, you currently have 50 measures that use the same time intelligence logic.<br>You need to reduce the number of measures, while maintaining the current functionality.<br>Solution: From Tabular Editor, you create a calculation group.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-10T11:46:00.000Z",
        "voteCount": 18,
        "content": "Answer is Yes"
      },
      {
        "date": "2022-12-13T08:06:00.000Z",
        "voteCount": 10,
        "content": "Answer should be Yes"
      },
      {
        "date": "2023-09-29T18:27:00.000Z",
        "voteCount": 1,
        "content": "For reduceing the measure , yes.\nFor reducing the speed, no."
      },
      {
        "date": "2023-06-05T03:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2023-03-28T03:47:00.000Z",
        "voteCount": 2,
        "content": "CG  reduce the number of measures"
      },
      {
        "date": "2023-03-27T01:37:00.000Z",
        "voteCount": 2,
        "content": "Calculation groups reduce the number of redundant measures"
      },
      {
        "date": "2023-01-14T23:57:00.000Z",
        "voteCount": 4,
        "content": "Calculation group is needed. \nhttps://databear.com/calculation-groups-in-power-bi/"
      },
      {
        "date": "2023-01-06T01:52:00.000Z",
        "voteCount": 4,
        "content": "Calculation groups can significantly reduce the number of redundant measures by grouping common measure expressions as calculation items."
      },
      {
        "date": "2022-12-30T03:33:00.000Z",
        "voteCount": 4,
        "content": "Yes. Calculation Group is the way to reduce the number of measures."
      },
      {
        "date": "2022-12-21T04:12:00.000Z",
        "voteCount": 4,
        "content": "Calculation group is needed."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90948-exam-dp-500-topic-1-question-52-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Power BI dataset named Dataset1.<br>In Dataset1, you currently have 50 measures that use the same time intelligence logic.<br>You need to reduce the number of measures, while maintaining the current functionality.<br>Solution: From DAX Studio, you write a query that uses grouping sets.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-13T08:07:00.000Z",
        "voteCount": 8,
        "content": "Answer is No"
      },
      {
        "date": "2022-12-10T11:49:00.000Z",
        "voteCount": 5,
        "content": "Answer should be No"
      },
      {
        "date": "2023-08-01T00:31:00.000Z",
        "voteCount": 2,
        "content": "Using grouping sets in DAX Studio allows you to create groups of measures, but it does not reduce the number of measures in the dataset. Grouping sets are used to aggregate data across multiple dimensions in a query result, but they do not help in reducing the number of individual measures."
      },
      {
        "date": "2023-06-05T03:19:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer. CG needed."
      },
      {
        "date": "2023-03-28T04:22:00.000Z",
        "voteCount": 3,
        "content": "dax studio has no grouping set and calculation groups neither"
      },
      {
        "date": "2023-03-27T01:39:00.000Z",
        "voteCount": 3,
        "content": "Grouping set doesnt exist on DAX STUDIO\nhttps://community.powerbi.com/t5/Desktop/DAX-Grouping-Sets/m-p/2079648#M774276"
      },
      {
        "date": "2023-01-23T10:16:00.000Z",
        "voteCount": 2,
        "content": "DAX has no grouping set: https://community.powerbi.com/t5/Desktop/DAX-Grouping-Sets/td-p/2078121"
      },
      {
        "date": "2023-01-06T03:12:00.000Z",
        "voteCount": 3,
        "content": "According to this: https://community.powerbi.com/t5/Desktop/DAX-Grouping-Sets/td-p/2078121, there is no such thing as a grouping set in DAX so the answer has to be no"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90926-exam-dp-500-topic-1-question-53-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. The files contain more than 40 million rows of UTF-8-encoded business names, survey names, and participant counts. The database is configured to use the default collation.<br>The queries use OPENROWSET and infer the schema shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image60.png\"><br>You need to recommend changes to the queries to reduce I/O reads and tempdb usage.<br>Solution: You recommend defining an external table for the Parquet files and updating the query to use the table.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T01:57:00.000Z",
        "voteCount": 10,
        "content": "You are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. So, a View would do the trick, not an Ext Table, hence my answer would be NO. The correct in my opinion is a View."
      },
      {
        "date": "2024-01-18T03:10:00.000Z",
        "voteCount": 1,
        "content": "NO is the answer"
      },
      {
        "date": "2023-08-03T01:19:00.000Z",
        "voteCount": 1,
        "content": "I choose B (NO) in the best practices reference it only mentions to Use CETAS to enhance query performance and joins: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool"
      },
      {
        "date": "2023-08-01T00:40:00.000Z",
        "voteCount": 2,
        "content": "Defining an external table for the Parquet files and updating the query to use the table can help reduce I/O reads and tempdb usage in Azure Synapse Analytics serverless SQL pool. By creating an external table, you are registering the Parquet files as a table in the database, which allows you to access the data directly without the need for OPENROWSET and schema inference. This can lead to improved query performance and reduced I/O and tempdb usage.\n\nUsing an external table can also simplify query development and improve data access efficiency. The table's metadata is maintained in the data catalog, which can help optimize query execution."
      },
      {
        "date": "2023-08-03T01:18:00.000Z",
        "voteCount": 1,
        "content": "I don't find your explanation in the best practice reference page for serverless sql pool: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool"
      },
      {
        "date": "2023-06-05T03:26:00.000Z",
        "voteCount": 1,
        "content": "Bis correct answer."
      },
      {
        "date": "2023-05-27T09:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-01-09T20:11:00.000Z",
        "voteCount": 2,
        "content": "I think it's B"
      },
      {
        "date": "2022-12-30T04:16:00.000Z",
        "voteCount": 2,
        "content": "Can't recommend it definitely but I would go for the yes because it is considered optimal to always use external table in Serverless SQL pool.\n\nNative external tables that you can use to read and export data in various data formats such as CSV and Parquet. Native external tables are available in serverless SQL pools, and they are in public preview in dedicated SQL pools. Writing/exporting data using CETAS and the native external tables is available only in the serverless SQL pool, but not in the dedicated SQL pools.\n\nThe native external tables are the recommended solution in the pools where they are generally available. If you need to access external data, always use the native tables in serverless pools. In dedicated pools, you should switch to the native tables for reading Parquet files once they are in GA.\n\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"
      },
      {
        "date": "2022-12-30T09:28:00.000Z",
        "voteCount": 6,
        "content": "I have change my mind. \n\nI have read that external tables are useful for smaller datasets such as reference data or dimensions. The answer would be no\nSource: https://www.serverlesssql.com/optimisation/external-tables-vs-views-which-to-use/"
      },
      {
        "date": "2022-12-21T04:15:00.000Z",
        "voteCount": 3,
        "content": "No in my opinion."
      },
      {
        "date": "2022-12-13T08:11:00.000Z",
        "voteCount": 2,
        "content": "We can define the schema in an External Table\nCheck this: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#create-external-table"
      },
      {
        "date": "2022-12-10T06:01:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is A"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91448-exam-dp-500-topic-1-question-54-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. The files contain more than 40 million rows of UTF-8-encoded business names, survey names, and participant counts. The database is configured to use the default collation.<br>The queries use OPENROWSET and infer the schema shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image60.png\"><br>You need to recommend changes to the queries to reduce I/O reads and tempdb usage.<br>Solution: You recommend using OPENROWSET WITH to explicitly specify the maximum length for businessName and surveyName.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-13T01:25:00.000Z",
        "voteCount": 9,
        "content": "Parquet files don't contain metadata about maximum character column length. So serverless SQL pool infers it as varchar(8000).\nYou can see an example like this question in the following link:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#check-inferred-data-types"
      },
      {
        "date": "2023-03-27T02:57:00.000Z",
        "voteCount": 1,
        "content": "It is exactly what I found! Thanks for sharing :)"
      },
      {
        "date": "2023-08-01T00:49:00.000Z",
        "voteCount": 1,
        "content": "Using OPENROWSET WITH to explicitly specify the maximum length for businessName and surveyName does not meet the goal of reducing I/O reads and tempdb usage in Azure Synapse Analytics serverless SQL pool. Specifying the maximum length using OPENROWSET WITH will only enforce a length constraint on the columns but will not directly impact I/O reads or tempdb usage."
      },
      {
        "date": "2023-07-04T05:24:00.000Z",
        "voteCount": 2,
        "content": "I will select B"
      },
      {
        "date": "2023-06-26T11:28:00.000Z",
        "voteCount": 1,
        "content": "on 40M rows reducing the default varchar(8000) to a smaller size will improve I/O and tempDb."
      },
      {
        "date": "2023-04-02T07:09:00.000Z",
        "voteCount": 2,
        "content": "by defaut inferred data types show varchar(8000) with or without WITH clause"
      },
      {
        "date": "2023-03-30T23:23:00.000Z",
        "voteCount": 2,
        "content": "Parquet files don't contain metadata about maximum character column length. So serverless SQL pool infers it as varchar(8000).\n\nYou can optimize inferred data types, using WITH to specify max length\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#check-inferred-data-types"
      },
      {
        "date": "2023-03-30T23:48:00.000Z",
        "voteCount": 1,
        "content": "Answer= NO  .  I correct myself:  The Schema definition is a best practice, but it doesnt explain a reduction of IO.\n\nUse proper collation reduces the I/O\nData in a Parquet file is organized in row groups. Serverless SQL pool skips row groups based on the specified predicate in the WHERE clause, which reduces IO. The result is increased query performance.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#check-inferred-data-types"
      },
      {
        "date": "2022-12-29T00:46:00.000Z",
        "voteCount": 4,
        "content": "You don't need to use OPENROWSET WITH when reading Parquet files. \nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"
      },
      {
        "date": "2023-01-06T03:17:00.000Z",
        "voteCount": 1,
        "content": "bc of the automatic schema that is even emphasized in the question scenario so I would definitely go with no as the correct answer."
      },
      {
        "date": "2023-03-25T00:29:00.000Z",
        "voteCount": 1,
        "content": "Check the Explicitly specify schema, I would vote A"
      },
      {
        "date": "2022-12-13T08:13:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91336-exam-dp-500-topic-1-question-55-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. The files contain more than 40 million rows of UTF-8-encoded business names, survey names, and participant counts. The database is configured to use the default collation.<br>The queries use OPENROWSET and infer the schema shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image60.png\"><br>You need to recommend changes to the queries to reduce I/O reads and tempdb usage.<br>Solution: You recommend defining a data source and view for the Parquet files. You recommend updating the query to use the view.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-31T04:59:00.000Z",
        "voteCount": 8,
        "content": "\"views are generally faster and have more features such as OPENROWSET\"\n\"External tables require an explicit defined schema while views can use OPENROWSET to provide automatic schema inference allowing for more flexibility (but note that an explicitly defined schema can provide faster performance)\"\n\nSource: https://www.jamesserra.com/archive/2020/11/external-tables-vs-t-sql-views-on-files-in-a-data-lake/"
      },
      {
        "date": "2022-12-21T04:16:00.000Z",
        "voteCount": 5,
        "content": "Yes. Views is my answer for auto schema."
      },
      {
        "date": "2023-08-01T00:56:00.000Z",
        "voteCount": 1,
        "content": "When you define a data source and view for the Parquet files, you create metadata that specifies the schema of the data and its location. This eliminates the need for automatic schema inference during query execution, reducing I/O reads and improving performance."
      },
      {
        "date": "2023-04-15T03:58:00.000Z",
        "voteCount": 1,
        "content": "Views are recommended for large datasets because they can have partition pruning. External table don't and are recommended for smaller datasets"
      },
      {
        "date": "2022-12-25T07:29:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      },
      {
        "date": "2022-12-13T08:13:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-12-13T01:37:00.000Z",
        "voteCount": 3,
        "content": "this is the correct. \"External tables\" require an explicit defined schema while \"views\" can use OPENROWSET to provide \"automatic schema inference\" allowing for more flexibility."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91451-exam-dp-500-topic-1-question-56-discussion/",
    "body": "HOTSPOT -<br>You are creating a Power BI Desktop report.<br>You add a Python visual to the report page.<br>You plan to create a scatter chart to visualize the data.<br>You add Python code to the Python script editor.<br>You need to create the scatter chart.<br>How should you complete the Python code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image61.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image62.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T08:26:00.000Z",
        "voteCount": 7,
        "content": "Correct"
      },
      {
        "date": "2023-06-26T04:17:00.000Z",
        "voteCount": 1,
        "content": "matplotlib.pyploy\nchart.show()"
      },
      {
        "date": "2023-04-02T15:59:00.000Z",
        "voteCount": 1,
        "content": "As Homer says: Exaaaaact \n                  ...D'ouch!"
      },
      {
        "date": "2023-03-27T03:15:00.000Z",
        "voteCount": 1,
        "content": "OK.  checked it on: \nhttps://www.w3schools.com/python/matplotlib_scatter.asp"
      },
      {
        "date": "2023-01-06T03:23:00.000Z",
        "voteCount": 2,
        "content": "In a few different Qs from this site examples of Python code contain the exact same parts of code that the provided answers suggest, so if this is not correct, Microsoft ppl making the Q would contradict themselves. There's your reassurance that the answers are correct."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92379-exam-dp-500-topic-1-question-57-discussion/",
    "body": "You are configuring a Power BI report for accessibility as shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image63.png\"><br>You need to change the default colors of all three visuals to make the report more accessible to users who have color vision deficiency.<br>Which two settings should you configure in the Customize theme window? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTheme colors",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDivergent colors\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSentiment colors\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirst-level elements colors"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-02T00:40:00.000Z",
        "voteCount": 9,
        "content": "Divergent colors - Colors used in conditional formatting to show where a data point falls in a range.\nSentiment colors - Colors used in key performance indicator (KPI) visuals and waterfall charts to indicate positive, negative, or neutral results."
      },
      {
        "date": "2023-01-06T03:32:00.000Z",
        "voteCount": 2,
        "content": "these sound both correct so I think I have to agree"
      },
      {
        "date": "2023-08-18T23:14:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Divergent colors\n- Sentiment colors"
      },
      {
        "date": "2023-06-26T11:53:00.000Z",
        "voteCount": 1,
        "content": "For this specific report, from left to right, top to bottom.\nVisual 1; no updates needed\nVisual2: divergent colors (colors used in conditional formatting to show where a data point falls in a range) need to be refined\nVisual3; Waterfall charts can use sentiment colors (needs to be switched on)"
      },
      {
        "date": "2023-05-09T11:56:00.000Z",
        "voteCount": 2,
        "content": "Would select B and C"
      },
      {
        "date": "2023-03-27T03:48:00.000Z",
        "voteCount": 3,
        "content": "Sentiment colors for the  KPI and Waterfall charts\n Divergent colors :  It works ok the Matrix graph"
      },
      {
        "date": "2022-12-21T10:31:00.000Z",
        "voteCount": 2,
        "content": "https://www.c-sharpcorner.com/article/customize-current-theme-from-power-bi-desktop-without-using-json-part-one/"
      },
      {
        "date": "2022-12-21T10:30:00.000Z",
        "voteCount": 3,
        "content": "I choose BC \nTheme colors\n&nbsp;\nWe can set eight(8) different theme colors considering different data colors used for different charts, lines etc.\n&nbsp;\nSentiment colors\n&nbsp;\nThis can be used to indicate KPI visual as well as to represent positive and negative number for waterfall chart and to show positive, neutral and negative results.\n&nbsp;\nDivergent colors\n&nbsp;\nThis can be used to represent conditional formatting to represent where data point falls in a range."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96571-exam-dp-500-topic-1-question-58-discussion/",
    "body": "You use Azure Synapse Analytics and Apache Spark notebooks to explore native visuals.<br>You need to use PySpark to gain access to the visual libraries.<br>Which Python libraries should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeaborn and TensorFlow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeaborn only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMatplotlib and Seaborn\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMatplotlib only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMatplotlib and TensorFlow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTensorFlow only"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-18T23:13:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Matplotlib and Seaborn"
      },
      {
        "date": "2023-06-26T12:05:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-data-visualization#additional-libraries"
      },
      {
        "date": "2023-06-26T04:32:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer."
      },
      {
        "date": "2023-04-02T16:39:00.000Z",
        "voteCount": 2,
        "content": "C is correct."
      },
      {
        "date": "2023-02-20T23:53:00.000Z",
        "voteCount": 2,
        "content": "C  is correct.\n\nSeaborn is a data visualization library based on Matplotlib, mainly used for Statistical graphs.\n\nMatplotlib is used for line plots, scatter plots, histograms, bar charts, pie charts and many other different plots"
      },
      {
        "date": "2023-01-23T01:54:00.000Z",
        "voteCount": 1,
        "content": "100% Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/90950-exam-dp-500-topic-1-question-59-discussion/",
    "body": "DRAG DROP -<br>You are using DAX Studio to query an XMLA endpoint.<br>You need to identify the duplicate values in a column named Email in a table named Subscription.<br>How should you complete the DAX expression? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image65.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image66.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T00:43:00.000Z",
        "voteCount": 20,
        "content": "Should be (1) EVALUATE and (2) CURRENT GROUP"
      },
      {
        "date": "2022-12-31T02:18:00.000Z",
        "voteCount": 14,
        "content": "EVALUATE &amp; CURRENTGROUP is correct. Tested!"
      },
      {
        "date": "2023-06-26T04:34:00.000Z",
        "voteCount": 5,
        "content": "EVALUATE\nCURRENTGROUP"
      },
      {
        "date": "2023-05-09T11:58:00.000Z",
        "voteCount": 2,
        "content": "EVALUATE not CALCULATE since you are using DAX Studio. Then CURRENT GROUP as well"
      },
      {
        "date": "2023-04-02T17:32:00.000Z",
        "voteCount": 2,
        "content": "I prefer Evaluate and currentgroup. \nWe can't use calculate instead we use calculatetable that is not an option and is not necesary"
      },
      {
        "date": "2022-12-10T12:46:00.000Z",
        "voteCount": 1,
        "content": "Can anyone give the answer?"
      },
      {
        "date": "2022-12-26T04:08:00.000Z",
        "voteCount": 4,
        "content": "1.  EVALUATE  (because its a Query)\n2. Currentgroup"
      },
      {
        "date": "2023-04-04T00:41:00.000Z",
        "voteCount": 3,
        "content": "EVALUATE is a function that can be used to return the result of a table expression as a table. In this case, the FILTER and GROUPBY functions are used to create a table expression that returns the duplicate values in the Email column of the Subscription table.\nCALCULATE, on the other hand, is used to manipulate the filter context of a measure in DAX. It takes an expression as its first argument that defines the measure to be calculated, and one or more filter expressions as its subsequent arguments. It cannot be used to return a table like EVALUATE does.\nSo, in summary, EVALUATE is the correct function to use in this case to return the table of duplicate email values, and CALCULATE is not a suitable alternative.\nby Chat GPT"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92859-exam-dp-500-topic-1-question-60-discussion/",
    "body": "You have a Power BI report that contains one visual.<br>You need to provide users with the ability to change the visual type without affecting the view for other users.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tabular Editor, create a new perspective.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Visual options in Report settings, select Use the modern visual header with updated styling options.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Report setting, select Personalize visuals.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Bookmarks pane, select Focus mode, and then select Add."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T03:23:00.000Z",
        "voteCount": 8,
        "content": "Check https://learn.microsoft.com/en-us/power-bi/create-reports/power-bi-personalize-visuals?tabs=powerbi-desktop"
      },
      {
        "date": "2023-08-18T23:14:00.000Z",
        "voteCount": 2,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- From Report setting, select Personalize visuals"
      },
      {
        "date": "2023-06-26T04:35:00.000Z",
        "voteCount": 2,
        "content": "C is the correct answer."
      },
      {
        "date": "2023-01-23T01:56:00.000Z",
        "voteCount": 2,
        "content": "definitely correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92860-exam-dp-500-topic-1-question-61-discussion/",
    "body": "You have a kiosk that displays a Power BI report page. The report uses a dataset that uses Import storage mode.<br>You need to ensure that the report page updates all the visuals every 30 minutes.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the data sources to use DirectQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect Auto page refresh.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Power BI embedded.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the XMLA endpoint.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Microsoft Power Automate visual to the report page.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the data sources to use a streaming dataset."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-26T03:39:00.000Z",
        "voteCount": 9,
        "content": "check https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-automatic-page-refresh"
      },
      {
        "date": "2023-01-06T03:55:00.000Z",
        "voteCount": 2,
        "content": "Normally I would use automatic refresh when data changes more frequently, but other options make no sense so by using elimination I would say AB is correct. Additionally, the link provides a screenshot example defining the auto reoccurring time interval for the refresh of exactly 30min, so let's stick with Microsoft's example."
      },
      {
        "date": "2023-09-14T02:46:00.000Z",
        "voteCount": 1,
        "content": "i guess it's correct however, if it's imported, you cannot switch to direct query, once a report is 'import mode' it cant be switched, so you would have to rebuild and configure auto refresh, i don't see any other options in the list that would be good"
      },
      {
        "date": "2023-08-29T02:03:00.000Z",
        "voteCount": 1,
        "content": "if Import and DirectQuery conflicting? Maybe A is not Corrcet\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-use-directquery"
      },
      {
        "date": "2023-08-25T08:08:00.000Z",
        "voteCount": 1,
        "content": "AB Correct"
      },
      {
        "date": "2023-06-26T04:37:00.000Z",
        "voteCount": 1,
        "content": "A and B are correct answers - as Maazi referenced.\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-automatic-page-refresh"
      },
      {
        "date": "2023-04-02T18:30:00.000Z",
        "voteCount": 3,
        "content": "as Maazi said https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-automatic-page-refresh"
      },
      {
        "date": "2023-02-01T18:40:00.000Z",
        "voteCount": 2,
        "content": "Open AI says the following:\n\nYou should perform the following actions:\n\nB. Select Auto page refresh: This option allows you to configure the report page to refresh automatically at a specified interval. In this case, you would set the refresh interval to 30 minutes to ensure that the report page updates all visuals every 30 minutes.\n\nE. Add a Microsoft Power Automate visual to the report page: Power Automate is a cloud-based service that allows you to automate tasks and workflows. By adding a Power Automate visual to the report page, you can configure it to refresh the report page at a specific interval, such as every 30 minutes.\n\nNote that you do not need to perform the other actions listed (Configure the data sources to use DirectQuery, Enable Power BI embedded, Enable the XMLA endpoint, Configure the data sources to use a streaming dataset) to achieve your desired outcome of updating the report page every 30 minutes. The options you need are \"Auto page refresh\" and \"Microsoft Power Automate.\""
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96446-exam-dp-500-topic-1-question-62-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trollup",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfreqItems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescribe\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-25T08:08:00.000Z",
        "voteCount": 1,
        "content": "d is correct"
      },
      {
        "date": "2023-04-02T18:31:00.000Z",
        "voteCount": 2,
        "content": "d is correct"
      },
      {
        "date": "2023-01-22T00:31:00.000Z",
        "voteCount": 3,
        "content": "Duplicate of 101 and 103. Correct answer is D"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93292-exam-dp-500-topic-1-question-63-discussion/",
    "body": "HOTSPOT -<br>You have the following code in an Azure Synapse notebook.<br><img src=\"https://img.examtopics.com/dp-500/image68.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the code.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image69.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image70.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-07-08T20:36:00.000Z",
        "voteCount": 2,
        "content": "since these two sets do not share the same axis, the results are both stacked bar(same x) and grouped bar chart(different x),!"
      },
      {
        "date": "2023-04-02T18:58:00.000Z",
        "voteCount": 2,
        "content": "ok, is stacked bar, tested"
      },
      {
        "date": "2023-03-27T07:28:00.000Z",
        "voteCount": 3,
        "content": "I ran the code, and the Answer it OK! \nimport matplotlib.pyplot as plt\n\nx1= [1,3,4,5,6,7,9]\ny1= [4,7,2,4,7,8,3]\nx2= [2,4,6,8,10]\ny2= [5,6,2,6,2]\n\nplt.bar(x1,y1, label = \"Blue Item\", color= 'b')\nplt.bar(x2,y2, label = \"Green Item\", color= 'g')\nplt.plot()\nplt.xlabel(\"Number\")\nplt.ylabel(\"Height\")\nplt.title(\"My chart\")\nplt.legend()\nplt.show()"
      },
      {
        "date": "2023-01-30T02:22:00.000Z",
        "voteCount": 1,
        "content": "very similar example here that confirms the code outcome: https://www.geeksforgeeks.org/create-a-stacked-bar-plot-in-matplotlib/"
      },
      {
        "date": "2023-01-06T10:40:00.000Z",
        "voteCount": 3,
        "content": "answer is correct"
      },
      {
        "date": "2023-01-04T02:44:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct. I ran the code."
      },
      {
        "date": "2022-12-30T09:23:00.000Z",
        "voteCount": 2,
        "content": "Please confirm the option"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91423-exam-dp-500-topic-1-question-64-discussion/",
    "body": "HOTSPOT -<br>You have the following code in an Azure Synapse notebook.<br><img src=\"https://img.examtopics.com/dp-500/image72.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the code.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image73.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image74.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T06:15:00.000Z",
        "voteCount": 26,
        "content": "The answer should be \"one scatterplot\" and \"three markers\"."
      },
      {
        "date": "2023-01-05T17:13:00.000Z",
        "voteCount": 7,
        "content": "I ran the code, Ramiel answer is correct: \"1 scatterplot\"  and \"3 markers\""
      },
      {
        "date": "2023-03-27T07:21:00.000Z",
        "voteCount": 4,
        "content": "I ran the code too.  \nThe ANSWEAR =  1 Scatterplot  and 3 markers"
      },
      {
        "date": "2023-03-27T07:29:00.000Z",
        "voteCount": 4,
        "content": "import matplotlib.pyplot as plt\n\nx1= [2, 3, 4]\ny1= [5, 5, 5]\n\nx2= [1, 2, 3, 4, 5]\ny2= [2, 3, 2, 3, 4]\n\nx3= [1, 2, 3, 4, 5]\ny3= [6, 8, 7, 8, 7]\nplt.scatter(x1,y1)\nplt.scatter(x2,y2,marker= 'v', color= 'r')\nplt.scatter(x2,y3,marker= '^', color= 'm')\nplt.title('Scatter Plot')\n\nplt.show()"
      },
      {
        "date": "2023-12-28T03:47:00.000Z",
        "voteCount": 1,
        "content": "1 scatterplot with 3 markers\nran the code"
      },
      {
        "date": "2023-09-15T00:25:00.000Z",
        "voteCount": 1,
        "content": "confirmed!! ran code, 1 plot, 3 markers"
      },
      {
        "date": "2023-08-20T19:44:00.000Z",
        "voteCount": 1,
        "content": "A Scatter plot is a plot in which coordinates are shown as markers(dots) on the graph. A Scatter plot is useful for showing the relationship between the variables.\n\nIn matplotlib, a scatter plot is implemented using the scatter() function, which takes at least two parameters, x-axis data and y-axis data.\nTherefore, there are 3 scatter() functions means that there are 3 scatter plots\nThe anwser, I think, is 3 scatterplots &amp; 3 makers"
      },
      {
        "date": "2023-09-15T00:26:00.000Z",
        "voteCount": 1,
        "content": "I ran the code, 1 plot, 3 markers"
      },
      {
        "date": "2023-07-21T18:37:00.000Z",
        "voteCount": 1,
        "content": "one scatterplot - 3 marker - tested"
      },
      {
        "date": "2023-06-26T12:40:00.000Z",
        "voteCount": 2,
        "content": "It's multiple (3) scatterplots in one figure (and using three markers to help distinguish the three scatterplots)"
      },
      {
        "date": "2023-04-02T19:05:00.000Z",
        "voteCount": 2,
        "content": "3 scatterplot and 3 markers"
      },
      {
        "date": "2023-03-28T22:12:00.000Z",
        "voteCount": 1,
        "content": "OK, using Chet GPT:\n\ni have this code in Azure Synapse Notebook:\n\nimport matplotlib.pyplot as plt\nx1= [2, 3, 4]\ny1= [5, 5, 5]\n\nx2= [1, 2, 3, 4, 5]\ny2= [2, 3, 2, 3, 4]\n\ny3= [6, 8, 7, 8, 7]\nplt.scatter(x1,y1)\nplt.scatter(x2,y2,marker= 'v', color= 'r')\nplt.scatter(x2,y3,marker= '^', color= 'm')\nplt.title('Scatter Plot')\n\nplt.show()\n\nHow many markers and scatterplots will i get???"
      },
      {
        "date": "2023-03-28T22:13:00.000Z",
        "voteCount": 1,
        "content": "Chat GPT answer:\nYou will get three scatter plots, one for each set of data points (x1, y1), (x2, y2), and (x2, y3). The first scatter plot will have circular markers, the second scatter plot will have downward-pointing triangles as markers, and the third scatter plot will have upward-pointing triangles as markers. Therefore, you will have a total of two different markers."
      },
      {
        "date": "2023-03-25T01:34:00.000Z",
        "voteCount": 2,
        "content": "to those really do EDA or visualization in python, should know there should only one plot figure"
      },
      {
        "date": "2023-02-22T00:13:00.000Z",
        "voteCount": 1,
        "content": "OK, If I see the comments from cookiemonster42, I would say that the right answer is: \n\"3 scatterplot\" and \"2 markers\".\nI havent ran the code, but reading it one could infer the outcome."
      },
      {
        "date": "2023-02-21T02:57:00.000Z",
        "voteCount": 1,
        "content": "The answer is 3 scatter plots and 3 markers  like cookiemonster42 explained below"
      },
      {
        "date": "2023-02-07T22:52:00.000Z",
        "voteCount": 2,
        "content": "This is a Python script that generates a scatter plot using the Matplotlib library. The script generates 3 scatter plots, with the following characteristics:\n\nFirst scatter plot:\nx-axis data: [2, 3, 4]\ny-axis data: [5, 5, 5]\nSecond scatter plot:\nx-axis data: [1, 2, 3, 4, 5]\ny-axis data: [2, 3, 2, 3, 4]\nMarker style: 'v'\nColor: 'r' (red)\nThird scatter plot:\nx-axis data: [1, 2, 3, 4, 5]\ny-axis data: [6, 8, 7, 8, 7]\nMarker style: '^'\nColor: 'm' (magenta)\nThe final line, plt.show(), is used to display the scatter plot in a GUI window."
      },
      {
        "date": "2023-02-10T00:31:00.000Z",
        "voteCount": 1,
        "content": "So, the correct answer should be 3 scatterplot and 3 markers???"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93807-exam-dp-500-topic-1-question-65-discussion/",
    "body": "HOTSPOT -<br>You use Advanced Editor in Power Query Editor to edit a query that references two tables named Sales and Commission.<br>A sample of the data in the Sales table is shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image76.png\"><br>A sample of the data in the Commission table is shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image77.png\"><br>You need to merge the tables by using Power Query Editor without losing any rows in the Sales table.<br>How should you complete the query? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image78.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image79.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-18T23:15:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Join\n- LeftOuter"
      },
      {
        "date": "2023-06-26T04:58:00.000Z",
        "voteCount": 1,
        "content": "Join\nLeftOuter"
      },
      {
        "date": "2023-04-03T03:23:00.000Z",
        "voteCount": 1,
        "content": "in Power Q is  joinNested and leftouter \nhttps://learn.microsoft.com/en-us/power-query/merge-queries-left-anti\nhttps://learn.microsoft.com/en-us/power-query/merge-queries-left-outer"
      },
      {
        "date": "2023-01-23T02:06:00.000Z",
        "voteCount": 2,
        "content": "Considering that Left.Outer per definition keeps all the rows in the \"left\" table, here Sales, the answer is definetly correct"
      },
      {
        "date": "2023-01-10T10:01:00.000Z",
        "voteCount": 3,
        "content": "Just created two tables same data and combine them in Power BI, join and leftouter is correct."
      },
      {
        "date": "2023-01-04T02:07:00.000Z",
        "voteCount": 3,
        "content": "Is this correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95911-exam-dp-500-topic-1-question-66-discussion/",
    "body": "You have a Power BI dataset named Dataset1 that uses DirectQuery against an Azure SQL database named DB1.<br>DB1 is a transactional database in the third normal form.<br>You need to recommend a solution to minimize how long it takes to execute the query. The solution must maintain the current functionality.<br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the relationships from Dataset1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the tables in DB1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate calculated columns in Dataset1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDenormalize the tables in DB1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-18T23:15:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Denormalize the tables in DB1"
      },
      {
        "date": "2023-06-26T12:48:00.000Z",
        "voteCount": 3,
        "content": "D(enormalize)"
      },
      {
        "date": "2023-04-03T03:45:00.000Z",
        "voteCount": 2,
        "content": "I think D is correct"
      },
      {
        "date": "2023-01-18T16:25:00.000Z",
        "voteCount": 1,
        "content": "shouldn't it be B, because long tables are taking longer to process?"
      },
      {
        "date": "2023-01-30T02:27:00.000Z",
        "voteCount": 1,
        "content": "no because \"DB1 is a transactional database in the third normal form\" --&gt; the last part of the sentence means that it is already normalized."
      },
      {
        "date": "2023-01-21T02:47:00.000Z",
        "voteCount": 4,
        "content": "Denormalized tables are quicker to read than normalized, so I believe this is correct https://medium.com/@innerbit/when-and-how-you-should-denormalize-a-relational-database-75047344ebac"
      },
      {
        "date": "2023-12-17T05:18:00.000Z",
        "voteCount": 1,
        "content": "A query performs faster in a big denormalized table.\nNormalized databases requires more joins, which are expensive query processes."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93148-exam-dp-500-topic-1-question-67-discussion/",
    "body": "You have a file named File1.txt that has the following characteristics:<br><br>A header row -<br><br>Tab delimited values -<br><br>UNIX-style line endings -<br>You need to read File1.txt by using an Azure Synapse Analytics serverless SQL pool.<br>Which query should you execute?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT*<br>FROM OPENROWSET(<br>BULK \u2018file1.txt\u2019,<br>DATA_SOURCE = \u2018Sql1\u2019,<br>FORMAT = \u2018CSV\u2019, PARSER_VERSION = \u20182.0\u2019,<br>FIELDTERMINATOR = \u2018\\t\u2019,<br>ROWTERMINATOR = \u20180x0a\u2019,<br><br>FIRSTROW= 2 -<br>)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT*<br>FROM OPENROWSET(<br>BULK \u2018file1.txt\u2019,<br>DATA_SOURCE = \u2018Sql1\u2019,<br>FORMAT = \u2018CSV\u2019, PARSER_VERSION = \u20182.0\u2019,<br>FIELDTERMINATOR = \u2018,\u2019,<br>ROWTERMINATOR = \u2018\\n\u2019,<br><br>FIRSTROW= 2 -<br>)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT*<br>FROM OPENROWSET(<br>BULK \u2018file1.txt\u2019,<br>DATA_SOURCE = \u2018Sql1\u2019,<br>FORMAT = \u2018CSV\u2019, PARSER_VERSION = \u20182.0\u2019,<br>FIELDTERMINATOR = \u2018,\u2019,<br>ROWTERMINATOR = \u20180x0a\u2019,<br><br>FIRSTROW= 2 -<br>)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT*<br>FROM OPENROWSET(<br>BULK \u2018file1.txt\u2019,<br>DATA_SOURCE = \u2018Sql1\u2019,<br>FORMAT = \u2018CSV\u2019, PARSER_VERSION = \u20182.0\u2019,<br>FIELDTERMINATOR = \u2018\\t\u2019,<br>ROWTERMINATOR = \u20180x0a\u2019,<br><br>FIRSTROW= 1 -<br>)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-29T03:23:00.000Z",
        "voteCount": 6,
        "content": "Ref: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-single-csv-file"
      },
      {
        "date": "2023-04-03T03:49:00.000Z",
        "voteCount": 1,
        "content": "a is correct"
      },
      {
        "date": "2023-03-30T22:01:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT:\nBased on the characteristics of the File1.txt, the correct query to execute in Azure Synapse Analytics serverless SQL pool is option A.\nThis query uses the OPENROWSET function to read a CSV file, which is specified by the BULK option. The FIELDTERMINATOR is set to a tab character (\\t), and the ROWTERMINATOR is set to the UNIX-style line ending (0x0a). The FIRSTROW option is set to 2 to skip the header row. Option B uses a comma as the field terminator, which does not match the tab-delimited format of the file. Option C uses a comma as the field terminator and the UNIX-style line ending as the row terminator, which does not match the characteristics of the file. Option D sets the FIRSTROW option to 1, which would include the header row in the result set.\nSo, A."
      },
      {
        "date": "2023-04-04T00:08:00.000Z",
        "voteCount": 1,
        "content": "CHAT GPT :\nBased on the characteristics of the File1.txt, the correct query to read the file using an Azure Synapse Analytics serverless SQL pool is:  D\n\nExplanation:\nThe file has a header row, so it's important to specify that the first row is not a header row in the query by setting FIRSTROW=1.\nThe file is tab-delimited, so FIELDTERMINATOR should be set to '\\t'.\nThe file has UNIX-style line endings, so ROWTERMINATOR should be set to '0x0a'.\nThe FORMAT option should be set to 'CSV', even though it is not strictly a CSV file because it is tab-delimited. This is because CSV format can handle tab-delimited files as well.\nOptions A, B, and C are not correct because they have incorrect field terminators or row terminators, or they do not specify that the first row is a header row."
      },
      {
        "date": "2023-02-01T18:41:00.000Z",
        "voteCount": 1,
        "content": "It's C, open AI confirmed it :)"
      },
      {
        "date": "2023-01-23T02:33:00.000Z",
        "voteCount": 4,
        "content": "This part of the A solution is the key and makes all other solutions incorrect (t for tab): FIELDTERMINATOR = \u2018\\t'"
      },
      {
        "date": "2023-01-23T23:57:00.000Z",
        "voteCount": 2,
        "content": "....and FIRSTROW= 2, because the Headers."
      },
      {
        "date": "2023-03-27T07:53:00.000Z",
        "voteCount": 1,
        "content": "You are right \nTerminating character Tab = '\\t'\nNewline character\t=    '\\n'\n\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/import-export/specify-field-and-row-terminators-sql-server?view=sql-server-ver16"
      },
      {
        "date": "2023-04-26T04:07:00.000Z",
        "voteCount": 1,
        "content": "Option D contains FIELDTERMINATOR = \u2018\\t' as well FYI."
      },
      {
        "date": "2023-01-05T11:26:00.000Z",
        "voteCount": 1,
        "content": "that website makes it look like the answer is c with the same field terminator and row terminator."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91018-exam-dp-500-topic-1-question-68-discussion/",
    "body": "HOTSPOT -<br>You have a Power BI dataset that has the query dependencies shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image81.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image82.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image83.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-13T01:26:00.000Z",
        "voteCount": 18,
        "content": "Orders query is executed 3 times but creating a dataflow will reduce data refresh times, not the table.buffer\nhttps://learn.microsoft.com/en-us/power-bi/guidance/power-query-referenced-queries"
      },
      {
        "date": "2022-12-21T10:38:00.000Z",
        "voteCount": 10,
        "content": "3 and Dataflow"
      },
      {
        "date": "2023-08-18T23:16:00.000Z",
        "voteCount": 4,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- 3\n- Replacing the Orders query with a dataflow"
      },
      {
        "date": "2023-06-26T10:49:00.000Z",
        "voteCount": 2,
        "content": "3\nDataflow"
      },
      {
        "date": "2023-01-02T00:22:00.000Z",
        "voteCount": 7,
        "content": "3 and Dataflow.\n\nWhen the data model is refreshed, it's often assumed that Power Query retrieves the Query1 result, and that it's reused by referenced queries. This thinking is incorrect. In fact, Power Query executes Query2, Query3, and Query4 separately. Query1 is executed three times. The multiple executions can result in slow data refresh, and negatively impact on the data source.\n\nThe use of the Table.Buffer function in Query1 won't eliminate the additional data retrieval. We recommend you create a dataflow instead. Using a dataflow can improve data refresh time, and reduce impact on your data sources.\n\nSource: https://learn.microsoft.com/en-us/power-bi/guidance/power-query-referenced-queries"
      },
      {
        "date": "2022-12-11T07:23:00.000Z",
        "voteCount": 2,
        "content": "should be 0"
      },
      {
        "date": "2022-12-11T07:27:00.000Z",
        "voteCount": 4,
        "content": "*3*. Did not see the reference query"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94226-exam-dp-500-topic-1-question-69-discussion/",
    "body": "You are creating an external table by using an Apache Spark pool in Azure Synapse Analytics. The table will contain more than 20 million rows partitioned by date. The table will be shared with the SQL engines.<br>You need to minimize how long it takes for a serverless SQL pool to execute a query data against the table.<br>In which file format should you recommend storing the table data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Parquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-02T23:07:00.000Z",
        "voteCount": 1,
        "content": "D is correct \nReference: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#prepare-files-for-querying"
      },
      {
        "date": "2023-08-18T23:16:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n Apache Parquet"
      },
      {
        "date": "2023-04-16T01:49:00.000Z",
        "voteCount": 1,
        "content": "From Chatgpt:\nBy using Apache Parquet format for the external table, you can minimize the query execution time for serverless SQL pools in Azure Synapse Analytics because:\n\nColumnar storage: Apache Parquet stores data in a columnar format, which allows for highly efficient and fast data access. This means that queries against the external table can be executed faster because only the relevant columns are read.\n\nCompression: Apache Parquet uses a highly efficient compression algorithm, which reduces the size of the data on disk. Smaller data size means less data to transfer, which results in faster query execution time.\n\nPartitioning: Apache Parquet supports partitioning, which allows you to subdivide the external table into smaller, more manageable files. When querying the table, only the relevant partitions are scanned, which makes query execution faster.\n\nOverall, by using Apache Parquet for the external table, you can significantly reduce the amount of time it takes for a serverless SQL pool to execute a query against the table, making it a more performant solution for analyzing large datasets."
      },
      {
        "date": "2023-03-08T03:12:00.000Z",
        "voteCount": 2,
        "content": "Well, this link doesn't give the answer directly, but MS indirectly states that you should use Apache Parquet files for your SQL serverless pool.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/get-started-analyze-sql-on-demand"
      },
      {
        "date": "2023-01-06T13:09:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93287-exam-dp-500-topic-1-question-70-discussion/",
    "body": "You are running a diagnostic against a query as shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image84.png\"><br>What can you identify from the diagnostics query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSome query steps are folding.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe query is timing out.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tElevated permissions are being used to query records.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll the query steps are folding.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-04T04:09:00.000Z",
        "voteCount": 1,
        "content": "I understand that all query could be sent to source, for this reason it is folded"
      },
      {
        "date": "2023-01-09T21:22:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct,\nnon of the steps added to query  prevent folding, which means folding should be working fine in all steps\n\nSteps that prevent folding: \nhttps://learn.microsoft.com/en-us/power-query/power-query-folding#transformations-that-prevent-folding"
      },
      {
        "date": "2022-12-30T09:17:00.000Z",
        "voteCount": 2,
        "content": "Please confirm"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94193-exam-dp-500-topic-1-question-71-discussion/",
    "body": "DRAG DROP -<br>You are configuring Azure Synapse Analytics pools to support the Azure Active Directory groups shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image85.png\"><br>Which type of pool should each group use? To answer, drag the appropriate pool types to the groups. Each pool type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image86.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image87.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-26T10:51:00.000Z",
        "voteCount": 3,
        "content": "Apache\nDedicated\nServerless"
      },
      {
        "date": "2023-04-04T19:29:00.000Z",
        "voteCount": 1,
        "content": "I think this is ok, but not sure"
      },
      {
        "date": "2023-03-27T22:18:00.000Z",
        "voteCount": 1,
        "content": "it ok! :)"
      },
      {
        "date": "2023-01-21T02:56:00.000Z",
        "voteCount": 1,
        "content": "I believe this is correct https://www.royalcyber.com/resources/blogs/dedicated-sql-pool-vs-serverless-sql/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95044-exam-dp-500-topic-1-question-72-discussion/",
    "body": "DRAG DROP -<br>You have a Power BI dataset. The dataset contains data that is updated frequently.<br>You need to improve the performance of the dataset by using incremental refreshes.<br>Which four actions should you perform in sequence to enable the incremental refreshes? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image88.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image89.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-13T02:41:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2023-07-18T23:29:00.000Z",
        "voteCount": 2,
        "content": "This is correct :)"
      },
      {
        "date": "2023-06-26T10:53:00.000Z",
        "voteCount": 3,
        "content": "Create\nApply\nDefine\nPublish"
      },
      {
        "date": "2023-04-04T19:34:00.000Z",
        "voteCount": 2,
        "content": "is corrrrrrect"
      },
      {
        "date": "2023-01-13T06:44:00.000Z",
        "voteCount": 3,
        "content": "the correct sequence is: 1. define the RangeStart and RangeEnd parameters 2. apply customer filter to the Date column 3. define incremental refresh policy 4. publish the model\n\nsource: https://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-configure"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92879-exam-dp-500-topic-1-question-73-discussion/",
    "body": "You develop a solution that uses a Power BI Premium capacity. The capacity contains a dataset that is expected to consume 50 GB of memory.<br>Which two actions should you perform to ensure that you can publish the model successfully to the Power BI service? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestart the capacity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish an initial dataset that is less than 10 GB.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Max Offline Dataset Size setting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvoke a refresh to load historical data based on the incremental refresh policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the complete dataset."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T02:24:00.000Z",
        "voteCount": 3,
        "content": "It's in the docs: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models#enable-large-semantic-models\n\n--&gt; Publish the model as a semantic model to the service.\n--&gt; Invoke a refresh to load historical data based on the incremental refresh policy."
      },
      {
        "date": "2023-04-04T19:44:00.000Z",
        "voteCount": 2,
        "content": "in practice B y D is the best solution"
      },
      {
        "date": "2023-03-28T22:19:00.000Z",
        "voteCount": 1,
        "content": "Chat GPT:\nThe correct options to ensure successful publishing of a Power BI Premium dataset that is expected to consume 50 GB of memory are:\n\nC. Increase the Max Offline Dataset Size setting: Power BI Premium capacity has a default limit of 10 GB for the maximum offline dataset size that can be published to the service. You can increase this limit to accommodate larger datasets. In this case, you should increase the limit to 50 GB to support the dataset.\n\nE. Publish the complete dataset: To ensure that the complete dataset is published successfully, you should not publish an initial dataset that is less than 10 GB, as suggested in option B. Also, restarting the capacity, as suggested in option A, is not necessary in this scenario. Invoking a refresh to load historical data based on the incremental refresh policy, as suggested in option D, is optional and does not affect the publishing process.\n\nTherefore, the correct actions to perform are to increase the Max Offline Dataset Size setting to 50 GB and publish the complete dataset."
      },
      {
        "date": "2023-10-02T23:50:00.000Z",
        "voteCount": 1,
        "content": "C is definitely wrong please read below as per the logic provided below comments in documentation negate it, anyhow a smaller dataset could have a larger in memory size and to support mroe than 10 GB one needs to definitely enable Large Dataset storage format which is not there in options.\nTo safeguard the performance of the system, an additional SKU-specific hard ceiling for max offline dataset size is applied, regardless of the configured value. The additional SKU-specific hard ceiling in the below table does not apply to Power BI datasets stored in large dataset storage format.\n\nEM1/A1\t\tEM2/A2\tEM3/A3\tP1/A4\tP2/A5\tP3/A6\tP4/A7\tP5/A8\nHard ceiling for Max Offline Dataset Size\t\t3 GB\t5 GB\t6 GB\t10 GB\t10 GB\t10 GB\t10 GB\n\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-workloads#max-offline-dataset-size"
      },
      {
        "date": "2023-03-27T22:53:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models"
      },
      {
        "date": "2023-01-13T06:52:00.000Z",
        "voteCount": 2,
        "content": "We need to differentiate between the limit for a dataset and limit for a data model that will be uploaded to the service. As this link (https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models) states, for Premium the option to enable large datasets does not affect the PBI Desktop model upload size, which is still limited to 10 GB. \"Instead, datasets can grow beyond that limit in the service on refresh.\" . In conclusion, until you upload it it has to be smaller than 10GB and afterwards it can grow beyond that. Incremental refresh is a must obviously."
      },
      {
        "date": "2023-02-10T01:49:00.000Z",
        "voteCount": 2,
        "content": "The link you mention also includes the steps to enable a large dataset for a new model published to the service, which are: \n1. Create a model in Power BI Desktop. If your dataset will become larger and progressively consume more memory, be sure to configure Incremental Refresh\n2. Publish the model as a dataset to the service \n3. In the service &gt; dataset &gt; Settings, expand Large dataset storage format, set the slider to On and select Apply\n\nDoesn't that mean that D &amp; E are the correct answers? The incremental refresh is obvious from step 1 but doesn't step 2 mean you should publish the complete dataset? Nowhere in those steps does it say you need to first publish 10 GB of your dataset."
      },
      {
        "date": "2023-10-02T23:55:00.000Z",
        "voteCount": 1,
        "content": "Complete dataset is of 50 GB size so you could publish the complete data set hence E is wrong.\nFirst you publish the dataset where you had configured the incremental refresh in powerbi desktop file and it's size is less than 10 GB which makes option B correct\nThan you do what is mentioned in option D which would make the dataset to consume 50GB of memory maybe but for that you would have to enable large dataset storage format keep that in mind"
      },
      {
        "date": "2023-01-09T21:31:00.000Z",
        "voteCount": 4,
        "content": "I think BD is correct."
      },
      {
        "date": "2022-12-30T21:50:00.000Z",
        "voteCount": 4,
        "content": "BD is correct."
      },
      {
        "date": "2022-12-29T04:42:00.000Z",
        "voteCount": 2,
        "content": "I think D &amp; E are the correct answers. \nSee the section \"Enable large Datasets\" at https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models"
      },
      {
        "date": "2022-12-26T07:26:00.000Z",
        "voteCount": 3,
        "content": "D + B is correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91042-exam-dp-500-topic-1-question-74-discussion/",
    "body": "You have a Power BI workspace named Workspace1 in a Premium capacity. Workspace1 contains a dataset.<br>During a scheduled refresh, you receive the following error message: \u201cUnable to save the changes since the new dataset size of 11,354 MB exceeds the limit of 10,240 MB.\u201d<br>You need to ensure that you can refresh the dataset.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange License mode to Premium per user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Large dataset storage format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect Workspace1 to an Azure Data Lake Storage Gen2 account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the location of the Premium capacity."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-11T10:05:00.000Z",
        "voteCount": 13,
        "content": "Answer is B"
      },
      {
        "date": "2022-12-16T08:49:00.000Z",
        "voteCount": 7,
        "content": "I think is B\nhttps://blog.crossjoin.co.uk/2022/11/07/understanding-the-unable-to-save-the-changes-since-the-new-dataset-of-size-error-in-power-bi/"
      },
      {
        "date": "2023-10-17T09:04:00.000Z",
        "voteCount": 1,
        "content": "yeah but it says, Workspace1 in a Premium capacity. so answer is D"
      },
      {
        "date": "2023-06-26T10:56:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-04-04T19:56:00.000Z",
        "voteCount": 2,
        "content": "b is ok"
      },
      {
        "date": "2023-03-27T23:20:00.000Z",
        "voteCount": 2,
        "content": "The Answer D , would be ok if the topic would explain the region where the workspace was located. Some regions don't support large datasets, and the large dataset storage format option is disabled. \nCheck the list : https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models#region-availability \nBecause the question/case doesn't  express a Region limitation, it should be ok to select opc B, regarding to:  Enable the Large dataset storage format option to use datasets in Power BI Premium that are larger than the default limit.\nLink here: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models"
      },
      {
        "date": "2023-02-14T04:48:00.000Z",
        "voteCount": 1,
        "content": "The message error says \"Unable to save the changes since the new dataset size of 11,354 MB exceeds the limit of 10,240 MB.\", so it means that the limit is already 10GB so the \"large dataset storage format\" is already turned on.\nTherefore the only feasible option is D to change a new SKU with more resources.\nThis article is very clear:\nhttps://blog.crossjoin.co.uk/2022/11/07/understanding-the-unable-to-save-the-changes-since-the-new-dataset-of-size-error-in-power-bi/"
      },
      {
        "date": "2023-02-14T05:32:00.000Z",
        "voteCount": 1,
        "content": "sorry guys, ignore my previous comment.\nThe default dataset size for power bi premium is 10 GB, but enabling the \"large dataset storage format\" option we can have a larger dataset"
      },
      {
        "date": "2023-02-10T01:51:00.000Z",
        "voteCount": 3,
        "content": "I agree with previous comments. Since you're already in a Premium capacity, you should enable the Large dataset storage format. I don't see what the location of the Premium capacity has to do with this as per answer D..."
      },
      {
        "date": "2023-01-11T08:22:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-12-22T06:21:00.000Z",
        "voteCount": 4,
        "content": "It should be B"
      },
      {
        "date": "2022-12-19T05:41:00.000Z",
        "voteCount": 3,
        "content": "Agreed with previous commenters, I think the answer should be B:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92870-exam-dp-500-topic-1-question-75-discussion/",
    "body": "DRAG DROP -<br>You have a Power BI dataset that contains the following measures:<br><br>Budget -<br><br>Actuals -<br><br>Forecast -<br>You create a report that contains 10 visuals.<br>You need provide users with the ability to use a slicer to switch between the measures in two visuals only.<br>You create a dedicated measure named CG Measure Switch.<br>How should you complete the DAX expression for the Actuals measure? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image90.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image91.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-26T06:03:00.000Z",
        "voteCount": 21,
        "content": "I think it should be SELECTEDMEASURENAME and SELECTEDMEASURE. \nSELECTEDVALUE requires at least one parameter and this has to be a column in a table."
      },
      {
        "date": "2023-01-06T13:41:00.000Z",
        "voteCount": 6,
        "content": "SELECTEDMEASURENAME and SELECTEDMEASURE are correct"
      },
      {
        "date": "2023-06-26T10:58:00.000Z",
        "voteCount": 1,
        "content": "SELECTEDMEASURENAME\nSELECTEDMEASURE"
      },
      {
        "date": "2023-06-27T04:48:00.000Z",
        "voteCount": 1,
        "content": "See Q#98"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95046-exam-dp-500-topic-1-question-76-discussion/",
    "body": "HOTSPOT -<br>You are configuring an aggregation table as shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image92.png\"><br>The detail table is named FactSales and the aggregation table is named FactSales(Agg).<br>You need to aggregate SalesAmount for each store.<br>Which type of summarization should you use for SalesAmount and StoreKey? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image93.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image140.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-10-04T05:45:00.000Z",
        "voteCount": 2,
        "content": "SUM \nGROUP BY"
      },
      {
        "date": "2023-08-18T23:18:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Sum\n- GroupBy"
      },
      {
        "date": "2023-04-04T20:11:00.000Z",
        "voteCount": 3,
        "content": "is okkk"
      },
      {
        "date": "2023-01-13T06:57:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91857-exam-dp-500-topic-1-question-77-discussion/",
    "body": "You have a Power BI dataset that uses DirectQuery against an Azure SQL database.<br>Multiple reports use the dataset.<br>A database administrator reports that too many queries are being sent from Power BI to the database.<br>You need to reduce the number of queries sent to the database. The solution must meet the following requirements:<br>DirectQuery must continue to be used.<br>Visual interactions in all the reports must remain as they are configured currently.<br>Consumers of the reports must only be allowed to apply filters from the Filter pane.<br>Which two settings should you select? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a single Apply button to the filter pane to apply changes at once\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd Apply buttons to all basic filters to apply changes when you\u2019re ready",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisabling cross highlighting/filtering by default",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIgnore the Privacy Levels and potentially improve performance",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd an Apply button to each slicer to apply changes when you\u2019re ready\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-31T02:04:00.000Z",
        "voteCount": 8,
        "content": "IMO the answers are A&amp;D.\n\nSince \"Consumers of the reports must only be allowed to apply filters from the Filter pane\", then B &amp; E are wrong.\n\nAlso, there is noted that \"Visual interactions in all the reports must remain as they are configured currently.\" which would be violated by C.\n\nTo support D answer, there is no information that the data contains sensitive information so this change can be considered reasonable: \"Caution: Enabling Fast Combine by selecting Ignore the Privacy levels and potentially improve performance in the Workbook Settings dialog could expose sensitive or confidential data to an unauthorized person. Do not enable Fast Combine unless you are confident that the data source does not contain sensitive or confidential data.\"\n\nSource: https://support.microsoft.com/en-us/office/set-privacy-levels-power-query-cc3ede4d-359e-4b28-bc72-9bee7900b540"
      },
      {
        "date": "2023-07-16T08:59:00.000Z",
        "voteCount": 3,
        "content": "Since there is only one data source, the SQL database, there is nothing to combine considering privacy levels. So how could fast combine reduce queries if there are no sources to combine?"
      },
      {
        "date": "2023-08-18T23:18:00.000Z",
        "voteCount": 7,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Add a single Apply button to the filter pane to apply changes at once\n- Add an Apply button to each slicer to apply changes when you\u2019re ready"
      },
      {
        "date": "2024-02-07T22:12:00.000Z",
        "voteCount": 1,
        "content": "A and C \nD can be taken  because there is a single source \nB, E - Consumers of the reports must only be allowed to apply filters from the Filter pane\","
      },
      {
        "date": "2023-08-06T08:20:00.000Z",
        "voteCount": 2,
        "content": "A. Add a single Apply button to the filter pane to apply changes at once. This will prevent Power BI from sending multiple queries to the database as users interact with the filter pane. Instead, Power BI will only send one query when the user clicks the Apply button.\n\nC. Disabling cross highlighting/filtering by default. Cross highlighting and filtering are features that allow users to filter visuals based on the selection in other visuals. When these features are enabled, Power BI sends multiple queries to the database as users interact with the visuals. Disabling these features will reduce the number of queries sent to the database."
      },
      {
        "date": "2023-07-13T21:22:00.000Z",
        "voteCount": 1,
        "content": "I so Wonder how people Vote for E. Since requirements is \"Consumers of the reports must only be allowed to apply filters from the Filter pane.\" there shouldn't be slicers in the solution at all. For me, AB is the only remaining reasonable solution."
      },
      {
        "date": "2023-06-26T11:08:00.000Z",
        "voteCount": 3,
        "content": "A and E are the logical answers."
      },
      {
        "date": "2023-06-14T00:16:00.000Z",
        "voteCount": 3,
        "content": "A. Add a single Apply button to the filter pane to apply changes at once: By adding a single Apply button to the filter pane, users can apply all the selected filters at once, rather than each filter triggering a separate query. This helps reduce the number of queries sent to the database.\n\nE. Add an Apply button to each slicer to apply changes when you're ready: By adding an Apply button to each slicer, users can choose when to apply the changes made through the slicer. This way, the queries are only sent to the database when the users explicitly click the Apply button, reducing the overall number of queries."
      },
      {
        "date": "2023-04-06T03:36:00.000Z",
        "voteCount": 1,
        "content": "I will agree with ajaysharma2061 because option e: says \"Add an Apply button to each slicer to apply changes when you\u2019re ready\" \nIf this sentece were \"Add an Apply button to ALL slicer to apply changes when you\u2019re ready\" i'll select this\nIn this case opcion c is better than e"
      },
      {
        "date": "2023-04-06T03:39:00.000Z",
        "voteCount": 1,
        "content": "Sorry, with opcion c I broke the interaction between charts and keep this is a requirement"
      },
      {
        "date": "2023-03-29T11:33:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A and C.\n\nA. Add a single Apply button to the filter pane to apply changes at once: By adding a single Apply button to the filter pane, users can apply all the selected filters at once, reducing the number of queries sent to the database.\n\nC. Disabling cross highlighting/filtering by default: Disabling cross-highlighting and filtering by default will limit the queries generated by visual interactions.\n\nB and E are incorrect as they suggest adding Apply buttons to filters or slicers, which may cause more queries to be sent to the database as users interact with them.\n\nD is also incorrect as ignoring privacy levels can potentially create security issues and is not related to reducing the number of queries sent to the database."
      },
      {
        "date": "2023-04-28T01:32:00.000Z",
        "voteCount": 2,
        "content": "C changes the visual interactions which is supposed to remain unchanged as per requirements so its incorrect"
      },
      {
        "date": "2023-03-28T00:35:00.000Z",
        "voteCount": 2,
        "content": "Reduce number of queries sent by : Slicers (E)  and Filters (A) \nIn that way you can improve the performance\nhttps://www.datapears.com/post/power-bi-query-reduction-add-an-apply-all-filters-button-to-your-report"
      },
      {
        "date": "2023-03-19T23:51:00.000Z",
        "voteCount": 1,
        "content": "Options B and E, adding Apply buttons to basic filters and slicers respectively, may help reduce the number of queries sent to the database, but they don't meet the requirement of allowing consumers to only apply filters from the Filter pane.\n\nOption D, ignoring privacy levels, is not related to reducing the number of queries sent to the database. It's a security setting that determines how Power BI handles data sources with different levels of privacy."
      },
      {
        "date": "2023-03-19T23:54:00.000Z",
        "voteCount": 2,
        "content": "A. Add a single Apply button to the filter pane to apply changes at once\nC. Disabling cross highlighting/filtering by default\n\nExplanation:\n\nAdding a single Apply button to the filter pane allows consumers of the reports to apply filters from the Filter pane, rather than sending queries to the database every time a filter is changed. This will help reduce the number of queries sent to the database.\n\nDisabling cross highlighting/filtering by default also helps reduce the number of queries sent to the database. Cross highlighting/filtering is a feature that allows interactions between visuals in a report. By disabling this feature, Power BI won't send queries to the database for each interaction, which can help improve performance."
      },
      {
        "date": "2023-03-19T23:55:00.000Z",
        "voteCount": 1,
        "content": "BTW: above answer was given by Chat GPT..."
      },
      {
        "date": "2023-02-10T04:35:00.000Z",
        "voteCount": 1,
        "content": "I don't understand how the answer most people select could be D when the question says you should reduce the number of queries sent to the database. Changing the privacy levels doesn't affect the number of queries sent to the database does it?"
      },
      {
        "date": "2023-02-10T02:36:00.000Z",
        "voteCount": 4,
        "content": "The right answer is A and E.\nA: because we send just a query when all the filters are set.\nE: slicers are not included in the single Apply button to the filter pane, so we must put an extra button to run the slicers once they are set.\n\nWe cannot select the option C because the requirement says that \"the visual interactions in all the reports must remain as they are configured currently\", so interaction between the visual must exist\nThe option D does not reduce the number of queries sent to the source."
      },
      {
        "date": "2023-02-08T01:09:00.000Z",
        "voteCount": 1,
        "content": "A for sure because as a result only one query will be sent to AAS model instead of multiple.\n\nD because When privacy levels are set on a data source, queries from Power BI to the data source have to be evaluated to ensure that only authorized data is returned. This process can add extra time to the query execution and impact performance. In some cases, having many privacy levels or complex privacy level evaluations can have a significant impact on performance. However, it is important to maintain privacy levels to ensure the protection of sensitive data. If performance is a concern, other methods such as optimizing the data model or using DirectQuery or Live Connection instead of Import can help improve performance while still maintaining privacy levels."
      },
      {
        "date": "2023-02-10T02:30:00.000Z",
        "voteCount": 1,
        "content": "You are wrong, the requirement is to reduce the number of queries sent to the database and not the spent time for each query."
      },
      {
        "date": "2023-01-26T00:46:00.000Z",
        "voteCount": 2,
        "content": "These are the one that make more sense...tricky though"
      },
      {
        "date": "2023-01-10T20:41:00.000Z",
        "voteCount": 2,
        "content": "answer is correct A,E"
      },
      {
        "date": "2022-12-26T07:33:00.000Z",
        "voteCount": 3,
        "content": "Consumers of the reports must only be allowed to apply filters from the Filter pane. So E cannot be correct. I'd pick A &amp; C"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92565-exam-dp-500-topic-1-question-78-discussion/",
    "body": "DRAG DROP -<br>You have a Power BI dataset that contains two tables named Table1 and Table2. The dataset is used by one report.<br>You need to prevent project managers from accessing the data in two columns in Table1 named Budget and Forecast.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image95.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image96.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-26T07:35:00.000Z",
        "voteCount": 18,
        "content": "1. Create Role\n2. Open TE\n3. Set table to Read\n4. Set Column to None"
      },
      {
        "date": "2022-12-29T05:06:00.000Z",
        "voteCount": 3,
        "content": "I agree with these steps.\nRef: https://www.fourmoo.com/2021/03/02/configure-ols-in-power-bi-using-tabular-editor-to-limit-access-to-non-financial-measures/"
      },
      {
        "date": "2023-02-10T02:52:00.000Z",
        "voteCount": 3,
        "content": "I agree with you.\nWe can see a good example in this video:\nhttps://www.youtube.com/watch?v=PAX5GP9SkTA"
      },
      {
        "date": "2023-06-26T11:14:00.000Z",
        "voteCount": 1,
        "content": "From jeroen12345:\n1. Create Role\n2. Open TE\n3. Set table to Read\n4. Set Column to None"
      },
      {
        "date": "2023-04-06T04:06:00.000Z",
        "voteCount": 1,
        "content": "PBI: Create Role\nPBI: filter role\nTE open\nTE: set col to none"
      },
      {
        "date": "2023-03-28T01:56:00.000Z",
        "voteCount": 2,
        "content": "The Answer it is ok, I only disagree with the order\nlink: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-ols?tabs=column \n1. Create  the role\n2. Add a DAX Filter to the PM Role\n3. Open Tabular Editor\n4. OLS -&gt; Set Column to None (It allows PM see the table, but not the specific columns required)"
      },
      {
        "date": "2023-08-15T10:40:00.000Z",
        "voteCount": 1,
        "content": "I agree with solref and DarioReymago.\nCreate the Role\nFilter the role\nOpen Tabular Editor \nSet Budget and Forecast column to none."
      },
      {
        "date": "2022-12-23T04:42:00.000Z",
        "voteCount": 1,
        "content": "I would replace, Tabular Editor with Dax Studio in the given answer."
      },
      {
        "date": "2022-12-25T14:33:00.000Z",
        "voteCount": 4,
        "content": "It won't be DAX Studio as you can only implement (OLS) through Tabular Editor"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92789-exam-dp-500-topic-1-question-79-discussion/",
    "body": "You have a Power BI data model.<br>You need to refresh the data from the source every 15 minutes.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a scheduled refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the storage mode of the dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the XMLA endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefine an incremental refresh policy."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-25T14:34:00.000Z",
        "voteCount": 9,
        "content": "You cannot schedule a refresh on 15-minute increments. Only 30, unless using XMLA endpoint."
      },
      {
        "date": "2022-12-30T06:24:00.000Z",
        "voteCount": 1,
        "content": "How do you know it's increment?"
      },
      {
        "date": "2023-01-26T08:34:00.000Z",
        "voteCount": 1,
        "content": "You need ro refresh it every 15 minute. Meaning its incremental."
      },
      {
        "date": "2023-03-28T02:14:00.000Z",
        "voteCount": 1,
        "content": "You are right!"
      },
      {
        "date": "2024-02-27T11:33:00.000Z",
        "voteCount": 1,
        "content": "it is XMLA. \n\"If the semantic model resides on a Premium capacity, you can schedule up to 48 refreshes per day in the semantic model settings. For more information, see Configure scheduled refresh later in this article. Semantic models on a Premium capacity with the XMLA endpoint enabled for read-write support unlimited refresh operations when configured programmatically with TMSL or PowerShell.\"\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/refresh-data"
      },
      {
        "date": "2023-10-04T06:18:00.000Z",
        "voteCount": 2,
        "content": "It didn't say incremental refresh. therefore, it can't ne XMLA. I go for option A"
      },
      {
        "date": "2023-06-26T11:18:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer."
      },
      {
        "date": "2023-04-06T04:13:00.000Z",
        "voteCount": 2,
        "content": "B or C, opcion c is better"
      },
      {
        "date": "2023-04-06T00:37:00.000Z",
        "voteCount": 3,
        "content": "Chat GPT\nA. Configure a scheduled refresh.\nTo refresh data from the source every 15 minutes, you need to configure a scheduled refresh for your Power BI data model. The scheduled refresh setting allows you to define how often the data should be refreshed from the source, and you can set the frequency to as low as 15 minutes.\nOption B, changing the storage mode of the dataset, is not relevant to the issue of refreshing the data from the source every 15 minutes.\nOption C, enabling the XMLA endpoint, is not necessary to refresh the data from the source every 15 minutes.\nOption D, defining an incremental refresh policy, is not necessary to refresh the data from the source every 15 minutes. Incremental refresh is useful when you have a large dataset and want to reduce the data processing time by only updating the data that has changed since the last refresh."
      },
      {
        "date": "2023-04-04T00:24:00.000Z",
        "voteCount": 1,
        "content": "This question is talking about PowerBI Desktop, all the responses are talking about the service."
      },
      {
        "date": "2023-04-06T04:09:00.000Z",
        "voteCount": 2,
        "content": "I'm desagree, it say PowerBI , not PowerBI Desktop"
      },
      {
        "date": "2023-03-28T02:13:00.000Z",
        "voteCount": 1,
        "content": "Refresh operations through the XMLA endpoint are not limited to 48 refreshes per day, and the scheduled refresh timeout is not imposed.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools \nhttps://www.youtube.com/watch?v=YEIKzeNCqGg"
      },
      {
        "date": "2023-01-07T01:05:00.000Z",
        "voteCount": 3,
        "content": "If the dataset resides on a Premium capacity, you can schedule up to 48 refreshes per day in the dataset settings. For more information, see Configure scheduled refresh later in this article. Datasets on a Premium capacity with the XMLA endpoint enabled for read-write support unlimited refresh operations when configured programmatically with TMSL or PowerShell.\n\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/refresh-data"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93142-exam-dp-500-topic-1-question-80-discussion/",
    "body": "You have a dataset that contains a table named UserPermissions. UserPermissions contains the following data.<br><img src=\"https://img.examtopics.com/dp-500/image97.png\"><br>You plan to create a security role named User Security for the dataset.<br>You need to filter the dataset based on the current users.<br>What should you include in the DAX expression?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[UserPermissions] = USERPRINCIPALNAME()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[User] = USERPRINCIPALNAME()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[User] = USEROBJECTID()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[UserPermissions] = USERNAME()",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[User] = USERNAME()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T11:26:00.000Z",
        "voteCount": 3,
        "content": "E is correct answer. \n\nhttps://dax.guide/username/"
      },
      {
        "date": "2023-01-16T02:52:00.000Z",
        "voteCount": 1,
        "content": "I agree but thcnically you could use B as well because: \"You can use username() within this expression. Be aware that username() has the format of DOMAIN\\username within Power BI Desktop. Within the Power BI service and Power BI Report Server, it's in the format of the user's User Principal Name (UPN). Alternatively, you can use userprincipalname(), which always returns the user in the format of their user principal name, username@contoso.com.\"\n\nsource: https://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-rls"
      },
      {
        "date": "2022-12-29T11:26:00.000Z",
        "voteCount": 2,
        "content": "Answer E is correct"
      },
      {
        "date": "2022-12-29T03:07:00.000Z",
        "voteCount": 1,
        "content": "Please let me know answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92566-exam-dp-500-topic-1-question-81-discussion/",
    "body": "You have an Azure Synapse Analytics serverless SQL pool.<br>You need to catalog the serverless SQL pool by using Azure Purview.<br>Which three actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed identity in Azure Active Directory (Azure AD).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Storage Blob Data Reader role to the Azure Purview managed service identity (MSI) for the storage account associated to the Synapse Analytics workspace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Reader role to the Azure Purview managed service identity (MSI) for the Synapse Analytics workspace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the Owner role to the Azure Purview managed service identity (MSI) for the Azure Purview resource group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister a data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BCE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCE",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-23T04:51:00.000Z",
        "voteCount": 12,
        "content": "The Answer should be, in order:\nE. Register a data source\nC. Set the Reader role and enter your Microsoft Purview account name.\nB. Set the Storage blob data reader role and enter your Microsoft Purview account name.\n\nSource: https://learn.microsoft.com/en-us/azure/purview/register-scan-synapse-workspace?tabs=MI#authentication-for-enumerating-serverless-sql-database-resources"
      },
      {
        "date": "2024-02-02T15:02:00.000Z",
        "voteCount": 1,
        "content": "The answer is E, B, C\nhttps://microsoftlearning.github.io/mslearn-synapse/Instructions/Labs/10-Synapse-purview.html"
      },
      {
        "date": "2023-08-18T23:20:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Assign the Storage Blob Data Reader role to the Azure Purview managed service identity (MSI) for the storage account associated to the Synapse Analytics workspace.\n- Assign the Reader role to the Azure Purview managed service identity (MSI) for the Synapse Analytics workspace.\n- Register a data source."
      },
      {
        "date": "2023-06-27T07:19:00.000Z",
        "voteCount": 1,
        "content": "BCE\n\nNot A, because purview MSI is created by using The Microsoft Purview MSI Configuration script.https://learn.microsoft.com/en-us/azure/purview/tutorial-msi-configuration#prerequisites"
      },
      {
        "date": "2023-06-26T11:32:00.000Z",
        "voteCount": 2,
        "content": "B, C and E are correct. \nDetails are listed here: https://learn.microsoft.com/en-us/azure/purview/register-scan-synapse-workspace?tabs=MI"
      },
      {
        "date": "2023-04-12T00:32:00.000Z",
        "voteCount": 1,
        "content": "THe Answer should be ABE\nNote that assigning the Reader role to the Azure Purview managed service identity (MSI) for the Synapse Analytics workspace (option C) and the Owner role to the Azure Purview managed service identity (MSI) for the Azure Purview resource group (option D) are not required to catalog the serverless SQL pool in Azure Purview."
      },
      {
        "date": "2023-03-28T03:09:00.000Z",
        "voteCount": 2,
        "content": "B. C. E \nhttps://learn.microsoft.com/en-us/azure/purview/register-scan-synapse-workspace?tabs=MI#azure-synapse-serverless-database"
      },
      {
        "date": "2022-12-23T04:53:00.000Z",
        "voteCount": 2,
        "content": "Managed Identity is optional, you can also set service principal or SQL Authentication:\nhttps://learn.microsoft.com/en-us/azure/purview/register-scan-synapse-workspace?tabs=SQLAuth#apply-permissions-to-scan-the-contents-of-the-workspace"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93143-exam-dp-500-topic-1-question-82-discussion/",
    "body": "HOTSPOT -<br>You need to recommend an automated solution to monitor Power BI user activity. The solution must meet the following requirements:<br>Security admins must identify when users export reports from Power BI within five days of a new sensitivity label being applied to the artifacts in Power BI.<br>Power BI admins must identify updates or changes to the Power BI capacity.<br>The principle of least privilege must be used.<br>Which log should you include in the recommendation for each group? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image98.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image99.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-03-28T05:13:00.000Z",
        "voteCount": 5,
        "content": "Both cases are \"PBI Activity log\" \nPlease check it on: \nhttps://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"
      },
      {
        "date": "2023-03-29T11:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with you. Power BI Activity log captures all the information regarding Power BI such as export, view etc activities. Unfied Audit Log for O365 needed for other O365 Services such as Word, Excel etc."
      },
      {
        "date": "2023-12-22T02:07:00.000Z",
        "voteCount": 1,
        "content": "agree.\n\"Power BI audit logs include sensitivity label information about activities such as applying, removing, and changing labels, as well as about activities such as viewing reports, dashboards, etc. This gives Power BI and security admins visibility over sensitive data consumption\"\n\nSince Power BI activity log has least privilege than Unified Audit log O365, it's the right answer for both cases.\n\nsee link:\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-security-sensitivity-label-overview"
      },
      {
        "date": "2023-06-26T11:43:00.000Z",
        "voteCount": 1,
        "content": "1. Unified Audit log - least priv required, logs sensitivity labelling, therefore only users with Audit Log permissions (GA/auditors) can access.\n2. PBI Activity log - only logs PBI audit events\n\nhttps://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing#choosing-a-log-source"
      },
      {
        "date": "2024-03-21T04:28:00.000Z",
        "voteCount": 1,
        "content": "The Power BI activity log also provides logs on sensitivity labels, as well as exports.\n\n\"Whenever a sensitivity label on a semantic model, report, dashboard, or dataflow is applied, changed, or removed, that activity is recorded in the audit log for Power BI. You can track these activities in the unified audit log or in the Power BI activity log. \"\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-security-sensitivity-label-overview"
      },
      {
        "date": "2023-04-16T01:53:00.000Z",
        "voteCount": 1,
        "content": "I think, given answer is correct."
      },
      {
        "date": "2023-04-07T20:18:00.000Z",
        "voteCount": 1,
        "content": "weird question, both can be solve by PBI Activity log &amp; unified"
      },
      {
        "date": "2023-01-23T03:09:00.000Z",
        "voteCount": 2,
        "content": "I would go with Power Bi activity log for the first one because exporting look like a type of activity that you could filter (see here https://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing). And for the second one I would go with unified audit log in Microsoft 365 as capacities for Premium are out of PBI scope (just as PBI capacity admins cannot change them but global admins can)."
      },
      {
        "date": "2023-01-10T21:43:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is correct, \n365 Audit is more about user activities, and power bi log is about the capacity and BI service related log\n\nhttps://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing#use-the-activity-log"
      },
      {
        "date": "2023-01-07T01:28:00.000Z",
        "voteCount": 3,
        "content": "Power BI Admin: Power BI Activity Log\n\nhttps://learn.microsoft.com/en-us/power-bi/admin/service-admin-auditing"
      },
      {
        "date": "2023-01-06T10:26:00.000Z",
        "voteCount": 4,
        "content": "You might want to actually look up some of these on your own and not just expect everyone else to look it up for you and report back."
      },
      {
        "date": "2022-12-29T03:10:00.000Z",
        "voteCount": 1,
        "content": "Is option correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92413-exam-dp-500-topic-1-question-83-discussion/",
    "body": "You have a Power BI workspace that contains one dataset and four reports that connect to the dataset.<br>The dataset uses import storage mode and contains the following data source:<br>A CSV file in an Azure Storage account.<br>An Azure Database for PostgreSQL database.<br>You plan to use deployment pipelines to promote the content from development to test to production. There will be different data source locations for each stage.<br>What should you include in the deployment pipeline to ensure that the appropriate data source locations are used during each stage?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tauto-binding across pipelines",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata source rules",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tselective deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tparameter rules\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-30T22:17:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is D and here is why:\nIn the best practices for deployment, there is written that you can use both data source rules and parameters as well, however, they recommend parameters.\n\nSource: https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-best-practices#use-parameters-in-your-model\n\nAlso, data source rules is supported for limited data sources where I can not find Azure storage Account so I think data source won't be applicable to our case.\nSource: https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started#supported-data-sources-for-dataflow-and-dataset-rules"
      },
      {
        "date": "2022-12-21T23:15:00.000Z",
        "voteCount": 5,
        "content": "D is the answer. This is based on what I read from https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-best-practices"
      },
      {
        "date": "2023-02-10T04:21:00.000Z",
        "voteCount": 2,
        "content": "The option D (parameter rules) in the right answer.\nData source rules work for some specific sources only, below you can see the supported data sources for data set rules:\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-create-rules#supported-data-sources-for-dataflow-and-dataset-rules"
      },
      {
        "date": "2023-01-23T03:14:00.000Z",
        "voteCount": 1,
        "content": "Step 4 from this link https://learn.microsoft.com/en-us/power-bi/create-reports/deployment-pipelines-get-started makes me think we should go with D."
      },
      {
        "date": "2023-03-28T05:36:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the link :)"
      },
      {
        "date": "2022-12-25T14:47:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is B. Data Source rules are used to switch datasources when using Deployment Pipelines. Parameter Rules only work when you have created parameters within the Power BI Desktop File (.pbix)"
      },
      {
        "date": "2022-12-29T07:20:00.000Z",
        "voteCount": 2,
        "content": "Having cross-check this again, I'm leaning towards B too. Will be good to see what others think too."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92041-exam-dp-500-topic-1-question-84-discussion/",
    "body": "You are planning a Power BI solution for a customer.<br>The customer will have 200 Power BI users. The customer identifies the following requirements:<br>Ensure that all the users can create paginated reports.<br>Ensure that the users can create reports containing AI visuals.<br>Provide autoscaling of the CPU resources during heavy usage spikes.<br>You need to recommend a Power BI solution for the customer. The solution must minimize costs.<br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Power BI Premium per capacity\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI Report Server",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI Premium per user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI Pro per user"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T02:15:00.000Z",
        "voteCount": 6,
        "content": "https://powerbi.microsoft.com/en-au/pricing/\n\n\"Enable autoscale with your Azure subscription to automatically scale Power BI Premium capacity.\""
      },
      {
        "date": "2024-01-21T09:00:00.000Z",
        "voteCount": 1,
        "content": "This question is incorrect itself. You still need 200 pro licenses with Premium Capacity if people are building reports. That would make you lean to PPU, but it doesn't have autoscale (because it uses shared capacity)."
      },
      {
        "date": "2023-08-04T19:39:00.000Z",
        "voteCount": 3,
        "content": "Answer is A - Autoscale is not available with PPU"
      },
      {
        "date": "2023-08-04T00:34:00.000Z",
        "voteCount": 2,
        "content": "Power BI Premium per capacity provides dedicated resources for the entire organization, which can be beneficial when you have a large number of users who need access to the same Premium features. It allows for greater control and flexibility in managing resources and workloads across the organization.\n\nHowever, if the customer has 200 Power BI users and they all require access to paginated reports and AI visuals, Power BI Premium per user (option C) would be more cost-effective. With Power BI Premium per user, each individual user gets access to the Premium features, and the customer only pays for the licenses needed for the 200 users.\n\nIn summary, both options (Power BI Premium per capacity and Power BI Premium per user) can meet the stated requirements, but option C (Power BI Premium per user) is likely to be the more cost-effective choice for a customer with 200 Power BI users."
      },
      {
        "date": "2023-06-28T03:20:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2023-06-27T09:59:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-faq#how-can-i-control-the-costs-of-autoscaling-\n\n\"Autoscaling is an optional feature of Power BI Premium\""
      },
      {
        "date": "2023-03-28T05:50:00.000Z",
        "voteCount": 3,
        "content": "https://powerbi.microsoft.com/en-au/pricing/#premium-add-on-card-autoscale \nPremium capacity enables customers to automatically add compute capacity to avoid slowdowns under heavy use, using Autoscale."
      },
      {
        "date": "2023-02-10T04:47:00.000Z",
        "voteCount": 2,
        "content": "Power BI Premium per user supports autoscaling compute capacity (It's supported for all Power BI premiums):\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-auto-scale\n\nAlso, it supports until 200 users, and it\u00b4s cheaper than Power BI Premium per capacity"
      },
      {
        "date": "2024-04-09T04:53:00.000Z",
        "voteCount": 1,
        "content": "Power BI Premium Per User (PPU):\n\nPaginated Reports: Power BI Premium Per User (PPU) allows all users to create and consume paginated reports, which are a premium feature not included in Power BI Pro.\nAI Visuals: PPU includes access to AI visuals such as key influencers, decomposition trees, and anomaly detection, enabling users to leverage advanced analytics capabilities.\nAutoscaling and Cost Savings: PPU provides autoscaling of CPU resources based on usage demands, ensuring optimal performance during peak times while scaling down during lighter usage periods.\nCost-Effective: PPU is priced per user per month, making it cost-effective for organizations with a moderate number of users like your customer (200 users)."
      },
      {
        "date": "2022-12-31T02:27:00.000Z",
        "voteCount": 4,
        "content": "Because of auto scale, Premium Per Capacity must be selected"
      },
      {
        "date": "2022-12-25T14:49:00.000Z",
        "voteCount": 3,
        "content": "It's better to use PPU when it's less than 250 users. Answer C is correct."
      },
      {
        "date": "2022-12-25T14:50:00.000Z",
        "voteCount": 4,
        "content": "Scratch that. Because of autoscale A is correct."
      },
      {
        "date": "2022-12-18T22:15:00.000Z",
        "voteCount": 3,
        "content": "Reference  : https://powerbi.microsoft.com/en-au/pricing/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92568-exam-dp-500-topic-1-question-85-discussion/",
    "body": "HOTSPOT -<br>You need to configure a source control solution for Azure Synapse Analytics. The solution must meet the following requirements:<br>Code must always be merged to the main branch before being published, and the main branch must be used for publishing resources.<br>The workspace templates must be stored in the publish branch.<br>A branch named dev123 will be created to support the development of a new feature.<br>What should you do? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image101.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image102.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-07-10T10:06:00.000Z",
        "voteCount": 8,
        "content": "Set the collaboration branch to: main \nBase the dev123 branch on: main (you can not base dev123 on the Publish branch)"
      },
      {
        "date": "2022-12-31T05:40:00.000Z",
        "voteCount": 7,
        "content": "main &amp; workspace_publish.\n\nI think the answer is correct.\n\n\"By default, Synapse Studio generates the workspace templates and saves them into a branch called workspace_publish.\"\n\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control#configure-publishing-settings"
      },
      {
        "date": "2023-08-18T01:42:00.000Z",
        "voteCount": 3,
        "content": "main &amp; main\nYou will usually always want to base a feature branch off the main branch. The publish branch does not contain any of the actual resources, just a template for deployment."
      },
      {
        "date": "2023-04-16T02:17:00.000Z",
        "voteCount": 4,
        "content": "I think answer is main and main. Collaboration branch is main and dev branch should be based on main know. Why do we need to base it on publish branch generated by workspace?"
      },
      {
        "date": "2023-04-11T03:29:00.000Z",
        "voteCount": 4,
        "content": "main &amp; workspace_publish\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control#creating-feature-branches"
      },
      {
        "date": "2022-12-23T04:56:00.000Z",
        "voteCount": 5,
        "content": "I think the answer should be:\n- main\n- publish\nAs the question states:\nThe workspace templates must be stored in the publish branch."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93144-exam-dp-500-topic-1-question-86-discussion/",
    "body": "You have five Power BI reports that contain R script data sources and R visuals.<br>You need to publish the reports to the Power BI service and configure a daily refresh of datasets.<br>What should you include in the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Power BI Embedded capacity",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta workspace that connects to an Azure Data Lake Storage Gen2 account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan on-premises data gateway (personal mode)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan on-premises data gateway (standard mode)"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-30T22:55:00.000Z",
        "voteCount": 8,
        "content": "To schedule refresh of your R visuals or dataset, enable scheduled refresh and install an on-premises data gateway (personal mode) on the computer containing the workbook and R.\n\nSource: https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-r-in-query-editor#considerations-and-limitations"
      },
      {
        "date": "2023-01-17T20:34:00.000Z",
        "voteCount": 4,
        "content": "C is correct.\n\n*On-premise data gateway (personal mode): 1 user, no sharing with other users, for you to publish reports without sharing the data source with others. Allows only A specific user to manage data within the data source. It means other users Can\u2019t connect to it.\n\n*On-premise data gateway (standard mode): allow a team of users to connect, ideal for a large number of users using the reports and need to acces to the data sources."
      },
      {
        "date": "2022-12-29T03:17:00.000Z",
        "voteCount": 1,
        "content": "Is this correct ?"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94759-exam-dp-500-topic-1-question-87-discussion/",
    "body": "You have a 2-GB Power BI dataset.<br>You need to ensure that you can redeploy the dataset by using Tabular Editor. The solution must minimize how long it will take to apply changes to the dataset from powerbi.com.<br>Which two actions should you perform in powerbi.com? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable service principal authentication for read-only admin APIs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect the target workspace to an Azure Data Lake Storage Gen2 account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn on Large dataset storage format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable XMLA read-write.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-28T09:24:00.000Z",
        "voteCount": 1,
        "content": "Enable XMLA read-write to connect the dataset to Tabular Editor\nEnable the large dataset format to get better performance, as this is recommended even for datasets that do not qualify as large (10GB)\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools"
      },
      {
        "date": "2023-06-27T04:21:00.000Z",
        "voteCount": 3,
        "content": "C and D are correct answers."
      },
      {
        "date": "2023-02-10T06:21:00.000Z",
        "voteCount": 3,
        "content": "You need XMLA read-write for Tabular Editor for metadata operations: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools\n\nAnd as pointed out below, you need to enable the Large dataset storage format when using XMLA endpoint write operations"
      },
      {
        "date": "2023-01-10T22:42:00.000Z",
        "voteCount": 3,
        "content": "correct\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-large-models"
      },
      {
        "date": "2023-01-23T03:21:00.000Z",
        "voteCount": 2,
        "content": "The part after \"even\" from that link removes all doubt considering that the dataset in question is only 2GB: While required for datasets to grow beyond 10 GB, enabling the Large dataset storage format setting has other benefits. If you're planning to use XMLA endpoint-based tools for dataset write operations, be sure to enable the setting, even for datasets that you wouldn't necessarily characterize as a large dataset. When enabled, the large dataset storage format can improve XMLA write operations performance."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93145-exam-dp-500-topic-1-question-88-discussion/",
    "body": "You have new security and governance protocols for Power BI reports and datasets. The new protocols must meet the following requirements:<br>New reports can be embedded only in locations that require authentication.<br>Live connections are permitted only for workspaces that use Premium capacity datasets.<br>Which three actions should you recommend performing in the Power BI Admin portal? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Embed Codes, delete all the codes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Capacity settings, set XMLA Endpoint to Read Write.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Premium per user settings, set XMLA Endpoint to Off.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tenant settings, set Publish to web to Disable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ADE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ADE",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "ABE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BDE",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "CDE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-29T11:45:00.000Z",
        "voteCount": 7,
        "content": "Only A, D, E makes sense"
      },
      {
        "date": "2023-01-04T11:35:00.000Z",
        "voteCount": 6,
        "content": "I changed m answer. B would not be correct because if you disable XMLA then you won't be able to embed the reports in the locations that require authentication. Plus, when the user uses Analyze service feature he/she goes authorization"
      },
      {
        "date": "2023-08-06T12:31:00.000Z",
        "voteCount": 4,
        "content": "The correct answers are A, B, and E.\n\nA. From Embed Codes, delete all the codes. This will prevent users from embedding reports in locations that do not require authentication.\nB. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets. This will prevent users from connecting to datasets using XMLA endpoints, which is a way to programmatically access data in Power BI.\nE. From Tenant settings, set Publish to web to Disable. This will prevent users from publishing reports to the web, which is a way to make reports accessible to anyone with an internet connection."
      },
      {
        "date": "2023-08-06T12:31:00.000Z",
        "voteCount": 1,
        "content": "The other options are incorrect.\n\nC. From Capacity settings, set XMLA Endpoint to Read Write. This is not necessary, as the Allow XMLA endpoints setting in Tenant settings will control whether or not XMLA endpoints are enabled for all capacities.\nD. From the Premium per user settings, set XMLA Endpoint to Off. This is not necessary, as the Allow XMLA endpoints setting in Tenant settings will control whether or not XMLA endpoints are enabled for all capacities, including Premium per user capacities."
      },
      {
        "date": "2023-03-29T11:22:00.000Z",
        "voteCount": 4,
        "content": "Correct Answers: B, D , E\n\nBased on the requirements, the following three actions should be recommended in the Power BI Admin portal:\n\nB. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets. This will restrict the use of XMLA endpoints and prevent users from analyzing on-premises datasets in Excel.\n\nD. From the Premium per user settings, set XMLA Endpoint to Off. This will ensure that users with Premium per user licenses cannot use XMLA endpoints.\n\nE. From Tenant settings, set Publish to web to Disable. This will prevent reports and dashboards from being shared publicly using Publish to web, ensuring that they are only embedded in locations that require authentication.\n\nTherefore, the correct answers are B (disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets), D (set XMLA Endpoint to Off for Premium per user settings), and E (set Publish to web to Disable in Tenant settings)."
      },
      {
        "date": "2023-03-21T23:12:00.000Z",
        "voteCount": 1,
        "content": "The requirements state that new reports can only be embedded in authenticated locations and live connections should only be allowed for workspaces with Premium capacity datasets. To meet these requirements, the following actions should be recommended in the Power BI Admin portal:\n\nB. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets: This will disable the ability to use XMLA endpoints and analyze on-premises datasets, which helps to ensure that data is only accessible through authenticated channels.\n\nC. From Capacity settings, set XMLA Endpoint to Read Write: This will allow read and write access to the XMLA endpoint for Premium capacity workspaces, which can help to ensure that data is only accessible through authenticated channels.\n\nE. From Tenant settings, set Publish to web to Disable: This will disable the ability to publish reports to the web, which can help to ensure that data is only accessible through authenticated channels."
      },
      {
        "date": "2023-03-21T23:12:00.000Z",
        "voteCount": 1,
        "content": "Therefore, the recommended actions are:\n\nB. From Tenant settings, disable Allow XMLA endpoints and Analyze in Excel with on-premises datasets.\nC. From Capacity settings, set XMLA Endpoint to Read Write.\nE. From Tenant settings, set Publish to web to Disable."
      },
      {
        "date": "2023-03-21T23:13:00.000Z",
        "voteCount": 1,
        "content": "BTW: These answers were given by Chat GPT"
      },
      {
        "date": "2023-03-29T11:22:00.000Z",
        "voteCount": 1,
        "content": "Please ask ChatGPT Again that are you sure and this will change the answer. You can try two three times, till you get the right answer ;)"
      },
      {
        "date": "2023-02-16T06:24:00.000Z",
        "voteCount": 1,
        "content": "C,D,E\n\nC - I think that the XMLA Endopoint property must be enabled read-write by the capacity admin (by default this is set to read only):\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#security\nAlso, you can see here that \"Allow XMLA endpoints and Analyze in Excel with on-premises datasets\" option should be enabled, so the option B is not correct.\n\nD - As the question says \"Live connections are permitted only for workspaces that use Premium capacity datasets\" then we should disable the XMLA Endpoint for Premium per user settings\n\nE - Set Publish to web to disable will avoid to publish to public webs, also when this option is disabled then all the existing Embed Codes are disabled too, so it's not necessary to delete them (Option A would be optional in this case)\nhttps://learn.microsoft.com/en-us/power-bi/admin/service-admin-portal-embed-codes#disable-embed-codes"
      },
      {
        "date": "2023-01-11T09:42:00.000Z",
        "voteCount": 4,
        "content": "ADE is correct"
      },
      {
        "date": "2022-12-30T10:02:00.000Z",
        "voteCount": 3,
        "content": "BC can't be both happen simultaneously:\n\"In addition to the XMLA Endpoint property being enabled read-write by the capacity admin, the tenant-level setting Allow XMLA endpoints and Analyze in Excel with on-premises datasets must be enabled in the admin portal.\"\nSource: https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#security"
      },
      {
        "date": "2022-12-29T03:17:00.000Z",
        "voteCount": 1,
        "content": "Please confirm the options"
      },
      {
        "date": "2023-01-30T00:48:00.000Z",
        "voteCount": 2,
        "content": "ADE is the correct answer for sure"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/microsoft/view/91858-exam-dp-500-topic-1-question-89-discussion/",
    "body": "You need to provide users with a reproducible method to connect to a data source and transform the data by using an AI function. The solution must meet the following requirement:<br>Minimize development effort.<br>Avoid including data in the file.<br>Which type of file should you create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIT\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIDS",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIX"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-31T07:14:00.000Z",
        "voteCount": 6,
        "content": "B)  \nCreating a PBIDS file is the best option to provide users with a reproducible method to connect to a data source and transform the data by using an AI function while minimizing development effort and avoiding including data and visualization in the file."
      },
      {
        "date": "2023-01-07T17:49:00.000Z",
        "voteCount": 5,
        "content": "PBIT, because \"Avoid including data in the file.\""
      },
      {
        "date": "2023-09-15T01:58:00.000Z",
        "voteCount": 3,
        "content": "avoid keeping data in the FILE !!!! &gt; so A PBIT \n\nsince PBIX and PBIDS keep data in file"
      },
      {
        "date": "2023-04-12T04:05:00.000Z",
        "voteCount": 4,
        "content": "https://www.youtube.com/watch?v=GJoxzfIQo6o\na) wiht template we reduce development effort"
      },
      {
        "date": "2023-03-29T11:17:00.000Z",
        "voteCount": 3,
        "content": "Option A\n\nRemember, a PBIT (Power BI Template) file can include source connection details and report layout, and can be used to provide users with a reproducible method to connect to a data source and transform the data by using an AI function.\n\nA PBIT file is similar to a PBIX file, but it does not include any data. It contains the report layout, data source connection details, and other settings required to create a report. By using a PBIT file, you can provide users with a pre-built report template that they can use to connect to a data source and generate a report.\n\nTherefore, both A (PBIT) and B (PBIDS) could be correct answers depending on the exact requirements of the solution. If the solution requires providing a pre-built report template that includes source connection details and report layout, then A (PBIT) would be the correct answer. If the solution only requires providing a data source connection that can be reused across multiple reports or models, then B (PBIDS) would be the correct answer."
      },
      {
        "date": "2023-02-14T00:28:00.000Z",
        "voteCount": 1,
        "content": "As per previous comments, since you need to include AI functions, a PBIDS (which is only a link to the datasource) isn't enough and PBIX includes the data so PBIT is the correct answer."
      },
      {
        "date": "2023-01-23T03:27:00.000Z",
        "voteCount": 2,
        "content": "The \"Working with Power BI and PBIDS\" section of this link: https://www.mssqltips.com/sqlservertip/6444/power-bi-data-source-pbids-files/ clearly shows that users working with PBIDS get direct access to the data itself. This contradicts the second requisite in the question so it has to be PBIT (in case you weren't convinced so far)."
      },
      {
        "date": "2022-12-31T05:23:00.000Z",
        "voteCount": 3,
        "content": "PBIT is correct answer because we are transforming data with an AI function in addition to connecting data source.\n\nPBIT are used to store: queries, parameters, data models (schemas, relationships between tables, measures, etc.), report pages, visualizations and other report elements.\nIf you want to save only the link to the data source you will create a PBIDS file.\n\nSource: https://excelk.com/en/pbit-and-pbids-files/"
      },
      {
        "date": "2022-12-23T05:06:00.000Z",
        "voteCount": 3,
        "content": "The answer should be: B. PBIDS\nPBIDS files are templates of a connection to data:\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/desktop-data-sources#using-pbids-files-to-get-data"
      },
      {
        "date": "2022-12-16T09:12:00.000Z",
        "voteCount": 3,
        "content": "Answer Is B, maek a connection to a data source"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93146-exam-dp-500-topic-1-question-90-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the Power BI data model shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"https://img.examtopics.com/dp-500/image120.png\"><br>Users indicate that when they build reports from the data model, the reports take a long time to load.<br>You need to recommend a solution to reduce the load times of the reports.<br>Solution: You recommend moving all the measures to a calculation group.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-28T03:26:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer."
      },
      {
        "date": "2024-02-13T04:29:00.000Z",
        "voteCount": 1,
        "content": "Correct  Answer"
      },
      {
        "date": "2022-12-30T22:47:00.000Z",
        "voteCount": 4,
        "content": "Answer is NO.\nMoving measures to calculation group only reduces number of measures and has no impact on the calculation speed."
      },
      {
        "date": "2023-01-30T02:17:00.000Z",
        "voteCount": 1,
        "content": "Also, I see no measures in the depiction of the model (there are no small calculator icons next to fields i.e. columns)"
      },
      {
        "date": "2024-02-13T04:30:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer,  calculation groups only reduced the number of measures, and readability of report , it has  nothing to do  with  the report  load times."
      },
      {
        "date": "2022-12-29T03:20:00.000Z",
        "voteCount": 1,
        "content": "Please confirm the answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96647-exam-dp-500-topic-1-question-91-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the Power BI data model shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"https://img.examtopics.com/dp-500/image121.png\"><br>Users indicate that when they build reports from the data model, the reports take a long time to load.<br>You need to recommend a solution to reduce the load times of the reports.<br>Solution: You recommend creating a perspective that contains the commonly used fields.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-23T14:50:00.000Z",
        "voteCount": 7,
        "content": "I think B (no), because perspective only affects the view of the dataset, not it's structure."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96740-exam-dp-500-topic-1-question-92-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the Power BI data model shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"https://img.examtopics.com/dp-500/image122.png\"><br>Users indicate that when they build reports from the data model, the reports take a long time to load.<br>You need to recommend a solution to reduce the load times of the reports.<br>Solution: You recommend denormalizing the data model.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-14T00:54:00.000Z",
        "voteCount": 5,
        "content": "This link clearly explains the answer https://data-mozart.com/mastering-dp-500-exam-optimize-data-model-by-using-denormalization/"
      },
      {
        "date": "2023-10-12T01:38:00.000Z",
        "voteCount": 1,
        "content": "Definitely the answer should no as per the best practices which suggest using star schema then the deformalized single table\nhttps://learn.microsoft.com/en-us/power-bi/guidance/star-schema\n\nAdvantage of using star schema slim model over denormalized table fat model is very well captured in this article\nhttps://www.sqlbi.com/articles/power-bi-star-schema-or-single-table/\n\nBut these questions really depend upon the different options that come up here if there is no question with star schema then this might be correct but i would go with Star Schema option if i do not know what the other questions options would be given."
      },
      {
        "date": "2023-09-15T02:05:00.000Z",
        "voteCount": 1,
        "content": "see q108 &gt; normalizing is not the way"
      },
      {
        "date": "2023-06-28T03:26:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer."
      },
      {
        "date": "2023-01-24T07:01:00.000Z",
        "voteCount": 3,
        "content": "Denormalization is an answer here"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95526-exam-dp-500-topic-1-question-93-discussion/",
    "body": "DRAG DROP -<br>You have a shared dataset in Power BI named Dataset1.<br>You have an on-premises Microsoft SQL Server database named DB1.<br>You need to ensure that Dataset1 refreshes data from DB1.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><img src=\"https://img.examtopics.com/dp-500/image123.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image124.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-18T23:23:00.000Z",
        "voteCount": 7,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Install the on-premises data gateway (standard mode)\n- From powerbi.com, add a data source to the gateway clusters\n- From powerbi.com, configure Dataset1 to use a data gateway"
      },
      {
        "date": "2023-10-29T23:43:00.000Z",
        "voteCount": 1,
        "content": "Agree with you, but, why not a gateway in a personal mode?"
      },
      {
        "date": "2023-12-27T07:38:00.000Z",
        "voteCount": 1,
        "content": "standard mode is recommended \nhttps://powerbidocs.com/2021/02/22/personal-vs-on-premises-data-gateway-standard/"
      },
      {
        "date": "2023-04-13T02:49:00.000Z",
        "voteCount": 2,
        "content": "It can be ok but\ncluster is for high availability and use more than 1 gateway at the same time for high volumen , the needed dont talk about this and does not talk about more than 1 gateway.\nthis step is not necesary in fact"
      },
      {
        "date": "2023-04-13T02:55:00.000Z",
        "voteCount": 2,
        "content": "I correct myself, it is ok\nhttps://powerbi.tips/2019/10/building-a-gateway-cluster/"
      },
      {
        "date": "2023-01-16T05:20:00.000Z",
        "voteCount": 2,
        "content": "I would agree with this answer"
      },
      {
        "date": "2023-01-23T03:35:00.000Z",
        "voteCount": 2,
        "content": "Also the 2nd step (with clusters) makes perfect sense considering how clusters are defined here https://learn.microsoft.com/en-us/data-integration/gateway/service-gateway-high-availability-clusters"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96440-exam-dp-500-topic-1-question-94-discussion/",
    "body": "You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use import storage mode.<br>A database administrator reports a drastic increase in the number of queries sent from the Power BI service to an Azure SQL database since the creation of the deployment pipeline.<br>An investigation into the issue identifies the following:<br>One of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.<br>When publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.<br>You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.<br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Capacity settings in the Power BI Admin portal, reduce the Max Intermediate Row Set Count setting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the dataset to use a composite model that has a DirectQuery connection to the fact table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the large dataset storage format for workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Capacity settings in the Power BI Admin portal, increase the Max Intermediate Row Set Count setting."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-26T06:43:00.000Z",
        "voteCount": 1,
        "content": "C says ChatGPT."
      },
      {
        "date": "2023-10-12T02:03:00.000Z",
        "voteCount": 1,
        "content": "sorry did not vote for the answer C"
      },
      {
        "date": "2023-10-12T02:01:00.000Z",
        "voteCount": 1,
        "content": "To reduce the size of queries so it does not refresh the complete dataset i think most logical solution is incremental refresh the recommended approach is to enable the large dataset storage format for workspace\n\nFor datasets published to workspaces assigned to Premium capacities, if you think the dataset will grow beyond 1 GB, you can improve refresh operation performance and ensure the dataset doesn't max out size limits by enabling Large dataset storage format before performing the first refresh operation in the service.\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/incremental-refresh-overview#publish"
      },
      {
        "date": "2023-06-27T04:38:00.000Z",
        "voteCount": 3,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-01-23T03:40:00.000Z",
        "voteCount": 4,
        "content": "Not only what per_ing said, but also A and D apply only to DirectQuery mode (whereas the Q clearly states Import) and the Large format setting makes no sense for a dataset of the size 1GB considering that the Premium capacity has the 10GB limit and we know we are talking about Premium because that is the prerequisites for deployment pipelines altogether. In short, the system of elimination (if not common sense) leaves us with B."
      },
      {
        "date": "2023-01-22T00:13:00.000Z",
        "voteCount": 2,
        "content": "I think this is correct as it is one of the suggestions from Microsoft https://learn.microsoft.com/en-us/power-bi/guidance/import-modeling-data-reduction#switch-to-mixed-mode"
      },
      {
        "date": "2023-10-12T01:58:00.000Z",
        "voteCount": 1,
        "content": "The recommendation is for reduction of size of the dataset and here we need to reduce the size of the queries during refresh so this solution does not makes sense"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93175-exam-dp-500-topic-1-question-95-discussion/",
    "body": "You are using GitHub as a source control solution for an Azure Synapse Studio workspace.<br>You need to modify the source control solution to use an Azure DevOps Git repository.<br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisconnect from the GitHub repository.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pull request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the workspace to live mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the active branch."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-18T23:23:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Disconnect from the GitHub repository"
      },
      {
        "date": "2022-12-29T08:27:00.000Z",
        "voteCount": 4,
        "content": "Source: https://learn.microsoft.com/en-us/azure/synapse-analytics/cicd/source-control"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96444-exam-dp-500-topic-1-question-96-discussion/",
    "body": "You have a Power BI tenant that contains 10 workspaces.<br>You need to create dataflows in three of the workspaces. The solution must ensure that data engineers can access the resulting data by using Azure Data Factory.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and save the dataflows to the internal storage of Power BI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the managed identity for Data Factory as a member of the workspaces.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssociate the Power BI tenant to an Azure Data Lake Storage account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and save the dataflows to an Azure Data Lake Storage account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-22T00:24:00.000Z",
        "voteCount": 6,
        "content": "I think this is correct https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-09-29T00:20:00.000Z",
        "voteCount": 1,
        "content": "would gues bd"
      },
      {
        "date": "2023-08-18T23:24:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Add the managed identity for Data Factory as a member of the workspaces.\n- Create and save the dataflows to an Azure Data Lake Storage account."
      },
      {
        "date": "2023-04-13T03:49:00.000Z",
        "voteCount": 2,
        "content": "ByD as ajaysharma2061 said"
      },
      {
        "date": "2023-03-29T11:12:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: BD\n\nTo ensure that data engineers can access the resulting data by using Azure Data Factory, you should perform the following two actions:\n\nB. Add the managed identity for Data Factory as a member of the workspaces: This step ensures that Data Factory has access to the Power BI workspaces where the dataflows are located.\n\nD. Create and save the dataflows to an Azure Data Lake Storage account: This step ensures that the dataflows are saved to a location that can be accessed by Azure Data Factory.\n\nTherefore, the correct answers are B and D.\n\nOption A is incorrect because saving dataflows to the internal storage of Power BI does not provide a location that can be accessed by Azure Data Factory.\n\nOption C is also incorrect because associating the Power BI tenant to an Azure Data Lake Storage account does not automatically save the dataflows to the storage account. The dataflows must still be created and saved to the storage account separately."
      },
      {
        "date": "2023-04-25T10:18:00.000Z",
        "voteCount": 6,
        "content": "You just gave the answer in your Option C explanation. Answers are C and D because you associate the PBI tenant to an Azure Data Lake Storage account, THEN create and save the dataflows to an Azure Data Lake Storage account. Each answer is PART of a solution."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95528-exam-dp-500-topic-1-question-97-discussion/",
    "body": "You are implementing a reporting solution that has the following requirements:<br>Reports for external customers must support 500 concurrent requests. The data for these reports is approximately 7 GB and is stored in Azure Synapse Analytics.<br>Reports for the security team use data that must have local security rules applied at the database level to restrict access. The data being reviewed is 2 GB.<br>Which storage mode provides the best response time for each group of users?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport for the external customers and DirectQuery for the security team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectQuery for the external customers and import for the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDirectQuery for the external customers and DirectQuery for the security team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport for the external customers and import for the security team."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-17T20:52:00.000Z",
        "voteCount": 7,
        "content": "Agree that A is correct. Thanks for your links."
      },
      {
        "date": "2023-01-17T18:50:00.000Z",
        "voteCount": 5,
        "content": "A should be correct answer"
      },
      {
        "date": "2023-08-18T23:25:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Import for the external customers and DirectQuery for the security team."
      },
      {
        "date": "2023-04-13T04:33:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-directquery-about#the-underlying-source-defines-security-rules"
      },
      {
        "date": "2023-01-16T05:46:00.000Z",
        "voteCount": 5,
        "content": "A is the correct answer. The external users problem is solved by Import mode, as explained here https://community.powerbi.com/t5/Service/One-report-for-multiple-users-simultaneously/m-p/1749731#M124530. And the security team requirements involves security at the data source level which can only be achieved through DirectQuery as explained under DirectQuery use cases on this link https://learn.microsoft.com/en-us/power-bi/connect-data/desktop-directquery-about."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92195-exam-dp-500-topic-1-question-98-discussion/",
    "body": "DRAG DROP -<br>You have a Power BI dataset that contains the following measures:<br><br>Budget -<br><br>Actuals -<br><br>Forecast -<br>You create a report that contains 10 visuals.<br>You need to provide users with the ability to use a slicer to switch between the measures in two visuals only.<br>You create a measure named CG Measure Switch.<br>How should you complete the DAX expression for the Actuals measure? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><img src=\"https://img.examtopics.com/dp-500/image127.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image128.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-23T15:03:00.000Z",
        "voteCount": 10,
        "content": "SELECTEDMEASURENAME and SELECTEDMEASURE\nAs Maazi said, ISSELECTEDMEASURE() returns boolean and SELECTEDVALUE is irrelevant."
      },
      {
        "date": "2022-12-20T07:55:00.000Z",
        "voteCount": 7,
        "content": "The answer should be:\nSELECTEDMEASURENAME()\nand\nSELECTEDVALUE()\n\nThis question already appeared, and this was the provided (correct, IMO) answer."
      },
      {
        "date": "2022-12-22T04:23:00.000Z",
        "voteCount": 4,
        "content": "I agree with you. Moreover, the ISSELECTEDMEASURE() function returns a boolean"
      },
      {
        "date": "2022-12-26T09:19:00.000Z",
        "voteCount": 9,
        "content": "Should be SELECTEDMEASURENAME and SELECTEDMEASURE"
      },
      {
        "date": "2023-06-07T17:12:00.000Z",
        "voteCount": 3,
        "content": "Yes, you are right. This same question already exists in #75. So, the answer should be SELECTEDMEASURENAME()and SELECTEDVALUE()."
      },
      {
        "date": "2023-06-27T04:46:00.000Z",
        "voteCount": 2,
        "content": "SELECTEDMEASURENAME\nSELECTEDMEASURE\nAs per Q#75"
      },
      {
        "date": "2023-03-28T23:08:00.000Z",
        "voteCount": 5,
        "content": "SELECTEDMEASURENAME and SELECTEDMEASURE\n* ISSELECTEDMEASURE  requires a list of measure names  and in this case it was not provided, and the calculation is evaluating an equal (=) for specific measure name."
      },
      {
        "date": "2023-01-13T21:56:00.000Z",
        "voteCount": 4,
        "content": "answer is correct, SELECTEDVALUE need a column name as input, you can't use SELECTEDVALUE(), check below\nhttps://learn.microsoft.com/en-us/dax/selectedvalue-function"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92321-exam-dp-500-topic-1-question-99-discussion/",
    "body": "You discover a poorly performing measure in a Power BI data model.<br>You need to review the query plan to analyze the amount of time spent in the storage engine and the formula engine.<br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTabular Editor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertipaq Analyzer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerformance analyzer in Power BI Desktop"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-31T01:36:00.000Z",
        "voteCount": 8,
        "content": "It is 100% Dax Studio. Although Vertipaq Analyzer is part of DAX Stuidio, Vertipaq Analyzer only shows details about the tables and columns, not the time it took for each engine to execute the query."
      },
      {
        "date": "2022-12-31T16:46:00.000Z",
        "voteCount": 2,
        "content": "Yes you are correct."
      },
      {
        "date": "2023-12-19T02:17:00.000Z",
        "voteCount": 1,
        "content": "The Query Plan is a DAX Studio feature."
      },
      {
        "date": "2023-08-18T23:25:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- DAX Studio"
      },
      {
        "date": "2023-04-13T05:47:00.000Z",
        "voteCount": 2,
        "content": "C &amp; D can be ok, but C show more details than D"
      },
      {
        "date": "2023-03-28T23:41:00.000Z",
        "voteCount": 1,
        "content": "It is confusing, but it is OK = D\nPerformance Analyzer can find out how each of your report elements, such as visuals and DAX formulas are performing. \nLink: https://learn.microsoft.com/en-us/power-bi/create-reports/desktop-performance-analyzer"
      },
      {
        "date": "2023-01-23T03:57:00.000Z",
        "voteCount": 1,
        "content": "I agree that it may be confusing that the Vertipaq is technically integrated into DAX studio BUT the Server Timings in DAX definitely show the information re. function and storage engine consumption (even visually with blue and yellow if that jogs your memory) whereas the information in Vertipaq analyzer are separated into tabs such as Table Tab, Relationship Tab and so on and do not mention the wanted information."
      },
      {
        "date": "2023-01-23T03:58:00.000Z",
        "voteCount": 1,
        "content": "*formula engine (sorry)"
      },
      {
        "date": "2023-01-07T18:09:00.000Z",
        "voteCount": 4,
        "content": "DAX Studio"
      },
      {
        "date": "2022-12-27T06:39:00.000Z",
        "voteCount": 1,
        "content": "Weird question, Vertipaq Analyser is correct, but that is a feature in both DAX studio and Tabular Editor 3. I would pick B, but C and A could also be considered correct."
      },
      {
        "date": "2022-12-31T16:47:00.000Z",
        "voteCount": 2,
        "content": "Changed my mind. Its DAX studio"
      },
      {
        "date": "2022-12-23T05:22:00.000Z",
        "voteCount": 2,
        "content": "Information regarding the FE and the SE can be accessed using Vertipaq Analyzer, which is a feature from DAX Studio."
      },
      {
        "date": "2022-12-21T03:21:00.000Z",
        "voteCount": 2,
        "content": "In DAX Studio this can be seen.. seems wrong answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92977-exam-dp-500-topic-1-question-100-discussion/",
    "body": "You plan to modify a Power BI dataset.<br>You open the Impact analysis panel for the dataset and select Notify contacts.<br>Which contacts will be notified when you use the Notify contacts feature?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Power BI admins",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe workspace admins of any workspace that uses the dataset\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tall the workspace members of any workspace that uses the dataset",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tany users that accessed a report that uses the dataset within the last 30 days"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T06:43:00.000Z",
        "voteCount": 8,
        "content": "It will be send to the contact list of the workspace. Which by default contains the workspace admins (but can be modified). B"
      },
      {
        "date": "2022-12-31T01:23:00.000Z",
        "voteCount": 5,
        "content": "Assuming that contact list has not modified for the workspaces then the answer is B, because by default the contact list only includes workspace admins.\n \nWhen you click on Notify Contacts, the window pops up and shows that \"An email notification will be sent to all the contacts for all impacted workspaces, including workspaces you don't have access to.\" So if other users are added in the contact list, then they will also receive email"
      },
      {
        "date": "2023-08-18T23:26:00.000Z",
        "voteCount": 1,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- the workspace admins of any workspace that uses the dataset"
      },
      {
        "date": "2023-04-13T06:19:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/collaborate-share/service-dataset-impact-analysis#notify-contacts\n\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-create-the-new-workspaces#create-a-contact-list"
      },
      {
        "date": "2023-01-13T21:59:00.000Z",
        "voteCount": 4,
        "content": "it's B\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-create-the-new-workspaces#create-a-contact-list"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96447-exam-dp-500-topic-1-question-101-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trollup",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcov",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescribe\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-26T04:43:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer."
      },
      {
        "date": "2023-04-13T18:09:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D"
      },
      {
        "date": "2023-01-22T00:31:00.000Z",
        "voteCount": 4,
        "content": "Duplicate of 62 and 103. Correct answer is D"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93312-exam-dp-500-topic-1-question-102-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You are using an Azure Synapse Analytics serverless SQL pool to query a collection of Apache Parquet files by using automatic schema inference. The files contain more than 40 million rows of UTF-8-encoded business names, survey names, and participant counts. The database is configured to use the default collation.<br>The queries use OPENROWSET and infer the schema shown in the following table.<br><img src=\"https://img.examtopics.com/dp-500/image130.png\"><br>You need to recommend changes to the queries to reduce I/O reads and tempdb usage.<br>Solution: You recommend using OPENROWSET WITH to explicitly define the collation for businessName and surveyName as Latin1_General_100_BIN2_UTF8.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-12T22:56:00.000Z",
        "voteCount": 2,
        "content": "The comments are very descriptive and suggest answer as A"
      },
      {
        "date": "2023-09-29T18:50:00.000Z",
        "voteCount": 1,
        "content": "I sitck with AAAAAAA"
      },
      {
        "date": "2023-09-11T19:07:00.000Z",
        "voteCount": 2,
        "content": "Make sure that you are explicilty specifying some UTF-8 collation (for example Latin1_General_100_BIN2_UTF8) for all string columns in WITH clause or set some UTF-8 collation at database level. \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"
      },
      {
        "date": "2023-08-31T23:52:00.000Z",
        "voteCount": 1,
        "content": "https://techcommunity.microsoft.com/t5/azure-synapse-analytics-blog/always-use-utf-8-collations-to-read-utf-8-text-in-serverless-sql/ba-p/1883633"
      },
      {
        "date": "2023-10-03T09:14:00.000Z",
        "voteCount": 1,
        "content": "But according to that same link, the database has to be dropped beforehand. Last paragraph says \" NOTE: If you have existing tables that used default database collation, changing default database collation would not be applied on them. You would need to drop and re-create external tables so they can pickup new default.\" So i think the answer is NO"
      },
      {
        "date": "2023-10-03T09:44:00.000Z",
        "voteCount": 3,
        "content": "please disregard as here we are not changing the database collation. Answer should be YES. From the copilot: \nReducing I/O Reads\nWhen you query Parquet files in Azure Synapse Analytics, the service can push down certain filter predicates to the storage layer. This means that only the relevant row groups are read from the Parquet files, which can significantly reduce I/O reads. However, this predicate pushdown is only possible if the collation of the column in the Parquet file matches the collation of the column in SQL Server. By explicitly setting the collation to Latin1_General_100_BIN2_UTF8, you ensure that the collations match, enabling predicate pushdown and reducing I/O reads.\n\nReducing tempdb Usage\nWhen you query data with a different collation than the server\u2019s default collation, SQL Server needs to perform a collation conversion. This conversion happens in memory and can increase tempdb usage. By setting the collation at the column level to match the data\u2019s actual collation, you avoid these conversions and reduce tempdb usage."
      },
      {
        "date": "2023-06-30T19:01:00.000Z",
        "voteCount": 1,
        "content": "Defining the collation for the columns as Latin1_General_100_BIN2_UTF8 will not directly reduce I/O reads and tempdb usage. Collation determines the sorting and comparison rules for character data and does not have a direct impact on I/O or tempdb usage.\n\nTo reduce I/O reads and tempdb usage, you should consider the following approaches:\n\n    Implement appropriate indexing on the columns used in the queries to improve query performance and reduce the need for extensive I/O reads.\n    Optimize the query logic by using efficient filters and aggregations to minimize the amount of data read and processed.\n    Consider partitioning the data based on relevant criteria, such as date or another logical partitioning key, to improve query performance and reduce I/O reads.\n    Utilize appropriate compression techniques for the Parquet files to reduce their size, leading to reduced I/O reads."
      },
      {
        "date": "2023-04-25T09:38:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-data-storage"
      },
      {
        "date": "2023-04-13T18:33:00.000Z",
        "voteCount": 3,
        "content": "a is correct \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"
      },
      {
        "date": "2023-03-29T00:53:00.000Z",
        "voteCount": 4,
        "content": "The Latin1_General_100_BIN2_UTF8 collation has additional performance optimization that works only for parquet and Cosmos DB.\nLINK: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files \n\nCollations: The default inheritance can be overridden by explicitly stating a different collation for a character-based data type."
      },
      {
        "date": "2023-02-13T23:41:00.000Z",
        "voteCount": 4,
        "content": "Data in a Parquet file is organized in row groups. Serverless SQL pool skips row groups based on the specified predicate in the WHERE clause, which reduces IO. The result is increased query performance.\nIf we want to filter by businessName and/or by survey name then is recommended to use the Latin1_General_100_BIN2_UTF8 collation\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool#use-proper-collation-to-utilize-predicate-pushdown-for-character-columns"
      },
      {
        "date": "2023-01-17T21:00:00.000Z",
        "voteCount": 3,
        "content": "B is the correct one."
      },
      {
        "date": "2023-01-11T16:16:00.000Z",
        "voteCount": 3,
        "content": "using \"OPENROWSET WITH to explicitly define the collation for businessName and surveyName as Latin1_General_100_BIN2_UTF8\", will allow you to read the string properly, but I think it has nothing to do with decreasing the I/O, so most likely it's NO"
      },
      {
        "date": "2023-01-07T18:16:00.000Z",
        "voteCount": 4,
        "content": "No,\n\nSolution: You recommend defining a data source and view for the Parquet files. You recommend updating the query to use the view."
      },
      {
        "date": "2022-12-30T12:10:00.000Z",
        "voteCount": 4,
        "content": "You don't need to use the OPENROWSET WITH clause when reading Parquet files\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"
      },
      {
        "date": "2023-04-13T18:33:00.000Z",
        "voteCount": 3,
        "content": "Sorry Maazi, all that link that you refere say that the correct answer is A"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96448-exam-dp-500-topic-1-question-103-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br>You need to present the data distribution statistics from a Data Frame in a tabular view.<br>Which method should you invoke on the Data Frame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsample",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescribe\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfreqItems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplain"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-29T05:01:00.000Z",
        "voteCount": 1,
        "content": "No.101"
      },
      {
        "date": "2023-12-19T02:35:00.000Z",
        "voteCount": 1,
        "content": "duplicated question"
      },
      {
        "date": "2023-06-26T04:43:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer."
      },
      {
        "date": "2023-01-22T00:32:00.000Z",
        "voteCount": 2,
        "content": "Duplicate of 62 and 101. Correct answer is B"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96449-exam-dp-500-topic-1-question-104-discussion/",
    "body": "You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use import storage mode.<br>A database administrator reports a drastic increase in the number of queries sent from the Power Bi service to an Azure SQL database since the creation of the deployment pipeline.<br>An investigation into the issue identifies the following:<br>One of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.<br>When publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.<br>You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.<br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest the authors of the deployment pipeline datasets to reduce the number of datasets republished during development.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the dataset, delete the fact table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the dataset to use a composite model that has a DirectQuery connection to the fact table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Capacity settings in the Power Bi Admin portal, reduce the Max Intermediate Row Set Count setting."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-12T23:05:00.000Z",
        "voteCount": 1,
        "content": "sqm question as Q20 and Q94 but different options, with these set of options i would go with option C as the question wants us to reduce the queries during refresh time so changing to composite models seems most relevant solution"
      },
      {
        "date": "2023-03-29T11:08:00.000Z",
        "voteCount": 1,
        "content": "Option C: Direct Query \n\nGiven the scenario, the best solution to reduce the size of queries sent to the database when dataset changes are published to development, test, or production is to configure the dataset to use a composite model that has a DirectQuery connection to the fact table.\n\nUsing a DirectQuery connection allows you to keep the data in the data source and query it directly when needed, rather than importing the entire dataset into Power BI. This can significantly reduce the amount of data that needs to be transferred between the data source and Power BI, which in turn can reduce the number of queries sent to the data source."
      },
      {
        "date": "2023-03-29T01:24:00.000Z",
        "voteCount": 1,
        "content": "ITS THE SAME THAN 94"
      },
      {
        "date": "2023-06-27T12:18:00.000Z",
        "voteCount": 1,
        "content": "different answers"
      },
      {
        "date": "2023-01-22T00:33:00.000Z",
        "voteCount": 3,
        "content": "Duplicate of question 94"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/microsoft/view/92335-exam-dp-500-topic-1-question-105-discussion/",
    "body": "HOTSPOT -<br>You are building a Power BI dataset that contains a table named Calendar. Calendar contains the following calculated column.<br><img src=\"https://img.examtopics.com/dp-500/image131.png\"><br>You need to create a measure that will perform a fiscal prior year-to-date calculation that meets the following requirements:<br>Returns the fiscal prior year-to-date value for [Sales Amount]<br>Uses a fiscal year end of June 30<br>Produces no result for dates in the future<br>How should you complete the DAX expression? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image132.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image133.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-21T05:29:00.000Z",
        "voteCount": 14,
        "content": "pfFlag would either be Future or Past, so seems redundant to filter on TODAY(), imo should be \"Past\""
      },
      {
        "date": "2023-01-23T04:11:00.000Z",
        "voteCount": 2,
        "content": "My thoughts exactly. Or, looking from another angle, why would the pfFlag be mentioned and explained at all if one of its outcome values (Future/Past) is not part of the solution?"
      },
      {
        "date": "2023-01-23T15:15:00.000Z",
        "voteCount": 5,
        "content": "pfFlag can only be Past or Future, in the context of the question, it's Past"
      },
      {
        "date": "2023-12-14T01:29:00.000Z",
        "voteCount": 2,
        "content": "calculatetable\nsameperiodlastyear\npast"
      },
      {
        "date": "2023-06-28T05:58:00.000Z",
        "voteCount": 2,
        "content": "Calculate\nSameperiod\nPast"
      },
      {
        "date": "2023-04-13T18:55:00.000Z",
        "voteCount": 2,
        "content": "oky except last must be \"past\""
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94357-exam-dp-500-topic-1-question-106-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have a Power Bi dataset named Dataset1.<br>In Dataset1, you currently have 50 measures that use the same time intelligence logic.<br>You need to reduce the number of measures, while maintaining the current functionality.<br>Solution: From Power BI Desktop, you create a hierarchy.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-07T18:24:00.000Z",
        "voteCount": 4,
        "content": "No,\n\nIt should be \"From Tabular Editor, you create a calculation group.\""
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94358-exam-dp-500-topic-1-question-107-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have the Power BI data model shown in the exhibit. (Click the Exhibit tab.)<br><img src=\"https://img.examtopics.com/dp-500/image134.png\"><br>Users indicate that when they build reports from the data model, the reports take a long time to load.<br>You need to recommend a solution to reduce the load times of the reports.<br>Solution: You recommend normalizing the data model.<br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-07T18:25:00.000Z",
        "voteCount": 5,
        "content": "No.\n\nIt should be \"You recommend denormalizing the data model\""
      },
      {
        "date": "2023-01-16T07:23:00.000Z",
        "voteCount": 5,
        "content": "exactly, because the shown model is already normalized"
      },
      {
        "date": "2023-04-25T08:54:00.000Z",
        "voteCount": 1,
        "content": "Normalizing increases performance, denormalizing data can cause data redundancy."
      },
      {
        "date": "2023-10-12T23:16:00.000Z",
        "voteCount": 1,
        "content": "Definitely the answer should yes as per the best practices which suggest using star schema (normalized schema) then the denormalized single table\nhttps://learn.microsoft.com/en-us/power-bi/guidance/star-schema\n\nAdvantage of using star schema slim model over denormalized table fat model is very well captured in this article\nhttps://www.sqlbi.com/articles/power-bi-star-schema-or-single-table/"
      },
      {
        "date": "2023-04-25T08:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is Yes. Normalizing increases performance, denormalizing data can cause data redundancy."
      },
      {
        "date": "2023-04-25T08:54:00.000Z",
        "voteCount": 1,
        "content": "Answer is Yes. Normalizing increases performance, denormalizing data can cause data redundancy."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93149-exam-dp-500-topic-1-question-108-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfreqItems",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcorr",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsummary\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trollup"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-30T22:40:00.000Z",
        "voteCount": 9,
        "content": "Correct answer is Summary. Corr shows correlation between columns and it has nothing to do with data distribution statistics.\nSource: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html"
      },
      {
        "date": "2023-01-22T00:37:00.000Z",
        "voteCount": 6,
        "content": "Duplicate of 62, 101 and 103 only with other answer options. I believe the correct is still \"describe\" even though that is not an option here"
      },
      {
        "date": "2023-09-22T04:52:00.000Z",
        "voteCount": 1,
        "content": "question returns often with different answers\nhowever I believe it should always be summary\n.describe() function takes cols:String*(columns in df) as optional args.\n\n\n.summary() function takes statistics:String*(count,mean,stddev..etc) as optional args."
      },
      {
        "date": "2023-04-03T00:23:00.000Z",
        "voteCount": 1,
        "content": "DataFrame.summary(*statistics)[source]\nhttps://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html\n\nIt is the same question than 62 , 101 and 103. But in those cases the answer was \"describe\" and it's the same if you are looking for dataframe statistics detail \ndf.describe(['age']).show()\nhttps://spark.apache.org/docs/3.1.1/api/python/reference/api/pyspark.sql.DataFrame.describe.html"
      },
      {
        "date": "2023-02-14T05:10:00.000Z",
        "voteCount": 1,
        "content": "It should definitely be summary or describe, either works. Summary shows count, mean, stddev, min, max and quartiles: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.summary.html \nDescribe shows count, mean, stddev, min and max: https://spark.apache.org/docs/3.1.2/api/python/reference/api/pyspark.sql.DataFrame.describe.html#pyspark.sql.DataFrame.describe\nThe differences seem to be that summary is newer and includes the percentiles at 25%, 50% and 75%."
      },
      {
        "date": "2023-02-02T19:13:00.000Z",
        "voteCount": 1,
        "content": "summary"
      },
      {
        "date": "2022-12-29T03:28:00.000Z",
        "voteCount": 1,
        "content": "Please confirm"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94359-exam-dp-500-topic-1-question-109-discussion/",
    "body": "You are using DAX Studio to analyze a slow-running report query.<br>You need to identify inefficient join operations in the query.<br>What should you review?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe query statistics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe query plan\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe query history"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-06-30T19:13:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-04-13T19:09:00.000Z",
        "voteCount": 1,
        "content": "I think is B"
      },
      {
        "date": "2023-01-26T01:38:00.000Z",
        "voteCount": 2,
        "content": "B, Query Plan."
      },
      {
        "date": "2023-01-13T22:47:00.000Z",
        "voteCount": 3,
        "content": "I think the answer B is correct, there is nothing called Query Statistics in DAX Studio"
      },
      {
        "date": "2023-01-10T10:37:00.000Z",
        "voteCount": 3,
        "content": "A.) it doesn't say that the data source for the Power BI Report is a SQL database and B.) the website you referenced is about finding slow running queries on a SQL Database and C.) that tool has nothing to do with Dax Studios. And in trying to find long running DQX queries, it is looking at the query plan https://www.sqlbi.com/tv/query-performance-tuning-in-dax-studio/"
      },
      {
        "date": "2023-01-07T18:32:00.000Z",
        "voteCount": 1,
        "content": "we need to enable the Query Statistics for this session\n\nhttps://www.sqlshack.com/how-to-identify-slow-running-queries-in-sql-server/"
      },
      {
        "date": "2023-01-24T20:23:00.000Z",
        "voteCount": 1,
        "content": "Isn't it for SQL server, not DAX Studio?"
      },
      {
        "date": "2023-04-13T19:08:00.000Z",
        "voteCount": 1,
        "content": "thats true that link is for sql"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95769-exam-dp-500-topic-1-question-110-discussion/",
    "body": "You have a Power Bi dataset that contains the following measure.<br><img src=\"https://img.examtopics.com/dp-500/image135.png\"><br>You need to improve the performance of the measure without affecting the logic or the results.<br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a variable and replace the values for [Sales Amount].",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace both CALCULATE functions by using a variable that contains the CALCULATE function.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the alternative result of BLANK() from the DIVIDE function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove 'Calendar'[Flag] = \"YTD\" from the code."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-28T03:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct answer."
      },
      {
        "date": "2023-04-13T19:11:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-03-29T03:02:00.000Z",
        "voteCount": 2,
        "content": "It is OK!"
      },
      {
        "date": "2023-01-17T21:20:00.000Z",
        "voteCount": 4,
        "content": "Yes, B is correct. use a function to replace the repeated query"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93150-exam-dp-500-topic-1-question-111-discussion/",
    "body": "You are optimizing a dataflow in a Power BI Premium capacity. The dataflow performs multiple joins.<br>You need to reduce the load time of the dataflow.<br>Which two actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute foldable operations before non-foldable operations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the ingestion operations and transformation operations in a single dataflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute non-foldable operations before foldable operations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the ingestion operations and transformation operations in separate dataflows.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the memory assigned to the dataflows."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-30T19:16:00.000Z",
        "voteCount": 2,
        "content": "To reduce the load time of a dataflow in Power BI Premium, you should consider the following actions:\n\nA. Execute foldable operations before non-foldable operations: Foldable operations can be pushed down to the data source and executed there, reducing the amount of data transferred to Power BI. By executing foldable operations early in the dataflow, you can minimize the data transfer and improve performance.\n\nD. Place the ingestion operations and transformation operations in separate dataflows: Separating ingestion operations (such as loading data from a source) and transformation operations (such as data cleansing or joining) into separate dataflows can help optimize the load time. In this way, the ingestion operations can be executed less frequently, while the transformation operations can be performed on an as-needed basis, reducing the overall load time.\n\nThese actions aim to optimize the dataflow's performance by leveraging foldable operations and optimizing the workflow structure."
      },
      {
        "date": "2023-01-17T21:25:00.000Z",
        "voteCount": 4,
        "content": "For A:\n\n*Query folding is the mechanism to push Power Query transformations back to the source to reduce the load of Power Query\u2026. The query is much more complicated, as it has to merge (Join) the source tables to form the result. But the database engine is optimised for doing such stuff and will perform this task much better than Power Query.\n\nhttps://towardsdatascience.com/exploring-query-folding-in-power-query-8288fb3c9c2f"
      },
      {
        "date": "2022-12-30T22:25:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct. You should split the ETL stage into two separate dataflows.\nSource: https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-premium-features?tabs=gen2#using-the-enhanced-compute-engine"
      },
      {
        "date": "2022-12-29T03:28:00.000Z",
        "voteCount": 2,
        "content": "Please confirm"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93184-exam-dp-500-topic-1-question-112-discussion/",
    "body": "You need to save Power BI dataflows in an Azure Storage account.<br>Which two prerequisites are required to support the configuration? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe connection must be created by a user that is assigned the Storage Blob Data Owner role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflows must exist already for any directly connected Power BI workspaces.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe storage account must be protected by using an Azure Firewall.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe storage account must be created in a separate Azure region from the Power BI tenant and workspaces.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe storage account must have hierarchical namespace enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-29T09:57:00.000Z",
        "voteCount": 6,
        "content": "Source: https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-03-29T04:56:00.000Z",
        "voteCount": 2,
        "content": "You are so right! \nPrerequisites\nTo bring your own ADLS Gen 2 account, you must have Owner permission at the storage account layer.\nThe storage account must be created with the Hierarchical Namespace (HNS) enabled."
      },
      {
        "date": "2023-01-07T18:38:00.000Z",
        "voteCount": 5,
        "content": "AE are correct\n\n*The user must have Storage Blob Data Owner role, Storage Blob Data Reader role, and an Owner role at the storage account level (scope should be this resource and not inherited). Any applied role changes may take a few minutes to sync, and must sync before the following steps can be completed in the Power BI service.\n\n*The storage account must be created with the Hierarchical Namespace (HNS) enabled."
      },
      {
        "date": "2023-06-28T03:42:00.000Z",
        "voteCount": 1,
        "content": "A and E are correct answers. \nLogical!"
      },
      {
        "date": "2023-04-13T19:15:00.000Z",
        "voteCount": 3,
        "content": "A &amp; E are correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94361-exam-dp-500-topic-1-question-113-discussion/",
    "body": "HOTSPOT -<br>You have the Power BI workspaces shown in the following exhibit.<br><img src=\"https://img.examtopics.com/dp-500/image136.png\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br><img src=\"https://img.examtopics.com/dp-500/image137.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image138.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-25T11:44:00.000Z",
        "voteCount": 8,
        "content": "why Infrastructure? It's ppu, you should choose embedded space with A tier, so Sales is the answer. Correct me if I'm wrong."
      },
      {
        "date": "2023-02-13T06:07:00.000Z",
        "voteCount": 5,
        "content": "I agree with you, with Power BI embedded the customers don\u00b4t need to sign in using Power BI credentials to view the embedded content (so they don\u00b4t need a Power BI license):\nhttps://learn.microsoft.com/en-us/power-bi/developer/embedded/embedded-analytics-power-bi#embed-for-your-customers\n\nSo for me, the right answers are:\nSales Orders\nAdmin"
      },
      {
        "date": "2023-03-29T05:29:00.000Z",
        "voteCount": 6,
        "content": "Power BI Embedded is an Azure offer that includes \"A SKUs\", this allow developers who want to embed visuals into their applications.\nThe embed for customers solution allow customers to be external users, and they don't need to sign in using Power BI credentials to view the embedded content.\n1)Sales Orders\n2)Admin"
      },
      {
        "date": "2023-10-11T05:38:00.000Z",
        "voteCount": 1,
        "content": "Sale Orders\nAdmin"
      },
      {
        "date": "2023-06-28T03:47:00.000Z",
        "voteCount": 1,
        "content": "Sales Orders\nAdmin"
      },
      {
        "date": "2023-01-07T18:43:00.000Z",
        "voteCount": 3,
        "content": "Admin: An orphaned workspace is one that does not have an admin assigned. It\u2019s easy to Recover an orphan from this screen. Simply select the workspace and click Recover, then add yourself or another user as an admin.\n\nhttps://dataveld.com/2019/01/31/what-is-an-orphaned-workspace-in-power-bi/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/microsoft/view/96455-exam-dp-500-topic-1-question-114-discussion/",
    "body": "You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use Import storage mode.<br>A database administrator reports a drastic increase in the number of queries sent from the Power BI service to an Azure SQL database since the creation of the deployment pipeline.<br>An investigation into the issue identifies the following:<br>One of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.<br>When publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.<br>You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.<br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the large dataset storage format for workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset parameter to reduce the fact table row count in the development and test pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRequest the authors of the deployment pipeline datasets to reduce the number of datasets republished during development.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off auto refresh when publishing the dataset changes to the Power BI service."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T04:43:00.000Z",
        "voteCount": 1,
        "content": "answer d won't reduce the size of the dataset, B will"
      },
      {
        "date": "2023-04-04T02:38:00.000Z",
        "voteCount": 1,
        "content": "Option D, creating a dataset parameter to reduce the fact table row count in the development and test pipelines, is the recommended solution for this scenario. By creating a dataset parameter, the fact table row count can be reduced, and only the required data can be refreshed during the dataset refresh. This approach can significantly reduce the number of queries sent to the database and improve performance. Once the changes are published to the production pipeline, the dataset parameter can be removed to refresh the entire fact table.\nChat GPT"
      },
      {
        "date": "2023-01-22T00:53:00.000Z",
        "voteCount": 2,
        "content": "Duplicate of question 20"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/microsoft/view/115732-exam-dp-500-topic-1-question-115-discussion/",
    "body": "You have a deployment pipeline for a Power BI workspace. The workspace contains two datasets that use import storage mode.<br><br>A database administrator reports a drastic increase in the number of queries sent from the Power Bi service to an Azure SQL database since the creation of the deployment pipeline.<br><br>An investigation into the issue identifies the following:<br><br>\u2022\tOne of the datasets is larger than 1 GB and has a fact table that contains more than 500 million rows.<br>\u2022\tWhen publishing dataset changes to development, test, or production pipelines, a refresh is triggered against the entire dataset.<br><br>You need to recommend a solution to reduce the size of the queries sent to the database when the dataset changes are published to development, test, or production.<br><br>What should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the large dataset storage format for workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the dataset, delete the fact table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset parameter to reduce the fact table row count in the development and test pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Capacity settings in the Power BI Admin portal, reduce the Max Intermediate Row Set Count setting."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-21T04:44:00.000Z",
        "voteCount": 1,
        "content": "see comment below"
      },
      {
        "date": "2023-09-15T01:23:00.000Z",
        "voteCount": 1,
        "content": "the question is, you need to reduce the size when they are sent &gt; a parameter reduces the size, but can be disabled online so the pbix is published faster"
      },
      {
        "date": "2023-07-19T03:03:00.000Z",
        "voteCount": 1,
        "content": "C seems to be best available option, however we should have full data in both production and test env so I wouldn't pick C having something better"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/microsoft/view/116464-exam-dp-500-topic-1-question-116-discussion/",
    "body": "You have a Power BI workspace named Workspace1 that contains five dataflows.<br><br>You need to configure Workspace1 to store the dataflows in an Azure Data Lake Storage Gen2 account.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the Data source settings in the dataflow queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable load for all dataflow queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the dataflow queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-26T20:08:00.000Z",
        "voteCount": 1,
        "content": "Thats an A for sure."
      },
      {
        "date": "2023-10-26T07:05:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT is adamant it's B."
      },
      {
        "date": "2023-09-03T03:59:00.000Z",
        "voteCount": 1,
        "content": "same question to N18"
      },
      {
        "date": "2023-08-06T23:23:00.000Z",
        "voteCount": 4,
        "content": "The question says to configure the Workspace1....\n\nAnswer is C.\nFinally, you can connect to any ADLS Gen 2 from the Admin portal, but if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\n\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-07-26T02:31:00.000Z",
        "voteCount": 4,
        "content": "In Q18 we have the same situation. Q18 answers are below from which option A was selected\n  &gt;&gt;\tA. From the Power BI Admin portal, enable tenant-level storage. \n\tB. Disable load for all dataflow queries. \n\tC. Delete the dataflow queries. \n\tD. Change the Data source settings in the dataflow queries."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/microsoft/view/114562-exam-dp-500-topic-1-question-117-discussion/",
    "body": "You are using an Azure Synapse Analytics serverless SQL pool to query network traffic logs in the Apache Parquet format.<br><br>A sample of the data is shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image141.png\"><br><br>You need to create a Transact-SQL query that will return the source IP address.<br><br>Which function should you use in the select statement to retrieve the source IP address?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON_VALUE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONVERT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFIRST_VALUE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFOR_JSON"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-10-05T07:40:00.000Z",
        "voteCount": 1,
        "content": "Agree with source supplied by Martin_Nbg"
      },
      {
        "date": "2023-09-24T12:56:00.000Z",
        "voteCount": 1,
        "content": "none of the answers is correct.\nIt should be Openrowset."
      },
      {
        "date": "2023-07-09T02:42:00.000Z",
        "voteCount": 3,
        "content": "Correct, see https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-nested-types"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/microsoft/view/120731-exam-dp-500-topic-1-question-118-discussion/",
    "body": "You have a Power BI report that contains a bar chart. The bar chart displays sales by country.<br><br>You need to ensure that a summary of the data shown on the bar chart is accessible by using a screen reader.<br><br>What should you configure on the bar chart?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconditional formatting",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe layer order",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\talt text\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe tab order"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T01:04:00.000Z",
        "voteCount": 1,
        "content": "confirmed C &gt; open power bi, create bar chart add data and go to the alt text setting, it literally states that this will only be shown by screen readers"
      },
      {
        "date": "2023-09-14T02:30:00.000Z",
        "voteCount": 1,
        "content": "Alt text is the right answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/microsoft/view/116395-exam-dp-500-topic-1-question-119-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br><br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br><br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdescribe\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcorr",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcov"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-22T04:57:00.000Z",
        "voteCount": 1,
        "content": ".describe() function takes cols:String*(columns in df) as optional args.\n.summary() function takes statistics:String*(count,mean,stddev..etc) as optional args."
      },
      {
        "date": "2023-09-15T01:08:00.000Z",
        "voteCount": 1,
        "content": "DataFrame.describe(percentiles=None, include=None, exclude=None)[source]\nGenerate descriptive statistics.\n&gt; describe\n\ncorr &gt; shows correlation between 2 series\ncov &gt; compute pairwise covariance\nexplain &gt;"
      },
      {
        "date": "2023-07-25T05:06:00.000Z",
        "voteCount": 2,
        "content": "duplicate question 103"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/microsoft/view/116396-exam-dp-500-topic-1-question-120-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool and a Microsoft Purview account.<br><br>The Microsoft Purview account has been granted sufficient permissions to the dedicated SQL pool.<br><br>You need to ensure that the SQL pool is scanned by Microsoft Purview.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data policy.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch the data catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRegister a data source.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data share connection."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T01:10:00.000Z",
        "voteCount": 1,
        "content": "After data sources are registered in your Microsoft Purview account, the next step is to scan the data sources. The scanning process establishes a connection to the data source and captures technical metadata like names, file size, columns, and so on. \n\nhttps://learn.microsoft.com/en-us/purview/concept-scans-and-ingestion"
      },
      {
        "date": "2023-07-25T05:08:00.000Z",
        "voteCount": 2,
        "content": "C is correct, duplicate with q19"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/microsoft/view/114567-exam-dp-500-topic-1-question-122-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a dataset that is populated from a list of business categories in a source database. The list of categories changes over time.<br><br>You use Power BI Report Builder to create a paginated report. The report has a report parameter named BusinessCategory.<br><br>You need to modify BusinessCategory to ensure that when the report opens, a drop-down list displays all the business categories, and all the business categories are selected.<br><br>How should you configure BusinessCategory? To answer, drag the appropriate options to the correct settings. Each option may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image142.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image143.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-07-09T02:53:00.000Z",
        "voteCount": 8,
        "content": "Solution should be Query and Query. Requirements say, available categories change over time and all currently available categories should be default. Thus a query is needed for both. See https://learn.microsoft.com/en-us/sql/reporting-services/tutorial-add-a-parameter-to-your-report-report-builder?view=sql-server-ver16#AvailableValues"
      },
      {
        "date": "2023-11-27T09:20:00.000Z",
        "voteCount": 1,
        "content": "Could someone please help me to add some comment on question #121 and other questions without any comment so that I can access them? I can't afford Contribution Access."
      },
      {
        "date": "2023-09-22T05:00:00.000Z",
        "voteCount": 1,
        "content": "i would also use 2 times query"
      },
      {
        "date": "2023-09-22T00:56:00.000Z",
        "voteCount": 1,
        "content": "Can same answer be drop twice ?"
      },
      {
        "date": "2023-09-14T02:39:00.000Z",
        "voteCount": 4,
        "content": "It should be query and query for both: -\nhttps://www.youtube.com/watch?v=XA5twJ2wLd0"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/microsoft/view/115737-exam-dp-500-topic-1-question-123-discussion/",
    "body": "You have a Power BI tenant.<br><br>You need to ensure that all reports use a consistent set of colors and fonts. The solution must ensure that the colors and fonts can be applied to existing reports.<br><br>What should you create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta report theme file\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta PBIX file",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Power BI template"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-26T01:20:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2023-09-15T00:54:00.000Z",
        "voteCount": 2,
        "content": "theme!"
      },
      {
        "date": "2023-07-19T03:54:00.000Z",
        "voteCount": 1,
        "content": "A, because of req to apply theme to existing reports"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/microsoft/view/114579-exam-dp-500-topic-1-question-124-discussion/",
    "body": "You need to use Power BI to ingest data from an API. The API requires that an API key be passed in the headers of the request.<br><br>Which type of authentication should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\torganizational account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBasic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWeb API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAnonymous"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-10T11:56:00.000Z",
        "voteCount": 7,
        "content": "I think it should be Web API because of the API key. Check this out: https://learn.microsoft.com/en-us/power-query/connector-authentication#select-an-authentication-method"
      },
      {
        "date": "2023-10-10T11:59:00.000Z",
        "voteCount": 3,
        "content": "this is a better link https://learn.microsoft.com/en-us/power-query/connectors/web/web"
      },
      {
        "date": "2023-09-25T11:17:00.000Z",
        "voteCount": 2,
        "content": "Basic authentication allows you to pass a username (in this case, the API key) and an optional password in the headers of the request, which is a common method for APIs that require API key-based authentication whereas Anonymous option is used when the API does not require any form of authentication, meaning you don't need to pass an API key in the headers and the question requires API key passed in the headers of the request."
      },
      {
        "date": "2023-09-25T11:20:00.000Z",
        "voteCount": 1,
        "content": "Also check this, https://community.fabric.microsoft.com/t5/Desktop/quot-A-web-API-Key-can-only-be-specified-when-a-web-API-key-name/td-p/2739227"
      },
      {
        "date": "2023-09-03T04:37:00.000Z",
        "voteCount": 1,
        "content": "change mind, answer is B\nThe specified API key typically falls under \"Basic\" authentication rather than anonymous authentication. Basic authentication involves passing a username and password or API key in the request to verify the source of the request. This method is usually used for authentication that requires a certain level of security but does not involve complex OAuth2.0 or Azure AD integration scenarios."
      },
      {
        "date": "2023-07-09T03:14:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is Anonymous, see https://community.fabric.microsoft.com/t5/Desktop/A-web-API-key-can-only-be-specified-when-a-web-API-key-name-is/m-p/547468/highlight/true#M257397"
      },
      {
        "date": "2023-09-03T04:29:00.000Z",
        "voteCount": 1,
        "content": "source \nhttps://www.dalesandro.net/accessing-rest-apis-with-basic-auth-and-api-key-in-power-query/"
      },
      {
        "date": "2023-09-15T00:55:00.000Z",
        "voteCount": 1,
        "content": "confirm! D (anonymous) not basic"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/microsoft/view/120786-exam-dp-500-topic-1-question-125-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>From Power Query Editor, you profile the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image144.png\"><br><br>The IoT GUID and IoT ID columns are unique to each row in the query.<br><br>You need to analyze IoT events by the hour and day of the year. The solution must improve dataset performance.<br><br>Solution: You change the IoT DateTime column to the date data type.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-15T00:39:00.000Z",
        "voteCount": 2,
        "content": "guess not, since you need 'hour'"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/microsoft/view/120787-exam-dp-500-topic-1-question-126-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>From Power Query Editor, you profile the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image144.png\"><br><br>The IoT GUID and IoT ID columns are unique to each row in the query.<br><br>You need to analyze IoT events by the hour and day of the year. The solution must improve dataset performance.<br><br>Solution: You remove the IoT ID column and retain the IoT GUID column.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-19T04:03:00.000Z",
        "voteCount": 1,
        "content": "you want to split IoT Date/Time by date and hour"
      },
      {
        "date": "2024-02-16T01:06:00.000Z",
        "voteCount": 1,
        "content": "Remove the GUID because it is a string column and in any power bi model the numeric columns query faster than string columns"
      },
      {
        "date": "2023-12-14T06:01:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nagree with Deloro : even though removing GUID is a better solution, removing ID also improves performance"
      },
      {
        "date": "2023-09-29T00:42:00.000Z",
        "voteCount": 1,
        "content": "check below why"
      },
      {
        "date": "2023-09-19T14:17:00.000Z",
        "voteCount": 2,
        "content": "It is better to keep ID (whole number) than GuID.\nSo correct answer is No"
      },
      {
        "date": "2023-09-15T00:41:00.000Z",
        "voteCount": 2,
        "content": "guess not, rather remove GUID's than ID's since its better to use surrogates"
      },
      {
        "date": "2023-09-29T00:42:00.000Z",
        "voteCount": 2,
        "content": "actually, i would say yes since you're removing data so it will be improving however i would remove the guid"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113785-exam-dp-500-topic-1-question-127-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this question, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>From Power Query Editor, you profile the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image144.png\"><br><br>The IoT GUID and IoT ID columns are unique to each row in the query.<br><br>You need to analyze IoT events by the hour and day of the year. The solution must improve dataset performance.<br><br>Solution: You split the IoT DateTime column into a column named Date and a column named Time.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-26T03:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct one"
      },
      {
        "date": "2024-01-15T02:22:00.000Z",
        "voteCount": 1,
        "content": "answer: A"
      },
      {
        "date": "2023-12-14T06:03:00.000Z",
        "voteCount": 1,
        "content": "answer : A"
      },
      {
        "date": "2023-12-14T06:14:00.000Z",
        "voteCount": 1,
        "content": "splitting datetime will remove redundant data and keeps dataset size down"
      },
      {
        "date": "2023-09-30T07:16:00.000Z",
        "voteCount": 2,
        "content": "Yes, for sure . It is in best practises"
      },
      {
        "date": "2023-08-18T23:30:00.000Z",
        "voteCount": 3,
        "content": "I took the exam a few days ago (14/8/2023) and I passed the exam with a score of 915.\nMy answer was:\n- Yes"
      },
      {
        "date": "2023-07-29T06:55:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is : A\n\nCheck this link : https://powerbi.microsoft.com/fr-be/blog/best-practice-rules-to-improve-your-models-performance/"
      },
      {
        "date": "2023-07-19T01:30:00.000Z",
        "voteCount": 1,
        "content": "Split Date and time do improve dataset performance"
      },
      {
        "date": "2023-06-30T16:18:00.000Z",
        "voteCount": 1,
        "content": "No, this does not meet the goal. Splitting the IoT DateTime column into separate Date and Time columns will not improve dataset performance or enable analysis by the hour and day of the year. Splitting the column only separates the date and time components, but it does not provide the necessary functionality to analyze events by the hour and day of the year."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113784-exam-dp-500-topic-1-question-128-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI tenant that uses a primary domain named fabrikam.com and contains a Power BI Premium workspace named Workspace1. Workspace1 contains a dataset named DS1. DS1 is an imported model that has one data source.<br><br>You have a guest user named user1@contoso.com that is assigned the Contributor role for Workspace1.<br><br>You need to ensure that User1 can connect to the XMLA endpoint of DS1.<br><br>How should you complete the URL of the XMLA connection? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image145.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image146.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-14T07:26:00.000Z",
        "voteCount": 3,
        "content": "Contoso and workspace1\n\"B2B users must specify their organization UPN in tenant name\". \nin this question, Contoso is the B2B tenant.\n\ncheck the link \nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools#connecting-to-a-premium-workspace"
      },
      {
        "date": "2023-10-26T07:11:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says Fabrikam.com and DS1"
      },
      {
        "date": "2023-09-15T00:49:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools\n&gt; would go for contoso and workspace \n&gt; it states that myorg can also be used (in the same tenant) \n&gt; fabrikam would be B2B\n\n\nWorkspaces assigned to a capacity have a connection string in URL format. For example:\n\npowerbi://api.powerbi.com/v1.0/[tenant name]/[workspace name].\n\nApplications connecting to the workspace use the URL as if it were an Analysis Services server name. For example:\n\npowerbi://api.powerbi.com/v1.0/contoso.com/Sales Workspace.\n\nUsers with UPNs in the same tenant (not B2B) can replace the tenant name with myorg. For example:\n\npowerbi://api.powerbi.com/v1.0/myorg/Sales Workspace.\n\nB2B users must specify their organization UPN in tenant name. For example:\n\npowerbi://api.powerbi.com/v1.0/fabrikam.com/Sales Workspace."
      },
      {
        "date": "2023-09-15T00:50:00.000Z",
        "voteCount": 4,
        "content": "sorry, fabrikam and workspace since you have a guest user!"
      },
      {
        "date": "2023-09-04T23:19:00.000Z",
        "voteCount": 2,
        "content": "agreee with DS1\npowerbi://api.powerbi.com/v1.0/fabrikam.com/DS1"
      },
      {
        "date": "2023-09-04T23:38:00.000Z",
        "voteCount": 2,
        "content": "#Edit\npowerbi://api.powerbi.com/v1.0/myorg/Workspace1/DS1\n\n#URL\nhttps://learn.microsoft.com/zh-cn/power-bi/enterprise/service-premium-connect-tools"
      },
      {
        "date": "2023-09-15T00:51:00.000Z",
        "voteCount": 2,
        "content": "workspace see link https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools"
      },
      {
        "date": "2023-08-05T12:17:00.000Z",
        "voteCount": 2,
        "content": "\"B2B users must specify their organization UPN in tenant name\"\nAnswer would be contoso.com and Workspace1"
      },
      {
        "date": "2023-07-29T23:34:00.000Z",
        "voteCount": 3,
        "content": "Answer is frabrikam.com  \n, workspace1  for guest user .  /myorg only work for local users (same tennant)\n \nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools"
      },
      {
        "date": "2023-06-30T16:17:00.000Z",
        "voteCount": 2,
        "content": "To complete the URL of the XMLA connection for User1 to connect to the XMLA endpoint of DS1 in Workspace1, you should use the following format:\n\npowerbi://api.powerbi.com/v1.0/myorg/Workspace1/DS1"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113786-exam-dp-500-topic-1-question-130-discussion/",
    "body": "You have a Power BI report hosted on the Power BI service. The report displays expenses by department for department managers and contains a line chart that shows expenses by month.<br><br>You need to ensure that users can choose between viewing the report as a line chart or a column chart. The solution must minimize development and maintenance effort.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a mobile report that contains a column chart.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a column chart a bookmark, and a button for users to choose a visual.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable report readers to personalize visuals.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate report page for users to view a column chart."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-22T10:51:00.000Z",
        "voteCount": 1,
        "content": "this question appeared on PB-300, POWERBI's exam. its B. a bookmork to hold the button for choosing column and line chart"
      },
      {
        "date": "2023-06-30T16:26:00.000Z",
        "voteCount": 2,
        "content": "Enable report readers to personalize visuals allows users to customize the visuals based on their preferences. They can choose to switch between a line chart and a column chart without requiring additional development or maintenance effort from the report creator."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113783-exam-dp-500-topic-1-question-131-discussion/",
    "body": "You have a PostgreSQL database named db1.<br><br>You have a group of data analysts that will create Power BI datasets. Each analyst will use data from a different schema in db1.<br><br>You need to simplify the process for the analysts to initially connect to db1 when using Power BI Desktop.<br><br>Which type of file should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIX",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPBIDS\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-30T16:14:00.000Z",
        "voteCount": 3,
        "content": "PBIDS is correct. To simplify the process for the data analysts to initially connect to the PostgreSQL database (db1) when using Power BI Desktop, you should use a PBIDS (Power BI Data Source) file."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/microsoft/view/115741-exam-dp-500-topic-1-question-132-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are building a financial report by using Power BI.<br><br>You have a table named financials that contains two columns named Date and Sales.<br><br>You need to create a DAX measure that calculates the relative change in sales compared to the previous quarter.<br><br>How should you complete the measure? To answer, select the appropriate options m the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image147.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image148.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-14T07:47:00.000Z",
        "voteCount": 1,
        "content": "CALCULATE\nDATEADD\nDIVIDE"
      },
      {
        "date": "2023-09-05T00:47:00.000Z",
        "voteCount": 1,
        "content": "Sales QoQ% =\nIF (\n  NOT ISFILTERED('financials'[Date]),\n  ERROR(\"Uh oh.\"),\n  VAR PREV_QUARTER =\n    CALCULATE(\n      SUM('financials'[Sales]),\n      DATEADD('financials'[Date].[Date], -1, QUARTER)\n    )\n  RETURN\n    DIVIDE(SUM('financials'[Sales]) - PREV_QUARTER, PREV_QUARTER)\n)"
      },
      {
        "date": "2023-08-07T03:26:00.000Z",
        "voteCount": 3,
        "content": "CALCULATE\nDATEADD\nDIVIDE"
      },
      {
        "date": "2023-07-19T04:39:00.000Z",
        "voteCount": 3,
        "content": "CALCULATE\nDATEADD\nDIVIDE"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113782-exam-dp-500-topic-1-question-133-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br><br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br><br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsummary\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcov",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsample",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trollup"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-30T16:13:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/microsoft/view/113781-exam-dp-500-topic-1-question-134-discussion/",
    "body": "You have an Azure Synapse Analytics dataset that contains data about jet engine performance.<br><br>You need to score the dataset to identify the likelihood of an engine failure.<br><br>Which function should you use in the query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPREDICT\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCAST",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGROUPING",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPIVOT"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-09-26T11:42:00.000Z",
        "voteCount": 2,
        "content": "Predict!"
      },
      {
        "date": "2023-08-07T03:31:00.000Z",
        "voteCount": 1,
        "content": "Predict.\nFor sure!"
      },
      {
        "date": "2023-06-30T16:12:00.000Z",
        "voteCount": 1,
        "content": "Cirrect"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126110-exam-dp-500-topic-1-question-136-discussion/",
    "body": "DRAG DROP<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br><br>Existing environment<br> -<br><br>Litware, Inc. is a retail company that sells outdoor recreational goods and accessories. The company sells goods both online and its stores located in six countries.<br><br><br>Azure Resources<br> -<br><br>Litware has the following Azure resources:<br><br>\u2022\tAn Azure Synapse Analytics workspace named synapseworkspace1<br>\u2022\tAn Azure Data Lake Storage Gen2 account named datalake1 that is associated with synapseworkspace1<br>\u2022\tA Synapse Analytics dedicated SQL pool named SQLDW<br><br><br>Dedicated SQL Pool<br> -<br><br>SQLDW contains a dimensional model that contains the following tables.<br><br><img src=\"https://img.examtopics.com/dp-500/image149.png\"><br><br>SQLDW contains the following additional tables.<br><br><img src=\"https://img.examtopics.com/dp-500/image150.png\"><br><br>SQLDW contains a view named dbo.CustomerPurchases that creates a distinct list of values from dbo.Customer [customerID], dbo.Customer [CustomerEmail], dbo.Product [ProductID] and dbo.Product [ProductName].<br><br>The sales data in SQLDW is updated every 30 minutes. Records in dbo.SalesTransactions are updated in SQLDW up to three days after being created. The records do NOT change after three days.<br><br><br>Power BI<br> -<br><br>Litware has a new Power BI tenant that contains an empty workspace named Sales Analytics.<br><br>All users have Power BI Premium per user licenses.<br><br>IT data analytics are workspace administrators. The IT data analysts will create datasets and reports.<br><br>A single imported dataset will be created to support the company\u2019s sales analytics goals. The dataset will be refreshed every 30 minutes.<br><br><br>Requirements<br> -<br><br><br>Analytics Goals<br> -<br><br>Litware identifies the following analytics goals:<br><br>\u2022\tProvide historical reporting of sales by product and channel over time.<br>\u2022\tAllow sales managers to perform ad hoc sales reporting with minimal effort.<br>\u2022\tPerform market basket analysis to understand which products are commonly purchased in the same transaction.<br>\u2022\tIdentify which customers should receive promotional emails based on their likelihood of purchasing promoted products.<br><br>Litware plans to monitor the adoption of Power BI reports over time. The company wants custom Power BI usage reporting that includes the percent change of users that view reports in the Sales Analytics workspace each month.<br><br><br>Security Requirements<br> -<br><br>Litware identifies the following security requirements for the analytics environment:<br><br>\u2022\tAll the users in the sales department and the marketing department must be able to see Power BI reports that contain market basket analysis and data about which customers are likely to purchase a product.<br>\u2022\tCustomer contact data in SQLDW and the Power BI dataset must be labeled as Sensitive. Records must be kept of any users that use the sensitive data.<br>\u2022\tSales associates must be prevented from seeing the CustomerEmail column in Power BI reports.<br>\u2022\tSales managers must be prevented from modifying reports created by other users.<br><br>Development Process Requirements<br><br>Litware identifies the following development process requirements:<br><br>\u2022\tSQLDW and datalake1 will act as the development environment. Once feature development is complete, all entities in synapseworkspace1 will be promoted to a test workspace, and then to a production workspace.<br>\u2022\tPower BI content must be deployed to test and production by using deployment pipelines.<br>\u2022\tAll SQL scripts must be stored in Azure Repos.<br><br>The IT data analysts prefer to build Power BI reports in Synapse Studio.<br><br><br>You have an Azure Synapse Analytics serverless SQL pool.<br><br>You need to return a list of files and the number of rows in each file.<br><br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image153.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image154.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-14T07:58:00.000Z",
        "voteCount": 1,
        "content": "Count Big\nOpenRowSet"
      },
      {
        "date": "2023-11-14T07:43:00.000Z",
        "voteCount": 3,
        "content": "Same question as 45. So the answer is ACTUALLY:\nCount Big\nOpenRowSet"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128173-exam-dp-500-topic-1-question-137-discussion/",
    "body": "You are using a Python notebook in an Apache Spark pool in Azure Synapse Analytics.<br><br>You need to present the data distribution statistics from a DataFrame in a tabular view.<br><br>Which method should you invoke on the DataFrame?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsummary",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\texplain",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcov",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcorr"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-09T23:24:00.000Z",
        "voteCount": 1,
        "content": "Question is same as Q62 and Q101 see answers there too"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128592-exam-dp-500-topic-1-question-138-discussion/",
    "body": "You deploy a tabular model named DM1 to a Power BI Premium capacity. DM1 was created as an import model.<br><br>You change a fact table named Table1 into a hybrid table.<br><br>What else occurred on DM1 automatically after the change?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTable1 was removed from the scheduled refresh.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDimension tables that relate to Table1 were changed to dual storage mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA related aggregation table was created.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA DirectQuery partition was added to Table1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-14T08:25:00.000Z",
        "voteCount": 1,
        "content": "https://community.fabric.microsoft.com/t5/Desktop/What-is-the-purpose-of-Hybrid-Tables-w-Incremental-Refresh/td-p/3069472#:~:text=Hybrid%20tables%20can%20complement%20incremental,interacts%20with%20the%20data%20model."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126257-exam-dp-500-topic-1-question-139-discussion/",
    "body": "You have a Power BI tenant.<br><br>You invite an external consultant to work in the tenant.<br><br>You need to grant the consultant access to the tenant. The solution must meet the following requirements:<br><br>\u2022\tThe consultant must be able to consume, create, and update reports and datasets.<br>\u2022\tThe consultant must access content by using Azure AD B2B.<br><br>Which settings should you enable in the Power BI Admin portal? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInvite external users to your organization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow Azure Active Directory guest users to access Power BI",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow Azure Active Directory guest users to edit and manage content in the organization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExternal sharing"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-10T00:49:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answers are A B and C as per the information in \nhttps://learn.microsoft.com/en-us/power-bi/guidance/whitepaper-azure-b2b-power-bi#ad-hoc-or-planned-sharing-of-power-bi-apps"
      },
      {
        "date": "2023-11-20T01:43:00.000Z",
        "voteCount": 1,
        "content": "Correct Answers: B &amp; C"
      },
      {
        "date": "2023-11-15T05:38:00.000Z",
        "voteCount": 3,
        "content": "Asked CGPT:\n\nSo, the correct selections are:\n\nB. Allow Azure Active Directory guest users to access Power BI\nC. Allow Azure Active Directory guest users to edit and manage content in the organization"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128180-exam-dp-500-topic-1-question-140-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are enhancing a Power BI model that has DAX calculations.<br><br>You need to create a DAX measure that returns the year-to-date total sales from the same date from the previous calendar year.<br><br>How should you complete the measure? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image155.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image156.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-10T00:56:00.000Z",
        "voteCount": 2,
        "content": "Is Correct, tried it out too. Also refer to similar question\nhttps://www.examtopics.com/discussions/microsoft/view/51355-exam-da-100-topic-4-question-6-discussion/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128182-exam-dp-500-topic-1-question-141-discussion/",
    "body": "You have a Power BI tenant that is linked to an Azure AD tenant. The tenant contains a group named Group1.<br><br>You create a paginated report that does NOT use row-level security (RLS).<br><br>You need to send a copy of the report to Group1 every Monday at 9 AM UTC.<br><br>Which two actions should you include in the solution? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a subscription for Group1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the paginated report to a Pro workspace.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a data alert for Group1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the paginated report to a Premium Per User workspace."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-22T08:38:00.000Z",
        "voteCount": 2,
        "content": "Pro licence is sufficient to publish paginated reports. \nhttps://learn.microsoft.com/en-us/power-bi/paginated-reports/paginated-reports-save-to-power-bi-service#requirements"
      },
      {
        "date": "2023-12-10T01:03:00.000Z",
        "voteCount": 2,
        "content": "Although it is a correct answer but i would suggest to go with pro license as it is cheaper than PPU.\nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/end-user-subscribe?tabs=creator"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128183-exam-dp-500-topic-1-question-142-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>From Power Query Editor, you profile the data shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image157.png\"><br><br>The IoT GUID and IoT ID columns are unique to each row in the query.<br><br>You need to analyze IoT events by the hour and day of the year. The solution must improve dataset performance.<br><br>Solution: You remove the IoT GUID column and retain the IoT ID column.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-23T01:32:00.000Z",
        "voteCount": 1,
        "content": "Yes,  IoT ID column is enough"
      },
      {
        "date": "2023-12-10T01:09:00.000Z",
        "voteCount": 1,
        "content": "Question caem in PL-300 link to correct answer's Question: https://www.examtopics.com/discussions/microsoft/view/94716-exam-pl-300-topic-2-question-50-discussion/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127112-exam-dp-500-topic-1-question-143-discussion/",
    "body": "You have a Power BI tenant and an Azure subscription named Sub1. The Power BI tenant and Sub1 are linked to a single Azure AD tenant.<br><br>In Sub1, you create a storage account named storage1.<br><br>You need to configure a Power BI workspace to store dataflows in storage1. The solution must use the principle of least privilege.<br><br>Which three roles should you assign for storage1? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReader",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Contributor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContributor",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOwner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Owner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Reader\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "DEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "DEF",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-14T08:38:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2023-11-30T19:36:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-11-24T06:22:00.000Z",
        "voteCount": 1,
        "content": "DEF according to the document: https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      },
      {
        "date": "2023-11-24T06:21:00.000Z",
        "voteCount": 1,
        "content": "DEF according to the document: https://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126667-exam-dp-500-topic-1-question-144-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a team of business analysts that uses Power BI for departmental analytics.<br><br>The team creates and manages PBIX files.<br><br>You need to implement a source control solution for the PBIX files. The solution must meet the following requirements:<br><br>\u2022\tProvide version tracking and roll back.<br>\u2022\tMinimize version conflicts between edits.<br>\u2022\tEnsure that all the analysts can access the files.<br>\u2022\tPrevent multiple analysts from editing the same version of a file.<br><br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image158.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image159.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-03-31T11:55:00.000Z",
        "voteCount": 1,
        "content": "From Sabbths document is supposed to be OneDrive or SharePoint no Git."
      },
      {
        "date": "2023-11-24T06:48:00.000Z",
        "voteCount": 4,
        "content": "According to Doc, the best answers seem to be SharePoint and file checkout: https://learn.microsoft.com/en-us/training/modules/design-power-bi-application-lifecycle-management-strategy/3-recommend-source-control-strategy"
      },
      {
        "date": "2023-11-20T02:10:00.000Z",
        "voteCount": 2,
        "content": "The correct answers are: Git and Lock file"
      },
      {
        "date": "2023-11-20T02:09:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is : File locking"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127113-exam-dp-500-topic-1-question-145-discussion/",
    "body": "You have a Power BI workspace named workspace1 that contains three reports and two dataflows.<br><br>You have an Azure Data Lake Storage account named storage1.<br><br>You need to integrate workspace1 and storage1.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn storage1, create a folder named dataflows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the dataflows from workspace1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn storage1, create a folder named reports.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the reports from workspace1."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-14T09:07:00.000Z",
        "voteCount": 1,
        "content": "workspace should be empty of dataflows before connection to ADLS2"
      },
      {
        "date": "2023-11-30T19:48:00.000Z",
        "voteCount": 1,
        "content": "Correct. You have to first delete all dataflows from the workspace\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration#detach-azure-data-lake-gen-2-from-a-workspace-or-tenant"
      },
      {
        "date": "2023-11-24T06:50:00.000Z",
        "voteCount": 2,
        "content": "\"Finally, you can connect to any ADLS Gen 2 from the Admin portal, but if you connect directly to a workspace, you must first ensure there are no dataflows in the workspace before connecting.\"\nhttps://learn.microsoft.com/en-us/power-bi/transform-model/dataflows/dataflows-azure-data-lake-storage-integration"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127115-exam-dp-500-topic-1-question-146-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI tenant that is configured as shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image160.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image161.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image162.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-09T18:11:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt sayd B,B :\n\n1. A new external user \"must be added in Azure AD to access Power BI content\" since the ability to invite external users to your organization is disabled.\n\n2. For an existing guest user to edit a report, you must modify a \"tenant\" setting, as the current tenant settings do not allow Azure Active Directory guest users to edit and manage content in the organization."
      },
      {
        "date": "2023-11-30T19:52:00.000Z",
        "voteCount": 2,
        "content": "I believe BB. it says a 'NEW' external user which implies that he is not yet added to Azure AD"
      },
      {
        "date": "2023-11-24T07:00:00.000Z",
        "voteCount": 1,
        "content": "I think it will be C and A (report):\n\"Allow Azure Active Directory guest users to edit and manage content in the organization\nThis setting allows Azure AD B2B guest users to have full access to the browsing experience using the left-hand navigation pane in the organization. Guest users who have been assigned workspace roles or specific item permissions continue to have those roles and/or permissions, even if this setting is disabled.\"\nhttps://learn.microsoft.com/en-us/fabric/admin/service-admin-portal-export-sharing"
      },
      {
        "date": "2023-12-10T05:16:00.000Z",
        "voteCount": 1,
        "content": "If we refer to \"Power BI Admin Portal provides the *allow external guest users to edit and manage content in the organization setting* in Tenant settings\" so for second drop down option B \"Tenant\" is correct. I think we should stick to it because other settings are on similar lines that are shared in question."
      },
      {
        "date": "2023-12-10T05:16:00.000Z",
        "voteCount": 1,
        "content": "link to above forgot to provide: https://powerbi.microsoft.com/nl-be/blog/azure-ad-b2b-guest-users-can-now-edit-and-manage-content-in-power-bi-to-collaborate-better-across-organizations/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128470-exam-dp-500-topic-1-question-147-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a Power BI data model that contains two tables named Products and Sales.<br><br>A one-to-many relationship exists between the tables.<br><br>You have a report that contains a report-level filter for Products.<br><br>You need to create a DAX measure that will return the percent of total sales for each product. The measure must respect the report-level filter when calculating the total.<br><br>How should you complete the measure? To answer, drag the appropriate DAX functions to the correct targets. Each function may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image163.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image164.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-07-10T05:30:00.000Z",
        "voteCount": 1,
        "content": "It's CALCULATE and ALLSELECTED.\nWhen ALLSELECTED is used, all the report level filters are preserved and that is what exactly the question is about."
      },
      {
        "date": "2023-12-14T09:16:00.000Z",
        "voteCount": 1,
        "content": "1. CALCULATE\n2. ALLSELECTED\n\nsecond is not ALL because \"ALLSELECTED only ignores the filters coming from inside the visual itself, whereas ALL ignores any filters coming from anywhere in this visual or other visual on the page\"\n\nhttps://radacad.com/power-bi-dax-all-vs-allselected#:~:text=The%20ALLSELECTED%20is%20also%20a,other%20visual%20on%20the%20page."
      },
      {
        "date": "2023-12-13T02:47:00.000Z",
        "voteCount": 2,
        "content": "For dropbox 2 it should be ALLSELECTED b/c question states: The measure must respect the report-level filter when calculating the total.\nhttps://learn.microsoft.com/en-us/dax/allselected-function-dax"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127568-exam-dp-500-topic-1-question-148-discussion/",
    "body": "You have an Azure Synapse Analytics notebook.<br><br>You run the %%sql magic command to render data into an Apache Spark DataFrame named df1, and then you run the following code.<br><br>display(df1, summary = true)<br><br>Which three attributes will be returned by the command? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttype\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trange",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmissing\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tunique\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tordinal"
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 4,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T02:27:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-12-13T02:52:00.000Z",
        "voteCount": 2,
        "content": "Correct\nReference: https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-data-visualization"
      },
      {
        "date": "2023-11-30T20:52:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/microsoft/view/134739-exam-dp-500-topic-1-question-151-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Power BI Premium capacity.<br><br>From the Power BI Premium Capacity Metrics app, you discover the following:<br><br>\u2022\tThere is insufficient CPU to execute dataset refreshes.<br>\u2022\tOut-of-memory throttling occurs when the dataset is waiting.<br><br>You need to recommend a solution to resolve the performance issues.<br><br>Solution: You scale up the capacity.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T02:31:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128205-exam-dp-500-topic-1-question-152-discussion/",
    "body": "You have a Power BI report that contains a card. The card displays the value for year-to-date revenue.<br><br>You need to ensure that screen reader users can read the value when initially interacting with the card. The value must stay updated as the dataset is refreshed.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the card into a text box.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the alt text by using conditional formatting with a DAX measure.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the value to the chart title text.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPopulate the alt text with a static value."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T02:31:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-09T18:18:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nB. Populate the alt text by using conditional formatting with a DAX measure.\n\nThis approach allows the alt text to be dynamically updated as the dataset refreshes, ensuring that the value read by screen readers is current. Static values or converting the card into a text box would not meet the requirement for the value to stay updated. Adding the value to the chart title text would make it visible to all users, not just screen reader users, and might not be the intended design."
      },
      {
        "date": "2023-12-10T05:45:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct because when a report consumer navigates to a visual, the screen reader will read out the title, visual type, and any alt text if that has been set.\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/desktop-accessibility-creating-reports"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128208-exam-dp-500-topic-1-question-154-discussion/",
    "body": "You are creating a Power BI report that will contain multiple visuals.<br><br>You need to ensure that the report is accessible to users who use a screen reader.<br><br>Which two configurations should you perform? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconsistent colors for each visual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tconsistent fonts for each visual",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\talt text for each visual\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe tab order for each page\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlay Axis for each page"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T18:28:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt:\nTo ensure that the report is accessible to users who use a screen reader, you should perform the following configurations:\n\nC. Alt text for each visual - This provides a textual description of the visual, which can be read by screen readers.\n\nD. The tab order for each page - This allows screen reader users to navigate through the visuals in a logical sequence.\n\nConsistent colors (A) and fonts (B) might improve general accessibility but are not specifically related to screen reader accessibility. Play Axis (E) is a feature for playing animations in visuals and does not relate to screen reader accessibility."
      },
      {
        "date": "2023-12-10T06:07:00.000Z",
        "voteCount": 2,
        "content": "C: Alt Text is read out loud by screen reader so it is definitely correct\nD: Tab Order is for keyboard navigation but if any visual tab order is marked as hidden then they are not announced by screen reader so this setting could be used in this sense but tab order for page still seems not a good option but this is better than other options available so i would go with it"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128209-exam-dp-500-topic-1-question-155-discussion/",
    "body": "HOTSPOT<br> -<br><br>You deploy a tabular model to the Power BI service. The model is developed and maintained by using Microsoft Visual Studio. Power BI connects to the model by using the Connect live option.<br><br>You need to recommend a source control solution for the model and the report files.<br><br>What should you include in the recommendation for each type of file? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image168.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image169.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-14T09:32:00.000Z",
        "voteCount": 5,
        "content": "OneDrive\nGit\n\ncheck Best practices section in below link\n\n\"If developing .pbix and .bim files, still use OneDrive or SharePoint for .pbix files, and use Git for .bim files.\"\nhttps://learn.microsoft.com/en-us/training/modules/design-power-bi-application-lifecycle-management-strategy/3-recommend-source-control-strategy"
      },
      {
        "date": "2024-03-14T13:01:00.000Z",
        "voteCount": 1,
        "content": "git for everything!"
      },
      {
        "date": "2024-01-09T18:30:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt :\nFor a source control solution to manage different versions and changes of code and files, the following recommendations would apply:\n\n- .pbix files: **Git repository**\n- .bim files: **Git repository**\n\nGit repositories are designed to handle source control, providing versioning, branching, and merging capabilities, which are essential for development processes, including those for Power BI report files (.pbix) and tabular model files (.bim)."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129092-exam-dp-500-topic-1-question-156-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool that contains a table named dbo.DimProduct.<br><br>You need to write a query to meet the following requirements:<br><br>\u2022\tReturn the ranked position of each product ordered by list price descending within each product category, allowing gaps in the ranking values after ties.<br>\u2022\tSpecify the quartile in which each product's list price falls within each product category.<br><br>How should you complete the query? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image170.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image171.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-09T18:32:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt:\nTo complete the query according to the requirements:\n\n- To return the ranked position of each product ordered by list price descending within each product category, allowing gaps in the ranking values after ties, you should use `RANK()` function because it will assign the same rank to tied values, with the next values receiving a rank that includes the gap (e.g., 1, 2, 2, 4).\n\n- To specify the quartile in which each product's list price falls within each product category, you should use `NTILE(4)` function because it will divide the rows into four equal groups and assign a quartile number to each row.\n\nThe final selections should be:\n\n- For ranking: `RANK() OVER (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Rank`\n- For quartile: `NTILE(4) OVER (PARTITION BY ProductCategory ORDER BY ListPrice DESC) AS Quartile`"
      },
      {
        "date": "2023-12-20T07:12:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126267-exam-dp-500-topic-1-question-157-discussion/",
    "body": "You have a Power BI report named Report1. Report1 is connected to an imported dataset that contains a matrix. The matrix displays five measures.<br><br>When users view the report, they discover that the matrix is slow to render.<br><br>You need to review the execution times and durations in the formula engine and the storage engine for the DAX query executed by the matrix.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDAX Studio\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server Profiler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerformance analyzer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVertipaq Analyzer"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-22T22:52:00.000Z",
        "voteCount": 2,
        "content": "DAX Studio"
      },
      {
        "date": "2023-12-19T09:30:00.000Z",
        "voteCount": 1,
        "content": "DAX Studio, no doubt"
      },
      {
        "date": "2023-12-10T06:31:00.000Z",
        "voteCount": 2,
        "content": "refer link: https://www.sqlbi.com/articles/formula-engine-and-storage-engine-in-dax/"
      },
      {
        "date": "2023-12-01T08:37:00.000Z",
        "voteCount": 1,
        "content": "The answer is Dax Studio"
      },
      {
        "date": "2023-11-15T06:20:00.000Z",
        "voteCount": 3,
        "content": "Should be A, DAX Studio."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127591-exam-dp-500-topic-1-question-158-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI report and dataset in Power BI Desktop. The dataset contains a calculation group named CG1 that contains four calculation items. The Current calculation item is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-500/image172.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image173.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image174.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-26T02:01:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer"
      },
      {
        "date": "2023-12-01T08:39:00.000Z",
        "voteCount": 1,
        "content": "I think it's correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129094-exam-dp-500-topic-1-question-159-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics workspace.<br><br>You create an Azure Data Lake Storage Gen2 account and upload a CSV file named File1.csv.<br><br>You need to use Synapse Studio to query the data in File1.csv by using a serverless SQL pool.<br><br>Which Transact-SQL operator should you include in the query?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOPENQUERY",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOPENROWSET",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSTRING_SPLIT",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOPENDATASOURCE"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-20T07:17:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129096-exam-dp-500-topic-1-question-160-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>The enterprise analytics team needs to notify the report owners of the planned changes. The solution must minimize administrative effort.<br><br>What should the team do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocate all the downstream reports, identify all the users that are assigned the Admin role, and then send an email to each user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a PowerShell script to access the Power BI API that retrieves a list of downstream reports and a list of workspace administrators, and then send an email to each workspace administrator.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform impact analysis on the Financial Model dataset to notify the workspace contacts.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform impact analysis feature on the lake database data source to notify the workspace contacts."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-20T07:25:00.000Z",
        "voteCount": 1,
        "content": "Correct answer \nhttps://learn.microsoft.com/en-us/power-bi/collaborate-share/service-dataset-impact-analysis"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128620-exam-dp-500-topic-1-question-161-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to recommend a solution for the analysts in the Finance and Accounting business unit to mitigate the increase in maintenance of their assets in the Power BI tenant.<br><br>Which two actions should you recommend? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Microsoft Purview to search for datasets that contain the relevant data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform impact analysis on the relevant data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a live connection to a Power BI dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Power BI template app."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-04-02T03:28:00.000Z",
        "voteCount": 1,
        "content": "Options B and D, while useful in certain contexts, do not directly address the primary issue of analysts creating new datasets unnecessarily. Impact analysis is more about understanding the consequences of changes to existing datasets, and template apps are a way to package and distribute content, which doesn't directly prevent the creation of new, redundant datasets."
      },
      {
        "date": "2024-01-09T18:51:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nThe correct actions to recommend for the analysts in the Finance and Accounting business unit to mitigate the increase in maintenance of their assets in the Power BI tenant are:\n\nA. Use Microsoft Purview to search for datasets that contain the relevant data.\nC. Create a live connection to a Power BI dataset.\n\nAction A helps analysts find existing datasets, preventing unnecessary duplication. Action C encourages the reuse of existing datasets through live connections, which also helps to reduce redundancy and maintenance. Action B is useful for understanding the impact of changes, but it does not directly address the problem of analysts creating new datasets when existing ones already meet their needs."
      },
      {
        "date": "2024-01-09T18:59:00.000Z",
        "voteCount": 1,
        "content": "Finaly its AB"
      },
      {
        "date": "2023-12-20T07:28:00.000Z",
        "voteCount": 1,
        "content": "A. Use Microsoft Purview to search for datasets that contain the relevant data.\nB. Perform Impact Analysis"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128622-exam-dp-500-topic-1-question-162-discussion/",
    "body": "HOTSPOT<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br><br>General Overview<br> -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure<br> -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment<br> -<br><br><br>Data Storage<br> -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting<br> -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems<br> -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements<br> -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to recommend a security solution for the Power BI tenant to control external data sharing. The solution must meet the technical requirements.<br><br>What should you recommend? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image176.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image177.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-23T00:56:00.000Z",
        "voteCount": 2,
        "content": "answer is correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126264-exam-dp-500-topic-1-question-163-discussion/",
    "body": "HOTSPOT<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br><br>General Overview<br> -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure<br> -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment<br> -<br><br><br>Data Storage<br> -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting<br> -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems<br> -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements<br> -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>The enterprise analytics team needs to create a view for the survey response data. The solution must meet the technical requirements.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image178.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image179.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-23T01:04:00.000Z",
        "voteCount": 6,
        "content": "First is obviously BULK\nSecond is CSV not JSON. in Microsoft documentation, it's clearly mentioned to use 'csv' as FORMAT in OPENROWSET.\n\nsee link:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-documents"
      },
      {
        "date": "2023-12-10T07:24:00.000Z",
        "voteCount": 1,
        "content": "Second is definitely json but for first there is already BULK in statement so we don't to place any keyword before that this question is very confusing"
      },
      {
        "date": "2023-11-15T06:13:00.000Z",
        "voteCount": 4,
        "content": "Should be BULK and JSON."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127107-exam-dp-500-topic-1-question-164-discussion/",
    "body": "DRAG DROP<br> -<br><br>This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study<br> -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview<br> -<br><br><br>General Overview<br> -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure<br> -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment<br> -<br><br><br>Data Storage<br> -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting<br> -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems<br> -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements<br> -<br><br><br>Planned Changes<br> -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements<br> -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to ensure that the new process for deploying reports and datasets to the User Experience workspace meets the technical requirements.<br><br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-500/image180.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image181.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-15T07:44:00.000Z",
        "voteCount": 2,
        "content": "1.Create a Power BI deployment pipeline.\n2.Assign the workspaces to the correct stages.\n3.Create deployment rules for the test and prod stages.\n\n3: dev stage does not provide option to define deployment rules"
      },
      {
        "date": "2023-11-24T03:19:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is: \nCreate a Power BI deployment pipeline.\nAssign the workspaces to the correct stages.\nCreate deployment rules for the development and test stages."
      },
      {
        "date": "2023-12-01T10:48:00.000Z",
        "voteCount": 1,
        "content": "I agree but I have a doubt : all 30 PowerBI workspaces are in Pro license, while premium licensing is required for deployment pipelines in Power bi ?"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127594-exam-dp-500-topic-1-question-165-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>The enterprise analytics team needs to resolve the DAX measure performance issues.<br><br>What should the team do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Performance analyzer in Power BI Desktop to get the DAX durations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DAX Studio to get detailed statistics on the server timings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse DAX Studio to review the Vertipaq Analyzer metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Tabular Editor to create calculation groups."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-12T12:51:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer."
      },
      {
        "date": "2023-12-27T08:32:00.000Z",
        "voteCount": 3,
        "content": "B should be the right answer.\nPerformance Analyzer indicates if slowliness is coming from DAX, visual or waiting time (other), while here, it's already identified that slowliness is at DAX level. Hence, Performance Analyzer has no added value. In addition, DAX Studio gives detailed metrics about CPU, Storage Engine, Formula Engine,... durations (and more) which can identify the root cause of DAX low performance. \ncheck link:\nhttps://blog.enterprisedna.co/query-plan-server-timings-in-dax-studio/"
      },
      {
        "date": "2023-12-01T10:56:00.000Z",
        "voteCount": 1,
        "content": "A should be correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128214-exam-dp-500-topic-1-question-166-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to optimize the workflow for the creation of reports and the adjustment of tables by the enterprise analytics team.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a tenant-level storage connection to Power BI.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a linked service in workspace1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an integration runtime in workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant setting, enable Use global search for Power BI."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-23T01:25:00.000Z",
        "voteCount": 1,
        "content": "B is correct. \nAnalysts should switch between Power BI and Synapse workspaces which is \"tedious\" as mentioned in the case study. solution is to link Power BI service to Synapse workspace to enable working in the same platform."
      },
      {
        "date": "2023-12-10T07:38:00.000Z",
        "voteCount": 1,
        "content": "Answer B seems incorrect but i am unsure of what is the correct answer. Someone please confirm if it is clear to them"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/microsoft/view/132632-exam-dp-500-topic-1-question-167-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to meet the technical requirements for deploying reports and datasets to the User Experience workspace.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Corporate Data Models and User Experience workspaces, select Allow contributors to update the app.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Corporate Data Models and User Experience workspace, set License mode to Premium per user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Tenant settings, set Allow specific users to turn on external data sharing to Enable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Developer settings, set Allow service principals to use Power BI APIs to Enable."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-02-01T01:00:00.000Z",
        "voteCount": 1,
        "content": "D. Allowing service principals to use Power BI APIs enables automated processes"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/microsoft/view/132631-exam-dp-500-topic-1-question-168-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>You need to recommend changes to the Power BI tenant to meet the technical requirements for external data sharing.<br><br>Which tenant setting should you recommend disabling?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow shareable links to grant access to everyone in your organization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllow Azure Active Directory guest users to edit and manage content in the organization\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsers can reassign personal workspaces",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShow Azure Active Directory guests in lists of suggested people"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-04-26T04:50:00.000Z",
        "voteCount": 2,
        "content": "keyword \"Disabling\""
      },
      {
        "date": "2024-02-18T12:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct - \"The external users must be prevented from publishing or modifying content in the tenant.\""
      },
      {
        "date": "2024-02-01T00:59:00.000Z",
        "voteCount": 2,
        "content": "B. From ChatGPT"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129074-exam-dp-500-topic-1-question-169-discussion/",
    "body": "This is a case study. Case studies are not timed separately. You can use as much exam time as you would like to complete each case. However, there may be additional case studies and sections on this exam. You must manage your time to ensure that you are able to complete all questions included on this exam in the time provided.<br><br>To answer the questions included in a case study, you will need to reference information that is provided in the case study. Case studies might contain exhibits and other resources that provide more information about the scenario that is described in the case study. Each question is independent of the other questions in this case study.<br><br>At the end of this case study, a review screen will appear. This screen allows you to review your answers and to make changes before you move to the next section of the exam. After you begin a new section, you cannot return to this section.<br><br><br>To start the case study -<br>To display the first question in this case study, click the Next button. Use the buttons in the left pane to explore the content of the case study before you answer the questions. Clicking these buttons displays information such as business requirements, existing environment, and problem statements. If the case study has an All Information tab, note that the information displayed is identical to the information displayed on the subsequent tabs. When you are ready to answer a question, click the Question button to return to the question.<br><br><br>Overview -<br><br><br>General Overview -<br><br>Fabrikam, Inc. is a software company that develops educational software for higher education.<br><br><br>Organizational Structure -<br><br>Fabrikam has the following business units:<br><br>\u2022\tFinance and Accounting<br>\u2022\tSales and Marketing<br>\u2022\tOperations<br>\u2022\tProduct<br><br>The Product business unit contains the following groups:<br><br>\u2022\tUser experience designers<br>\u2022\tSoftware engineers<br>\u2022\tProduct managers<br>\u2022\tTesters<br><br>The Operations business unit contains an information technology (IT) group. The IT group contains an enterprise analytics team and an information security team.<br><br><br>Existing Environment -<br><br><br>Data Storage -<br><br>Fabrikam has three Azure Synapse Analytics workspaces named workspace1prod, workspace1test, and workspace1dev. Each workspace is connected to an Azure Data Lake Storage account and contains a lake database that is accessed by using the built-in serverless SQL pool. The data in the Data Lake Storage accounts is available in the lake databases for analysts from every business unit to query and analyze by using Power BI.<br><br>The company imports the following files into the Data Lake Storage accounts:<br><br>\u2022\tUser experience data stored as JSON files<br>\u2022\tFinance data stored as CSV files<br>\u2022\tSales data stored as CSV files<br><br><br>Reporting -<br><br>Fabrikam has a Power BI tenant that contains 30 workspaces in Pro license mode. The data in the workspaces is a mix of Import and DirectQueiy datasets. All reports are interactive.<br><br>Fabrikam has three frequently used workspaces as shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-500/image175.png\"><br><br>The Corporate Data Models workspace contains a dataset named Financial Model that is used by reports in the P&amp;L workspace. Financial Model is maintained by the enterprise analytics team. The Corporate Data Models workspace and the User Experience workspace have corresponding development and test workspaces.<br><br><br>User Problems -<br><br>Administrators report an increase in the maintenance of Power BI tenant assets due to analysts in the Finance and Accounting business unit who create new Power BI datasets when the existing datasets already meet their needs.<br><br>Analysts in the Product business unit report the following issues:<br><br>\u2022\tDatasets are published to the User Experience workspace, while the data sources reference workspace1test.<br>\u2022\tThe parsing of user experience data in Power Query is very slow.<br><br>The enterprise analytics team identifies two DAX measures in the Financial Model dataset that are consistently slow to execute. The team must identify all the reports that use the Financial Model dataset and notify the report owners of changes to the measures.<br><br>Members of the enterprise analytics team report that creating Power BI reports and adjusting tables and views in Azure Synapse is tedious because they must switch between the Power BI workspaces and the Azure Synapse workspaces.<br><br>The information security team identifies that the user experience data is being shared externally.<br><br><br>Requirements -<br><br><br>Planned Changes -<br><br>Fabrikam plans to implement the following changes:<br><br>\u2022\tPower BI will be registered as a data source in Microsoft Purview.<br>\u2022\tThe analysts in the Product business unit will create a more automated process for deploying Power BI reports and datasets to the User Experience workspace.<br><br>The enterprise analytics team plans to perform the following tasks:<br><br>\u2022\tUpdate the DAX calculations in the Financial Model dataset.<br>\u2022\tCreate views in the Azure Synapse workspaces to speed up the parsing of user experience data.<br>\u2022\tCreate and document the change management process for shared Power BI datasets.<br><br><br>Technical Requirements -<br><br>From Microsoft Purview, analysts in all the business units must be able to see all the assets in the Power BI tenant and the Azure Synapse workspaces. Power BI asset information must include lineage to identify the data sources used by each report.<br><br>The information security team must identify all the Power BI reports and datasets that contain Personally Identifiable Information (PII).<br><br>Fabrikam requires a security solution for the Power BI tenant. The solution must meet the following requirements:<br><br>\u2022\tAccess to the tenant by external users must be approved by a manager and granted by the IT group.<br>\u2022\tThe external users must be prevented from publishing or modifying content in the tenant.<br>\u2022\tUsers must be prevented from sharing Power BI reports publicly to the internet.<br><br>The new process for deploying Power BI reports and datasets to the User Experience workspace must ensure that the datasets point to the lake database to which the relevant dataset is deployed. The views in each lake database must present the data in a tabular format.<br><br><br>The IT group registers the Power BI tenant as a data source in Microsoft Purview.<br><br>You need to ensure that all the analysts can view the assets in the Power BI tenant. The solution must meet the technical requirements for Microsoft Purview and Power BI.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a scan.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Power BI gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSearch the data catalog.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a linked service."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-27T06:00:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-12-20T04:21:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C\nhttps://community.fabric.microsoft.com/t5/Service/Create-a-data-catalog-list-of-dataflows-datasets-reports/m-p/2620224"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/microsoft/view/127833-exam-dp-500-topic-1-question-170-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI tenant that contains two workspaces named WorkspaceA and WorkspaceB. WorkspaceA contains centralized datasets.<br><br>You need to ensure that users can create reports by using the datasets in WorkspaceA and store the reports in WorkspaceB.<br><br>Which setting should you enable and for which resource? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image182.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image183.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-04T19:13:00.000Z",
        "voteCount": 6,
        "content": "https://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-admin-across-workspaces\nUse datasets across workspaces\nThe Tenant"
      },
      {
        "date": "2024-04-26T04:54:00.000Z",
        "voteCount": 1,
        "content": "Use datasets across workspaces\nTenant\nhttps://learn.microsoft.com/en-us/power-bi/connect-data/service-datasets-admin-across-workspaces"
      },
      {
        "date": "2024-01-09T20:33:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nTo ensure that users can create reports by using the datasets in WorkspaceA and store the reports in WorkspaceB, the setting that should be enabled is:\n\n- Use datasets across workspaces\n\nAnd the resource this setting should be applied to is:\n\n- The datasets\n\nThis setting allows for dataset sharing across workspaces within Power BI, which is necessary for the scenario described."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129250-exam-dp-500-topic-1-question-171-discussion/",
    "body": "You have an Azure SQL database named DB1.<br><br>You have a Power BI tenant that contains 20 workspaces. Multiple workspaces contain datasets that use DB1 as a data source.<br><br>You need to identity how many datasets use DB1 as a data source across the tenant.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan XLMA endpoint",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Power BI REST API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPower BI data source files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlineage view\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-22T23:32:00.000Z",
        "voteCount": 2,
        "content": "Data lineage shows list of datasets using a specific datasource via \"Show impact across workspaces\" button"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128624-exam-dp-500-topic-1-question-172-discussion/",
    "body": "You have a Power BI tenant.<br><br>You plan to publish 10 paginated reports that will be used by a team of business analysts.<br><br>You need to create a new workspace for the reports. The solution must minimize costs.<br><br>Which license mode should you select for the new workspace?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPro\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPremium per user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEmbedded",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPremium per capacity"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T03:04:00.000Z",
        "voteCount": 1,
        "content": "Pro licence is correct\nhttps://learn.microsoft.com/en-us/power-bi/paginated-reports/paginated-reports-faq#do-i-need-a-pro-license-to-create-and-publish-paginated-reports-"
      },
      {
        "date": "2024-02-27T06:06:00.000Z",
        "voteCount": 1,
        "content": "But the Pro license allows report creators to share content with other Pro users within the service, but it does not include the ability to publish or consume paginated reports, which is a feature reserved for Power BI Premium..."
      },
      {
        "date": "2023-12-22T23:36:00.000Z",
        "voteCount": 1,
        "content": "PRO licence is sufficient to publish paginated reports"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128545-exam-dp-500-topic-1-question-173-discussion/",
    "body": "You have a development workspace in the Power BI service.<br><br>You plan to use Tabular Editor to develop and deploy a new dataset, and then use Power BI Desktop to create and publish a report to the Power BI service.<br><br>Which three components should you include in the solution? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Read Write XMLA endpoint in a capacity\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta Premium Per User workspace",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe Read Write XMLA endpoint in the tenant",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta .pbix file that has an imported dataset",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta .pbix file that has a live connection\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "AE",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T20:39:00.000Z",
        "voteCount": 1,
        "content": "Chaygpt :"
      },
      {
        "date": "2024-01-09T20:39:00.000Z",
        "voteCount": 2,
        "content": "For the described workflow, option D (a .pbix file that has an imported dataset) is not the correct choice because:\n\n- When using Tabular Editor for development, the dataset is managed externally from Power BI Desktop. Once the dataset is deployed to the Power BI service via Tabular Editor, Power BI Desktop connects to this dataset using a live connection rather than importing the dataset directly into the .pbix file.\n- A .pbix file with an imported dataset suggests that the dataset is self-contained within the file, which contradicts the development workflow that involves Tabular Editor and a dataset managed in the Power BI service.\n\nTherefore, A, B, and E are the correct components to include in the solution for this particular development and deployment workflow."
      },
      {
        "date": "2023-12-20T08:11:00.000Z",
        "voteCount": 1,
        "content": "A: Enable Read write for the XMLA in the capacity settings not the tenant settings\nB: you need to have a ppu or Prmuim capacity to enable xmla read write\nE: with tabular editor, we need to live connection"
      },
      {
        "date": "2023-12-14T01:54:00.000Z",
        "voteCount": 1,
        "content": "A: The setting is available only on capacity and not tenant. https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools\nE: Yes to fulfil requirement: \"Power BI Desktop to create and publish a report to the Power BI service\"\n\nB: Not correct as requirement says that we already have a development workspace\nC: Incorrect as setting is available only on capacity and not tenant. https://learn.microsoft.com/en-us/power-bi/enterprise/service-premium-connect-tools\nD: if we use Tabular Editor we don't need .pbix file with imported dataset https://docs.tabulareditor.com/te3/tutorials/new-pbi-model.html"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129102-exam-dp-500-topic-1-question-174-discussion/",
    "body": "You are creating a Power BI deployment pipeline that will be used to deploy a paginated report.<br><br>You need to ensure that the paginated report is updated to point to the correct dataset for each stage.<br><br>What should you configure in the deployment pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta data source rule\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta selective deployment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta parameter rule"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T20:44:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nWhen working with Power BI deployment pipelines, especially when needing to ensure that a paginated report points to the correct dataset for each stage, you should configure:\n\nA. a data source rule\n\nData source rules in deployment pipelines allow you to modify the connection information so that the report points to the appropriate dataset for the given stage in the pipeline (development, test, production). This ensures that when the report is deployed to each stage, it is connected to the correct data source for that environment."
      },
      {
        "date": "2023-12-20T08:23:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer.\nfor paginated reports, the only option available is datasource rule\ncheck the link https://learn.microsoft.com/en-us/fabric/cicd/deployment-pipelines/create-rules#supported-data-sources-for-dataflow-and-dataset-rules"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128555-exam-dp-500-topic-1-question-175-discussion/",
    "body": "HOTSPOT<br> -<br><br>You deploy an 80-GB compressed data model to a Power BI Premium workspace.<br><br>You discover that the model has the following issues:<br><br>\u2022\tThe model recently increased excessively in size.<br>\u2022\tDate columns are formatted inconsistently.<br>\u2022\tMeasures calculate slowly.<br><br>Which tool should you use to troubleshoot each issue? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image184.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image185.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-04-23T20:16:00.000Z",
        "voteCount": 1,
        "content": "Power query diagnostic\nTabular Editor\nDax studio"
      },
      {
        "date": "2024-01-09T20:46:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nTo troubleshoot the issues in the Power BI data model, you would use the following tools:\n\n- The model has recently increased excessively in size: **DAX Studio**\n  - DAX Studio can be used to analyze the model and its storage by using the VertiPaq Analyzer feature, which can identify large tables, columns, and relationships that may be contributing to the size increase.\n\n- Date columns are not consistently formatted: **Tabular Editor**\n  - Tabular Editor can quickly inspect and update the formatting of date columns across the entire model.\n\n- Measures are calculating slowly: **DAX Studio**\n  - DAX Studio can analyze the performance of DAX measures through query execution breakdowns and performance tuning features."
      },
      {
        "date": "2023-12-23T04:03:00.000Z",
        "voteCount": 1,
        "content": "DAX Studio\nTabular Editor (using Best Practice Analyser)\nDAX Studio"
      },
      {
        "date": "2023-12-27T08:36:00.000Z",
        "voteCount": 1,
        "content": "check below link:\nhttps://datamartin.ca/2022/09/29/tools-to-analyze-the-performance-of-your-power-bi-solution/"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129078-exam-dp-500-topic-1-question-176-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a hybrid tenant in Microsoft Azure Active Directory (AD), part of Microsoft Entra, that syncs by using Azure AD Connect.<br><br>You are building a dataset by using Power BI Desktop. The dataset will be deployed to the Power BI service.<br><br>You need to implement object-level security (OLS) based on a table named User Corporate Role that contains the following two columns:<br><br>\u2022\tUser: Contains user identities synced from on-premises Active Directory in the form of Domain\\user<br>\u2022\tCorporate Role: A text column<br><br>What should you use to configure OLS, and which Power BI function should you use to retrieve the user identities? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image186.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image187.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-23T02:43:00.000Z",
        "voteCount": 3,
        "content": "should be username()"
      },
      {
        "date": "2023-12-20T04:39:00.000Z",
        "voteCount": 1,
        "content": "It is true"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126672-exam-dp-500-topic-1-question-177-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a Power BI model that contains three tables named Sales, Marketing, and Delivery. The Sales table relates to the Marketing and Delivery tables. The Marketing and Delivery tables do NOT relate to each other.<br><br>You need to create a measure to meet the following requirements:<br><br>\u2022\tSum the values in a column named Sales[SalesAmount].<br>\u2022\tEnsure that the tables and relationships remain unchanged.<br>\u2022\tIf a user applies a filter to a column named Marketing[Region], apply the same filter to a column named Delivery[Region].<br><br>How should you complete the DAX expression? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image188.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image189.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-11-20T05:21:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is: TREATAS and VALUES"
      },
      {
        "date": "2024-01-09T20:51:00.000Z",
        "voteCount": 1,
        "content": "treatas and Values.\nChatgpt :\nTo create a DAX measure according to the requirements, you should use the following expression:\n\nCALCULATE(\n    SUM(Sales[SalesAmount]),\n    TREATAS(VALUES(Marketing[Region]), Delivery[Region])\n)\n\nHere's what each function does in this context:\n\n- `CALCULATE` changes the context in which the data is aggregated and is used here to sum the `SalesAmount` while applying a filter context.\n- `SUM(Sales[SalesAmount])` adds up all the values in the `SalesAmount` column from the Sales table.\n- `TREATAS` takes the values from the `Marketing[Region]` column and treats them as if they were values in the `Delivery[Region]` column, effectively transferring the filter context from the Marketing table to the Delivery table. This is necessary since there is no direct relationship between Sales and Delivery, but both are related to Marketing."
      },
      {
        "date": "2023-12-15T10:30:00.000Z",
        "voteCount": 2,
        "content": "Treatas and Values \nhttps://learn.microsoft.com/en-us/dax/treatas-function"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129103-exam-dp-500-topic-1-question-178-discussion/",
    "body": "HOTSPOT<br> -<br><br>You use Power Query Editor to transform data.<br><br>You need to create a function named ConvertF2C that accepts a temperature value in Fahrenheit and returns a temperature value in Celsius.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image190.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image191.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-09T20:55:00.000Z",
        "voteCount": 1,
        "content": "Tempc + convertf2c, chatgpt :\nFor the Power Query Editor function that converts Fahrenheit to Celsius, you should define the function parameter as the temperature in Fahrenheit. The function itself should then perform the conversion. Here's how you should complete the code:\n\n- For the parameter: `(TempF as number) =&gt;`\n- For the function name in the `in` clause: `ConvertF2C`\n\nThe complete function in Power Query M language should look like this:\n\n```\n(TempF as number) =&gt; \nlet\n    TempC = (TempF - 32) * 5/9\nin\n    ConvertF2C\n```"
      },
      {
        "date": "2024-01-09T20:56:00.000Z",
        "voteCount": 1,
        "content": "I mean tempf + convertf2c sorry"
      },
      {
        "date": "2024-01-15T02:52:00.000Z",
        "voteCount": 2,
        "content": "ChatGPT has lead you a little astray :). The Let statement is defining a list of variables, in this case only TempC. The In statement can then reference these variables. ConvertF2C isn't defined in the Let so wouldn't be recognised by the In.\n\nThe correct answer should be TempF + TempC."
      },
      {
        "date": "2023-12-20T08:34:00.000Z",
        "voteCount": 1,
        "content": "Source \nTempC\nhttps://datamadness.medium.com/how-to-write-functions-in-power-query-4a29302760ce"
      },
      {
        "date": "2023-12-22T23:56:00.000Z",
        "voteCount": 2,
        "content": "I correct myself: \nFirst is (TempF as number) since TempF should be passed as a parameter\nSecond, not sure but it should be TempC to retrieve the result of expression from the \"let\" statement to the \"in\" statement"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/microsoft/view/128221-exam-dp-500-topic-1-question-179-discussion/",
    "body": "You have a Power BI dataflow named DF1 that contains the following columns:<br><br>\u2022\tOrderID<br>\u2022\tSaleDate<br>\u2022\tProductID<br>\u2022\tSalesAmount<br>\u2022\tProductCategory<br><br>You create a Power BI dataset named DS1 that uses DF1 as a data source. DS1 creates an aggregated view of the data in DF1. The view contains the following columns:<br><br>\u2022\tSaleMonth<br>\u2022\tSalesAmount<br>\u2022\tProductCategory<br><br>You need to minimize how long it takes to refresh DS1.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a computed entity to DF1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a custom function in DS1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a linked entity in DF1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a computed column to DS1."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-15T12:20:00.000Z",
        "voteCount": 3,
        "content": "\"To perform in-storage computations, you first must create the dataflow and bring data into that Power BI dataflow storage. After you have a dataflow that contains data, you can create computed tables, which are tables that do in-storage computations.\"\nhttps://learn.microsoft.com/en-us/power-query/dataflows/computed-tables"
      },
      {
        "date": "2023-12-10T08:25:00.000Z",
        "voteCount": 3,
        "content": "Computed Table/Entity would help to reduce the refresh time of dataset\nhttps://learn.microsoft.com/en-us/power-query/dataflows/computed-tables"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/microsoft/view/129104-exam-dp-500-topic-1-question-180-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have a deep neural network (DNN) machine learning model named Model1 and an Azure Synapse Analytics workspace named Workspace1.<br><br>You need to score the data in Workspace1 by using Model1 and Transact-SQL code.<br><br>Which Transact-SQL function should you use to score the data, and which format should you use for Model1? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image192.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image193.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-09T21:05:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nTo score the data in the Azure Synapse Analytics workspace using Model1 and Transact-SQL code, you should use the following:\n\n- Function: `PREDICT`\n- Format: `ONNX`\n\nThe `PREDICT` function is used in T-SQL for scoring machine learning models, and ONNX (Open Neural Network Exchange) is a popular open format for deep learning models like DNNs."
      },
      {
        "date": "2023-12-20T08:53:00.000Z",
        "voteCount": 2,
        "content": "Predict \nONNX\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/queries/predict-transact-sql?view=sql-server-ver16"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130113-exam-dp-500-topic-1-question-181-discussion/",
    "body": "You have a Power BI dataset that has only the necessary fields visible for report development.<br><br>You need to ensure that only 25 fields are available when end users personalize visuals.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tabular Editor, create a new perspective.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Tabular Editor, create a new role.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHide all the fields in the dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure object-level security (OLS)."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-10T03:44:00.000Z",
        "voteCount": 2,
        "content": "D is correct. Perspectives are not a security tool, it is only used in personalized visuals for better user experience\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/power-bi-personalize-visuals?tabs=powerbi-desktop"
      },
      {
        "date": "2024-01-09T22:15:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nObject-Level Security (OLS) in Power BI is used to restrict access to certain rows in a database based on the user's role, which is not what is needed in the scenario described. OLS would not limit the number of fields available; it would limit the data within those fields. Perspectives, on the other hand, are designed to limit which fields or tables are visible or available to the user, which directly addresses the requirement of making only 25 fields available for report personalization."
      },
      {
        "date": "2024-01-01T17:15:00.000Z",
        "voteCount": 2,
        "content": "To ensure that only 25 fields are available when end users personalize visuals in Power BI, you should configure object-level security (OLS)1.\n\nObject-level security (OLS) in Power BI allows you to restrict the visibility of tables and columns in a dataset1. When OLS is configured, users without the required permission will receive a message that the field can\u2019t be found for all report visuals using that field1.\n\nSo, the correct answer is D. Configure object-level security (OLS)"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130283-exam-dp-500-topic-1-question-182-discussion/",
    "body": "You have the following code in a cell in an Azure Synapse notebook.<br><br><img src=\"https://img.examtopics.com/dp-500/image194.png\"><br><br>You need to replace the comment on the first line of the code with the correct magic command.<br><br>Which magic command should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%sparkr",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%pyspark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%spark",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t%%sql"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-04T02:25:00.000Z",
        "voteCount": 1,
        "content": "%%pyspark is correct. Exact example is in the MS documentation.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks#use-multiple-languages"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130285-exam-dp-500-topic-1-question-183-discussion/",
    "body": "You have a Power BI report that contains a single page. The page contains two line charts and one bar chart.<br><br>You need to ensure that users can perform the following tasks for all three visuals:<br><br>\u2022\tSwitch the measures used in the visuals.<br>\u2022\tChange the visual type.<br>\u2022\tAdd a legend.<br><br>The solution must minimize development effort.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable personalization for each visual.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable personalization for the report.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a bookmark for each acceptable combination of visual type, measure, and legend in the bar chart.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdit the interactions between the three visuals."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T02:31:00.000Z",
        "voteCount": 1,
        "content": "Correct.\nhttps://learn.microsoft.com/en-us/power-bi/create-reports/power-bi-personalize-visuals?tabs=powerbi-desktop#what-report-users-can-change"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126696-exam-dp-500-topic-1-question-184-discussion/",
    "body": "You have an Apache Spark notebook that contains PySpark code and is used to explore data in an Azure Synapse Analytics workspace.<br><br>You need to create data visualizations in the notebook by using built-in libraries.<br><br>Which two libraries can you use? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGleam",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlotly",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSeaborn\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMatplotlib\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-20T08:59:00.000Z",
        "voteCount": 1,
        "content": "same as Question 58"
      },
      {
        "date": "2023-11-20T14:52:00.000Z",
        "voteCount": 4,
        "content": "Answer is Seaborn and Matplotlib\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-data-visualization-tutorial#visualize-data"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130108-exam-dp-500-topic-1-question-185-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics workspace that contains an Apache Spark pool.<br><br>You create a notebook and configure a cell that runs the following SparkSQL query.<br><br>SELECT ProductID, ProductName, Category<br><br>From products<br> -<br><br>You need to create a column chart by using the built-in charting capability. The solution must visualize the distribution of product IDs across product categories.<br><br>How should you complete the column chart? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-500/image195.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-500/image196.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-09T22:22:00.000Z",
        "voteCount": 1,
        "content": "Chaggpt :\nTo create a column chart that visualizes the distribution of product IDs across product categories after running your SparkSQL query, you'll need to set the following options in the Chart view:\n\n- **Key**: `Category` - This will be your x-axis, which groups the data by Category.\n- **Values**: `ProductID` - This will determine the values that are being aggregated and visualized, such as the count of ProductIDs.\n- **Aggregation**: `COUNT` - Since you want to visualize the distribution of ProductIDs, you'll use the COUNT aggregation to show how many ProductIDs fall into each category.\n\nThese settings will create a column chart where each column represents a category, and the height of the column shows the number of ProductIDs within that category."
      },
      {
        "date": "2024-01-01T14:41:00.000Z",
        "voteCount": 2,
        "content": "Answer is: Category, ProductID and Count.\ndf_grouped = df.groupBy(\"Category\").agg(count(\"ProductID\")"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137322-exam-dp-500-topic-1-question-186-discussion/",
    "body": "You plan to deploy a Power BI Premium tenant.<br><br>You need to specify the capacity of the tenant.<br><br>Which unit should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRU/s",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tv-cores\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tQPUs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDTUs"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-26T19:17:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is v-cores.\nhttps://learn.microsoft.com/en-us/power-bi/enterprise/service-admin-premium-manage"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137533-exam-dp-500-topic-1-question-189-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have a Power Query query in a Power BI report. The query maps source database columns to the input columns of a time series machine learning model.<br><br>You discover that the model fails to process a source time field.<br><br>You need to ensure that the model processes the field.<br><br>Solution: You convert the time column into the Text data type.<br><br>Does this meet the requirement?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-30T03:38:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/microsoft/view/137381-exam-dp-500-topic-1-question-193-discussion/",
    "body": "You have an Azure Synapse Analytics workspace that is connected to a data lake.<br><br>You train an Open Neural Network Exchange (ONNX) model named Model1.<br><br>You need to implement Model1.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScore Model1 by using OPENJSON().",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad Model1 into a binary variable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad Model1 into a character variable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScore Model1 by using PREDICT()."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-27T20:25:00.000Z",
        "voteCount": 2,
        "content": "I believe the ONNX model should be loaded before it is scored (using PREDICT). They are binary so should be stored in a binary variable.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-predict"
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/microsoft/view/136963-exam-dp-500-topic-1-question-194-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics serverless SQL pool named Pool1.<br><br>You plan to deploy a data lake that will record the history of transactions executed against Pool1.<br><br>You need to recommend which type of file to use to store the history. The solution must ensure that the history is written in the scope of the related transaction.<br><br>Which file type should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelta",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-22T19:59:00.000Z",
        "voteCount": 2,
        "content": "Parquet is correct answer. Refer to Question Number # 69."
      }
    ],
    "examNameCode": "dp-500",
    "topicNumber": "1"
  }
]