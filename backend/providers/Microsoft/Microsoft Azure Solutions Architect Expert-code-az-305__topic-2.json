[
  {
    "topic": 2,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67957-exam-az-305-topic-2-question-1-discussion/",
    "body": "You have 100 servers that run Windows Server 2012 R2 and host Microsoft SQL Server 2014 instances. The instances host databases that have the following characteristics:<br>\u2711 Stored procedures are implemented by using CLR.<br>\u2711 The largest database is currently 3 TB. None of the databases will ever exceed 4 TB.<br>You plan to move all the data from SQL Server to Azure.<br>You need to recommend a service to host the databases. The solution must meet the following requirements:<br>\u2711 Whenever possible, minimize management overhead for the migrated databases.<br>\u2711 Ensure that users can authenticate by using Azure Active Directory (Azure AD) credentials.<br>\u2711 Minimize the number of database changes required to facilitate the migration.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database elastic pools",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Managed Instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database single databases",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSQL Server 2016 on Azure virtual machines"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 89,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-11T05:30:00.000Z",
        "voteCount": 44,
        "content": "CLR is supported on SQL Managed instance and not on Azure SQL Database."
      },
      {
        "date": "2022-08-03T09:58:00.000Z",
        "voteCount": 3,
        "content": "But it is also supported for elastic pools and these are probably easier to manage? SQL Manage Instance is a service to reach nearly 100% compatibility with your on-prem machines but that was not required here."
      },
      {
        "date": "2023-01-12T10:52:00.000Z",
        "voteCount": 6,
        "content": "As far as I know, CLR is not supported in AZ SQL database elastic pools. Do you have a doc reference to prove your statement?"
      },
      {
        "date": "2024-01-03T03:17:00.000Z",
        "voteCount": 1,
        "content": "See my comment for reference"
      },
      {
        "date": "2021-12-29T09:51:00.000Z",
        "voteCount": 11,
        "content": "B is correct"
      },
      {
        "date": "2024-08-09T04:24:00.000Z",
        "voteCount": 2,
        "content": "This question was in the exam August 2024 and I answered with this same answer. I scored 870"
      },
      {
        "date": "2024-06-21T10:46:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\nAzure SQL Managed Instance:\n\nMinimized Management Overhead: It is a fully managed PaaS service that reduces management tasks compared to running SQL Server on VMs.\nAzure AD Authentication: Supports Azure AD integration, allowing users to authenticate using Azure AD credentials.\nCompatibility: It offers high compatibility with SQL Server, minimizing the need for changes during migration, especially for databases using CLR stored procedures."
      },
      {
        "date": "2024-06-21T10:47:00.000Z",
        "voteCount": 3,
        "content": "Why Not Other Options:\nA. Azure SQL Database elastic pools:\nDesigned for managing multiple databases with variable and unpredictable usage patterns, but might require significant changes for databases using CLR.\nC. Azure SQL Database single databases:\nLike elastic pools, it might require significant changes for databases using CLR and doesn't provide the same level of compatibility as Managed Instance.\nD. SQL Server 2016 on Azure virtual machines:\nIncreases management overhead since it requires maintaining VMs and SQL Server instances, which is against the goal of minimizing management overhead."
      },
      {
        "date": "2024-04-28T21:37:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct \nKeyword is CLR as SQL MI one that supports it"
      },
      {
        "date": "2024-01-03T03:16:00.000Z",
        "voteCount": 3,
        "content": "Some high-level guidelines might be:\n-Use Elastic pools if you need to group a large number of single database that don't need all instance Transact-SQL functionalities that exist in SQL Server.\n-Use Managed Instance if you want to migrate a large number of SQL Server database that heavily use instance level features such as CLR, Service Broker, SQL Agent, etc."
      },
      {
        "date": "2024-01-03T03:17:00.000Z",
        "voteCount": 1,
        "content": "See: https://learn.microsoft.com/en-us/answers/questions/842070/elastic-pool-in-azure-sql-sql-server-managed-insta"
      },
      {
        "date": "2023-09-21T15:14:00.000Z",
        "voteCount": 7,
        "content": "Correct Answer - B: Azure SQL Managed Instance\nGiven the requirements and the need for a seamless migration with reduced management overhead, Azure SQL Managed Instance is the most appropriate choice. \n\nReduced Management Overhead \n- As a fully managed instance in Azure, it offloads many of the administrative tasks, such as backups, patching, and scaling.\n\nAzure AD Authentication \n- Natively supports Azure Active Directory (Azure AD) credentials, providing integrated and secure authentication.\n\nMinimal Database Changes\n- Offers broad compatibility with SQL Server features, ensuring a smooth transition from on-premises SQL Server 2014 environments.\n\nCLR Support\n- Supports Common Language Runtime (CLR) procedures, which is essential given that your stored procedures use CLR.\n\nDatabase Size\n- Capable of handling databases of significant size, ensuring it can accommodate databases that approach or exceed 3 TB."
      },
      {
        "date": "2023-09-17T08:38:00.000Z",
        "voteCount": 9,
        "content": "This question is on today's exam.\nI passed the exam today 17-09-2023 with a score of 906/1000.\nThe exam is easier than AZ-104."
      },
      {
        "date": "2023-03-21T08:50:00.000Z",
        "voteCount": 7,
        "content": "B. Azure SQL Managed Instance\n\nAzure SQL Managed Instance is a fully managed SQL Server instance hosted in Azure that supports most of the SQL Server features. It provides easier migration from on-premises SQL Server with minimal database changes, while also minimizing management overhead.\n\nHere's how Azure SQL Managed Instance meets your requirements:\n\nMinimizes management overhead: As a fully managed service, Azure SQL Managed Instance handles many administrative tasks like automatic backups, patching, and monitoring.\n\nAzure AD authentication: Azure SQL Managed Instance supports Azure Active Directory (Azure AD) authentication, which allows users to authenticate using their Azure AD credentials.\n\nMinimizes database changes: Since Azure SQL Managed Instance is highly compatible with SQL Server, migrating to it requires minimal changes to the databases. It supports features like CLR, which are not available in Azure SQL Database."
      },
      {
        "date": "2023-02-18T00:04:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/features-comparison?view=azuresql#features-of-sql-database-and-sql-managed-instance\nAzure SQL Managed Instance\nCommon language runtime - CLR\n- Yes, but without access to file system in CREATE ASSEMBLY statement"
      },
      {
        "date": "2023-02-08T03:53:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-01-27T11:33:00.000Z",
        "voteCount": 1,
        "content": "B. Azure SQL Managed Instance"
      },
      {
        "date": "2023-01-24T11:00:00.000Z",
        "voteCount": 1,
        "content": "B. Azure SQL Managed Instance"
      },
      {
        "date": "2023-01-18T17:07:00.000Z",
        "voteCount": 1,
        "content": "B was my first guess."
      },
      {
        "date": "2022-12-28T19:44:00.000Z",
        "voteCount": 3,
        "content": "Exam Question 12/28/2022"
      },
      {
        "date": "2022-12-07T00:39:00.000Z",
        "voteCount": 1,
        "content": "why not D,\nhttps://learn.microsoft.com/en-us/azure/azure-sql/azure-sql-iaas-vs-paas-what-is-overview?view=azuresql#comparison-table\nsupport all feature, up to 256 TB, full control, easiest migration\n,requirement only mentioned an easy way to migrate, not maintenance, not high availability,"
      },
      {
        "date": "2023-01-24T06:35:00.000Z",
        "voteCount": 2,
        "content": "\"Whenever possible, minimize management overhead for the migrated databases\""
      },
      {
        "date": "2022-12-07T00:33:00.000Z",
        "voteCount": 1,
        "content": "why not D  \nhttps://learn.microsoft.com/en-us/azure/active-directory/manage-apps/plan-sso-deployment#single-sign-on-options\nsupport all feature ,  up to 256 TB,  full control , easiest migration"
      },
      {
        "date": "2023-01-24T06:36:00.000Z",
        "voteCount": 2,
        "content": "\"Whenever possible, minimize management overhead for the migrated databases\"\n\nDB on VMs isn't exactly the easiest to manage, definitely not more than Managed Instance, which by definiton is well... managed."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67513-exam-az-305-topic-2-question-2-discussion/",
    "body": "You have an Azure subscription that contains an Azure Blob Storage account named store1.<br>You have an on-premises file server named Server1 that runs Windows Server 2016. Server1 stores 500 GB of company files.<br>You need to store a copy of the company files from Server1 in store1.<br>Which two possible Azure services achieve this goal? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Logic Apps integration account",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Import/Export job\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Analysis services On-premises data gateway",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Batch account"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 76,
        "isMostVoted": true
      },
      {
        "answer": "BE",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-09T23:27:00.000Z",
        "voteCount": 36,
        "content": "B &amp; C are correct"
      },
      {
        "date": "2021-12-15T21:38:00.000Z",
        "voteCount": 11,
        "content": "https://docs.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction#move-data-to-blob-storage"
      },
      {
        "date": "2023-05-26T03:33:00.000Z",
        "voteCount": 17,
        "content": "A. an Azure Logic Apps integration account\nno, this is an integration service with visual flows with If-Then style logic. It does not support a way to import data from on-premise to blobstorage\n\nB. an Azure Import/Export job\nAgree, with other people here. \n\nC. Azure Data Factory\nAgree, is a way of importing data, but looking at 500GB it is a bit of overkill\n\nD. an Azure Analysis services On-premises data gateway\nnot a data import option\n\nE. an Azure Batch account\nIs part of Azure Batch service and involve HPC job scheduling etc. but is not a way of importing or exporting data from on-premise to Azure\n\nNote:\nFor 500GB we would probably use AzCopy instead.\nIf it was a Typo and actually 500TB we would use Azure Data Box Heavy or maybe the Azure Import/Export Service if you provide your own drives."
      },
      {
        "date": "2024-06-27T05:24:00.000Z",
        "voteCount": 1,
        "content": "B and C"
      },
      {
        "date": "2024-06-21T10:49:00.000Z",
        "voteCount": 1,
        "content": "B. an Azure Import/Export job\n\nWhy: You can use the Azure Import/Export service to securely transfer large amounts of data to Azure Blob Storage by shipping hard drives to an Azure data center.\nC. Azure Data Factory\n\nWhy: Azure Data Factory can be used to create a data pipeline that moves files from on-premises to Azure Blob Storage, enabling automated and scheduled transfers."
      },
      {
        "date": "2024-06-21T10:49:00.000Z",
        "voteCount": 1,
        "content": "Why Not Other Options:\nA. Azure Logic Apps integration account: Designed for integrating workflows and not typically used for bulk data transfer.\nD. Azure Analysis Services On-premises data gateway: Used for accessing on-premises data sources from Azure Analysis Services, not for transferring files to Blob Storage.\nE. Azure Batch account: Intended for running large-scale parallel and batch compute jobs, not for transferring files to Blob Storage."
      },
      {
        "date": "2024-04-28T21:33:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2024-01-21T09:30:00.000Z",
        "voteCount": 4,
        "content": "appeared in Exam 01/2024"
      },
      {
        "date": "2023-12-08T08:56:00.000Z",
        "voteCount": 1,
        "content": "Well B &amp; C seem to be the answers. For B, though windows 2016 is NOT a supported version based on following link.\n\nhttps://learn.microsoft.com/en-us/azure/import-export/storage-import-export-requirements"
      },
      {
        "date": "2024-02-14T03:42:00.000Z",
        "voteCount": 1,
        "content": "I think those OS requirements where only meant to describe older versions of Windows that are still support (I know, this is bad documentation form MS part, but MS Learn is far from perfect, documentation wise.)\n\nThe support is about the waimportexport.exe tool used.\n\nhttps://learn.microsoft.com/en-us/previous-versions/azure/storage/common/storage-import-export-tool-preparing-hard-drives-import#requirements-for-waimportexportexe\n\nstates Windows 7, Windows Server 2008 R2, or a newer Windows operating system are supported!"
      },
      {
        "date": "2023-11-20T03:02:00.000Z",
        "voteCount": 4,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-09-21T15:18:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer - B &amp; C: Azure Import/Export &amp; Azure Data Factory\n \nAzure Import/Export: \n- This is used for transferring large amounts of data to and from Azure Blob, File, and Disk storage using physical hard drives. It would be suitable for transferring 500 GB of data. \n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-import-export-service\n\nAzure Data Factory:\n- Azure Data Factory is a cloud-based data integration service that can move and integrate data from various sources to various destinations. It would be suitable for copying files from Server1 to Blob Storage.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/introduction"
      },
      {
        "date": "2023-09-17T08:38:00.000Z",
        "voteCount": 5,
        "content": "This question is on today's exam.\nThe exam is easier than AZ-104."
      },
      {
        "date": "2023-09-04T23:43:00.000Z",
        "voteCount": 4,
        "content": "Got this on Sept. 5, 2023"
      },
      {
        "date": "2023-05-26T10:26:00.000Z",
        "voteCount": 2,
        "content": "ok, I will go with ADF, however I dont see question mentioning the connectivity between on-prem and Azure AD. I think ADF can only be used when on-prem is connected with Azure AD."
      },
      {
        "date": "2023-03-21T08:53:00.000Z",
        "voteCount": 5,
        "content": "B. an Azure Import/Export job\nC. Azure Data Factory\n\nB. Azure Import/Export job: This service allows you to securely import or export large amounts of data to or from Azure Blob Storage by shipping hard disk drives to an Azure data center. You can use the Azure Import/Export service to transfer the company files from your on-premises server to the Azure Blob Storage account.\n\nC. Azure Data Factory: It is a cloud-based data integration service that enables you to create, schedule, and manage data pipelines. You can create a pipeline in Azure Data Factory to copy data from your on-premises file server to Azure Blob Storage. You will need to use a Self-hosted Integration Runtime installed on your on-premises server to facilitate the data movement between your on-premises server and Azure Blob Storage."
      },
      {
        "date": "2023-02-26T11:17:00.000Z",
        "voteCount": 6,
        "content": "This was a question was on my exam today (2/26/23) - Scored 844\nI agree with this answer"
      },
      {
        "date": "2023-02-26T07:07:00.000Z",
        "voteCount": 2,
        "content": "files is not fit for data factory"
      },
      {
        "date": "2023-09-24T08:26:00.000Z",
        "voteCount": 2,
        "content": "Data Factory can move files.  It isn't just for DBs.  I accidentally upvoted this when I went to click reply."
      },
      {
        "date": "2023-02-19T00:12:00.000Z",
        "voteCount": 3,
        "content": "BC is the answer.\n\nhttps://learn.microsoft.com/en-gb/azure/storage/blobs/storage-blobs-introduction#move-data-to-blob-storage\nA number of solutions exist for migrating existing data to Blob Storage:\n- Azure Data Factory supports copying data to and from Blob Storage by using the account key, a shared access signature, a service principal, or managed identities for Azure resources. \n- The Azure Import/Export service provides a way to import or export large amounts of data to and from your storage account using hard drives that you provide."
      },
      {
        "date": "2023-02-09T01:12:00.000Z",
        "voteCount": 1,
        "content": "LOL, I see sarcasm in the voted answers.  Or may be it's just me seeing the question differently~"
      },
      {
        "date": "2023-02-09T01:47:00.000Z",
        "voteCount": 4,
        "content": "I mean B is possible, but a really stupid solution unless there's a typo, it's actually 500TB!\nMy answer are B &amp; E.\nMy real life choice is Azure File storage."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67833-exam-az-305-topic-2-question-3-discussion/",
    "body": "You have an Azure subscription that contains two applications named App1 and App2. App1 is a sales processing application. When a transaction in App1 requires shipping, a message is added to an Azure Storage account queue, and then App2 listens to the queue for relevant transactions.<br>In the future, additional applications will be added that will process some of the shipping requests based on the specific details of the transactions.<br>You need to recommend a replacement for the storage account queue to ensure that each additional application will be able to read the relevant transactions.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tone Azure Data Factory pipeline",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmultiple storage account queues",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tone Azure Service Bus queue",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tone Azure Service Bus topic\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 67,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-16T15:27:00.000Z",
        "voteCount": 25,
        "content": "Correct answer - D"
      },
      {
        "date": "2022-01-20T18:28:00.000Z",
        "voteCount": 20,
        "content": "No doubt, the Service Bus Topic is exactly what you would need if multiple applications want to send messages to consumers."
      },
      {
        "date": "2023-08-25T22:24:00.000Z",
        "voteCount": 2,
        "content": "Correct. Below url will provide better understanding on those services - https://learn.microsoft.com/en-us/training/modules/design-application-architecture/3-design-messaging-solution"
      },
      {
        "date": "2023-11-20T03:02:00.000Z",
        "voteCount": 4,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-09-11T15:58:00.000Z",
        "voteCount": 5,
        "content": "Correct Answer - D: Azure Service Bus Topic\n- The key detail in the requirement is that in the future, multiple applications will process the shipping requests. This implies a need for a publish-subscribe model where multiple subscribers (applications) can independently process messages (transactions).\n\n- Azure Service Bus Topic: Topics in Azure Service Bus support the publish-subscribe pattern. Multiple subscribers can independently retrieve filtered or unfiltered messages from the topic. This is the best fit for the described requirement.\n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions"
      },
      {
        "date": "2023-03-21T09:11:00.000Z",
        "voteCount": 5,
        "content": "D. one Azure Service Bus topic\n\nIn this scenario, you should recommend using an Azure Service Bus topic. Topics provide a publish-subscribe messaging pattern that allows multiple subscribers to independently retrieve messages based on their specific needs. As you add more applications to process shipping requests, each application can subscribe to the topic and filter the messages based on the transaction details."
      },
      {
        "date": "2023-02-22T09:13:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions#topics-and-subscriptions\nA queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern. It's useful for scaling to large numbers of recipients. Each published message is made available to each subscription registered with the topic. Publisher sends a message to a topic and one or more subscribers receive a copy of the message."
      },
      {
        "date": "2023-01-28T01:14:00.000Z",
        "voteCount": 1,
        "content": "D. one Azure Service Bus topic\nCorrect answer\n\nTopic is for one to many"
      },
      {
        "date": "2022-10-22T20:25:00.000Z",
        "voteCount": 2,
        "content": "service bus topic - 1:N"
      },
      {
        "date": "2022-09-02T04:25:00.000Z",
        "voteCount": 6,
        "content": "Answer is C. \nThe shipping must be handled by only ONE receiver at a time. If you use D (Topic) several subscribers can receive the message and processes the shipment resulting in several shipments.\n\nQueue does not mean only one receiver but only ONE AT A TIME to process the message.\nhttps://medium.com/awesome-azure/azure-difference-between-azure-service-bus-queues-and-topics-comparison-azure-servicebus-queue-vs-topic-4cc97770b65\n\nQueues\nQueues offer First In, First Out (FIFO) message delivery to one or more competing consumers. That is, receivers typically receive and process messages in the order in which they were added to the queue. And, only one message consumer receives and processes each message.\nhttps://docs.microsoft.com/en-us/azure/service-bus-messaging/service-bus-queues-topics-subscriptions"
      },
      {
        "date": "2022-09-02T04:37:00.000Z",
        "voteCount": 14,
        "content": "Sorry answer is D not because of having multiple consumers but because of the need of filtering based on the transaction details. Publisher sends a message to a topic and one or more subscribers receive a copy of the message, depending on filter rules set on these subscriptions."
      },
      {
        "date": "2022-07-14T02:44:00.000Z",
        "voteCount": 2,
        "content": "Azure service bus topic is support many application"
      },
      {
        "date": "2022-06-26T00:42:00.000Z",
        "voteCount": 1,
        "content": "A queue allows processing of a message by a single consumer. In contrast to queues, topics and subscriptions provide a one-to-many form of communication in a publish and subscribe pattern."
      },
      {
        "date": "2022-06-22T03:17:00.000Z",
        "voteCount": 6,
        "content": "did my Exam today. This was on there."
      },
      {
        "date": "2022-05-23T14:51:00.000Z",
        "voteCount": 1,
        "content": "Correct answer: D"
      },
      {
        "date": "2022-04-26T11:27:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - D\nService Bus Topic"
      },
      {
        "date": "2022-04-04T08:07:00.000Z",
        "voteCount": 4,
        "content": "Came in exam today 04/04/2022"
      },
      {
        "date": "2022-03-27T09:50:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D"
      },
      {
        "date": "2022-03-10T15:46:00.000Z",
        "voteCount": 4,
        "content": "Appeared in my exam, March 10th, 2022. I chose D."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67443-exam-az-305-topic-2-question-4-discussion/",
    "body": "HOTSPOT -<br>You need to design a storage solution for an app that will store large amounts of frequently used data. The solution must meet the following requirements:<br>\u2711 Maximize data throughput.<br>\u2711 Prevent the modification of data for one year.<br>\u2711 Minimize latency for read and write operations.<br>Which Azure Storage account type and storage service should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0009200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0009300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: BlockBlobStorage -<br>Block Blob is a premium storage account type for block blobs and append blobs. Recommended for scenarios with high transactions rates, or scenarios that use smaller objects or require consistently low storage latency.<br><br>Box 2: Blob -<br>The Archive tier is an offline tier for storing blob data that is rarely accessed. The Archive tier offers the lowest storage costs, but higher data retrieval costs and latency compared to the online tiers (Hot and Cool). Data must remain in the Archive tier for at least 180 days or be subject to an early deletion charge.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/archive-blob",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-21T12:34:00.000Z",
        "voteCount": 61,
        "content": "The solution is CORRECT, because BlockBlobStorage provide a very low latency(x40) (Read and Write) and Throughput (x5)\n\nBECAUSE:  One big file is splitted in  \"blobs\" that are processed in parallel (for read and write)\n\n\nhttps://azure.microsoft.com/en-us/blog/premium-block-blob-storage-a-new-level-of-performance/"
      },
      {
        "date": "2023-01-19T01:29:00.000Z",
        "voteCount": 4,
        "content": "Yes. the key word is maximize data throughput"
      },
      {
        "date": "2022-03-27T12:00:00.000Z",
        "voteCount": 36,
        "content": "Correct answer, but given reasoning for Archive Tier is wrong.\nYou achieve the immutability requirement through a Time-Based Retention Policy at the container-level. That will prevent write and delete operations for all blobs in the container for a given period (in this case, 1 year)."
      },
      {
        "date": "2024-06-21T11:01:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct.\nBlockBlobStorage is the optimal storage account type because it is designed to handle large-scale, high-throughput operations and supports features that prevent data modification.\nBlob storage service is the right choice for storing large amounts of unstructured data and minimizing latency."
      },
      {
        "date": "2024-04-28T22:00:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-10-19T20:10:00.000Z",
        "voteCount": 5,
        "content": "correct ans:\nAzure StorageV2 with Premium performance(immutable storage: no data modification for 1 year) \nblob storage"
      },
      {
        "date": "2024-02-14T04:07:00.000Z",
        "voteCount": 4,
        "content": "Azure StorageV2 only supports Standard performance:\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#storage-accounts\n\nImmutable storage is supported by both BlockBlobStorage and StorageV2:\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview#supported-account-configurations"
      },
      {
        "date": "2024-03-08T17:22:00.000Z",
        "voteCount": 5,
        "content": "True, I have tried to create storage account and there is only two options: \n1- Standard (General Purpose V2) \n2- Premium (BlockBlob, FileShare or PageBlob)\n\nthere is no premium storageV2!"
      },
      {
        "date": "2023-08-05T20:53:00.000Z",
        "voteCount": 10,
        "content": "Got this on Aug 5th, 2023."
      },
      {
        "date": "2023-07-23T09:59:00.000Z",
        "voteCount": 4,
        "content": "But if one of the requirements is to prevent the modification of data for one year, I think the correct choice would be a StorageV2 with standard performance because blob immutability is only available in General Purpose V2 storage account"
      },
      {
        "date": "2023-09-19T07:08:00.000Z",
        "voteCount": 3,
        "content": "No, blob immutability is also available in Premium block blob (BlockBlobStorate in the question).\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview#supported-account-configurations"
      },
      {
        "date": "2023-07-11T10:39:00.000Z",
        "voteCount": 8,
        "content": "Based on your requirements, the following options should be chosen:\n\n1. Storage account type: BlockBlobStorage. \nThis storage account type offers high-performance block blobs and append blobs. It provides the highest throughput for object storage in Azure, which will help maximize data throughput. Block blobs are ideal for storing text and binary data, such as documents and media files.\n\n2. Storage service: Blob. \nBlob storage is optimized for storing massive amounts of unstructured data, and it can handle frequently accessed data very well. Also, Azure Blob Storage supports object-level immutability policies, which can prevent the modification of data for a certain period, satisfying your requirement to prevent data modification for one year."
      },
      {
        "date": "2023-02-17T23:58:00.000Z",
        "voteCount": 6,
        "content": "1. BlockBlobStorage\n2. Blob\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-block-blob-premium\nPremium block blob storage accounts make data available via high-performance hardware. Data is stored on solid-state drives (SSDs) which are optimized for low latency. SSDs provide higher throughput compared to traditional hard drives. File transfer is much faster because data is stored on instantly accessible memory chips. All parts of a drive accessible at once. By contrast, the performance of a hard disk drive (HDD) depends on the proximity of data to the read/write heads."
      },
      {
        "date": "2023-02-18T00:00:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview#supported-account-configurations\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes."
      },
      {
        "date": "2023-02-16T07:25:00.000Z",
        "voteCount": 13,
        "content": "Solution is Correct. \nYou create a Storage Account (not StorageV2 something), then you chose the performance:\n  - Standard: type will be General Purpose v2\n  - Premium: 3 choices of type  \n       - BlockBlob\n       - FilesShares\n       - PageBlobs\n\nSo here we should choice premium block blob type, with blob \n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json"
      },
      {
        "date": "2023-04-25T09:35:00.000Z",
        "voteCount": 1,
        "content": "that was helpful thanks"
      },
      {
        "date": "2023-02-09T18:55:00.000Z",
        "voteCount": 4,
        "content": "Was there such a thing called \"StorageV2 with Premium performance\"?  Is this question sort of outdated?"
      },
      {
        "date": "2023-02-06T02:08:00.000Z",
        "voteCount": 3,
        "content": "When you try to create a new storage account, there are two options : Standard and Premium. When you click on Premium, there are three options - Block Blobs, Page Blobs and File Shares. We need Block Blobs to satisfy the first and last requirements. For the second requirement we need Blob for the retention ability."
      },
      {
        "date": "2023-01-28T01:16:00.000Z",
        "voteCount": 2,
        "content": "BlockBlobStorage \nBlob\nCorrect answer\n\nThanks to all who have mentioned the exam dates"
      },
      {
        "date": "2023-01-23T11:23:00.000Z",
        "voteCount": 6,
        "content": "I am a bit confused because storageV2 premium includes blockblob, isnt it?"
      },
      {
        "date": "2023-08-16T11:20:00.000Z",
        "voteCount": 2,
        "content": "Yeah, it also always confuses me... But I get it finally. StorageV2 with Premium performance doesn't even exists. If we use Premium performance, we DON'T have StorageV2 as a storage kind. If we use standard StorageV2, it will be then just called StorageV2. StorageV2 with Premium/Standard performance should NOT be even an option as above: Standard is only option for StorageV2; Using Premium Performance don't uses StorageV2 at all"
      },
      {
        "date": "2023-08-16T11:25:00.000Z",
        "voteCount": 1,
        "content": "I have tested that in the Azure and I am still confused..\n\nPremium Block Blob: BlockBlobStorage Kind\nPremium File Share: FileStorage Kind\nPremium Page Blob: StorageV2 (???????????)\nStandard: StorageV2"
      },
      {
        "date": "2024-01-21T11:41:00.000Z",
        "voteCount": 1,
        "content": "I am also confused."
      },
      {
        "date": "2022-12-28T19:44:00.000Z",
        "voteCount": 6,
        "content": "Exam Question 12/28/2022"
      },
      {
        "date": "2022-12-16T03:33:00.000Z",
        "voteCount": 2,
        "content": "Here answer options makes confusion between account performance and premium account type: \n#1- Premium Block Blobs\n#2- OK\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview#types-of-storage-accounts"
      },
      {
        "date": "2022-11-24T15:11:00.000Z",
        "voteCount": 7,
        "content": "The first dropdown does not make sense. BlockBlobStorage and FileStorage are examples of \"StorageV2 with Premium performance\", so if you want to select Blobs then how do you decide between BlockBlobStorage and \"StorageV2 with Premium performance\". It is like deciding between Ferrari and car."
      },
      {
        "date": "2022-12-29T17:33:00.000Z",
        "voteCount": 3,
        "content": "I think v2 is only for standard? And premium doesn't refer to v2? \nAnd BlockBlob is only available in premium (premium has blockblob, pageblob or file blob), so even though the options are badly worded, but seems correct."
      },
      {
        "date": "2023-01-12T11:36:00.000Z",
        "voteCount": 2,
        "content": "The answer should be \"StorageV2 with Premium performance\"\nI don't think blockblobstorage is an example of \"StorageV2 with Premium performance\" as it is available in standard as well."
      },
      {
        "date": "2023-01-12T11:44:00.000Z",
        "voteCount": 1,
        "content": "ignore the above reply... it's literally called as premium block blob. My bad.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview#types-of-storage-accounts"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68197-exam-az-305-topic-2-question-5-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains the storage accounts shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0009400001.png\" class=\"in-exam-image\"><br>You plan to implement two new apps that have the requirements shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0009400002.png\" class=\"in-exam-image\"><br>Which storage accounts should you recommend using for each app? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0009500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0009600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Storage1 and storage3 only<br>Need to use Standard accounts.<br>Data stored in a premium block blob storage account cannot be tiered to hot, cool, or archive using Set Blob Tier or using Azure Blob Storage lifecycle management<br>Box 2: Storage1 and storage4 only<br>Azure File shares requires Premium accounts. Only Storage1 and storage4 are premium.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#feature-support https://docs.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share?tabs=azure-portal#basics",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-28T02:56:00.000Z",
        "voteCount": 62,
        "content": "I think the proposed answer is correct. \nApp1: Storage1 and storage3 only\nApp2: Storage1 and storage4 only\n\nNote: Storage2, StorageV2 with Premium Performance does NOT exist \nhttps://docs.microsoft.com/en-ca/azure/storage/common/storage-account-overview?toc=/azure/storage/blobs/toc.json#types-of-storage-accounts"
      },
      {
        "date": "2021-12-31T14:38:00.000Z",
        "voteCount": 5,
        "content": "App1: \"Lifecycle management policies are supported for block blobs and append blobs in general-purpose v2, premium block blob, and Blob Storage accounts.\" from https://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview so answer is 1 and 3 (2 does not exist. go ahead, try to create one :)"
      },
      {
        "date": "2023-09-13T04:02:00.000Z",
        "voteCount": 3,
        "content": "While this is true, the only available option within a Lifecycle Management Policy for a premium block blob is to Delete the blob. Therefore Premium Block blobs don't have any access tiers (same as other premium storage accounts)"
      },
      {
        "date": "2024-03-08T17:26:00.000Z",
        "voteCount": 3,
        "content": "is this old question? because i tried to create storage account and found this only two options: \n1- Standard (General Purpose V2) \n2- Premium (BlockBlob, FileShare or PageBlob)\n\nthere is no premium storageV2!  then Storage2(premium StorageV2) doesn't exist?\n\nmay someone explain here?"
      },
      {
        "date": "2024-05-20T06:53:00.000Z",
        "voteCount": 2,
        "content": "StorageV2 means GPv2 and there's no such thing as Premium only Standard&gt; refer to MS documentation, but premium block blob exists"
      },
      {
        "date": "2023-07-24T12:23:00.000Z",
        "voteCount": 1,
        "content": "If you create a premium pageblob and in the portal generate an export template the ARM type is StandardV2 and the SKU.tier is Premium. In the configuration blade for the premium pageblob you can choose Hot or Cool storage tiers."
      },
      {
        "date": "2022-02-01T18:31:00.000Z",
        "voteCount": 5,
        "content": "Storage2, StorageV2 with premium performance is the same as Premium Page Blobs according to this reference: https://www.ais.com/how-to-choose-the-right-kind-of-azure-storage-account/"
      },
      {
        "date": "2022-02-12T04:19:00.000Z",
        "voteCount": 47,
        "content": "GENERATION V1 ==&gt; CANNOT HAVE LIFECYCLE\nGENERATION V2 =&gt; CAN HAVE LIFECYCLE\nPREMIUM FILE STORAGE ==&gt; CANNOT HAVE LIFECYCLE\nPREMIUM BLOG ==&gt; CANNOT HAVE LIFECYCLE (FYI - I TESTED THESE) . MORE OF FYI\n\nI TESTED ALL ABOVE\n\nTHEREFORE \nSTANDARD ==&gt; LIFE CYCLE YES (STORAGE 1 AND STORAGE 3)\n\nAPPS DATA - STORAGE 1 AND 4"
      },
      {
        "date": "2022-02-12T04:25:00.000Z",
        "voteCount": 7,
        "content": "STORAGE 2 ==&gt; V2 PREMIUM ==&gt; THIS SERVICE DOES NOT EXIST IN AZURE\n\nSTORAGE V1 STANDARD ONLY EXIST (WHICH IS WHY STORAGE 2 IS NEVER AN ANSWER)"
      },
      {
        "date": "2023-08-16T11:30:00.000Z",
        "voteCount": 1,
        "content": "STORAGE V2 PREMIUM EXISTS IN THE AZURE. It's called Premium Page Blob. In Premium Page Blob you don't have both Lifecycle Management and File Shares, only blobs"
      },
      {
        "date": "2024-03-18T06:47:00.000Z",
        "voteCount": 1,
        "content": "Please note : in the 1st image, storage2 and storage4 have premium performances.\nso shouldn't for APP2, the answer be storage 2 &amp; 4 ?\nbut i don't see that combination in answers, so does anyone know whether the given answer is correct ?"
      },
      {
        "date": "2024-01-21T11:54:00.000Z",
        "voteCount": 1,
        "content": "This anwswer in the website looks wrong to me, look below the reason I say that.\nhttps://learn.microsoft.com/en-us/answers/questions/1289726/life-cycle-management-and-access-tiers\n\n\"This feature is available for Blob storage (standard) accounts, GPv2 (general-purpose v2) accounts, and block blob storage accounts (premium). With lifecycle management policies, you can transition blobs between the hot, cool, and archive tiers within these supported storage accounts.\""
      },
      {
        "date": "2024-01-21T11:55:00.000Z",
        "voteCount": 1,
        "content": "Lifecycle Management Policies: Lifecycle management policies allow you to define rules that automatically move or delete blobs based on their age or other criteria. This feature is available for Blob storage (standard) accounts, GPv2 (general-purpose v2) accounts, and block blob storage accounts (premium). With lifecycle management policies, you can transition blobs between the hot, cool, and archive tiers within these supported storage accounts."
      },
      {
        "date": "2023-11-20T03:05:00.000Z",
        "voteCount": 3,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-06-17T09:50:00.000Z",
        "voteCount": 3,
        "content": "Storage1 (Standard storage v2) can have both blobs with tiers and file shares\nStorage2 (Premium storage v2) does not exist. When toy select premium you have to select Block blobs, File shares, or page blobs. If you select Block blobs you will find out that there are no tiers. Because premium storage should be fast. Having a Cool access tier does not have any sense here.\nStorage3 (Standard BlobStorage) is technically a reference to 1 of the services in Standard Storage v2 (BlobStorage), there are other services like TableService, File Service, and Queue Service. This one can have tiers for blobs as it is the same thing as Standard Storage v2.\nStorage4 (Premium FileStorage) is the file storage by definition. It does not have blob storage features like life cycles and tiers."
      },
      {
        "date": "2023-05-01T16:07:00.000Z",
        "voteCount": 3,
        "content": "Proposed answers are correct.\n- Only the Standard type/kind storage accounts support lifecycle management\n- Only the StorageV2 Standard offers \"File shares\" (\"File shares\" is not an option on a StorageV2 Premium) &amp; of course the FileStorage type/kind."
      },
      {
        "date": "2023-05-07T13:10:00.000Z",
        "voteCount": 1,
        "content": "It is true, I just created a StorageV2 (general purpose v2) Premium storage account in my lab &amp; it only provides for \"Containers\". That's it."
      },
      {
        "date": "2023-04-25T10:49:00.000Z",
        "voteCount": 6,
        "content": "Given answer is correct:\nApp1 can use Storage 1 and 3\nApp2 can use Storage 1 and 4\n*But* given explanation for App2 is wrong. Also, question may be outdated, since Azure uses Kind not Type. \nExplanation: \nAccording to this (see below), file shares are available via *both* Premium/File Shares and Standard general purpose version 2.\nSince Storage 1 is Type(Kind) StorageV2 and standard Tier, then App2 can use it for a file share. \nSince Storage 2 doesnt exist, (there is no StorageV2 Premium) then answer is not relevant. \nSince Storage 3 is  Type(Kind) BlobStorage, then it is not suitable (see below)\nSince Storage 4 is Type(Kind)File Storage and Premium Tier then App2 can use it for a file share."
      },
      {
        "date": "2023-04-25T10:49:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/storage/files/understanding-billing#storage-units\n\"Azure Files provides two distinct billing models: provisioned and pay-as-you-go. The provisioned model is only available for *premium* file shares, which are file shares deployed in the FileStorage storage account kind. The pay-as-you-go model is only available for standard file shares, which are file shares deployed in the *general purpose version 2* (GPv2) storage account kind.\""
      },
      {
        "date": "2023-04-08T09:34:00.000Z",
        "voteCount": 3,
        "content": "It said in the comment box 2: Storage1 and storage4 only\nAzure File shares requires Premium accounts. Only Storage1 and storage4 are premium. But storage 1 is Standard so I think it should be Storage 4 only for Box 2."
      },
      {
        "date": "2023-07-23T02:18:00.000Z",
        "voteCount": 1,
        "content": "I thought the same, but NFS is premium, Azure file share is on both:\nhttps://learn.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share?tabs=azure-portal#:~:text=What%20are%20the%20performance%20requirements%20for%20your%20Azure%20file%20share%3F%0AAzure%20Files%20offers%20standard%20file%20shares%20which%20are%20hosted%20on%20hard%20disk%2Dbased%20(HDD%2Dbased)%20hardware%2C%20and%20premium%20file%20shares%2C%20which%20are%20hosted%20on%20solid%2Dstate%20disk%2Dbased%20(SSD%2Dbased)%20hardware."
      },
      {
        "date": "2023-04-03T04:31:00.000Z",
        "voteCount": 1,
        "content": "This was in the exam on 04/01/2023 (mm/dd/yyyy)"
      },
      {
        "date": "2023-02-17T23:55:00.000Z",
        "voteCount": 3,
        "content": "1. Storage1 and storage3 only.\n2. Storage1 and storage4 only.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview\nLifecycle management policies are supported for block blobs and append blobs in general-purpose v2, premium block blob, and Blob Storage accounts. Lifecycle management doesn't affect system containers such as the $logs or $web containers."
      },
      {
        "date": "2023-02-28T07:57:00.000Z",
        "voteCount": 3,
        "content": "Got this in Feb 2023 exam."
      },
      {
        "date": "2023-02-17T23:55:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/storage/files/storage-files-planning#management-concepts\nThere are two main types of storage accounts you will use for Azure Files deployments:\n- General purpose version 2 (GPv2) storage accounts: GPv2 storage accounts allow you to deploy Azure file shares on standard/hard disk-based (HDD-based) hardware. In addition to storing Azure file shares, GPv2 storage accounts can store other storage resources such as blob containers, queues, or tables.\n- FileStorage storage accounts: FileStorage storage accounts allow you to deploy Azure file shares on premium/solid-state disk-based (SSD-based) hardware. FileStorage accounts can only be used to store Azure file shares; no other storage resources (blob containers, queues, tables, etc.) can be deployed in a FileStorage account. Only FileStorage accounts can deploy both SMB and NFS file shares."
      },
      {
        "date": "2023-02-11T11:02:00.000Z",
        "voteCount": 3,
        "content": "So, the right is, App1 can be use 1, 2 and 3, because all of them can have a blobo to usa lifecycle, and app2 can use storage 1,2, and 4, because we can create Azure Files Share on both!"
      },
      {
        "date": "2023-02-11T11:00:00.000Z",
        "voteCount": 1,
        "content": "I think you guys are wrong.\nLook, Azure file Share can be created on Storage Accountv2 Standard and Premium. And lifecycle can be used on a blob , even if he is on a storageV2."
      },
      {
        "date": "2023-02-06T02:28:00.000Z",
        "voteCount": 6,
        "content": "When you create a new storage account, there is no other option that StorageV2. So new storage accounts are all V2. What you can choose is performance - standard or premium. Standard includes all storage object types - blobs, tables, ques and file shares. Premium is only for block blobs, page blobs and file shares. So in this case file shares are supported by Storage 1, storage 2 and storage 4. \nThe correct answers are :\nStorage 1 and storage 3.\nStorage 1, storage 2 and storage 4."
      },
      {
        "date": "2023-02-09T19:22:00.000Z",
        "voteCount": 2,
        "content": "This question is either outdated or confusing because the doc  https://learn.microsoft.com/en-us/azure/storage/common/storage-account-overview listed just 4 account type.  But through your GUI 'test', it seems the question is legit.  And if so, the answer for App1 should be Storage1,2&amp;3."
      },
      {
        "date": "2023-01-28T01:29:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct\n\nApp1-\nstorage 1-StorageV2-Standard\nstorage 3-BlobStorage-Standard\n\nApp2\nstorage 1-StorageV2-Standard\nstorage 4-FileStorage-Premium"
      },
      {
        "date": "2023-01-19T00:14:00.000Z",
        "voteCount": 1,
        "content": "getting confused - \nAgreed on App1: Storage 1 and 3 only\nBut App2: Im inclined to say 1,3,4 at the very least but thats not available as an answer.\nazure file share can come in general purpose v2 flavor? https://learn.microsoft.com/en-us/azure/storage/files/storage-how-to-create-file-share?tabs=azure-portal"
      },
      {
        "date": "2022-11-28T04:41:00.000Z",
        "voteCount": 2,
        "content": "Lifecycle management policies are supported for block blobs and append blobs in general-purpose v2, premium block blob, and Blob Storage accounts. Lifecycle management doesn't affect system containers such as the $logs or $web containers."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67444-exam-az-305-topic-2-question-6-discussion/",
    "body": "You are designing an application that will be hosted in Azure.<br>The application will host video files that range from 50 MB to 12 GB. The application will use certificate-based authentication and will be available to users on the internet.<br>You need to recommend a storage option for the video files. The solution must provide the fastest read performance and must minimize storage costs.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 65,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-15T21:37:00.000Z",
        "voteCount": 38,
        "content": "Correct answer - C\n\nAzure Blob storage is Microsoft's object storage solution for the cloud. Blob storage is optimized for storing massive amounts of unstructured data, such as text or binary data.\nBlob storage is ideal for:\nServing images or documents directly to a browser.\nStoring files for distributed access.\nStreaming video and audio.\nStoring data for backup and restore, disaster recovery, and archiving.\nStoring data for analysis by an on-premises or Azure-hosted service.\nObjects in Blob storage can be accessed from anywhere in the world via HTTP or HTTPS. Users or client applications can access blobs via URLs, the Azure Storage REST API, Azure PowerShell, Azure CLI, or an Azure Storage client library.\n\n\nhttps://docs.microsoft.com/en-gb/azure/storage/common/storage-introduction#blob-storage"
      },
      {
        "date": "2021-12-29T09:06:00.000Z",
        "voteCount": 8,
        "content": "C is correct, Azure Blob"
      },
      {
        "date": "2024-04-28T22:24:00.000Z",
        "voteCount": 1,
        "content": "Given answer C is correct due to cost as B is overkill and expensive"
      },
      {
        "date": "2023-03-21T09:55:00.000Z",
        "voteCount": 6,
        "content": "C. Azure Blob Storage\n\nIn this scenario, I recommend using Azure Blob Storage for hosting the video files. Azure Blob Storage is a cost-effective, scalable, and durable storage service that is suitable for storing large, unstructured data like video files.\n\nHere's why Azure Blob Storage is the right choice:\n\nFast read performance: Azure Blob Storage provides fast read performance for serving video files to users on the internet.\n\nCost-effective: Blob Storage offers a competitive pricing model that minimizes storage costs.\n\nScalability: Azure Blob Storage can scale to store a large number of files, making it suitable for hosting video files of various sizes."
      },
      {
        "date": "2023-02-17T23:48:00.000Z",
        "voteCount": 4,
        "content": "C is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blobs-introduction#about-blob-storage\nBlob Storage is designed for:\n- Serving images or documents directly to a browser.\n- Storing files for distributed access.\n- Streaming video and audio.\n- Writing to log files.\n- Storing data for backup and restore, disaster recovery, and archiving.\n- Storing data for analysis by an on-premises or Azure-hosted service."
      },
      {
        "date": "2023-01-28T01:40:00.000Z",
        "voteCount": 1,
        "content": "C. Azure Blob Storage\nThanks all who have mentioned the exam dates"
      },
      {
        "date": "2023-02-06T06:43:00.000Z",
        "voteCount": 2,
        "content": "remember - Fastest read - C. Azure Blob Storage"
      },
      {
        "date": "2023-01-17T09:02:00.000Z",
        "voteCount": 1,
        "content": "blob storage is the way to store video files in azure"
      },
      {
        "date": "2022-08-18T14:24:00.000Z",
        "voteCount": 2,
        "content": "Does anyone know why B is not the answer?"
      },
      {
        "date": "2022-09-02T04:56:00.000Z",
        "voteCount": 5,
        "content": "Datalake is design for Big Data analytics, not service videos files to consumers"
      },
      {
        "date": "2022-11-11T05:52:00.000Z",
        "voteCount": 3,
        "content": "And it's expensive"
      },
      {
        "date": "2022-07-12T04:02:00.000Z",
        "voteCount": 2,
        "content": "Why not datalake? Blob (hot tier) is fast in read, but does not optimize storage costs, actually (in hot tier) it optimizes costs read/write transactions, but has higher cost of storage. Onyone can confirm please?"
      },
      {
        "date": "2022-08-03T10:10:00.000Z",
        "voteCount": 2,
        "content": "Even if it is cheaper I don't know if it actually supports cert based auth. And it's definitely not made to stream videos to end users."
      },
      {
        "date": "2022-07-05T11:21:00.000Z",
        "voteCount": 1,
        "content": "C. Blob Storage"
      },
      {
        "date": "2022-06-28T15:59:00.000Z",
        "voteCount": 1,
        "content": "Respuesta es la c"
      },
      {
        "date": "2022-05-23T15:24:00.000Z",
        "voteCount": 1,
        "content": "C is correct, Azure Blob"
      },
      {
        "date": "2022-04-09T04:11:00.000Z",
        "voteCount": 5,
        "content": "In my exam, 9 april 22, 817/1000, I chose this answer"
      },
      {
        "date": "2022-03-31T20:22:00.000Z",
        "voteCount": 2,
        "content": "in my exam on 31 Mar 22"
      },
      {
        "date": "2022-02-10T06:33:00.000Z",
        "voteCount": 2,
        "content": "Correct answer - C"
      },
      {
        "date": "2021-12-13T04:23:00.000Z",
        "voteCount": 4,
        "content": "The given answer is correct."
      },
      {
        "date": "2021-12-09T10:31:00.000Z",
        "voteCount": 3,
        "content": "the answer needs to be azure files as you need to be able to store video files up to 12GB which blob can't do. Azure files can store individual files of sizes up to 100GB"
      },
      {
        "date": "2021-12-13T16:22:00.000Z",
        "voteCount": 3,
        "content": "wrong, lokulluz is correct. max blob is 4.7tb"
      },
      {
        "date": "2021-12-13T02:52:00.000Z",
        "voteCount": 8,
        "content": "max. File Size is 4,7TB, hence given answer is correct.\n\nhttps://azure.microsoft.com/de-de/blog/general-availability-larger-block-blobs-in-azure-storage/"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68095-exam-az-305-topic-2-question-7-discussion/",
    "body": "You are designing a SQL database solution. The solution will include 20 databases that will be 20 GB each and have varying usage patterns.<br>You need to recommend a database platform to host the databases. The solution must meet the following requirements:<br>\u2711 The solution must meet a Service Level Agreement (SLA) of 99.99% uptime.<br>\u2711 The compute resources allocated to the databases must scale dynamically.<br>\u2711 The solution must have reserved capacity.<br>Compute charges must be minimized.<br><img src=\"/assets/media/exam-media/04224/0009700004.png\" class=\"in-exam-image\"><br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan elastic pool that contains 20 Azure SQL databases\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t20 databases on a Microsoft SQL server that runs on an Azure virtual machine in an availability set",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t20 databases on a Microsoft SQL server that runs on an Azure virtual machine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t20 instances of Azure SQL Database serverless"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 71,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-15T21:43:00.000Z",
        "voteCount": 22,
        "content": "Correct answer - A\nDatabases vary in usage so an elastic pool would fit best."
      },
      {
        "date": "2021-12-29T09:08:00.000Z",
        "voteCount": 12,
        "content": "A is correct. Elastic pool is needed for SLA 99,95 % and auto scale."
      },
      {
        "date": "2024-06-21T11:16:00.000Z",
        "voteCount": 2,
        "content": "A. An elastic pool that contains 20 Azure SQL databases\n\nReasons:\nSLA of 99.99% uptime: Azure SQL Database offers a high availability SLA of 99.99%.\nDynamic Scaling: Elastic pools dynamically allocate compute resources across the databases based on demand, which helps in managing varying usage patterns.\nReserved Capacity: Elastic pools support reserved capacity, which can help reduce costs.\nMinimized Compute Charges: By pooling resources, elastic pools optimize the overall cost, making compute charges more efficient compared to other options.\nReference:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql"
      },
      {
        "date": "2024-04-30T04:23:00.000Z",
        "voteCount": 2,
        "content": "in the Exam April 2024"
      },
      {
        "date": "2023-11-22T04:26:00.000Z",
        "voteCount": 7,
        "content": "it was a exam Question"
      },
      {
        "date": "2023-10-04T00:07:00.000Z",
        "voteCount": 1,
        "content": "Elastic pool is the best fit solution based on the requirements."
      },
      {
        "date": "2023-09-11T16:06:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer - A: Elastic Pool containing 20 Azure SQL databases\n- The primary consideration in the scenario is the presence of 20 databases, each 20 GB, with varying usage patterns. There's a need to ensure 99.99% uptime, dynamic scalability of compute resources, and reserved capacity, while also minimizing compute charges.\n\n- Elastic Pool with Azure SQL databases: Elastic pools in Azure are specifically designed to manage multiple databases with varying loads. They allow these databases to share resources in a cost-effective manner, guaranteeing dynamic scalability and a high SLA of 99.99% uptime. This aligns with the question requirements.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview"
      },
      {
        "date": "2023-04-29T12:51:00.000Z",
        "voteCount": 6,
        "content": "The answer is correct. I had this question in my exam today, I passed with 979."
      },
      {
        "date": "2023-03-21T09:57:00.000Z",
        "voteCount": 10,
        "content": "A. an elastic pool that contains 20 Azure SQL databases\n\nElastic pools in Azure SQL Database are designed to handle multiple databases with varying usage patterns within a shared resource pool. This option meets the following requirements:\n\nSLA of 99.99% uptime: Azure SQL Database provides an SLA of 99.99% uptime, ensuring high availability for your databases.\n\nDynamic scaling of compute resources: Elastic pools allow you to allocate resources dynamically, adjusting to the varying usage patterns of your databases.\n\nReserved capacity: Elastic pools enable you to reserve capacity for multiple databases within the pool, ensuring resources are available when needed.\n\nMinimize compute charges: By sharing resources among the databases within the elastic pool, you can minimize compute charges while still meeting the performance requirements."
      },
      {
        "date": "2023-02-17T23:47:00.000Z",
        "voteCount": 7,
        "content": "A is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-pool-overview?view=azuresql\nAzure SQL Database elastic pools are a simple, cost-effective solution for managing and scaling multiple databases that have varying and unpredictable usage demands. The databases in an elastic pool are on a single server and share a set number of resources at a set price. Elastic pools in SQL Database enable software as a service (SaaS) developers to optimize the price performance for a group of databases within a prescribed budget while delivering performance elasticity for each database."
      },
      {
        "date": "2023-02-11T14:34:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer:A\n\nC-Not correct because;\nIf you want access your database OS level. You can use this option.It main way.\n\nD-Not correct because;\nMicrosoft quotation;\nServerless option. Use the serverless compute tier for a single SQL database. You're billed only for the amount of compute used."
      },
      {
        "date": "2023-01-28T01:44:00.000Z",
        "voteCount": 2,
        "content": "A. an elastic pool that contains 20 Azure SQL databases\nThanks to all who have mentioned the exam dates"
      },
      {
        "date": "2023-01-17T09:04:00.000Z",
        "voteCount": 1,
        "content": "Different usage patterns across dbs is the key thing here"
      },
      {
        "date": "2023-01-15T22:39:00.000Z",
        "voteCount": 6,
        "content": "shown on 1/14/23"
      },
      {
        "date": "2023-01-14T14:58:00.000Z",
        "voteCount": 1,
        "content": "\"varying usage patterns\" =&gt; Elastic pool"
      },
      {
        "date": "2022-12-28T19:45:00.000Z",
        "voteCount": 4,
        "content": "Exam Question 12/28/2022"
      },
      {
        "date": "2022-09-18T10:27:00.000Z",
        "voteCount": 1,
        "content": "SQL Database Reserved Capacity, Reservation can be assigned to either a single Azure Subscription or shared, and there\u2019s vCore Size Flexibility as well where the Reservation can be applied dynamically to any databases and elastic pools within a performance tier and region.\nDynamic scalability is different from autoscale. Autoscale is when a service scales automatically based on criteria, whereas dynamic scalability allows for manual scaling with a minimal downtime. Single databases in Azure SQL Database can be scaled manually, or in the case of the Serverless tier, set to automatically scale the compute resources. Elastic pools, which allow databases to share resources in a pool, can currently only be scaled manually."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68096-exam-az-305-topic-2-question-8-discussion/",
    "body": "HOTSPOT -<br>You have an on-premises database that you plan to migrate to Azure.<br>You need to design the database architecture to meet the following requirements:<br>\u2711 Support scaling up and down.<br>\u2711 Support geo-redundant backups.<br>\u2711 Support a database of up to 75 TB.<br>\u2711 Be optimized for online transaction processing (OLTP).<br>What should you include in the design? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0009900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0010000001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure SQL Database -<br>Azure SQL Database:<br>Database size always depends on the underlying service tiers (e.g. Basic, Business Critical, Hyperscale).<br>It supports databases of up to 100 TB with Hyperscale service tier model.<br>Active geo-replication is a feature that lets you to create a continuously synchronized readable secondary database for a primary database. The readable secondary database may be in the same Azure region as the primary, or, more commonly, in a different region. This kind of readable secondary databases are also known as geo-secondaries, or geo-replicas.<br>Azure SQL Database and SQL Managed Instance enable you to dynamically add more resources to your database with minimal downtime.<br><br>Box 2: Hyperscale -<br>Incorrect Answers:<br>\u2711 SQL Server on Azure VM: geo-replication not supported.<br>\u2711 Azure Synapse Analytics is not optimized for online transaction processing (OLTP).<br>\u2711 Azure SQL Managed Instance max database size is up to currently available instance size (depending on the number of vCores).<br>Max instance storage size (reserved) - 2 TB for 4 vCores<br>- 8 TB for 8 vCores<br>- 16 TB for other sizes<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview https://medium.com/awesome-azure/azure-difference-between-azure-sql-database-and-sql-server-on-vm-comparison-azure-sql-vs-sql-server-vm-cf02578a1188",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-05T19:03:00.000Z",
        "voteCount": 51,
        "content": "Answer is correct. -Azure SQL Database with Hyperscale(support up to 100TB).\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale#:~:text=A%20Hyperscale%20database%20is%20created%20with%20a%20starting,about%20Hyperscale%20pricing%2C%20see%20Azure%20SQL%20Database%20Pricing\n\nManaged Instance is incorrect because the database limit is 2-8TB max.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/resource-limits#:~:text=Up%20to%20280%2C%20unless%20the%20instance%20storage%20size,TB%29%20and%20Azure%20Premium%20Disk%20storage%20allocation%20space."
      },
      {
        "date": "2023-02-11T23:53:00.000Z",
        "voteCount": 6,
        "content": "Azure SQL MI also does not support active geo replication."
      },
      {
        "date": "2024-02-14T05:26:00.000Z",
        "voteCount": 1,
        "content": "The limit for MI is now 16 TB - still far away from 75 TB."
      },
      {
        "date": "2022-04-10T19:46:00.000Z",
        "voteCount": 14,
        "content": "75T can only be supported by hyperscale."
      },
      {
        "date": "2023-11-19T13:24:00.000Z",
        "voteCount": 3,
        "content": "The key is that only Hyperscale can deal with 75 Tb, All other have limit of 4 Tb"
      },
      {
        "date": "2023-03-21T10:00:00.000Z",
        "voteCount": 10,
        "content": "Azure SQL Database Hyperscale:\n\nSupport scaling up and down: The Hyperscale service tier supports scaling compute resources up and down based on your workload requirements.\nSupport geo-redundant backups: It offers automatic backups with the ability to enable geo-redundant backups to ensure data durability in case of regional disasters.\nSupport a database of up to 75 TB: Hyperscale supports databases up to 100 TB in size, which meets the requirement of 75 TB.\nBe optimized for online transaction processing (OLTP): Azure SQL Database Hyperscale is designed to handle OLTP workloads with high performance and low latency.\nIn summary, you should include Azure SQL Database with the Hyperscale service tier in your database architecture design to meet all the listed requirements."
      },
      {
        "date": "2023-02-17T23:45:00.000Z",
        "voteCount": 4,
        "content": "1. Azure SQL DB\n2. Hyperscale\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql#what-are-the-hyperscale-capabilities\nThe Hyperscale service tier in Azure SQL Database provides the following additional capabilities:\n- Support for up to 100 TB of database size.\n- Higher overall performance due to higher transaction log throughput and faster transaction commit times regardless of data volumes.\n- Rapid Scale up - you can, in constant time, scale up your compute resources to accommodate heavy workloads when needed, and then scale the compute resources back down when not needed."
      },
      {
        "date": "2023-02-11T14:44:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct;\n\nlet me compare with  explain on microsoft official site;\n\nhttps://learn.microsoft.com/en-us/training/modules/design-data-storage-solution-for-relational-data/4-design-for-sql-server-azure#:~:text=SQL%20Server%20licenses.-,Compare%20Azure%20SQL%20deployment%20options,-You%27ve%20reviewed%20the"
      },
      {
        "date": "2023-01-28T01:49:00.000Z",
        "voteCount": 6,
        "content": "Answer is correct\nBox 1: Azure SQL Database\nBox 2: Hyperscale - key point 75TB\n\nAzure SQL Database with Hyperscale\n\nThanks all who have mentioned the exam dates"
      },
      {
        "date": "2022-12-28T19:45:00.000Z",
        "voteCount": 7,
        "content": "Exam Question 12/28/2022"
      },
      {
        "date": "2022-11-07T21:42:00.000Z",
        "voteCount": 2,
        "content": "Ans - Correct"
      },
      {
        "date": "2022-09-19T12:31:00.000Z",
        "voteCount": 1,
        "content": "Correct.\n\nResource limits for SQL Database tiers\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql#compare-resource-limits"
      },
      {
        "date": "2022-08-22T22:34:00.000Z",
        "voteCount": 1,
        "content": "\"Support geo-redundant backups\". - this is not the same as geo replication as the answer states.... Both MI and SQL variants have auto backup that is stored as geo redundant blobs replicated to a paired region.  But yes, hyperscale is correct due to DB size."
      },
      {
        "date": "2022-07-09T17:31:00.000Z",
        "voteCount": 5,
        "content": "The reason to choose Hyperscale, since its the design of db migration: \nThe Hyperscale service tier in Azure SQL Database provides the following additional capabilities:\n\nSupport for up to 100 TB of database size.\nFast database backups (based on file snapshots stored in Azure Blob storage) regardless of size with no IO impact on compute resources.\nFast database restores (based on file snapshots) in minutes rather than hours or days (not a size of data operation).\nHigher overall performance due to higher transaction log throughput and faster transaction commit times regardless of data volumes.\nRapid scale out - you can provision one or more read-only replicas for offloading your read workload and for use as hot-standbys.\nRapid Scale up - you can, in constant time, scale up your compute resources to accommodate heavy workloads when needed, and then scale the compute resources back down when not needed."
      },
      {
        "date": "2022-05-23T15:33:00.000Z",
        "voteCount": 1,
        "content": "Answers are correct - Azure SAL Database with Hyperscale."
      },
      {
        "date": "2022-05-08T12:15:00.000Z",
        "voteCount": 4,
        "content": "was in exam 8 May 22"
      },
      {
        "date": "2022-04-09T04:11:00.000Z",
        "voteCount": 5,
        "content": "In my exam, 9 april 22, 817/1000, I chose this answer : Database/Hyperscale"
      },
      {
        "date": "2022-03-31T20:24:00.000Z",
        "voteCount": 2,
        "content": "in my exam on 31 Mar 22"
      },
      {
        "date": "2022-03-31T20:21:00.000Z",
        "voteCount": 1,
        "content": "in my exam on 31 Mar 22"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68097-exam-az-305-topic-2-question-9-discussion/",
    "body": "You are planning an Azure IoT Hub solution that will include 50,000 IoT devices.<br>Each device will stream data, including temperature, device ID, and time data. Approximately 50,000 records will be written every second. The data will be visualized in near real time.<br>You need to recommend a service to store and query the data.<br>Which two services can you recommend? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Grid",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB SQL API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Time Series Insights\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-03T08:19:00.000Z",
        "voteCount": 53,
        "content": "C and D are correect: \nNeed to find a service to store and query the data.\n\tA. Azure Table Storage: You can't query data.\n\tB. Azure Event Grid: You can't store or query data.\n\tC. Azure Cosmos DB SQL API: You can store and query data.\n\tD. Azure Time Series Insights: You can store and query data."
      },
      {
        "date": "2024-07-31T19:22:00.000Z",
        "voteCount": 2,
        "content": "Not so sure about Table Storage. According to https://learn.microsoft.com/en-us/azure/storage/tables/table-storage-overview :\n\nAzure Table storage stores large amounts of structured data. The service is a NoSQL datastore which accepts authenticated calls from inside and outside the Azure cloud.\n\nYou can use Table storage to store and query huge sets of structured, non-relational data, and your tables will scale as demand increases.\n\nSounds like a better fit than Cosmos DB in this case perhaps?"
      },
      {
        "date": "2021-12-15T21:50:00.000Z",
        "voteCount": 21,
        "content": "C &amp; D appear to be correct. \n\nCosmos dB SQL API is somewhat confusing as an accurate answer though:\nhttps://docs.microsoft.com/en-gb/azure/cosmos-db/introduction#solutions-that-benefit-from-azure-cosmos-db"
      },
      {
        "date": "2021-12-15T21:52:00.000Z",
        "voteCount": 7,
        "content": "https://docs.microsoft.com/en-gb/azure/cosmos-db/use-cases#iot-and-telematics"
      },
      {
        "date": "2024-04-28T22:32:00.000Z",
        "voteCount": 1,
        "content": "Given answer CD is correct"
      },
      {
        "date": "2023-12-28T08:27:00.000Z",
        "voteCount": 1,
        "content": "I think its Event Grid and Time Series Insights:\n\nAzure Event Grid is used at different stages of data pipelines to achieve a diverse set of integration goals.\n\nMQTT messaging. IoT devices and applications can communicate with each other over MQTT. Event Grid can also be used to route MQTT messages to Azure services or custom endpoints for further data analysis, visualization, or storage. This integration with Azure services enables you to build data pipelines that start with data ingestion from your IoT devices.\n\nhttps://learn.microsoft.com/en-us/azure/event-grid/overview"
      },
      {
        "date": "2023-11-22T02:02:00.000Z",
        "voteCount": 5,
        "content": "Appeared on 11/21/2023"
      },
      {
        "date": "2023-08-03T05:35:00.000Z",
        "voteCount": 1,
        "content": "C and D"
      },
      {
        "date": "2023-03-21T10:03:00.000Z",
        "voteCount": 6,
        "content": "C. Azure Cosmos DB SQL API\nD. Azure Time Series Insights\n\nC. Azure Cosmos DB SQL API: Azure Cosmos DB is a globally distributed, multi-model database service that is designed for high throughput and low-latency scenarios. The SQL API for Cosmos DB provides a JSON-based, document-oriented database that can be used to store and query the IoT data. It can handle large volumes of data and scale horizontally, making it suitable for the high volume of records generated by the IoT devices.\n\nD. Azure Time Series Insights: This is an Azure service specifically designed for analyzing time-series data in near real-time. It can ingest, store, and query large amounts of time-series data generated by IoT devices. It also provides visualization capabilities to monitor and explore the data, which makes it suitable for the described scenario."
      },
      {
        "date": "2023-02-22T09:10:00.000Z",
        "voteCount": 3,
        "content": "Same as Question 20.\nhttps://www.examtopics.com/discussions/microsoft/view/94045-exam-az-305-topic-2-question-20-discussion"
      },
      {
        "date": "2023-02-22T09:10:00.000Z",
        "voteCount": 5,
        "content": "CD is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db\nAzure Cosmos DB is ideal for IoT workloads because it's capable of:\n- Ingesting device telemetry data at high rates, and return indexed queries with low latency and high availability.\n- Storing JSON format from different device vendors, which provides flexibility in payload schema.\n- By using wire protocol\u2013compatible API endpoints for Cassandra, MongoDB, SQL, Gremlin, etcd, and table databases, and built-in support for Jupyter Notebook files."
      },
      {
        "date": "2023-02-22T09:10:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi\nAzure Time Series Insights Gen2 is an open and scalable end-to-end IoT analytics service featuring best-in-class user experiences and rich APIs to integrate its powerful capabilities into your existing workflow or application.\n\nYou can use it to collect, process, store, query and visualize data at Internet of Things (IoT) scale--data that's highly contextualized and optimized for time series.\n\nAzure Time Series Insights Gen2 is designed for ad hoc data exploration and operational analysis allowing you to uncover hidden trends, spotting anomalies, and conduct root-cause analysis. It's an open and flexible offering that meets the broad needs of industrial IoT deployments."
      },
      {
        "date": "2023-02-10T09:21:00.000Z",
        "voteCount": 7,
        "content": "The Time Series Insights (TSI) service is being deprecated.\nWe will likely see Azure Data Explorer as a replacement for a real time (\"hot path\") data store option.\nhttps://learn.microsoft.com/en-us/azure/time-series-insights/migration-to-adx\nhttps://learn.microsoft.com/en-us/azure/architecture/data-guide/scenarios/time-series#dataflow"
      },
      {
        "date": "2023-01-28T01:52:00.000Z",
        "voteCount": 1,
        "content": "C. Azure Cosmos DB SQL API\nD. Azure Time Series Insights"
      },
      {
        "date": "2023-01-25T14:29:00.000Z",
        "voteCount": 1,
        "content": "MongoDB API is the right answer..."
      },
      {
        "date": "2023-01-24T11:02:00.000Z",
        "voteCount": 1,
        "content": "C. Azure Cosmos DB SQL API\nD. Azure Time Series Insights"
      },
      {
        "date": "2023-01-06T16:24:00.000Z",
        "voteCount": 3,
        "content": "For those who picked B as a correct option, you may have confused Azure Event Grid with Azure Event Hub."
      },
      {
        "date": "2022-11-10T11:19:00.000Z",
        "voteCount": 1,
        "content": "C &amp; D correct answers"
      },
      {
        "date": "2022-10-10T21:12:00.000Z",
        "voteCount": 1,
        "content": "CD is the correct answer."
      },
      {
        "date": "2022-08-17T05:18:00.000Z",
        "voteCount": 2,
        "content": "Ans. C,D\n*Solutions that benefit from Azure Cosmos DB\nAny web, mobile, gaming, and IoT application that needs to handle massive amounts of data, reads, and writes at a global scale with near-real response times for a variety of data will benefit from Cosmos DB's guaranteed high availability, high throughput, low latency, and tunable consistency. Learn about how Azure Cosmos DB can be used to build IoT and telematics, retail and marketing, gaming and web and mobile applications.\n\n*Many time series-based systems, such as Internet of things (IoT) scenarios, capture data in real time by using a real-time processing architecture.\nAzure IoT Hub, Azure Event Hubs, or Kafka on HDInsight ingest data from one or more data sources into the stream processing layer.\nThe stream processing layer processes the data, and can hand off the processed data to a machine learning service for predictive analytics.\nAn analytical data store like Azure Data Explorer, HBase, Azure Cosmos DB, or Azure Data Lake stores the processed data.\nAn analytics and reporting application or service like Power BI or OpenTSDB for HBase can display the time series data for analysis."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67751-exam-az-305-topic-2-question-10-discussion/",
    "body": "You are designing an application that will aggregate content for users.<br>You need to recommend a database solution for the application. The solution must meet the following requirements:<br>\u2711 Support SQL commands.<br>\u2711 Support multi-master writes.<br>\u2711 Guarantee low latency read operations.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB SQL API\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database that uses active geo-replication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Hyperscale",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Database for PostgreSQL"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 57,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-12T13:27:00.000Z",
        "voteCount": 23,
        "content": "Correct answer - A"
      },
      {
        "date": "2022-12-28T19:47:00.000Z",
        "voteCount": 17,
        "content": "Exam Question 12/28/2022. Instead of the  Cosmos DB SQL, the option was Cosmos DB NoSQL"
      },
      {
        "date": "2024-04-28T22:36:00.000Z",
        "voteCount": 3,
        "content": "Given answer A is correct \nAzure Cosmos DB SQL API or can be rewarded to Azure Cosmos DB for NOSQL API\n\nAzure Cosmos DB for NoSQL Azure Cosmos DB's core, or native API for working with documents. Supports fast, flexible development with familiar SQL query language and client libraries for .NET, JavaScript, Python, and Java."
      },
      {
        "date": "2023-11-20T03:11:00.000Z",
        "voteCount": 5,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-09-19T09:49:00.000Z",
        "voteCount": 4,
        "content": "Azure SQL Database that uses active geo-replication cannot be an answer because it does not offer multi-master write. Active geo-replication is a feature that lets you create a continuously synchronized readable secondary database for a primary database. The secondary database is read-only. So, Cosmos DB SQL is the only right answer.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/active-geo-replication-overview?view=azuresql"
      },
      {
        "date": "2023-07-11T11:05:00.000Z",
        "voteCount": 3,
        "content": "A. Azure Cosmos DB SQL API\n\nAzure Cosmos DB is a globally distributed, multi-model database service. It offers turnkey global distribution, automatically replicating your data to any number of Azure regions so you can achieve low latency access from anywhere in the world.\n\nCosmos DB supports various APIs for data access including SQL (Core) API, which uses SQL commands. It provides multi-master support, which allows you to perform writes on any of your replicas and replicate data across all of them for high availability. So it will cover your requirement of supporting multi-master writes.\n\nIn terms of guaranteeing low latency read operations, Azure Cosmos DB offers &lt;10 ms latencies at the 99th percentile for reads and writes, which would serve your need of low latency reads."
      },
      {
        "date": "2023-02-17T23:42:00.000Z",
        "voteCount": 5,
        "content": "A is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction#key-benefits\n- Gain unparalleled SLA-backed speed and throughput, fast global access, and instant elasticity. Real-time access with fast read and write latencies globally, and throughput and consistency all backed by SLAs\n- Multi-region writes and data distribution to any Azure region with just a button."
      },
      {
        "date": "2023-02-11T15:08:00.000Z",
        "voteCount": 1,
        "content": "Correct;\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-multi-master?tabs=api-async#:~:text=data%20globally%20pane.-,Under%20the%20Multi%2Dregion%20writes%20option%2C%20choose%20enable.%20It%20automatically%20adds%20the%20existing%20regions%20to%20read%20and%20write%20regions.,-You%20can%20add"
      },
      {
        "date": "2023-01-28T02:00:00.000Z",
        "voteCount": 1,
        "content": "A. Azure Cosmos DB SQL API\n\nAs Ghoshy mentioned, the option could have been Cosmos DB NoSQL also"
      },
      {
        "date": "2023-01-14T18:08:00.000Z",
        "voteCount": 5,
        "content": "In the exam I had in Jan 2023, I got the same question with different answering items.\nThere was no \u201cAzure Cosmos DB SQL API \u201d, and the answering items presented were\nAzure Cosmos DB for NoSQL\nAzure Cosmos DB for PostgreSQL"
      },
      {
        "date": "2023-10-29T07:36:00.000Z",
        "voteCount": 1,
        "content": "Which one was the correct one?"
      },
      {
        "date": "2022-05-23T15:42:00.000Z",
        "voteCount": 1,
        "content": "Correct answer - A\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-multi-master?tabs=api-async"
      },
      {
        "date": "2022-04-26T21:54:00.000Z",
        "voteCount": 1,
        "content": "Correct answer - A\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-multi-master?tabs=api-async"
      },
      {
        "date": "2022-04-09T04:11:00.000Z",
        "voteCount": 4,
        "content": "In my exam, 9 april 22, 817/1000, I chose this answer"
      },
      {
        "date": "2022-03-31T07:05:00.000Z",
        "voteCount": 2,
        "content": "A is the good choice"
      },
      {
        "date": "2022-02-28T15:59:00.000Z",
        "voteCount": 3,
        "content": "only A is correct"
      },
      {
        "date": "2022-02-23T10:14:00.000Z",
        "voteCount": 4,
        "content": "On the AZ-305 2/22/22"
      },
      {
        "date": "2022-02-14T01:01:00.000Z",
        "voteCount": 8,
        "content": "Only Cosmos DB supports multi-master writes:\nhttps://docs.microsoft.com/en-us/azure/cosmos-db/sql/how-to-multi-master?tabs=api-async"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67837-exam-az-305-topic-2-question-11-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains the SQL servers on Azure shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0010300001.png\" class=\"in-exam-image\"><br>The subscription contains the storage accounts shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0010300002.png\" class=\"in-exam-image\"><br>You create the Azure SQL databases shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0010300003.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0010400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0010400002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Yes -<br>Auditing works fine for a Standard account.<br><br>Box 2: No -<br>Auditing limitations: Premium storage is currently not supported.<br><br>Box 3: No -<br>Auditing limitations: Premium storage is currently not supported.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview#auditing-limitations",
    "votes": [],
    "comments": [
      {
        "date": "2022-02-12T04:41:00.000Z",
        "voteCount": 175,
        "content": "YNN\n\nCONCEPT TO REMEMBER\n1. TO WRITE INTO STORAGE, MUST BE IN SAME REGION\n2. TO WIRTE IN LOG ANALYTICS SPACE - CAN BE IN DIFFERENT REGION\n\nSINCE WE ARE USING CONCEPT 1, CAN ONLY WRITE INTO SAME REGION\n\nIT HAS NOTHING TO DO WITH PRICING TIER"
      },
      {
        "date": "2023-01-04T07:22:00.000Z",
        "voteCount": 23,
        "content": "Why are you writing in capital, LOL ?"
      },
      {
        "date": "2023-01-20T22:35:00.000Z",
        "voteCount": 87,
        "content": "because it's SQL :)"
      },
      {
        "date": "2023-01-27T12:56:00.000Z",
        "voteCount": 6,
        "content": "According to MS documentation: If you are deploying from the Azure portal, be sure that the storage account is in the same region as your database and server. If you are deploying through other methods, the storage account can be in any region."
      },
      {
        "date": "2024-04-18T00:04:00.000Z",
        "voteCount": 1,
        "content": "So are you implying that it should be YYY?"
      },
      {
        "date": "2023-08-20T10:45:00.000Z",
        "voteCount": 2,
        "content": "it's wrong,this has nothing to do with regions.!!!!!!!!"
      },
      {
        "date": "2024-02-20T13:58:00.000Z",
        "voteCount": 3,
        "content": "you are wrong. For auditing, storage account needs to be in same region as db. I have tested just now in lab. It won't show any storage acocunts in other regions while configuring auditing."
      },
      {
        "date": "2021-12-13T10:30:00.000Z",
        "voteCount": 67,
        "content": "answer sould be Yes, No, No \nAuditing limitations\nPremium storage is currently not supported.\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview"
      },
      {
        "date": "2021-12-15T09:38:00.000Z",
        "voteCount": 4,
        "content": "I saw this in document, but I am not sure that is mean sql database or storage account."
      },
      {
        "date": "2021-12-15T09:41:00.000Z",
        "voteCount": 15,
        "content": "BTW, the region is not the same.\nY, N, N should be correct."
      },
      {
        "date": "2022-03-15T05:59:00.000Z",
        "voteCount": 1,
        "content": "Who said that? Storage have indicated the Resource group only not the region"
      },
      {
        "date": "2022-06-19T17:14:00.000Z",
        "voteCount": 2,
        "content": "Makkros yes it DOES indicate the location."
      },
      {
        "date": "2021-12-15T22:46:00.000Z",
        "voteCount": 5,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview#auditing-limitations"
      },
      {
        "date": "2022-09-26T18:52:00.000Z",
        "voteCount": 3,
        "content": "Storage 2 can be standard BlobStorage, therefore this explanation is not correct"
      },
      {
        "date": "2024-10-15T02:39:00.000Z",
        "voteCount": 1,
        "content": "Yes\nNo\nNo"
      },
      {
        "date": "2024-08-22T22:07:00.000Z",
        "voteCount": 1,
        "content": "If you are deploying from the Azure portal, make sure that the storage account is in the same region as your database and server. If you are deploying through other methods, the storage account can be in any region.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/auditing-setup?view=azuresql"
      },
      {
        "date": "2024-06-21T11:32:00.000Z",
        "voteCount": 1,
        "content": "it should be Y Y Y\nKey Points from Azure Documentation:\nRegion: Audit logs can be stored in any region within the same subscription.\nPricing Tier: Pricing tiers (Standard, Premium) do not restrict auditing capabilities.\nAccount Kind: Audit logs can be stored in both general-purpose v2 (StorageV2) and BlobStorage accounts."
      },
      {
        "date": "2024-09-07T08:20:00.000Z",
        "voteCount": 2,
        "content": "as per microsoft ,: Premium storage with BlockBlobStorage is supported. Standard storage is supported. However, for audit to write to a storage account behind a VNet or firewall, you must have a general-purpose v2 storage account. If you have a general-purpose v1 or Blob Storage account, upgrade to a general-purpose v2 storage account."
      },
      {
        "date": "2024-05-03T04:16:00.000Z",
        "voteCount": 1,
        "content": "YYY\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/auditing-setup?view=azuresql#audit-to-storage-destination\nIt has nothing to do with the pricing tier of the SQL server or the stoprage.\nIt does depend on what is being used to configure the auditing.\nIf you try to configure via Azure Portal, then it needs to be same region.\nIf you are trying to configure via CLI, then it can be any region."
      },
      {
        "date": "2024-05-05T05:30:00.000Z",
        "voteCount": 2,
        "content": "U Must have been talking to Bill Gates!! Ans YNN"
      },
      {
        "date": "2024-03-20T06:26:00.000Z",
        "voteCount": 1,
        "content": "Yes\nNo\nNo\n1. is a given\n2. https://learn.microsoft.com/en-us/azure/azure-sql/database/audit-write-storage-account-behind-vnet-firewall?view=azuresql#prerequisites\nThe storage account must be on the same tenant and at the same location as the logical SQL server (it's OK to be on different subscriptions).\n3. BlockBlobStorage in not mentioned!? (Blob Storage)\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/audit-write-storage-account-behind-vnet-firewall?view=azuresql#prerequisites\nThe premium storage with BlockBlobStorage is supported"
      },
      {
        "date": "2024-02-20T14:00:00.000Z",
        "voteCount": 2,
        "content": "Test in Lab.\nAnswer is indeed Y,N,N.\nReason:\nWhile configuring auditing for azure sql db, azure portal only will show storage accounts which are in same region. Both standard and premium storage accounts can be used but they need to be in same region as db."
      },
      {
        "date": "2024-01-21T09:31:00.000Z",
        "voteCount": 6,
        "content": "appeared in Exam 01/2024"
      },
      {
        "date": "2023-12-13T10:44:00.000Z",
        "voteCount": 3,
        "content": "Y, N, N should be the correct response now. as of december 2023, Premium storage is supported for SQL Auditing - but you can't write to a different region (just try it in the portal, see if storages in different regions pop up ;)"
      },
      {
        "date": "2023-11-20T03:29:00.000Z",
        "voteCount": 5,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-10-17T04:21:00.000Z",
        "voteCount": 2,
        "content": "I tried. MY SQL server is in West Europe. I created a Standard V2 storage account in North Europe. When I try to configure the auditing the storage account does not show up in the dropdown. A standard v2 storage account in west Europe is in the list however."
      },
      {
        "date": "2023-09-26T14:22:00.000Z",
        "voteCount": 2,
        "content": "yes no no. blobstorage is unsupported. blockblobstorage is supported.."
      },
      {
        "date": "2023-09-20T10:02:00.000Z",
        "voteCount": 3,
        "content": "YYY\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/auditing-overview#auditing-limitations\nPremium storage with BlockBlobStorage is supported. Standard storage is supported. However, for audit to write to a storage account behind a VNet or firewall, you must have a general-purpose v2 storage account. If you have a general-purpose v1 or Blob Storage account, upgrade to a general-purpose v2 storage account. For specific instructions see, Write audit to a storage account behind VNet and firewall. For more information, see Types of storage accounts."
      },
      {
        "date": "2023-09-17T08:39:00.000Z",
        "voteCount": 9,
        "content": "This question is on today's exam.\nThe exam is easier than AZ-104."
      },
      {
        "date": "2023-09-18T06:31:00.000Z",
        "voteCount": 5,
        "content": "Keep doing these comments you\u2019re my hero"
      },
      {
        "date": "2023-09-09T04:38:00.000Z",
        "voteCount": 1,
        "content": "YNN. Only one limitation: \"If you have a general-purpose v1 or Blob Storage account, upgrade to a general-purpose v2 storage account.\" Storage2 is not usable in this case."
      },
      {
        "date": "2023-09-15T09:19:00.000Z",
        "voteCount": 2,
        "content": "No, man. Microsoft says:\n\"However, for audit to write to a storage account  !!! behind a VNet or firewall !!!, you must have a general-purpose v2 storage account. If you have a general-purpose v1 or Blob Storage account, upgrade to a general-purpose v2 storage account.\" \nHere there isn't firewall o vnet, so storage2 is valid.\nAnswer is YYY"
      },
      {
        "date": "2023-08-07T14:04:00.000Z",
        "voteCount": 1,
        "content": "What is the correct answer then ?"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79391-exam-az-305-topic-2-question-12-discussion/",
    "body": "DRAG DROP -<br>You plan to import data from your on-premises environment to Azure. The data is shown in the following table.<br><img src=\"/assets/media/exam-media/04224/0010500001.png\" class=\"in-exam-image\"><br>What should you recommend using to migrate the data? To answer, drag the appropriate tools to the correct data sources. Each tool may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04224/0010500002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0010500003.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Data Migration Assistant -<br>The Data Migration Assistant (DMA) helps you upgrade to a modern data platform by detecting compatibility issues that can impact database functionality in your new version of SQL Server or Azure SQL Database. DMA recommends performance and reliability improvements for your target environment and allows you to move your schema, data, and uncontained objects from your source server to your target server.<br>Incorrect:<br>AzCopy is a command-line utility that you can use to copy blobs or files to or from a storage account.<br>Box 2: Azure Cosmos DB Data Migration Tool<br>Azure Cosmos DB Data Migration Tool can used to migrate a SQL Server Database table to Azure Cosmos.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/dma/dma-overview<br>https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-migrationchoices",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-02T05:47:00.000Z",
        "voteCount": 23,
        "content": "Correct.\n1. https://docs.microsoft.com/en-us/azure/azure-sql/migration-guides/database/sql-server-to-sql-database-overview?view=azuresql\nData migration services\n2. https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-migrationchoices"
      },
      {
        "date": "2023-09-17T08:39:00.000Z",
        "voteCount": 23,
        "content": "This question is on today's exam.\nThe exam is easier than AZ-104."
      },
      {
        "date": "2023-09-18T06:33:00.000Z",
        "voteCount": 3,
        "content": "My hero &lt;3"
      },
      {
        "date": "2024-02-23T04:56:00.000Z",
        "voteCount": 2,
        "content": "No he's not"
      },
      {
        "date": "2024-06-21T13:04:00.000Z",
        "voteCount": 2,
        "content": "From the SQL Server 2012 database to an Azure SQL database:\n\nData Migration Assistant: This tool helps assess, migrate, and validate the data from SQL Server to Azure SQL Database.\nFrom the table in the SQL Server 2014 database to an Azure Cosmos DB account that uses the SQL API:\n\nAzure Cosmos DB Data Migration Tool: This tool is specifically designed for migrating data to Azure Cosmos DB, supporting various source data formats."
      },
      {
        "date": "2024-06-21T13:05:00.000Z",
        "voteCount": 1,
        "content": "AzCopy: This tool is primarily used for copying data to and from Azure Storage (blobs, files, and tables) but not for migrating SQL Server databases.\n\nData Management Gateway: This tool is used for connecting on-premises data sources to Azure Data Factory for data integration and ETL processes. It's not specifically designed for SQL Server to Azure SQL Database or Cosmos DB migrations.\n\nData Migration Assistant: This tool is the appropriate choice for migrating SQL Server databases to Azure SQL Database.\n\nAzure Cosmos DB Data Migration Tool: This tool is suitable for migrating data from various sources to Azure Cosmos DB."
      },
      {
        "date": "2024-04-28T22:39:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2024-01-21T09:31:00.000Z",
        "voteCount": 4,
        "content": "appeared in Exam 01/2024"
      },
      {
        "date": "2023-09-29T18:08:00.000Z",
        "voteCount": 7,
        "content": "Got this on Sept. 29, 2023"
      },
      {
        "date": "2023-09-24T23:22:00.000Z",
        "voteCount": 6,
        "content": "I had question at 24th Sep 2023"
      },
      {
        "date": "2023-07-11T11:14:00.000Z",
        "voteCount": 8,
        "content": "Answers:\n1. From the SQL server 2012 database to Azure SQL Database: Data Migration Assistant. \nThe Data Migration Assistant (DMA) helps you upgrade to a modern data platform by detecting compatibility issues that can impact database functionality in your new version of SQL Server or Azure SQL Database. DMA recommends performance and reliability improvements for your target environment and allows you to move your schema, data, and uncontained objects from your source server to your target server.\n   \n2. From the table in the SQL server 2014 database to Azure Cosmos DB: Azure Cosmos DB Data Migration tool. \nThe Azure Cosmos DB Data Migration tool is an open-source solution that imports data to Azure Cosmos DB from a variety of sources, including SQL Server. For the SQL API, the tool supports import from JSON files, MongoDB, SQL Server, CSV files, Azure Table storage, Amazon DynamoDB, and other Azure Cosmos DB databases."
      },
      {
        "date": "2023-03-22T11:31:00.000Z",
        "voteCount": 1,
        "content": "If helps I found github project from Azure about cosmos db data migration tool, it seems to be active https://github.com/Azure/azure-documentdb-datamigrationtool"
      },
      {
        "date": "2023-01-28T02:28:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct\nBox 1: Data Migration Assistant\nBox 2: Azure Cosmos DB Data Migration Tool"
      },
      {
        "date": "2023-01-28T02:28:00.000Z",
        "voteCount": 4,
        "content": "Thanks all who have mentioned the exam dates"
      },
      {
        "date": "2023-01-09T11:34:00.000Z",
        "voteCount": 3,
        "content": "I think there is a mistake in this question - there is no Azure Cosmos DB for SQL API.The supported APIs are:\nAzure Cosmos DB for NoSQL\nAzure Cosmos DB for MongoDB\nAzure Cosmos DB for PostgreSQL\nAzure Cosmos DB for Cassandra\nAzure Cosmos DB for Gremlin\nAzure Cosmos DB for Table\nIf question means Azure Cosmos DB for NoSQL API, then if we look in the table in https://docs.microsoft.com/en-us/azure/cosmos-db/cosmosdb-migrationchoices the tool should be Azure Data Factory.\nAzure Cosmos DB Data Migration tool is not mentioned in related article. I found other article about this tool https://azure.microsoft.com/en-us/updates/documentdb-data-migration-tool/ but it is old and I think that tool is obsolete, as well as the question."
      },
      {
        "date": "2023-01-15T06:47:00.000Z",
        "voteCount": 1,
        "content": "PostgreSQL is one kind of SQL"
      },
      {
        "date": "2023-05-26T06:03:00.000Z",
        "voteCount": 1,
        "content": "It's mentioned here that SQL Servers are supported with the Desktop Migration Tool:\nhttps://learn.microsoft.com/en-gb/azure/cosmos-db/how-to-migrate-desktop-tool?tabs=azure-cli"
      },
      {
        "date": "2022-10-12T06:30:00.000Z",
        "voteCount": 6,
        "content": "appeared on 11th Oct 2022"
      },
      {
        "date": "2022-09-21T00:05:00.000Z",
        "voteCount": 1,
        "content": "The Data management gateway is a client agent that you must install in your on-premises environment to copy data between cloud and on-premises data stores"
      },
      {
        "date": "2022-09-05T00:09:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79393-exam-az-305-topic-2-question-13-discussion/",
    "body": "You store web access logs data in Azure Blob Storage.<br>You plan to generate monthly reports from the access logs.<br>You need to recommend an automated process to upload the data to Azure SQL Database every month.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMicrosoft SQL Server Migration Assistant (SSMA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData Migration Assistant (DMA)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzCopy",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 40,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T05:49:00.000Z",
        "voteCount": 17,
        "content": "Correct \nhttps://docs.microsoft.com/en-us/azure/data-factory/tutorial-copy-data-tool"
      },
      {
        "date": "2023-03-22T09:26:00.000Z",
        "voteCount": 12,
        "content": "D. Azure Data Factory\n\nYou should recommend using Azure Data Factory for this scenario. Azure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines. In this case, you can create a pipeline to automatically extract data from the Azure Blob Storage, transform the data if needed, and load it into the Azure SQL Database on a monthly basis. This will help you generate the required monthly reports from the access logs."
      },
      {
        "date": "2024-04-28T22:50:00.000Z",
        "voteCount": 1,
        "content": "Given answer D is correct\nAzure Data Factory is a cloud-based data integration service that allows you to create, schedule, and manage data pipelines. In this case, you can create a pipeline to automatically extract data from the Azure Blob Storage, transform the data if needed, and load it into the Azure SQL Database on a monthly basis. This will help you generate the required monthly reports from the access logs. \nIntegrate all your data with Azure Data Factory, a fully managed, serverless data integration service. Visually integrate data sources with more than 90 built-in, maintenance-free connectors at no added cost. Easily construct ETL (extract, transform, and load) and ELT (extract, load, and transform) processes code-free in an intuitive environment or write your own code. Then deliver integrated data to Azure Synapse Analytics to unlock business insights."
      },
      {
        "date": "2023-03-22T11:30:00.000Z",
        "voteCount": 2,
        "content": "If helps I found github project from Azure about cosmos db data migration tool, it seems to be active https://github.com/Azure/azure-documentdb-datamigrationtool"
      },
      {
        "date": "2023-02-18T20:44:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/introduction\nBig data requires a service that can orchestrate and operationalize processes to refine these enormous stores of raw data into actionable business insights. Azure Data Factory is a managed cloud service that's built for these complex hybrid extract-transform-load (ETL), extract-load-transform (ELT), and data integration projects."
      },
      {
        "date": "2023-02-10T01:01:00.000Z",
        "voteCount": 1,
        "content": "None of the answers are an automated process..."
      },
      {
        "date": "2023-02-10T01:07:00.000Z",
        "voteCount": 1,
        "content": "my bad, it says \"include in the recommendation\""
      },
      {
        "date": "2023-02-01T22:10:00.000Z",
        "voteCount": 1,
        "content": "D. Azure Data Factory"
      },
      {
        "date": "2023-01-28T02:31:00.000Z",
        "voteCount": 2,
        "content": "D. Azure Data Factory"
      },
      {
        "date": "2022-09-12T03:21:00.000Z",
        "voteCount": 4,
        "content": "Given is correct"
      },
      {
        "date": "2022-09-05T00:11:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/78601-exam-az-305-topic-2-question-14-discussion/",
    "body": "You have an Azure subscription.<br>Your on-premises network contains a file server named Server1. Server1 stores 5 \u05c0\u00a2\u05c0\u2019 of company files that are accessed rarely.<br>You plan to copy the files to Azure Storage.<br>You need to implement a storage solution for the files that meets the following requirements:<br>\u2711 The files must be available within 24 hours of being requested.<br>\u2711 Storage costs must be minimized.<br>Which two possible storage solutions achieve this goal? Each correct answer presents a complete solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure Blob Storage account that is configured for the Cool default access tier. Create a blob container, copy the files to the blob container, and set each file to the Archive access tier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a general-purpose v1 storage account. Create a blob container and copy the files to the blob container.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a general-purpose v2 storage account that is configured for the Cool default access tier. Create a file share in the storage account and copy the files to the file share.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a general-purpose v2 storage account that is configured for the Hot default access tier. Create a blob container, copy the files to the blob container, and set each file to the Archive access tier.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a general-purpose v1 storage account. Create a fie share in the storage account and copy the files to the file share."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 62,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-01T00:02:00.000Z",
        "voteCount": 22,
        "content": "I believe the correct answers are A and D, since the archive tier is the cheapest for storing data.\nIn addition, a maximum of 15 hours may be required to rehydrate the data from an archive tier; the requirements are met."
      },
      {
        "date": "2022-09-02T06:54:00.000Z",
        "voteCount": 10,
        "content": "https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\nWhile a blob is in the Archive tier, it can't be read or modified. To read or download a blob in the Archive tier, you must first rehydrate it to an online tier, either Hot or Cool. Data in the Archive tier can take up to 15 hours to rehydrate, depending on the priority you specify for the rehydration operation. For more information about blob rehydration, see Overview of blob rehydration from the Archive tier."
      },
      {
        "date": "2024-08-09T04:27:00.000Z",
        "voteCount": 2,
        "content": "This question appeared in the exam in August 2024 and I gave this same answer. I scored 870"
      },
      {
        "date": "2024-02-05T01:37:00.000Z",
        "voteCount": 6,
        "content": "Got this on exam Feb 4, 2024"
      },
      {
        "date": "2023-10-24T20:33:00.000Z",
        "voteCount": 2,
        "content": "I beleive A and C is correct answer"
      },
      {
        "date": "2024-03-01T23:17:00.000Z",
        "voteCount": 4,
        "content": "A is fine, but the tricky part for C is that the second sentence mentions that it is a File storage, not Blob storage.\nAs lifecycle management is not applicable for Files, you cannot store the data in cheaper (eg. Archival) tiers, meaning that your overall cost will be higher, thus D becomes a better choice."
      },
      {
        "date": "2023-07-11T11:21:00.000Z",
        "voteCount": 7,
        "content": "A &amp; D\n\nThe available access tiers include:\n\n- Hot: Optimized for storing data that is accessed frequently.\n- Cool: Optimized for storing data that is infrequently accessed and stored for at least 30 days.\n- Archive: Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements (on the order of hours).\n\nSince the files are accessed rarely and you need to minimize storage costs, the Archive tier is appropriate. Both A and D suggest setting the files to the Archive access tier.\n\nPlease note that Archive tier data is offline and it takes time to rehydrate data to an online tier if/when access is needed, but it satisfies your requirement of the files being available within 24 hours of being requested. In addition, creating an Azure Blob Storage or general-purpose v2 storage account allows you to utilize these access tiers, as they are not available in the general-purpose v1 accounts."
      },
      {
        "date": "2023-04-30T08:06:00.000Z",
        "voteCount": 2,
        "content": "A &amp; D are correct"
      },
      {
        "date": "2023-03-18T08:30:00.000Z",
        "voteCount": 9,
        "content": "B and E cannot have access tier because are v1 storage accounts\nC is a File share, that cannot have access tier\nThen: A, D"
      },
      {
        "date": "2023-04-25T11:41:00.000Z",
        "voteCount": 1,
        "content": "Thanks"
      },
      {
        "date": "2023-02-17T23:38:00.000Z",
        "voteCount": 5,
        "content": "AD is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\nArchive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the archive tier should be stored for a minimum of 180 days."
      },
      {
        "date": "2023-02-11T23:00:00.000Z",
        "voteCount": 3,
        "content": "What's the size in the question?  All I see is \"5 \u05c0\u00a2\u05c0\u2019\"!!\n\nWhat's the min blob size?  Like if I store a 1Kb file, what size does it end up in cost?"
      },
      {
        "date": "2023-01-28T02:41:00.000Z",
        "voteCount": 3,
        "content": "Keyword is archive tier for cost minimization"
      },
      {
        "date": "2023-01-18T01:12:00.000Z",
        "voteCount": 1,
        "content": "Archiving tier is a must in this scenario for cost optimization"
      },
      {
        "date": "2023-01-05T13:12:00.000Z",
        "voteCount": 3,
        "content": "A and C"
      },
      {
        "date": "2022-09-19T13:12:00.000Z",
        "voteCount": 2,
        "content": "Archive tier rehydration time is a claimed 15 hours. This meets their needs at the lowest cost.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview"
      },
      {
        "date": "2022-09-15T05:20:00.000Z",
        "voteCount": 1,
        "content": "What is meant by 'set each file to the Archive access tier' in answer A and D? It says in A the storage account is Cool and in D it is Hot. You can only set one access tier, no? Why do they refer at the end to Archive?"
      },
      {
        "date": "2022-09-20T20:50:00.000Z",
        "voteCount": 5,
        "content": "You can only create the storage account as hot or cool.\nOnce you get them there, you're sending the files to archive.\n\nIn this case, it doesn't really matter which tier you create the account as... the end result is the same."
      },
      {
        "date": "2022-09-09T10:21:00.000Z",
        "voteCount": 3,
        "content": "A and D is correct, C is not correct, AFAIK only blobs can be changed to archive access tier."
      },
      {
        "date": "2022-09-06T06:08:00.000Z",
        "voteCount": 4,
        "content": "only A and C allow to choose a cold tier which is the correct one for this scenario"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79418-exam-az-305-topic-2-question-15-discussion/",
    "body": "You have an app named App1 that uses two on-premises Microsoft SQL Server databases named DB1 and DB2.<br>You plan to migrate DB1 and DB2 to Azure<br>You need to recommend an Azure solution to host DB1 and DB2. The solution must meet the following requirements:<br>\u2711 Support server-side transactions across DB1 and DB2.<br>\u2711 Minimize administrative effort to update the solution.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttwo Azure SQL databases in an elastic pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttwo databases on the same Azure SQL managed instance\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttwo databases on the same SQL Server instance on an Azure virtual machine",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttwo Azure SQL databases on different Azure SQL Database servers"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 54,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T06:56:00.000Z",
        "voteCount": 21,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/database/elastic-transactions-overview?view=azuresql\nA server-side distributed transactions using Transact-SQL are available only for Azure SQL Managed Instance. Distributed transaction can be executed only between Managed Instances that belong to the same Server trust group. In this scenario, Managed Instances need to use linked server to reference each other."
      },
      {
        "date": "2023-03-22T09:38:00.000Z",
        "voteCount": 13,
        "content": "B. two databases on the same Azure SQL managed instance\n\nAn Azure SQL Managed Instance is a fully managed SQL Server Database Engine hosted in Azure that provides most of the SQL Server capabilities. It supports features like cross-database queries and transactions, which is crucial for your requirement of supporting server-side transactions across DB1 and DB2. Additionally, since it's a fully managed solution, it minimizes the administrative effort needed to update and maintain the system."
      },
      {
        "date": "2024-06-21T13:56:00.000Z",
        "voteCount": 2,
        "content": "B. Two databases on the same Azure SQL managed instance\n\nReasoning:\nSupport server-side transactions: Azure SQL Managed Instance supports distributed transactions across multiple databases, meeting the requirement for server-side transactions.\nMinimize administrative effort: Managed instances provide a managed environment with automatic patching, backups, and high availability, reducing the administrative effort compared to managing SQL Server on virtual machines or separate databases on different servers."
      },
      {
        "date": "2024-06-21T13:58:00.000Z",
        "voteCount": 3,
        "content": "why other options are not suitable:\n\nA. Two Azure SQL databases in an elastic pool:\n\nReason: Elastic pools are designed for cost efficiency and managing multiple databases with varying resource demands, but they do not support distributed transactions across databases.\nC. Two databases on the same SQL Server instance on an Azure virtual machine:\n\nReason: While this setup supports distributed transactions, it requires significant administrative effort to manage the virtual machine, including patching, backups, and high availability.\nD. Two Azure SQL databases on different Azure SQL Database servers:\n\nReason: This setup does not support distributed transactions across different servers and increases administrative complexity."
      },
      {
        "date": "2024-04-28T22:54:00.000Z",
        "voteCount": 1,
        "content": "The Given answer B is correct - \nThe keyword is server-side transaction execution across multiple dbs. it's one of the key features of SQL managed instance.\nHowever Client-side transaction execution is feature of both SQL MI and AZ SQL DB"
      },
      {
        "date": "2023-02-17T19:58:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-transactions-overview?view=azuresql#transact-sql-development-experience\nA server-side distributed transactions using Transact-SQL are available only for Azure SQL Managed Instance. Distributed transaction can be executed only between instances that belong to the same Server trust group. In this scenario, managed instances need to use linked server to reference each other."
      },
      {
        "date": "2023-02-12T02:21:00.000Z",
        "voteCount": 1,
        "content": "B is the best answer no doubt, but D is also a correct answer according to the doc.  See limitations 1st point!  This doc is missing a  lot of things..."
      },
      {
        "date": "2023-01-28T02:43:00.000Z",
        "voteCount": 1,
        "content": "B. two databases on the same Azure SQL managed instance"
      },
      {
        "date": "2023-01-18T01:14:00.000Z",
        "voteCount": 2,
        "content": "The thing is server-side transaction execution across multiple dbs. it's one of the key features of SQL managed instance."
      },
      {
        "date": "2022-12-30T03:00:00.000Z",
        "voteCount": 7,
        "content": "12/29/2022 -&gt; This article describes using elastic database transactions which allow you to run distributed transactions across cloud databases for Azure SQL Database and Azure SQL Managed Instance."
      },
      {
        "date": "2023-01-28T02:43:00.000Z",
        "voteCount": 1,
        "content": "Thanks for mentioning the date"
      },
      {
        "date": "2022-10-10T22:47:00.000Z",
        "voteCount": 4,
        "content": "Azure SQL DB does NOT support server-side transaction, only client-side. \nRef: https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-transactions-overview?view=azuresql#common-scenarios"
      },
      {
        "date": "2022-09-19T13:17:00.000Z",
        "voteCount": 2,
        "content": "Elastic query for Azure SQL Databases is currently in preview mode, which would allow this. \nFor now, SQL MI is the right answer though."
      },
      {
        "date": "2022-10-10T22:47:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL DB does NOT support server-side transaction, only client-side. \nRef: https://learn.microsoft.com/en-us/azure/azure-sql/database/elastic-transactions-overview?view=azuresql#common-scenarios"
      },
      {
        "date": "2024-02-14T05:47:00.000Z",
        "voteCount": 1,
        "content": "You posted the docs, but did you actually read them? Seems not. Let's do it together: \n\n\"A server-side experience (code written in stored procedures or server-side scripts) using Transact-SQL is available for SQL Managed Instance only\" \n\nAnd then it continues telling you that you can either have transactions between DB or DB MIs like this: \n\n\"Running elastic database transactions between Azure SQL Database and Azure SQL Managed Instance is not supported. Elastic database transaction can only span across a set of databases in SQL Database or a set databases across managed instances.\""
      },
      {
        "date": "2022-09-05T00:19:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-09-04T18:19:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct.\nKeywords: instance to instance/minimal management"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79423-exam-az-305-topic-2-question-16-discussion/",
    "body": "You need to design a highly available Azure SQL database that meets the following requirements:<br>\u2711 Failover between replicas of the database must occur without any data loss.<br>\u2711 The database must remain available in the event of a zone outage.<br>\u2711 Costs must be minimized.<br>Which deployment option should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Hyperscale",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Premium\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Basic",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Managed Instance General Purpose"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 44,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-19T13:39:00.000Z",
        "voteCount": 21,
        "content": "I had a near-impossible time finding documentation for this, so I just went to my own portal and checked.  \n\nSQL Database options that support Geo Redundancy\n\nvCore model\n- General Purpose (at additional cost)\n- Business Critical\nDTU model\n- Premium\n\nSQL MI does not support geo-redundancy at all."
      },
      {
        "date": "2023-09-25T13:46:00.000Z",
        "voteCount": 14,
        "content": "Sept 25, 2023.  There are 2 errors in jellybiscuit's comment:\n\n1) the scenario calls for Zone redundancy (handle a zone outage).  It does not require Geo redundancy\n\n2) Managed Instances now allow Zone Redundant (ZRS), Geo Redundant (GRS), and Geo Zone Redundant (GZRS).  It depends on the Region, but the scenario does not include a Region.   https://learn.microsoft.com/en-us/azure/azure-sql/managed-instance/automated-backups-overview?view=azuresql\n\nSo both MI &amp; SQL DB Premium will work, but SQL DB premium is considerably less expensive.  I tried both in the Azure Pricing Calculator (Zone redudant &amp; different sizes).  SQL DB Premium is always far cheaper than Managed Instance.\nAzure Pricing Calculator:  https://azure.microsoft.com/en-us/pricing/calculator/"
      },
      {
        "date": "2022-09-02T07:10:00.000Z",
        "voteCount": 13,
        "content": "Answer is correct but explanation is wrong for C. You need General Purpose level as a minimum, not premium.\n\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla?view=azuresql&amp;tabs=azure-powershell\n\nNB: Zone-redundant configuration is not available in SQL Managed Instance. In SQL Database this feature is only available when the Gen5 hardware is selected."
      },
      {
        "date": "2024-02-15T08:55:00.000Z",
        "voteCount": 1,
        "content": "I am curious how can be compared the prices of DTU and vCore based solutions without describing any further needs? \n\nI created the below order based on https://azure.microsoft.com/en-us/pricing/details/azure-sql-database/single/\n\n-- Azure SQL Database Standard (DTU-based; Zone Redundancy not fully available)\n-- Azure SQL Database Hyperscale (vCore-based)\n-- Azure SQL Database General Purpose (vCore-based)\n-- Azure SQL Database Premium (DTU-based)\n-- Azure SQL Database Business Critical (vCore-based)\n\t&amp; Azure SQL Database Basic does not provide zone redundancy at all."
      },
      {
        "date": "2023-10-25T20:39:00.000Z",
        "voteCount": 1,
        "content": "I think A is correct answer to support automatic failover with no dataloss."
      },
      {
        "date": "2023-08-22T17:01:00.000Z",
        "voteCount": 2,
        "content": "why not Azure SQL Database basic?\nAlso, I don't find the option to specify basic or premium!!!"
      },
      {
        "date": "2023-09-25T12:58:00.000Z",
        "voteCount": 2,
        "content": "Azure SQL DB - Basic &amp; Standard only have LRS.  They do not have ZRS (which is required by the scenario).  The other Azure SQL DB service tiers have ZRS:  DTU (Premium) and vCore (General Purpose, Business Critical, and Hyperscale)\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla?view=azuresql&amp;tabs=azure-powershell"
      },
      {
        "date": "2023-12-28T09:12:00.000Z",
        "voteCount": 1,
        "content": "According to this article, basic and standard both support ZRS: \n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-dtu?view=azuresql"
      },
      {
        "date": "2024-01-18T06:30:00.000Z",
        "voteCount": 3,
        "content": "I think that's for backups only. According to this articale, ZRS is only supported on Azure SQL Premium: https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla?view=azuresql&amp;tabs=azure-powershell"
      },
      {
        "date": "2024-02-20T14:21:00.000Z",
        "voteCount": 1,
        "content": "Basic supports ZRS and GRS for its backup and not for the database itself."
      },
      {
        "date": "2023-04-30T08:24:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct\nAzure SQL Managed Instance also offers high availability and disaster recovery capabilities, but it does not support zone redundant configuration. Instead, it uses a different approach called instance failover groups to provide high availability across different regions. Instance failover groups enable you to create and manage groups of managed instances that fail over together during a regional outage, allowing you to maintain availability of your database workloads."
      },
      {
        "date": "2023-04-03T06:29:00.000Z",
        "voteCount": 4,
        "content": "this was in the exam on 01/04/2023"
      },
      {
        "date": "2023-03-22T09:40:00.000Z",
        "voteCount": 4,
        "content": "B. Azure SQL Database Premium\n\nTo meet the requirements of a highly available Azure SQL database with no data loss during failover and availability during a zone outage, you should use Azure SQL Database Premium. The Premium tier provides built-in support for active geo-replication, which allows you to create readable secondary replicas in different regions, ensuring the database remains available in the event of a zone outage. Additionally, the Premium tier offers better performance and more resources compared to the Basic and General Purpose tiers, while Hyperscale, although highly scalable, can be more costly than the Premium tier."
      },
      {
        "date": "2023-02-17T19:56:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/sql-database-paas-overview?view=azuresql#service-tiers\nThe Premium service tier is designed for OLTP applications with high transaction rates and low latency I/O requirements. It offers the highest resilience to failures by using several isolated replicas."
      },
      {
        "date": "2023-01-28T02:44:00.000Z",
        "voteCount": 2,
        "content": "B. Azure SQL Database Premium"
      },
      {
        "date": "2023-01-27T13:50:00.000Z",
        "voteCount": 1,
        "content": "Zone-redundant configuration is currently in preview for SQL Managed Instance, and only available for the Business Critical service tier. https://learn.microsoft.com/en-us/azure/azure-sql/database/high-availability-sla?view=azuresql&amp;tabs=azure-powershell"
      },
      {
        "date": "2022-09-04T18:29:00.000Z",
        "voteCount": 3,
        "content": "B is correct\nIf D is \"Azure SQL DATABASE General Purpose\", then D is correct.\nAzure SQL database general purpose support Zone but Azure MI general purpose doesn't support zone redundancy."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/78936-exam-az-305-topic-2-question-17-discussion/",
    "body": "HOTSPOT -<br>You are planning an Azure Storage solution for sensitive data. The data will be accessed daily. The dataset is less than 10 GB.<br>You need to recommend a storage solution that meets the following requirements:<br>\u2711 All the data written to storage must be retained for five years.<br>\u2711 Once the data is written, the data can only be read. Modifications and deletion must be prevented.<br>\u2711 After five years, the data can be deleted, but never modified.<br>\u2711 Data access charges must be minimized.<br>What should you recommend? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0011000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0011000002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: General purpose v2 with Hot access tier for blobs<br>Note:<br>* All the data written to storage must be retained for five years.<br>* Data access charges must be minimized<br>Hot tier has higher storage costs, but lower access and transaction costs.<br>Incorrect:<br>Not Archive: Lowest storage costs, but highest access, and transaction costs.<br>Not Cool: Lower storage costs, but higher access and transaction costs.<br>Box 2: Storage account resource lock<br>As an administrator, you can lock a subscription, resource group, or resource to prevent other users in your organization from accidentally deleting or modifying critical resources. The lock overrides any permissions the user might have.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-01T00:15:00.000Z",
        "voteCount": 138,
        "content": "gpv2 hot tier, container access policy to configure a time-based retention policy for immutable storage.\nStorage account resource lock does not prevent data editing or deletion, but only the storage account deletion."
      },
      {
        "date": "2022-09-01T05:22:00.000Z",
        "voteCount": 7,
        "content": "agree 100%"
      },
      {
        "date": "2022-10-11T08:44:00.000Z",
        "voteCount": 1,
        "content": "yes you set the resources lock as read-only and delete prevention but can to for data, that is only for resources change not for in the data."
      },
      {
        "date": "2022-10-06T06:23:00.000Z",
        "voteCount": 4,
        "content": "You can set the storage resource lock to CannotDelete and ReadOnly isnt?\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/management/lock-resources?tabs=json"
      },
      {
        "date": "2022-10-11T08:43:00.000Z",
        "voteCount": 11,
        "content": "yes you set the resources lock as read-only and delete prevention but can to for data, that is only for resources change not for in the data."
      },
      {
        "date": "2022-09-02T07:23:00.000Z",
        "voteCount": 46,
        "content": "Answer is GPv2 HOT to have frequent access :\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\n\nAnswer is container access (immutable) policy at least at the container scope.\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview"
      },
      {
        "date": "2024-04-28T23:04:00.000Z",
        "voteCount": 2,
        "content": "i would go for gpv2 hot tier and container access policy"
      },
      {
        "date": "2024-04-02T22:32:00.000Z",
        "voteCount": 1,
        "content": "1 Hot tier\n2 should be access policy.\nPS Locking a storage account does not protect containers or blobs within that account from being deleted or overwritten!!!"
      },
      {
        "date": "2024-04-01T19:37:00.000Z",
        "voteCount": 1,
        "content": "I think the right choice for second question is\n\"Container Access Level\"\nwe can adjust access level to read only"
      },
      {
        "date": "2024-01-21T20:30:00.000Z",
        "voteCount": 1,
        "content": "Immutability policies can be scoped to a blob version or to a container. How an object behaves under an immutability policy depends on the scope of the policy. For more information about policy scope for each type of immutability policy, see the following sections:\n\nTime-based retention policy scope\nLegal hold scope\nDepending on the scope, you can configure both a time-based retention policy and a legal hold for a resource (container or blob version)."
      },
      {
        "date": "2023-11-20T03:33:00.000Z",
        "voteCount": 5,
        "content": "Got this on Nov. 17, 2023"
      },
      {
        "date": "2023-10-29T08:16:00.000Z",
        "voteCount": 3,
        "content": "1. \"Hot tier\". Lower access transaction costs, meats requirement \"Data access charges must be minimized\"\n\n2. \"Container access policy\" seems to be the best one. I still struggle to find it in the documentation (only found immutable storage references)"
      },
      {
        "date": "2024-02-28T05:49:00.000Z",
        "voteCount": 3,
        "content": "1. Wrong, the cheapest tier is Archive, then Cool, then Hot. The reason why we need Hot access is because this data has to be accessed a lot on a daily basis."
      },
      {
        "date": "2023-10-22T07:57:00.000Z",
        "voteCount": 9,
        "content": "This question appeared on my Exam today 10/22/2023\nTotal of 48 questions"
      },
      {
        "date": "2023-11-17T02:38:00.000Z",
        "voteCount": 1,
        "content": "answer?"
      },
      {
        "date": "2023-10-17T07:02:00.000Z",
        "voteCount": 1,
        "content": "General purpose v2 with Hot access tier and Storage resource lock."
      },
      {
        "date": "2023-08-15T04:02:00.000Z",
        "voteCount": 3,
        "content": "its general puprose v2 with hot tier\ncontainer access policy"
      },
      {
        "date": "2023-07-13T08:48:00.000Z",
        "voteCount": 14,
        "content": "1. Storage Account type: c. GP v2 Hot.\n\nConsidering the data will be accessed daily, the Hot access tier is the most cost-effective for storing frequently accessed data.\n\n2. Configuration to prevent the modification and deletions: Container access policy.\n\nThe Container access policy is indeed the place to configure Azure's Immutable Blob Storage to ensure data is retained without modifications or deletions for a specified amount of time, which suits your needs. The Azure Blob Storage's Immutable Blob Storage feature provides a WORM (Write Once, Read Many) capability which aligns with your requirements perfectly."
      },
      {
        "date": "2023-05-26T06:47:00.000Z",
        "voteCount": 8,
        "content": "Moderator/Admins: could you please update the answer. We have a lot of consense here that the answers are General Purpose v2 Blobs + hot tier AND Container Access Policy are needed to get the desired outcome."
      },
      {
        "date": "2023-04-30T08:34:00.000Z",
        "voteCount": 6,
        "content": "General Purpose V2 Hot tier\nContainer access policy\n\nI would recommend using Azure hot Blob Storage with a WORM (Write Once Read Many) policy. WORM policies prevent data from being modified or deleted after it has been written, and they can be applied to individual blobs or entire containers."
      },
      {
        "date": "2023-04-30T08:35:00.000Z",
        "voteCount": 2,
        "content": "To implement this solution, you can follow these steps:\n\nCreate an Azure Blob Storage account and enable the WORM feature. This can be done through the Azure Portal or via Azure CLI or PowerShell.\n\nCreate a new blob container for your sensitive data.\n\nSet the WORM policy for the container to enforce write-once-read-many access for all blobs in the container.\n\nUpload your sensitive data to the blob container.\n\nConfigure a retention period of five years for the data in the container."
      },
      {
        "date": "2023-03-18T08:52:00.000Z",
        "voteCount": 7,
        "content": "General Purpose V2 Hot tier\nContainer access policy"
      },
      {
        "date": "2023-03-02T09:38:00.000Z",
        "voteCount": 4,
        "content": "Second is Container Access Policy"
      },
      {
        "date": "2023-02-21T09:13:00.000Z",
        "voteCount": 3,
        "content": "It should be Hot Tier with Container Access Policy. Storage Account Resource Lock does not prevent the data from being modified/deleted inside the container.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-protection-overview#overview-of-data-protection-options"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80098-exam-az-305-topic-2-question-18-discussion/",
    "body": "HOTSPOT -<br>You are designing a data storage solution to support reporting.<br>The solution will ingest high volumes of data in the JSON format by using Azure Event Hubs. As the data arrives, Event Hubs will write the data to storage. The solution must meet the following requirements:<br>\u2711 Organize data in directories by date and time.<br>\u2711 Allow stored data to be queried directly, transformed into summarized tables, and then stored in a data warehouse.<br>\u2711 Ensure that the data warehouse can store 50 TB of relational data and support between 200 and 300 concurrent read operations.<br>Which service should you recommend for each type of data store? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04224/0011200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04224/0011200002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure Data Lake Storage Gen2<br>Azure Data Explorer integrates with Azure Blob Storage and Azure Data Lake Storage (Gen1 and Gen2), providing fast, cached, and indexed access to data stored in external storage. You can analyze and query data without prior ingestion into Azure Data Explorer. You can also query across ingested and uningested external data simultaneously.<br>Azure Data Lake Storage is optimized storage for big data analytics workloads.<br>Use cases: Batch, interactive, streaming analytics and machine learning data such as log files, IoT data, click streams, large datasets<br>Box 2: Azure SQL Database Hyperscale<br>Azure SQL Database Hyperscale is optimized for OLTP and high throughput analytics workloads with storage up to 100TB.<br>A Hyperscale database supports up to 100 TB of data and provides high throughput and performance, as well as rapid scaling to adapt to the workload requirements. Connectivity, query processing, database engine features, etc. work like any other database in Azure SQL Database.<br>Hyperscale is a multi-tiered architecture with caching at multiple levels. Effective IOPS will depend on the workload.<br>Compare to:<br>General purpose: 500 IOPS per vCore with 7,000 maximum IOPS<br>Business critical: 5,000 IOPS with 200,000 maximum IOPS<br>Incorrect:<br>* Azure Synapse Analytics Dedicated SQL pool.<br><br>Max database size: 240 TB -<br>A maximum of 128 concurrent queries will execute and remaining queries will be queued.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-explorer/data-lake-query-data https://docs.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-04T22:05:00.000Z",
        "voteCount": 43,
        "content": "Azure Synapse Analytics SQL pool only support 128 concurrent queries:\n\"A maximum of 128 concurrent queries will execute and remaining queries will be queued\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-service-capacity-limits\nAzure Sql hyperscale have read replica... and supports up to 100TB data size.\nSo I think the correct answer should be Hyperscale"
      },
      {
        "date": "2023-07-13T08:55:00.000Z",
        "voteCount": 29,
        "content": "1. Data store for the ingestion data: b. Azure Data Lake Storage Gen2. \n\nAzure Data Lake Storage Gen2 is designed for big data analytics, it combines the power of a high-performance file system with massive scale and economy to help you speed up your big data analytics. It allows the data to be organized in directories by date and time. \n\n2. Data store for the data warehouse: c. Azure SQL Database Hyperscale. \n\nAzure SQL Database Hyperscale is a highly scalable service tier that is designed to provide high performance, and supports up to 100 TB of data. The Hyperscale service tier in Azure SQL Database is the newest service tier in the vCore-based purchasing model. This service tier is a highly scalable storage and compute performance tier that leverages the Azure architecture to scale out the storage and compute resources for an Azure SQL Database substantially beyond the limits available for the General Purpose and Business Critical service tiers."
      },
      {
        "date": "2024-09-09T06:37:00.000Z",
        "voteCount": 1,
        "content": "Data store for the ingested data:    Azure Data Lake Storage Gen2\nData store for the data warehouse: Azure Synapse Analytics dedicated SQL pools"
      },
      {
        "date": "2024-06-21T14:17:00.000Z",
        "voteCount": 1,
        "content": "Data store for the ingested data:\n\nAzure Data Lake Storage Gen2: This service is optimized for big data analytics and can organize data by date and time. It also supports hierarchical namespace which is useful for efficient data organization.\nData store for the data warehouse:\n\nAzure Synapse Analytics dedicated SQL pools: This service is designed to handle large-scale data warehousing, can store 50 TB of relational data, and supports high concurrency for read operations, making it suitable for your needs."
      },
      {
        "date": "2024-09-03T09:41:00.000Z",
        "voteCount": 1,
        "content": "The question's number of concurrent queries is between 200-300. \nAzure Synapse Analytics  supports up to 128 concurrent queries, which is well below the question requirement. Then Azure Synapse is not the best option here."
      },
      {
        "date": "2024-06-21T14:19:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL Database Hyperscale is best suited for:\n\nLarge transactional databases.\nWorkloads requiring high performance and scalability for OLTP (Online Transaction Processing).\nAzure Synapse Analytics Dedicated SQL Pools is best suited for:\n\nLarge-scale data warehousing.\nComplex analytical queries and reporting.\nScenarios requiring high concurrency for read operations."
      },
      {
        "date": "2024-05-04T01:01:00.000Z",
        "voteCount": 1,
        "content": "For the data store for ingested data, Azure Data Lake Storage Gen2 is recommended. It is designed to handle high volumes of data and allows you to organize data in directories by date and time. It also supports direct querying of stored data. \n\nFor the data warehouse, Azure Synapse Analytics Dedicated SQL Pools is recommended. It is designed to handle large volumes of relational data (up to petabytes) and supports a high number of concurrent read operations. It also allows for data transformation into summarized tables. Azure Synapse Analytics integrates seamlessly with Azure Data Lake Storage Gen2, providing an end-to-end data solution."
      },
      {
        "date": "2024-09-03T09:42:00.000Z",
        "voteCount": 1,
        "content": "The question's number of concurrent queries is between 200-300. \nAzure Synapse Analytics  supports up to 128 concurrent queries, which is well below the question requirement. Then Azure Synapse is not the best option here."
      },
      {
        "date": "2024-03-19T20:49:00.000Z",
        "voteCount": 1,
        "content": "Final Answer:\n1. Azure Data Lake Storage Gen2.\n2. Azure SQL Database Hyperscale."
      },
      {
        "date": "2024-01-04T03:40:00.000Z",
        "voteCount": 15,
        "content": "Was on my Exam Today - 4th Jan 2024"
      },
      {
        "date": "2023-11-22T16:40:00.000Z",
        "voteCount": 1,
        "content": "For the ingested data, I recommend using zure Data Lake Storage Gen2. \nIt is a highly scalable and cost-effective data lake solution for big data analytics. \n\nFor the data warehouse, I recommend using Azure Synapse Analytics (formerly SQL Data Warehouse). It is an analytics service that brings together enterprise data warehousing and Big Data analytics. It gives you the freedom to query data on your terms, using either serverless or provisioned resources at scale. It can store 50 TB of relational data and support between 200 and 300 concurrent read operations."
      },
      {
        "date": "2023-08-08T15:41:00.000Z",
        "voteCount": 1,
        "content": "What would be right answer ?"
      },
      {
        "date": "2023-04-05T10:11:00.000Z",
        "voteCount": 3,
        "content": "If You dont know what to choose, choose cheapest one or \"more cost safe\" so IMO, Azure SQL Database Hyperscale is the answer even if Synapse meets requirements."
      },
      {
        "date": "2023-03-22T09:52:00.000Z",
        "voteCount": 2,
        "content": "1. Data store for the ingested data: B. Azure Data Lake Storage Gen2\nAzure Data Lake Storage Gen2 is designed for big data analytics workloads and supports organizing data in directories by date and time, as well as hierarchical namespace. It also allows stored data to be queried directly and is well-integrated with Azure Event Hubs.\n\n2. Data store for the data warehouse: C. Azure SQL Database Hyperscale is an alternative option for the data store for the data warehouse. It is a highly scalable service tier for single databases within Azure SQL Database that can auto-scale up to 100 TB. It supports a large number of concurrent connections and offers rapid scaling capabilities."
      },
      {
        "date": "2023-03-19T09:32:00.000Z",
        "voteCount": 3,
        "content": "The second is hyperscale: https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale-frequently-asked-questions-faq?view=azuresql#how-can-i-choose-between-azure-synapse-analytics-and-azure-sql-database-hyperscale- based on Microsoft docs. For this type of scenarios, Hyperscale works"
      },
      {
        "date": "2023-03-19T09:26:00.000Z",
        "voteCount": 3,
        "content": "Hyperscale is OLTP not OLAP (Data warehouse). Synapse is a DW"
      },
      {
        "date": "2023-02-26T03:55:00.000Z",
        "voteCount": 4,
        "content": "1. Azure Data Lake Storage Gen2\n2. Azure SQL DB Hyperscale\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction\nAzure Data Lake Storage Gen2 is a set of capabilities dedicated to big data analytics, built on Azure Blob Storage.\n\nData Lake Storage Gen2 converges the capabilities of Azure Data Lake Storage Gen1 with Azure Blob Storage. For example, Data Lake Storage Gen2 provides file system semantics, file-level security, and scale. Because these capabilities are built on Blob storage, you'll also get low-cost, tiered storage, with high availability/disaster recovery capabilities.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql#what-are-the-hyperscale-capabilities\nThe Hyperscale service tier in Azure SQL Database provides the following additional capabilities:\n- Support for up to 100 TB of database size."
      },
      {
        "date": "2023-02-23T12:30:00.000Z",
        "voteCount": 2,
        "content": "I think the answer should be Azure Synapse Analytics SQLPool \nbcz: once data is stored in ADLS in directories, data needs to be queried directly and transformed and stored in tables.\nSynapse has that capability. ???"
      },
      {
        "date": "2023-02-11T23:05:00.000Z",
        "voteCount": 1,
        "content": "synapse vs hyperscale which is the better answer?"
      },
      {
        "date": "2023-02-11T01:38:00.000Z",
        "voteCount": 5,
        "content": "Box 1: Azure Data Lake Storage Gen2\nBox 2: Azure SQL Database Hyperscale"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/93991-exam-az-305-topic-2-question-19-discussion/",
    "body": "You have an app named App1 that uses an on-premises Microsoft SQL Server database named DB1.<br><br>You plan to migrate DB1 to an Azure SQL managed instance.<br><br>You need to enable customer managed Transparent Data Encryption (TDE) for the instance. The solution must maximize encryption strength.<br><br>Which type of encryption algorithm and key length should you use for the TDE protector?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRSA 3072\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAES 256",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRSA 4096",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRSA 2048"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-22T09:56:00.000Z",
        "voteCount": 25,
        "content": "A. RSA 3072\n\nRSA 3072 provides a higher level of encryption strength compared to RSA 2048. While RSA 4096 offers even stronger encryption, it is not supported by Azure SQL Database and Azure SQL Managed Instance for TDE protectors.\n\nBy choosing RSA 3072 for the TDE protector, you ensure strong encryption for your Azure SQL Managed Instance while complying with the platform's requirements. This will help protect sensitive data and maintain compliance with relevant security standards and regulations."
      },
      {
        "date": "2024-03-08T19:05:00.000Z",
        "voteCount": 2,
        "content": "Correct, Reference:\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#:~:text=TDE%20protector%20can%20only%20be%20an%20asymmetric%2C%20RSA%2C%20or%20RSA%20HSM%20key.%20The%20supported%20key%20lengths%20are%202048%20bits%20and%203072%20bits."
      },
      {
        "date": "2023-05-12T11:13:00.000Z",
        "voteCount": 9,
        "content": "The Answer is A and here is why...\nPer https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&amp;tabs=azure-portal, \n\nif the TDE uses the system managed key,  it uses a built in certificate for encryption, hence AES 256\nif the TDE uses a customer managed key, then it uses an asymmetric RSA key at 2048 or 3072\n\nAnd since the question says TDE is using the customer managed key... the answer is A Viola!"
      },
      {
        "date": "2024-07-26T02:07:00.000Z",
        "voteCount": 1,
        "content": "Actualmente RSA 4096 ya es compatible con Azure SQL Database"
      },
      {
        "date": "2024-01-04T03:41:00.000Z",
        "voteCount": 9,
        "content": "Was on my exam today - 4th Jan 2024"
      },
      {
        "date": "2023-11-22T04:27:00.000Z",
        "voteCount": 6,
        "content": "it was a exam Question"
      },
      {
        "date": "2023-09-15T09:58:00.000Z",
        "voteCount": 2,
        "content": "RSA 3072, because is custom managed"
      },
      {
        "date": "2023-05-29T01:42:00.000Z",
        "voteCount": 3,
        "content": "There are a lot of confusing elements in this question.\nAt first it mentions on-premise SQL Server, which would allow AES or RSA ...\nHowever, the system is to be migrated over to Azure.\nAnd here the requirements for customer managed TDE are pretty clear and are listed here:\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\n\nAES can be enabled as an additional Infrastructure encryption to have two layers, but that was not the question here."
      },
      {
        "date": "2023-05-12T10:48:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?source=recommendations&amp;view=azuresql#requirements-for-configuring-tde-protector\n\nA. 3072"
      },
      {
        "date": "2023-02-19T00:10:00.000Z",
        "voteCount": 5,
        "content": "A is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\nTDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes."
      },
      {
        "date": "2023-02-02T03:14:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector"
      },
      {
        "date": "2023-02-01T22:35:00.000Z",
        "voteCount": 2,
        "content": "Answer A because Azure SQL Database and Azure Synapse Analytics support RSA 3072-bit key length for customer managed TDE with Bring Your Own Key (BYOK) configurations"
      },
      {
        "date": "2023-01-31T20:28:00.000Z",
        "voteCount": 4,
        "content": "A. RSA 3072\n( TDE protector can only be an asymmetric, RSA, or RSA HSM key. The supported key lengths are 2048 bytes and 3072 bytes.)"
      },
      {
        "date": "2023-01-28T04:41:00.000Z",
        "voteCount": 4,
        "content": "A. RSA 3072"
      },
      {
        "date": "2023-01-28T04:41:00.000Z",
        "voteCount": 2,
        "content": "A. RSA 3072"
      },
      {
        "date": "2023-01-27T07:17:00.000Z",
        "voteCount": 3,
        "content": "The answer is AES 256\n\nTransparent Data Encryption (TDE) in Azure SQL Managed Instance uses the Advanced Encryption Standard (AES) algorithm to encrypt the data stored in the database and its backups. The AES algorithm is a symmetric encryption algorithm and it supports key lengths of 128, 192, and 256 bits. Among these, AES 256 provides the highest encryption strength and is considered the most secure option for TDE. Therefore, you should use AES 256 for the TDE protector.\n\nCheck MS docs: https://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?view=azuresql&amp;tabs=azure-portal"
      },
      {
        "date": "2023-02-07T12:28:00.000Z",
        "voteCount": 5,
        "content": "Per following contents in \n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector\n\nTo provide Azure SQL customers with two layers of encryption of data at rest, infrastructure encryption (using AES-256 encryption algorithm) with platform managed keys is being rolled out. This provides an addition layer of encryption at rest along with TDE with customer-managed keys, which is already available.\n\nASE is platform managed key, this question is asking for customer managed keys, for now only RSA is qualified."
      },
      {
        "date": "2023-01-22T07:32:00.000Z",
        "voteCount": 1,
        "content": "Only RSA 3072 and RSA 2048 are supported for TDE protector\nmaximum encryption possible is RSA 3072\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#requirements-for-configuring-tde-protector"
      },
      {
        "date": "2023-01-27T07:21:00.000Z",
        "voteCount": 1,
        "content": "The information provided is not accurate. Transparent Data Encryption (TDE) in Azure SQL Managed Instance uses the Advanced Encryption Standard (AES) algorithm to encrypt the data stored in the database and its backups. AES algorithm is a symmetric encryption algorithm, it supports key lengths of 128, 192, and 256 bits. Among these, AES 256 provides the highest encryption strength and is considered the most secure option for TDE.\nRSA is not used for TDE. RSA is an asymmetric encryption algorithm, it is used in many different encryption scenarios, not just for TDE.\nTherefore, you should use AES 256 for the TDE protector."
      },
      {
        "date": "2023-01-09T11:05:00.000Z",
        "voteCount": 2,
        "content": "From what I can find, I agree with A, RSA 3072 maximum encryption.\nAES256 for built-in cert.\nAs per below URL, with SQL MI customer managed key \nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql\n\nIt's not RSA4096 since that's for storage encryption as per below;\nhttps://learn.microsoft.com/en-us/azure/storage/common/customer-managed-keys-overview\n&amp;\nhttps://learn.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key\n&amp;\nhttps://learn.microsoft.com/en-us/azure/virtual-machines/disk-encryption"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94045-exam-az-305-topic-2-question-20-discussion/",
    "body": "You are planning an Azure IoT Hub solution that will include 50,000 IoT devices.<br><br>Each device will stream data, including temperature, device ID, and time data. Approximately 50,000 records will be written every second. The data will be visualized in near real time.<br><br>You need to recommend a service to store and query the data.<br><br>Which two services can you recommend? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Grid",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB for NoSQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Time Series Insights\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-05T08:59:00.000Z",
        "voteCount": 20,
        "content": "Cleared the exam on 01/05/23 with 871 / 1000. Chose CD. There is a similar question on ET where the option was SQL DB API"
      },
      {
        "date": "2023-01-28T04:42:00.000Z",
        "voteCount": 1,
        "content": "Thanks for mentioning the exam date"
      },
      {
        "date": "2023-01-14T02:14:00.000Z",
        "voteCount": 17,
        "content": "A. Azure Table Storage -&gt; Throughput: scalability limit of 20,000 operations/s. -&gt; Not enough for this question\nB. Azure Event Grid -&gt; It is only a broker, not a storage solution\nTherefore, C and D are right\n\nRefs:\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/table/\nhttps://learn.microsoft.com/en-us/azure/event-grid/overview"
      },
      {
        "date": "2024-01-21T09:32:00.000Z",
        "voteCount": 6,
        "content": "appeared in Exam 01/2024"
      },
      {
        "date": "2023-10-13T17:58:00.000Z",
        "voteCount": 2,
        "content": "The Time Series Insights (TSI) service will no longer be supported after March 2025. Consider migrating existing TSI environments to alternative solutions (such as Azure Data Explorer) as soon as possible.\n\nAzure Data Explorer is a fast, fully managed data analytics service for real-time and time-series analysis on large volumes of data streams from business activities, human operations, applications, websites, Internet of Things (IoT) devices, and other sources.\n\nhttps://learn.microsoft.com/en-us/azure/time-series-insights/migration-to-adx"
      },
      {
        "date": "2023-04-12T07:05:00.000Z",
        "voteCount": 4,
        "content": "Same as Question #9"
      },
      {
        "date": "2023-03-22T10:06:00.000Z",
        "voteCount": 8,
        "content": "C. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nBoth Azure Cosmos DB and Azure Time Series Insights are suitable services for storing and querying the data in this scenario.\n\nC. Azure Cosmos DB for NoSQL is a globally distributed, multi-model database service that can handle large amounts of data with low-latency and high throughput. Its support for various consistency levels and partitioning strategies makes it suitable for handling IoT data at scale.\n\nD. Azure Time Series Insights is a fully managed, real-time analytics service specifically designed for time-series data generated by IoT devices. It provides storage, visualization, and advanced querying capabilities for time-series data, making it an ideal choice for handling data from a large number of IoT devices and visualizing it in near real-time."
      },
      {
        "date": "2023-03-22T10:01:00.000Z",
        "voteCount": 1,
        "content": "C. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nBoth Azure Cosmos DB and Azure Time Series Insights are suitable services for storing and querying the data in this scenario.\n\nC. Azure Cosmos DB for NoSQL is a globally distributed, multi-model database service that can handle large amounts of data with low-latency and high throughput. Its support for various consistency levels and partitioning strategies makes it suitable for handling IoT data at scale.\n\nD. Azure Time Series Insights is a fully managed, real-time analytics service specifically designed for time-series data generated by IoT devices. It provides storage, visualization, and advanced querying capabilities for time-series data, making it an ideal choice for handling data from a large number of IoT devices and visualizing it in near real-time."
      },
      {
        "date": "2023-02-22T09:09:00.000Z",
        "voteCount": 4,
        "content": "CD is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/architecture/solution-ideas/articles/iot-using-cosmos-db\nAzure Cosmos DB is ideal for IoT workloads because it's capable of:\n- Ingesting device telemetry data at high rates, and return indexed queries with low latency and high availability.\n- Storing JSON format from different device vendors, which provides flexibility in payload schema.\n- By using wire protocol\u2013compatible API endpoints for Cassandra, MongoDB, SQL, Gremlin, etcd, and table databases, and built-in support for Jupyter Notebook files."
      },
      {
        "date": "2023-02-22T09:09:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/time-series-insights/overview-what-is-tsi\nAzure Time Series Insights Gen2 is an open and scalable end-to-end IoT analytics service featuring best-in-class user experiences and rich APIs to integrate its powerful capabilities into your existing workflow or application.\n\nYou can use it to collect, process, store, query and visualize data at Internet of Things (IoT) scale--data that's highly contextualized and optimized for time series.\n\nAzure Time Series Insights Gen2 is designed for ad hoc data exploration and operational analysis allowing you to uncover hidden trends, spotting anomalies, and conduct root-cause analysis. It's an open and flexible offering that meets the broad needs of industrial IoT deployments."
      },
      {
        "date": "2023-01-28T04:42:00.000Z",
        "voteCount": 2,
        "content": "C. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights"
      },
      {
        "date": "2023-01-24T12:56:00.000Z",
        "voteCount": 1,
        "content": "MongoDB..."
      },
      {
        "date": "2023-01-18T07:35:00.000Z",
        "voteCount": 2,
        "content": "CD is correct."
      },
      {
        "date": "2023-01-27T07:24:00.000Z",
        "voteCount": 2,
        "content": "C. Azure Cosmos DB for NoSQL\nD. Azure Time Series Insights\n\nAzure Cosmos DB is a globally distributed, multi-model database service that can be used to store and query large amounts of data with low latency. Cosmos DB supports various data models, including NoSQL, and is designed for high throughput and low latency. It can be used to store the data from the IoT devices and can handle the high write and read throughput required for the solution.\n\nAzure Time Series Insights is a time-series data platform that is designed for analyzing time-stamped data. It can be used to visualize the data from the IoT devices in near real-time, providing a way to monitor and analyze the device data in real-time. It also has built-in support for IoT data, making it a good choice for this scenario."
      },
      {
        "date": "2023-01-05T09:40:00.000Z",
        "voteCount": 4,
        "content": "Seems correct."
      },
      {
        "date": "2023-01-05T09:31:00.000Z",
        "voteCount": 3,
        "content": "CD\n\nReal-time access with fast read and write latencies globally, and throughput and consistency all backed by SLAs\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction\n\nAzure Time Series Insights is a fully managed analytics, storage, and visualization service that makes it simple to explore and analyze billions of IoT events simultaneously.\nhttps://learn.microsoft.com/en-us/azure/time-series-insights/time-series-insights-explorer"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95594-exam-az-305-topic-2-question-21-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are planning an Azure Storage solution for sensitive data. The data will be accessed daily. The dataset is less than 10 GB.<br><br>You need to recommend a storage solution that meets the following requirements:<br><br>\u2022\tAll the data written to storage must be retained for five years.<br>\u2022\tOnce the data is written, the data can only be read. Modifications and deletion must be prevented.<br>\u2022\tAfter five years, the data can be deleted, but never modified.<br>\u2022\tData access charges must be minimized.<br><br>What should you recommend? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image166.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image167.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-16T10:09:00.000Z",
        "voteCount": 72,
        "content": "1. correct\n2. Should be COntainer Policy for immutable storage. A resource lock does not prevent removal of files and folders. Prevents deleting resource inside the resource group"
      },
      {
        "date": "2023-01-28T04:43:00.000Z",
        "voteCount": 29,
        "content": "1 is correct\n2 - Container access policy"
      },
      {
        "date": "2024-06-22T10:31:00.000Z",
        "voteCount": 2,
        "content": "Storage account type: General purpose v2 with Hot access tier for blobs: This option provides a balance between performance and cost, ideal for daily access.\nConfiguration to prevent modifications and deletions: Container access policy with an immutable blob policy: This ensures that data cannot be modified or deleted, strictly adhering to the requirement of read-only access until the data can be deleted after five years."
      },
      {
        "date": "2024-06-22T10:32:00.000Z",
        "voteCount": 1,
        "content": "Storage account resource lock:\n\nReason against: While a resource lock can prevent the entire storage account or container from being deleted, it doesn't inherently enforce immutability of the data. Resource locks are more suited for protecting against accidental deletion or modification of the resource itself, not the data within it.\nThe Cool access tier is optimized for infrequently accessed data, which means lower storage costs but higher access charges. Since the data will be accessed daily, the higher access charges could lead to higher overall costs, which does not align with the requirement to minimize data access charges"
      },
      {
        "date": "2024-05-05T06:01:00.000Z",
        "voteCount": 1,
        "content": "Storage account type: General purpose v2 with HOT access tier for blobs, it is most expensive in storage cost but CHEAPEST in access costs and hence meets requirements\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview\n\nOnce the data is written, the data can only be read. Modifications and deletion must be prevented. is called WORM and can only be done via policy and hence container access policy.\nFor those who say Storage resource lock is NOT correct because it can achieve the required goal however it fails to stop it being modified i.e. in order to delete the storage/blob you  need to remove the lock from read and make it modify at that moment no guarantee that no one is accessing it and modifying it and other problem is resource lock are way to administrative as compared to Container policy (automated) i.e. who will remember to remove lock of storage acc and allow for delete without ever being modified after 5 years!! Good Luck :)"
      },
      {
        "date": "2024-05-04T01:38:00.000Z",
        "voteCount": 1,
        "content": "Storage account type: General purpose v2 with Cool access tier for blobs. This option is suitable because it allows storing data that is infrequently accessed, which helps minimize data access charges, and it supports blob storage which can handle datasets under 10 GB. \nConfiguration to prevent modifications and deletions: Storage account resource lock. This option prevents modifications or deletions of the storage account resources, ensuring that once written, the data can only be read as required."
      },
      {
        "date": "2024-05-05T05:52:00.000Z",
        "voteCount": 1,
        "content": "I tend to disagree, please read my comments.. cool is more expensive in access charges than hot, resource lock will not allow you delete without remove the lock and hence too much administration"
      },
      {
        "date": "2024-02-14T06:02:00.000Z",
        "voteCount": 2,
        "content": "Box 2 is a container access policy (for immutable storage) \nA resource lock will only prevent the modification/deletion of the said resource, not of the data inside of that resource."
      },
      {
        "date": "2023-04-12T07:06:00.000Z",
        "voteCount": 9,
        "content": "Same as Question #17\n\n1. GPv2 with hot access tier for blobs 2. Container access policy"
      },
      {
        "date": "2023-09-26T09:51:00.000Z",
        "voteCount": 4,
        "content": "It's not the same question #17. This question have a different option (Premium Block blobs) and this is the correct answer, because access cost to Premium Block is cheaper than GPv2 hot tier"
      },
      {
        "date": "2023-03-22T10:15:00.000Z",
        "voteCount": 13,
        "content": "1. Storage account type:\nC. General purpose v2 with hot access tier for blobs\nThe hot access tier provides lower data access costs compared to the cool access tier, making it more suitable for minimizing charges when data is accessed daily. Although the cool tier has lower storage costs, the data access charges are higher, which would not be ideal for your scenario. Premium block blobs are meant for high-performance scenarios and are not necessary for a small dataset of less than 10 GB.\n\n2. Configuration to prevent modifications and deletions:\nB. Container access policy\nYou can create a container access policy with specific permissions (in this case, read-only) and set an expiry time of five years. This policy prevents modifications and deletions, while still allowing the data to be read. After five years, the policy will expire, and the data can be deleted but not modified. Storage account resource locks and container access level settings don't offer the same granularity of control over the data as the container access policy."
      },
      {
        "date": "2023-02-27T12:36:00.000Z",
        "voteCount": 6,
        "content": "1. Premium block blobs\n2. Container Policy\nhttps://azure.microsoft.com/en-us/pricing/details/storage/blobs/\nOperations and data transfer"
      },
      {
        "date": "2023-03-28T07:26:00.000Z",
        "voteCount": 1,
        "content": "This is correct answer!!!\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview"
      },
      {
        "date": "2023-02-22T08:08:00.000Z",
        "voteCount": 2,
        "content": "premium block blob, container access policy\nWe only need to minimize access charge, not storage cost. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-block-blob-premium\nIn other words, to access same amount of data within a given time - 500 million reads /second. That is much faster than GPv2 hot tier milisecond access time"
      },
      {
        "date": "2023-10-13T18:10:00.000Z",
        "voteCount": 1,
        "content": "The question mentioned \"The data will be accessed daily.\". Can not assume super high read frequency."
      },
      {
        "date": "2023-02-17T19:46:00.000Z",
        "voteCount": 2,
        "content": "1. GPv2 with hot access tier for blobs\n2. Container access policy\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview\nImmutable storage for Azure Blob Storage enables users to store business-critical data in a WORM (Write Once, Read Many) state. While in a WORM state, data cannot be modified or deleted for a user-specified interval. By configuring immutability policies for blob data, you can protect your data from overwrites and deletes."
      },
      {
        "date": "2023-02-15T10:11:00.000Z",
        "voteCount": 2,
        "content": "For Data Storage and Access, Premium will be the cheapest. https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"
      },
      {
        "date": "2023-02-13T03:57:00.000Z",
        "voteCount": 1,
        "content": "The second answer is wrong. A resource lock does exactly what the name suggests - it locks the resource itself. To prevent modification of the files, a container access policy is needed."
      },
      {
        "date": "2023-02-02T14:08:00.000Z",
        "voteCount": 2,
        "content": "Why not Premium block blobs and the storage amount is small.  Access charges are cheapest in the scenario"
      },
      {
        "date": "2023-02-04T06:40:00.000Z",
        "voteCount": 1,
        "content": "Imho - GPV2 will be cheaper than Premium Block Blobs. You can check with Azure pricing calculator."
      },
      {
        "date": "2023-02-06T12:35:00.000Z",
        "voteCount": 2,
        "content": "Check it out Premium is cheaper for data access\nhttps://azure.microsoft.com/en-us/pricing/details/storage/blobs/?cdn=disable"
      },
      {
        "date": "2023-01-21T23:55:00.000Z",
        "voteCount": 5,
        "content": "GPv2 and Container Policy"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95788-exam-az-305-topic-2-question-22-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are designing a data analytics solution that will use Azure Synapse and Azure Data Lake Storage Gen2.<br><br>You need to recommend Azure Synapse pools to meet the following requirements:<br><br>\u2022\tIngest data from Data Lake Storage into hash-distributed tables.<br>\u2022\tImplement query, and update data in Delta Lake.<br><br>What should you recommend for each requirement? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image168.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image169.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-02-05T10:55:00.000Z",
        "voteCount": 35,
        "content": "The second question is confusing, and I am not sure what the answer is\n- Can query delta lake with Serverless SQL pool but won't be able to update it. \n- Only Apache Spark pools support updates to Delta Lakes files. It can also be used to query long-time series as well if I understand the doc correctly...\n\nI think the answer to 2 is Apache Spark tools on that basis..."
      },
      {
        "date": "2024-03-06T07:13:00.000Z",
        "voteCount": 2,
        "content": "The question mentions 'Data Lake Storage', not Delta Lake - there is no explicit indication that the data is stored in a delta lake format. Therefore I don't think that the Spark pool is needed.\n\nNevertheless, Delta Lake is indeed a very confusing name for what is essentially a data format (\"optimized storage layer\")."
      },
      {
        "date": "2024-03-06T07:15:00.000Z",
        "voteCount": 1,
        "content": "Ah I take it back, Delta lake is also mentioned later, sry for the confusion."
      },
      {
        "date": "2024-03-20T00:01:00.000Z",
        "voteCount": 1,
        "content": "Apache Spark pools in Azure Synapse enable data engineers to modify Delta Lake files\nTaken from:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format"
      },
      {
        "date": "2023-02-07T05:27:00.000Z",
        "voteCount": 2,
        "content": "Agree.\nFrom what I can find SQL pool can't update delta lake files only Apache Spark can do that, assuming article is accurate below;\n\nhttps://www.jamesserra.com/archive/2022/03/azure-synapse-and-delta-lake/#:~:text=Serverless%20SQL%20pools%20do%20not%20support%20updating%20delta,in%20Azure%20Synapse%20Analytics%20to%20update%20Delta%20Lake."
      },
      {
        "date": "2023-01-27T07:41:00.000Z",
        "voteCount": 22,
        "content": "The answer is correct.\n\nAzure Synapse Analytics (also named SQL Data Warehouse) is a cloud-based analytics service that allows you to analyze large amounts of data using a combination of on-demand and provisioned resources. It offers several different options for working with data, including:\n\n- Dedicated SQL pool:  It's best for big and complex tasks.\n\n- Serverless Apache Spark pool: It's best for big data analysis and machine learning tasks using Spark SQL and Spark DataFrames.\n\n- Serverless SQL pool: This is a service that automatically adjusts the amount of resources you use based on your needs. You only pay for what you use. It's best for small to medium-sized tasks and tasks that change often."
      },
      {
        "date": "2023-08-16T12:00:00.000Z",
        "voteCount": 27,
        "content": "How can you spent so much time to give explained answers, but you still get them wrong? First answer is correct, second one is Apache Spark pool.\n\nServerless SQL pool doesn't provides updates: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format. Do you see any information about updates there?\n\nUpdates are possible in Apache Spark: https://docs.delta.io/latest/delta-update.html\n\nBtw - what \"Apache Spark is best for big data analysis and ML tasks\" have in common with Delta Lake updates? Are you copying the answers from the ChatGPT? I have worked with Databricks for 2 years and Apache Spark is the right answer. Apache Spark can be also used for small scenarios as it's not that expensive and is often used by data engineers, not just big data engineers"
      },
      {
        "date": "2023-08-16T12:02:00.000Z",
        "voteCount": 10,
        "content": "Last note - Hash-distributed tables are used for VERY LARGE FACT TABLES. As per documentation (https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute): Consider using a hash-distributed table when:\n\nThe table size on disk is more than 2 GB."
      },
      {
        "date": "2024-10-15T00:34:00.000Z",
        "voteCount": 1,
        "content": "1.\tIngest data from Data Lake Storage into hash-distributed tables:\n\t\u2022\tA dedicated SQL pool\nExplanation: Hash-distributed tables are a feature of dedicated SQL pools in Azure Synapse. They allow for efficient data distribution and parallel processing, which is ideal for large-scale data ingestion from Data Lake Storage.\n\t2.\tImplement, query, and update data in Delta Lake:\n\t\u2022\tA serverless Apache Spark pool\nExplanation: Serverless Apache Spark pools in Azure Synapse support Delta Lake, providing full read and write capabilities. They allow you to implement, query, and update Delta Lake tables effectively.\n\nAnswer Area:\n\n\t1.\tA dedicated SQL pool\n\t2.\tA serverless Apache Spark pool"
      },
      {
        "date": "2024-09-10T20:08:00.000Z",
        "voteCount": 1,
        "content": "dedicated SQL pool\nserverless Apache Spark pool"
      },
      {
        "date": "2024-08-09T04:31:00.000Z",
        "voteCount": 2,
        "content": "This question appeared in the exam, August 2024, I gave this same answer for box 1 but answered Apache Spark Pool for box 2. I scored 870"
      },
      {
        "date": "2024-06-27T02:07:00.000Z",
        "voteCount": 1,
        "content": "From Copilot:\n\nTo meet the requirements for ingesting data from Data Lake Storage into hash-distributed tables, you should recommend A Dedicated SQL pool. This option is designed for large-scale, high-performance, and secure analytics on Azure.\n\nFor implementing, querying, and updating data in Delta Lake, you should recommend A serverless Apache Spark pool. This option allows you to run big data analytics and artificial intelligence workloads with Apache Spark, which is compatible with Delta Lake.\n\nThese recommendations align with Azure's best practices for performance and scalability when working with Synapse and Data Lake Storage Gen2. If you need further details or assistance with the setup, feel free to ask."
      },
      {
        "date": "2024-06-22T11:02:00.000Z",
        "voteCount": 2,
        "content": "Ingest data from Data Lake Storage into hash-distributed tables: A dedicated SQL pool\nImplement, query, and update data in Delta Lake: A serverless Apache Spark poo"
      },
      {
        "date": "2024-06-22T11:02:00.000Z",
        "voteCount": 2,
        "content": "Requirement 1: Ingest data from Data Lake Storage into hash-distributed tables\nA dedicated SQL pool: This pool is specifically designed for high-performance data warehousing. It allows for the ingestion of large datasets into hash-distributed tables, optimizing performance and scalability. Hash distribution is a key feature of dedicated SQL pools to enhance query performance for large datasets.\nRecommendation: A dedicated SQL pool\n\nRequirement 2: Implement, query, and update data in Delta Lake\nA serverless Apache Spark pool: Delta Lake is an open-source storage layer that brings ACID transactions to Apache Spark. It is optimized for big data workloads and is best utilized with Apache Spark pools. The serverless Apache Spark pool in Azure Synapse provides a managed Spark environment, ideal for working with Delta Lake for querying, updating, and managing large datasets."
      },
      {
        "date": "2024-05-05T06:10:00.000Z",
        "voteCount": 1,
        "content": "Box 1: is correct =&gt; A hash-distributed table distributes table rows across the Compute nodes by using a deterministic hash function to assign each row to one distribution.\nSince identical values always hash to the same distribution, SQL Analytics has built-in knowledge of the row locations. In dedicated SQL pool this knowledge is used to minimize data movement during queries, which improves query performance.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\nBox 2: should be Apache Spark pools \nin Azure Synapse enable data engineers to modify Delta Lake files\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-delta-lake-format"
      },
      {
        "date": "2024-05-04T01:43:00.000Z",
        "voteCount": 1,
        "content": "Ingest data from Data Lake Storage into hash-distributed tables: For this requirement, I recommend using a Dedicated SQL pool in Azure Synapse. This service is designed for large-scale data processing and supports creating hash-distributed tables to optimize query performance. \nImplement, query, and update data in Delta Lake: For this requirement, I recommend using a Serverless Apache Spark pool in Azure Synapse. This service provides capabilities for working with Delta Lake as it offers an analytics service that can handle big data processing tasks without the need to provision or manage clusters."
      },
      {
        "date": "2024-03-23T02:39:00.000Z",
        "voteCount": 2,
        "content": "Dedicated SQL Pools\n\nPurpose: Dedicated SQL pools provide massive parallel processing (MPP) capabilities ideal for handling large volumes of data. They are optimized for complex queries over large datasets and are suitable for building enterprise-level, big data analytics solutions.\n\nSpark Pools\n\nPurpose: Spark pools in Azure Synapse provide a fully managed Apache Spark environment. They are designed to handle big data processing, analytics, and machine learning tasks. Spark pools can process data in various formats and from multiple sources, including Azure Data Lake Storage."
      },
      {
        "date": "2024-03-14T14:52:00.000Z",
        "voteCount": 1,
        "content": "from ChatGPT : For implementing, querying, and updating data in Delta Lake, the most suitable option among the ones you listed would be A serverless Apache Spark pool.\n\nHere's why:\n\n    Integration with Delta Lake: Apache Spark is tightly integrated with Delta Lake, offering native support for reading from and writing to Delta tables. This integration ensures seamless compatibility and efficient data processing capabilities"
      },
      {
        "date": "2023-11-26T09:46:00.000Z",
        "voteCount": 3,
        "content": "OPTION 2: SERVERLESS APACHE SPARK POOL"
      },
      {
        "date": "2023-09-29T18:10:00.000Z",
        "voteCount": 5,
        "content": "Got this on Sept. 29, 2023"
      },
      {
        "date": "2023-09-24T23:23:00.000Z",
        "voteCount": 5,
        "content": "I had question at 24th Sep 2023"
      },
      {
        "date": "2023-09-19T23:38:00.000Z",
        "voteCount": 4,
        "content": "Serverless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/resources-self-help-sql-on-demand?tabs=x80070002#delta-lake"
      },
      {
        "date": "2023-08-15T01:17:00.000Z",
        "voteCount": 6,
        "content": "From MSFT docs: \n\nServerless SQL pools don't support updating Delta Lake files. You can use serverless SQL pool to query the latest version of Delta Lake. Use Apache Spark pools in Synapse Analytics to update Delta Lake."
      },
      {
        "date": "2023-05-12T10:59:00.000Z",
        "voteCount": 5,
        "content": "To meet the requirements of ingesting data from Data Lake Storage into hash-distributed tables and implementing query and update operations in Delta Lake, the recommended Azure Synapse pool options are as follows:\n\nIngest Data from Data Lake Storage into Hash-Distributable Tables:\n\nA dedicated SQL pool: This option allows you to leverage the power of the dedicated SQL pool (formerly SQL Data Warehouse) in Azure Synapse to perform high-performance ingest operations into hash-distributed tables. The dedicated SQL pool is optimized for large-scale data warehousing scenarios.\nImplement Query and Update Data in Delta Lake:\n\nA serverless Apache Spark pool: This option allows you to use Apache Spark as a serverless processing engine within Azure Synapse. Spark provides robust support for querying and updating data in Delta Lake, which is an open-source storage layer for reliable big data processing."
      },
      {
        "date": "2024-02-14T06:04:00.000Z",
        "voteCount": 2,
        "content": "It would be nice to also include a disclaimer saying that this is a response generated by ChatGPT or another similar tool."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95575-exam-az-305-topic-2-question-23-discussion/",
    "body": "You have an on-premises storage solution.<br><br>You need to migrate the solution to Azure. The solution must support Hadoop Distributed File System (HDFS).<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage Gen2\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure NetApp Files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Share",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Table storage"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-16T08:55:00.000Z",
        "voteCount": 10,
        "content": "Azure Data Lake Storage Gen2: This is a fully managed, cloud-native data lake that supports the HDFS protocol. It allows you to store and analyze large amounts of data in its native format, without the need to move or transform the data."
      },
      {
        "date": "2023-03-22T10:22:00.000Z",
        "voteCount": 8,
        "content": "A. Azure Data Lake Storage Gen2\n\nAzure Data Lake Storage Gen2 is the best choice for migrating your on-premises storage solution to Azure with support for Hadoop Distributed File System (HDFS). It is a highly scalable and cost-effective storage service designed for big data analytics, providing integration with Azure HDInsight, Azure Databricks, and other Azure services. It is built on Azure Blob Storage and combines the advantages of HDFS with Blob Storage, offering a hierarchical file system, fine-grained security, and high-performance analytics."
      },
      {
        "date": "2024-04-29T03:48:00.000Z",
        "voteCount": 1,
        "content": "Given answer A is correct\nAzure Data Lake Storage Gen2:  is designed for big data analytics, it combines the power of a high-performance file system with massive scale and economy to help you speed up your big data analytics. It allows the data to be organized in directories by date and time. It is the best choice for migrating your on-premises storage solution to Azure with support for Hadoop Distributed File System (HDFS). It is a highly scalable and cost-effective storage service designed for big data analytics, providing integration with Azure HDInsight, Azure Databricks, and other Azure services. It is built on Azure Blob Storage and combines the advantages of HDFS with Blob Storage, offering a hierarchical file system, fine-grained security, and high-performance analytics. Azure Data Lake Storage is optimized storage for big data analytics workloads."
      },
      {
        "date": "2024-03-23T10:44:00.000Z",
        "voteCount": 1,
        "content": "\"Azure Data Lake Storage Gen2 is primarily designed to work with Hadoop and all frameworks that use the Apache Hadoop Distributed File System (HDFS) as their data access layer.\"\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#hadoop-compatible-access"
      },
      {
        "date": "2023-08-28T12:17:00.000Z",
        "voteCount": 1,
        "content": "Azure DataLake Storage Gen2 has its own private API on top of https protocol, which is not compatible with HDFS internal protocol (used by NameNode and DataNode servers of the \"Distributed File System\" in hadoop)  ...\nHowever, the java API class hadoop \"FileSystem\" has an implementation for abfs \nsee here : https://github.com/apache/hadoop/blob/trunk/hadoop-tools/hadoop-azure/src/main/java/org/apache/hadoop/fs/azurebfs/AzureBlobFileSystem.java#L133"
      },
      {
        "date": "2023-02-17T19:41:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction#key-features-of-data-lake-storage-gen2\nHadoop compatible access: Data Lake Storage Gen2 allows you to manage and access data just as you would with a Hadoop Distributed File System (HDFS). The new ABFS driver (used to access data) is available within all Apache Hadoop environments. These environments include Azure HDInsight, Azure Databricks, and Azure Synapse Analytics."
      },
      {
        "date": "2023-01-31T10:14:00.000Z",
        "voteCount": 4,
        "content": "Correct\n\nhttps://learn.microsoft.com/en-us/azure/architecture/guide/hadoop/apache-hdfs-migration"
      },
      {
        "date": "2023-01-28T04:56:00.000Z",
        "voteCount": 1,
        "content": "A. Azure Data Lake Storage Gen2"
      },
      {
        "date": "2023-02-06T08:24:00.000Z",
        "voteCount": 2,
        "content": "remember HDFS"
      },
      {
        "date": "2023-01-22T08:05:00.000Z",
        "voteCount": 2,
        "content": "Azure Data Lake Gen 2"
      },
      {
        "date": "2023-01-17T06:46:00.000Z",
        "voteCount": 2,
        "content": "Data Lake Storage Gen2 allows you to manage and access data just as you would with a Hadoop Distributed File System (HDFS)\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/100297-exam-az-305-topic-2-question-24-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an on-premises app named App1.<br><br>Customers use App1 to manage digital images.<br><br>You plan to migrate App1 to Azure.<br><br>You need to recommend a data storage solution for App1. The solution must meet the following image storage requirements:<br><br>\u2022\tEncrypt images at rest.<br>\u2022\tAllow files up to 50 MB.<br>\u2022\tManage access to the images by using Azure Web Application Firewall (WAF) on Azure Front Door.<br><br>The solution must meet the following customer account requirements:<br><br>\u2022\tSupport automatic scale out of the storage.<br>\u2022\tMaintain the availability of App1 if a datacenter fails.<br>\u2022\tSupport reading and writing data from multiple Azure regions.<br><br>Which service should you include in the recommendation for each type of data? To answer, drag the appropriate services to the correct type of data. Each service may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct answer is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image176.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image177.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-05T04:54:00.000Z",
        "voteCount": 22,
        "content": "Box 1 - Azure blob storage \nThe requirement to be accessible through a WAF limit the options to the Blob storage.\nBox 2 - Cosmos DB\nConcurrent writes from multiple regions make this the only option."
      },
      {
        "date": "2023-03-22T10:26:00.000Z",
        "voteCount": 15,
        "content": "Box 1 - Image storage: A. Azure Blob Storage\n\nAzure Blob Storage is a suitable choice for storing digital images, as it supports encryption at rest, handles large file sizes (up to 50 MB or even larger), and can be used in conjunction with Azure Web Application Firewall (WAF) on Azure Front Door.\n\nBox 2 - Customer accounts: B. Azure Cosmos DB\n\nAzure Cosmos DB is a highly scalable, globally distributed, multi-model database service that supports automatic scale-out, ensures high availability even in the event of a datacenter failure, and allows for reading and writing data from multiple Azure regions. This makes it an ideal choice for storing customer account data in your scenario."
      },
      {
        "date": "2024-04-29T03:50:00.000Z",
        "voteCount": 3,
        "content": "Given answer is correct"
      },
      {
        "date": "2024-01-21T09:33:00.000Z",
        "voteCount": 9,
        "content": "appeared in Exam 01/2024"
      },
      {
        "date": "2023-02-22T09:04:00.000Z",
        "voteCount": 8,
        "content": "1. Azure Blob storage\n2. Azure Cosmos DB\n\nhttps://learn.microsoft.com/en-us/azure/frontdoor/scenario-storage-blobs\nAzure Front Door accelerates the delivery of static content from Azure Storage blobs, and enables a secure and scalable architecture. Static content delivery is useful for many different use cases, including website hosting and file delivery\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction#guaranteed-speed-at-any-scale\n- Multi-region writes and data distribution to any Azure region with just a button"
      },
      {
        "date": "2023-02-21T11:34:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/99419-exam-az-305-topic-2-question-25-discussion/",
    "body": "You are designing an application that will aggregate content for users.<br><br>You need to recommend a database solution for the application. The solution must meet the following requirements:<br><br>\u2022\tSupport SQL commands.<br>\u2022\tSupport multi-master writes.<br>\u2022\tGuarantee low latency read operations.<br><br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB for NoSQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database that uses active geo-replication",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure SQL Database Hyperscale",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB for PostgreSQL"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-16T07:27:00.000Z",
        "voteCount": 10,
        "content": "cosmos for the multi writer\npostgre is not good at reading"
      },
      {
        "date": "2023-03-26T03:27:00.000Z",
        "voteCount": 4,
        "content": "No SQL supports SQLquery ?"
      },
      {
        "date": "2024-06-06T21:23:00.000Z",
        "voteCount": 1,
        "content": ":))) it sounds funny."
      },
      {
        "date": "2023-07-17T20:58:00.000Z",
        "voteCount": 34,
        "content": "It is not a \"No SQL\", it is a \"Not Only SQL\". \nHence, it does support SQL query. Hope it helps."
      },
      {
        "date": "2024-02-14T06:11:00.000Z",
        "voteCount": 7,
        "content": "This is an unfortunate abbreviation, to say the least :) Should have called it SQL+ or something that shows SQL availability, plus other options."
      },
      {
        "date": "2023-04-18T23:02:00.000Z",
        "voteCount": 8,
        "content": "Yes, it does. \n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/nosql/how-to-dotnet-query-items\n- The Azure Cosmos DB for NoSQL supports the use of Structured Query Language (SQL) to perform queries on items in containers."
      },
      {
        "date": "2023-04-28T02:26:00.000Z",
        "voteCount": 15,
        "content": "microsoft is weird"
      },
      {
        "date": "2023-03-22T10:28:00.000Z",
        "voteCount": 6,
        "content": "2 / 2\n\nA. Azure Cosmos DB for NoSQL\n\nAzure Cosmos DB is a globally distributed, multi-model database service that supports SQL commands, multi-master writes, and guarantees low latency read operations. It supports a variety of NoSQL data models including document, key-value, graph, and column-family. Azure Cosmos DB provides automatic and instant scalability, high availability, and low latency globally by replicating and synchronizing data across multiple Azure regions.\n\nOn the other hand, Azure SQL Database and Azure SQL Database Hyperscale are traditional relational database services that do not natively support multi-master writes."
      },
      {
        "date": "2024-04-29T03:53:00.000Z",
        "voteCount": 2,
        "content": "Given answer A is correct\nCosmo DB for NOSQL or Cosmo DB SQL API provides the following features\n\u2022 Support SQL commands.\n\u2022 Support multi-master writes.\n\u2022 Guarantee low latency read operations."
      },
      {
        "date": "2024-03-16T07:17:00.000Z",
        "voteCount": 1,
        "content": "Why \"D - Azure Cosmos DB for PostgreSQL\" is incorrect ?\nAzure Cosmos DB for PostgreSQL fulfils all the requirements in the question"
      },
      {
        "date": "2023-12-08T16:27:00.000Z",
        "voteCount": 2,
        "content": "Content aggregation is the main objective. Content can be structured or unstructured.  NoSQL = \"Not Only SQL\" i.e. it can deal with any kind of content - structured or unstructured. Cosmos DB support multi master writes synchronously during replication and provide low latency for reads across regions as long as distributed DBs are nearer (local) to users who access it. So answer is Azure Cosmos DB for NoSQL."
      },
      {
        "date": "2023-07-28T02:48:00.000Z",
        "voteCount": 3,
        "content": "Although Azure Cosmos supports Multi Region Writes, Azure Cosmos for PostgreSQL  does not.\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/postgresql/introduction#fully-managed-resilient-database"
      },
      {
        "date": "2023-06-30T17:39:00.000Z",
        "voteCount": 2,
        "content": "As for speed, NoSQL is generally faster than SQL, especially for key-value storage in our experiment"
      },
      {
        "date": "2023-02-18T07:30:00.000Z",
        "voteCount": 4,
        "content": "Same as Question 10.\nhttps://www.examtopics.com/discussions/microsoft/view/67751-exam-az-305-topic-2-question-10-discussion"
      },
      {
        "date": "2023-03-26T03:30:00.000Z",
        "voteCount": 6,
        "content": "nope . Here it  is Cosmos DB for NoSQL earlier question had right optionAzure Cosmos DB for SQL API."
      },
      {
        "date": "2023-02-18T07:30:00.000Z",
        "voteCount": 3,
        "content": "A is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/introduction#key-benefits\n- Gain unparalleled SLA-backed speed and throughput, fast global access, and instant elasticity. Real-time access with fast read and write latencies globally, and throughput and consistency all backed by SLAs\n- Multi-region writes and data distribution to any Azure region with just a button."
      },
      {
        "date": "2023-02-16T15:09:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/99422-exam-az-305-topic-2-question-26-discussion/",
    "body": "You plan to migrate on-premises MySQL databases to Azure Database for MySQL Flexible Server.<br><br>You need to recommend a solution for the Azure Database for MySQL Flexible Server configuration. The solution must meet the following requirements:<br><br>\u2022\tThe databases must be accessible if a datacenter fails.<br>\u2022\tCosts must be minimized.<br><br>Which compute tier should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBurstable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGeneral Purpose\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMemory Optimized"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-25T05:42:00.000Z",
        "voteCount": 27,
        "content": "B is the answer.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability#limitations\nHere are some considerations to keep in mind when you use high availability:\n- High availability isn't supported in the burstable compute tier."
      },
      {
        "date": "2023-02-27T21:26:00.000Z",
        "voteCount": 1,
        "content": "But the question did not ask for high availability"
      },
      {
        "date": "2023-02-27T21:29:00.000Z",
        "voteCount": 1,
        "content": "sry, my bad, it is Zone Redundant High Availability. It should be B"
      },
      {
        "date": "2023-02-28T07:58:00.000Z",
        "voteCount": 12,
        "content": "Got this in Feb 2023 exam."
      },
      {
        "date": "2023-05-15T09:25:00.000Z",
        "voteCount": 13,
        "content": "B. General Purpose\n\nThe General Purpose compute tier provides a balance between performance and cost. It is suitable for most common workloads and offers a good combination of CPU and memory resources. It provides high availability and fault tolerance by utilizing Azure's infrastructure across multiple datacenters. This ensures that the databases remain accessible even if a datacenter fails.\n\nThe Burstable compute tier (option A) is designed for workloads with variable or unpredictable usage patterns. It provides burstable CPU performance but may not be the optimal choice for ensuring availability during a datacenter failure.\n\nThe Memory Optimized compute tier (option C) is designed for memory-intensive workloads that require high memory capacity. While it provides excellent performance for memory-bound workloads, it may not be necessary for minimizing costs or meeting the specified requirements."
      },
      {
        "date": "2024-08-23T11:57:00.000Z",
        "voteCount": 1,
        "content": "it's B\nZone redundancy:\n\nThe zone-redundancy option is only available in regions that support availability zones.\n\nZone-redundancy is not supported for:\n\nAzure Database for PostgreSQL \u2013 Single Server SKU.\nBurstable compute tier.\nRegions with single-zone availability.\nwith burstable we don'T have availability options to choose (tested) thats mean another database replica in another zone wich we will swith after datacenter outage, the only thing that our data is stored in zone redundant so we won't loose it if we have datacenter outage"
      },
      {
        "date": "2024-05-04T02:06:00.000Z",
        "voteCount": 1,
        "content": "Based on the requirements, the Burstable compute tier would be the most suitable choice for the Azure Database for MySQL Flexible Server configuration.\nThe Burstable tier is ideal for workloads that don\u2019t need continuous full compute capacity. It provides better cost optimization controls with the ability to stop/start the server. Moreover, the flexible server architecture allows users to opt for high availability within a single availability zone and across multiple availability zones, ensuring that the databases remain accessible even if a datacenter fails."
      },
      {
        "date": "2024-04-29T03:57:00.000Z",
        "voteCount": 1,
        "content": "I would go for B\nFlexible Server is a fully managed production-ready database service designed for more granular control and flexibility over database management functions and configuration settings. The flexible server architecture allows users to opt for high availability within a single availability zone and across multiple availability zones. Flexible servers provide better cost optimization controls with the ability to stop/start the server and burstable compute tier, ideal for workloads that don't need full compute capacity continuously. Flexible Server also supports reserved instances allowing you to save up to 63% cost, which is ideal for production workloads with predictable compute capacity requirements."
      },
      {
        "date": "2024-04-03T20:03:00.000Z",
        "voteCount": 1,
        "content": "I think answer is correct. https://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage"
      },
      {
        "date": "2024-04-03T01:50:00.000Z",
        "voteCount": 1,
        "content": "The databases must be accessible if a datacenter fails!\nAzure availability zones are at least three physically separate groups of datacenters within each Azure region.\nIf one datacenter fails then the data will be available in other datacenter in same zone.\n\nSo the answer is correct. \n\nhttps://learn.microsoft.com/en-us/azure/reliability/reliability-postgresql-flexible-server"
      },
      {
        "date": "2024-03-08T20:28:00.000Z",
        "voteCount": 1,
        "content": "Tested in Lab, B - General Purpose is correct. \n\nBurstable doesn't support high availability. Only General Purpose or Business Critical"
      },
      {
        "date": "2023-12-09T03:41:00.000Z",
        "voteCount": 2,
        "content": "A. Burstable. It provides the lowest cost. Both Burstable and General Purpose provide Zone Redundancy\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage\nhttps://azure.microsoft.com/en-us/pricing/details/mysql/"
      },
      {
        "date": "2023-12-27T16:09:00.000Z",
        "voteCount": 2,
        "content": "High availability isn't supported in the burstable compute tier.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability"
      },
      {
        "date": "2024-01-19T06:57:00.000Z",
        "voteCount": 1,
        "content": "Therefore, while the Burstable compute tier offers significant cost and flexibility advantages for certain types of workloads, it is not recommended for production workloads that require consistent CPU performance. Note that the Burstable tier doesn't support functionality of creating Read Replicas and High availability feature. For such workloads and features, other compute tiers, such as the General Purpose or Business Critical are more appropriate.\n\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-service-tiers-storage#performance-limitations-of-burstable-series-instances"
      },
      {
        "date": "2023-09-29T18:11:00.000Z",
        "voteCount": 2,
        "content": "Got this on Sept. 29, 2023"
      },
      {
        "date": "2023-09-24T23:24:00.000Z",
        "voteCount": 3,
        "content": "I had question at 24th Sep 2023"
      },
      {
        "date": "2023-09-25T11:50:00.000Z",
        "voteCount": 2,
        "content": "How was the exam? I'm writing tomorrow on the 26th"
      },
      {
        "date": "2023-09-20T11:16:00.000Z",
        "voteCount": 3,
        "content": "When trying to enable high availability with the Burstable tier, I see this message:\n\"High availability is not supported with the compute tier choice. If you would like to configure high availability, the compute tier will be upgraded to the General Purpose compute tier or you may choose a different compute tier by clicking 'Configure Server' below.\"\nSo, General Purpose is the answer to minimize costs."
      },
      {
        "date": "2023-09-18T12:04:00.000Z",
        "voteCount": 1,
        "content": "General purpose provides Zone redundanty"
      },
      {
        "date": "2023-07-12T23:11:00.000Z",
        "voteCount": 1,
        "content": "Zone redundancy requirement which General purpose provides."
      },
      {
        "date": "2023-06-10T19:02:00.000Z",
        "voteCount": 2,
        "content": "Burstable is not zone redundant - so B is correct"
      },
      {
        "date": "2023-04-25T12:29:00.000Z",
        "voteCount": 1,
        "content": "High availability isn't supported in the burstable compute tier.\nhttps://learn.microsoft.com/en-us/azure/mysql/flexible-server/concepts-high-availability#limitations"
      },
      {
        "date": "2023-04-05T17:53:00.000Z",
        "voteCount": 1,
        "content": "Has to be B, if the requirement states \"The databases must be accessible if a datacenter fails.\".\n\nChecked in portal, could not set up high availability unless I switched from burstable to Gen-Purpose."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/106681-exam-az-305-topic-2-question-27-discussion/",
    "body": "You are designing an app that will use Azure Cosmos DB to collate sales from multiple countries.<br><br>You need to recommend an API for the app. The solution must meet the following requirements:<br><br>\u2022\tSupport SQL queries.<br>\u2022\tSupport geo-replication.<br>\u2022\tStore and access data relationally.<br><br>Which API should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Cassandra",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPostgreSQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMongoDB",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNoSQL"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-19T01:05:00.000Z",
        "voteCount": 23,
        "content": "Correct answer: B\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api\n\nStore data relationally:\n- NoSQL stores data in document format\n- MongoDB stores data in a document structure (BSON format)\n\nSupport SQL Queries:\n- Apache Cassandra uses Cassandra Query Language (CQL)\n\nIf you\u2019re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice."
      },
      {
        "date": "2023-07-13T09:25:00.000Z",
        "voteCount": 10,
        "content": "The correct answer is B. PostgreSQL.\n\nAzure Cosmos DB's API for PostgreSQL provides full support for SQL queries, geo-replication, and allows you to store and access data relationally. It offers automatic and instant scalability, global distribution, and effortless replication of data across Azure regions, fulfilling all of your mentioned requirements.\n\nA. Apache Cassandra is a NoSQL database that does not natively support SQL queries. While it does offer some SQL-like capabilities, it is not a fully relational database.\n\nC. MongoDB is a NoSQL database and does not support the relational data model, although it does provide SQL-like query language.\n\nD. NoSQL is a type of database design that can store and retrieve data, but it isn't a specific API. Also, not all NoSQL databases support SQL queries and relational data storage."
      },
      {
        "date": "2024-09-10T09:59:00.000Z",
        "voteCount": 1,
        "content": "These are databases not APIs so I don't quite understand this question."
      },
      {
        "date": "2023-11-21T20:37:00.000Z",
        "voteCount": 3,
        "content": "If you\u2019re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice. To learn more, see the Azure Cosmos DB for PostgreSQL introduction."
      },
      {
        "date": "2023-05-15T09:26:00.000Z",
        "voteCount": 4,
        "content": "B. PostgreSQL\n\nAzure Cosmos DB provides support for multiple APIs, each tailored to different data models and query languages. The PostgreSQL API is well-suited for applications that require relational data storage and the ability to execute SQL queries. It offers compatibility with the PostgreSQL wire protocol and supports standard SQL syntax, allowing you to leverage your existing SQL skills and tools.\n\nAdditionally, the PostgreSQL API in Azure Cosmos DB provides built-in support for geo-replication, allowing you to replicate your data across multiple regions for high availability and disaster recovery purposes. This ensures that your data is accessible and resilient even in the event of a regional outage or failure.\n\nTherefore, the recommended API for Azure Cosmos DB in this scenario is the PostgreSQL API."
      },
      {
        "date": "2023-04-25T12:31:00.000Z",
        "voteCount": 1,
        "content": "If you\u2019re looking for a managed open source relational database with high performance and geo-replication, Azure Cosmos DB for PostgreSQL is the recommended choice. To learn more, see the\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api#api-for-postgresql"
      },
      {
        "date": "2023-04-19T05:35:00.000Z",
        "voteCount": 1,
        "content": "B: PostgreSQL\n\nBut finding proper info in one place.....\nI am not DB guy at all.\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/choose-api#api-for-postgresql"
      },
      {
        "date": "2023-04-19T03:11:00.000Z",
        "voteCount": 3,
        "content": "API for NoSQL is native to Azure Cosmos DB."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/106695-exam-az-305-topic-2-question-28-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an app that generates 50,000 events daily.<br><br>You plan to stream the events to an Azure event hub and use Event Hubs Capture to implement cold path processing of the events. The output of Event Hubs Capture will be consumed by a reporting system.<br><br>You need to identify which type of Azure storage must be provisioned to support Event Hubs Capture, and which inbound data format the reporting system must support.<br><br>What should you identify? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image189.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image190.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-29T09:24:00.000Z",
        "voteCount": 147,
        "content": "Man sometimes I think I know what I'm talking about with Azure, and then I see a question like this and I question my sanity."
      },
      {
        "date": "2024-09-03T14:11:00.000Z",
        "voteCount": 1,
        "content": "You're not the only one."
      },
      {
        "date": "2024-01-18T13:14:00.000Z",
        "voteCount": 9,
        "content": "I guess we are on the same boat"
      },
      {
        "date": "2023-10-14T12:38:00.000Z",
        "voteCount": 8,
        "content": "Seriously, who knows this by heart?"
      },
      {
        "date": "2024-04-29T16:49:00.000Z",
        "voteCount": 3,
        "content": "Bill Gates"
      },
      {
        "date": "2023-09-13T12:06:00.000Z",
        "voteCount": 18,
        "content": "You are not alone"
      },
      {
        "date": "2023-07-13T09:30:00.000Z",
        "voteCount": 33,
        "content": "1. Storage Type: Azure Data Lake Storage Gen2\n\nAzure Event Hubs Capture allows captured data to be written either to Azure Blob Storage or Azure Data Lake Storage Gen2. Given the nature of the data and its use in reporting and analysis, Azure Data Lake Storage Gen2 is the more appropriate choice because it is designed for big data analytics.\n\n2. Data format: Avro\n\nEvent Hubs Capture uses Avro format for the data it captures. Avro is a row-oriented format that is suitable for various data types, it's compact, fast, binary, and enables efficient and fast serialization of data. This makes it a good choice for Event Hubs Capture."
      },
      {
        "date": "2024-08-09T04:32:00.000Z",
        "voteCount": 3,
        "content": "This question appeared in the exam, August 2024, I gave this same answer provided here. I scored 870"
      },
      {
        "date": "2024-06-24T05:38:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct.\nAzure Data Lake Storage Gen2 is designed for big data analytics and is highly scalable, making it suitable for storing large volumes of event data.\nAvro is a compact, fast binary format supported by Event Hubs Capture, optimized for efficiency and performance in data streaming scenarios."
      },
      {
        "date": "2024-06-24T05:39:00.000Z",
        "voteCount": 1,
        "content": "why not other options ?\nStorage Type:\nPremium block blobs: Designed for high-performance workloads but not optimized for big data analytics and hierarchical storage.\nPremium file shares: Suitable for high-performance file sharing but lacks the scalability and analytics features of ADLS Gen2.\nData Format:\nApache Parquet: Columnar storage format optimized for read-heavy operations, but not natively supported by Event Hubs Capture.\nJSON: Readable and widely used, but less efficient in terms of storage and performance compared to Avro for streaming data."
      },
      {
        "date": "2024-05-21T02:44:00.000Z",
        "voteCount": 1,
        "content": "Response from Gemini AI:\n\nStorage Type: Event Hubs Capture allows captured data to be written to either Azure Blob Storage or Azure Data Lake Storage Gen2. However, for cold path processing scenarios, which involve analyzing historical data, Azure Data Lake Storage Gen2 is the more suitable choice. It's designed for big data analytics workloads and offers better performance and scalability for working with large datasets captured from event hubs.\n\nInbound Data Format: Event Hubs Capture uses Avro format for the captured data. Avro is a widely used open-source data format specifically designed for data exchange. It's a row-oriented, binary format that provides rich data structures with inline schema definition. This makes it efficient for storage and easy for various analytics tools and reporting systems to understand and process the captured event data."
      },
      {
        "date": "2024-03-08T20:47:00.000Z",
        "voteCount": 1,
        "content": "Based on Gemini AI:\nBox 2-"
      },
      {
        "date": "2024-03-08T20:49:00.000Z",
        "voteCount": 1,
        "content": "Box 2 - Avro\n\nExplanation:\n\nSupported Formats: While Event Hubs itself can handle various data formats including JSON, Avro, and Apache Parquet, Event Hubs Capture specifically writes data in Apache Avro format. This format is well-suited for cold path processing due to its:\n - Compact nature\n - Speed\n - Ability to represent complex data structures\n - Inline schema definition for easier data understanding\n\nWhy not JSON or Parquet?\nJSON: While JSON is a common data interchange format, it can be less efficient for cold path processing due to its larger size compared to Avro.\nParquet: Although Azure Stream Analytics can be used to capture Event Hubs data in Parquet format, Event Hubs Capture itself doesn't directly support Parquet."
      },
      {
        "date": "2024-03-03T13:27:00.000Z",
        "voteCount": 1,
        "content": "For streaming Avro is made for it compared to Parquet as it row oriented format so if you have batch in the question =&gt; Parquet, Streaming =&gt; Avro"
      },
      {
        "date": "2023-11-26T23:55:00.000Z",
        "voteCount": 1,
        "content": "Correct given answers.\n\n\"Azure Data Lake Storage Gen 2\" or \"Azure Storage Account\" can be used as a Storage Account via the Portal. Just to be sure, I created a Premium Storage Account (blob) and this is - NOT - a valid option to store the Captured files.\n\nBy default Avro is selected via the Portal. Also Parquet and Delta Lake (preview) are supported via the Portal."
      },
      {
        "date": "2023-11-26T12:16:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer:\n- Azure Data Lake Storage Gen2\n- Apache Parquet\n\nI am thinkin rather best-practice driven rather than looking into docs. If I'd set up an analytics service in Azure, I'd prefer Databricks. In Databricks I am always working with Parquet files rather than Avro.\n\nAvro is often used in case of streaming. Single messages can be compressed and a schema is still enforced. But the question is only about analytics.\n\nWhile as I am preferring Avro in context of streaming, I am preferring Parquet for data analysis."
      },
      {
        "date": "2023-10-26T18:58:00.000Z",
        "voteCount": 2,
        "content": "avro or apache parquet both are correct answer. however apache parquet is columnar storage format that provides efficient commpression and query performance."
      },
      {
        "date": "2023-09-29T07:54:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works\n\nThe capture can be in Parquet format however if you use no code editor which is outside the scope of the question."
      },
      {
        "date": "2023-09-07T22:35:00.000Z",
        "voteCount": 2,
        "content": "Capture data to ADLS Gen2 in Parquet format: https://learn.microsoft.com/en-us/azure/stream-analytics/event-hubs-parquet-capture-tutorial"
      },
      {
        "date": "2023-09-26T09:16:00.000Z",
        "voteCount": 1,
        "content": "Capture data to ADLS Gen2 in Parquet format tile."
      },
      {
        "date": "2023-07-03T02:00:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Lake Storage Gen2 is not a premium storage account. It is a storage account type that provides a unified storage solution for both structured and unstructured data. As Premium Storage options are not supported by Event Hubs Capture"
      },
      {
        "date": "2023-05-31T11:49:00.000Z",
        "voteCount": 4,
        "content": "To support Event Hubs Capture, the appropriate Azure storage type is Azure Data Lake Storage Gen2. Event Hubs Capture is specifically designed to write captured events directly to Azure Data Lake Storage Gen2, providing a durable and scalable storage solution.\n\nRegarding the inbound data format that the reporting system must support, the data format supported by Event Hubs Capture is Apache Avro. Event Hubs Capture writes the captured events in Avro format by default. Therefore, the reporting system should be able to consume and process data in the Apache Avro format.\n\nSo the correct selections would be:\n\nStorage Type: Azure Data Lake Storage Gen2\nData Format: Apache Avro"
      },
      {
        "date": "2023-05-29T02:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is not correct I side and agree with the explanation by Sanaie. \nAzure Data Lake Storage Gen2, as premium storage options are not supported by Event Hubs Capture.\n\nApache Parquet is better suited for data analytics compared to Avro and JSON. \nAvro and Parquet are the only supported formats I have seen in the documentation.\nAs we have an analytics case here I would suggest Parquet. \nAvro, however, is the default option and doesn't need any specific configurations."
      },
      {
        "date": "2023-04-24T05:38:00.000Z",
        "voteCount": 2,
        "content": "Event hub writes only in Avro format\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview"
      },
      {
        "date": "2023-05-12T10:08:00.000Z",
        "voteCount": 2,
        "content": "That's not true, it can also write in Parquet if you use the no code editor.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works"
      },
      {
        "date": "2023-04-19T05:44:00.000Z",
        "voteCount": 6,
        "content": "Correct answers.\n\nhttps://learn.microsoft.com/en-us/azure/event-hubs/event-hubs-capture-overview#how-event-hubs-capture-works\n\nAlso:\n\nThe destination storage (Azure Storage or Azure Data Lake Storage) account must be in the same subscription as the event hub.\nEvent Hubs doesn't support capturing events in a premium storage account."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/115064-exam-az-305-topic-2-question-29-discussion/",
    "body": "You have the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/az-305/image195.png\"><br><br>CDB1 hosts a container that stores continuously updated operational data.<br><br>You are designing a solution that will use AS1 to analyze the operational data daily.<br><br>You need to recommend a solution to analyze the data without affecting the performance of the operational data store.<br><br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics with PolyBase data loading",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Link for Azure Cosmos DB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Cosmos DB change feed"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-13T09:35:00.000Z",
        "voteCount": 20,
        "content": "The correct answer is C. Azure Synapse Link for Azure Cosmos DB.\n\nAzure Synapse Link for Azure Cosmos DB creates a tight integration between Azure Cosmos DB and Azure Synapse Analytics, allowing you to run near real-time analytics over operational data in Azure Cosmos DB. It creates a \"no-ETL\" (Extract, Transform, Load) environment that allows you to analyze data directly without affecting the performance of the transactional workload, which is exactly what is required in this scenario.\n\nA. Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors would require ETL operations which might impact the performance of the operational data store.\n\nB. Azure Synapse Analytics with PolyBase data loading is more appropriate for loading data from external data sources such as Azure Blob Storage or Azure Data Lake Storage. \n\nD. Azure Cosmos DB change feed doesn't directly address the need for analytics without affecting the performance of the operational data store."
      },
      {
        "date": "2023-09-19T06:19:00.000Z",
        "voteCount": 6,
        "content": "Azure Synapse Link for Azure Cosmos DB.\n\nAzure Synapse Link for Azure Cosmos DB is a cloud-native hybrid transactional and analytical processing (HTAP) capability that enables near real time analytics over operational data in Azure Cosmos DB. Azure Synapse Link creates a tight seamless integration between Azure Cosmos DB and Azure Synapse Analytics. It enables customers to run near real-time analytics over their operational data with full performance isolation from their transactional workloads and without an ETL pipeline\n\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link"
      },
      {
        "date": "2024-10-15T00:56:00.000Z",
        "voteCount": 1,
        "content": "C is correct\nhttps://learn.microsoft.com/en-us/azure/cosmos-db/synapse-link"
      },
      {
        "date": "2024-08-09T04:32:00.000Z",
        "voteCount": 1,
        "content": "This question appeared in the exam, August 2024, I gave this same answer as listed here. I scored 870"
      },
      {
        "date": "2024-06-24T05:41:00.000Z",
        "voteCount": 1,
        "content": "Azure Synapse Link enables real-time analytics on operational data stored in Azure Cosmos DB without impacting the performance of the operational database.\nIt allows seamless integration between Azure Cosmos DB and Azure Synapse Analytics, providing a hybrid transactional and analytical processing (HTAP) capability."
      },
      {
        "date": "2024-06-24T05:42:00.000Z",
        "voteCount": 1,
        "content": "A. Azure Data Factory with Azure Cosmos DB and Azure Synapse Analytics connectors\nReason: While Azure Data Factory can move data from Cosmos DB to Synapse Analytics, it typically involves batch processing, which may not be real-time and could affect operational data performance during the ETL process.\nB. Azure Synapse Analytics with PolyBase data loading\nReason: PolyBase is excellent for loading large volumes of data but is designed for structured data and batch loading, not continuous real-time analysis, and may impact the performance during large data transfers.\nD. Azure Cosmos DB change feed\nReason: The change feed provides a continuous log of changes but requires additional processing to integrate with Synapse Analytics, which can complicate the solution and impact performance."
      },
      {
        "date": "2024-01-26T22:30:00.000Z",
        "voteCount": 3,
        "content": "the same as topic 1st question 26."
      },
      {
        "date": "2023-11-12T11:16:00.000Z",
        "voteCount": 2,
        "content": "same question as before i forgot the number"
      },
      {
        "date": "2023-09-25T14:13:00.000Z",
        "voteCount": 4,
        "content": "I agree with others &amp; their logic.  Just adding my vote."
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/121187-exam-az-305-topic-2-question-30-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription. The subscription contains an Azure SQL managed instance that stores employee details, including social security numbers and phone numbers.<br><br>You need to configure the managed instance to meet the following requirements:<br><br>\u2022\tThe helpdesk team must see only the last four digits of an employee\u2019s phone number.<br>\u2022\tCloud administrators must be prevented from seeing the employee\u2019s social security numbers.<br><br>What should you enable for each column in the managed instance? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image205.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image206.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-22T08:18:00.000Z",
        "voteCount": 26,
        "content": "I agree with the answer.\n\nDynamic data masking helps prevent unauthorized access to sensitive data by enabling customers to designate how much of the sensitive data to reveal with minimal effect on the application layer.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview\n\nAlways Encrypted is a feature designed to protect sensitive data, such as credit card numbers or national/regional identification numbers (for example, U.S. social security numbers), stored in Azure SQL Database, Azure SQL Managed Instance, and SQL Server databases.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/encryption/always-encrypted-database-engine"
      },
      {
        "date": "2024-09-14T07:23:00.000Z",
        "voteCount": 1,
        "content": "Dynamic data masking\nAlways Encrypted"
      },
      {
        "date": "2024-06-24T05:45:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct.\nDynamic Data Masking: Masks sensitive data in the result set of a query, limiting data exposure without changing the data in the database.\nAlways Encrypted: Ensures sensitive data is encrypted both at rest and in motion, and can only be decrypted by the client application that has the encryption keys."
      },
      {
        "date": "2024-03-08T21:04:00.000Z",
        "voteCount": 2,
        "content": "Based on Gemini AI:\nBox2 for SSN: \"Always Encrypted\": This feature encrypts data at rest and in transit, ensuring it remains unreadable even by Azure administrators with access to the database server. The decryption keys are stored in Azure Key Vault, separate from the database, requiring stricter access controls.\n\nWhy not the other options?\n- Column Encryption: While it encrypts data at rest, the decryption key is stored within the database server, potentially accessible to cloud administrators.\n- Dynamic Data Masking: This technique masks data within the query results but doesn't encrypt the data itself. Cloud administrators could still access the underlying data."
      },
      {
        "date": "2023-10-14T12:45:00.000Z",
        "voteCount": 1,
        "content": "Can't Dynamic Data Masking also be used to mask the SSN's?"
      },
      {
        "date": "2023-10-20T00:48:00.000Z",
        "voteCount": 3,
        "content": "The wording of the question is bad.  The question only states \"Cloud administrators must be prevented from seeing the employee\u2019s social security numbers\", but not mentions about others.  So I also thought it should also be dynamic masking."
      },
      {
        "date": "2024-01-21T13:42:00.000Z",
        "voteCount": 1,
        "content": "Me too"
      },
      {
        "date": "2023-12-07T13:00:00.000Z",
        "voteCount": 6,
        "content": "DDM won't address the SSN requirement: \"Administrative users and roles can always view unmasked data via the CONTROL permission, which includes both the ALTER ANY MASK and UNMASK permission. Administrative users or roles such as sysadmin, serveradmin, or db_owner have CONTROL permissions on the database by design, and can view unmasked data.\"\n\nReference:\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16#permissions"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/121178-exam-az-305-topic-2-question-31-discussion/",
    "body": "You plan to use an Azure Storage account to store data assets.<br><br>You need to recommend a solution that meets the following requirements:<br><br>\u2022\tSupports immutable storage<br>\u2022\tDisables anonymous access to the storage account<br>\u2022\tSupports access control list (ACL)-based Azure AD permissions<br><br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Lake Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure NetApp Files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Blob Storage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-29T09:14:00.000Z",
        "voteCount": 46,
        "content": "In terms of supporting immutable storage, both Azure Data Lake storage and Azure Blob storage are correct. But ACL is supported by Azure Data Lake storage, not supported by Azure Blob storage. See below link.\n\nhttps://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-comparison-with-blob-storage\n\nSo the correct answer is B."
      },
      {
        "date": "2024-05-17T12:18:00.000Z",
        "voteCount": 5,
        "content": "The above link is outdated. Here is the new one: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\n\nAnswer is correct."
      },
      {
        "date": "2023-09-22T09:35:00.000Z",
        "voteCount": 11,
        "content": "Azure Data Lake Storage.\n\"Azure Data Lake Storage Gen2 implements an access control model that supports both Azure role-based access control (Azure RBAC) and POSIX-like access control lists (ACLs).\"\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\n\"Immutable storage for Azure Data Lake Storage is now generally available.\"\nhttps://azure.microsoft.com/en-us/updates/immutable-storage-for-azure-data-lake-storage-is-now-generally-available/"
      },
      {
        "date": "2023-09-22T23:00:00.000Z",
        "voteCount": 1,
        "content": "I\u00b4m agree"
      },
      {
        "date": "2024-06-24T06:54:00.000Z",
        "voteCount": 2,
        "content": "In the context of storing data assets in an Azure Storage account, the need is to ensure that data is immutable, anonymous access is disabled, and that Azure AD-based access control is supported. The recommended solution to meet these requirements is Azure Blob Storage. This service lets you configure immutable storage policies, enforce security with mandatory authentication, and apply granular permissions using Azure AD ACLs. Alternatives such as Azure Files, Azure Data Lake Storage and Azure NetApp Files were considered, but they do not offer immutable access for example."
      },
      {
        "date": "2024-06-24T05:48:00.000Z",
        "voteCount": 2,
        "content": "While Azure Blob Storage also meets these requirements, ADLS Gen2 is specifically designed for big data analytics, providing additional capabilities such as hierarchical namespace and optimized performance for large datasets."
      },
      {
        "date": "2024-09-02T03:58:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Lake Storage isn't a dedicated service or account type. Instead, it's implemented as a set of capabilities that you use with the Blob Storage service of your Azure Storage account, ref: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction. Hope this helps"
      },
      {
        "date": "2024-06-03T00:07:00.000Z",
        "voteCount": 1,
        "content": "Both Azure Data Lake Storage and Azure Blob Storage do support what stated in the request, and anonymous access is disabled by default in both types of storage. However, in my opinion, the question is missing a couple of important hints that could make us head towards a solution or the other one: 1) Cost-effective solution - in this case, the choice would be Azure Blob Storage; 2) Optimised for big data analytics workloads - that would hint for Azure Data Lake Storage. Maybe the question in the real exam is different, so an assumption must be made: minimise costs - which means that the only viable choice is D. Azure Blob Storage."
      },
      {
        "date": "2024-06-01T11:11:00.000Z",
        "voteCount": 2,
        "content": "You should include Azure Blob Storage in the recommendation. Here\u2019s why:\n\nImmutable Storage: Azure Blob Storage supports immutable storage through features like Blob Immutability Policies, which allow you to set policies to make data immutable for a specified period.\nDisables Anonymous Access: Azure Blob Storage allows you to configure access settings to disable anonymous access to the storage account.\nACL-Based Azure AD Permissions: Azure Blob Storage supports ACL-based permissions with Azure AD, allowing fine-grained access control to blobs and containers.\nTherefore, the correcT is D"
      },
      {
        "date": "2024-04-29T17:03:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer B is correct as ADLS provides all features required\nGolden rule if you see choice between ADLS and ABS, 9 out of 10 you are safe to choose ADSL as it is Gen 2 and it is the future going forward as ABS v2 will see the same faith as ABS v1"
      },
      {
        "date": "2024-03-30T15:01:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Lake Storage."
      },
      {
        "date": "2024-03-20T08:12:00.000Z",
        "voteCount": 2,
        "content": "For your Azure Storage account that needs to support immutable storage, disable anonymous access, and support ACL-based Azure AD permissions, I recommend using Azure Data Lake Storage Gen2. Here\u2019s why:\n\nImmutable Storage: Azure Data Lake Storage Gen2 supports immutable storage, which is essential for scenarios requiring write-once-read-many (WORM) policies1.\nDisabling Anonymous Access: It allows you to disable anonymous access to the storage account, ensuring that data access is restricted to authenticated and authorized users only1.\nACL-based Azure AD Permissions: Azure Data Lake Storage Gen2 implements an access control model that supports both Azure role-based access control (Azure RBAC) and POSIX-like access control lists (ACLs), which are crucial for fine-grained access control1.\nThis solution aligns with your requirements and provides a robust and secure environment for storing your data assets.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2024-03-15T14:56:00.000Z",
        "voteCount": 3,
        "content": "B\nazure blob storage does not support ACL"
      },
      {
        "date": "2024-03-14T15:14:00.000Z",
        "voteCount": 1,
        "content": "The recommended solution that meets the provided requirements is Azure Blob Storage.\n\nHere's why:\n\n    Supports Immutable Storage: Azure Blob Storage supports the \"Immutable Blob\" feature, which allows you to store data in a WORM (Write Once, Read Many) state. Once data is written to an immutable blob, it cannot be modified or deleted for a specified retention period, making it suitable for compliance and regulatory requirements.\n\n    Disables Anonymous Access: Azure Blob Storage allows you to disable anonymous access to the storage account, ensuring that only authorized users or applications can access the stored data. This enhances security by preventing unauthorized access to your data assets.\n\n    Supports ACL-based Azure AD Permissions: Azure Blob Storage supports access control lists (ACLs) for managing permissions on blobs and containers. You can grant access to users and groups in Azure Active Directory (Azure AD) and define granular permissions using ACLs, providing fine-grained control over who can access the data stored in the storage account."
      },
      {
        "date": "2024-03-05T15:44:00.000Z",
        "voteCount": 1,
        "content": "this disables anonymous access: https://learn.microsoft.com/en-us/azure/storage/blobs/anonymous-read-access-prevent?tabs=portal\n\nblob supports immutable storage: https://learn.microsoft.com/en-us/azure/storage/blobs/immutable-storage-overview"
      },
      {
        "date": "2024-02-04T05:12:00.000Z",
        "voteCount": 2,
        "content": "This discussion makes feel that this question is an overkill requiring very sowcific knowledge. After getting through discussion and searching internet I still don't know if blob supports posix ACL, only blob, only adls or both?"
      },
      {
        "date": "2024-01-22T04:08:00.000Z",
        "voteCount": 1,
        "content": "I'm inclined to agree with Houzer. Azure Data Lake seems overkill for the required solution. \n\nI also did ask the question to Co-pilot and the answer was D, based on Blob hitting all requirements at a lower transactional cost. Not a great question, as Gen2 for ADL is not specified either.\n\nACL is supported by both Azure Blob Storage and Azure Data Lake Storage Gen2, but with some differences. Azure Blob Storage supports container-level ACLs, which apply to all the blobs in the container. Azure Data Lake Storage Gen2 supports file-level and directory-level ACLs, which are more granular and flexible."
      },
      {
        "date": "2024-05-12T10:53:00.000Z",
        "voteCount": 2,
        "content": "Yes, you are correct.\n\nAzure Blob Storage does support container-level access control through shared access signatures (SAS) and Azure role-based access control (RBAC). However, these controls apply at the container level and not at the individual blob level.\n\nOn the other hand, Azure Data Lake Storage Gen2 supports POSIX-like access control lists (ACLs) at the file and directory level. This allows for more granular control over who can access individual files and directories, which can be important for data security and compliance.\n\nSo, if you need fine-grained, ACL-based Azure AD permissions at the file or directory level, Azure Data Lake Storage Gen2 would be the better choice."
      },
      {
        "date": "2024-01-11T02:03:00.000Z",
        "voteCount": 1,
        "content": "I think I'll have to go with D on this one - Azure Bob Storage.\n\nAzure Data Lake Storage could also be a viable option for storing data assets, especially for big data analytics workloads. However, there are a few reasons why Azure Blob Storage might be more suitable for your specific requirements:\n\n- Both Azure Data Lake Storage and Azure Blob Storage support immutable storage1. However, Azure Blob Storage provides more flexibility with time-based retention policies.\n\n- Azure Blob Storage does not permit anonymous access by default, which aligns with the requirement to disable anonymous access."
      },
      {
        "date": "2024-01-11T02:03:00.000Z",
        "voteCount": 3,
        "content": "While Azure Data Lake Storage supports POSIX ACLs, Azure Blob Storage supports AAD based access control. This can provide more granular control over permissions.\n\nAzure Blob Storage is more cost-effective in the given scenario, especially if you\u2019re not dealing with big data analytics workloads for which Azure Data Lake Storage is optimised, so if the use case doesn\u2019t involve such workloads, you might not fully utilise its capabilities - why pay for something you don't use?"
      },
      {
        "date": "2023-12-29T12:56:00.000Z",
        "voteCount": 1,
        "content": "Blob supports ACL, is cheaper than Data Lake"
      },
      {
        "date": "2023-12-14T16:09:00.000Z",
        "voteCount": 1,
        "content": "Definitely D"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/121180-exam-az-305-topic-2-question-32-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are designing a storage solution that will ingest, store, and analyze petabytes (PBs) of structured, semi-structured, and unstructured text data. The analyzed data will be offloaded to Azure Data Lake Storage Gen2 for long-term retention.<br><br>You need to recommend a storage and analytics solution that meets the following requirements:<br>\u2022\tStores the processed data<br>\u2022\tProvides interactive analytics<br>\u2022\tSupports manual scaling, built-in autoscaling, and custom autoscaling<br><br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image207.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image208.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-29T09:45:00.000Z",
        "voteCount": 44,
        "content": "Data Lake Analytics does not work with  ADLS Gen2 and it will be retired on February 29, 2024. So I support the answer for Data Explorer + KQL.\n\nhttps://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-overview"
      },
      {
        "date": "2023-09-24T15:15:00.000Z",
        "voteCount": 25,
        "content": "Sept 24 2023\nData Explorer + KQL\nThis is my own research &amp; logic (Google + my brain), not a ChatGPT answer.  I haven't taken the exam yet.\n\nThe scenario criteria maps directly to Data Explorer \u201cDecision Criteria\u201d &amp; tree in this link:   When to use Data Explorer https://learn.microsoft.com/en-us/training/modules/intro-to-azure-data-explorer/4-when-to-use-azure-data-explorer  \n\nThe scenario does not include anything that prevents the use of DE based on that link &amp; Data Explorer CAN scale to PB (it's a big data platform)\n\nThe scenario doesn\u2019t specifically say \u201creal-time\u201d or say what\u2019s producing the data, but ingesting/storing PBs of data + interactive analytics ...  to me that means \"streaming + real-time analytics\".\n\nand KQL (Kusto Query Language) because that's the query language for Data Explorer"
      },
      {
        "date": "2023-09-25T06:22:00.000Z",
        "voteCount": 2,
        "content": "Actually, I think you are right, I just reviewed the 'Data Explorer' documentation and in this case I think the key is in the autoscaling requirement\n\nhttps://learn.microsoft.com/en-us/azure/data-explorer/manage-cluster-horizontal-scaling\n\nSo I think it should be Data Explorer and KQL"
      },
      {
        "date": "2024-10-15T01:03:00.000Z",
        "voteCount": 1,
        "content": "Agreed. Data Explorer specifically states it can ingest petabytes of data in it's overview and the language for that is KQL\nhttps://learn.microsoft.com/en-us/azure/data-explorer/data-explorer-overview#what-makes-azure-data-explorer-unique"
      },
      {
        "date": "2024-08-09T04:33:00.000Z",
        "voteCount": 3,
        "content": "This question appeared in the exam, August 2024, I gave the answer Azure Data Explorer for box 1 and answered KQL for box 2. I scored 870"
      },
      {
        "date": "2024-08-25T14:12:00.000Z",
        "voteCount": 1,
        "content": "Data lake analytics retired on Feb 23, 2024 https://learn.microsoft.com/en-us/previous-versions/azure/data-lake-analytics/data-lake-analytics-overview.\n\nMicrosoft wouldn't accept a correct answer that contained a depricated product or feature."
      },
      {
        "date": "2024-06-24T07:07:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is correct.\nU-SQL (Unified Structured Query Language) is a query language developed by Microsoft for large-scale distributed and parallel data processing. It combines the familiarity and ease of use of SQL with the ability to handle structured and unstructured data. U-SQL is designed to work in big data environments like Azure Data Lake Analytics, enabling developers and analysts to write powerful queries that efficiently process data from multiple sources and formats."
      },
      {
        "date": "2024-06-24T05:51:00.000Z",
        "voteCount": 1,
        "content": "Azure Data Explorer: Highly optimized for real-time and ad-hoc data exploration, especially for large volumes of data.\nKQL: The primary query language for Azure Data Explorer, offering powerful and efficient querying capabilities."
      },
      {
        "date": "2024-06-24T05:51:00.000Z",
        "voteCount": 1,
        "content": "Why Not Other Options:\nFor Storage and Interactive Analytics:\nAzure Data Lake Analytics: Ideal for on-demand analytics jobs using U-SQL but not as interactive or real-time as Azure Data Explorer.\nLog Analytics: Primarily for monitoring and operational data, not optimized for large-scale data storage and interactive analytics.\nQuery Language:\nTransact-SQL: Best suited for relational databases like SQL Server and Azure SQL Database; not used with Azure Data Explorer.\nU-SQL: Specific to Azure Data Lake Analytics and not as suitable for interactive, real-time querying like KQL"
      },
      {
        "date": "2024-04-29T17:17:00.000Z",
        "voteCount": 1,
        "content": "I would go for \n1. Data Explorer as meets the requirements and built for it\n2. KQL\n\nAzure Data Explorer (ADX) is a fast and highly scalable data exploration and analytics service that is designed to ingest, analyze, and visualize large volumes of structured and unstructured data in real-time. ADX provides a powerful query language that allows users to perform ad-hoc analysis and explore large datasets quickly. It is commonly used for log analytics, IoT telemetry data analysis, and other real-time analytics use cases."
      },
      {
        "date": "2024-03-20T13:08:00.000Z",
        "voteCount": 1,
        "content": "Final Answer:\n1. Data Explorer \n2. KQL"
      },
      {
        "date": "2024-03-01T06:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: Data Explorer + KQL\n\nhttps://learn.microsoft.com/en-us/azure/data-explorer/data-explorer-overview#when-should-you-use-azure-data-explorer\n\nWhen should you use Azure Data Explorer?\nUse the following questions to help decide if Azure Data Explorer is right for your use case:\n\nInteractive analytics: Is interactive analysis part of the solution? For example, aggregation, correlation, or anomaly detection.\nVariety, Velocity, Volume: Is your schema diverse? Do you need to ingest massive amounts of data in near real-time?\nData organization: Do you want to analyze raw data? For example, not fully curated star schema.\nQuery concurrency: Will multiple users or processes use Azure Data Explorer?\nBuild vs Buy: Do you plan on customizing your data platform?"
      },
      {
        "date": "2023-11-26T10:14:00.000Z",
        "voteCount": 4,
        "content": "Analytics: Azure Data Explorer + KQL\n\nAzure Data Explorer provides interactive analytics. It allows you to examine structured, semi-structured, and unstructured data with improvised, interactive, fast queries4. You can use Azure Data Explorer Web UI, web client for Azure Data Explorer, or Kusto.Explorer, a rich windows client for Azure Data Explorer. To connect to your Azure Data Explorer cluster, you can use Jupyter notebooks, Spark connector, any TDS-compliant SQL client, and JDBC and ODBC connections"
      },
      {
        "date": "2023-11-22T04:29:00.000Z",
        "voteCount": 1,
        "content": "ASKED% IMPORTant"
      },
      {
        "date": "2023-10-19T08:09:00.000Z",
        "voteCount": 4,
        "content": "Data Lake Analytics doesn't work with Azure Data Lake Storage Gen2 yet until further notice.\n\nhttps://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-overview#works-with-all-your-azure-data"
      },
      {
        "date": "2023-09-22T09:44:00.000Z",
        "voteCount": 3,
        "content": "Right answer, Azure Data Lake Analytics with U-SQL.\n\"Data Lake Analytics dynamically provisions resources and lets you do analytics on terabytes to petabytes of data.\"\n\"Data Lake Analytics includes U-SQL, a query language that extends the familiar, simple, declarative nature of SQL with the expressive power of C#.\"\nhttps://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-overview\n\nData Explorer is not an answer because it can only elastically scale to terabytes of data."
      },
      {
        "date": "2023-09-24T15:27:00.000Z",
        "voteCount": 1,
        "content": "You are incorrect with the statement that \"Data Explorer is not an answer because it only scales elastically to terabytes of data\". That sounds like a ChatGPT explanation.\n\nThe reason I say that is the Data Explorer marketing material says that it \"ingests TBs of data within minutes\" .... ready more ... it also says that it handles PBs of data overall for ingestion/storage/analysis\n\nThis is the Data Explorer marketing page:  https: //azure.microsoft.com/en-us/products/data-explorer#:~:text=Scale%20to%20petabytes%20of%20streaming,to%2012%20Mbps%20per%20core\n\nData Explorer is a big data analytics platform.  Here's the Overview page https://learn.microsoft.com/en-us/azure/data-explorer/data-explorer-overview\n\nSee my original comment elsewhere"
      },
      {
        "date": "2023-09-22T06:02:00.000Z",
        "voteCount": 4,
        "content": "Data Lake Analytics will soon be retired so I guess this question will be removed from the exam but.\nLanguage for Data Lake Analytics is U-SQL.\n\nhttps://learn.microsoft.com/en-us/azure/data-lake-analytics/data-lake-analytics-overview"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/121181-exam-az-305-topic-2-question-33-discussion/",
    "body": "HOTSPOT<br> -<br><br>You plan to use Azure SQL as a database platform.<br><br>You need to recommend an Azure SQL product and service tier that meets the following requirements:<br>\u2022\tAutomatically scales compute resources based on the workload demand<br>\u2022\tProvides per second billing<br><br>What should you recommend? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image209.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image210.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-09-22T09:54:00.000Z",
        "voteCount": 20,
        "content": "I agree with the answer.\n\"Serverless is a compute tier for single databases in Azure SQL Database that automatically scales compute based on workload demand and bills for the amount of compute used per second. The serverless compute tier is available in the General Purpose service tier and currently in preview in the Hyperscale service tier.\"\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview"
      },
      {
        "date": "2024-03-08T21:33:00.000Z",
        "voteCount": 3,
        "content": "Correct. Reference highlighting the exact answer:\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql&amp;tabs=general-purpose#:~:text=Serverless%20is%20a%20compute%20tier%20for%20single%20databases%20in%20Azure%20SQL%20Database%20that%20automatically%20scales%20compute%20based%20on%20workload%20demand%20and%20bills%20for%20the%20amount%20of%20compute%20used%20per%20second."
      },
      {
        "date": "2023-11-12T11:26:00.000Z",
        "voteCount": 8,
        "content": "i dont see serverless in the question why ppl talk about that?\nis it possible that the question changed?"
      },
      {
        "date": "2023-11-12T23:31:00.000Z",
        "voteCount": 7,
        "content": "@thamaster no, they mean General Purpose service tier. Reading the question and documentation the most suitable answer is related to Serverless compute tier which can be achieved choosing the General purpose Service Tier."
      },
      {
        "date": "2024-09-17T21:54:00.000Z",
        "voteCount": 2,
        "content": "I \"love\" how Microsoft came up with the \"General Purpose\" and \"Standard\" terms, both sort of suggesting more or less the same thing, but some products use one of them and other products use the other one... Microsoft, are you trying DELIBERATELY to trick and confuse us?"
      },
      {
        "date": "2024-09-14T07:27:00.000Z",
        "voteCount": 1,
        "content": "A single Azure SQL database\nService tier: Hyperscale"
      },
      {
        "date": "2024-09-10T20:46:00.000Z",
        "voteCount": 1,
        "content": "-Single Azure SQL Database\n-Hyperscale"
      },
      {
        "date": "2024-08-23T13:55:00.000Z",
        "voteCount": 1,
        "content": "Serverless is a compute tier for single databases in Azure SQL Database that automatically scales compute based on workload demand and bills for the amount of compute used per second. The serverless compute tier also automatically pauses databases during inactive periods when only storage is billed and automatically resumes databases when activity returns. The serverless compute tier is available in the General Purpose service tier and the Hyperscale service tier."
      },
      {
        "date": "2024-06-24T05:53:00.000Z",
        "voteCount": 2,
        "content": "the given answer is correct.\nA single Azure SQL database: Provides the flexibility to scale compute resources independently.\nGeneral Purpose: This service tier supports serverless configuration, which automatically scales compute resources based on demand and provides per-second billing."
      },
      {
        "date": "2024-06-24T05:54:00.000Z",
        "voteCount": 2,
        "content": "Why Not Other Options:\nAzure SQL Product:\nAn Azure SQL Database elastic pool: Designed for managing multiple databases with varying and unpredictable usage patterns. It doesn't automatically scale individual databases; instead, it manages resource sharing among multiple databases.\nAzure SQL Managed Instance: Provides near 100% compatibility with SQL Server, but does not support serverless tier for automatic scaling.\nService Tier:\nBasic, Business Critical, Standard: These tiers do not support the serverless configuration needed for automatic scaling and per-second billing.\nHyperscale: Optimized for large databases and fast scaling, but does not support serverless auto-scaling.\nConclusion:\nThe combination of A single Azure SQL database with the General Purpose tier is the best fit to meet the requirements of automatic scaling based on workload demand and per-second billing."
      },
      {
        "date": "2024-10-13T23:51:00.000Z",
        "voteCount": 1,
        "content": "Hyperscale does allows for serverless auto-scaling. 'Automatic scale-up, scale-down, and billing for compute based on usage with serverless compute.' https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql. Check under Hyperscale Capabilities section"
      },
      {
        "date": "2024-05-04T02:49:00.000Z",
        "voteCount": 1,
        "content": "Azure SQL Product: You should choose \u201cA single Azure SQL database\u201d. This product is designed to automatically scale compute resources based on the workload demand. \nService Tier: The \u201cHyperscale\u201d service tier would be the best fit. It provides highly scalable storage and compute performance that can auto-scale up to 100 TB per database, which aligns with the requirement of automatically scaling compute resources. Additionally, Hyperscale service tier provides rapid scaling capabilities without downtime."
      },
      {
        "date": "2024-05-01T17:07:00.000Z",
        "voteCount": 4,
        "content": "This question seems incomplete. Both General Purpose and Hyperscale serverless compute tiers work here. Something is missing to the requirements of the question. I wonder if this is an old question and the tiers have changed since it was in circulation. Anyway, pick your poison. General Purpose and Hyperscale technically both work here. Single Azure SQL DB in the only only that supports per-second billing. The others require provisioned compute tiers. Cheers"
      },
      {
        "date": "2024-05-27T10:25:00.000Z",
        "voteCount": 1,
        "content": "The question is ok. If they don't provide more details it doesn't make any sense to choose more expensive solution. By default choose the cheapest option which is General Purpose here."
      },
      {
        "date": "2024-04-09T04:05:00.000Z",
        "voteCount": 2,
        "content": "I tend to disagree with answer and those who say Hyperscale is NOT publicly available,  obviously you are NOT practicing what you study!! can easily be checked by creating SQL DB in azure under vCore compute\n\nanswer is\nSingle  SQL DB - as all agreed\nHyperscale service tier and below is why (NOTE no mention of cost as condition)\nThe widest variety of workloads, including those workloads with highly scalable storage and read-scale requirements. Offers higher resilience to failures by allowing configuration of more than one high availability secondary replica. The Hyperscale service tier is suitable for all workload types. Its cloud native architecture provides independently scalable compute and storage to support the widest variety of traditional and modern applications. Compute and storage resources in Hyperscale substantially exceed the resources available in the General Purpose and Business Critical tiers \nFollow below"
      },
      {
        "date": "2024-09-18T00:37:00.000Z",
        "voteCount": 1,
        "content": "Practising Hyperscale just for the sake of passing the exam would be a bit of an overkill, don't you think?"
      },
      {
        "date": "2024-04-09T04:05:00.000Z",
        "voteCount": 1,
        "content": "Keywords are below:\n* widest variety of workloads, including those workloads with highly scalable storage and read-scale requirements\n* Its cloud native architecture provides independently scalable compute and storage to support the widest variety of traditional and modern applications\n* Compute and storage resources in Hyperscale substantially exceed the resources available in the General Purpose and Business Critical tiers\n\nBelow is link shows it is available in serveless compute\nhttps://techcommunity.microsoft.com/t5/azure-sql-blog/general-availability-serverless-for-hyperscale-in-azure-sql/ba-p/4053589"
      },
      {
        "date": "2023-12-08T19:11:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql&amp;tabs=general-purpose\n\nBased on above link, both answers are correct."
      },
      {
        "date": "2023-11-26T10:20:00.000Z",
        "voteCount": 4,
        "content": "1: SINGLE SQL DATABASE\n2: GENERAL PURPOSE SERVICE TIER"
      },
      {
        "date": "2023-11-08T11:09:00.000Z",
        "voteCount": 2,
        "content": "Both \"General Purpose\" and \"Hyperscale\" support Serverless, but \"Serverless for Hyperscale is currently in preview.\" So, better to select \"General Purpose\"."
      },
      {
        "date": "2023-10-26T21:11:00.000Z",
        "voteCount": 3,
        "content": "give answer is correct. Hyperscale doen't provide per second billing: https://learn.microsoft.com/en-us/azure/azure-sql/database/service-tier-hyperscale?view=azuresql"
      },
      {
        "date": "2023-10-23T07:58:00.000Z",
        "voteCount": 4,
        "content": "To meet the requirements of automatically scaling compute resources based on workload demand and providing per-second billing, you should recommend the following:\n\nAzure SQL Product: Azure SQL Database\n\nService Tier: Hyperscale\n\nAzure SQL Database Hyperscale provides automatic scaling of compute resources based on the workload demand, allowing your database to handle varying workloads effectively. It also offers per-second billing, which means you are billed based on the actual usage, providing cost efficiency. Hyperscale is particularly suitable for large and mission-critical databases that require high performance and scalability."
      },
      {
        "date": "2023-12-08T19:13:00.000Z",
        "voteCount": 2,
        "content": "Hyperscale tier is still in preview, it is NOT in production/published to public yet.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql&amp;tabs=general-purpose"
      },
      {
        "date": "2023-12-09T09:44:00.000Z",
        "voteCount": 3,
        "content": "not public? I use it daily at work. Stop using chat gpt"
      },
      {
        "date": "2023-09-22T06:07:00.000Z",
        "voteCount": 3,
        "content": "Azure SQL DB with vCore and Serverless provides per second billing\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/serverless-tier-overview?view=azuresql&amp;tabs=general-purpose"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122642-exam-az-305-topic-2-question-34-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription.<br><br>You need to deploy a solution that will provide point-in-time restore for blobs in storage accounts that have blob versioning and blob soft delete enabled.<br><br>Which type of blob should you create, and what should you enable for the accounts? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image220.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image221.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-10-06T01:07:00.000Z",
        "voteCount": 19,
        "content": "The answers look correct according to documentation:\n\nPoint-in-time restore for block blobs: https://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview\nThis mentions enabling the change feed: https://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-manage?tabs=portal#enable-and-configure-point-in-time-restore"
      },
      {
        "date": "2024-10-15T01:20:00.000Z",
        "voteCount": 1,
        "content": "Yup - \"You must enable the change feed on your storage account to begin capturing and recording changes. Disable the change feed to stop capturing changes\" \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal#enable-and-disable-the-change-feed"
      },
      {
        "date": "2023-11-22T02:36:00.000Z",
        "voteCount": 8,
        "content": "Appeared on 11/21/2023"
      },
      {
        "date": "2024-09-14T07:31:00.000Z",
        "voteCount": 1,
        "content": "Block\nImmutable blob storage"
      },
      {
        "date": "2024-09-20T04:51:00.000Z",
        "voteCount": 2,
        "content": "I see your post another time, question by question and usually you provide wrong answers, and even so you do it intentionally or not - provide some proof/evidence or any way of your thinking so we could all check it. The way you answer those questions just pisses off af."
      },
      {
        "date": "2024-05-04T02:57:00.000Z",
        "voteCount": 1,
        "content": "To deploy a solution that provides point-in-time restore for blobs in storage accounts with blob versioning and blob soft delete enabled, you should create a Block Blob. \nFor the accounts, you should enable Immutable Blob Storage. This ensures that data cannot be modified or deleted after it has been written, providing a safeguard for your data."
      },
      {
        "date": "2024-04-09T04:25:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct\nCheck below all in there - look at Data Protection =&gt; Recovery and Tracking\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-blob-change-feed?tabs=azure-portal"
      },
      {
        "date": "2024-03-18T03:06:00.000Z",
        "voteCount": 5,
        "content": "The answer is correct\n1. Only block blobs in a standard general-purpose v2 storage account can be restored as part of a point-in-time restore operation. Append blobs, page blobs, and premium block blobs aren't restored\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview\n2. Prequesits to enable restoring \nSoft delete\nChange feed\nBlob versioning"
      },
      {
        "date": "2024-03-01T01:40:00.000Z",
        "voteCount": 1,
        "content": "Point-in-time restore requires that the following Azure Storage features be enabled before you can enable point-in-time restore:\n\nSoft delete\nChange feed\nBlob versioning\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview"
      },
      {
        "date": "2023-10-20T06:33:00.000Z",
        "voteCount": 4,
        "content": "Only block blobs in a standard general-purpose v2 storage account can be restored as part of a point-in-time restore operation. Append blobs, page blobs, and premium block blobs aren't restored.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview#limitations-and-known-issues"
      },
      {
        "date": "2023-10-11T05:08:00.000Z",
        "voteCount": 4,
        "content": "Change feed is a prerequisite feature for Object Replication and Point-in-time restore for block blobs."
      },
      {
        "date": "2023-10-20T06:27:00.000Z",
        "voteCount": 4,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/point-in-time-restore-overview#prerequisites-for-point-in-time-restore"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122643-exam-az-305-topic-2-question-35-discussion/",
    "body": "HOTSPOT<br> -<br><br>Your company, named Contoso, Ltd., has an Azure subscription that contains the following resources:<br><br>\u2022\tAn Azure Synapse Analytics workspace named contosoworkspace1<br>\u2022\tAn Azure Data Lake Storage account named contosolake1<br>\u2022\tAn Azure SQL database named contososql1<br><br>The product data of Contoso is copied from contososql1 to contosolake1.<br><br>Contoso has a partner company named Fabrikam Inc. Fabrikam has an Azure subscription that contains the following resources:<br><br>\u2022\tA virtual machine named FabrikamVM1 that runs Microsoft SQL Server 2019<br>\u2022\tAn Azure Storage account named fabrikamsa1<br><br>Contoso plans to upload the research data on FabrikamVM1 to contosolake1. During the upload, the research data must be transformed to the data formats used by Contoso.<br><br>The data in contosolake1 will be analyzed by using contosoworkspace1.<br><br>You need to recommend a solution that meets the following requirements:<br><br>\u2022\tUpload and transform the FabrikamVM1 research data.<br>\u2022\tProvide Fabrikam with restricted access to snapshots of the data in contosoworkspace1.<br><br>What should you recommend for each requirement? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image222.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image223.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-10-06T01:22:00.000Z",
        "voteCount": 19,
        "content": "Provided answers look correct.\n\nFor ETL operations use Azure Data Factory and Azure Synapse Pipelines are based on Azure Data Factory.\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/data-integration/concepts-data-factory-differences\n\nFor restricted access use Azure Data Share:\nAzure Data Share enables organizations to securely share data with multiple customers and partners. Data providers are always in control of the data that they've shared and Azure Data Share makes it simple to manage and monitor what data was shared, when and by whom.\nIn this case snapshot-based sharing should be used.\nSource: https://learn.microsoft.com/en-us/azure/data-share/overview"
      },
      {
        "date": "2023-12-23T07:46:00.000Z",
        "voteCount": 12,
        "content": "This question appeared on my exam today.\nAnswer is correct"
      },
      {
        "date": "2024-09-11T00:55:00.000Z",
        "voteCount": 1,
        "content": "-Azure Synapse pipelines\n-Azure Data Share"
      },
      {
        "date": "2024-06-24T06:01:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct.\nAzure Synapse pipelines: They allow you to orchestrate data movement and transformation efficiently, integrating well with other Azure services.\nAzure Data Share: Provides a secure way to share data snapshots with restricted access, ensuring Fabrikam has controlled and read-only access to the necessary data."
      },
      {
        "date": "2024-03-21T17:03:00.000Z",
        "voteCount": 1,
        "content": "Data Warehouse Layer: Use Azure Synapse Analytics (formerly known as Azure SQL Data Warehouse). \nManaged Serving Layer for OLAP: Use Azure Analysis Services. Azure Analysis Services is an enterprise-grade analytics engine that provides a managed OLAP database for running and serving complex analytical models."
      },
      {
        "date": "2024-03-07T02:15:00.000Z",
        "voteCount": 10,
        "content": "Got his on my exam on 06/Mar/24"
      },
      {
        "date": "2023-11-11T12:16:00.000Z",
        "voteCount": 5,
        "content": "Azure synapse pipelines - Azure Synapse Pipelines is a cloud-based data integration service that allows you to create data-driven workflows for orchestrating and automating data movement and data transformation\nhttps://www.sqlshack.com/export-data-from-azure-sql-database-to-azure-data-lake-storage/\n\nAzure Data Share -  Azure Data Share is a simple and safe service for sharing big data with external organizations2. It allows you to easily share data with other organizations, and it provides capabilities to ensure that only authorized users have access to the shared data.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/how-to-set-up-access-control"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/122759-exam-az-305-topic-2-question-36-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are designing a data pipeline that will integrate large amounts of data from multiple on-premises Microsoft SQL Server databases into an analytics platform in Azure. The pipeline will include the following actions:<br><br>\u2022\tDatabase updates will be exported periodically into a staging area in Azure Blob storage.<br>\u2022\tData from the blob storage will be cleansed and transformed by using a highly parallelized load process.<br>\u2022\tThe transformed data will be loaded to a data warehouse.<br>\u2022\tEach batch of updates will be used to refresh an online analytical processing (OLAP) model in a managed serving layer.<br>\u2022\tThe managed serving layer will be used by thousands of end users.<br><br>You need to implement the data warehouse and serving layers.<br><br>What should you use? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image224.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image225.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-12-13T08:27:00.000Z",
        "voteCount": 29,
        "content": "Trick to remember:\nSynapse Analytics - massive parallel processing\nAnalysis Services - OLAP"
      },
      {
        "date": "2023-10-07T05:35:00.000Z",
        "voteCount": 12,
        "content": "The selected answer is correct.\n\nData Warehouse: Azure Synapse Analytics (formerly SQL Data Warehouse)\nAzure Synapse Analytics is a massively parallel processing (MPP) data warehouse that can handle large amounts of data and provides a scalable solution for analytics.\n\nManaged Serving Layer: Azure Analysis Services\nAzure Analysis Services provides a fully managed platform-as-a-service (PaaS) solution for online analytical processing (OLAP) and data modeling. It is suitable for serving analytical models to thousands of end users."
      },
      {
        "date": "2024-06-24T06:06:00.000Z",
        "voteCount": 1,
        "content": "The given answer is correct.\nAzure Synapse Analytics dedicated SQL pool: Provides a scalable, high-performance data warehouse solution that can efficiently handle large amounts of data and supports complex queries.\nAzure Analysis Services: Offers powerful analytical processing (OLAP) capabilities, enabling fast and interactive data analysis for thousands of end users."
      },
      {
        "date": "2023-10-07T04:44:00.000Z",
        "voteCount": 11,
        "content": "Here's how the pipeline would work:\n\nPeriodically export database updates to Azure Blob storage.\nUse Azure Data Factory to cleanse and transform the data from Blob storage.\nLoad the transformed data into your Azure Synapse Analytics data warehouse.\nUse Azure Analysis Services to create and manage OLAP models based on the data in your data warehouse.\nEnd users can connect to Azure Analysis Services to query and analyze the data."
      },
      {
        "date": "2023-10-07T04:43:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130623-exam-az-305-topic-2-question-37-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription.<br><br>You need to deploy a relational database. The solution must meet the following requirements:<br><br>\u2022\tSupport multiple read-only replicas.<br>\u2022\tAutomatically load balance read-only requests across all the read-only replicas.<br>\u2022\tMinimize administrative effort<br><br>What should you use? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/az-305/image243.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/az-305/image244.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-12T10:34:00.000Z",
        "voteCount": 9,
        "content": "As part of the requirement -&gt; Support multiple read-only replicas.\n\nHyperscale is the right choice. Business critical tier has only 1 additional read replica. \nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/service-tiers-sql-database-vcore?view=azuresql"
      },
      {
        "date": "2024-01-12T05:28:00.000Z",
        "voteCount": 5,
        "content": "The given answers are correct. Please refer MS doc, it says that \"In Premium and Business Critical service tiers, only one of the read-only replicas is accessible at any given time. Hyperscale supports multiple read-only replicas.\" Hence for load balancing between multiple read replica, first need is that they should be available, which makes Hyperscale suitable for this. I hope this helps. https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql"
      },
      {
        "date": "2024-03-07T05:10:00.000Z",
        "voteCount": 1,
        "content": "what about \"An Azure SQL Database elastic pool\" for first box? elastic pool can have Hyperscale service tier... and read-scale-out seems to depend only on service tier... (https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql)"
      },
      {
        "date": "2024-09-24T15:30:00.000Z",
        "voteCount": 1,
        "content": "Why would Elastic Pools or Managed Instance not be an option for Box1?"
      },
      {
        "date": "2024-04-09T05:18:00.000Z",
        "voteCount": 2,
        "content": "It is hard one to figure out (Between Hyperscale and business critical) but based on this comment from the requirements\n* Automatically load balance read-only requests across all the read-only replicas\nOnly Hyperscale can do this\nNote: for thos who made comment that \"Business critical tier has only 1 additional read replica.\" is NOT true, see link below and as per comment\n\"\"each single database or elastic pool database in the Premium and Business Critical service tier is automatically provisioned with a primary read-write replica and one or more secondary read-only replicas. The secondary replicas are provisioned with the same compute size as the primary replica. **\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql"
      },
      {
        "date": "2024-01-08T12:00:00.000Z",
        "voteCount": 1,
        "content": "First box seems correct.\n\nFor the second one, the Azure SQL Database in the Business Critical or Hyperscale service tiers automatically provisions a primary read-write replica and one or more secondary read-only replicas, so theoretically both are valid, not sure which one to choose, maybe Hyperscale is overkill."
      },
      {
        "date": "2024-01-12T05:28:00.000Z",
        "voteCount": 5,
        "content": "Please refer MS doc, it says that \"In Premium and Business Critical service tiers, only one of the read-only replicas is accessible at any given time. Hyperscale supports multiple read-only replicas.\"  Hence for load balancing between multiple read replica, first need is that they should be available, which makes Hyperscale suitable for this. I hope this helps. https://learn.microsoft.com/en-us/azure/azure-sql/database/read-scale-out?view=azuresql"
      },
      {
        "date": "2024-01-13T02:33:00.000Z",
        "voteCount": 2,
        "content": "Thank you!"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130404-exam-az-305-topic-2-question-38-discussion/",
    "body": "You have an app named App1 that uses an Azure Blob Storage container named app1data.<br><br>App1 uploads a cumulative transaction log file named File1.txt to a block blob in app1data once every hour. File1.txt only stores transaction data from the current day.<br><br>You need to ensure that you can restore the last uploaded version of File1.txt from any day for up to 30 days after the file was overwritten. The solution must minimize storage space.<br><br>What should you include in the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcontainer soft delete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tblob snapshots",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tblob soft delete",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tblob versioning\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-08T12:03:00.000Z",
        "voteCount": 7,
        "content": "I believe the given answer is correct. The key here is that File1.txt is changed every hour. Theoretically, you can do this with B as well but you need to configure a given time when the snapshot is taken. With blob versioning, you have access to 24 versions, and you can restore for example a version that took place 6 hours ago as opposed to when the blob snapshot took place (you will only have a single version, the one that the snapshot captured at the moment it was taken). For this reason I would go with D."
      },
      {
        "date": "2024-10-15T01:30:00.000Z",
        "voteCount": 1,
        "content": "\"Microsoft recommends maintaining fewer than 1000 versions per blob\"\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/versioning-overview"
      },
      {
        "date": "2024-01-13T02:43:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-overview\n\nNote\nBlob versioning offers a superior way to maintain previous versions of a blob. For more information, see Blob versioning.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/versioning-overview"
      },
      {
        "date": "2024-01-13T02:46:00.000Z",
        "voteCount": 1,
        "content": "I was debating whether to go with Blob Snapshots in the end because our requirement is that we only need one version of the file, so we can schedule a snapshot to take place let's say right before midnight, and it would work just fine. However, there's some administrative overhead to do so.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-manage-dotnet\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/snapshots-overview\n\nTo automate the snapshot creation process, you can use Azure Logic Apps or Azure Functions."
      },
      {
        "date": "2024-01-13T02:47:00.000Z",
        "voteCount": 6,
        "content": "I will, however, stick with Blob Versioning, even Microsoft suggests this is a better way. And to minimise the costs you can simply use lifecycle policies to delete old versions."
      },
      {
        "date": "2024-03-21T09:06:00.000Z",
        "voteCount": 1,
        "content": "Yep, seems all right."
      },
      {
        "date": "2024-01-09T07:37:00.000Z",
        "voteCount": 5,
        "content": "I will vote for B here, as the requirement is to minimize storage costs and also states it is only needed to archive the *last uploaded version* of the day. \n\nI am not aware of a native solution to schedule those snapshots, but worst case we could use scheduled Azure Functions for that."
      },
      {
        "date": "2024-09-20T05:25:00.000Z",
        "voteCount": 1,
        "content": "I believe option D. \nAlso, spoke to chat about it in comparison of minimize storage space:\nWhy Blob Versioning Minimizes Storage:\nVersioning only stores a copy when the blob is modified, unlike snapshots which can be more space-intensive due to frequent captures.\nRetention policies can be configured with versioning to automatically delete older versions (e.g., after 30 days), helping to control storage space over time.\nConclusion:\nBlob Versioning is the better choice for minimizing storage space, as it optimizes storage by only keeping changes between versions and can automatically prune older versions according to a defined retention period. This makes it more storage-efficient compared to snapshots.\n\nSo, blob versioning remains the best solution for this scenario, as it balances the need for version recovery and storage efficiency."
      },
      {
        "date": "2024-08-08T06:09:00.000Z",
        "voteCount": 1,
        "content": "with blob snapshot we can fulfill the requirement of keeping the last version of the file of a particular day while minimizing storage space."
      },
      {
        "date": "2024-06-24T06:24:00.000Z",
        "voteCount": 1,
        "content": "D. Blob Versioning\nJustification:\nBlob Versioning: Automatically keeps previous versions of an object when it is overwritten, enabling you to restore any version within the retention period.\nStorage Efficiency: Only stores the changes, minimizing the additional storage required."
      },
      {
        "date": "2024-04-29T12:05:00.000Z",
        "voteCount": 1,
        "content": "Solution must minimize storage space... Versioning for sure. Snapshots every hour would cause a lot of administrative load to manage storage space consumption."
      },
      {
        "date": "2024-04-13T04:36:00.000Z",
        "voteCount": 3,
        "content": "It says \"cumulative transaction log file named File1.txt \" means last file updated will have all the changes happened every hour. Taking snapshot of the last updated file at midnight will suffice the requirement.\nAnswer B"
      },
      {
        "date": "2024-02-20T15:41:00.000Z",
        "voteCount": 1,
        "content": "I feel B is thr right answer.\nReasons:\nWe need only the last updated version for each day, so taking snapshot of a day at midnight would be sufficient.\nSecondly, we need to minimise storage space. versioning will make 24 versions for each day. Lifecycle mgmt rules can easily delete the snapshot after 30 days."
      },
      {
        "date": "2024-03-16T02:37:00.000Z",
        "voteCount": 2,
        "content": "No. \nThe file is updated hourly, so the last updated version is also hourly. \nYou need to restore the last upload, which is hourly, not daily. \nStorage is minimized by keeping daily data available."
      },
      {
        "date": "2024-02-05T13:43:00.000Z",
        "voteCount": 5,
        "content": "Couldn't be more obvious to me. Versioning. File is uploaded every hour and we need to make sure we can restore the last upload. Snapshots are daily, a restore after 11pm would mean you'd miss out on the last 23 uploads. Question is pants but the answer is still clear."
      },
      {
        "date": "2024-02-02T19:10:00.000Z",
        "voteCount": 2,
        "content": "I will go with blob-versioning for the reason that version optimizes storage by additionally storing the delta and not the entire data as in case of snapshots"
      },
      {
        "date": "2024-01-22T06:53:00.000Z",
        "voteCount": 2,
        "content": "Storage space: Blob versioning and daily snapshots both consume storage space, but blob versioning might consume more space if there are frequent changes to the blob. Daily snapshots only create one copy of the blob per day, while blob versioning creates a new version every time the blob is modified or deleted. Therefore, blob versioning might be more suitable for blobs that are rarely changed, while daily snapshots might be more suitable for blobs that are frequently changed.\n\nIt is a very circumstantial question, I do not feel that enough information is provided. \n\nCopilot suggests Versioning would be the best option, but it is a terrible question."
      },
      {
        "date": "2024-01-05T05:57:00.000Z",
        "voteCount": 4,
        "content": "Versioning will store all 24 cumulative files daily. Instead we can do daily snapshots at the end of a day as it has cumulative data for the whole day. It will add administrative overhead, but reduce storage usage."
      },
      {
        "date": "2024-02-08T07:13:00.000Z",
        "voteCount": 3,
        "content": "Versioning will store 24*delta content of file which is comparable stirage to daily snapshot"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  },
  {
    "topic": 2,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130346-exam-az-305-topic-2-question-39-discussion/",
    "body": "You have 12 on-premises data sources that contain customer information and consist of Microsoft SQL Server, MySQL, and Oracle databases.<br><br>You have an Azure subscription.<br><br>You plan to create an Azure Data Lake Storage account that will consolidate the customer information for analysis and reporting.<br><br>You need to recommend a solution to automatically copy new information from the data sources to the Data Lake Storage account by using extract, transform and load (ETL). The solution must minimize administrative effort.<br><br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Explorer",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Share",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Studio"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T07:41:00.000Z",
        "voteCount": 8,
        "content": "Azure Data Factory is correct.\n\n&gt; Big data requires a service that can orchestrate and operationalize processes to refine these enormous stores of raw data into actionable business insights. Azure Data Factory is a managed cloud service that's built for these complex hybrid extract-transform-load (ETL), extract-load-transform (ELT), and data integration projects.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/introduction"
      },
      {
        "date": "2024-04-29T19:54:00.000Z",
        "voteCount": 1,
        "content": "Given answer A is correct\nIntegrate all your data with Azure Data Factory, a fully managed, serverless data integration service. Visually integrate data sources with more than 90 built-in, maintenance-free connectors at no added cost. Easily construct ETL (extract, transform, and load) and ELT (extract, load, and transform) processes code-free in an intuitive environment or write your own code. Then deliver integrated data to Azure Synapse Analytics to unlock business insights. \nEasy rehosting of SQL Server Integration Services to build ETL and ELT pipelines code-free with built-in Git and support for continuous integration and continuous delivery (CI/CD). \nPay-as-you-go, fully managed serverless cloud service that scales on demand for a cost-effective solution. \nMore than 90 built-in connectors for ingesting all your on-premises and software as a service (SaaS) data to orchestrate and monitor at scale."
      },
      {
        "date": "2024-03-07T00:37:00.000Z",
        "voteCount": 2,
        "content": "TBH for the exam like this the question seems to be a bit too simple."
      },
      {
        "date": "2024-01-04T10:17:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct\nhttps://azure.microsoft.com/en-in/products/data-factory"
      }
    ],
    "examNameCode": "az-305",
    "topicNumber": "2"
  }
]