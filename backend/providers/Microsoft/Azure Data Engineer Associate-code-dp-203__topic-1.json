[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67437-exam-dp-203-topic-1-question-1-discussion/",
    "body": "You have a table in an Azure Synapse Analytics dedicated SQL pool. The table was created by using the following Transact-SQL statement.<br><img src=\"/assets/media/exam-media/04259/0001600001.png\" class=\"in-exam-image\"><br>You need to alter the table to meet the following requirements:<br>\u2711 Ensure that users can identify the current manager of employees.<br>\u2711 Support creating an employee reporting hierarchy for your entire company.<br>\u2711 Provide fast lookup of the managers' attributes such as name and job title.<br>Which column should you add to the table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[ManagerEmployeeID] [smallint] NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[ManagerEmployeeKey] [smallint] NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[ManagerEmployeeKey] [int] NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[ManagerName] [varchar](200) NULL"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 87,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:40:00.000Z",
        "voteCount": 51,
        "content": "Answer C. Smallint eliminates A and B. But I would name the field [ManagerEmployeeID] [int] NULL since it should reference EmployeeID, not EmployeeKey since this one is IDENTITY."
      },
      {
        "date": "2022-06-29T22:26:00.000Z",
        "voteCount": 3,
        "content": "Why is smallint not possible. It would still support more than enough range to list all the employees in the company and save space + speed up the lookup."
      },
      {
        "date": "2023-01-30T09:31:00.000Z",
        "voteCount": 8,
        "content": "Because the ManagerEmployeeKey should match the Data type of EmployeeKey  which is INT"
      },
      {
        "date": "2022-06-29T22:28:00.000Z",
        "voteCount": 3,
        "content": "Smallint is up to 32,767: https://docs.microsoft.com/en-us/sql/t-sql/data-types/int-bigint-smallint-and-tinyint-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2022-11-08T21:10:00.000Z",
        "voteCount": 5,
        "content": "IDENTITY keyword suggests that it's a surrogate key and it's a good practice to have a separate field for the surrogate key which is not a business field like emp no or emp name"
      },
      {
        "date": "2022-12-15T20:28:00.000Z",
        "voteCount": 5,
        "content": "The answer is [ManagerEmployeeKey] [int] NULL because its a foreign key in the DimEmployee table connected to the primary key of the DimManager table. This column can be null because an employee can be a manager and there is no one he/she is reporting to."
      },
      {
        "date": "2024-09-23T00:23:00.000Z",
        "voteCount": 1,
        "content": "Dedicated SQL pool (formerly SQL DW) stores data in relational tables with columnar storage. So DimEmployee and DimManager tables should be the dimension tables in the DW.\n\nThe answer is [ManagerEmployeeKey] [int] NULL because its the foreign key in the DimEmployee table connected to the primary key of the DimManager table. This column can be null because an employee can be a manager and there is no one he/she is reporting to."
      },
      {
        "date": "2024-09-23T00:23:00.000Z",
        "voteCount": 4,
        "content": "Based on the case study we need a dimension table(For Managers) and a foreign key in the Employee table that links to the dim table. Its implied that the EmployeeKey is the primary key for the employee table  for the simple fact that the identity function increments the numbers for each row in the table. So based off this you can infer that you need to add a foreign key to the Employee table which would be called ManagerEmployeeKey."
      },
      {
        "date": "2024-09-20T00:40:00.000Z",
        "voteCount": 1,
        "content": "Smallint can be useful, upto limited range beyond it's defined limit it will not and then need to alter the key to INT."
      },
      {
        "date": "2024-08-08T22:12:00.000Z",
        "voteCount": 1,
        "content": "I think C"
      },
      {
        "date": "2023-10-16T10:09:00.000Z",
        "voteCount": 1,
        "content": "You need a key to link the Manager to an employee"
      },
      {
        "date": "2023-09-11T21:26:00.000Z",
        "voteCount": 2,
        "content": "This is implied from the schema"
      },
      {
        "date": "2023-08-29T06:40:00.000Z",
        "voteCount": 1,
        "content": "c is correct"
      },
      {
        "date": "2023-07-14T15:16:00.000Z",
        "voteCount": 2,
        "content": "Manager is also an employee so the column type should match."
      },
      {
        "date": "2023-06-18T16:38:00.000Z",
        "voteCount": 1,
        "content": "C correct"
      },
      {
        "date": "2023-01-21T11:39:00.000Z",
        "voteCount": 1,
        "content": "C as the data types of the primary key should be same for the manager."
      },
      {
        "date": "2023-01-08T18:20:00.000Z",
        "voteCount": 1,
        "content": "Both EmployeeKey and EmployeeId are int, so the ManagerEmployeeKey or ManagerEmployeeId also should be int. So C is correct"
      },
      {
        "date": "2022-11-29T20:28:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer C"
      },
      {
        "date": "2022-08-10T02:04:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-07-03T08:42:00.000Z",
        "voteCount": 1,
        "content": "Answer C is correct"
      },
      {
        "date": "2022-06-30T09:13:00.000Z",
        "voteCount": 3,
        "content": "Answer: C.  Looking at the table it is build for OLAP where EmployeeKey will be serrogate key so basing on that the ManagerEmployeeKey Looks good, type should match and NULL constraint should be applied ( big boss has no boss)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79349-exam-dp-203-topic-1-question-2-discussion/",
    "body": "You have an Azure Synapse workspace named MyWorkspace that contains an Apache Spark database named mytestdb.<br>You run the following command in an Azure Synapse Analytics Spark pool in MyWorkspace.<br>CREATE TABLE mytestdb.myParquetTable(<br>EmployeeID int,<br>EmployeeName string,<br>EmployeeStartDate date)<br><br>USING Parquet -<br>You then use Spark to insert a row into mytestdb.myParquetTable. The row contains the following data.<br><img src=\"/assets/media/exam-media/04259/0001700001.png\" class=\"in-exam-image\"><br>One minute later, you execute the following query from a serverless SQL pool in MyWorkspace.<br><br>SELECT EmployeeID -<br>FROM mytestdb.dbo.myParquetTable<br>WHERE EmployeeName = 'Alice';<br>What will be returned by the query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t24\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan error\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta null value"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-29T07:27:00.000Z",
        "voteCount": 56,
        "content": "I did a test, waited for one minute and tried the query in a serverless sql pool and received 24 as the result, so I don't understand that B has been voted so much because the answer is A) 24 without a doubt"
      },
      {
        "date": "2024-03-01T09:44:00.000Z",
        "voteCount": 3,
        "content": "i test too and confirm that the right answer is A"
      },
      {
        "date": "2022-11-11T07:21:00.000Z",
        "voteCount": 4,
        "content": "Did you tried the same query that is presented here? with \"mytestdb.dbo.myParquetTable\"??"
      },
      {
        "date": "2023-01-30T13:33:00.000Z",
        "voteCount": 3,
        "content": "The table and Column names are case insensitive."
      },
      {
        "date": "2023-02-09T20:11:00.000Z",
        "voteCount": 6,
        "content": "I tried with all upper case, and it still return record for name Alice.\nAnswer is A"
      },
      {
        "date": "2022-10-19T10:30:00.000Z",
        "voteCount": 55,
        "content": "Answer is B, but not because of the lowercase. The case has nothing to do with the error.\nIf you look attentively, you will notice that we create table mytestdb.myParquetTable, but the select statement contains the reference to table mytestdb.dbo.myParquetTable (!!! - dbo).\nHere is the error message I got:\nError: spark_catalog requires a single-part namespace, but got [mytestdb, dbo]."
      },
      {
        "date": "2023-01-05T20:44:00.000Z",
        "voteCount": 20,
        "content": "But if you look at the docs, that's exactly what has been done\n https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql:~:text=mytestdb.myparquettable%22)%3B-,Now%20you%20can%20read%20the%20data%20from%20your%20serverless%20SQL%20pool%20as%20follows%3A,-SQL"
      },
      {
        "date": "2024-08-28T17:18:00.000Z",
        "voteCount": 4,
        "content": "NO The Create table is in Azure Synapse Analytics Spark pool and select is in the serverless"
      },
      {
        "date": "2024-07-28T14:09:00.000Z",
        "voteCount": 2,
        "content": "Yes, so the right answer if A. It will return an output without any errors"
      },
      {
        "date": "2023-07-05T09:42:00.000Z",
        "voteCount": 4,
        "content": "Thanks @psicktrick for the link"
      },
      {
        "date": "2023-11-20T20:10:00.000Z",
        "voteCount": 1,
        "content": "i think B option is the correct too"
      },
      {
        "date": "2023-10-17T00:01:00.000Z",
        "voteCount": 6,
        "content": "i think you don't about sql server bro, Dbo means database object so it is not a issue for this the correct answer is A"
      },
      {
        "date": "2024-04-05T12:01:00.000Z",
        "voteCount": 2,
        "content": "Dbo means database owner actually bro"
      },
      {
        "date": "2023-12-17T03:18:00.000Z",
        "voteCount": 2,
        "content": "kindly clarify, which can be the right option? the conversations are confusing. :( any explanations are appreciated. thank you!!"
      },
      {
        "date": "2024-09-27T23:26:00.000Z",
        "voteCount": 1,
        "content": "24 is correct answer\nLink : https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool"
      },
      {
        "date": "2024-09-23T00:27:00.000Z",
        "voteCount": 5,
        "content": "As per ChatGPT, The query will fail with an error message as the table myParquetTable is created using Spark and the USING Parquet option, which means it is stored in a Parquet file format. Serverless SQL pool does not support querying Parquet files directly, so it cannot query myParquetTable in its current form.\n\nTo make the table accessible from the serverless SQL pool, you need to create an external table that references the Parquet file. Then, you can query the external table instead.\n\nAssuming you have created an external table named myExternalParquetTable that references the Parquet file containing the data in myParquetTable, the query to select EmployeeID where EmployeeName is 'Alice' would be:\n\nSELECT EmployeeID\nFROM myExternalParquetTable\nWHERE EmployeeName = 'Alice';"
      },
      {
        "date": "2024-09-23T00:27:00.000Z",
        "voteCount": 3,
        "content": "The query will fail with an error because mytestdb.myParquetTable is a Spark table, not a SQL table.\n\nWhen you created the table using Spark, you used the Spark SQL syntax, and the table is stored in the Spark engine's metadata. Serverless SQL pool in Azure Synapse Analytics cannot directly query Spark tables; it can only query SQL tables.\n\nIf you want to query the data stored in the mytestdb.myParquetTable table using a serverless SQL pool, you need to create an external table that maps to the same Parquet file. You can do this by using the CREATE EXTERNAL TABLE statement in a SQL pool."
      },
      {
        "date": "2023-10-10T04:32:00.000Z",
        "voteCount": 2,
        "content": "or as \"Managed table\". The answer seems to be very similar to the section \"Create a managed table in Spark and query from serverless SQL pool\" in the link below:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#expose-a-spark-table-in-sql"
      },
      {
        "date": "2024-09-23T00:27:00.000Z",
        "voteCount": 1,
        "content": "Right answer is B.\nThe query must run into Serverless SQL poll, not into Apache spark.\n\n\"One minute later, you execute the following query from a serverless SQL pool in MyWorkspace.\"\n\nIf we run that query into apache spark pool, using notebook for example, we must use \"SELECT database.table\".\n\nSo, according the question, we must use serverless sql pool and, cause of that, we have to use \"SELECT database.dbo.table\" OR \"use database; select table\""
      },
      {
        "date": "2024-09-23T00:27:00.000Z",
        "voteCount": 3,
        "content": "The given query is trying to select the EmployeeID from the Parquet table myParquetTable in the mytestdb database, where the EmployeeName is 'Alice'. However, it seems like there's a minor typo in the query. The query should be like this:\n\nSELECT EmployeeID\nFROM mytestdb.myParquetTable\nWHERE EmployeeName = 'Alice';\n\n\nThe given query is trying to select the EmployeeID from the Parquet table myParquetTable in the mytestdb database, where the EmployeeName is 'Alice'. However, it seems like there's a minor typo in the query. The query should be like this:\n\nsql\nCopy code\nSELECT EmployeeID\nFROM mytestdb.myParquetTable\nWHERE EmployeeName = 'Alice';\nThe corrected query would return:\n\nA. 24\n\nSo, the correct answer is A. The query will return the EmployeeID, which is 24."
      },
      {
        "date": "2024-09-12T07:31:00.000Z",
        "voteCount": 1,
        "content": "The question is bad, the Spark database table created VIA Spark pool is not automatically generated in the SQL pool, the user should create one external table via serverless SQL pool first, the schema is not required in the external table. So whether schema dbo is needed in the 2nd query is depending on if dbo is included when creating external table."
      },
      {
        "date": "2024-09-05T12:16:00.000Z",
        "voteCount": 1,
        "content": "The question is to query from Serverless to Spark.\nTo query directly in Spark, must not use \"dbo\".\nTo query from Serverless, \"dbo\" is required.\nI tested this myself just now.\nCorrect: A."
      },
      {
        "date": "2024-09-01T20:05:00.000Z",
        "voteCount": 2,
        "content": "Answer A is correct based on logic once you create a table in Spark DB it will automatically Create it under db.table for serverless so query returns the result"
      },
      {
        "date": "2024-08-19T00:54:00.000Z",
        "voteCount": 3,
        "content": "the right answer is A"
      },
      {
        "date": "2024-08-08T15:22:00.000Z",
        "voteCount": 2,
        "content": "Answer is A, because after Create a table in Spark pool the same one will be automatically created under dbo for Serverless SQL Pool. so the query returns 24"
      },
      {
        "date": "2024-07-28T13:05:00.000Z",
        "voteCount": 1,
        "content": "Answer A ,\n\nYou have inserted one row and the ID is 24  , The select query is correct so B is wrong and there is no Null value in the inserted records"
      },
      {
        "date": "2024-07-28T11:45:00.000Z",
        "voteCount": 3,
        "content": "Learning, unsure about answer first but after checking document, can confirm it's A.\nQuestion is nearly same example as of doc."
      },
      {
        "date": "2024-07-17T23:22:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#examples"
      },
      {
        "date": "2024-07-17T14:46:00.000Z",
        "voteCount": 1,
        "content": "i think"
      },
      {
        "date": "2024-06-30T21:31:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A. Clearly explained in the link https://learn.microsoft.com/en-us/azure/synapse-analytics/metadata/table#create-a-managed-table-in-spark-and-query-from-serverless-sql-pool"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52163-exam-dp-203-topic-1-question-3-discussion/",
    "body": "DRAG DROP -<br>You have a table named SalesFact in an enterprise data warehouse in Azure Synapse Analytics. SalesFact contains sales data from the past 36 months and has the following characteristics:<br>\u2711 Is partitioned by month<br>\u2711 Contains one billion rows<br>\u2711 Has clustered columnstore index<br>At the beginning of each month, you need to remove data from SalesFact that is older than 36 months as quickly as possible.<br>Which three actions should you perform in sequence in a stored procedure? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0001900001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0001900002.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an empty table named SalesFact_work that has the same schema as SalesFact.<br>Step 2: Switch the partition containing the stale data from SalesFact to SalesFact_Work.<br>SQL Data Warehouse supports partition splitting, merging, and switching. To switch partitions between two tables, you must ensure that the partitions align on their respective boundaries and that the table definitions match.<br>Loading data into partitions with partition switching is a convenient way stage new data in a table that is not visible to users the switch in the new data.<br>Step 3: Drop the SalesFact_Work table.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/sql-data-warehouse/sql-data-warehouse-tables-partition",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-07T15:39:00.000Z",
        "voteCount": 47,
        "content": "Given answer D A C is correct."
      },
      {
        "date": "2021-09-08T02:47:00.000Z",
        "voteCount": 4,
        "content": "Yes. Once the partition is switched with an empty partition it is equivalent to truncating the partition from the original table"
      },
      {
        "date": "2024-09-23T01:10:00.000Z",
        "voteCount": 12,
        "content": "The answer should be D --&gt;  A  --&gt;  C.\n\nStep 1:\nCreate an empty table SalesFact_Work with same schema as SalesFact.\n\nStep 2:\nSwitch the partition (to be removed) from SalesFact to SalesFact_Work. The syntax is:\nALTER TABLE &lt;source table&gt; SWITCH PARTITION &lt;partition number&gt; to &lt;destination table&gt;\n\nStep 3:\nDelete the SalesFact_Work table."
      },
      {
        "date": "2024-09-23T01:10:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct:\nStep 1:\nCreate an empty table SalesFact_Work with same schema as SalesFact (that it will contains reocords older than 3 years)\n\nStep 2:\nSwitch the partition from SalesFact to SalesFact_Work. SO we're only doing metadata operations\n\nStep 3:\nDelete the SalesFact_Work table containing stale data and we're not losing any time or blocking target table"
      },
      {
        "date": "2024-09-23T01:10:00.000Z",
        "voteCount": 4,
        "content": "\"Partition switching can be used to quickly remove or replace a section of a table. For example, a sales fact table might contain just data for the past 36 months. At the end of every month, the oldest month of sales data is deleted from the table. This data could be deleted by using a delete statement to delete the data for the oldest month.\n\nHowever, deleting a large amount of data row-by-row with a delete statement can take too much time, as well as create the risk of large transactions that take a long time to rollback if something goes wrong. A more optimal approach is to drop the oldest partition of data. Where deleting the individual rows could take hours, deleting an entire partition could take seconds.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition#benefits-to-loads"
      },
      {
        "date": "2024-09-18T18:39:00.000Z",
        "voteCount": 1,
        "content": "FOR 3RD STEP: why should we drop the table instead of truncating? so that we can use it for subsequent months right?"
      },
      {
        "date": "2024-04-15T11:08:00.000Z",
        "voteCount": 1,
        "content": "Is given answer correct (D, A, C) ?"
      },
      {
        "date": "2024-03-31T12:55:00.000Z",
        "voteCount": 1,
        "content": "Why not use 'ALTER TABLE' to avoid the need for creating a new table next time and simply switch the partition instead?"
      },
      {
        "date": "2023-08-29T06:49:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-07-07T21:51:00.000Z",
        "voteCount": 2,
        "content": "Given answer - Options D A C are correct."
      },
      {
        "date": "2023-04-27T02:16:00.000Z",
        "voteCount": 3,
        "content": "Hi\nCan someone tell me why we cannot simply execute a delete statement?\nThanks"
      },
      {
        "date": "2023-05-08T15:46:00.000Z",
        "voteCount": 3,
        "content": "DELETE is intensive for a database to run. The solution should be faster as possible."
      },
      {
        "date": "2023-05-10T18:09:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/training/modules/analyze-optimize-data-warehouse-storage-azure-synapse-analytics/10-understand-rules-for-minimally-logged-operations"
      },
      {
        "date": "2023-01-14T01:31:00.000Z",
        "voteCount": 2,
        "content": "D A C IS CORRECT"
      },
      {
        "date": "2022-09-28T17:35:00.000Z",
        "voteCount": 1,
        "content": "very interesting questions:\nEvery partition has a name, which indicated by the mmYYYY perhaps. \nSo, if we know the name of the partition, we can drop that partition directly:\nDROP PARTITION SCHEME partition_scheme_name [ ; ]  \nHowever, if there is an index on the table DOPR Partition will not work. So, the it is correct.\nDAC."
      },
      {
        "date": "2022-08-26T13:11:00.000Z",
        "voteCount": 2,
        "content": "Answer is F - A - C\nhttps://docs.microsoft.com/es-es/archive/blogs/apsblog/azure-sql-dw-performance-ctaspartition-switching-vs-updatedelete"
      },
      {
        "date": "2022-08-26T13:15:00.000Z",
        "voteCount": 1,
        "content": "D is incorrect because we also need to copy the data onto the new table"
      },
      {
        "date": "2022-10-20T10:09:00.000Z",
        "voteCount": 1,
        "content": "F seems wrong as it says CTAS to copy the data"
      },
      {
        "date": "2022-08-13T10:27:00.000Z",
        "voteCount": 2,
        "content": "correct ans."
      },
      {
        "date": "2022-06-01T17:15:00.000Z",
        "voteCount": 1,
        "content": "D,A,C\n\nAzure Synapse does not support truncating partitions.  Currently, that is feature is only tied to MS SQL Server."
      },
      {
        "date": "2022-05-11T04:40:00.000Z",
        "voteCount": 1,
        "content": "Step 1: Create an empty table named SalesFact_work that has the same schema as SalesFact.\nStep 2: Switch the partition containing the stale data from SalesFact to SalesFact_Work.\nStep 3: Drop the SalesFact_Work table."
      },
      {
        "date": "2022-04-14T07:30:00.000Z",
        "voteCount": 1,
        "content": "D A C is the right option. \n\nFor more information, this doc discusses exactly this example: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51494-exam-dp-203-topic-1-question-4-discussion/",
    "body": "You have files and folders in Azure Data Lake Storage Gen2 for an Azure Synapse workspace as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0002000001.png\" class=\"in-exam-image\"><br>You create an external table named ExtTable that has LOCATION='/topfolder/'.<br>When you query ExtTable by using an Azure Synapse Analytics serverless SQL pool, which files are returned?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFile2.csv and File3.csv only",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFile1.csv and File4.csv only\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFile1.csv, File2.csv, File3.csv, and File4.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFile1.csv only"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:42:00.000Z",
        "voteCount": 123,
        "content": "I believe the answer should be B. \nIn case of a serverless pool a wildcard should be added to the location.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table"
      },
      {
        "date": "2024-05-16T19:00:00.000Z",
        "voteCount": 4,
        "content": "Serverless SQL pool can recursively traverse folders if you specify /** at the end of path. The following query will read all files from all folders and subfolders located in the csv/taxi folder.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files"
      },
      {
        "date": "2021-10-17T07:00:00.000Z",
        "voteCount": 40,
        "content": "I tested and proove you right, the answer is B. Remind the question is referring to serverless SQL and not dedicated SQL pool. \"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any subfolder.\""
      },
      {
        "date": "2024-09-20T00:42:00.000Z",
        "voteCount": 31,
        "content": "\"Serverless SQL pool can recursively traverse folders only if you specify /** at the end of path.\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files"
      },
      {
        "date": "2021-10-17T07:00:00.000Z",
        "voteCount": 4,
        "content": "The answer is B however. I could not make \"/**\" to work. somebody?"
      },
      {
        "date": "2021-06-07T09:50:00.000Z",
        "voteCount": 22,
        "content": "When you are quoting from Microsoft documentation, do not ADD in words to the sentence. 'Only' is not used."
      },
      {
        "date": "2024-09-23T01:13:00.000Z",
        "voteCount": 2,
        "content": "Correct answer , I test it and it works :\n\nCREATE EXTERNAL TABLE ext2 ([User] varchar(40), Feature varchar(40), Event varchar(40) , Time varchar(40))\n\tWITH (\n\tLOCATION = '/topfolder',\n\tDATA_SOURCE = ds2,\n\tFILE_FORMAT = csvFormat\n\t)\nGO\nSELECT COUNT(*) as cnt FROM ext2\nGO"
      },
      {
        "date": "2021-10-17T07:01:00.000Z",
        "voteCount": 2,
        "content": "This is for serverless SQL. What you showed is for dedicated SQL, behaviour is different as explained here: \"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any subfolder.\""
      },
      {
        "date": "2024-09-23T01:13:00.000Z",
        "voteCount": 2,
        "content": "Option is definitely \"B\"\n\nBelow is the documentation given on MS Docs:\n\nRecursive data for external tables\n\nUnlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder."
      },
      {
        "date": "2024-09-23T01:13:00.000Z",
        "voteCount": 1,
        "content": "(Seems from this document that Answer is \"C\")In SQL Server, the CREATE EXTERNAL TABLE statement creates the path and folder if it doesn't already exist. You can then use INSERT INTO to export data from a local SQL Server table to the external data source. For more information, see PolyBase Queries.\n\nIf you specify LOCATION to be a folder, a PolyBase query that selects from the external table will retrieve files from the folder and all of its subfolders. Just like Hadoop, PolyBase doesn't return hidden folders. It also doesn't return files for which the file name begins with an underline (_) or a period (.).\n\nIn this example, if LOCATION='/webdata/', a PolyBase query will return rows from mydata.txt and mydata2.txt. It won't return mydata3.txt because it's a file in a hidden folder. And it won't return _hidden.txt because it's a hidden file.\n\nRecursive data for external tables\n\nhttps://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver15&amp;tabs=dedicated"
      },
      {
        "date": "2024-09-23T01:12:00.000Z",
        "voteCount": 2,
        "content": "\"native external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder. Hadoop tables will return all files within any sub-folder.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table"
      },
      {
        "date": "2024-07-03T12:54:00.000Z",
        "voteCount": 1,
        "content": "I believe that the right answer is B, because from the root directory, there will be listed two files and two folders"
      },
      {
        "date": "2024-06-28T19:05:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL TABLE ExtTable (\n    EmployeeId INT,\n    EmployeeName VARCHAR(100),\n    EmployeeStartDate DATE\n)\nWITH (\n    LOCATION = '/topfolder/**',\n    DATA_SOURCE = your_data_source,\n    FILE_FORMAT = your_file_format\n);"
      },
      {
        "date": "2024-06-28T19:02:00.000Z",
        "voteCount": 1,
        "content": "choice is C, I dont understand why such a simple question trigger such many discussions"
      },
      {
        "date": "2024-06-27T02:47:00.000Z",
        "voteCount": 1,
        "content": "I think the answer should be B, because there are only two possibilites that would allow to return all the subfolders : \n- A hadoop external table\n- The query contains LOCATION='/webdata/**\nif this is a native external tables, they don't return subfolders"
      },
      {
        "date": "2024-06-15T16:15:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2024-06-03T23:06:00.000Z",
        "voteCount": 1,
        "content": "It is B"
      },
      {
        "date": "2024-03-23T18:30:00.000Z",
        "voteCount": 1,
        "content": "Not recursive wo **"
      },
      {
        "date": "2024-03-03T21:20:00.000Z",
        "voteCount": 5,
        "content": "I have created this case in my Datalake &amp; Synapse Severless SQL pool and run sql as below:\nselect top 10 *\nfrom openrowset(\n    bulk 'topfolder/',\n\tdata_source = 'Test',\n    format = 'csv',\n    parser_version = '2.0',\n    firstrow = 2\n\t) \nwith (\n        EmployeeId int\n) as rows\n\nThe answer for this one is B (only 2 files returned)"
      },
      {
        "date": "2024-02-02T23:45:00.000Z",
        "voteCount": 1,
        "content": "Answer should be B."
      },
      {
        "date": "2024-01-04T09:13:00.000Z",
        "voteCount": 1,
        "content": "Answer B is correct for me too."
      },
      {
        "date": "2024-01-02T15:43:00.000Z",
        "voteCount": 2,
        "content": "Agree it should be B. The question is a little off, because they don't specify whether using or not using a wildcard to do so. Assuming by default then, no wildcard is used, only those top-level files will be returned."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52254-exam-dp-203-topic-1-question-5-discussion/",
    "body": "HOTSPOT -<br>You are planning the deployment of Azure Data Lake Storage Gen2.<br>You have the following two reports that will access the data lake:<br>\u2711 Report1: Reads three columns from a file that contains 50 columns.<br>\u2711 Report2: Queries a single record based on a timestamp.<br>You need to recommend in which format to store the data in the data lake to support the reports. The solution must minimize read times.<br>What should you recommend for each report? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0002200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0002300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Report1: CSV -<br>CSV: The destination writes records as delimited data.<br><br>Report2: AVRO -<br>AVRO supports timestamps.<br>Not Parquet, TSV: Not options for Azure Data Lake Storage Gen2.<br>Reference:<br>https://streamsets.com/documentation/datacollector/latest/help/datacollector/UserGuide/Destinations/ADLS-G2-D.html",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-18T00:33:00.000Z",
        "voteCount": 209,
        "content": "1: Parquet - column-oriented binary file format\n2: AVRO -  Row based format, and has logical type timestamp\nhttps://youtu.be/UrWthx8T3UY"
      },
      {
        "date": "2022-04-16T23:24:00.000Z",
        "voteCount": 2,
        "content": "this is correct."
      },
      {
        "date": "2022-11-21T14:05:00.000Z",
        "voteCount": 2,
        "content": "Thanks for the video share, this really helps. Cheers."
      },
      {
        "date": "2021-06-10T23:51:00.000Z",
        "voteCount": 9,
        "content": "the web is full of old information. timestamp support has been added to parquet"
      },
      {
        "date": "2021-06-28T07:56:00.000Z",
        "voteCount": 25,
        "content": "Ok, but in 1st case we need only 3 of 50 columns. Parquet i columnar format. In 2nd Avro because ideal for read full row"
      },
      {
        "date": "2021-05-10T06:19:00.000Z",
        "voteCount": 31,
        "content": "Shouldn't the answer for Report 1 be Parquet? Because Parquet format is Columnar and should be best for reading a few columns only."
      },
      {
        "date": "2024-09-23T01:15:00.000Z",
        "voteCount": 3,
        "content": "The goal is: The solution must minimize read times.\n\nI made small test on Databrick plus DataLake.\nThe same file saved as Parquet and Avro\n9 mln of records.\nParquet ~150 MB\nAvro ~700MB\n\nReading Parquet is always 10 times faster that Avro.\nI checked:\n- for all data or  small range of data with condition\n- all or only one column\n\nSo I will select option:\n- Parquet\n- Parquet"
      },
      {
        "date": "2022-01-02T23:15:00.000Z",
        "voteCount": 1,
        "content": "how can be faster read is same as number of reads?"
      },
      {
        "date": "2024-09-23T01:15:00.000Z",
        "voteCount": 9,
        "content": "1. Parquet\n2. Avro\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices\n\n\"Consider using the Avro file format in cases where your I/O patterns are more write heavy, or the query patterns favor retrieving multiple rows of records in their entirety. \n\nConsider Parquet and ORC file formats when the I/O patterns are more read heavy or when the query patterns are focused on a subset of columns in the records.\""
      },
      {
        "date": "2022-03-21T11:45:00.000Z",
        "voteCount": 1,
        "content": "Thank you."
      },
      {
        "date": "2024-09-23T01:15:00.000Z",
        "voteCount": 9,
        "content": "To minimize read times for the two reports, it is recommended to store the data in the data lake in the parquet format.\n\nParquet is a columnar storage format that is optimized for querying large datasets. It stores data in a compact and efficient manner, allowing for fast querying and filtering of data.\n\nIn this case, Report1 needs to read only three columns from a file that contains 50 columns. Since parquet stores data in a columnar format, the query can skip reading the unnecessary columns and only read the required ones, which can greatly improve the read performance.\n\nReport2 needs to query a single record based on a timestamp. Parquet also supports efficient filtering and querying based on specific values, such as timestamps, making it a good choice for this report as well.\n\nOther formats, such as avro, csv, and tsv, may not provide the same level of performance for these types of queries. Therefore, it is recommended to use parquet to store the data in the data lake."
      },
      {
        "date": "2024-09-23T01:14:00.000Z",
        "voteCount": 1,
        "content": "Report 1: parquet\nColumnar Storage: Parquet stores data in columns, allowing efficient reading of only the required columns (3 out of 50).\nReport 2: Avro.\nFast Single-Record Access: Optimized row-based formats excel at quickly accessing individual records based on a specific condition, such as a timestamp.\nI don't understand why incorrect answers are being provided, causing confusion."
      },
      {
        "date": "2024-09-23T01:14:00.000Z",
        "voteCount": 3,
        "content": "Recommendations:\nReport1: Use Parquet\nReason: Parquet is a columnar storage format that is optimized for reading specific columns. Since Report1 needs to read only three columns out of 50, Parquet allows reading just those columns efficiently without scanning the entire file.\n\nReport2: Use Avro\nReason: Avro is a row-based storage format, which is efficient for retrieving entire rows based on a specific condition, such as a timestamp. It allows quick access to individual records, making it suitable for Report2's requirement."
      },
      {
        "date": "2024-09-17T12:11:00.000Z",
        "voteCount": 1,
        "content": "Why cant it be Parquet for both reports? Parquet supports timestamp too"
      },
      {
        "date": "2024-07-28T13:23:00.000Z",
        "voteCount": 1,
        "content": "1- Parquet 2- Avro\n\nParquet for Column selection(stored in columnar format) and Avro for row selection (stored in row format)"
      },
      {
        "date": "2024-04-22T23:56:00.000Z",
        "voteCount": 2,
        "content": "1. Parquet\n2. Avro"
      },
      {
        "date": "2024-03-11T14:49:00.000Z",
        "voteCount": 1,
        "content": "Parquet\nAvro"
      },
      {
        "date": "2024-01-29T22:53:00.000Z",
        "voteCount": 1,
        "content": "1. Parquet - Column oriented data store\n2. AVRO - supports timestamps"
      },
      {
        "date": "2024-01-02T15:46:00.000Z",
        "voteCount": 2,
        "content": "Agree:\n1: Parquet - ideal for columnar forma\n2: AVRO: Row-based with logical timestamp"
      },
      {
        "date": "2023-10-11T01:48:00.000Z",
        "voteCount": 1,
        "content": "--&gt; TLDR &lt;--\n   \t\t\t                AVRO\tPARQUET\tORC\nAnal. Queries\t\t\t                      v\t           v\nWrite Ops (ETL ops)\tv\t\t\nNested Data\t\t\t                       v\t\nACID Properties\t\t\t\t                          v\nSch.Flexibility\t\tv"
      },
      {
        "date": "2023-09-06T23:42:00.000Z",
        "voteCount": 1,
        "content": "1. Parquet --&gt; Column format\n2. AVRO --&gt; Row format with timestamp type"
      },
      {
        "date": "2023-08-29T07:12:00.000Z",
        "voteCount": 1,
        "content": "1- Parquet 2- Parquet"
      },
      {
        "date": "2023-09-10T03:13:00.000Z",
        "voteCount": 1,
        "content": "sorry 2 avro"
      },
      {
        "date": "2023-05-01T11:47:00.000Z",
        "voteCount": 2,
        "content": "Report 1: parquet\nReport 2: Avro"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52149-exam-dp-203-topic-1-question-6-discussion/",
    "body": "You are designing the folder structure for an Azure Data Lake Storage Gen2 container.<br>Users will query data by using a variety of services including Azure Databricks and Azure Synapse Analytics serverless SQL pools. The data will be secured by subject area. Most queries will include data from the current year or current month.<br>Which folder structure should you recommend to support fast queries and simplified folder security?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/{SubjectArea}/{DataSource}/{DD}/{MM}/{YYYY}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/{DD}/{MM}/{YYYY}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/{YYYY}/{MM}/{DD}/{SubjectArea}/{DataSource}/{FileData}_{YYYY}_{MM}_{DD}.csv",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t/{SubjectArea}/{DataSource}/{YYYY}/{MM}/{DD}/{FileData}_{YYYY}_{MM}_{DD}.csv\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-05-08T12:06:00.000Z",
        "voteCount": 63,
        "content": "D is correct\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#batch-jobs-structure"
      },
      {
        "date": "2024-07-28T13:27:00.000Z",
        "voteCount": 1,
        "content": "D - Secured by subject area so top level folder security, Year first to avoid too many subfolder/partitions by each date"
      },
      {
        "date": "2024-01-02T15:47:00.000Z",
        "voteCount": 1,
        "content": "Definitely D."
      },
      {
        "date": "2023-12-18T01:29:00.000Z",
        "voteCount": 2,
        "content": "bcz i said so"
      },
      {
        "date": "2023-08-29T07:13:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-08-04T07:39:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-05-02T16:03:00.000Z",
        "voteCount": 1,
        "content": "D Is the best way, filter subject area first the size queries will reduced just one and after by date."
      },
      {
        "date": "2023-04-26T18:52:00.000Z",
        "voteCount": 2,
        "content": "Should be D."
      },
      {
        "date": "2023-04-17T07:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-01-21T13:57:00.000Z",
        "voteCount": 1,
        "content": "yes its D :)"
      },
      {
        "date": "2022-12-15T21:23:00.000Z",
        "voteCount": 3,
        "content": "Serverless SQL Pools offers a straight-forward method of querying data including CSV, JSON, and Parquet format stored in Azure Storage.\n\nSo, setting up the csv files within azure storage in hive-formated folder hierarchy i.e. /{yyyy}/{mm}/{dd}/ actually helps in sql querying the data much faster since only the partitioned segment of the data is queried."
      },
      {
        "date": "2022-12-09T12:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-08-13T10:54:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-08-10T02:06:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-06-28T00:20:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-05-11T04:49:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-04-24T09:59:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67438-exam-dp-203-topic-1-question-7-discussion/",
    "body": "HOTSPOT -<br>You need to output files from Azure Data Factory.<br>Which file format should you use for each type of output? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0002500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0002600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Parquet -<br>Parquet stores data in columns, while Avro stores data in a row-based format. By their very nature, column-oriented data stores are optimized for read-heavy analytical workloads, while row-based databases are best for write-heavy transactional workloads.<br><br>Box 2: Avro -<br>An Avro schema is created using JSON format.<br>AVRO supports timestamps.<br>Note: Azure Data Factory supports the following file formats (not GZip or TXT).<br><br>Avro format -<br><img src=\"/assets/media/exam-media/04259/0002600002.png\" class=\"in-exam-image\"><br>\u2711 Binary format<br>\u2711 Delimited text format<br>\u2711 Excel format<br>\u2711 JSON format<br>\u2711 ORC format<br>\u2711 Parquet format<br>\u2711 XML format<br>Reference:<br>https://www.datanami.com/2018/05/16/big-data-file-formats-demystified",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-26T06:29:00.000Z",
        "voteCount": 44,
        "content": "Parquet and AVRO is correct option"
      },
      {
        "date": "2023-12-10T11:06:00.000Z",
        "voteCount": 1,
        "content": "Parquet and Avro are correct"
      },
      {
        "date": "2023-09-06T23:44:00.000Z",
        "voteCount": 2,
        "content": "The provided answer is correct: Parquet &amp; AVRO"
      },
      {
        "date": "2023-08-29T07:15:00.000Z",
        "voteCount": 1,
        "content": "1.parquet 2avro"
      },
      {
        "date": "2023-04-24T22:01:00.000Z",
        "voteCount": 4,
        "content": "1: PARQUET\nBecause Parquet is a columnar file format.\n\n2: AVRO\nBecause Avro is a row-based file format (as JSON) which is connected to logical timestamp."
      },
      {
        "date": "2023-02-08T09:13:00.000Z",
        "voteCount": 1,
        "content": "Parquet,Avro"
      },
      {
        "date": "2022-12-15T21:26:00.000Z",
        "voteCount": 2,
        "content": "1: PARQUET\nBecause Parquet is a columnar file format.\n\n2: AVRO\nBecause Avro is a row-based file format (as JSON) which is connected to logical timestamp."
      },
      {
        "date": "2022-09-08T18:34:00.000Z",
        "voteCount": 1,
        "content": "Parquet and AVRO is correct."
      },
      {
        "date": "2022-08-13T10:56:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-07-19T07:41:00.000Z",
        "voteCount": 3,
        "content": "Parquet and AVRO is correct option"
      },
      {
        "date": "2022-05-11T04:51:00.000Z",
        "voteCount": 3,
        "content": "agree with the answer"
      },
      {
        "date": "2022-03-05T00:08:00.000Z",
        "voteCount": 2,
        "content": "Parquet and AVRO is correct option"
      },
      {
        "date": "2022-01-25T04:10:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-20T08:34:00.000Z",
        "voteCount": 2,
        "content": "Parquet and AVRO is right."
      },
      {
        "date": "2021-12-25T08:51:00.000Z",
        "voteCount": 1,
        "content": "GZIP file format is one of supported Binary format by ADF. \nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#file-system-as-sink"
      },
      {
        "date": "2021-12-13T14:07:00.000Z",
        "voteCount": 2,
        "content": "agree with the answer"
      },
      {
        "date": "2021-12-09T09:30:00.000Z",
        "voteCount": 1,
        "content": "Respuesta correcta PARQUET &amp; AVRO."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53010-exam-dp-203-topic-1-question-8-discussion/",
    "body": "HOTSPOT -<br>You use Azure Data Factory to prepare data to be queried by Azure Synapse Analytics serverless SQL pools.<br>Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company.<br>You need to move the files to a different folder and transform the data to meet the following requirements:<br>\u2711 Provide the fastest possible query times.<br>\u2711 Automatically infer the schema from the underlying files.<br>How should you configure the Data Factory copy activity? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0002800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0002900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Preserver hierarchy -<br>Compared to the flat namespace on Blob storage, the hierarchical namespace greatly improves the performance of directory management operations, which improves overall job performance.<br><br>Box 2: Parquet -<br>Azure Data Factory parquet format is supported for Azure Data Lake Storage Gen2.<br>Parquet supports the schema property.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-introduction https://docs.microsoft.com/en-us/azure/data-factory/format-parquet",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-20T00:44:00.000Z",
        "voteCount": 163,
        "content": "1. Merge Files\n2. Parquet\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance"
      },
      {
        "date": "2021-12-25T08:46:00.000Z",
        "voteCount": 12,
        "content": "just want to add a bit more reference regarding copyBehavior in ADF plus info mentioned in Best Practice doc, so it shall be MergeFile first. \nhttps://docs.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory#file-system-as-sink"
      },
      {
        "date": "2021-05-25T14:50:00.000Z",
        "voteCount": 25,
        "content": "The smaller the files, the negative the performance so Merge and Parquet seems to be  the right answer."
      },
      {
        "date": "2021-10-18T22:19:00.000Z",
        "voteCount": 10,
        "content": "Larger files lead to better performance and reduced costs.\n\nTypically, analytics engines such as HDInsight have a per-file overhead that involves tasks such as listing, checking access, and performing various metadata operations. If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). S"
      },
      {
        "date": "2021-05-31T23:39:00.000Z",
        "voteCount": 10,
        "content": "It should be \n1)Merge Files - Question clearly says \"initially ingested as 10 small json files\". There is no hint on hierarchy or partition information. so clearly we need to merge these files for better performance\n2) Parquet -&gt; Always gives better performance for columnar based data"
      },
      {
        "date": "2024-07-29T10:11:00.000Z",
        "voteCount": 2,
        "content": "This question is somewhat confusing.  The detail states \"Each file contains the same data attributes and data from a subsidiary of your company\".  There is a possibility that each file is for a different subsidiary and would be stored in a different hierarchical folder, so the requirement to \"preserve hierarchy\" could be the right answer here."
      },
      {
        "date": "2024-03-14T04:14:00.000Z",
        "voteCount": 5,
        "content": "Dear Community,\n\nI would like to express my heartfelt gratitude for the thoughtful mock questions that have been shared. Your generosity in providing these valuable resources has been immensely helpful. As we engage in discussions and learn together, I am reminded of the strength and camaraderie that exists within our community.\n\nTo everyone who has contributed, whether by creating questions, participating in discussions, or simply offering encouragement, thank you. Your collective efforts make this community a vibrant and supportive place for learning and growth.\n\nLet us continue to share knowledge, support one another, and celebrate our shared passion for learning"
      },
      {
        "date": "2024-02-22T04:29:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/connector-file-system?tabs=data-factory"
      },
      {
        "date": "2024-02-13T03:42:00.000Z",
        "voteCount": 2,
        "content": "1. Flatten hierarchy - Performance:\tFastest;     Schema inference:     Straightforward;\n2. Parquet"
      },
      {
        "date": "2023-12-20T09:20:00.000Z",
        "voteCount": 4,
        "content": "Merge Files: This option combines the 10 JSON files into a single Parquet file, reducing overhead and improving query performance significantly.\nParquet: This columnar format is optimized for fast queries, especially when dealing with large datasets and selective column reads. It also supports compression and schema inference."
      },
      {
        "date": "2023-09-14T21:21:00.000Z",
        "voteCount": 2,
        "content": "My answer : Merge\nSince there is no mention of preserving the hierarchy, and the need is to make the process more efficient, merge is the way to go."
      },
      {
        "date": "2023-09-06T23:49:00.000Z",
        "voteCount": 2,
        "content": "MERGE FILES since you need to make transformation and data have the same attributes\nPARQUET because is the most effcient file format"
      },
      {
        "date": "2023-08-29T07:21:00.000Z",
        "voteCount": 2,
        "content": "- Merge - Parquet"
      },
      {
        "date": "2023-08-14T06:51:00.000Z",
        "voteCount": 4,
        "content": "ChatGPT confirms it's 1. Merge, 2. Parquet"
      },
      {
        "date": "2023-04-24T22:04:00.000Z",
        "voteCount": 5,
        "content": "1. Merge Files\n2. Parquet\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-performance-tuning-guidance"
      },
      {
        "date": "2023-03-29T05:41:00.000Z",
        "voteCount": 2,
        "content": "Hey, the answer here should be \"Merge Files\" and \"Parquet\".\n\nThe question said nothing about hierarchies."
      },
      {
        "date": "2023-02-08T09:14:00.000Z",
        "voteCount": 1,
        "content": "Merge small files will be best for fast retreiving. Parquet for sink file type"
      },
      {
        "date": "2023-02-08T02:20:00.000Z",
        "voteCount": 1,
        "content": "Either preserving or flattening hierarchy has little to no performance overhead, whereas merging files causes additional performance overhead. It's perserve"
      },
      {
        "date": "2022-12-15T21:34:00.000Z",
        "voteCount": 5,
        "content": "Copy Behaviour: MERGE FILES\nBecause the small files already have same data attributes i.e. same schema. So merging all the data into one single file and converting the file to parquet makes more sense to make the query time, space and cost efficient.\n\nSink/Destination File Type: PARQUET\nThis is a no-brainer because parquet is the most efficient file format in this case in terms of time, space and cost efficiency."
      },
      {
        "date": "2022-11-09T01:23:00.000Z",
        "voteCount": 2,
        "content": "I think \"Automatically infer the schema from the underlying files\" means we should keep the same hierarchy and not merge all the data into a single file. So I would say that the first one is Preserve Hiearchy."
      },
      {
        "date": "2024-09-18T10:48:00.000Z",
        "voteCount": 1,
        "content": "Automatically infer the schema from underlying files is between b/w ADLS and Serverless sql pool. And not between JSON and parquet.correct?"
      },
      {
        "date": "2023-01-24T20:02:00.000Z",
        "voteCount": 1,
        "content": "If you preserve the hierarchy, you will keep them in small files. This will affect the performance negatively. That is why Merge is better"
      },
      {
        "date": "2022-11-17T03:25:00.000Z",
        "voteCount": 1,
        "content": "Q:\"Files are initially ingested into an Azure Data Lake Storage Gen2 account as 10 small JSON files. Each file contains the same data attributes and data from a subsidiary of your company.\" \nSince all have same data and attributes we can Merge them in one file and automatically infer the schema from the underlying 10 small files.\n- Merge\n- Parquet"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62129-exam-dp-203-topic-1-question-9-discussion/",
    "body": "HOTSPOT -<br>You have a data model that you plan to implement in a data warehouse in Azure Synapse Analytics as shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0003000001.png\" class=\"in-exam-image\"><br>All the dimension tables will be less than 2 GB after compression, and the fact table will be approximately 6 TB. The dimension tables will be relatively static with very few data inserts and updates.<br>Which type of table should you use for each table? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0003100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0003300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Replicated -<br>Replicated tables are ideal for small star-schema dimension tables, because the fact table is often distributed on a column that is not compatible with the connected dimension tables. If this case applies to your schema, consider changing small dimension tables currently implemented as round-robin to replicated.<br><br>Box 2: Replicated -<br><br>Box 3: Replicated -<br><br>Box 4: Hash-distributed -<br>For Fact tables use hash-distribution with clustered columnstore index. Performance improves when two hash tables are joined on the same distribution column.<br>Reference:<br>https://azure.microsoft.com/en-us/updates/reduce-data-movement-and-make-your-queries-more-efficient-with-the-general-availability-of-replicated-tables/ https://azure.microsoft.com/en-us/blog/replicated-tables-now-generally-available-in-azure-sql-data-warehouse/",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-15T14:27:00.000Z",
        "voteCount": 237,
        "content": "The answer is correct.\nThe Dims are under 2gb so no point in use hash.\n\nCommon distribution methods for tables:\n\nThe table category often determines which option to choose for distributing the table.\nTable category\tRecommended distribution option\nFact\t-Use hash-distribution with clustered columnstore index. Performance improves when two hash tables are joined on the same distribution column.\nDimension - Use replicated for smaller tables. If tables are too large to store on each Compute node, use hash-distributed.\nStaging - Use round-robin for the staging table. The load with CTAS is fast. Once the data is in the staging table, use INSERT...SELECT to move the data to production tables.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview#common-distribution-methods-for-tables"
      },
      {
        "date": "2021-10-04T10:30:00.000Z",
        "voteCount": 3,
        "content": "Thanks, but where in the question does it indicate about Fact table has clustered columnstore index.?"
      },
      {
        "date": "2021-10-29T02:20:00.000Z",
        "voteCount": 3,
        "content": "Normally for big tables we use clustered columnstore index for optimal performance and compression. Since the table mentioned here is in TBs we can safely assume using this index is the best choice\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"
      },
      {
        "date": "2021-10-29T02:24:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview"
      },
      {
        "date": "2023-12-13T13:40:00.000Z",
        "voteCount": 1,
        "content": "Can you please explain for Dimension table it is mentioned that \"If tables are too large\" use Hash distribution. Here Too large means how much?I am waiting for your reply\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f\ud83d\ude4f"
      },
      {
        "date": "2023-12-20T09:31:00.000Z",
        "voteCount": 4,
        "content": "exceeding 10 gigabytes (GB) are often considered large"
      },
      {
        "date": "2022-06-08T10:39:00.000Z",
        "voteCount": 7,
        "content": "This is a wonderful explanation. Worth giving a like."
      },
      {
        "date": "2021-10-23T15:36:00.000Z",
        "voteCount": 48,
        "content": "Took the exam today, this question came out. \nAns: All the Dim tables --&gt; Replicated\nFact Tables --&gt; Hash Distributed"
      },
      {
        "date": "2024-07-30T22:06:00.000Z",
        "voteCount": 1,
        "content": "this explains better\nhttps://www.youtube.com/watch?v=seiJ2xpW0h8"
      },
      {
        "date": "2024-03-14T04:14:00.000Z",
        "voteCount": 3,
        "content": "Dear Community,\n\nI would like to express my heartfelt gratitude for the thoughtful mock questions that have been shared. Your generosity in providing these valuable resources has been immensely helpful. As we engage in discussions and learn together, I am reminded of the strength and camaraderie that exists within our community.\n\nTo everyone who has contributed, whether by creating questions, participating in discussions, or simply offering encouragement, thank you. Your collective efforts make this community a vibrant and supportive place for learning and growth.\n\nLet us continue to share knowledge, support one another, and celebrate our shared passion for learning"
      },
      {
        "date": "2024-03-13T01:38:00.000Z",
        "voteCount": 3,
        "content": "All Dim Tables, if less than 2 GB, will be REPLICATED.\nFor Fact Table, if major than 10 GB, will be HASH DIST."
      },
      {
        "date": "2023-12-18T02:10:00.000Z",
        "voteCount": 1,
        "content": "REplicated,REplicated,REplicated and hash"
      },
      {
        "date": "2023-09-06T23:51:00.000Z",
        "voteCount": 1,
        "content": "Replicated / Replicated / Replicated / Hash"
      },
      {
        "date": "2023-08-29T07:24:00.000Z",
        "voteCount": 1,
        "content": "Ans: All the Dim tables --&gt; Replicated Fact Tables --&gt; Hash Distributed\nis correct"
      },
      {
        "date": "2023-07-08T09:51:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/training/modules/design-multidimensional-schema-to-optimize-analytical-workloads/3-create-tables"
      },
      {
        "date": "2023-07-08T09:45:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#batch-jobs-structure \n\nDimension tables: Replicated distribution since data movement is absent here.\nFact Table: Hash distribution to improve performance of moving data."
      },
      {
        "date": "2022-12-15T21:42:00.000Z",
        "voteCount": 3,
        "content": "Dim_*: Replicated\nSince dimension tables are less likely to get frequent updates and are usually smaller in size, replicating them across all partitions makes logical sense. Also, Tables less than 2gb size should be replicated.\n\nFact_*: Hash Distributed\nSince Fact tables are huge and have frequent insert/delete/updates going on, hash distribution is the perfect distribution candidate. Also, Tables greater than 2gb size should be hash distributed."
      },
      {
        "date": "2022-08-13T11:03:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-30T03:40:00.000Z",
        "voteCount": 2,
        "content": "Just a better link that explains the decisions. Also watch the video, it's cool.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/massively-parallel-processing-mpp-architecture"
      },
      {
        "date": "2022-05-11T04:56:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct."
      },
      {
        "date": "2022-01-25T04:14:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      },
      {
        "date": "2021-12-26T20:37:00.000Z",
        "voteCount": 2,
        "content": "Ans is correct"
      },
      {
        "date": "2021-11-01T01:26:00.000Z",
        "voteCount": 1,
        "content": "Dimension are Replicated : \n\"Since the table has multiple copies, replicated tables work best when the table size is less than 2 GB compressed.\"\n\"Replicated tables may not yield the best query performance when:\n\nThe table has frequent insert, update, and delete operations\"\n\" We recommend using replicated tables instead of round-robin tables in most cases\"\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-guidance-for-replicated-tables"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53711-exam-dp-203-topic-1-question-10-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Lake Storage Gen2 container.<br>Data is ingested into the container, and then transformed by a data integration application. The data is NOT modified after that. Users can read files in the container but cannot modify the files.<br>You need to design a data archiving solution that meets the following requirements:<br>\u2711 New data is accessed frequently and must be available as quickly as possible.<br>\u2711 Data that is older than five years is accessed infrequently but must be available within one second when requested.<br>\u2711 Data that is older than seven years is NOT accessed. After seven years, the data must be persisted at the lowest cost possible.<br>\u2711 Costs must be minimized while maintaining the required availability.<br>How should you manage the data? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0003500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0003600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Move to cool storage -<br><br>Box 2: Move to archive storage -<br>Archive - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.<br>The following table shows a comparison of premium performance block blob storage, and the hot, cool, and archive access tiers.<br><img src=\"/assets/media/exam-media/04259/0003700001.png\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-20T00:45:00.000Z",
        "voteCount": 93,
        "content": "Answer should be\n1 - Cool\n2 - Archive\n\nComparison table shown access time for cool tier ttfb is milliseconds\n\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers#comparing-block-blob-storage-options"
      },
      {
        "date": "2022-05-24T09:31:00.000Z",
        "voteCount": 5,
        "content": "Right. #1 is Cool because it's clearly mentioned in the documentation that \"Older data sets that are not used frequently, but are expected to be available for immediate access\" \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#comparing-block-blob-storage-options"
      },
      {
        "date": "2021-09-14T02:49:00.000Z",
        "voteCount": 13,
        "content": "Tricky question, it says data that is OLDER THAN (&gt; 5 years), must be available within one second when requested\n\nBut the first question asks for Five-year-old data, which is =5, so it can also be hot storage\n\nSimilarly for the seven-year-old.\n\nNot sure, please confirm?"
      },
      {
        "date": "2022-07-28T07:26:00.000Z",
        "voteCount": 4,
        "content": "\"Costs must be minimized while maintaining the required availability.\" So cold and archive are the correct answers."
      },
      {
        "date": "2024-05-13T15:18:00.000Z",
        "voteCount": 1,
        "content": "If data is frequently being used that is true for till 5 year old data =&gt; Hot \nOlder than 5 year upto 10 year rarely used but still needed sometimes on moment notice =&gt; Cool\nData older than 10 years out of the blue moon used=&gt; Archive"
      },
      {
        "date": "2024-03-18T03:07:00.000Z",
        "voteCount": 1,
        "content": "Cool and Archive, correct answer"
      },
      {
        "date": "2023-12-20T09:38:00.000Z",
        "voteCount": 2,
        "content": "Cool Tier: Cost-effective storage for less frequently accessed data, accessible within seconds when needed.\nArchive Tier: Lowest-cost storage for rarely accessed data, ideal for long-term preservation without immediate access requirements."
      },
      {
        "date": "2023-09-06T23:56:00.000Z",
        "voteCount": 1,
        "content": "1. Cool --&gt; You will access infrequently but data must be avaliable in few time if you want to access them\n2. Archive --&gt; You will never acces them but you need to configure a data archiving solution, so you must retain them always and not delete the blob"
      },
      {
        "date": "2023-08-29T07:31:00.000Z",
        "voteCount": 1,
        "content": "1.cool and 2.archive"
      },
      {
        "date": "2023-08-04T07:45:00.000Z",
        "voteCount": 1,
        "content": "cool and archive"
      },
      {
        "date": "2023-07-23T11:41:00.000Z",
        "voteCount": 2,
        "content": "Question: Why can't we just delete the blob to save more cost?"
      },
      {
        "date": "2023-05-08T12:14:00.000Z",
        "voteCount": 2,
        "content": "Answer: Cool &amp; Archive"
      },
      {
        "date": "2022-12-15T21:47:00.000Z",
        "voteCount": 2,
        "content": "5-year old data: Cool Storage\nCool Storage: can be retrieved and accessed within seconds/minutes.\n\n7-year old data: Archive Storage\nArchive Storage: takes hours to retrieve and access the data.\n\nHot Storage: can be retrieved and accessed within a few milliseconds."
      },
      {
        "date": "2022-08-10T13:13:00.000Z",
        "voteCount": 1,
        "content": "1.Cool, 2.Archive"
      },
      {
        "date": "2022-05-11T04:57:00.000Z",
        "voteCount": 1,
        "content": "ans is correct"
      },
      {
        "date": "2022-01-25T04:16:00.000Z",
        "voteCount": 1,
        "content": "ans is correct"
      },
      {
        "date": "2022-01-03T15:12:00.000Z",
        "voteCount": 1,
        "content": "1. Cool Storage\n2. Archive Storage"
      },
      {
        "date": "2021-12-26T20:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2021-06-02T02:07:00.000Z",
        "voteCount": 2,
        "content": "Answer should be \n1-hot\n2-archive\nhttps://www.bmc.com/blogs/cold-vs-hot-data-storage/\n\nCold storage data retrieval can take much longer than hot storage. It can take minutes to hours to access cold storage data"
      },
      {
        "date": "2021-06-06T01:41:00.000Z",
        "voteCount": 1,
        "content": "I also doubt if its hot storage and archive.. because its mentioned 5-year-old has to be retrieved within seconds which is not possible via cold storage//"
      },
      {
        "date": "2021-06-20T23:47:00.000Z",
        "voteCount": 1,
        "content": "but the cost factor is also there. keeping the data in hot tier for 5 years vs cold tier for 5 years would add significant amount."
      },
      {
        "date": "2021-06-06T22:58:00.000Z",
        "voteCount": 5,
        "content": "Cold storage takes milliseconds to retrieve"
      },
      {
        "date": "2021-12-17T13:06:00.000Z",
        "voteCount": 4,
        "content": "https://www.bmc.com/blogs/cold-vs-hot-data-storage/\nIt isn't about  Azure !"
      },
      {
        "date": "2023-04-10T22:17:00.000Z",
        "voteCount": 2,
        "content": "did you read the entire article? it literally says at the end:\" Cloud options are changing how we look at data computation and data storage. \"; meaning: the article DOES NOT refer to Azure and cloud options"
      },
      {
        "date": "2021-05-28T04:56:00.000Z",
        "voteCount": 8,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/54529-exam-dp-203-topic-1-question-11-discussion/",
    "body": "DRAG DROP -<br>You need to create a partitioned table in an Azure Synapse Analytics dedicated SQL pool.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0003800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0003800002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DISTRIBUTION -<br>Table distribution options include DISTRIBUTION = HASH ( distribution_column_name ), assigns each row to one distribution by hashing the value stored in distribution_column_name.<br><br>Box 2: PARTITION -<br>Table partition options. Syntax:<br>PARTITION ( partition_column_name RANGE [ LEFT | RIGHT ] FOR VALUES ( [ boundary_value [,...n] ] ))<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse<br>?",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-20T00:46:00.000Z",
        "voteCount": 116,
        "content": "Correct answer by how to remember?  Distribution option before the Partition option because\u2026 \u2018D\u2019 comes before \u2018P\u2019 or because the system needs to know the algorithm (hash, round-robin, replicate) before it can start to Partition or segment the data. (seem reasonable?)"
      },
      {
        "date": "2021-06-04T07:04:00.000Z",
        "voteCount": 67,
        "content": "Answer is correct"
      },
      {
        "date": "2024-03-26T00:19:00.000Z",
        "voteCount": 1,
        "content": "Distribution \nPartition"
      },
      {
        "date": "2023-09-06T23:58:00.000Z",
        "voteCount": 1,
        "content": "Distribution &amp; Partition"
      },
      {
        "date": "2023-08-29T21:50:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-21T23:06:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct. Here you can find an example: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition#syntax-differences-from-sql-server"
      },
      {
        "date": "2022-12-15T22:27:00.000Z",
        "voteCount": 5,
        "content": "Answer is:\n\nDISTRIBUTION = HASH (id)\nPARTITION (ID RANGE LEFT\n\tFOR VALUES (1, 1000000, 2000000) )\n\n----------------------------------------------------------\nThe table option syntax for creating a partitioned table within Dedicated SQL pool:\n\n&lt;table_option&gt; ::=\n    {\n       CLUSTERED COLUMNSTORE INDEX -- default for Azure Synapse Analytics \n    }  \n    {\n        DISTRIBUTION = HASH ( distribution_column_name )\n    }\n    | PARTITION ( partition_column_name RANGE [ LEFT | RIGHT ] -- default is LEFT  \n        FOR VALUES ( [ boundary_value [,...n] ] ) )"
      },
      {
        "date": "2022-08-13T10:34:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-06T05:28:00.000Z",
        "voteCount": 2,
        "content": "Go with a logical explanation guys..what is this D before P..if u take it like that then C comes before D as well.. Try to grasp the logics.. answer is correct."
      },
      {
        "date": "2023-08-26T15:05:00.000Z",
        "voteCount": 1,
        "content": "SAVAGE XD"
      },
      {
        "date": "2022-05-11T05:00:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2022-04-16T16:23:00.000Z",
        "voteCount": 1,
        "content": "provided answer is correct"
      },
      {
        "date": "2022-01-25T04:17:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-12-31T03:57:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't VALUES(1,1000000, 200000) create a partition for records with ID &lt;= 1 which would mean 1 row?"
      },
      {
        "date": "2022-02-06T10:31:00.000Z",
        "voteCount": 3,
        "content": "Having three boundaries will lead to four partitions:\n1. Partition for values &lt; 1\n2. Partition for values from 1 to 999999\n3. Partition for values from 1000000 to 199999\n4. Partition for values &gt;= 2000000"
      },
      {
        "date": "2022-02-17T05:31:00.000Z",
        "voteCount": 2,
        "content": "but only &lt;= and &gt;. it is range left for values, right"
      },
      {
        "date": "2021-12-26T20:46:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2021-09-27T02:21:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      },
      {
        "date": "2021-09-01T09:06:00.000Z",
        "voteCount": 1,
        "content": "Indeed! Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/53664-exam-dp-203-topic-1-question-12-discussion/",
    "body": "You need to design an Azure Synapse Analytics dedicated SQL pool that meets the following requirements:<br>\u2711 Can return an employee record from a given point in time.<br>\u2711 Maintains the latest employee information.<br>\u2711 Minimizes query complexity.<br>How should you model the employee data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tas a temporal table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tas a SQL graph table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tas a degenerate dimension table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tas a Type 2 slowly changing dimension (SCD) table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 202,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-07T06:27:00.000Z",
        "voteCount": 181,
        "content": "D is correct (voting comment that people dont have to open discussion always, please upvote to help others)"
      },
      {
        "date": "2021-05-27T08:18:00.000Z",
        "voteCount": 82,
        "content": "Answer D;  Temporal table is better than SCD2, but it is not supported in Synpase yet"
      },
      {
        "date": "2021-06-08T00:55:00.000Z",
        "voteCount": 1,
        "content": "Here's the documentation for how to implement temporal tables in Synapse from 2019. \n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-temporary"
      },
      {
        "date": "2021-06-09T08:03:00.000Z",
        "voteCount": 19,
        "content": "Temporal tables and Temporary tables are two very distinct concepts. Your link has absolutely nothing to do with this question."
      },
      {
        "date": "2021-07-10T02:51:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/temporal-tables\nAnswer : A Temporal Tables"
      },
      {
        "date": "2021-10-29T02:43:00.000Z",
        "voteCount": 1,
        "content": "I think synapse doesn't support temporal tables. Please check the below comment by hsetin."
      },
      {
        "date": "2022-03-09T00:14:00.000Z",
        "voteCount": 1,
        "content": "though this not something relative to this question. temproal tables looks alike to delta table."
      },
      {
        "date": "2023-01-28T04:54:00.000Z",
        "voteCount": 2,
        "content": "Temporarytables is completely offtopic ,the catch here is ...at a point in time hence SCD is the way to go"
      },
      {
        "date": "2023-08-04T20:04:00.000Z",
        "voteCount": 1,
        "content": "temPORAL table purpose is  to query  point in time data which can perform beyond what SCD2 can do, but its not supported in synapse"
      },
      {
        "date": "2023-12-18T02:23:00.000Z",
        "voteCount": 2,
        "content": "scd 2 maintain the iasactive current time end time like this in table"
      },
      {
        "date": "2023-11-27T02:18:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2023-09-07T00:00:00.000Z",
        "voteCount": 1,
        "content": "D \n\nBecause is the way as you can get historical data for the employee"
      },
      {
        "date": "2023-08-29T21:51:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-06-12T10:44:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-04-26T20:08:00.000Z",
        "voteCount": 2,
        "content": "D is correct."
      },
      {
        "date": "2023-02-12T00:27:00.000Z",
        "voteCount": 5,
        "content": "it is D 100%, stop messing around with answers like \"temporal table\""
      },
      {
        "date": "2023-02-08T09:16:00.000Z",
        "voteCount": 1,
        "content": "SCD type 2 which has latest boolean flag to retreive"
      },
      {
        "date": "2022-12-15T22:34:00.000Z",
        "voteCount": 5,
        "content": "Type 1 SCD - This concept overwrites the existing value within the dimension table with the new value without retaining the old data.\n\nType 2 SCD - This concept maintains the versioning of the table. It creates and adds a new row with the new value and maintains the existing row containing the old value which usually is required for historic and reporting purposes.\n\nType 3 SCD - This concept creates a new column with the new value in the existing record but also retains the original column containing the old value which usually is required for historic and reporting purposes."
      },
      {
        "date": "2022-12-10T04:58:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-11-12T08:08:00.000Z",
        "voteCount": 1,
        "content": "SCD 2 - D"
      },
      {
        "date": "2022-08-13T10:36:00.000Z",
        "voteCount": 2,
        "content": "correct D"
      },
      {
        "date": "2022-08-10T13:16:00.000Z",
        "voteCount": 1,
        "content": "Maintains the latest employee information - SCD-Type2 (Ans-D)"
      },
      {
        "date": "2022-05-11T05:14:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      },
      {
        "date": "2022-04-24T04:39:00.000Z",
        "voteCount": 2,
        "content": "Temporal tables are not supported in Synapse so D is correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52121-exam-dp-203-topic-1-question-13-discussion/",
    "body": "You have an enterprise-wide Azure Data Lake Storage Gen2 account. The data lake is accessible only through an Azure virtual network named VNET1.<br>You are building a SQL pool in Azure Synapse that will use data from the data lake.<br>Your company has a sales team. All the members of the sales team are in an Azure Active Directory group named Sales. POSIX controls are used to assign the<br>Sales group access to the files in the data lake.<br>You plan to load data to the SQL pool every hour.<br>You need to ensure that the SQL pool can load the sales data from the data lake.<br>Which three actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each area selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the managed identity to the Sales group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the managed identity as the credentials for the data load process.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a shared access signature (SAS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd your Azure Active Directory (Azure AD) account to the Sales group.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the shared access signature (SAS) as the credentials for the data load process.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a managed identity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ABF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABF",
        "count": 32,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:47:00.000Z",
        "voteCount": 50,
        "content": "Correct Answer should be\nF. Create a managed identity.\nA. Add the managed identity to the Sales group.\nB. Use the managed identity as the credentials for the data load process."
      },
      {
        "date": "2022-01-05T08:06:00.000Z",
        "voteCount": 7,
        "content": "Is answer A properly worded?\n\"Add the managed identity to the Sales group\" should be  \"Add the Sales group to managed identity\""
      },
      {
        "date": "2024-09-28T07:20:00.000Z",
        "voteCount": 1,
        "content": "Managed Identity for synapse pool is already created and available by default, so I am not sure about F."
      },
      {
        "date": "2024-04-25T08:09:00.000Z",
        "voteCount": 2,
        "content": "So Since the synapse dedicated sql pool is only accessible through an Azure virtual network wouldn't it make more since for the following steps:\n\n1. Create a managed identity for synapse analytics workspace\n2. Give role based access to managed identity\n3. Use the managed identity as the credentials for the data load process.\n\nThis would not be giving the managed identity to the sales group and follows the least permissions principle"
      },
      {
        "date": "2024-01-16T05:16:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2023-08-29T21:57:00.000Z",
        "voteCount": 1,
        "content": "FAB is correct"
      },
      {
        "date": "2022-12-15T22:40:00.000Z",
        "voteCount": 6,
        "content": "The sequence is: FAB\ncreate managed identity  --&gt;  add managed identity to the sales group  --&gt;  use managed identity as credentials for data load process."
      },
      {
        "date": "2022-11-21T14:48:00.000Z",
        "voteCount": 1,
        "content": "First create a managed ID, Add the managed ID, use the managed ID"
      },
      {
        "date": "2022-08-16T06:14:00.000Z",
        "voteCount": 1,
        "content": "ABF is the correct ans"
      },
      {
        "date": "2022-08-13T10:43:00.000Z",
        "voteCount": 3,
        "content": "ABF is correct"
      },
      {
        "date": "2022-05-11T05:20:00.000Z",
        "voteCount": 1,
        "content": "correct answer is ABF"
      },
      {
        "date": "2022-04-16T16:26:00.000Z",
        "voteCount": 1,
        "content": "ABF is correct"
      },
      {
        "date": "2022-03-25T10:23:00.000Z",
        "voteCount": 2,
        "content": "FAB - create, add to group, use to load data"
      },
      {
        "date": "2022-01-03T01:08:00.000Z",
        "voteCount": 5,
        "content": "FAB should be correct"
      },
      {
        "date": "2021-12-29T09:45:00.000Z",
        "voteCount": 2,
        "content": "FAB is correct sequence"
      },
      {
        "date": "2021-12-28T20:53:00.000Z",
        "voteCount": 2,
        "content": "1.\tCreate a managed identity.\n2.\tAdd the managed identity to the Sales group.\n3.\tUse the managed identity as the credentials for the data load process."
      },
      {
        "date": "2021-12-26T21:42:00.000Z",
        "voteCount": 1,
        "content": "FAB is correct sequence"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60686-exam-dp-203-topic-1-question-14-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics dedicated SQL pool that contains the users shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0004000001.png\" class=\"in-exam-image\"><br>User1 executes a query on the database, and the query returns the results shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0004100001.jpg\" class=\"in-exam-image\"><br>User1 is the only user who has access to the unmasked data.<br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0004200001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0004300001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: 0 -<br>The YearlyIncome column is of the money data type.<br>The Default masking function: Full masking according to the data types of the designated fields<br>\u2711 Use a zero value for numeric data types (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real).<br>Box 2: the values stored in the database<br>Users with administrator privileges are always excluded from masking, and see the original data without any mask.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-01T09:19:00.000Z",
        "voteCount": 109,
        "content": "user 1 is admin, so he will see the value stored in dbms. \n1. 0\n2. Value in database"
      },
      {
        "date": "2021-10-24T11:50:00.000Z",
        "voteCount": 2,
        "content": "2 is wrong"
      },
      {
        "date": "2023-07-30T07:33:00.000Z",
        "voteCount": 4,
        "content": "Confirmed. Everything explained here https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql-mi"
      },
      {
        "date": "2021-08-26T00:38:00.000Z",
        "voteCount": 34,
        "content": "\u2022 Use a zero value for numeric data types (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real).\n\u2022 Use 01-01-1900 for date/time data types (date, datetime2, datetime, datetimeoffset, smalldatetime, time)."
      },
      {
        "date": "2024-03-23T19:03:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is correct as admins dont necessarily have access to ddmed data. 0 / values in db"
      },
      {
        "date": "2024-01-08T13:19:00.000Z",
        "voteCount": 1,
        "content": "* Use a zero value for numeric data types (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real)."
      },
      {
        "date": "2023-09-07T00:05:00.000Z",
        "voteCount": 2,
        "content": "User 1 --&gt; Data stored in the database because this user is server_admin and can access to unmasked data\nUser 2 --&gt; 0 because this user is db_reader and can't access to unmasked data and masked_function is set in default()\n\nYou have to pay attention to masked_function when a user can't access to unmasked data to know what the user will get in the queries"
      },
      {
        "date": "2023-08-29T21:58:00.000Z",
        "voteCount": 2,
        "content": "1. 0 \n2. Value stored in database"
      },
      {
        "date": "2023-01-13T17:23:00.000Z",
        "voteCount": 5,
        "content": "Based on the information provided in the scenario:\n\nWhen User2 queries the YearlyIncome column, the values returned will be [XXXX]. This is because User2 has the role of db datareader, which means that they do not have access to the unmasked data, and the data will be masked (replaced with 'XXXX') when they query it.\n\nWhen User1 queries the BirthDate column, the values returned will be [the values stored in the database]. This is because User1 has the role of Server admin, which means that they have access to the unmasked data, and the data will be shown as it is stored in the database when they query it."
      },
      {
        "date": "2023-01-16T06:55:00.000Z",
        "voteCount": 11,
        "content": "Default masking rule is: For numeric data types use a zero value (bigint, bit, decimal, int, money, numeric, smallint, smallmoney, tinyint, float, real).\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16"
      },
      {
        "date": "2022-12-15T22:50:00.000Z",
        "voteCount": 4,
        "content": "First case: output will be 0\nUser2 queries YearlyIncome column which (is_masked = 1) i.e. its confidential thus has very limited access. Since user2 is a simple db reader, the value wouldn't be viewed by the user.\n\nSecond case: value stored in the database\nUser1 is the admin or superuser with full access to the entire data within the database sys. So he will be able to view the brithdate column (is_masked=1) in the database 'sys'."
      },
      {
        "date": "2022-11-12T08:14:00.000Z",
        "voteCount": 1,
        "content": "User2 is a reader so he will see 0 querying YearlyIncome with default() mask;\nUser1 is admin and only he will see all stored values"
      },
      {
        "date": "2022-08-13T10:47:00.000Z",
        "voteCount": 5,
        "content": "correct"
      },
      {
        "date": "2022-06-29T23:38:00.000Z",
        "voteCount": 2,
        "content": "User 1: The value\nUser 2: XXXX\nsee https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql\nAND\nhttps://www.sqlshack.com/dynamic-data-masking-in-sql-server"
      },
      {
        "date": "2024-05-29T05:08:00.000Z",
        "voteCount": 1,
        "content": "nope, not XXXX because the column is a money type, so 0"
      },
      {
        "date": "2022-06-07T02:49:00.000Z",
        "voteCount": 2,
        "content": "According to https://docs.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16\n\n\"For date and time data types use 01.01.1900 00:00:00.0000000 (date, datetime2, datetime, datetimeoffset, smalldatetime, time).\"\n\nData masking as defalut to a date dateime smalldate should be 1900-01-01. Strangely there is no such option. Any ideas anyone?"
      },
      {
        "date": "2022-06-07T02:52:00.000Z",
        "voteCount": 2,
        "content": "Damn, User 1 only reads YearlyIncome (not date), so yes 0 is the correct answer"
      },
      {
        "date": "2022-05-11T05:22:00.000Z",
        "voteCount": 1,
        "content": "1. 0\n2. Value in database"
      },
      {
        "date": "2022-04-07T01:29:00.000Z",
        "voteCount": 1,
        "content": "How user2 can access data as it is masked?"
      },
      {
        "date": "2022-04-07T01:28:00.000Z",
        "voteCount": 1,
        "content": "Can Someone explain first option as in doc it says 0"
      },
      {
        "date": "2021-12-26T21:50:00.000Z",
        "voteCount": 9,
        "content": "1. 0 (Default values for money data type for masked function will written when queried by user2)\n2. Value in database ( As it is queried by user1 who is admin )"
      },
      {
        "date": "2021-10-25T00:43:00.000Z",
        "voteCount": 3,
        "content": "CORRECT"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61286-exam-dp-203-topic-1-question-15-discussion/",
    "body": "You have an enterprise data warehouse in Azure Synapse Analytics.<br>Using PolyBase, you create an external table named [Ext].[Items] to query Parquet files stored in Azure Data Lake Storage Gen2 without importing the data to the data warehouse.<br>The external table has three columns.<br>You discover that the Parquet files have a fourth column named ItemID.<br>Which command should you run to add the ItemID column to the external table?<br>A.<br><img src=\"/assets/media/exam-media/04259/0004400001.png\" class=\"in-exam-image\"><br>B.<br><img src=\"/assets/media/exam-media/04259/0004400002.png\" class=\"in-exam-image\"><br>C.<br><img src=\"/assets/media/exam-media/04259/0004400003.png\" class=\"in-exam-image\"><br>D.<br><img src=\"/assets/media/exam-media/04259/0004500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "C",
    "answerDescription": "Incorrect Answers:<br>A, D: Only these Data Definition Language (DDL) statements are allowed on external tables:<br>\u2711 CREATE TABLE and DROP TABLE<br>\u2711 CREATE STATISTICS and DROP STATISTICS<br>\u2711 CREATE VIEW and DROP VIEW<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-01T07:20:00.000Z",
        "voteCount": 57,
        "content": "C is correct\nhttps://www.examtopics.com/discussions/microsoft/view/19469-exam-dp-200-topic-1-question-27-discussion/"
      },
      {
        "date": "2023-02-20T03:36:00.000Z",
        "voteCount": 16,
        "content": "C is correct, since \"altering the schema or format of an external SQL table is not supported\".\nhttps://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/external-sql-tables"
      },
      {
        "date": "2024-01-18T09:10:00.000Z",
        "voteCount": 5,
        "content": "Got this question on my exam on january 17, C is correct"
      },
      {
        "date": "2023-09-07T00:08:00.000Z",
        "voteCount": 2,
        "content": "C is correct since you need to drop first and create again the external table"
      },
      {
        "date": "2022-12-15T23:04:00.000Z",
        "voteCount": 5,
        "content": "Answer is C. Drop the external table and recreate it.\n\nBecause the column which needs to be added is the ItemID which seems like a primary key. So we have to drop the table and recreate it. Had it been any other column, we could have used ALTER syntax to add a column like shown below:\n\nALTER EXTERNAL TABLE name action [, ... ]\n\nwhere action is one of:\n  ADD [COLUMN] column_name type\n  DROP [COLUMN] column\n  ALTER [COLUMN] column\n  TYPE type [USING expression]\n  OWNER TO new_owner"
      },
      {
        "date": "2022-11-12T08:20:00.000Z",
        "voteCount": 1,
        "content": "C is Correct"
      },
      {
        "date": "2022-11-09T04:17:00.000Z",
        "voteCount": 1,
        "content": "I still can't understand why it's not D."
      },
      {
        "date": "2022-12-03T15:32:00.000Z",
        "voteCount": 8,
        "content": "ALTER statement is not supported on external table, you need to DROP it and CREATE it again"
      },
      {
        "date": "2022-08-13T10:49:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-07-07T07:44:00.000Z",
        "voteCount": 2,
        "content": "C is correct. Even if you are confuse with other options. The clue here is keyword Location while creating external table, LOCATION = 'folder_or_filepath' : Specifies the folder or the file path and file name for the actual data."
      },
      {
        "date": "2022-03-16T15:06:00.000Z",
        "voteCount": 2,
        "content": "Good thing the details are shown here: \"The external table has three columns.\" And the solution yet reveals the column details. This doesn't make any sense to me. If C is the correct answer (only one that seems acceptable), then the question itself is flawed."
      },
      {
        "date": "2022-11-28T05:47:00.000Z",
        "voteCount": 2,
        "content": "The external table has 3 columns, but the files it references has 4 columns, so the external table has to be altered"
      },
      {
        "date": "2022-01-26T23:10:00.000Z",
        "voteCount": 1,
        "content": "c is correct."
      },
      {
        "date": "2021-09-27T02:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60780-exam-dp-203-topic-1-question-16-discussion/",
    "body": "HOTSPOT -<br>You have two Azure Storage accounts named Storage1 and Storage2. Each account holds one container and has the hierarchical namespace enabled. The system has files that contain data stored in the Apache Parquet format.<br>You need to copy folders and files from Storage1 to Storage2 by using a Data Factory copy activity. The solution must meet the following requirements:<br>\u2711 No transformations must be performed.<br>\u2711 The original folder structure must be retained.<br>\u2711 Minimize time required to perform the copy activity.<br>How should you configure the copy activity? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0004600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0004700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Parquet -<br>For Parquet datasets, the type property of the copy activity source must be set to ParquetSource.<br><br>Box 2: PreserveHierarchy -<br>PreserveHierarchy (default): Preserves the file hierarchy in the target folder. The relative path of the source file to the source folder is identical to the relative path of the target file to the target folder.<br>Incorrect Answers:<br>\u2711 FlattenHierarchy: All files from the source folder are in the first level of the target folder. The target files have autogenerated names.<br>\u2711 MergeFiles: Merges all files from the source folder to one file. If the file name is specified, the merged file name is the specified name. Otherwise, it's an autogenerated file name.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/format-parquet https://docs.microsoft.com/en-us/azure/data-factory/connector-azure-data-lake-storage",
    "votes": [],
    "comments": [
      {
        "date": "2024-09-20T00:49:00.000Z",
        "voteCount": 82,
        "content": "This could be binary as source and sink, since there are no transformations on files. I tend to believe that would be binary the correct anwer."
      },
      {
        "date": "2022-02-14T11:49:00.000Z",
        "voteCount": 5,
        "content": "Agree. I've checked it. With binary source and sink datasets it works."
      },
      {
        "date": "2021-10-10T13:25:00.000Z",
        "voteCount": 2,
        "content": "no it must be parquet because The type property of the dataset must be set to Binary. and it's parquet hear so answer are correct"
      },
      {
        "date": "2021-09-02T05:56:00.000Z",
        "voteCount": 6,
        "content": "I agree. If it's just copying then binary is fine and would probably be faster"
      },
      {
        "date": "2021-09-25T03:17:00.000Z",
        "voteCount": 9,
        "content": "agree. When using Binary dataset, the service does not parse file content but treat it as-is.\nNot parsing the file will save the time. (https://docs.microsoft.com/en-us/azure/data-factory/format-binary)\nSo Binary!"
      },
      {
        "date": "2024-06-18T02:55:00.000Z",
        "voteCount": 1,
        "content": "\"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\""
      },
      {
        "date": "2021-09-05T06:18:00.000Z",
        "voteCount": 64,
        "content": "Answer seems correct as data is store is parquet already and requirement is to do no transformation so answer is right"
      },
      {
        "date": "2022-05-16T06:39:00.000Z",
        "voteCount": 6,
        "content": "As question has mentioned,  Minimize time required to perform the copy activity.\nAnd binary is faster than Parquet. Hence, Binary is answer"
      },
      {
        "date": "2022-12-12T08:27:00.000Z",
        "voteCount": 6,
        "content": "No: req1 \"no transformation\", req2 \"Minimize time required to perform the copy activity\". Both must be met hence it's Parquet cause it's the second fastest choice and it requires no transformations."
      },
      {
        "date": "2023-05-07T23:21:00.000Z",
        "voteCount": 4,
        "content": "when doing a binary copy, you're not doing any transformation!"
      },
      {
        "date": "2024-09-23T01:49:00.000Z",
        "voteCount": 12,
        "content": "The answer is correct. 3 reasons. \n \nThe file format is Parquet.\nParquet has the 2nd fastest load time.\nNo data transformations should happen,\n\nIf we are going to quote articles, please read the WHOLE article before posting. Check out the formats that the binary can handle. \n\n\"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\""
      },
      {
        "date": "2022-12-28T07:45:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/data-factory/format-binary"
      },
      {
        "date": "2023-06-13T06:40:00.000Z",
        "voteCount": 2,
        "content": "Binary to binary copies the files as they are, retaining the same content, hence retaining the format and it;s faster than parquet, because it doesn;t require load  at all just copy."
      },
      {
        "date": "2024-09-23T01:49:00.000Z",
        "voteCount": 12,
        "content": "According to ChatGPT\n\nWhile \"binary\" dataset type would be the fastest in terms of copying the data from one Azure storage account to another, it would not be the correct option in this scenario because it does not retain the original format of the files.\n\nIf the files contain data stored in the Apache Parquet format, specifying the source dataset type as \"binary\" would cause Data Factory to treat the files as generic binary files, and it would copy the data as is, without recognizing the original format of the files. This would result in losing the original format of the files, and possibly losing the structure of the data, it could also make it more difficult to read the data.\n\nAlso, When you copy files using binary dataset type, Data Factory will not be able to detect the changes in files and it copies the entire data each time, this can be inefficient in terms of time and storage.\n\nit really gives shitty azure answers in general, but ill go for parquet for this one."
      },
      {
        "date": "2023-06-13T06:39:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT is plainly wrong, binary type retais the original parquet format, ebcause it means to copy the files as they are and it;s faster than parquet dataset, because it's doesn't require parsing the files. Binary is correct."
      },
      {
        "date": "2024-09-23T01:49:00.000Z",
        "voteCount": 1,
        "content": "The answer is still Source dataset type: Parquet Copy activity copy behavior: Preserve Hierarchy.\n\nEven though Binary can be used as the source dataset type, it is not the best option in this scenario. The original folder structure is important, and using Parquet as the source dataset type will ensure that it is preserved.\nSource dataset type:\tParquet\nCopy activity copy behavior:\tPreserve Hierarchy\nThis will ensure that the files are copied in their original format, and that the original folder structure is preserved in the destination container. This is the best option for this scenario, as it meets all of the requirements."
      },
      {
        "date": "2024-09-23T01:49:00.000Z",
        "voteCount": 3,
        "content": "Massimo Manganiello &lt;massimo.manganiello@gmail.com&gt;\n13:36 (49 minuti fa)\na me\n\nWhen it comes to efficiency, copying data from a Parquet file to another Parquet file is generally more efficient than copying to a binary format. This is because Parquet is a columnar storage format specifically designed for efficient data compression and query performance. It leverages advanced compression techniques and data encoding to minimize storage size and optimize query execution.\n\nCopying data from a Parquet file to a binary format may require additional steps and conversions. Binary formats, such as plain text or custom binary formats, may not have the same level of built-in compression and optimization as Parquet. Therefore, the copy process may involve additional serialization and deserialization steps, resulting in increased processing overhead and potentially larger storage requirements.\n\nIn summary, when the source and destination formats are both Parquet, copying between Parquet files is generally more efficient in terms of storage utilization and query performance.\n\nIn my opinion, the provided answer are corrects!"
      },
      {
        "date": "2024-01-30T07:50:00.000Z",
        "voteCount": 2,
        "content": "Source dataset type should be set to binary.\nThe reason for this is that you\u2019re not performing any transformations on the data, you\u2019re simply copying it from one location to another while retaining the original folder structure. The binary dataset in Azure Data Factory is used for copying files as-is without parsing the file data."
      },
      {
        "date": "2023-08-29T22:02:00.000Z",
        "voteCount": 3,
        "content": "Binary &amp; PerserveHierarchy"
      },
      {
        "date": "2023-08-19T21:36:00.000Z",
        "voteCount": 4,
        "content": "Binary &amp; PerserveHierarchy\n\nThe Parquet option is used when you want to copy data stored in the Apache Parquet format and perform transformations on the data during the copy activity. However, in this scenario, the requirement is to perform no transformations and minimize the time required to perform the copy activity. The Binary option is better suited for this scenario as it copies the data as-is, without performing any transformations, and minimizes the time required to perform the copy activity."
      },
      {
        "date": "2023-07-08T17:49:00.000Z",
        "voteCount": 4,
        "content": "Answer seems correct as data is store is parquet already and requirement is to do no transformation so answer is right.\n\nSource dataset type: Parquet\nCopy activity copy behavior: Preserve Hierarchy"
      },
      {
        "date": "2023-03-30T01:32:00.000Z",
        "voteCount": 1,
        "content": "Agree. I've checked it."
      },
      {
        "date": "2022-11-26T09:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct ."
      },
      {
        "date": "2022-11-15T14:53:00.000Z",
        "voteCount": 2,
        "content": "Binary - copy files as is in fastest way.\nPreserveHierarchy - for saving folder structure."
      },
      {
        "date": "2022-11-12T08:24:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct. No transformation and preserve hierarchy"
      },
      {
        "date": "2022-11-12T02:45:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer should be Binary, since it is stated that no transformations must be done.\n\n\"You can use Binary dataset in Copy activity, GetMetadata activity, or Delete activity. When using Binary dataset, the service does not parse file content but treat it as-is.\"\nhttps://learn.microsoft.com/en-us/azure/data-factory/format-binary\n\nI couldn't found any information saying that parquet won't be parsed if the source and sink are parquets files. So I -think- it will parse, and we can understand that it is a transformation."
      },
      {
        "date": "2024-02-04T03:35:00.000Z",
        "voteCount": 1,
        "content": "When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset. Why do you pretend to preserver the hierarchy if the same hierarchy \"When using Binary dataset in copy activity, you can only copy from Binary dataset to Binary dataset.\n\""
      },
      {
        "date": "2022-09-29T01:50:00.000Z",
        "voteCount": 7,
        "content": "Answer seems correct,  \nadvice don't overthink, the source is parquet and it's one of the options so it is parquet."
      },
      {
        "date": "2022-08-13T11:12:00.000Z",
        "voteCount": 2,
        "content": "given ans is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60427-exam-dp-203-topic-1-question-17-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 container that contains 100 TB of data.<br>You need to ensure that the data in the container is available for read workloads in a secondary region if an outage occurs in the primary region. The solution must minimize costs.<br>Which type of data redundancy should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgeo-redundant storage (GRS)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tread-access geo-redundant storage (RA-GRS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tzone-redundant storage (ZRS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlocally-redundant storage (LRS)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 36,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:50:00.000Z",
        "voteCount": 115,
        "content": "B is right\nGeo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. However, that data is available to be read only if the customer or Microsoft initiates a failover from the primary to secondary region. When you enable read access to the secondary region, your data is available to be read at all times, including in a situation where the primary region becomes unavailable."
      },
      {
        "date": "2022-01-07T22:57:00.000Z",
        "voteCount": 33,
        "content": "A looks correct answer. RA-GRS is always avialable because its auto failover. Since this is not asked in the question but more importantly the question is about reducing cost which GRS."
      },
      {
        "date": "2022-10-31T11:20:00.000Z",
        "voteCount": 15,
        "content": "The question clearly says \"is available for read workloads in a secondary region\". This is only available when choosing RA-GRS.* With GRS, when a disaster happens in the primary region, the user has to initiate a failover so that the secondary region becomes the primary region**. At no point you are reading from your secondary region with GRS. Hence i believe the answers should be B.\n*https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-redundant-storage\n**https://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance"
      },
      {
        "date": "2023-05-30T02:40:00.000Z",
        "voteCount": 2,
        "content": "You misunderstanding the question : GRS also give the possibilities to read. it's not specified that we need to read from the second region when the first is available\n+ You have to reduce the cost : GRS is cheaper than RA-GRS because GRS will be available only if the first region failover (in the subject we can read IF AN OUTAGE OCCURES) : https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"
      },
      {
        "date": "2022-02-13T09:27:00.000Z",
        "voteCount": 32,
        "content": "It should be A because of two reasons:\n1. Minimize cost\n2. When primary is unavailable. \nHence No need for RA_GRS"
      },
      {
        "date": "2022-11-28T10:39:00.000Z",
        "voteCount": 7,
        "content": "Exactly. This is the point. It clearly states ' in case of an outage'   RA-GRS --&gt; secondary region can be read also not in a case of outage"
      },
      {
        "date": "2023-05-05T02:43:00.000Z",
        "voteCount": 5,
        "content": "its not A, dude, if you dont understand the difference between GRS and RA-GRS then u need az 101. With GRS, the 2nd region is NEVER available for access until Microsoft fails over the first failed region. Otherwise, you can NEVER access the 2nd regions data. Hence RA-GRS."
      },
      {
        "date": "2023-07-25T02:38:00.000Z",
        "voteCount": 10,
        "content": "No need to be rude. The question specifies that the data in the second region needs to be available IF an outage occurs. So GRS is more than enough. It's not because you think otherwise that you're right."
      },
      {
        "date": "2024-09-20T00:50:00.000Z",
        "voteCount": 76,
        "content": "In my opinion, I believe the and answer is A, and this is why.  \n\nIn the question they state \"...available for read workloads in a secondary region IF AN OUTAGE OCCURES in the primary...\".  Well, answer B (RA-GRS) states in Microsoft documentation that RA-GRS is for when \"...your data is available to be read AT ALL TIMES, including in a situation where the primary region becomes unavailable.\"  \n\nTo me, the nature of the question is what is the cheapest solution which allows for failover to read workload, when there is an outage.  Answer (A).\n\nCommon sense would be 'A' too because that is probably the most often real-life use case."
      },
      {
        "date": "2021-12-28T21:17:00.000Z",
        "voteCount": 4,
        "content": "It's not about common sense rather about technology. With GRS, data remains available even if an entire data center becomes unavailable or if there is a widespread regional failure. There would be a down time when a region becomes unavailable. Alternately, you could implement read-access geo-redundant storage (RA-GRS), which provides read-access to the data in alternate locations."
      },
      {
        "date": "2024-09-23T01:52:00.000Z",
        "voteCount": 3,
        "content": "While it is true that the customer/Microsoft has to initiate the failover, this is not elaborated in any sense in the question. What is the point of GRS if you cannot read from it after a failover? It provides the service needed, at the lowest cost.\n\nThis would be different if there were keywords like \"available immediately without downtime\"  or \"automatically\" but there are none, so well, if a region fails, you fail over, and read from secondary region. \n\nBottom line: A. GRS"
      },
      {
        "date": "2024-09-23T01:52:00.000Z",
        "voteCount": 3,
        "content": "In this scenario, the data in the secondary region only needs to be available IF the data isn't available in the primary region. Both GRS and RA-GRS accomplish that. The difference between GRS and RA-GRS is that the data in RA-GRS is always readable, even if the primary region is up, which also makes it more expensive. That is not necessary in this case, so GRS is the answer.\nSource: https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"
      },
      {
        "date": "2024-09-23T01:52:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is:\nB. read-access geo-redundant storage (RA-GRS)\n\nWith RA-GRS, your data is not only replicated to a secondary region (geo-redundant storage, GRS) but also allows read access to the data in the secondary region. This means that if there is an outage in the primary region, you can access and read the data from the secondary region, providing business continuity and reducing downtime.\n\nWhile geo-redundant storage (GRS) on its own provides data redundancy across regions, it only allows read and write access in the primary region. To meet the requirement of having read access in the secondary region during an outage, RA-GRS is the appropriate option.\n\nZone-redundant storage (ZRS) and locally-redundant storage (LRS) do not provide the capability of data redundancy across regions, so they are not suitable for ensuring read access in a secondary region during a primary region outage."
      },
      {
        "date": "2024-09-23T01:51:00.000Z",
        "voteCount": 1,
        "content": "B. read-access geo-redundant storage (RA-GRS) Most \n\nWhen configured to use globally redundant storage (GRS, GZRS, and RA-GZRS), Azure copies your data asynchronously to a secondary geographic region located hundreds of miles away. This level of redundancy allows you to recover your data if there's an outage throughout the entire primary region.\n\nRead-access geo-redundant storage (RA-GRS) and read-access geo-zone-redundant storage (RA-GZRS) also provide geo-redundant storage, but offer the added benefit of read access to the secondary endpoint. These options are ideal for applications designed for high availability business-critical applications. If the primary endpoint experiences an outage, applications configured for read access to the secondary region can continue to operate. Microsoft recommends RA-GZRS for maximum availability and durability of your storage accounts.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance"
      },
      {
        "date": "2024-07-09T06:43:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer - question clearly says 'if an outage' - then A - GRS, if it had said Secondary region should be always available for read then RA-GRS"
      },
      {
        "date": "2024-07-07T06:52:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nTo ensure that the data in your Azure Data Lake Storage Gen2 container is available for read workloads in a secondary region in case of an outage in the primary region, while also minimizing costs, you should use Read-Access Geo-Redundant Storage (RA-GRS)."
      },
      {
        "date": "2024-04-14T21:49:00.000Z",
        "voteCount": 2,
        "content": "while GRS focuses solely on disaster recovery, RA-GRS extends this by allowing read access to secondary data\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"
      },
      {
        "date": "2024-04-04T00:46:00.000Z",
        "voteCount": 3,
        "content": "B. read-access geo-redundant storage (RA-GRS) \n\nIf you want to have read-access to the storage in another area, you must choose RA-GZRS since otherwise you'll have not the second storage URL. Impossible to choose the cheapest option in this case."
      },
      {
        "date": "2024-03-25T01:53:00.000Z",
        "voteCount": 1,
        "content": "It is clearly B. That's Azure Infrastructure 1.01. With GRS you cannot Read only failover"
      },
      {
        "date": "2024-03-23T19:22:00.000Z",
        "voteCount": 1,
        "content": "Minimize costs is a"
      },
      {
        "date": "2024-03-23T19:20:00.000Z",
        "voteCount": 1,
        "content": "Doc clearly states that With an account configured for GRS or GZRS, data in the secondary region is not directly accessible to users or applications, unless a failover occurs. The failover process updates the DNS entry provided by Azure Storage so that the secondary endpoint becomes the new primary endpoint for your storage account. During the failover process, your data is inaccessible"
      },
      {
        "date": "2024-03-17T00:24:00.000Z",
        "voteCount": 1,
        "content": "I'll go for B. Keep in mind with GRS the failover process might take about an hour and within that time you WON\u00b4T BE ABLE TO READ your data from the secondary region."
      },
      {
        "date": "2024-03-11T11:29:00.000Z",
        "voteCount": 1,
        "content": "RA-GRS is an option on top of GRS. So, the regions defined for GRS are the same for RA-GRS"
      },
      {
        "date": "2024-02-13T07:40:00.000Z",
        "voteCount": 1,
        "content": "Geo-redundant storage (with GRS or GZRS) replicates your data to another physical location in the secondary region to protect against regional outages. With an account configured for GRS or GZRS, data in the secondary region is not directly accessible to users or applications, unless a failover occurs. The failover process updates the DNS entry provided by Azure Storage so that the secondary endpoint becomes the new primary endpoint for your storage account. During the failover process, your data is inaccessible"
      },
      {
        "date": "2024-01-30T13:54:00.000Z",
        "voteCount": 1,
        "content": "the answer is RA-GRS.\nRead-access geo-redundant storage (RA-GRS) is more expensive than geo-redundant storage (GRS) because it is adding additional feature - the ability to read data from the secondary location."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60376-exam-dp-203-topic-1-question-18-discussion/",
    "body": "You plan to implement an Azure Data Lake Gen 2 storage account.<br>You need to ensure that the data lake will remain available if a data center fails in the primary Azure region. The solution must minimize costs.<br>Which type of replication should you use for the storage account?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgeo-redundant storage (GRS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgeo-zone-redundant storage (GZRS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlocally-redundant storage (LRS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tzone-redundant storage (ZRS)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 134,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-29T08:03:00.000Z",
        "voteCount": 104,
        "content": "First, about the Question:\nWhat fails? -&gt; The (complete) DataCenter, not the region and not components inside a DataCenter.\n\nSo, what helps us in this situation?\nLRS: \"..copies your data synchronously three times within a single physical location in the primary region.\" Important is here the SINGLE PHYSICAL LOCATION (meaning inside the same Data Center. So in our scenario all copies wouldn't work anymore.)\n-&gt; C is wrong.\nZRS: \"...copies your data synchronously across three Azure availability zones in the primary region\" (meaning, in different Data Centers. In our scenario this would meet the requirements)\n-&gt; D is right\nGRS/GZRS: are like LRS/ZRS but with the Data Centers in different azure regions. This works too but is more expensive than ZRS. So ZRS  is the right answer.\n\nhttps://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy"
      },
      {
        "date": "2022-03-16T15:17:00.000Z",
        "voteCount": 2,
        "content": "Yes, well said, that's the correct answer."
      },
      {
        "date": "2022-02-13T04:06:00.000Z",
        "voteCount": 2,
        "content": "Well explained!"
      },
      {
        "date": "2021-08-23T05:10:00.000Z",
        "voteCount": 78,
        "content": "This can't be correct. Should be D."
      },
      {
        "date": "2021-08-24T10:22:00.000Z",
        "voteCount": 1,
        "content": "Why, LRS is cheaper?"
      },
      {
        "date": "2021-09-08T04:56:00.000Z",
        "voteCount": 13,
        "content": "It is cheaper but LRS helps to replicate data in the same data center while ZRS replicates data synchronously across three storage clusters in one region. So if one data center fails you should go for ZRS."
      },
      {
        "date": "2021-10-24T12:54:00.000Z",
        "voteCount": 6,
        "content": "Also, note that the question talks about failure in \"a data center\". As long as other data centers are running fine(as in ZRS which will have many), ZRS would be the least expensive option."
      },
      {
        "date": "2024-09-23T01:54:00.000Z",
        "voteCount": 1,
        "content": "D \nRedundancy in the primary region\nData in an Azure Storage account is always replicated three times in the primary region. Azure Storage offers two options for how your data is replicated in the primary region:\n\nLocally redundant storage (LRS) copies your data synchronously three times within a single physical location in the primary region. LRS is the least expensive replication option, but is not recommended for applications requiring high availability or durability.\nZone-redundant storage (ZRS) copies your data synchronously across three Azure availability zones in the primary region. For applications requiring high availability, Microsoft recommends using ZRS in the primary region, and also replicating to a secondary region."
      },
      {
        "date": "2024-09-23T01:53:00.000Z",
        "voteCount": 5,
        "content": "LRS is the lowest-cost redundancy option and offers the least durability compared to other options. LRS protects your data against server rack and drive failures. However, if a disaster such as fire or flooding occurs within the data center, all replicas of a storage account using LRS may be lost or unrecoverable. To mitigate this risk, Microsoft recommends using zone-redundant storage (ZRS), geo-redundant storage (GRS), or geo-zone-redundant storage (GZRS).\n\nSource: https://docs.microsoft.com/en-us/azure/storage/common/storage-redundancy#locally-redundant-storage"
      },
      {
        "date": "2024-09-18T15:55:00.000Z",
        "voteCount": 1,
        "content": "The Correct Answer is B - based on microsoft documentation\nZRS provides excellent performance, low latency, and resiliency for your data if it becomes temporarily unavailable. However, ZRS by itself might not protect your data against a regional disaster where multiple zones are permanently affected. For protection against regional disasters, we recommend using geo-zone-redundant storage (GZRS), which uses ZRS in the primary region and also geo-replicates your data to a secondary region.\n\nhttps://learn.microsoft.com/en-us/azure/storage/files/files-redundancy#zone-redundant-storage"
      },
      {
        "date": "2024-03-11T11:27:00.000Z",
        "voteCount": 1,
        "content": "RA-GRS is an option on top of GRS. So, the regions defined for GRS are the same for RA-GRS"
      },
      {
        "date": "2023-12-21T08:31:00.000Z",
        "voteCount": 1,
        "content": "zone-redundant storage (ZRS)"
      },
      {
        "date": "2023-11-14T18:35:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why the answer is ZRS. ZRS redundancy is across availability zones, not other region. The question mentioned that \"if data center fails in the primary Azure region\". Isn't it ZRS will not be available when primary Azure region fails? Correct answer should then be GRS then, which redundancy is across region, and is lower cost compare to GZRS."
      },
      {
        "date": "2023-09-11T23:27:00.000Z",
        "voteCount": 1,
        "content": "According to Microsoft \"Locally redundant storage (LRS) replicates your storage account three times within a single data center in the primary region.\" Therefore, if a the center fails, all three copies will be unavailable. However, according to the condition, the data lake should remain available if a data center fails. Azure Data Lake Gen 2 storage is based on blob storage. There is no separate data redundancy options for the Azure Data Lake Gen 2 comparing to that of blob storage within a storage account. Thereofer, option C is incorrect."
      },
      {
        "date": "2023-08-29T22:10:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-06-26T22:22:00.000Z",
        "voteCount": 2,
        "content": "LRS/ZRS doesnt come to picture if anything needs to available in other regions. so GRS is rite one."
      },
      {
        "date": "2023-04-09T02:48:00.000Z",
        "voteCount": 2,
        "content": "I understood it this way, \n1. LRS : single 3-storey building in Frankfurt &gt; Each floor has a data center &gt; if the data center fails then everything is lost\n\n2. LRS: Single 3-storey building in Frankfurt and Berlin, if data in Frankfurt center is lost, then we still have in Berlin\n\nHence it's even cheaper because they are in the same Geolocation"
      },
      {
        "date": "2023-01-30T14:32:00.000Z",
        "voteCount": 1,
        "content": "Has the question recently changed? Most of the conversation below is talking about zone failures (Availability Zone) whereas the question is talking about a \"primary Azure region\" (region). In case of a region failure, would a GRS not be required as the ZRS will protect one if one of the availability zones go down (and not the entire region)?"
      },
      {
        "date": "2023-02-04T16:51:00.000Z",
        "voteCount": 1,
        "content": "sorry ignore above... i reread the question \"one of the data centres in the primary region\"."
      },
      {
        "date": "2023-01-13T17:32:00.000Z",
        "voteCount": 1,
        "content": "that the data lake remains available if a data center fails in the primary Azure region, while minimizing costs, you should use geo-redundant storage (GRS) for the storage account. GRS stores 3 copies of the data across 2 regions, so that if a data center fails in the primary region, the data can still be accessed from the secondary region and you only pay for the primary region's storage cost."
      },
      {
        "date": "2022-10-09T05:03:00.000Z",
        "voteCount": 1,
        "content": "Microsoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 workloads."
      },
      {
        "date": "2022-08-13T11:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-05-26T22:34:00.000Z",
        "voteCount": 1,
        "content": "D is correct as it talks about \"a data center\" means we can not use the LRS (LOCAL )"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60369-exam-dp-203-topic-1-question-19-discussion/",
    "body": "HOTSPOT -<br>You have a SQL pool in Azure Synapse.<br>You plan to load data from Azure Blob storage to a staging table. Approximately 1 million rows of data will be loaded daily. The table will be truncated before each daily load.<br>You need to create the staging table. The solution must minimize how long it takes to load the data to the staging table.<br>How should you configure the table? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0005000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0005100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hash -<br>Hash-distributed tables improve query performance on large fact tables. They can have very large numbers of rows and still achieve high performance.<br>Incorrect Answers:<br>Round-robin tables are useful for improving loading speed.<br><br>Box 2: Clustered columnstore -<br>When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed.<br><br>Box 3: Date -<br>Table partitions enable you to divide your data into smaller groups of data. In most cases, table partitions are created on a date column.<br>Partition switching can be used to quickly remove or replace a section of a table.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-23T14:07:00.000Z",
        "voteCount": 405,
        "content": "Round-Robin\nHeap\nNone"
      },
      {
        "date": "2024-08-19T06:37:00.000Z",
        "voteCount": 3,
        "content": "The ET engineer who answered probably thought we have to select the options which are outrageously foolish"
      },
      {
        "date": "2022-02-13T04:11:00.000Z",
        "voteCount": 22,
        "content": "Round- Robin\nHeap\nNone.\nNo brainer for this question."
      },
      {
        "date": "2021-10-24T21:45:00.000Z",
        "voteCount": 9,
        "content": "Agree 100%.\nAll in paragraphs under this: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview."
      },
      {
        "date": "2022-01-02T08:19:00.000Z",
        "voteCount": 4,
        "content": "Also agree 100%"
      },
      {
        "date": "2024-03-14T06:07:00.000Z",
        "voteCount": 1,
        "content": "Agree -  Round Robin, Heap and None are the correct options. The solution doesn't make any sense for a staging table to truncate every day without any JOINs.Don't know why it says Hash/Columnstored/Date."
      },
      {
        "date": "2021-08-29T06:31:00.000Z",
        "voteCount": 65,
        "content": "Round-robin - this is the simplest distribution model, not great for querying but fast to process\nHeap - no brainer when creating staging tables\nNo partitions - this is a staging table, why add effort to partition, when truncated daily?"
      },
      {
        "date": "2021-10-29T03:15:00.000Z",
        "voteCount": 2,
        "content": "Had doubts regarding why there is no need for a partition. While what you suggested is true won't it be better if there is a date partition to truncate the table ?"
      },
      {
        "date": "2022-02-11T07:41:00.000Z",
        "voteCount": 4,
        "content": "There is no filter on a truncate statement so no benefit in having a partition"
      },
      {
        "date": "2021-11-02T06:15:00.000Z",
        "voteCount": 1,
        "content": "Can you explain me why should we use heap?"
      },
      {
        "date": "2022-01-02T08:21:00.000Z",
        "voteCount": 12,
        "content": "The term heap basically refers to a table without a clustered index. Adding a clustered index to a temp table makes absolutely no sense and is a waste of compute resources for a table that would be entirely truncated daily. \n\nno clustered index = heap."
      },
      {
        "date": "2022-02-22T19:37:00.000Z",
        "voteCount": 3,
        "content": "DrTaz is right, in addition, when you populate an indexed table, you are also writing to the index, so this adds an additional overhead in the write process"
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 4,
        "content": "as per Microsoft document \n\nLoad to a staging table\nTo achieve the fastest loading speed for moving data into a data warehouse table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option.\n\nConsider that loading is usually a two-step process in which you first load to a staging table and then insert the data into a production data warehouse table. If the production table uses a hash distribution, the total time to load and insert might be faster if you define the staging table with the hash distribution. Loading to the staging table takes longer, but the second step of inserting the rows to the production table does not incur data movement across the distributions."
      },
      {
        "date": "2021-12-29T10:48:00.000Z",
        "voteCount": 1,
        "content": "It doesn't mention the prd table. Only the staging. So, round Robin/Heap is the answer, correct? tricky questions. \n:)"
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 15,
        "content": "Answer: Round-Robin (1), Heap (2), None (3).\nWithin this doc:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview\n#1. Search for \u201cUse round-robin for the staging table.\u201d\n#2. Search for: \u201cA heap table can be especially useful for loading data, such as a staging table,\u2026\u201d\nWithin this doc:\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition?context=/azure/synapse-analytics/context/context\n#3. Partitioning by date is useful when stage destination has data because you can hide the inserting data\u2019s new partition (to keep users from hitting it), complete the load and then unhide the new partition.\nHowever, in this question it states, \u201cthe table will be truncated before each daily load\u201d, so, it appears it\u2019s a true Staging table and there are no users with access, no existing data, and I see no reason to have a Date partition. To me, such a partition would do nothing but slow the load."
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 1,
        "content": "Since it's the staging table, the main focus should be minimizing the load time, as noted in the question. Heap table does not have any index and it's the fastest option for loading large amounts of data. Using a round-robin distribution will help to evenly distribute the data across all the distributions, further reducing the load time. As the data is truncated before each load, partitioning is not necessary, so it is best to choose None.\nRound-robin\nHeap\nNone"
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 2,
        "content": "Distribution: Round Robin\nIndexing: Clustered Columnstore\nPartitioning: Date\n\nThe recommended configuration for a staging table that will be loaded daily with approximately 1 million rows of data and truncated before each load is to use a round robin distribution, a clustered columnstore index, and date-based partitioning. Round robin distribution will evenly distribute the data across nodes, reducing the load time. Clustered columnstore index provides efficient compression and supports fast bulk load operations. Date-based partitioning will allow for easy archiving and maintenance of the table."
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 6,
        "content": "Round-Robing, Heap, None.\n\nThe question is to configure the staging table.\n\nAccording to the conditions, \"The solution must minimize how long it takes to load the data to the staging table.\"\n\nTherefore, loading time is the most essential condition here.\n\nAccording to Microsoft documentation at\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-overview\n\n\nDistribution: A round-robin table distributes table rows evenly across all distributions. The rows are distributed randomly. Loading data into a round-robin table is fast. But, queries can require more data movement than the other distribution methods.\n\nIndexing: A heap table can be especially useful for loading transient data, such as a staging table, which is transformed into a final table.\n\nPartitioning: None. Since the table is truncated before each daily load, we can not benefit of partitioning to drop date ranges."
      },
      {
        "date": "2024-09-23T01:55:00.000Z",
        "voteCount": 3,
        "content": "1 --&gt; REPLICATED \nSince table has always 1mln rows (it is a small staging table, loaded in truncate-insert everyday!)\nAlso Round-Robin is wrong, since 1mln/60nodes = almost 17k rows per compute node, which is a unrecommended situation. You haven't data enough to choose round-robin. \n\n2 --&gt; HEAP\nWhy do you have to choose clustered columnstore index? This is not a big table. \n\n3 --&gt; None\nSince you haven't data enough to create a efficient partition. It's more convenient to have only one partition in this case."
      },
      {
        "date": "2024-07-25T10:59:00.000Z",
        "voteCount": 2,
        "content": "Copilot\nSent by Copilot:\nTo optimize the loading process for your staging table in Azure Synapse, here are the recommended configurations:\n\nDistribution: Use Round-robin distribution. This method evenly distributes the data across all distributions, which helps in achieving balanced data loading and minimizes data skew.\nIndexing: Use Heap. Since the table will be truncated before each load, using a heap (a table without a clustered index) will speed up the data loading process as it avoids the overhead of maintaining indexes during the load.\nPartitioning: Do not partition the staging table. Partitioning can add overhead to the data loading process. Since the table is truncated daily, partitioning is not necessary and can be avoided to keep the load process efficient.\nThese configurations will help minimize the time it takes to load data into the staging table."
      },
      {
        "date": "2024-06-03T09:59:00.000Z",
        "voteCount": 1,
        "content": "RR\nHeap\nDate"
      },
      {
        "date": "2024-04-30T01:14:00.000Z",
        "voteCount": 1,
        "content": "Round robin Heap None"
      },
      {
        "date": "2024-04-11T09:39:00.000Z",
        "voteCount": 1,
        "content": "RR\nHeap\nNone"
      },
      {
        "date": "2023-11-16T06:49:00.000Z",
        "voteCount": 5,
        "content": "Round-Robin\nHeap\nNone"
      },
      {
        "date": "2023-08-29T22:13:00.000Z",
        "voteCount": 1,
        "content": "Round-Robin Heap None"
      },
      {
        "date": "2023-08-24T00:26:00.000Z",
        "voteCount": 1,
        "content": "Round-Robin\nHeap\nNone"
      },
      {
        "date": "2023-05-31T12:36:00.000Z",
        "voteCount": 2,
        "content": "Never ever use date partioning with hash distribution.\nThe correct answer is: Round robin, Heap and None"
      },
      {
        "date": "2023-04-25T19:47:00.000Z",
        "voteCount": 1,
        "content": "Round-robin - this is the simplest distribution model, not great for querying but fast to process\nHeap - no brainer when creating staging tables\nNo partitions - this is a staging table"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61794-exam-dp-203-topic-1-question-20-discussion/",
    "body": "You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.<br><img src=\"/assets/media/exam-media/04259/0005200001.png\" class=\"in-exam-image\"><br>FactPurchase will have 1 million rows of data added daily and will contain three years of data.<br>Transact-SQL queries similar to the following query will be executed daily.<br><br>SELECT -<br>SupplierKey, StockItemKey, IsOrderFinalized, COUNT(*)<br><br>FROM FactPurchase -<br><br>WHERE DateKey &gt;= 20210101 -<br><br>AND DateKey &lt;= 20210131 -<br>GROUP By SupplierKey, StockItemKey, IsOrderFinalized<br>Which table distribution will minimize query times?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treplicated",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash-distributed on PurchaseKey\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround-robin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash-distributed on IsOrderFinalized"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 71,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-22T07:30:00.000Z",
        "voteCount": 46,
        "content": "Correct"
      },
      {
        "date": "2022-07-24T04:17:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-09-10T05:35:00.000Z",
        "voteCount": 28,
        "content": "Is it  hash-distributed on PurchaseKey and not on IsOrderFinalized because 'IsOrderFinalized' yields less distributions(rows either contain yes,no values) compared to PurchaseKey?"
      },
      {
        "date": "2021-09-13T19:15:00.000Z",
        "voteCount": 9,
        "content": "Yes, your logic is correct!"
      },
      {
        "date": "2024-02-05T03:36:00.000Z",
        "voteCount": 1,
        "content": "Plus its better to use hash distribution on column where group by or joins are used"
      },
      {
        "date": "2024-07-06T02:53:00.000Z",
        "voteCount": 1,
        "content": "Optimal Distribution\nGiven that the query performs a GROUP BY on SupplierKey, StockItemKey, and IsOrderFinalized, the most balanced approach is to use Round-robin distribution. While it does not ensure that rows with the same key are stored together, it avoids data skew and ensures even distribution, which helps in achieving better performance for aggregate queries."
      },
      {
        "date": "2024-04-04T00:34:00.000Z",
        "voteCount": 2,
        "content": "almost exactly what's shown in a example of the official docs --&gt; https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column"
      },
      {
        "date": "2024-01-02T16:14:00.000Z",
        "voteCount": 1,
        "content": "Correct. Column with many unique values.  Also, it's USUALLY not a column that is used in whereclauses or groupings or such, which this isn't."
      },
      {
        "date": "2023-10-06T23:10:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choosing-a-distribution-column"
      },
      {
        "date": "2023-09-14T05:33:00.000Z",
        "voteCount": 1,
        "content": "why the answer says it cannot be a date column?"
      },
      {
        "date": "2023-09-11T22:38:00.000Z",
        "voteCount": 2,
        "content": "Hash-distributed tables improve query performance on large fact tables. The PurchaseKey has many unique values, does not have NULLs and is not a date column."
      },
      {
        "date": "2023-09-15T05:58:00.000Z",
        "voteCount": 3,
        "content": "why we cannot use data column?"
      },
      {
        "date": "2023-08-29T22:16:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-05T15:32:00.000Z",
        "voteCount": 1,
        "content": "in this case the sql where condition is on  datekey so  hash-distributed on PurchaseKey  or Round robin    distributed table  the sql cost will  be the same as it will be full table scan"
      },
      {
        "date": "2023-08-05T15:44:00.000Z",
        "voteCount": 3,
        "content": "on second thought if purchasekey is not unique what is the constraint and how its created  , as  the question didn't mention   more details , i would  go with round robin   not the has distributed"
      },
      {
        "date": "2023-05-11T04:52:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-05-09T14:11:00.000Z",
        "voteCount": 1,
        "content": "B. Hash the purchasekey to evenly distribute the data into 60 distributions."
      },
      {
        "date": "2023-02-01T11:44:00.000Z",
        "voteCount": 3,
        "content": "B is the Correct Answer"
      },
      {
        "date": "2023-01-25T04:23:00.000Z",
        "voteCount": 2,
        "content": "B is correct."
      },
      {
        "date": "2023-01-21T14:39:00.000Z",
        "voteCount": 5,
        "content": "Ideally there should be an option to create partition DateKey. When we use the partition key column in the where condition , the unwanted partition's data will be eliminated automatically.  that's the beauty of the partition and how it works in conjunction with the query. However, would like to know from the experts in the forum."
      },
      {
        "date": "2023-01-14T22:48:00.000Z",
        "voteCount": 1,
        "content": "what about B plus (imaginary) partitioning on date ? Or is error in question because Purchase Key by itself would not be very helpful"
      },
      {
        "date": "2022-12-04T05:16:00.000Z",
        "voteCount": 3,
        "content": "B is correct!!!\n\nBecause \"hash-distributed on IsOrderFinalized\" as a distribution column would only use 2 out of 60 distributions (for Yes, No) which is a waste of compute and time resources. \n\nSo \"hash-distributed on PurchaseKey\" with multiple unique values will utilize all 60 distributions and make the query process much faster and utilize all the compute efficiently."
      },
      {
        "date": "2024-02-17T15:44:00.000Z",
        "voteCount": 2,
        "content": "but purchasekey is unique and each entries will create unique purchasekey. So, how it is helpful in distribution using hashing on all unique entries purchasekey. Although it is fact-table, it is cost effective to use Roundrobin."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60955-exam-dp-203-topic-1-question-21-discussion/",
    "body": "HOTSPOT -<br>From a website analytics system, you receive data extracts about user interactions such as downloads, link clicks, form submissions, and video plays.<br>The data contains the following columns.<br><img src=\"/assets/media/exam-media/04259/0005400001.png\" class=\"in-exam-image\"><br>You need to design a star schema to support analytical queries of the data. The star schema will contain four tables including a date dimension.<br>To which table should you add each column? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0005500001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0005600001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: DimEvent -<br><br>Box 2: DimChannel -<br><br>Box 3: FactEvents -<br>Fact tables store observations or events, and can be sales orders, stock balances, exchange rates, temperatures, etc<br>Reference:<br>https://docs.microsoft.com/en-us/power-bi/guidance/star-schema",
    "votes": [],
    "comments": [
      {
        "date": "2021-10-24T21:56:00.000Z",
        "voteCount": 73,
        "content": "It seems to be correct"
      },
      {
        "date": "2022-03-21T07:38:00.000Z",
        "voteCount": 25,
        "content": "What is this question? It is poorly written. I couldn't even understand what's being asked here. It talks about 4 tables, yet the answer shows 3. Then, the columns mentioned in the question don't match the column/attributes shown in the 3 tables noted in the answer."
      },
      {
        "date": "2022-09-29T02:15:00.000Z",
        "voteCount": 1,
        "content": "the question is clear try to read it again, there is no need to design DIM_DATE"
      },
      {
        "date": "2022-06-25T05:09:00.000Z",
        "voteCount": 4,
        "content": "Question says,including a dimTime table so we need to design only 3 tables."
      },
      {
        "date": "2024-03-26T04:40:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2024-03-19T09:33:00.000Z",
        "voteCount": 1,
        "content": "Correct answer"
      },
      {
        "date": "2023-09-07T00:17:00.000Z",
        "voteCount": 1,
        "content": "Correct provided answer"
      },
      {
        "date": "2023-09-03T06:01:00.000Z",
        "voteCount": 1,
        "content": "EventCategory -&gt; dimEvent\nchannelGrouping -&gt; dimChannel\nTotalEvents -&gt; factEven"
      },
      {
        "date": "2023-05-11T04:53:00.000Z",
        "voteCount": 1,
        "content": "DimEvent / DimChannel / FactEvents"
      },
      {
        "date": "2023-02-01T11:48:00.000Z",
        "voteCount": 2,
        "content": "Category or Group will go with the table DIM, Total Events will go with the Fact"
      },
      {
        "date": "2022-08-13T11:28:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-26T22:30:00.000Z",
        "voteCount": 4,
        "content": "EventCategory -&gt; dimEvent\nchannelGrouping -&gt; dimChannel\nTotalEvents -&gt; factEven"
      },
      {
        "date": "2022-05-11T06:14:00.000Z",
        "voteCount": 1,
        "content": "EventCategory -&gt; dimEvent\nchannelGrouping -&gt; dimChannel\nTotalEvents -&gt; factEven"
      },
      {
        "date": "2022-05-08T00:04:00.000Z",
        "voteCount": 4,
        "content": "EventCategory ==&gt; dimEvent\nchannelGrouping ==&gt; dimChannel\nTotalEvents ==&gt; factEvent\n\nExplanation:\nA bit of knowledge of Google Analytics Universal helps to understand this question. eventCategory, eventAction and eventLabel all contain information about the event/action done on the website, and can be logically be grouped together. ChannelGrouping is about how the user came on the website (through Google, and advertisement, an email link, etc.) and is not related to events at all. It therefore would make sense to put it in a second dim table."
      },
      {
        "date": "2021-12-27T06:17:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct"
      },
      {
        "date": "2021-08-29T06:41:00.000Z",
        "voteCount": 4,
        "content": "I would add ChannelGrouping to DimEvents table. What would DimChannel table contain? only one column? No sense to me"
      },
      {
        "date": "2022-02-28T13:00:00.000Z",
        "voteCount": 1,
        "content": "I mean if you think about it, ChannelName (facebook,google,youtube), ChannelType (paid media, free posts, ads), ChannleDelivery (chrome, etc etc). Just thinking out loud"
      },
      {
        "date": "2021-09-01T00:33:00.000Z",
        "voteCount": 3,
        "content": "It is supposed to contain 4 tables. Date, Event, Fact so the logical conclusion would be to include the channel dimension. If it were up to me though I'd use the channel as a degenerate dimension and store it in fact table if it's the only information that we have provided."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62349-exam-dp-203-topic-1-question-22-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You convert the files to compressed delimited text files.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-10-05T06:35:00.000Z",
        "voteCount": 18,
        "content": "They said you need to prepare the files to copy,  maybe the mean we should make them less than 1MB ? so it will be A else would be B !!!!"
      },
      {
        "date": "2022-01-13T16:50:00.000Z",
        "voteCount": 5,
        "content": "The answer should be A.\nhttps://azure.microsoft.com/en-gb/blog/increasing-polybase-row-width-limitation-in-azure-sql-data-warehouse/"
      },
      {
        "date": "2021-10-06T05:36:00.000Z",
        "voteCount": 4,
        "content": "After reading the other questions oh this topic I go with A because the relevant part seems to be the compression."
      },
      {
        "date": "2023-12-20T00:36:00.000Z",
        "voteCount": 1,
        "content": "i think when compression is in question we should go  for parquet/avro bcz only they give compression"
      },
      {
        "date": "2024-09-20T00:52:00.000Z",
        "voteCount": 1,
        "content": "\"a\" is correct option"
      },
      {
        "date": "2024-09-20T00:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is yes"
      },
      {
        "date": "2024-09-20T00:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is A\nCompression doesn't not only help to reduce  the size or space occupied by a file in a storage but also increases the speed of file movement during   transfer"
      },
      {
        "date": "2023-02-01T11:49:00.000Z",
        "voteCount": 3,
        "content": "A will do the job"
      },
      {
        "date": "2022-11-26T10:02:00.000Z",
        "voteCount": 1,
        "content": "Delimited text file is true,"
      },
      {
        "date": "2022-10-07T20:14:00.000Z",
        "voteCount": 4,
        "content": "For the fastest load, use compressed delimited text files\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices"
      },
      {
        "date": "2022-08-14T05:41:00.000Z",
        "voteCount": 1,
        "content": "yes, answer is A"
      },
      {
        "date": "2022-07-26T02:19:00.000Z",
        "voteCount": 3,
        "content": "Selected Answer: A\nPolyBase can't load rows that have more than 1,000,000 bytes of data. When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data. This byte limitation is true regardless of the table schema.\nAll file formats have different performance characteristics. For the fastest load, use compressed delimited text files. Split large compressed files into smaller compressed files."
      },
      {
        "date": "2024-03-26T04:49:00.000Z",
        "voteCount": 1,
        "content": "PolyBase can load more than 1 mln rows since SQL Server 2019\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"
      },
      {
        "date": "2022-07-24T04:40:00.000Z",
        "voteCount": 2,
        "content": "A is correct ,with copy command\nPolyBase\t                               COPY\nNeeds CONTROL permission\tRelaxed permission\nHas row width limits\t                No row width limit\nNo delimiters within text\t        Supports delimiters in text\nFixed line delimiter\t                Supports custom column and row delimiters\nComplex to set up in code\t         Reduces amount of code"
      },
      {
        "date": "2022-06-07T22:49:00.000Z",
        "voteCount": 4,
        "content": "It's just a copy to storage so zipping it will work fine."
      },
      {
        "date": "2022-05-26T22:39:00.000Z",
        "voteCount": 1,
        "content": "It says about files compression. which will reduce the file size. so Answer is correct"
      },
      {
        "date": "2022-04-26T07:44:00.000Z",
        "voteCount": 2,
        "content": "A text file seems to be too simple an answer however true as per the microsoft link.I was thinking of parquet/avro files"
      },
      {
        "date": "2022-03-09T10:05:00.000Z",
        "voteCount": 3,
        "content": "From the question: \"75% of the rows contain description data that has an average length of 1.1 MB\". You can't \nFrom the documentation: \"When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data.\"\nSo 75% of rows aren't good for a delimited text files... why you said answer is yes?"
      },
      {
        "date": "2022-03-11T10:15:00.000Z",
        "voteCount": 3,
        "content": "I initially thought so too, however isn't this limit only relevant to PolyBase copy? It is not mentioned which method is used to transfer the data so you could fit more than 1mb into a column in the table if you want to, you just have to use something else e.g. COPY command."
      },
      {
        "date": "2022-01-27T01:47:00.000Z",
        "voteCount": 2,
        "content": "correct answer."
      },
      {
        "date": "2021-12-27T06:24:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2021-12-11T01:27:00.000Z",
        "voteCount": 2,
        "content": "Correcto"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61378-exam-dp-203-topic-1-question-23-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You copy the files to a table that has a columnstore index.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-12T06:37:00.000Z",
        "voteCount": 12,
        "content": "From the documentation, loads to heap table are faster than indexed tables. So, better to use heap table than columnstore index table in this case.\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index#heap-tables"
      },
      {
        "date": "2021-10-19T12:17:00.000Z",
        "voteCount": 8,
        "content": "Consider this sets one question:\nWhat should you do to improve loading times?\nWhat                      | Yes | No |\ncompressed          |  O   |   O  |\ncolumnstore          |  O   |   O  |\n&gt; 1MB                     |  O   |   O  |\n\nSo now answers should be clear"
      },
      {
        "date": "2021-10-22T07:31:00.000Z",
        "voteCount": 11,
        "content": "Can You explain this in more details?"
      },
      {
        "date": "2021-12-07T12:13:00.000Z",
        "voteCount": 7,
        "content": "I really didn't understand this , can you explain?"
      },
      {
        "date": "2022-08-09T00:56:00.000Z",
        "voteCount": 2,
        "content": "it's a virtualized chart that the guy want to simplified the question set."
      },
      {
        "date": "2022-09-09T02:02:00.000Z",
        "voteCount": 5,
        "content": "but it becomes more complicated with this chart haha"
      },
      {
        "date": "2022-09-15T04:44:00.000Z",
        "voteCount": 5,
        "content": "I Think what he tried to show was:\nSet Answer Matrix\n\nWhat should you do to improve loading times?\nWhat | Yes | No |\ncompressed | X | O |\ncolumnstore | O | X |\n&gt; 1MB | O | X |\n\nSo all three variations of this question and x is marking the correct answer."
      },
      {
        "date": "2023-09-03T06:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is no ,u use HEAP idx"
      },
      {
        "date": "2023-01-23T13:02:00.000Z",
        "voteCount": 1,
        "content": "For fast loading to a table, using a staging table which is a heap table."
      },
      {
        "date": "2023-01-21T14:46:00.000Z",
        "voteCount": 1,
        "content": "its always recommended to load the data into a staging where the table should be a heap table and data will be loaded using ROUND_ROBIN mechanism"
      },
      {
        "date": "2022-08-14T05:42:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-07-26T02:28:00.000Z",
        "voteCount": 4,
        "content": "Correct Answer: B\nTo achieve the fastest loading speed for moving data into a data warehouse table, load data into a staging table. Define the staging table as a heap and use round-robin for the distribution option"
      },
      {
        "date": "2022-07-24T05:18:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-04-07T23:39:00.000Z",
        "voteCount": 4,
        "content": "Columnstore index would be used for faster reading, but the question is only about faster loading. So for faster loading you want the least possible overhead. So the answer should be no. Am I right?"
      },
      {
        "date": "2022-04-26T07:47:00.000Z",
        "voteCount": 1,
        "content": "Yes load to a table without indexes for faster load right?"
      },
      {
        "date": "2022-02-28T02:24:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2022-01-27T01:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct."
      },
      {
        "date": "2022-01-15T00:14:00.000Z",
        "voteCount": 1,
        "content": "NO is the answer."
      },
      {
        "date": "2021-12-27T06:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-07T07:08:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: No."
      },
      {
        "date": "2021-10-06T04:09:00.000Z",
        "voteCount": 3,
        "content": "No, The index will expand the time of insertion"
      },
      {
        "date": "2021-09-02T06:58:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/guidance-for-loading-data. \"For the fastest load, use compressed delimited text files.\""
      },
      {
        "date": "2021-09-11T03:51:00.000Z",
        "voteCount": 4,
        "content": "But the row size also need to be &lt; 1 MB\nSo, files need to be modified to make all rows &lt; 1 MB\nAnswer: NO"
      },
      {
        "date": "2021-10-22T07:39:00.000Z",
        "voteCount": 1,
        "content": "In other words, i think that 100GB is much to much for the columnstore index memorywise. The documentation in unclear with the context of this particular question, but i think the ansewer is NO, as ithe given answer is the wrong idea anyways."
      },
      {
        "date": "2021-09-23T04:19:00.000Z",
        "voteCount": 2,
        "content": "Correct answer should be NO"
      },
      {
        "date": "2021-10-22T07:36:00.000Z",
        "voteCount": 1,
        "content": "Not Row size, row NUMBER have to be at maximum of 1,048,576 rows. \n\"When there is memory pressure, the columnstore index might not be able to achieve maximum compression rates. This effects query performance.\""
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62031-exam-dp-203-topic-1-question-24-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You modify the files to ensure that each row is more than 1 MB.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T07:54:00.000Z",
        "voteCount": 13,
        "content": "No, rows need to have less than 1 MB. A batch size between 100 K to 1M rows is the recommended baseline for determining optimal batch size capacity."
      },
      {
        "date": "2022-01-02T02:59:00.000Z",
        "voteCount": 8,
        "content": "PolyBase can't load rows that have more than 1,000,000 bytes of data. When you put data into the text files in Azure Blob storage or Azure Data Lake Store, they must have fewer than 1,000,000 bytes of data. This byte limitation is true regardless of the table schema."
      },
      {
        "date": "2022-03-11T10:20:00.000Z",
        "voteCount": 2,
        "content": "is it stated anywhere that we have to use PolyBase? What about COPY command?"
      },
      {
        "date": "2024-01-30T11:40:00.000Z",
        "voteCount": 1,
        "content": "Azure synapse analytics use polybase by default."
      },
      {
        "date": "2022-01-02T02:59:00.000Z",
        "voteCount": 2,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/data-loading-best-practices#prepare-data-in-azure-storage"
      },
      {
        "date": "2023-09-03T06:10:00.000Z",
        "voteCount": 1,
        "content": "Answer is no"
      },
      {
        "date": "2022-12-04T05:42:00.000Z",
        "voteCount": 2,
        "content": "B is correct!!!"
      },
      {
        "date": "2022-08-14T05:45:00.000Z",
        "voteCount": 1,
        "content": "B is correct, agree with explanation by Amar"
      },
      {
        "date": "2022-01-27T01:51:00.000Z",
        "voteCount": 4,
        "content": "B is correct."
      },
      {
        "date": "2021-12-27T06:33:00.000Z",
        "voteCount": 1,
        "content": "Answer is No"
      },
      {
        "date": "2021-12-07T07:09:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: No."
      },
      {
        "date": "2021-10-19T12:16:00.000Z",
        "voteCount": 1,
        "content": "Consider this sets one question:\nWhat should you do to improve loading times?\nWhat                      | Yes | No |\ncompressed          |  O   |   O  |\ncolumnstore          |  O   |   O  |\n&gt; 1MB                     |  O   |   O  |\n\nSo now answers should be clear"
      },
      {
        "date": "2021-11-24T01:57:00.000Z",
        "voteCount": 5,
        "content": "@Odoxtoom, can you please explain your answer and specify based on this matrix which option is correct."
      },
      {
        "date": "2021-12-25T02:05:00.000Z",
        "voteCount": 2,
        "content": "Yes \nNo\nNo"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67460-exam-dp-203-topic-1-question-25-discussion/",
    "body": "You build a data warehouse in an Azure Synapse Analytics dedicated SQL pool.<br>Analysts write a complex SELECT query that contains multiple JOIN and CASE statements to transform data for use in inventory reports. The inventory reports will use the data and additional WHERE parameters depending on the report. The reports will be produced once daily.<br>You need to implement a solution to make the dataset available for the reports. The solution must minimize query times.<br>What should you implement?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan ordered clustered columnstore index",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta materialized view\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tresult set caching",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta replicated table"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-07T14:34:00.000Z",
        "voteCount": 26,
        "content": "B is correct.\n\nMaterialized view and result set caching\n\nThese two features in dedicated SQL pool are used for query performance tuning. Result set caching is used for getting high concurrency and fast response from repetitive queries against static data.\n\nTo use the cached result, the form of the cache requesting query must match with the query that produced the cache. In addition, the cached result must apply to the entire query.\n\nMaterialized views allow data changes in the base tables. Data in materialized views can be applied to a piece of a query. This support allows the same materialized views to be used by different queries that share some computation for faster performance."
      },
      {
        "date": "2021-12-15T09:10:00.000Z",
        "voteCount": 12,
        "content": "B is the correct answer.\nA materialized view is a database object that contains the results of a query. A materialized view is not simply a window on the base table. It is actually a separate object holding data in itself. So query data against a materialized view with different filters should be quick.\nDifference Between View and Materialized View: \nhttps://techdifferences.com/difference-between-view-and-materialized-view.html"
      },
      {
        "date": "2024-01-18T09:22:00.000Z",
        "voteCount": 3,
        "content": "Got this question on my exam on january 17, B is correct"
      },
      {
        "date": "2024-01-09T14:46:00.000Z",
        "voteCount": 2,
        "content": "Materialized Views:\n\nCreate materialized views that store the results of the complex SELECT queries. Materialized views are precomputed views stored as tables, and they can significantly reduce query times by avoiding the need to recompute the results every time the query is executed."
      },
      {
        "date": "2024-01-03T03:24:00.000Z",
        "voteCount": 1,
        "content": "A. an ordered clustered columnstore index =&gt; this will impact the where clause so it is a valid option\nB. a materialized =&gt; can't be used since there is no aggregation\nC. result set caching =&gt; We don't know if the output query respects the limitation (10 gb ) so no\nD. a replicated table =&gt; sizes of lookups tables not mentioned so even if it is a possible solution it's not a suggested approach"
      },
      {
        "date": "2023-10-31T07:05:00.000Z",
        "voteCount": 4,
        "content": "Was on my exam today (31.10.2023)."
      },
      {
        "date": "2023-09-03T06:12:00.000Z",
        "voteCount": 1,
        "content": "Materialized view"
      },
      {
        "date": "2023-01-06T08:51:00.000Z",
        "voteCount": 3,
        "content": "There is no information that this query aggregates data.\n\n\"SELECT list in the materialized view definition needs to meet at least one of these two criteria:\n\nThe SELECT list contains an aggregate function.\nGROUP BY is used in the Materialized view definition and all columns in GROUP BY are included in the SELECT list. Up to 32 columns can be used in the GROUP BY clause.\"\n\nI'm not sure that B is correct answer.\nUnfortunately I cannot see better answer"
      },
      {
        "date": "2024-07-26T18:29:00.000Z",
        "voteCount": 1,
        "content": "A properly designed materialized view provides :\nReduce the execution time for complex queries with JOINs and aggregate functions. The more complex the query, the higher the potential for execution-time saving. The most benefit is gained when a query's computation cost is high and the resulting data set is small. So the right answer is B"
      },
      {
        "date": "2022-07-24T09:54:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-06-28T08:40:00.000Z",
        "voteCount": 1,
        "content": "B is correct acc to: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views\n\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/performance-tuning-materialized-views"
      },
      {
        "date": "2022-05-08T23:41:00.000Z",
        "voteCount": 2,
        "content": "B  materialized view"
      },
      {
        "date": "2022-04-16T16:37:00.000Z",
        "voteCount": 2,
        "content": "B is correct without a doubt"
      },
      {
        "date": "2022-03-21T08:48:00.000Z",
        "voteCount": 1,
        "content": "Why isn't the answer \"A\" when the query may have additional WHERE parameters depending on the report. That mean's the query isn't static and will change depending on the report. A clustered columstore index would provide a bettery query performance in case of a complex query where query isn't static."
      },
      {
        "date": "2022-06-25T00:35:00.000Z",
        "voteCount": 3,
        "content": "I was thinking on the same level initially but there are multiple tables informed and apply column store indexes an all tables would not ensure good results materialized view would store the results of complex calculations and it would be faster to just query those results i believe even if extra where clauses are applied"
      },
      {
        "date": "2022-01-27T02:00:00.000Z",
        "voteCount": 1,
        "content": "B correct."
      },
      {
        "date": "2021-12-30T02:52:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2021-12-27T06:35:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2021-12-16T07:33:00.000Z",
        "voteCount": 2,
        "content": "B materialized view"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67459-exam-dp-203-topic-1-question-26-discussion/",
    "body": "You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.<br>You plan to create a database named DB1 in Pool1.<br>You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.<br>Which format should you use for the tables in DB1?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tORC",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-14T07:45:00.000Z",
        "voteCount": 23,
        "content": "Both A and D are correct \n\"For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database. As such, you can shut down your Spark pools and still query Spark external tables from serverless SQL pool.\""
      },
      {
        "date": "2022-10-21T02:32:00.000Z",
        "voteCount": 5,
        "content": "\"For each Spark external table based on Parquet or CSV and located in Azure Storage, an external table is created in a serverless SQL pool database. As such, you can shut down your Spark pools and still query Spark external tables from serverless SQL pool.\"\n\nSo A and D. Parquet are faster so D"
      },
      {
        "date": "2022-11-15T16:41:00.000Z",
        "voteCount": 3,
        "content": "But they never asked about faster, so why it cant be A"
      },
      {
        "date": "2023-01-18T09:29:00.000Z",
        "voteCount": 19,
        "content": "In this business, time is money."
      },
      {
        "date": "2024-05-23T03:36:00.000Z",
        "voteCount": 1,
        "content": "Both A and D, but for priority i decide D"
      },
      {
        "date": "2023-09-03T06:14:00.000Z",
        "voteCount": 2,
        "content": "Prefer to D"
      },
      {
        "date": "2023-06-14T20:17:00.000Z",
        "voteCount": 3,
        "content": "Tipos de archivo adecuados para consultas anal\u00edticas\n- Si tiene cargas de trabajo basadas en Hive o Presto, vaya con ORC.\n- Si tiene cargas de trabajo basadas en Spark o Drill, vaya con Parquet."
      },
      {
        "date": "2022-08-14T09:37:00.000Z",
        "voteCount": 2,
        "content": "both A and D; D - Parquet is faster"
      },
      {
        "date": "2022-07-10T07:32:00.000Z",
        "voteCount": 3,
        "content": "Option D as the explanation suggests. Parquet is always faster than CSV being columnar data store."
      },
      {
        "date": "2022-06-25T01:43:00.000Z",
        "voteCount": 2,
        "content": "both A and D are okay, but Paraquet is faster than CSV, so answer is D."
      },
      {
        "date": "2022-05-26T22:50:00.000Z",
        "voteCount": 2,
        "content": "Both A &amp; D are correct , as the explanation also suggest same."
      },
      {
        "date": "2022-05-22T07:50:00.000Z",
        "voteCount": 2,
        "content": "Both A and D"
      },
      {
        "date": "2022-05-22T07:54:00.000Z",
        "voteCount": 1,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/metadata/database"
      },
      {
        "date": "2022-04-27T06:35:00.000Z",
        "voteCount": 2,
        "content": "Looks correct to me"
      },
      {
        "date": "2022-03-12T21:05:00.000Z",
        "voteCount": 2,
        "content": "JSON is also supported by Serverless SQL Pool but it is kinda complicated. Why is it not selected?"
      },
      {
        "date": "2022-02-23T08:31:00.000Z",
        "voteCount": 2,
        "content": "Looks correct to me"
      },
      {
        "date": "2022-02-19T07:05:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-27T02:03:00.000Z",
        "voteCount": 1,
        "content": "Both A and D are correct. as CSV and Parquet are correct answers."
      },
      {
        "date": "2021-12-27T07:33:00.000Z",
        "voteCount": 4,
        "content": "Parquet and CSV are correct"
      },
      {
        "date": "2021-12-09T17:59:00.000Z",
        "voteCount": 3,
        "content": "I think A and D are both correct answers."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67461-exam-dp-203-topic-1-question-27-discussion/",
    "body": "You are planning a solution to aggregate streaming data that originates in Apache Kafka and is output to Azure Data Lake Storage Gen2. The developers who will implement the stream processing solution use Java.<br>Which service should you recommend using to process the streaming data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Databricks\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-09T18:02:00.000Z",
        "voteCount": 24,
        "content": "Correct!"
      },
      {
        "date": "2023-09-03T06:16:00.000Z",
        "voteCount": 2,
        "content": "DataBricks with Java lang"
      },
      {
        "date": "2023-06-16T04:47:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-03-12T21:03:00.000Z",
        "voteCount": 2,
        "content": "D is the correct one for sure."
      },
      {
        "date": "2022-12-10T05:37:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2022-09-14T02:42:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-07-24T10:18:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2022-07-10T07:34:00.000Z",
        "voteCount": 3,
        "content": "Azure Databrics as the question is clearly asking the support for Java programming."
      },
      {
        "date": "2022-04-10T22:56:00.000Z",
        "voteCount": 1,
        "content": "why not C: Azure Stream Analytics?"
      },
      {
        "date": "2022-04-10T22:57:00.000Z",
        "voteCount": 2,
        "content": "I see, Azure Stream Analytics does not associate with Java"
      },
      {
        "date": "2022-05-23T04:00:00.000Z",
        "voteCount": 1,
        "content": "or databricks"
      },
      {
        "date": "2022-05-23T04:01:00.000Z",
        "voteCount": 2,
        "content": "kafka*"
      },
      {
        "date": "2022-04-26T07:57:00.000Z",
        "voteCount": 1,
        "content": "Yes Azure stream Analytics for streaming data?"
      },
      {
        "date": "2022-01-27T02:05:00.000Z",
        "voteCount": 2,
        "content": "correct."
      },
      {
        "date": "2021-12-27T06:42:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      },
      {
        "date": "2021-12-09T15:26:00.000Z",
        "voteCount": 4,
        "content": "Respuesta correcta Azure DataBricks."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67462-exam-dp-203-topic-1-question-28-discussion/",
    "body": "You plan to implement an Azure Data Lake Storage Gen2 container that will contain CSV files. The size of the files will vary based on the number of events that occur per hour.<br>File sizes range from 4 KB to 5 GB.<br>You need to ensure that the files stored in the container are optimized for batch processing.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the files to JSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the files to Avro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress the files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMerge the files\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:56:00.000Z",
        "voteCount": 51,
        "content": "You can not merge the files if u don't know how many files exist in ADLS2. In this case, you could easily create a file larger than 100 GB in size and decrease performance. so B is the correct answer. Convert to AVRO"
      },
      {
        "date": "2023-06-16T04:55:00.000Z",
        "voteCount": 15,
        "content": "Option B: Convert the files to Avro (WRONG&nbsp;FOR&nbsp;ME)\nWhile converting the files to Avro is a valid option for optimizing data storage and processing, it may not be the most suitable choice in this specific scenario. Avro is a binary serialization format that is efficient for compact storage and fast data processing. It provides schema evolution support and is widely used in big data processing frameworks like Apache Hadoop and Apache Spark.\n\nHowever, in the given scenario, the files are already in CSV format. Converting them to Avro would require additional processing and potentially introduce complexity. Avro is better suited for scenarios where data is generated or consumed by systems that natively support Avro or for cases where schema evolution is a critical requirement.\n\nOn the other hand, merging the files (Option D) is a more straightforward and common approach to optimize batch processing. It helps reduce the overhead associated with managing a large number of small files, improves data scanning efficiency, and enhances overall processing performance. Merging files is a recommended practice to achieve better performance and cost efficiency in scenarios where file sizes vary."
      },
      {
        "date": "2024-08-31T11:32:00.000Z",
        "voteCount": 1,
        "content": "Avro is often used when schema evolution and efficient serialization are needed, but merging files is the primary solution for optimizing batch processing when dealing with a large number of small files.\n\nASNWER IS D !"
      },
      {
        "date": "2022-04-29T07:26:00.000Z",
        "voteCount": 4,
        "content": "I can understand why you say not merge, but why avro?"
      },
      {
        "date": "2022-09-08T15:25:00.000Z",
        "voteCount": 2,
        "content": "Because we need to ensure files stored in the container are optimized for batch processing. converting the files to AVRO would be suitable for optimized for batch processing. So, the answer is \"Convert to AVRO\""
      },
      {
        "date": "2022-09-16T00:46:00.000Z",
        "voteCount": 3,
        "content": "The information about the file size is already given which is between 5KB to 5GB. So option D seems to be correct."
      },
      {
        "date": "2024-09-20T00:56:00.000Z",
        "voteCount": 26,
        "content": "If you store your data as many small files, this can negatively affect performance. In general, organize your data into larger sized files for better performance (256 MB to 100 GB in size). \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest"
      },
      {
        "date": "2024-09-20T00:56:00.000Z",
        "voteCount": 1,
        "content": "D. Merge the files\nExplanation:\nFile Merging: Merging small files into larger ones helps to reduce the overhead associated with processing many small files, which can negatively impact performance. Large files are typically more efficient for batch processing in distributed systems like Azure Data Lake and Azure Databricks because they minimize the number of tasks and the amount of metadata the system needs to manage.\n\nCompressing the Files (Option C Incorrect): While compression can reduce storage costs and improve I/O efficiency, it doesn't address the issue of small file sizes, which can still lead to inefficiencies in distributed processing.\n\nConverting to Avro (Option B Incorrect) or JSON (Option A Incorrect): Converting to a different file format like Avro or JSON could be beneficial for specific use cases, especially where schema evolution or specific query optimizations are required. However, this does not address the fundamental issue of optimizing file size for batch processing.\n\nTherefore, merging the files (Option D) is the most direct and effective way to optimize for batch processing in this scenario."
      },
      {
        "date": "2024-05-27T23:11:00.000Z",
        "voteCount": 1,
        "content": "Compress the files is my choise"
      },
      {
        "date": "2024-04-30T01:51:00.000Z",
        "voteCount": 1,
        "content": "pay attention to the \"number of events that occur per hour.\" That means it is streaming and you can group small files with streaming engines - it is D"
      },
      {
        "date": "2024-04-18T03:00:00.000Z",
        "voteCount": 1,
        "content": "Convert to AVRO is Legit"
      },
      {
        "date": "2024-04-14T19:13:00.000Z",
        "voteCount": 1,
        "content": "C. Compress the files\n\nCompression is beneficial for batch processing as it can significantly reduce the file size, which leads to faster transfer rates and can improve performance during batch processing tasks. It\u2019s particularly effective for large files, making it easier to handle and process them efficiently"
      },
      {
        "date": "2024-04-14T12:00:00.000Z",
        "voteCount": 1,
        "content": "option d"
      },
      {
        "date": "2024-01-31T07:32:00.000Z",
        "voteCount": 3,
        "content": "The answer is compress the files\n- we can't merge because we don't know how many files we will receive, and the performance will be decreased if the size of the merged files is too large.\n- converting the files to Avro will require additional processing and if we receive too many files it may cause complexity. \n- compressing the CSV files is the best choice in our scenario, compressing the files is a common practice for optimizing batch processing. It helps reduce storage space, minimize data transfer times, and improve overall performance."
      },
      {
        "date": "2024-01-23T14:35:00.000Z",
        "voteCount": 5,
        "content": "Crazy how OPTION D (which is wrong) has such a high ammount of votes. \nI'm going for C.compress files. Guys you don't know how many files you have, you can't design a system where this kind of randomness can make your system to fail."
      },
      {
        "date": "2024-01-09T15:04:00.000Z",
        "voteCount": 1,
        "content": "Binary Serialization:\n\nAvro uses a compact binary format, making it more efficient in terms of storage and transmission compared to plain text formats like CSV. This can be advantageous for batch processing scenarios, especially when dealing with large volumes of data.\nSchema Evolution:\n\nAvro supports schema evolution, allowing you to change the schema of your data without requiring modifications to the entire dataset or affecting backward compatibility. This flexibility is beneficial in scenarios where your data schema may evolve over time.\nCompression:\n\nWhile Avro itself is a binary format that provides some level of compression, you can further enhance compression by applying additional compression algorithms. This is particularly useful when dealing with large files, and it helps to reduce storage costs and improve data transfer efficiency."
      },
      {
        "date": "2024-01-03T03:40:00.000Z",
        "voteCount": 4,
        "content": "A. Convert the files to JSON =&gt; no sense\nB. Convert the files to Avro =&gt; my understanding is that the format of the file csv is given, so no\nC. Compress the files =&gt; for batch processing it's a win and it's the only option that you can assume true given the available information\nD. Merge the files =&gt; this can be true but not knowing how many files there is big issue"
      },
      {
        "date": "2023-12-22T08:34:00.000Z",
        "voteCount": 1,
        "content": "AVRO is binary format, so it will be optimized for batch processing. \nProblem with merging files is that it is still CSV, String typed which has to be parsed when processing later. Therefore, it would not qualify as being optimized for batch processing."
      },
      {
        "date": "2023-12-22T03:29:00.000Z",
        "voteCount": 1,
        "content": "compress the files"
      },
      {
        "date": "2023-12-17T03:54:00.000Z",
        "voteCount": 1,
        "content": "the confusing point in this question is that we will not know how many files are expected in an hour, if that's the case. will merging files really be helpful?"
      },
      {
        "date": "2023-09-15T01:32:00.000Z",
        "voteCount": 2,
        "content": "compressing the CSV files is the most practical and efficient way to optimize them for batch processing, especially when dealing with varying file sizes"
      },
      {
        "date": "2023-09-03T21:42:00.000Z",
        "voteCount": 4,
        "content": "Sometimes, data pipelines have limited control over the raw data, which has lots of small files. In general, we recommend that your system have some sort of process to aggregate small files into larger ones for use by downstream applications. If you're processing data in real time, you can use a real time streaming engine (such as Azure Stream Analytics or Spark Streaming) together with a message broker (such as Event Hubs or Apache Kafka) to store your data as larger files. As you aggregate small files into larger ones, consider saving them in a read-optimized format such as Apache Parquet for downstream processing.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices#optimize-for-data-ingest"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67463-exam-dp-203-topic-1-question-29-discussion/",
    "body": "HOTSPOT -<br>You store files in an Azure Data Lake Storage Gen2 container. The container has the storage policy shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0006500001.png\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0006600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0006700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: moved to cool storage -<br>The ManagementPolicyBaseBlob.TierToCool property gets or sets the function to tier blobs to cool storage. Support blobs currently at Hot tier.<br><br>Box 2: container1/contoso.csv -<br>As defined by prefixMatch.<br>prefixMatch: An array of strings for prefixes to be matched. Each rule can define up to 10 case-senstive prefixes. A prefix string must start with a container name.<br>Reference:<br>https://docs.microsoft.com/en-us/dotnet/api/microsoft.azure.management.storage.fluent.models.managementpolicybaseblob.tiertocool",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-16T08:01:00.000Z",
        "voteCount": 41,
        "content": "correct"
      },
      {
        "date": "2021-12-17T03:12:00.000Z",
        "voteCount": 3,
        "content": "why the .csv?"
      },
      {
        "date": "2021-12-21T07:31:00.000Z",
        "voteCount": 18,
        "content": "It matches anything that starts with \"container1/contoso\" and the csv in the answer is the only one that matches."
      },
      {
        "date": "2021-12-09T15:30:00.000Z",
        "voteCount": 8,
        "content": "Respuesta Cool Tier &amp; Container1/contoso.csv"
      },
      {
        "date": "2023-11-23T00:26:00.000Z",
        "voteCount": 1,
        "content": "The tireToCool values is empty, should this rule being skipped? \nShould the data keep in hot storage at day 30, and being deleted at day 60? \n\n\"tierToCool\": {\n                          \"daysAfterModificationGreaterThan\":\n\nhttps://stackoverflow.com/questions/62368999/skip-rule-creation-if-parameter-is-empty-in-azure-storage-life-cycle-management"
      },
      {
        "date": "2023-12-01T09:44:00.000Z",
        "voteCount": 1,
        "content": "It's not empty: the value \"30\" is on the next line"
      },
      {
        "date": "2023-09-03T06:19:00.000Z",
        "voteCount": 3,
        "content": "Move to Cool Tier &amp; Container1/contoso.csv"
      },
      {
        "date": "2023-05-10T12:41:00.000Z",
        "voteCount": 1,
        "content": "Move to cold storage is the right answer, but the statement, has to be \"files which are idle for more than 30 days have to be moved to a cold storage\", If the files get modified before the 30 day period, say weekly, it remains in the hot storage tier."
      },
      {
        "date": "2022-12-15T19:39:00.000Z",
        "voteCount": 1,
        "content": "ah yes  move to Cool storage and  Container1/contoso.csv \nbecause prefixmatch is Container1/contoso"
      },
      {
        "date": "2022-08-14T09:42:00.000Z",
        "voteCount": 2,
        "content": "Cool Tier &amp; Container1/contoso.csv"
      },
      {
        "date": "2022-07-25T03:40:00.000Z",
        "voteCount": 2,
        "content": "given answer is correct"
      },
      {
        "date": "2022-06-30T01:33:00.000Z",
        "voteCount": 1,
        "content": "I think the responses do not match the question. There is no policy for cold storage here (it only says delete after 60 days) and as far as I know there is no such thing as a default duration for moving things to the cold storage if lifecycle is enabled\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-policy-configure?tabs=azure-portal"
      },
      {
        "date": "2022-06-30T01:35:00.000Z",
        "voteCount": 3,
        "content": "sorry overlooked the cold tier policy, please ignore"
      },
      {
        "date": "2022-01-06T15:39:00.000Z",
        "voteCount": 2,
        "content": "shouldn't the question be greater than 60 days?"
      },
      {
        "date": "2022-01-28T05:26:00.000Z",
        "voteCount": 4,
        "content": "The files get deleted after 60 days but after 30 days they are moved to the cool storage."
      },
      {
        "date": "2021-12-27T07:36:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67536-exam-dp-203-topic-1-question-30-discussion/",
    "body": "You are designing a financial transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will have a clustered columnstore index and will include the following columns:<br>\u2711 TransactionType: 40 million rows per transaction type<br>\u2711 CustomerSegment: 4 million per customer segment<br>\u2711 TransactionMonth: 65 million rows per month<br>AccountType: 500 million per account type<br><img src=\"/assets/media/exam-media/04259/0006700005.png\" class=\"in-exam-image\"><br>You have the following query requirements:<br>\u2711 Analysts will most commonly analyze transactions for a given month.<br>\u2711 Transactions analysis will typically summarize transactions by transaction type, customer segment, and/or account type<br>You need to recommend a partition strategy for the table to minimize query times.<br>On which column should you recommend partitioning the table?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCustomerSegment",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccountType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransactionType",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransactionMonth\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-09-20T00:54:00.000Z",
        "voteCount": 8,
        "content": "Considering row count, the answer is Transaction Month or Account Type. Partitioning with Trans. Month, it will end up with hot partition and cannot utilize parallel processing, because all data for the query will be in the same partition. To minimize the processing time, parallel processing should happen. So Transaction Month can not be the answer. My answer is Account Type.\n\nThere is a similar question in Microsoft site and the explanation was given with answer as follows.\nProduct ensures parallelism when querying data from a given month within the same region, or multiple regions.\nUsing date and partitioning by month, all sales for a month will be in the same partition, not providing parallelism.\nAll sales for a given region will be in the same partition, not providing parallelism...."
      },
      {
        "date": "2023-05-23T04:07:00.000Z",
        "voteCount": 2,
        "content": "I think you are confusing partition with distribution. Each partition will still be distributed over 60 distributions."
      },
      {
        "date": "2023-07-25T06:31:00.000Z",
        "voteCount": 1,
        "content": "The Microsoft question is also about partitioning."
      },
      {
        "date": "2021-12-10T02:47:00.000Z",
        "voteCount": 7,
        "content": "Agree with D, should not be confused with Distribution column for Hash-distributed tables."
      },
      {
        "date": "2022-08-13T09:01:00.000Z",
        "voteCount": 4,
        "content": "Partitioning and Distributing are different concepts\nIn most cases data is partitioned on a date column that is closely tied to the order in which the data is loaded into the SQL pool.  \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      },
      {
        "date": "2024-09-20T00:54:00.000Z",
        "voteCount": 4,
        "content": "While you choose month, there is no parallelization at all, so this is the worst possible answer. \nThe correct answer is AccountType. This ensures the PARALELLIZATION, which is improving the performance. \nCustomerSegment and TransactionType are below 60 million, which is not enough for 60 nodes with 1 mln for each..."
      },
      {
        "date": "2024-05-17T23:29:00.000Z",
        "voteCount": 2,
        "content": "Parallelization is for distributions I believe, and every partition will automatically be distributed as well. Partitioning is done to reduce the amount of data that is scanned. If I partition the data by months, then each month partition is further distributed into 60 distributions which are split across different nodes, thereby ensuring parallelism"
      },
      {
        "date": "2024-09-20T00:54:00.000Z",
        "voteCount": 1,
        "content": "I assume, the listed figures are the number of unique values in each columns. TransactionMonth is a bad partition, as queries would typically fall into one partition. AccountType has the most unique values, those are better for partitioning."
      },
      {
        "date": "2024-08-19T07:31:00.000Z",
        "voteCount": 1,
        "content": "Don't worry about the listed figures. They are not specific to the question, but is copied text from an example provided on MS-Docs."
      },
      {
        "date": "2024-03-23T03:48:00.000Z",
        "voteCount": 1,
        "content": "The best column to partition the table on would be D. TransactionMonth.\n\nPartitioning is a database design technique which is used to improves performance, manageability, simplifies maintenance and reduce the response time of the query. Given that analysts will most commonly analyze transactions for a given month, partitioning on the TransactionMonth column would allow the database to quickly isolate the relevant rows for a given month, thereby minimizing query times. This is particularly beneficial when dealing with large volumes of data, as is the case here.\n\nMoreover, partitioning on TransactionMonth would also align with the typical transaction analysis patterns, which summarize transactions by transaction type, customer segment, and/or account type. This would allow for efficient querying and analysis of the data.\n\nTherefore, the TransactionMonth column would be the most suitable choice for partitioning the table."
      },
      {
        "date": "2024-01-31T06:22:00.000Z",
        "voteCount": 1,
        "content": "the answer is transaction month\n\"In most cases data is partitioned on a date column that is closely tied to the order in which the data is loaded into the SQL pool.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool"
      },
      {
        "date": "2024-03-05T23:40:00.000Z",
        "voteCount": 1,
        "content": "Hello. I have a question. What is difference between partitioning and distribution?"
      },
      {
        "date": "2023-11-09T19:35:00.000Z",
        "voteCount": 2,
        "content": "i felt partitioning then going with something that is not unique, if it is like hashing or even distribution depends on the case to go with more unique."
      },
      {
        "date": "2023-10-07T10:28:00.000Z",
        "voteCount": 2,
        "content": "Partitioning by date will lead to hot partitions since the most common query is by date. Therefore this is not a good idea. Next best option is AccountType. We want to take advantage of parallel processing to improve efficiency"
      },
      {
        "date": "2023-09-17T03:55:00.000Z",
        "voteCount": 2,
        "content": "Answer is D, because the analyst will be querying transactions for month, and then its mentioned that transaction analysis will be done on Transaction_type, customer_segment and account_type, meaning they won't be querying for an individual columns but all 3 at the same time, which means it's pointless to partition between these columns, so transaction month is the answer"
      },
      {
        "date": "2023-09-07T00:28:00.000Z",
        "voteCount": 2,
        "content": "Clustered columnstore index has 60 partitions and each partition needs a minimum of 1 million rows so TransactionMonth is the only that has more than 60 millions of rows.\n\nCorrect answer: D"
      },
      {
        "date": "2023-09-03T06:21:00.000Z",
        "voteCount": 1,
        "content": "Partition by month is a good idra"
      },
      {
        "date": "2023-08-04T09:09:00.000Z",
        "voteCount": 1,
        "content": "transaction month"
      },
      {
        "date": "2023-06-01T01:23:00.000Z",
        "voteCount": 4,
        "content": "B, as selecting the date column as a partition will make one partition a hot partition and won't be able to make use of parallel processing, so Account Type should be the correct answer."
      },
      {
        "date": "2023-05-09T14:17:00.000Z",
        "voteCount": 3,
        "content": "D is correct. In most of the cases, partition the data by date."
      },
      {
        "date": "2023-04-29T15:55:00.000Z",
        "voteCount": 2,
        "content": "Account Type"
      },
      {
        "date": "2023-03-03T01:08:00.000Z",
        "voteCount": 3,
        "content": "I think A.\nIf you partition over month, then when the reports are run for a given month all the data will just be contained in a single partition? For that type of query, you want the data split over the partitions. TransactionType is the most common report slicer with the largest number of rows per TransactionType (40M, compared to 4M for CustomerSegment). AccountType has 500M rows per value, but this is not used in all the queries (it says and/or in the question)."
      },
      {
        "date": "2023-05-05T00:22:00.000Z",
        "voteCount": 1,
        "content": "I think you are confusing partition with distribution. Each partition will still be distributed over 60 distributions. Its just to have \"smaller\" tables separated (partitioned) by month. So instead of looking through a huge table your querys only browse one or two smaller ones, hence way faster queue times"
      },
      {
        "date": "2023-07-25T06:33:00.000Z",
        "voteCount": 1,
        "content": "I agree that that sounds logical, but there is a Microsoft practice exam question that looks like this one, also about partitioning, which also mentions parallel processing."
      },
      {
        "date": "2023-02-03T22:33:00.000Z",
        "voteCount": 1,
        "content": "D is the right answer"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67538-exam-dp-203-topic-1-question-31-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Lake Storage Gen2 account named account1 that stores logs as shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0006800003.png\" class=\"in-exam-image\"><br>You do not expect that the logs will be accessed during the retention periods.<br>You need to recommend a solution for account1 that meets the following requirements:<br>\u2711 Automatically deletes the logs at the end of each retention period<br>\u2711 Minimizes storage costs<br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0006900003.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0006900004.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Store the infrastructure logs in the Cool access tier and the application logs in the Archive access tier<br>For infrastructure logs: Cool tier - An online tier optimized for storing data that is infrequently accessed or modified. Data in the cool tier should be stored for a minimum of 30 days. The cool tier has lower storage costs and higher access costs compared to the hot tier.<br>For application logs: Archive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours.<br>Data in the archive tier should be stored for a minimum of 180 days.<br>Box 2: Azure Blob storage lifecycle management rules<br>Blob storage lifecycle management offers a rule-based policy that you can use to transition your data to the desired access tier when your specified conditions are met. You can also use lifecycle management to expire data at the end of its life.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-10T02:51:00.000Z",
        "voteCount": 88,
        "content": "\"Data must remain in the Archive tier for at least 180 days or be subject to an early deletion charge. For example, if a blob is moved to the Archive tier and then deleted or moved to the Hot tier after 45 days, you'll be charged an early deletion fee equivalent to 135 (180 minus 45) days of storing that blob in the Archive tier.\" &lt;- from the sourced link.\n\nThis explains why we have to use two different access tiers rather than both as archive."
      },
      {
        "date": "2022-07-08T05:38:00.000Z",
        "voteCount": 2,
        "content": "Thanks a ton for explaining."
      },
      {
        "date": "2023-02-04T12:50:00.000Z",
        "voteCount": 1,
        "content": "Thanks for sharing this info."
      },
      {
        "date": "2023-06-14T16:08:00.000Z",
        "voteCount": 4,
        "content": "\"You do not expect that the logs will be accessed during the retention periods.\" - including deletes, i suppose.  you just let lifecycle management rule do the deletes after the retention period ... Archiving for cost-reduction?"
      },
      {
        "date": "2021-12-22T14:31:00.000Z",
        "voteCount": 20,
        "content": "The answers are correct.\n\nData must remain in the Archive tier for at least 180 days or be subject to an early deletion charge. For example, if a blob is moved to the Archive tier and then deleted or moved to the Hot tier after 45 days, you'll be charged an early deletion fee equivalent to 135 (180 minus 45) days of storing that blob in the Archive tier.\nA blob in the Cool tier in a general-purpose v2 accounts is subject to an early deletion penalty if it is deleted or moved to a different tier before 30 days has elapsed. This charge is prorated. For example, if a blob is moved to the Cool tier and then deleted after 21 days, you'll be charged an early deletion fee equivalent to 9 (30 minus 21) days of storing that blob in the Cool tier.\n\nhttps://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview"
      },
      {
        "date": "2024-05-27T23:29:00.000Z",
        "voteCount": 1,
        "content": "configuring a trigger in a pipeline to delete logs when they reach their expiration date is a valid approach to automatically manage log retention in Azure."
      },
      {
        "date": "2024-03-21T09:21:00.000Z",
        "voteCount": 1,
        "content": "Correct answers"
      },
      {
        "date": "2023-09-03T06:24:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-10T23:11:00.000Z",
        "voteCount": 3,
        "content": "Given answer is correct."
      },
      {
        "date": "2023-06-07T21:33:00.000Z",
        "voteCount": 1,
        "content": "The answers are correct"
      },
      {
        "date": "2023-05-11T04:55:00.000Z",
        "voteCount": 2,
        "content": "Infrastructure: archive access tier (and pay penalty for early deletion) / application: archive access tier ; Azure Blob storage lifecycle management rules"
      },
      {
        "date": "2023-04-14T05:20:00.000Z",
        "voteCount": 2,
        "content": "as per microsoft price lists: https://azure.microsoft.com/en-us/pricing/details/storage/blobs/  \nto store 50 terabytes in cool storage is around 500 usd/month\nto store 50 terabytes in archive storage is around 50 usd/month\nthe retention for infrastructure logs is 60 days; files must remain in archive for minimum of 180 otherwise early delete penalty is to be paid; oki doki, 180 days minus 60 days, is 120 days; so around 4 month; 4*50 usd = 200 usd; its still WAY cheaper than cool storage; and the logs will not be accessed during retention (and even if they are , in archive they can be also retrieved, who cares if it takes hours); id go for archive for both"
      },
      {
        "date": "2023-02-23T19:02:00.000Z",
        "voteCount": 1,
        "content": "As per my calculations of storing the data in both the tiers and early deletion penalty, the archive storage (for both) is much cheaper than cool tier and archive tier. Assumption is that the data is not accessed from archive tier. \nThe reference link for calculations : https://azure.microsoft.com/en-us/pricing/details/storage/blobs/"
      },
      {
        "date": "2023-02-02T06:48:00.000Z",
        "voteCount": 2,
        "content": "Given Answers are correct"
      },
      {
        "date": "2022-12-26T06:54:00.000Z",
        "voteCount": 3,
        "content": "While I understand the reason for such answer, I still have doubts. If we think about costs of storage then answer will be different. E.g. (west europe region, all costs for data retrevial are removed) Cool storage for 1 month for 1TB costs 11.10 Eur for 1 month (that is 11,10x6=66,6 for 180 days) and archive storage costs 3,41 Eur per month (that is 3,41x6=20,46 Eur for 6 months). Therefore Archive tear is cheeper even you need to keep the data for 180 days instead of 30.\nI would go for Archive tier in both cases."
      },
      {
        "date": "2022-12-26T06:57:00.000Z",
        "voteCount": 2,
        "content": "Ok, my comment is wrong because you MUST remove data at the end of retention period."
      },
      {
        "date": "2023-02-03T19:48:00.000Z",
        "voteCount": 1,
        "content": "Your comment is still valid though. You can delete data in the Archive tier in 30 days in subject to an early deletion penalty. But even if you got an early deletion penalty, the total cost of the Archive tier is still cheaper than that of the Cool tier"
      },
      {
        "date": "2022-07-25T04:24:00.000Z",
        "voteCount": 2,
        "content": "given answer is correct"
      },
      {
        "date": "2022-06-22T00:52:00.000Z",
        "voteCount": 1,
        "content": "I think that infrastructure logs should be Cool tier due to the 60 days retention period. At the Archive tier they will be early deletion charge (they should remain at least 180 days in Archive), as mentioned in the answer's provided link."
      },
      {
        "date": "2022-05-11T09:39:00.000Z",
        "voteCount": 4,
        "content": "The question says \"You do not expect that the logs will be accessed during the retention periods\" - so there is no reason to keep any of them as Cool, so the correct answer should be to put them both in Archive"
      },
      {
        "date": "2022-05-23T04:55:00.000Z",
        "voteCount": 6,
        "content": "yeah but because the infrastructure logs are &lt;180 days before deleting, there is a considerable fee to delete if in archive, so not the cheapest option."
      },
      {
        "date": "2022-04-27T00:46:00.000Z",
        "voteCount": 1,
        "content": "But the question says 360 days and 60 days for the 2 logs...whereas archive tier could store only upto 180 days .Also the cool tier has lesser storage cost /- hour as compared to archive tier.So should'nt the answer be cool tier for both?"
      },
      {
        "date": "2022-12-29T18:11:00.000Z",
        "voteCount": 1,
        "content": "Data in the archive tier should be stored for a minimum of 180 days - not a maximum."
      },
      {
        "date": "2021-12-27T07:50:00.000Z",
        "voteCount": 2,
        "content": "Answers are correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67454-exam-dp-203-topic-1-question-32-discussion/",
    "body": "You plan to ingest streaming social media data by using Azure Stream Analytics. The data will be stored in files in Azure Data Lake Storage, and then consumed by using Azure Databricks and PolyBase in Azure Synapse Analytics.<br>You need to recommend a Stream Analytics data output format to ensure that the queries from Databricks and PolyBase against the files encounter the fewest possible errors. The solution must ensure that the files can be queried quickly and that the data type information is retained.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvro"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-05-31T12:02:00.000Z",
        "voteCount": 31,
        "content": "Avro schema definitions are JSON records. Polybase does not support JSON so why supporting Avro then. A CSV does not contain the schema as it is everything marked as string. so only parquet is left to choose."
      },
      {
        "date": "2021-12-19T16:42:00.000Z",
        "voteCount": 23,
        "content": "Parquet can be quickly retrieved and maintain metadata in itself. Hence Parquet is correct answer."
      },
      {
        "date": "2023-09-03T06:27:00.000Z",
        "voteCount": 1,
        "content": "should be correct"
      },
      {
        "date": "2023-08-04T09:12:00.000Z",
        "voteCount": 1,
        "content": "Parquet"
      },
      {
        "date": "2022-07-25T04:35:00.000Z",
        "voteCount": 3,
        "content": "Parquet is correct"
      },
      {
        "date": "2022-05-26T23:07:00.000Z",
        "voteCount": 2,
        "content": "Parquet  is correct"
      },
      {
        "date": "2022-04-27T01:42:00.000Z",
        "voteCount": 1,
        "content": "Isnt JSON good for batch processing/streaming?"
      },
      {
        "date": "2022-05-22T08:07:00.000Z",
        "voteCount": 6,
        "content": "Indeed. However, we also want to query the data using PolyBase. Polybase doesn't support Avro.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql/load-data-overview#polybase-external-file-formats"
      },
      {
        "date": "2022-03-28T08:42:00.000Z",
        "voteCount": 5,
        "content": "I am confused! \nAvro has self-describing schema and good for quick loading (patching), why parquet?"
      },
      {
        "date": "2022-05-10T04:09:00.000Z",
        "voteCount": 7,
        "content": "Apparently, the deciding factor is the fact that PolyBase doesn't support AVRO, but it does support Parquet."
      },
      {
        "date": "2023-11-09T02:43:00.000Z",
        "voteCount": 1,
        "content": "\"Polybase currently supports only delimeted text, rcfile, orc and parquet formats.\"\n\nR: https://msdn.microsoft.com/en-us/library/dn935025.aspx"
      },
      {
        "date": "2022-01-27T03:18:00.000Z",
        "voteCount": 1,
        "content": "correct."
      },
      {
        "date": "2022-01-22T01:09:00.000Z",
        "voteCount": 1,
        "content": "Parquet is the correct answer"
      },
      {
        "date": "2021-12-09T12:59:00.000Z",
        "voteCount": 1,
        "content": "Respuesta correcta PARQUET"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67455-exam-dp-203-topic-1-question-33-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a partitioned fact table named dbo.Sales and a staging table named stg.Sales that has the matching table and partition definitions.<br>You need to overwrite the content of the first partition in dbo.Sales with the content of the same partition in stg.Sales. The solution must minimize load times.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert the data from stg.Sales into dbo.Sales.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the first partition from dbo.Sales to stg.Sales.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch the first partition from stg.Sales to dbo.Sales.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate dbo.Sales from stg.Sales."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 128,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T06:39:00.000Z",
        "voteCount": 73,
        "content": "The correct answer is C"
      },
      {
        "date": "2021-12-10T15:27:00.000Z",
        "voteCount": 35,
        "content": "this must be C. since the need is to overwrite dbo.Sales with the content of stg.Sales. \nSWITCH source TO target"
      },
      {
        "date": "2024-09-11T21:54:00.000Z",
        "voteCount": 1,
        "content": "As switching partition does not involve movement of data, dbo.sales partition should be switched to the partition of stg.Sales so that partition from stg.Sales will be available in dbo.Sales"
      },
      {
        "date": "2024-08-30T07:10:00.000Z",
        "voteCount": 1,
        "content": "must be c"
      },
      {
        "date": "2024-08-28T01:49:00.000Z",
        "voteCount": 1,
        "content": "it's C"
      },
      {
        "date": "2024-08-19T07:45:00.000Z",
        "voteCount": 1,
        "content": "(C) because By switching the first partition from stg.Sales to dbo.Sales, you effectively replace the content of the first partition in dbo.Sales with the content of the corresponding partition in stg.Sales. This approach ensures that the partition in dbo.Sales is updated almost instantly, with minimal load time.\nIncorrect Options:\nA. Insert the data: Inserting data into dbo.Sales from stg.Sales would involve more overhead, as it requires data movement and processing, which can be time-consuming.\nB. Switch the first partition from dbo.Sales to stg.Sales: This would move the data out of dbo.Sales but would not achieve the goal of updating dbo.Sales with the data from stg.Sales.\nD. Update dbo.Sales: Updating the data would involve reading and writing the data, which would take significantly more time compared to partition switching."
      },
      {
        "date": "2024-07-06T04:37:00.000Z",
        "voteCount": 1,
        "content": "SWITCH [ PARTITION source_partition_number_expression ] TO [ schema_name. ] target_table [ PARTITION target_partition_number_expression ]     SOURCE TO DETENTION"
      },
      {
        "date": "2024-06-23T02:19:00.000Z",
        "voteCount": 1,
        "content": "actually B is correct. We have to understand the sentence as: Change the content of the first partition of dbo.Sales with the content of the first partition of stage.Sales. Thus, switch the content of first partition of dbo.Sales to/with the first partition of stage.Sales."
      },
      {
        "date": "2024-06-16T02:40:00.000Z",
        "voteCount": 1,
        "content": "Logical is C"
      },
      {
        "date": "2024-04-30T02:38:00.000Z",
        "voteCount": 2,
        "content": "B is correct\nALTER TABLE FactInternetSales SWITCH PARTITION 2 TO FactInternetSales_20000101 PARTITION 2;\n\nALTER TABLE FactInternetSales SPLIT RANGE (20010101);\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition?context=%2Fazure%2Fsynapse-analytics%2Fcontext%2Fcontext"
      },
      {
        "date": "2024-04-22T07:09:00.000Z",
        "voteCount": 1,
        "content": "Response is B"
      },
      {
        "date": "2024-04-14T22:52:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C"
      },
      {
        "date": "2024-04-09T10:09:00.000Z",
        "voteCount": 1,
        "content": "switch the first partition of sales to stage.sales's partition"
      },
      {
        "date": "2024-04-09T10:05:00.000Z",
        "voteCount": 1,
        "content": "it is the way on wording....tricky. might be not native speaker or they shall ask differt way..."
      },
      {
        "date": "2024-03-26T21:51:00.000Z",
        "voteCount": 1,
        "content": "Partition switching is a powerful technique that allows you to move data between tables quickly and with minimal logging. It\u2019s especially useful when dealing with large datasets.\nYou switch the staging table into the production table, and the production table becomes the staging table.\nhttps://dba.stackexchange.com/questions/231918/best-solution-to-switch-staging-table-to-main-table-avoid-access-when-main-tabl"
      },
      {
        "date": "2024-01-22T08:18:00.000Z",
        "voteCount": 3,
        "content": "Why I am seeing B and C are both same?"
      },
      {
        "date": "2023-12-22T04:26:00.000Z",
        "voteCount": 1,
        "content": "The most efficient approach to overwrite the content of the first partition in dbo.Sales with the content of the same partition in stg.Sales while minimizing load times is to switch the first partition from stg.Sales to dbo.Sales. This is option C."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67557-exam-dp-203-topic-1-question-34-discussion/",
    "body": "You are designing a slowly changing dimension (SCD) for supplier data in an Azure Synapse Analytics dedicated SQL pool.<br>You plan to keep a record of changes to the available fields.<br>The supplier data contains the following columns.<br><img src=\"/assets/media/exam-media/04259/0007200001.png\" class=\"in-exam-image\"><br>Which three additional columns should you add to the data to create a Type 2 SCD? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsurrogate primary key\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\teffective start date\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbusiness key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlast modified date",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\teffective end date\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tforeign key"
    ],
    "answer": "ABE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ABE",
        "count": 190,
        "isMostVoted": true
      },
      {
        "answer": "BCE",
        "count": 27,
        "isMostVoted": false
      },
      {
        "answer": "AB",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T18:50:00.000Z",
        "voteCount": 127,
        "content": "The answer is ABE. A type 2 SCD requires a surrogate key to uniquely identify each record when versioning. \n\nSee https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types under SCD Type 2 \u201c the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member.\u201d\n\nA business key is already part of this table - SupplierSystemID. The column is derived from the source data."
      },
      {
        "date": "2024-09-03T14:35:00.000Z",
        "voteCount": 1,
        "content": "Correct, the answer is ABE indeed ! \n\nType 2 SCD involves keeping a full history of changes to the data by creating new records for changes rather than overwriting old data. Each version of the record is identified by a surrogate key and is valid for a specific period, defined by the effective start and end dates."
      },
      {
        "date": "2022-12-22T08:59:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2022-01-08T22:09:00.000Z",
        "voteCount": 24,
        "content": "WHAT ARE YOU GUYS TALKING ABOUT??? You are really misleading other people!!! No issue with the provided answer. Should be BCE!!!\n\nCheck this out:\nhttps://docs.microsoft.com/en-us/sql/integration-services/data-flow/transformations/slowly-changing-dimension-transformation?view=sql-server-ver15\n\"The Slowly Changing Dimension transformation requires at least one business key column.\"\n[Surrogate key] is not mentioned in this Microsoft documentation AT ALL!!!"
      },
      {
        "date": "2022-01-10T21:48:00.000Z",
        "voteCount": 4,
        "content": "Search for Business Keys in that page. and make sure you wear specs :D"
      },
      {
        "date": "2022-01-10T11:42:00.000Z",
        "voteCount": 1,
        "content": "Yes, because  SupplierSystemID is unique. But Microsoft questions are terribly misleading here. People think that SupplierSystemID is business key because of Supplier in it. Also, there are some really not good and not sufficient examples on Learn. See https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
      },
      {
        "date": "2024-07-24T17:42:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types\nType 2 SCD: \"The table must also define a surrogate key because the business key (in this instance, employee ID) won't be unique.\""
      },
      {
        "date": "2022-02-23T02:21:00.000Z",
        "voteCount": 3,
        "content": "I don't understand. \n1) What in your opinion should then be the business key. Can you explain please.\n2) SupplierSysteID ist uniqe in the source system. Is there a definition that the column need to be unique also in the DataWarehouse? If no, there ist the possibility to use it as business key. Am I wrong?"
      },
      {
        "date": "2022-04-05T18:20:00.000Z",
        "voteCount": 3,
        "content": "No you're not wrong, the unique identifier form the ERP system is the Business Key"
      },
      {
        "date": "2022-08-03T09:30:00.000Z",
        "voteCount": 1,
        "content": "https://www.mssqltips.com/sqlservertip/5602/why-surrogate-keys-are-needed-for-a-sql-server-data-warehouse/"
      },
      {
        "date": "2023-02-02T06:55:00.000Z",
        "voteCount": 3,
        "content": "https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types\n\nUnder Type 2 SCD, it indicates that it must have a surrogate key"
      },
      {
        "date": "2024-09-06T11:34:00.000Z",
        "voteCount": 1,
        "content": "In my humble opinion the SupplierSystemID column is the business key: it remains an IDENTIFY column to represent the surrogate key"
      },
      {
        "date": "2024-07-25T12:34:00.000Z",
        "voteCount": 1,
        "content": "Copilot\nSent by Copilot:\nTo create a Type 2 Slowly Changing Dimension (SCD) in Azure Synapse Analytics, you should add the following three columns:\n\nA. Surrogate primary key\n\nThis uniquely identifies each version of a record, allowing you to track changes over time.\nB. Effective start date\n\nThis indicates when a particular version of the record became active.\nE. Effective end date\n\nThis indicates when a particular version of the record was superseded by a new version.\nThese columns help maintain historical data by creating new records for each change, rather than updating existing records. This way, you can track the history of changes to the supplier data."
      },
      {
        "date": "2024-07-06T04:49:00.000Z",
        "voteCount": 1,
        "content": "The surrogate key is a must, so the proposed answer is incorrect in my view. The start date and end date for each record have always been an over-design in my view. This means that you have to maintain both records. The latest record of the set with the same business key is the latest record, and you only need one date, the valid from date, in that row. I have long stopped versioning with valid from and valid to dates. You can easily use a simple lag function to grab a previous valid to date for the previous record, which is then the valid from date for the latest record. So why still use both dates?"
      },
      {
        "date": "2024-05-24T03:12:00.000Z",
        "voteCount": 1,
        "content": "The answer is ABE, you need subrogate key,"
      },
      {
        "date": "2024-05-05T23:54:00.000Z",
        "voteCount": 2,
        "content": "ABE\nSCD Type 2 doesn't work with business key it requires a surrogate key to uniquely identify each record when versioning.,  \nfor example - an employee can have a unique key(i.e. Business key) in the organization, but when you create SCD Type 2, you need to preserve historical data, hence then even a unique key of an Employee can't uniquely different the record but Surrogate key will do it."
      },
      {
        "date": "2024-04-30T02:43:00.000Z",
        "voteCount": 1,
        "content": "ABE; SupplierSystemId is a business key"
      },
      {
        "date": "2024-04-20T03:05:00.000Z",
        "voteCount": 1,
        "content": "I think ABE, see as below:\n\"A Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. In this case, the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member. It also includes columns that define the date range validity of the version (for example, StartDate and EndDate) and possibly a flag column (for example, IsCurrent) to easily filter by current dimension members.\"\nhttps://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
      },
      {
        "date": "2024-03-26T22:08:00.000Z",
        "voteCount": 2,
        "content": "Surrogate primary key (ensure to include edited records) - A, effective start date - B, effective end date - E"
      },
      {
        "date": "2024-03-25T02:52:00.000Z",
        "voteCount": 2,
        "content": "Should be ABE. According to a Microsoft learning SCD Type 2 requires a surrogate key.\n\n\"A Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. In this case, the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member.\"\n\nhttps://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types"
      },
      {
        "date": "2024-02-12T03:02:00.000Z",
        "voteCount": 1,
        "content": "The surrogate key is an artificially generated key, usually an incremental or globally unique identifier.\nBusiness key derived from the actual business data and has business meaning. Examples could include a product code, customer ID, employee ID, or any other identifier that has relevance in the business context.\n\n\nI think this is it's business key \nEffective end and start date"
      },
      {
        "date": "2024-02-04T19:44:00.000Z",
        "voteCount": 2,
        "content": "Selected Answer: ABE\n\nThe answer is ABE. A type 2 SCD requires a surrogate key to uniquely identify each record when versioning.\n\nSee https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types under SCD Type 2 \u201c the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member.\u201d\n\nA business key is already part of this table - SupplierSystemID. The column is derived from the source data."
      },
      {
        "date": "2023-12-05T09:57:00.000Z",
        "voteCount": 3,
        "content": "I'm working now more then 13years with this stuff,    ABE is correct.  CHOPIN is wrong.\na 2 SCD needs a unique ID, this is the surrogate key, \nbesides, in the table given, there is already a business key , is the first column\nA business key is NOT unique in an 2 SCD\nhallo"
      },
      {
        "date": "2023-09-07T00:34:00.000Z",
        "voteCount": 1,
        "content": "Surrogate Key\nEffective Start Date\nEffective End Date"
      },
      {
        "date": "2023-09-03T06:30:00.000Z",
        "voteCount": 1,
        "content": "BCE  is answer"
      },
      {
        "date": "2023-08-29T16:47:00.000Z",
        "voteCount": 2,
        "content": "How come it can be anything other than ABE.\nABE is the correct answer."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67637-exam-dp-203-topic-1-question-35-discussion/",
    "body": "HOTSPOT -<br>You have a Microsoft SQL Server database that uses a third normal form schema.<br>You plan to migrate the data in the database to a star schema in an Azure Synapse Analytics dedicated SQL pool.<br>You need to design the dimension tables. The solution must optimize read operations.<br>What should you include in the solution? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0007400001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0007400002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Denormalize to a second normal form<br>Denormalization is the process of transforming higher normal forms to lower normal forms via storing the join of higher normal form relations as a base relation.<br>Denormalization increases the performance in data retrieval at cost of bringing update anomalies to a database.<br><br>Box 2: New identity columns -<br>The collapsing relations strategy can be used in this step to collapse classification entities into component entities to obtain flat dimension tables with single-part keys that connect directly to the fact table. The single-part key is a surrogate key generated to ensure it remains unique over time.<br>Example:<br><img src=\"/assets/media/exam-media/04259/0007500001.jpg\" class=\"in-exam-image\"><br>Note: A surrogate key on a table is a column with a unique identifier for each row. The key is not generated from the table data. Data modelers like to create surrogate keys on their tables when they design data warehouse models. You can use the IDENTITY property to achieve this goal simply and effectively without affecting load performance.<br>Reference:<br>https://www.mssqltips.com/sqlservertip/5614/explore-the-role-of-normal-forms-in-dimensional-modeling/ https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-27T03:30:00.000Z",
        "voteCount": 25,
        "content": "Answer correct."
      },
      {
        "date": "2023-05-17T14:56:00.000Z",
        "voteCount": 7,
        "content": "Always denormalize when moving from database to date warehouse."
      },
      {
        "date": "2023-05-23T04:22:00.000Z",
        "voteCount": 2,
        "content": "God no! Only dimensional models, read Inmon and Lindstedt approach."
      },
      {
        "date": "2024-08-19T07:57:00.000Z",
        "voteCount": 1,
        "content": "Why Denormalizing to 2nd Normal Form (2NF) is CORRECT?: In a star schema, dimension tables are typically denormalized to reduce the number of joins required during query execution. This denormalization improves query performance, particularly in read-heavy workloads typical in analytics environments like Azure Synapse. Denormalizing to 2NF involves reducing the number of tables and increasing the redundancy within the dimension tables, which is beneficial for faster reads.\n\nMaintaining 3NF: While 3rd Normal Form (3NF) minimizes redundancy and ensures data integrity, it also requires more complex joins during queries, which can slow down read operations in a star schema designed for analytics."
      },
      {
        "date": "2023-09-03T06:33:00.000Z",
        "voteCount": 2,
        "content": "denormalizing and IDENTITY"
      },
      {
        "date": "2023-02-02T06:57:00.000Z",
        "voteCount": 3,
        "content": "Answer Is correct"
      },
      {
        "date": "2022-07-25T06:31:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      },
      {
        "date": "2022-05-16T08:31:00.000Z",
        "voteCount": 5,
        "content": "'The solution must optimize read operations.' means denormalization"
      },
      {
        "date": "2021-12-27T08:04:00.000Z",
        "voteCount": 1,
        "content": "Answers are correct"
      },
      {
        "date": "2021-12-20T02:16:00.000Z",
        "voteCount": 4,
        "content": "answer is correct"
      },
      {
        "date": "2021-12-11T08:43:00.000Z",
        "voteCount": 1,
        "content": "While denormalizing does require implementing a lower level of normalization, the second normal form ONLY applies when a table has a composite primary key.  https://www.geeksforgeeks.org/second-normal-form-2nf/"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67646-exam-dp-203-topic-1-question-36-discussion/",
    "body": "HOTSPOT -<br>You plan to develop a dataset named Purchases by using Azure Databricks. Purchases will contain the following columns:<br>\u2711 ProductID<br>\u2711 ItemPrice<br>\u2711 LineTotal<br>\u2711 Quantity<br>\u2711 StoreID<br>\u2711 Minute<br>\u2711 Month<br>\u2711 Hour<br><br>Year -<br><img src=\"/assets/media/exam-media/04259/0007500010.png\" class=\"in-exam-image\"><br>\u2711 Day<br>You need to store the data to support hourly incremental load pipelines that will vary for each Store ID. The solution must minimize storage costs.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0007600002.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0007700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: partitionBy -<br>We should overwrite at the partition level.<br>Example:<br>df.write.partitionBy(\"y\",\"m\",\"d\")<br>.mode(SaveMode.Append)<br>.parquet(\"/data/hive/warehouse/db_name.db/\" + tableName)<br>Box 2: (\"StoreID\", \"Year\", \"Month\", \"Day\", \"Hour\", \"StoreID\")<br>Box 3: parquet(\"/Purchases\")<br>Reference:<br>https://intellipaat.com/community/11744/how-to-partition-and-write-dataframe-in-spark-without-deleting-partitions-with-no-new-data",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-27T08:07:00.000Z",
        "voteCount": 23,
        "content": "Answers are correct"
      },
      {
        "date": "2021-12-11T09:54:00.000Z",
        "voteCount": 8,
        "content": "correct"
      },
      {
        "date": "2024-03-26T23:01:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct"
      },
      {
        "date": "2024-01-18T09:24:00.000Z",
        "voteCount": 5,
        "content": "Got this question on my exam on january 17, the answer is correct"
      },
      {
        "date": "2023-09-15T03:02:00.000Z",
        "voteCount": 1,
        "content": "Answers are correct"
      },
      {
        "date": "2023-09-03T06:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-11-26T18:51:00.000Z",
        "voteCount": 3,
        "content": "Why parquet option?  Can anyone explain."
      },
      {
        "date": "2023-10-26T13:22:00.000Z",
        "voteCount": 26,
        "content": "Because Parquet is always the answer."
      },
      {
        "date": "2023-05-01T15:08:00.000Z",
        "voteCount": 5,
        "content": "The solution must minimize storage costs."
      },
      {
        "date": "2023-07-02T01:23:00.000Z",
        "voteCount": 5,
        "content": "I guess because of the requirement \"reducing storage costs\""
      },
      {
        "date": "2022-11-25T10:03:00.000Z",
        "voteCount": 6,
        "content": "Can somebody explain why are we partitioning by StoreId, Year, Month, Day and Hour instead of just StoreID and Hour?"
      },
      {
        "date": "2022-11-28T07:04:00.000Z",
        "voteCount": 34,
        "content": "if partitioned by storeid and hour only, the same hours from different days would go to the same partition, that would be innefficient"
      },
      {
        "date": "2022-09-03T05:50:00.000Z",
        "voteCount": 3,
        "content": "Can someone explain why parquet and not saveAsTable option?"
      },
      {
        "date": "2022-12-09T11:43:00.000Z",
        "voteCount": 7,
        "content": "Parquet is columnar, so faster to be read by Azure Synapse Analytics via CETAs."
      },
      {
        "date": "2022-07-25T07:09:00.000Z",
        "voteCount": 3,
        "content": "given answers are correct"
      },
      {
        "date": "2022-06-12T07:40:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-03-30T00:18:00.000Z",
        "voteCount": 4,
        "content": "ans should be saveAsTable. format is defined by format() method."
      },
      {
        "date": "2022-01-10T13:19:00.000Z",
        "voteCount": 6,
        "content": "Can anyone explain why it's Partitioning and not Bucketing pls?"
      },
      {
        "date": "2022-01-10T13:27:00.000Z",
        "voteCount": 2,
        "content": "Is it a question of correct syntax (numBuckets int the number of buckets to save) or is it smth else?"
      },
      {
        "date": "2022-02-13T01:52:00.000Z",
        "voteCount": 10,
        "content": "There should be a different folder for each store. Partitioning will create separate folder for each storeId. In bucketing, multiple stores having same hash value can be present in the same file, so multiple storeIds can be present under a single file."
      },
      {
        "date": "2022-04-25T23:14:00.000Z",
        "voteCount": 6,
        "content": "Bucketing feature (part of data skipping index) was removed and microsoft recommends using DeltaLake, which uses the partition syntax. \nhttps://docs.microsoft.com/en-us/azure/databricks/spark/latest/spark-sql/dataskipping-index"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67647-exam-dp-203-topic-1-question-37-discussion/",
    "body": "You are designing a partition strategy for a fact table in an Azure Synapse Analytics dedicated SQL pool. The table has the following specifications:<br>\u2711 Contain sales data for 20,000 products.<br>Use hash distribution on a column named ProductID.<br><img src=\"/assets/media/exam-media/04259/0007700003.png\" class=\"in-exam-image\"><br>\u2711 Contain 2.4 billion records for the years 2019 and 2020.<br>Which number of partition ranges provides optimal compression and performance for the clustered columnstore index?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t40\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t240",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t400",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t2,400"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 37,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-11T10:00:00.000Z",
        "voteCount": 25,
        "content": "correct"
      },
      {
        "date": "2022-09-03T12:13:00.000Z",
        "voteCount": 13,
        "content": "2,4bn/60=40M"
      },
      {
        "date": "2024-07-07T11:17:00.000Z",
        "voteCount": 1,
        "content": "2.4B = 2400M\n2400/60 = 40"
      },
      {
        "date": "2024-07-06T04:09:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-09-07T00:40:00.000Z",
        "voteCount": 2,
        "content": "2,400,000,000 / 60,000,000 = 40"
      },
      {
        "date": "2023-09-03T06:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-08-07T00:26:00.000Z",
        "voteCount": 1,
        "content": "OPTION A"
      },
      {
        "date": "2023-02-10T08:34:00.000Z",
        "voteCount": 12,
        "content": "No of automatic Distributions is 60. So each distribution will have 2.4 B / 60 = 40M. For a good performance each partition within a distribution ( some time called buckets of data ) should have 1M rows per bucket. So 40M / 1M = 40 partitions."
      },
      {
        "date": "2023-04-25T06:14:00.000Z",
        "voteCount": 4,
        "content": "Another way to think about this:\n\nThe number of records for the period stated = 2.4 billion\nNumber of underlying (\"automatic\") distributions: 60\n\n2.4 billion / 60 distributions = 40 million rows\n40 million / 40 partitions = 1 million rows\n\nAs stated, 1 million rows per distribution are optimal for compression and performance. Divide the 40 million rows with the other partitioning options and you have too few rows per distribution -&gt; suboptimal."
      },
      {
        "date": "2023-02-08T09:09:00.000Z",
        "voteCount": 3,
        "content": "Considering that:\nHaving too many partitions can reduce the effectiveness of clustered columnstore indexes\nif each partition has fewer than 1 million rows. \nDedicated SQL pools automatically partition your data into 60 databases\nSo a table with no partiton (or just one partition) has 60Milion of records\n\nI have use this logic, simple proportion:\n\n1 partion : 60M = x = 2.4 B  ==&gt; 1 : 60 M = x : 2400 M ==&gt; x = 2400 / 60 ==&gt; x = 40 partitions"
      },
      {
        "date": "2023-02-08T09:11:00.000Z",
        "voteCount": 3,
        "content": "1 partion : 60M =  x : 2.4 B\n1 partion : 60 M = x : 2400 M\n ==&gt; x = 2400 / 60 \n ==&gt; x = 40 partitions"
      },
      {
        "date": "2023-02-02T07:32:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer"
      },
      {
        "date": "2022-10-04T03:39:00.000Z",
        "voteCount": 4,
        "content": "Very simple go with the smallest partition because too many partitions affect peformance"
      },
      {
        "date": "2022-08-14T10:37:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-06-12T07:42:00.000Z",
        "voteCount": 1,
        "content": "Optimal distribution is up to 60 instances"
      },
      {
        "date": "2022-05-24T17:09:00.000Z",
        "voteCount": 3,
        "content": "quick maths"
      },
      {
        "date": "2022-04-28T23:32:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-04-11T13:55:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-03-14T02:00:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/51384-exam-dp-203-topic-1-question-38-discussion/",
    "body": "HOTSPOT -<br>You are creating dimensions for a data warehouse in an Azure Synapse Analytics dedicated SQL pool.<br>You create a table by using the Transact-SQL statement shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0007900001.png\" class=\"in-exam-image\"><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the graphic.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0008000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0008000002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Type 2 -<br>A Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. In this case, the dimension table must use a surrogate key to provide a unique reference to a version of the dimension member. It also includes columns that define the date range validity of the version (for example, StartDate and EndDate) and possibly a flag column (for example,<br>IsCurrent) to easily filter by current dimension members.<br>Incorrect Answers:<br>A Type 1 SCD always reflects the latest values, and when changes in source data are detected, the dimension table data is overwritten.<br><br>Box 2: a business key -<br>A business key or natural key is an index which identifies uniqueness of a row based on columns that exist naturally in a table according to business rules. For example business keys are customer code in a customer table, composite of sales order header number and sales order item line number within a sales order details table.<br>Reference:<br>https://docs.microsoft.com/en-us/learn/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/3-choose-between-dimension-types",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-05T02:05:00.000Z",
        "voteCount": 221,
        "content": "product key is a surrogate key as it is an identity column"
      },
      {
        "date": "2024-09-03T14:39:00.000Z",
        "voteCount": 3,
        "content": "TOTALLY AGREE HERE  !!!! \nThe ProductKey column is defined as an IDENTITY(1,1) column, which means it is an auto-incremented value and serves as a unique identifier for each row. This is characteristic of a surrogate key, which is used in dimensional modeling to uniquely identify records in a dimension table, especially in a Type 2 SCD."
      },
      {
        "date": "2021-05-16T11:35:00.000Z",
        "voteCount": 17,
        "content": "Agree on the surrogate key, exactly.\n\n\"In data warehousing, IDENTITY functionality is particularly important as it makes easier the creation of surrogate keys.\"\n\nWhy ProductKey is certainly not a business key: \"The IDENTITY value in Synapse is not guaranteed to be unique if the user explicitly inserts a duplicate value with 'SET IDENTITY_INSERT ON' or reseeds IDENTITY\". Business key is an index which identifies uniqueness of a row and here Microsoft says that identity doesn't guarantee uniqueness.\n\nReferences:\nhttps://azure.microsoft.com/en-us/blog/identity-now-available-with-azure-sql-data-warehouse/\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-identity"
      },
      {
        "date": "2021-09-26T00:05:00.000Z",
        "voteCount": 13,
        "content": "Type 2\n\nIn order to support type 2 changes, we need to add four columns to our table:\n\n\u00b7 Surrogate Key \u2013 the original ID will no longer be sufficient to identify the specific record we require, we therefore need to create a new ID that the fact records can join to specifically.\n\n\u00b7 Current Flag \u2013 A quick method of returning only the current version of each record\n\n\u00b7 Start Date \u2013 The date from which the specific historical version is active\n\n\u00b7 End Date \u2013 The date to which the specific historical version record is active\n\nWith these elements in place, our table will now look like:"
      },
      {
        "date": "2024-01-24T22:20:00.000Z",
        "voteCount": 5,
        "content": "You know the start and end date of the selling, but not the version of the product (size, color....) If the waterpump  size has changed for a car by rhe producer but the function product id are the same, selling is continuous you can not store it into this structure. But you can store at the timepoints and see validation point in creation date of the record. Therefore it is type 1"
      },
      {
        "date": "2021-05-11T17:04:00.000Z",
        "voteCount": 51,
        "content": "Type2 because there are start and end columns and ProductKey is a surrogate key. ProductNumber seems a business key."
      },
      {
        "date": "2021-05-28T04:13:00.000Z",
        "voteCount": 118,
        "content": "The start and end columns are for when to when the product was being sold, not for metadata purposes. That makes it:      \nType 1 \u2013 No History\nUpdate record directly, there is no record of historical values, only current state"
      },
      {
        "date": "2021-06-03T02:24:00.000Z",
        "voteCount": 2,
        "content": "Exactly how I saw it"
      },
      {
        "date": "2021-09-15T21:40:00.000Z",
        "voteCount": 2,
        "content": "When the product is not being sold anymore, it becomes a historical record. Hence Type 2."
      },
      {
        "date": "2021-12-19T19:00:00.000Z",
        "voteCount": 1,
        "content": "With type 2, you normally don't update any column of a row other than row start date and end date."
      },
      {
        "date": "2022-02-20T03:27:00.000Z",
        "voteCount": 4,
        "content": "It is type 1 not 2"
      },
      {
        "date": "2022-11-16T14:41:00.000Z",
        "voteCount": 2,
        "content": "I agree with the first part. From just the table it's impossible to know if the changes in the products are ignored or are updated, if you don't see the ETL. I suppose there is some mistake in the name of the fields start end effective fields."
      },
      {
        "date": "2024-08-19T08:12:00.000Z",
        "voteCount": 2,
        "content": "SURROGATE KEY AND SCD 1:\nThe absence of a Current_Flag or similar indicator and the RowUpdatedDateTime column might suggest that the DimProduct table is not strictly adhering to a Type 2 Slowly Changing Dimension (SCD). Instead, it seems to be implementing a Type 1 Slowly Changing Dimension (SCD). Further evidence for SCD 1:\n\nRowUpdatedDateTime: The RowUpdatedDateTime column indicates when the row was last modified, which is consistent with Type 1 SCDs where data is overwritten.\n\nNo Historical Tracking: There is no mechanism to track historical changes (e.g., Current_Flag, EffectiveDate, EndDate). Without these, the table does not store previous versions of the data, reinforcing that this is a Type 1 SCD."
      },
      {
        "date": "2024-07-20T12:20:00.000Z",
        "voteCount": 1,
        "content": "The ProductKey is a surrogate key, as you can se the query use IDENTITY(1,1) to identify unique values in the table. If was a Business Key,  the query don't need to specific the INDENTITY, usually we use Concat to create new business Key or naturally the database return that."
      },
      {
        "date": "2024-07-06T05:19:00.000Z",
        "voteCount": 1,
        "content": "Operational transaction systems don't use keys as their ID columns. They generally use product IDs, which refer to a business key. When we hit the dimension design for star schemas, we generally name them product keys. \n\nThe second column in the table, which is product source ID, I've read as being an ID combined with the source where the record has come from, which makes it the business key. This is generally what we do for multi-sourced dimensions, i.e., different systems that can deliver products into a single dimension table. \n\nI wouldn't create a product source ID column; I would create a source column and a product ID column, which then become the combined business key. These questions are overly simplified and not providing enough clear guidance, which is simply confusing and leads to incorrect answers and inaccurate results."
      },
      {
        "date": "2024-05-18T12:33:00.000Z",
        "voteCount": 2,
        "content": "A Product can stop selling gets a sellendDate and the table does not receive a new version of the record. On the other hand a SCD1 can have a SK to simplify joins"
      },
      {
        "date": "2024-04-30T02:59:00.000Z",
        "voteCount": 2,
        "content": "This is a far too easy question to get it wrong by the site admins"
      },
      {
        "date": "2024-04-20T15:40:00.000Z",
        "voteCount": 1,
        "content": "I think: Type 1 and Surrogate key"
      },
      {
        "date": "2024-08-28T03:59:00.000Z",
        "voteCount": 1,
        "content": "why need surrogate key for type 1 ?"
      },
      {
        "date": "2024-04-08T09:18:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer : \nType 2 SCD (start date , end date , surrogate key)\nSurrogate key as its an IDENTITY column."
      },
      {
        "date": "2024-03-22T02:44:00.000Z",
        "voteCount": 1,
        "content": "Type 1 because there is a time of row update, and not a column validity flag.\nSurrogate key, identity column is always a surrogate ."
      },
      {
        "date": "2024-02-24T06:37:00.000Z",
        "voteCount": 1,
        "content": "The SellStartDate and SellEndDate wont be updated. They seem to be fixed columns. As you have a RowInsert and a RowUpdate it would be deemed to be a Type 1 because the row would be updated in place rather than a new row created (Type 2). Plus as the ID is an identity column it is a Surrogate Key - it has been created by the system not by the business. Business Keys are Keys that are provided by the business with business logic, such as an order number or a customer number (e.g. CUST-00001)."
      },
      {
        "date": "2024-01-23T00:29:00.000Z",
        "voteCount": 3,
        "content": "Type 1 plus surrogate Key"
      },
      {
        "date": "2023-12-05T10:21:00.000Z",
        "voteCount": 15,
        "content": "13 years in this stuff\nproduct key is a surrogate , business key comes from the ERP business app\nand it is a type 1  NOT 2\nan insert and update column does not tell you the date from and to , hallo"
      },
      {
        "date": "2023-09-07T00:42:00.000Z",
        "voteCount": 2,
        "content": "Type 2 &amp; Surrogate Key"
      },
      {
        "date": "2023-09-05T22:33:00.000Z",
        "voteCount": 3,
        "content": "Ans 1 - TYPE 2\nAns 2 - Product key is a surrogate key (identity column)\n\nNote: Product number would be the business key if I had to pick one"
      },
      {
        "date": "2023-09-03T06:43:00.000Z",
        "voteCount": 2,
        "content": "t2 &amp; is a surrogate key."
      },
      {
        "date": "2023-08-29T16:53:00.000Z",
        "voteCount": 4,
        "content": "SCD: Type1\nProductKey is Surrogate key."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52251-exam-dp-203-topic-1-question-39-discussion/",
    "body": "You are designing a fact table named FactPurchase in an Azure Synapse Analytics dedicated SQL pool. The table contains purchases from suppliers for a retail store. FactPurchase will contain the following columns.<br><img src=\"/assets/media/exam-media/04259/0008200001.png\" class=\"in-exam-image\"><br>FactPurchase will have 1 million rows of data added daily and will contain three years of data.<br>Transact-SQL queries similar to the following query will be executed daily.<br><br>SELECT -<br>SupplierKey, StockItemKey, COUNT(*)<br><br>FROM FactPurchase -<br><br>WHERE DateKey &gt;= 20210101 -<br><br>AND DateKey &lt;= 20210131 -<br>GROUP By SupplierKey, StockItemKey<br>Which table distribution will minimize query times?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treplicated",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash-distributed on PurchaseKey\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround-robin",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash-distributed on DateKey"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-08-07T23:35:00.000Z",
        "voteCount": 64,
        "content": "From the documentation the answer is clear enough. B  is the right answer. \nWhen choosing a distribution column, select a distribution column that: \"Is not a date column. All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work.\""
      },
      {
        "date": "2021-10-16T23:02:00.000Z",
        "voteCount": 8,
        "content": "To minimize data movement, select a distribution column that:\n\nIs used in JOIN, GROUP BY, DISTINCT, OVER, and HAVING clauses.\n\n\"PurchaseKey\" is not used in the group by"
      },
      {
        "date": "2022-11-02T22:59:00.000Z",
        "voteCount": 2,
        "content": "A distribution column should have high cardinality to ensure even distribution over nodes."
      },
      {
        "date": "2021-10-16T23:00:00.000Z",
        "voteCount": 7,
        "content": "Consider using the round-robin distribution for your table in the following scenarios:\n\nWhen getting started as a simple starting point since it is the default\nIf there is no obvious joining key\nIf there is no good candidate column for hash distributing the table\nIf the table does not share a common join key with other tables\nIf the join is less significant than other joins in the query"
      },
      {
        "date": "2021-08-07T07:12:00.000Z",
        "voteCount": 19,
        "content": "I think the answer should be D for that specific query. If you look at the datatypes, DateKey is an INT datatype not a DATE datatype."
      },
      {
        "date": "2021-08-07T09:27:00.000Z",
        "voteCount": 5,
        "content": "and thet statement that Fact table will be added 1 million rows daily means that each datekey value has an equal amount of rows associated with that value."
      },
      {
        "date": "2022-01-05T19:09:00.000Z",
        "voteCount": 2,
        "content": "But the DateKey is used in the WHERE clause."
      },
      {
        "date": "2022-03-14T02:43:00.000Z",
        "voteCount": 1,
        "content": "I agree, date key is int, and besides, even if it was a date, when you query a couple days then 1 million rows per distribution is not that much. So what if you are going to use only a couple distributions to do the job? Isn't it still faster than using all distributions to process all of the records to get the required date range?"
      },
      {
        "date": "2021-09-04T04:22:00.000Z",
        "voteCount": 7,
        "content": "https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute  this link says date filed , NOT a date Data type. B is correct"
      },
      {
        "date": "2022-03-14T02:46:00.000Z",
        "voteCount": 2,
        "content": "n.b. if we look at the example query itself the date range is 31 days so we will use 31 distributions out of 60, and only process ~31 million records"
      },
      {
        "date": "2024-08-24T01:20:00.000Z",
        "voteCount": 1,
        "content": "To optimize parallel processing in Azure Synapse Analytics, it's important to select a distribution column or set of columns that:\n\nHas many unique values.\nDoes not have many nulls.\nIs not a date column.\nPartitioning by DateKey does not meet these criteria. Date columns typically have limited unique values, can contain many nulls, and, by nature, are date fields. Given these constraints, DateKey is not suitable for distribution.\n\nFor fact tables, which usually exceed 2 GB in size, a hash distribution is recommended. In this scenario, using PurchaseKey as the distribution key is ideal as it aligns with all the specified criteria: it has many unique values, is unlikely to have nulls, and is not a date column. This approach ensures more efficient data distribution and query performance.\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2024-07-21T15:35:00.000Z",
        "voteCount": 1,
        "content": "The real approach in actual project is limiting the dateKey:\n DistributionKey AS HASHBYTES('MD5', CONCAT(YEAR(CAST(DateKey AS DATE)), FORMAT(CAST(DateKey AS DATE), 'MM')))\n)\nWITH\n(\n    DISTRIBUTION = HASH(DistributionKey),\n    CLUSTERED COLUMNSTORE INDEX\n);"
      },
      {
        "date": "2024-07-13T21:39:00.000Z",
        "voteCount": 1,
        "content": "total votes on B:85 on D; 23"
      },
      {
        "date": "2024-07-13T21:37:00.000Z",
        "voteCount": 1,
        "content": "Answer could be only D"
      },
      {
        "date": "2024-07-06T04:25:00.000Z",
        "voteCount": 1,
        "content": "You don't want to hash on the Date column generally, definitely not when its being included in the Where clause, PurchaseKey is the only acceptable option given as the table is too large for roundrobin. I want to know if those other columns in the group by could possibly be used also?"
      },
      {
        "date": "2024-04-20T00:04:00.000Z",
        "voteCount": 2,
        "content": "Question 20 and 39 Is same"
      },
      {
        "date": "2024-07-25T06:09:00.000Z",
        "voteCount": 1,
        "content": "is not the same: \"D.&nbsp;hash-distributed on IsOrderFinalized\""
      },
      {
        "date": "2024-03-05T00:40:00.000Z",
        "voteCount": 2,
        "content": "B is my correct answer"
      },
      {
        "date": "2024-02-17T01:35:00.000Z",
        "voteCount": 1,
        "content": "Explain me something - if you use Purchasekey as a hash distribution, and then want to do a partition, which column will you use for partition, we mostly date column for partition, but if we use date column then during query execution where you want to query data for let's say Jan month, wouldn't the query will need data from multiple nodes? eventually slowing the results? isn't it easy to keep data on a single node get faster results. Am I missing anything here?"
      },
      {
        "date": "2024-01-30T06:46:00.000Z",
        "voteCount": 1,
        "content": "For me.\nI chose D.\nThe reason is that the\" datekey\"  is of tyoe \"integer\" not \" Date\" .This  qualifies it to be used as a  non-auto-incremental  surrogate key  for the fact table."
      },
      {
        "date": "2024-02-09T13:55:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column-with-data-that-distributes-evenly"
      },
      {
        "date": "2023-12-23T02:23:00.000Z",
        "voteCount": 3,
        "content": "Something not immediately clear to me was that distributing and partitioning are different, hence I was confused that one should not distribute over date columns. \n\nBottom line is, do not distribute over date columns but you can partition over them. In this question they specifically ask about distribution method. Query optimization for large tables directly points to hashing."
      },
      {
        "date": "2023-11-28T12:55:00.000Z",
        "voteCount": 4,
        "content": "You want to distribute by productKey and partition by date.  Then all distributions will be looked at in parallel and then, within each distribution, only the desired partitions will be looked at.   Thereby, the query is fully scaled out and the quickest it can be."
      },
      {
        "date": "2023-10-21T02:59:00.000Z",
        "voteCount": 4,
        "content": "B) the chosen distribution column should not be used in WHERE clauses; thus, we can discard DateKey (even though it is not a Date data type) to minimize data movement. The chosen distribution column must have many unique values; thus we potentially have 2 candidates: PurchaseKey or PurchaseOrderID; however, the chosen one should have no NULLS or only a few, making PurchaseKey the ideal in order to distribute evenly.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2023-10-03T12:01:00.000Z",
        "voteCount": 2,
        "content": "Is there any \"official\" answer to this?\n    A. Replicated: Replicated tables have copies of the entire table on each distribution. While this option can eliminate data movement, it may not be the most efficient choice for very large tables with frequent updates.\n    B. Hash-Distributed on PurchaseKey: Hash distribution on \"PurchaseKey\" may lead to data skew if \"PurchaseKey\" doesn't have a wide range of unique values. Additionally, it doesn't align with the primary filtering condition on \"DateKey.\"\n    C. Round-Robin: Round-robin distribution ensures even data distribution, but it doesn't take advantage of data locality for specific types of queries.\n    D. Hash-Distributed on DateKey: Distributing on \"DateKey\" aligns with your primary filtering condition, but it's a date column. This could lead to clustering by date, especially if many users filter on the same date.\n\nNone of the answers seem to fit. D could be the best guess but it's a date column."
      },
      {
        "date": "2023-09-03T06:46:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-06-29T04:08:00.000Z",
        "voteCount": 1,
        "content": "B is total nonsense if PurchaseKey has a unique value for every row it would end up distributing it evenly so same as round-robin. Distributing by date would slow down the query because in a situation presented in the question only 31 out of 60 distributions would be used. So in my opinion C is the correct answer."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67468-exam-dp-203-topic-1-question-40-discussion/",
    "body": "You are implementing a batch dataset in the Parquet format.<br>Data files will be produced be using Azure Data Factory and stored in Azure Data Lake Storage Gen2. The files will be consumed by an Azure Synapse Analytics serverless SQL pool.<br>You need to minimize storage costs for the solution.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Snappy compression for the files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse OPENROWSET to query the Parquet files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external table that contains a subset of columns from the Parquet files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore all data as string in the Parquet files."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 57,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-14T11:01:00.000Z",
        "voteCount": 72,
        "content": "Answer should be A, because this talks about minimizing storage costs, not querying costs"
      },
      {
        "date": "2024-03-15T03:19:00.000Z",
        "voteCount": 2,
        "content": "I found this comparison of compression methods, which explained that A should not be the answer.\nhttps://www.linkedin.com/pulse/comparison-compression-methods-parquet-file-format-saurav-mohapatra/\n\"BROTLI : This is a relatively new codec which offers very high compression ratio , but with lower compression and decompression speeds. This codec is useful when storage space is a major constraint. This technique also offers parallel processing that other methods don't.\""
      },
      {
        "date": "2022-01-11T14:50:00.000Z",
        "voteCount": 24,
        "content": "Isn't snappy a default compressionCodec for parquet in azure?\nhttps://docs.microsoft.com/en-us/azure/data-factory/format-parquet"
      },
      {
        "date": "2023-12-23T02:51:00.000Z",
        "voteCount": 2,
        "content": "Very confused at first, after thinking about it and rereading this is what I found:\nIt says we are implementing the batch process in parquet format, so we should think about a situation where we write the file and specify snappy compression as an argument explicitly. \n\nThe phrasing is very confusing I have to say, but if you argue from a 'query externally' perspective, then B and C would yield the same benefit. Therefore, A makes the most sense and connects best with the question."
      },
      {
        "date": "2021-12-11T10:12:00.000Z",
        "voteCount": 23,
        "content": "C is the correct answer, as an external table with a subset of columns with parquet files would be cost-effective."
      },
      {
        "date": "2022-04-29T07:51:00.000Z",
        "voteCount": 2,
        "content": "in serverless sql pool you don't create a copy of the data, so how could be cost effective?"
      },
      {
        "date": "2022-11-27T06:41:00.000Z",
        "voteCount": 1,
        "content": "Don't forget that there is Transaction cost part of storage cost, so taking a subset of columns will lower transaction cost consequently storage cost."
      },
      {
        "date": "2022-05-22T08:29:00.000Z",
        "voteCount": 5,
        "content": "This is not correct. \n1. External tables are are not saved in the database. (This is why they're external)\n2. You're assuming that the SQL Serverless pools have a local storage. They don't -- &gt; https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-serverless-sql-pool"
      },
      {
        "date": "2022-06-17T21:58:00.000Z",
        "voteCount": 2,
        "content": "well there is a possibility to create an external table and load only the required columns using openrowset in serverless sql pool to a different container in ADLS. Remember serverless sql pool does support cetas with openrowset but dedicated pool doesn't support loading data using openrowset. So basically the solution could be load the required columns using cetas using openrowset to a differnet container and delete the source data from previous container after loading the filtered data to a different container in ADLS"
      },
      {
        "date": "2022-06-17T22:08:00.000Z",
        "voteCount": 4,
        "content": "check this https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas. Answer C is correct"
      },
      {
        "date": "2024-09-02T19:17:00.000Z",
        "voteCount": 1,
        "content": "Using Snappy compression (Option A) is specifically designed to reduce the size of Parquet files, thereby directly minimizing storage costs."
      },
      {
        "date": "2024-07-06T04:37:00.000Z",
        "voteCount": 1,
        "content": "The question is confusing but I believe it is C, because you can use CETAS to store this external table in Gen2 (this is the storage solution), from there you will query it using serverless SQL pool."
      },
      {
        "date": "2024-04-30T03:13:00.000Z",
        "voteCount": 2,
        "content": "A, B and C they are all acceptable, D is just stupid\nBut pay attention to \"You need to minimize storage costs for the solution\" that means snappy parquet compresson - A is correct"
      },
      {
        "date": "2024-04-14T23:13:00.000Z",
        "voteCount": 3,
        "content": "Use Snappy compression for the files is the only answer, which is about minimizing cost of storage. While one is using serverless SQL pool, the external tables are available, which are the only metadata..."
      },
      {
        "date": "2024-04-01T00:04:00.000Z",
        "voteCount": 4,
        "content": "Using Snappy compression for the Parquet files helps minimize storage costs while still maintaining good compression efficiency. Snappy is a compression library that offers a good balance between compression ratio and processing speed. By compressing the data using Snappy, you can significantly reduce the amount of storage required for your dataset.\n\nOption B, using OPENROWSET to query the Parquet files, doesn't directly impact storage costs. It's a method for querying data but doesn't address storage optimization.\n\nOption C, creating an external table with a subset of columns, may help reduce query costs by minimizing the amount of data that needs to be processed during queries. However, it doesn't directly address storage costs.\n\nOption D, storing all data as strings in the Parquet files, would likely increase storage costs rather than minimize them. Storing data as strings without appropriate compression would result in larger file sizes compared to using efficient compression algorithms like Snappy."
      },
      {
        "date": "2024-03-30T05:48:00.000Z",
        "voteCount": 2,
        "content": "A. Use Snappy compression for the files."
      },
      {
        "date": "2024-02-28T17:48:00.000Z",
        "voteCount": 3,
        "content": "The answer is C - Parquet has default SNAPPY compression which cannot be overwritten so why would I apply SNAPPY again?"
      },
      {
        "date": "2024-02-24T06:46:00.000Z",
        "voteCount": 2,
        "content": "Further information required for this question. There isn't enough information to go off as to what is being asked. The initial question is in regards to storage which would result in using the snappy compression answer. If you are asking about querying the data then this should be clearly defined in the question. If someone was to create a User Story with regards to this (As a Manager I want to store data in the data lake at the reduced cost) then you wouldn't be providing them with an External table. You would give them information on storage."
      },
      {
        "date": "2024-01-10T09:41:00.000Z",
        "voteCount": 1,
        "content": "Snappy compression can reduce the size of Parquet files by up to 70%. This can save you a significant amount of money on storage costs."
      },
      {
        "date": "2023-09-08T03:05:00.000Z",
        "voteCount": 2,
        "content": "Snappy"
      },
      {
        "date": "2023-09-03T06:50:00.000Z",
        "voteCount": 2,
        "content": "using compression"
      },
      {
        "date": "2023-08-24T03:12:00.000Z",
        "voteCount": 1,
        "content": "To minimize storage costs for the solution, you should use Snappy compression for the files. Snappy is a fast and efficient data compression and decompression library that can be used to compress Parquet files. This will help reduce the size of the data files and minimize storage costs in Azure Data Lake Storage Gen2. So, the correct answer is A. Use Snappy compression for the files"
      },
      {
        "date": "2023-07-25T03:39:00.000Z",
        "voteCount": 1,
        "content": "When presented with only the options of column pruning (variant of this is C) and compression (example of it would be snappy), ChatGPT choses C."
      },
      {
        "date": "2023-06-28T12:44:00.000Z",
        "voteCount": 10,
        "content": "I would go by exclusion:\nA. Use Snappy compression for the files. ---&gt; nothing against this!\nB. Use OPENROWSET to query the Parquet files. --&gt; doing this I just get a preview of the parquet files\nC. Create an external table that contains a subset of columns from the Parquet files. --&gt; no body asked for a subset\nD. Store all data as string in the Parquet --&gt; nobody asked that"
      },
      {
        "date": "2023-07-25T08:28:00.000Z",
        "voteCount": 1,
        "content": "Storing data as a string would also make the file size bigger"
      },
      {
        "date": "2023-06-24T06:39:00.000Z",
        "voteCount": 4,
        "content": "As per chat gpt , answer is A"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67532-exam-dp-203-topic-1-question-41-discussion/",
    "body": "DRAG DROP -<br>You need to build a solution to ensure that users can query specific files in an Azure Data Lake Storage Gen2 account from an Azure Synapse Analytics serverless SQL pool.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0008400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0008500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an external data source<br>You can create external tables in Synapse SQL pools via the following steps:<br>1. CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.<br>2. CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.<br>3. CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format.<br>Step 2: Create an external file format object<br>Creating an external file format is a prerequisite for creating an external table.<br>Step 3: Create an external table<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-10T18:24:00.000Z",
        "voteCount": 37,
        "content": "Looks correct answer"
      },
      {
        "date": "2023-06-23T14:16:00.000Z",
        "voteCount": 17,
        "content": "1. CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\n2. CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\n3. CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format."
      },
      {
        "date": "2024-03-27T02:27:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct"
      },
      {
        "date": "2023-09-03T06:50:00.000Z",
        "voteCount": 1,
        "content": "source, format ,external"
      },
      {
        "date": "2022-08-23T00:22:00.000Z",
        "voteCount": 6,
        "content": "If I run CREATE EXTERNAL FILE FORMAT before CREATE EXTERNAL DATA SOURCE, will it change something?"
      },
      {
        "date": "2023-04-25T06:36:00.000Z",
        "voteCount": 4,
        "content": "It doesn't really matter in which order you create a file format or a data source. I have done it in different orders before."
      },
      {
        "date": "2023-03-15T01:02:00.000Z",
        "voteCount": 2,
        "content": "Can anyone who knows answer this, would like to know too. Asked chatgpt and it says there's no functional difference. However, it is generally considered a best practice to create the external file format before the external data source."
      },
      {
        "date": "2023-03-28T04:44:00.000Z",
        "voteCount": 5,
        "content": "\"More than one order of answer choices is correct\", so it seems to me that CREATE EXTERNAL FILE FORMAT before CREATE EXTERNAL DATA SOURCE should be accepted too."
      },
      {
        "date": "2022-07-25T10:27:00.000Z",
        "voteCount": 1,
        "content": "given answer is correct"
      },
      {
        "date": "2022-07-18T03:46:00.000Z",
        "voteCount": 1,
        "content": "why creating a query that creates a table is not correct?"
      },
      {
        "date": "2022-08-04T03:00:00.000Z",
        "voteCount": 3,
        "content": "CeTAS also exports the query results to Blob storage or Data LAke. Its not a requirement in this question :) https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-cetas"
      },
      {
        "date": "2022-06-29T06:00:00.000Z",
        "voteCount": 5,
        "content": "Correct:\nSee Microsoft docs:\nYou can create external tables in Synapse SQL pools via the following steps:\n\n1) CREATE EXTERNAL DATA SOURCE to reference an external Azure storage and specify the credential that should be used to access the storage.\n2) CREATE EXTERNAL FILE FORMAT to describe format of CSV or Parquet files.\n3) CREATE EXTERNAL TABLE on top of the files placed on the data source with the same file format."
      },
      {
        "date": "2022-05-26T23:37:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-05-09T01:38:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-03-17T19:49:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-01-28T01:25:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-01-04T03:04:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2021-12-10T02:07:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67533-exam-dp-203-topic-1-question-42-discussion/",
    "body": "You are designing a data mart for the human resources (HR) department at your company. The data mart will contain employee information and employee transactions.<br>From a source system, you have a flat extract that has the following fields:<br>\u2711 EmployeeID<br><br>FirstName -<br><img src=\"/assets/media/exam-media/04259/0008500003.png\" class=\"in-exam-image\"><br>\u2711 LastName<br>\u2711 Recipient<br>\u2711 GrossAmount<br>\u2711 TransactionID<br>\u2711 GovernmentID<br>\u2711 NetAmountPaid<br>\u2711 TransactionDate<br>You need to design a star schema data model in an Azure Synapse Analytics dedicated SQL pool for the data mart.<br>Which two tables should you create? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dimension table for Transaction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dimension table for EmployeeTransaction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta dimension table for Employee\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta fact table for Employee",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta fact table for Transaction\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T18:28:00.000Z",
        "voteCount": 23,
        "content": "Correct Answer . Emp info as Dimension &amp; trans table as fact"
      },
      {
        "date": "2023-10-08T01:29:00.000Z",
        "voteCount": 2,
        "content": "Correct. A dimension is like a dictionary of information, therefore will hold the customer data such as name, address, date of birth, whatever. The fact table contains the facts. Usually numbers, usually the data that we are getting from a system from which we will create metrics later on. Therefore for transactions."
      },
      {
        "date": "2023-09-07T00:47:00.000Z",
        "voteCount": 1,
        "content": "Correct answers"
      },
      {
        "date": "2023-09-03T06:51:00.000Z",
        "voteCount": 1,
        "content": "CE is correct"
      },
      {
        "date": "2023-02-02T08:06:00.000Z",
        "voteCount": 3,
        "content": "Correct Answers"
      },
      {
        "date": "2022-11-11T02:29:00.000Z",
        "voteCount": 3,
        "content": "Correct answer"
      },
      {
        "date": "2022-07-25T10:31:00.000Z",
        "voteCount": 2,
        "content": "correct answer - CE"
      },
      {
        "date": "2022-06-19T06:46:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-11T04:55:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-05-09T01:40:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-02T05:55:00.000Z",
        "voteCount": 1,
        "content": "why not fact table for employee and dim table for transactions"
      },
      {
        "date": "2022-06-06T22:15:00.000Z",
        "voteCount": 3,
        "content": "do you even know what is fact or dim table? If you know you wouldn't be asking this question"
      },
      {
        "date": "2022-07-12T02:57:00.000Z",
        "voteCount": 13,
        "content": "If we would know we wouldn't be here."
      },
      {
        "date": "2022-04-17T06:00:00.000Z",
        "voteCount": 1,
        "content": "CE is correct"
      },
      {
        "date": "2022-04-11T00:14:00.000Z",
        "voteCount": 2,
        "content": "CE is the correct answer"
      },
      {
        "date": "2022-03-21T13:47:00.000Z",
        "voteCount": 2,
        "content": "CE is correct"
      },
      {
        "date": "2022-02-21T21:07:00.000Z",
        "voteCount": 1,
        "content": "Dimension for employee and fact for transactions."
      },
      {
        "date": "2022-01-28T01:30:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-12-10T02:09:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67534-exam-dp-203-topic-1-question-43-discussion/",
    "body": "You are designing a dimension table for a data warehouse. The table will track the value of the dimension attributes over time and preserve the history of the data by adding new rows as the data changes.<br>Which type of slowly changing dimension (SCD) should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tType 0",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tType 1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tType 2\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tType 3"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T02:10:00.000Z",
        "voteCount": 18,
        "content": "Correct"
      },
      {
        "date": "2022-05-30T08:15:00.000Z",
        "voteCount": 8,
        "content": "Kind of question that teacher leaves in paper for one free mark."
      },
      {
        "date": "2023-09-07T00:47:00.000Z",
        "voteCount": 1,
        "content": "Preserve history of changes... Type 2 is correct"
      },
      {
        "date": "2023-09-03T06:53:00.000Z",
        "voteCount": 1,
        "content": "SCD2 is correct"
      },
      {
        "date": "2023-08-07T00:34:00.000Z",
        "voteCount": 1,
        "content": "Type 2"
      },
      {
        "date": "2023-01-29T13:40:00.000Z",
        "voteCount": 3,
        "content": "Type 2 is correct"
      },
      {
        "date": "2023-01-04T23:20:00.000Z",
        "voteCount": 3,
        "content": "Type 2- Mainitains Row for each version on dimension data."
      },
      {
        "date": "2022-11-27T01:11:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-11-11T02:32:00.000Z",
        "voteCount": 4,
        "content": "Correct:\ntype 1 - does not keep history of previuos values\ntype 3 - updates all rows, not required"
      },
      {
        "date": "2022-07-25T10:32:00.000Z",
        "voteCount": 1,
        "content": "right answer given"
      },
      {
        "date": "2022-07-01T01:22:00.000Z",
        "voteCount": 1,
        "content": "C is right answer"
      },
      {
        "date": "2022-06-19T06:47:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-09T02:09:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-09T01:41:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-03T22:28:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-03-31T08:40:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2022-02-21T21:08:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/microsoft/view/82714-exam-dp-203-topic-1-question-44-discussion/",
    "body": "DRAG DROP -<br>You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2. Each file has a header row followed by a properly formatted carriage return (/ r) and line feed (/n).<br>You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.<br>You need to skip the header row when you import the files into the data warehouse. Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: Each correct selection is worth one point<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0008800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0008800002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an external data source that uses the abfs location<br>Create External Data Source to reference Azure Data Lake Store Gen 1 or 2<br>Step 2: Create an external file format and set the First_Row option.<br>Create External File Format.<br>Step 3: Use CREATE EXTERNAL TABLE AS SELECT (CETAS) and configure the reject options to specify reject values or percentages<br>To use PolyBase, you must create external tables to reference your external data.<br>Use reject options.<br>Note: REJECT options don't apply at the time this CREATE EXTERNAL TABLE AS SELECT statement is run. Instead, they're specified here so that the database can use them at a later time when it imports data from the external table. Later, when the CREATE TABLE AS SELECT statement selects data from the external table, the database will use the reject options to determine the number or percentage of rows that can fail to import before it stops the import.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects https://docs.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-18T13:00:00.000Z",
        "voteCount": 37,
        "content": "1) create database scoped credentials\n2) create external source\n3) create file format\n4) create external table (it not supports CTAS)"
      },
      {
        "date": "2023-06-28T12:58:00.000Z",
        "voteCount": 6,
        "content": "It supports, it is a dedicated SQL pool (means not severless), reading the question:\nYou are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase.\n--&gt; provided answers are correct in my opinion."
      },
      {
        "date": "2022-11-19T04:46:00.000Z",
        "voteCount": 18,
        "content": "Because it's saying \"You have data stored in thousands of CSV files in Azure Data Lake Storage Gen2\" and \"You are implementing a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics by using PolyBase\" assumption is that we already have database credentials, so the answer is:\n1) create external source\n2) create file format\n3) create external table"
      },
      {
        "date": "2023-05-23T04:54:00.000Z",
        "voteCount": 4,
        "content": "No, CETAS is not used for loading Azure Synapse Analytics. It's used to export data from and not to!"
      },
      {
        "date": "2024-04-14T23:28:00.000Z",
        "voteCount": 1,
        "content": "The provided answer is correct.\n1) source\n2) file format\n3) CETAS - this is dedicated SQL Pool. So, it is preferred. While external table is the only option for SERVERLESS SQL Pool...\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&amp;tabs=powershell#examples"
      },
      {
        "date": "2024-03-30T06:07:00.000Z",
        "voteCount": 6,
        "content": "The correct answer are : \n1) create database scoped credentials\n2) create external source\n3) create file format\nWhy ? : Focus on this below statement in question. \nBefore building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics.\nExplanation: You just need to specify about prerequisite database object ( Not Loading pattern which is about Creating External Table )"
      },
      {
        "date": "2023-12-09T20:20:00.000Z",
        "voteCount": 2,
        "content": "I think the answer (A, B, D) is very clearly explained on this page:  https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
      },
      {
        "date": "2023-10-08T02:23:00.000Z",
        "voteCount": 2,
        "content": "I believe the only reason why \"create database scoped credential\" is not a right answer is because it should be a managed identity instead of a service principal. Service principals are used for applications outside of the Azure Environment (such as SQL Server, as some comments here refer to SQL Server documentation). But since we are using Synapse Analytics environment, we use managed identities. \n\nCheck the link: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=powershell#g-use-create-external-table-as-select-with-a-view-as-the-source\n\nAnd if you want to compare to SQL Server documentation, where indeed they use Service principal keys (which is not the situation we are given in this question):\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-data-lake-store"
      },
      {
        "date": "2023-10-07T05:15:00.000Z",
        "voteCount": 2,
        "content": "The provided Answer is correct check yourself, goto F section in the following link\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-as-select-transact-sql?view=azure-sqldw-latest&amp;tabs=powershell#examples"
      },
      {
        "date": "2024-04-29T03:27:00.000Z",
        "voteCount": 1,
        "content": "F section shows all the options given in this questions :)\nmeaning including CETAS and data credential also"
      },
      {
        "date": "2023-09-08T03:17:00.000Z",
        "voteCount": 1,
        "content": "Before building the loading pattern, you need to prepare the required database objects in Azure Synapse Analytics - Loading pattern is CETAS, so answer \n\nDSC\nDS\nEFF"
      },
      {
        "date": "2023-09-03T06:56:00.000Z",
        "voteCount": 3,
        "content": "source ,format ,external"
      },
      {
        "date": "2023-07-27T08:56:00.000Z",
        "voteCount": 1,
        "content": "The answer is correct, \n- Create database scoped credential: \"This step is required only for Kerberos-secured Hadoop clusters.\"\nIn this case, the previous step does not apply."
      },
      {
        "date": "2023-06-23T14:21:00.000Z",
        "voteCount": 5,
        "content": "1. Create database scoped credential\n2. Create external data source\n3. Create external file format\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16#create-external-tables-for-azure-blob-storage"
      },
      {
        "date": "2023-04-17T02:46:00.000Z",
        "voteCount": 1,
        "content": "the given answer is correct imo; \"'PolyBase loads can be run using CTAS or INSERT INTO. CTAS will minimize transaction logging and is the fastest way to load your data. \"'\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool#use-polybase-to-load-and-export-data-quickly"
      },
      {
        "date": "2023-04-17T02:53:00.000Z",
        "voteCount": 3,
        "content": "sorry, wrong link: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16\n\"'PolyBase can now use CETAS to create an external table and then export, in parallel, the result of a Transact-SQL SELECT statement to Azure Data Lake Storage Gen2, Azure Storage Account V2, and S3-compatible object storage.\"'\n\"'Creates an external table and then exports, in parallel, the results of a Transact-SQL SELECT statement.\n\nFor Azure Synapse Analytics and Analytics Platform System, Hadoop or Azure Blob storage are supported.\"'"
      },
      {
        "date": "2023-02-02T05:21:00.000Z",
        "voteCount": 5,
        "content": "When using serverless SQL pool, CETAS is used to create an external table and export query results to Azure Storage Blob or Azure Data Lake Storage Gen2 and we need to implement a pattern that batch loads the files daily into a dedicated SQL pool in Azure Synapse Analytics, so:\n1) create database scoped credentials\n2) create external source\n3) create file format"
      },
      {
        "date": "2023-01-20T15:54:00.000Z",
        "voteCount": 1,
        "content": "In this question clearly stating that, \n\"Before building the loading pattern, you need to prepare the required database objects\"\nSo Database objects list \n1) Data Source \n2) Data File Format\n3) Table ( need to set up skip the header row)\nMy Answer is 1) Create external source; 2) Create File Format ; 3) Create External Table"
      },
      {
        "date": "2023-01-06T03:48:00.000Z",
        "voteCount": 3,
        "content": "CTAS for external table is to write the result of the query (select) in a destination folder.\nSo the good answer for this question is :\n) create database scoped credentials\n2) create external source\n3) create file format"
      },
      {
        "date": "2022-12-25T22:07:00.000Z",
        "voteCount": 2,
        "content": "Should be -\n1) create database scoped credentials\n2) create external source\n3) create file format\n\nAs per https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16\n- Check under : Create external tables for Azure Blob Storage - CTAS is not an option"
      },
      {
        "date": "2023-04-17T02:43:00.000Z",
        "voteCount": 1,
        "content": "yes it is an option PolyBase loads can be run using CTAS or INSERT INTO. CTAS will minimize transaction logging and is the fastest way to load your data. \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool#use-polybase-to-load-and-export-data-quickly"
      },
      {
        "date": "2023-04-17T02:51:00.000Z",
        "voteCount": 1,
        "content": "sorry, wrong link: https://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-t-sql-objects?view=sql-server-ver16  \n\"'PolyBase can now use CETAS to create an external table and then export, in parallel, the result of a Transact-SQL SELECT statement to Azure Data Lake Storage Gen2, Azure Storage Account V2, and S3-compatible object storage.\"'\n\"'Creates an external table and then exports, in parallel, the results of a Transact-SQL SELECT statement.\n\nFor Azure Synapse Analytics and Analytics Platform System, Hadoop or Azure Blob storage are supported.\"'"
      },
      {
        "date": "2022-11-27T06:50:00.000Z",
        "voteCount": 2,
        "content": "What is azure active directory application? is it managed identity?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67531-exam-dp-203-topic-1-question-45-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Synapse Analytics dedicated SQL pool that will contain a fact table for transactions from the first half of the year 2020.<br>You need to ensure that the table meets the following requirements:<br>\u2711 Minimizes the processing time to delete data that is older than 10 years<br>\u2711 Minimizes the I/O for queries that use year-to-date values<br>How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0009000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0009100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: PARTITION -<br>RANGE RIGHT FOR VALUES is used with PARTITION.<br>Part 2: [TransactionDateID]<br>Partition on the date column.<br>Example: Creating a RANGE RIGHT partition function on a datetime column<br>The following partition function partitions a table or index into 12 partitions, one for each month of a year's worth of values in a datetime column.<br>CREATE PARTITION FUNCTION [myDateRangePF1] (datetime)<br>AS RANGE RIGHT FOR VALUES ('20030201', '20030301', '20030401',<br>'20030501', '20030601', '20030701', '20030801',<br>'20030901', '20031001', '20031101', '20031201');<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-partition-function-transact-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-10T02:06:00.000Z",
        "voteCount": 25,
        "content": "Correct"
      },
      {
        "date": "2022-11-13T06:17:00.000Z",
        "voteCount": 7,
        "content": "Correct answer, giveaway is \"RANGE RIGHT\""
      },
      {
        "date": "2023-10-04T13:28:00.000Z",
        "voteCount": 1,
        "content": "Have a small doubt.\nWe are creating partition on what field?\nShouldn't it be the columnstore index."
      },
      {
        "date": "2023-09-03T06:57:00.000Z",
        "voteCount": 1,
        "content": "Partition"
      },
      {
        "date": "2023-01-29T13:54:00.000Z",
        "voteCount": 5,
        "content": "correct answer given"
      },
      {
        "date": "2022-07-25T10:45:00.000Z",
        "voteCount": 3,
        "content": "ans is  correct"
      },
      {
        "date": "2022-07-01T01:33:00.000Z",
        "voteCount": 4,
        "content": "I can see Keyword \"Range right for values\" pointing to \"Partition\", then \"TransactionDateID\"\n is the column on which partition needs to be done, rather than the TransactionID."
      },
      {
        "date": "2022-05-02T05:58:00.000Z",
        "voteCount": 2,
        "content": "How are we ensuring \"Minimizes the processing time to delete data that is older than 10 years\"?"
      },
      {
        "date": "2022-06-06T23:24:00.000Z",
        "voteCount": 4,
        "content": "while deleting we can use switch partition. It is efficient than delete statement, so partition by date column in good but the question says the TransactionDate as int field which is wrong. It should be date type"
      },
      {
        "date": "2022-09-29T12:34:00.000Z",
        "voteCount": 3,
        "content": "it's a date_ID and it's correct it will be linked to a date table via the date_ID to get the date"
      },
      {
        "date": "2022-03-08T08:04:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-01-28T02:45:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-01-08T03:06:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67530-exam-dp-203-topic-1-question-46-discussion/",
    "body": "You are performing exploratory analysis of the bus fare data in an Azure Data Lake Storage Gen2 account by using an Azure Synapse Analytics serverless SQL pool.<br>You execute the Transact-SQL query shown in the following exhibit.<br><img src=\"/assets/media/exam-media/04259/0009300001.jpg\" class=\"in-exam-image\"><br>What do the query results include?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly CSV files in the tripdata_2020 subfolder.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll files that have file names that beginning with \"tripdata_2020\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAll CSV files that have file names that contain \"tripdata_2020\".",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly CSV that have file names that beginning with \"tripdata_2020\".\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-10T02:06:00.000Z",
        "voteCount": 19,
        "content": "Correct"
      },
      {
        "date": "2024-02-06T01:09:00.000Z",
        "voteCount": 1,
        "content": "Just a tricky question to think about:\nIs the csv/bufare/tripdata_2020_something/something/something.csv file will be loaded?\nI think YES :) But it is not matched with answer A, because not ONLY the tripdata_2020 folder will be loaded."
      },
      {
        "date": "2024-02-06T01:10:00.000Z",
        "voteCount": 1,
        "content": "And therefore the D is not exactly true."
      },
      {
        "date": "2023-02-03T23:33:00.000Z",
        "voteCount": 5,
        "content": "D is the correct Answer"
      },
      {
        "date": "2023-09-07T00:52:00.000Z",
        "voteCount": 1,
        "content": "Correct is D"
      },
      {
        "date": "2023-09-03T06:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-05-02T09:53:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-02-02T08:15:00.000Z",
        "voteCount": 3,
        "content": "D is the correct Answer"
      },
      {
        "date": "2023-01-27T02:34:00.000Z",
        "voteCount": 2,
        "content": "Sorry but I don't understand.\nFile or Directory that start with \"tripdata_2020\" can selected.\n/tripdata_2020/a.csv\n/tripdata_2020_a_b.csv\n/tripdata_2020/2020/1/1/myfile.csv\nSo question is very not clear.\nD question is partially correct"
      },
      {
        "date": "2023-02-06T12:44:00.000Z",
        "voteCount": 2,
        "content": "Only file, there is no / after 2020 in the exercise."
      },
      {
        "date": "2023-01-25T23:41:00.000Z",
        "voteCount": 4,
        "content": "Answer D is correct. Would be great if it was also correct grammatically."
      },
      {
        "date": "2022-11-11T03:13:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-08-10T01:35:00.000Z",
        "voteCount": 2,
        "content": "CSV that have file names that beginning with \"tripdata_2020\" ."
      },
      {
        "date": "2022-07-25T10:48:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-06-11T23:23:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-05-26T23:42:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-04-17T06:04:00.000Z",
        "voteCount": 3,
        "content": "on this one you need to pay attention to wording"
      },
      {
        "date": "2022-04-08T23:48:00.000Z",
        "voteCount": 1,
        "content": "D all good"
      },
      {
        "date": "2022-04-06T00:57:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-03-22T11:25:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74667-exam-dp-203-topic-1-question-47-discussion/",
    "body": "DRAG DROP -<br>You use PySpark in Azure Databricks to parse the following JSON input.<br><img src=\"/assets/media/exam-media/04259/0009400001.png\" class=\"in-exam-image\"><br>You need to output the data in the following tabular format.<br><img src=\"/assets/media/exam-media/04259/0009400002.png\" class=\"in-exam-image\"><br>How should you complete the PySpark code? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the spit bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0009500001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0009500002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: select -<br><br>Box 2: explode -<br><br>Bop 3: alias -<br>pyspark.sql.Column.alias returns this column aliased with a new name or names (in the case of expressions that return more than one column, such as explode).<br>Reference:<br>https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Column.alias.html https://docs.microsoft.com/en-us/azure/databricks/sql/language-manual/functions/explode",
    "votes": [],
    "comments": [
      {
        "date": "2022-06-16T06:49:00.000Z",
        "voteCount": 23,
        "content": "The final line with the blank looks incorrect... surely it should be:\nexplode(\"persons.dogs\").alias(\"dog\"))\n\n(Assuming this, the answer is correct, otherwise I don't think it makes any sense)."
      },
      {
        "date": "2023-03-02T05:10:00.000Z",
        "voteCount": 17,
        "content": "ah \"persons\".alias(\"persons\") what a fun and useful and nice alias"
      },
      {
        "date": "2023-09-03T06:59:00.000Z",
        "voteCount": 2,
        "content": "syntax is correct"
      },
      {
        "date": "2023-03-14T22:33:00.000Z",
        "voteCount": 12,
        "content": "dbutils.fs.put(\"/tmp/source.json\", source_json, True)\nsource_df = spark.read.option(\"multiline\", \"true\").json(\"/tmp/source.json\")\npersons = source_df.select(explode(\"persons\").alias(\"persons\"))\npersons_dogs = persons.select(col(\"persons.name\").alias(\"owner\"), col(\"persons.age\").alias(\"age\"), explode(col(\"persons.dog\")).alias(\"dog_name\"))\npersons_dogs.display()"
      },
      {
        "date": "2022-07-25T10:55:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      },
      {
        "date": "2022-07-22T03:54:00.000Z",
        "voteCount": 5,
        "content": "Correct, but last .alias(\"dog\") is quite unnecessary because the column name is alredy 'dog'. I guess that is for safety measurement."
      },
      {
        "date": "2023-02-23T13:03:00.000Z",
        "voteCount": 3,
        "content": "The column name in the json is dogs, not dog"
      },
      {
        "date": "2022-04-27T04:12:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74417-exam-dp-203-topic-1-question-48-discussion/",
    "body": "HOTSPOT -<br>You are designing an application that will store petabytes of medical imaging data.<br>When the data is first created, the data will be accessed frequently during the first week. After one month, the data must be accessible within 30 seconds, but files will be accessed infrequently. After one year, the data will be accessed infrequently but must be accessible within five minutes.<br>You need to select a storage strategy for the data. The solution must minimize costs.<br>Which storage tier should you use for each time frame? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0009700001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0009800001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hot -<br>Hot tier - An online tier optimized for storing data that is accessed or modified frequently. The Hot tier has the highest storage costs, but the lowest access costs.<br><br>Box 2: Cool -<br>Cool tier - An online tier optimized for storing data that is infrequently accessed or modified. Data in the Cool tier should be stored for a minimum of 30 days. The<br>Cool tier has lower storage costs and higher access costs compared to the Hot tier.<br><br>Box 3: Cool -<br>Not Archive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the<br>Archive tier should be stored for a minimum of 180 days.<br><img src=\"/assets/media/exam-media/04259/0009900001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://www.altaro.com/hyper-v/azure-archive-storage/",
    "votes": [],
    "comments": [
      {
        "date": "2022-07-25T10:58:00.000Z",
        "voteCount": 22,
        "content": "right , Hot-cool-cool"
      },
      {
        "date": "2022-04-25T01:53:00.000Z",
        "voteCount": 9,
        "content": "Correct answer!\nHot, Cool, Cool"
      },
      {
        "date": "2023-09-07T00:53:00.000Z",
        "voteCount": 2,
        "content": "Hot\nCool\nCool"
      },
      {
        "date": "2023-09-03T07:00:00.000Z",
        "voteCount": 2,
        "content": "hot,cool,cool"
      },
      {
        "date": "2023-05-07T22:00:00.000Z",
        "voteCount": 3,
        "content": "Hot, Cool, Cool"
      },
      {
        "date": "2022-06-30T02:38:00.000Z",
        "voteCount": 2,
        "content": "Isn't the cool storage enough for initial requirements and also required for the other options? So shouldn't the answer be cool in all places? that would be cool"
      },
      {
        "date": "2022-07-11T12:49:00.000Z",
        "voteCount": 5,
        "content": "Reads in Cool Tier are more expensive than in Hot Tier. Since the data will be accessed frequently in the first week, it makes sense to store it in hot tier to minimize costs."
      },
      {
        "date": "2022-05-07T21:14:00.000Z",
        "voteCount": 2,
        "content": "Why would it not be be Hot Cool and Archive"
      },
      {
        "date": "2022-05-09T04:00:00.000Z",
        "voteCount": 5,
        "content": "After one year, the data will be accessed infrequently but must be accessible within five minutes."
      },
      {
        "date": "2023-05-07T06:44:00.000Z",
        "voteCount": 2,
        "content": "hydration on the archive tier is hours, not minutes. hence its hot, cool, cool."
      },
      {
        "date": "2022-05-09T00:47:00.000Z",
        "voteCount": 20,
        "content": "\"After one year, the data will be accessed infrequently but must be accessible within five minutes\"\nThe latency for the first bytes, is \"hours\" for the archive. so because they want to be able to access the data within 5 min, you need to place it in \"cool\"\n\nSo the answer is correct."
      },
      {
        "date": "2022-05-07T21:13:00.000Z",
        "voteCount": 1,
        "content": "I dont know"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74669-exam-dp-203-topic-1-question-49-discussion/",
    "body": "You have an Azure Synapse Analytics Apache Spark pool named Pool1.<br>You plan to load JSON files from an Azure Data Lake Storage Gen2 container into the tables in Pool1. The structure and data types vary by file.<br>You need to load the files into the tables. The solution must maintain the source data types.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Conditional Split transformation in an Azure Synapse data flow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Get Metadata activity in Azure Data Factory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data by using the OPENROWSET Transact-SQL command in an Azure Synapse Analytics serverless SQL pool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data by using PySpark.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-04-27T04:15:00.000Z",
        "voteCount": 39,
        "content": "Should be D, it's about Apache Spark pool, not serverless SQL pool."
      },
      {
        "date": "2024-01-10T12:22:00.000Z",
        "voteCount": 5,
        "content": "If your JSON files have a consistent structure and data types, then OPENROWSET is a good option. However, if your JSON files have a varying structure and data types, then PySpark is a better option."
      },
      {
        "date": "2024-08-12T00:25:00.000Z",
        "voteCount": 1,
        "content": "The answer is C because with external table you can load the data with solution must maintain the source data types."
      },
      {
        "date": "2024-07-07T11:57:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nUsing PySpark in an Apache Spark pool within Azure Synapse Analytics is the most flexible and powerful way to handle JSON files with varying structures and data types. PySpark can infer schema and handle complex data transformations, making it well-suited for loading heterogeneous JSON data into tables while preserving the original data types."
      },
      {
        "date": "2024-07-06T07:00:00.000Z",
        "voteCount": 1,
        "content": "When loading data into an Apache Spark pool, especially when dealing with inconsistent file structures, PySpark (the Python API for Spark) is generally the better choice over OpenRowset. This is because PySpark offers greater flexibility, better performance, and more robust handling of varied and complex data structures."
      },
      {
        "date": "2024-06-29T01:29:00.000Z",
        "voteCount": 1,
        "content": "should be D"
      },
      {
        "date": "2023-10-08T03:04:00.000Z",
        "voteCount": 2,
        "content": "We have an \"Azure Synapse Analytics Apache Spark pool\" therefore, we use Spark. There is no information about a serverless SQL Pool"
      },
      {
        "date": "2023-09-03T07:02:00.000Z",
        "voteCount": 2,
        "content": "Should be D"
      },
      {
        "date": "2023-06-23T14:28:00.000Z",
        "voteCount": 3,
        "content": "PySpark provides a powerful and flexible programming interface for processing and loading data in Azure Synapse Analytics Apache Spark pools. With PySpark, you can leverage its JSON reader capabilities to infer the schema and maintain the source data types during the loading process."
      },
      {
        "date": "2023-06-08T08:24:00.000Z",
        "voteCount": 4,
        "content": "To load JSON files from an Azure Data Lake Storage Gen2 container into tables in an Azure Synapse Analytics Apache Spark pool, you can use PySpark. PySpark provides a flexible and powerful framework for working with big data in Apache Spark.\n\nTherefore, the correct answer is:\n\nD. Load the data by using PySpark.\n\nYou can use PySpark to read the JSON files from Azure Data Lake Storage Gen2, infer the schema, and load the data into tables in the Spark pool while maintaining the source data types. PySpark provides various functions and methods to handle JSON data and perform transformations as needed before loading it into tables."
      },
      {
        "date": "2023-05-23T10:48:00.000Z",
        "voteCount": 1,
        "content": "Option D: Load the data by using PySpark"
      },
      {
        "date": "2023-05-02T10:25:00.000Z",
        "voteCount": 1,
        "content": "The question stated that \"You have an Azure Synapse Analytics Apache Spark pool named Pool1.\", so this question is about Spark pool"
      },
      {
        "date": "2023-04-19T10:52:00.000Z",
        "voteCount": 4,
        "content": "As stated by Microsoft, \"Serverless SQL pool can automatically synchronize metadata from Apache Spark. A serverless SQL pool database will be created for each database existing in serverless Apache Spark pools.\". So even though the files in Azure Storage were created with Apache Spark, you can still query them using OPENROWSET with a serverless SQL Pool\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-spark-tables"
      },
      {
        "date": "2024-04-14T23:42:00.000Z",
        "voteCount": 1,
        "content": "We are dealing with varying JSON. There is nothing about this option by the link you've provided. The correct answer is D..."
      },
      {
        "date": "2023-11-21T13:08:00.000Z",
        "voteCount": 2,
        "content": "As the question states that \"You need to load the files into the tables\" , through serverless sql pool we cannot load data. so the answer should be D"
      },
      {
        "date": "2023-03-15T21:39:00.000Z",
        "voteCount": 3,
        "content": "To load JSON files from an Azure Data Lake Storage Gen2 container into the tables in an Apache Spark pool in Azure Synapse Analytics while maintaining the source data types, you should use PySpark."
      },
      {
        "date": "2023-02-20T08:58:00.000Z",
        "voteCount": 2,
        "content": "PySpark is the Python API for Apache Spark, which is a distributed computing framework that can handle large-scale data processing."
      },
      {
        "date": "2022-11-30T19:26:00.000Z",
        "voteCount": 2,
        "content": "Should be D, it's about Apache Spark pool, not serverless SQL pool."
      },
      {
        "date": "2022-09-26T00:58:00.000Z",
        "voteCount": 2,
        "content": "Its a spark pool"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74670-exam-dp-203-topic-1-question-50-discussion/",
    "body": "You have an Azure Databricks workspace named workspace1 in the Standard pricing tier. Workspace1 contains an all-purpose cluster named cluster1.<br>You need to reduce the time it takes for cluster1 to start and scale up. The solution must minimize costs.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a global init script for workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cluster policy in workspace1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade workspace1 to the Premium pricing tier.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pool in workspace1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-05-31T05:52:00.000Z",
        "voteCount": 13,
        "content": "Answer D is correct. Azure Databricks pools reduce cluster start and auto-scaling times by maintaining a set of idle, ready-to-use instances."
      },
      {
        "date": "2023-05-08T05:00:00.000Z",
        "voteCount": 9,
        "content": "D is accurate answer and this link show this info explicty \nhttps://learn.microsoft.com/en-us/azure/databricks/clusters/cluster-config-best-practices"
      },
      {
        "date": "2024-05-01T01:37:00.000Z",
        "voteCount": 2,
        "content": "I found this question on my exam yesterday, 30/04/2024, and I put C. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-05-03T05:57:00.000Z",
        "voteCount": 1,
        "content": "The answer C seems to be false, given that we have to minimize the costs and the prenium tier has a higher cost"
      },
      {
        "date": "2024-04-14T23:45:00.000Z",
        "voteCount": 1,
        "content": "The solution must minimize costs, so upgrade to Premium is not an option."
      },
      {
        "date": "2024-01-28T20:59:00.000Z",
        "voteCount": 3,
        "content": "This came in my today's exam . Answer D is correct ."
      },
      {
        "date": "2024-01-10T12:24:00.000Z",
        "voteCount": 4,
        "content": "The best solution to reduce the time it takes for cluster1 to start and scale up while minimizing costs is to create a pool in workspace1.\n\nA pool is a group of preconfigured clusters that share resources and can be quickly provisioned and scaled up or down as needed. This can significantly reduce the time it takes for cluster1 to start and scale up, as the resources are already provisioned and ready to use."
      },
      {
        "date": "2023-12-21T14:33:00.000Z",
        "voteCount": 3,
        "content": "Got this question today on the exam. Selected Upgrade to Premium tier, was incorrect of course"
      },
      {
        "date": "2023-09-03T07:02:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-08-18T01:42:00.000Z",
        "voteCount": 1,
        "content": "C is the answer"
      },
      {
        "date": "2023-06-19T01:19:00.000Z",
        "voteCount": 1,
        "content": "To reduce the time it takes for cluster1 to start and scale up in Azure Databricks workspace, the first step you should take is to create a pool in workspace1."
      },
      {
        "date": "2023-06-07T08:18:00.000Z",
        "voteCount": 2,
        "content": "The correct answer in this scenario would be B. Create a cluster policy in workspace1.\n\nCreating a cluster policy allows you to define a set of rules and configurations that apply to all clusters within the workspace. By creating a cluster policy, you can optimize the cluster startup and scaling behavior.\n\nWith a cluster policy, you can specify settings such as the minimum and maximum number of worker nodes, the idle timeout duration, and the auto-termination behavior. By tuning these settings, you can reduce the time it takes for cluster1 to start and scale up, ensuring that resources are efficiently utilized.\n\nConfiguring a global init script (option A) or creating a pool (option D) may have other benefits, but they are not directly related to reducing the time it takes for cluster1 to start and scale up. Upgrading the workspace to the Premium pricing tier (option C) may offer additional features but is not necessary to address the specific requirement of minimizing cluster startup and scaling time while minimizing costs."
      },
      {
        "date": "2023-06-07T08:15:00.000Z",
        "voteCount": 1,
        "content": "How can it be C), as the question clearly states 'minimize costs'"
      },
      {
        "date": "2023-05-23T10:54:00.000Z",
        "voteCount": 1,
        "content": "I think it's \nB. Create a cluster policy in workspace1\nCluster policies in Azure Databricks allow you to define rules and configurations for cluster creation and termination. By creating a cluster policy, you can optimize the cluster start time and scale-up process based on your specific requirements."
      },
      {
        "date": "2023-05-11T04:56:00.000Z",
        "voteCount": 1,
        "content": "D is correct answer"
      },
      {
        "date": "2023-05-07T22:03:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D"
      },
      {
        "date": "2023-05-02T10:29:00.000Z",
        "voteCount": 2,
        "content": "C and D are practical but the question asked to minimize the costs. Thus, the answer is D."
      },
      {
        "date": "2023-04-04T18:01:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74291-exam-dp-203-topic-1-question-51-discussion/",
    "body": "HOTSPOT -<br>You are building an Azure Stream Analytics job that queries reference data from a product catalog file. The file is updated daily.<br>The reference data input details for the file are shown in the Input exhibit. (Click the Input tab.)<br><img src=\"/assets/media/exam-media/04259/0010200001.jpg\" class=\"in-exam-image\"><br>The storage account container view is shown in the Refdata exhibit. (Click the Refdata tab.)<br><img src=\"/assets/media/exam-media/04259/0010300001.png\" class=\"in-exam-image\"><br>You need to configure the Stream Analytics job to pick up the new reference data.<br>What should you configure? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0010400001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0010500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: {date}/product.csv -<br>In the 2nd exhibit we see: Location: refdata / 2020-03-20<br>Note: Path Pattern: This is a required property that is used to locate your blobs within the specified container. Within the path, you may choose to specify one or more instances of the following 2 variables:<br>{date}, {time}<br>Example 1: products/{date}/{time}/product-list.csv<br>Example 2: products/{date}/product-list.csv<br><br>Example 3: product-list.csv -<br><br>Box 2: YYYY-MM-DD -<br>Note: Date Format [optional]: If you have used {date} within the Path Pattern that you specified, then you can select the date format in which your blobs are organized from the drop-down of supported formats.<br>Example: YYYY/MM/DD, MM/DD/YYYY, etc.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data",
    "votes": [],
    "comments": [
      {
        "date": "2022-07-22T04:15:00.000Z",
        "voteCount": 25,
        "content": "correct. \n\nReasons:\n1. {dat}/{time}/product.csv\n\nMore detailed things should be put at the last.\n\n2. YYYY-MM-DD\n\nif you choose YYYY/MM/DD, the system will think this is a file path."
      },
      {
        "date": "2023-01-20T16:23:00.000Z",
        "voteCount": 20,
        "content": "second box is straight forwarded answer YYYY-MM-DD\nFirst Box = {date}/product.csv - Because the requirement is reference data loaded on daily basis, so it may be once in a day not hourly or timely."
      },
      {
        "date": "2023-09-04T01:30:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-01-20T16:36:00.000Z",
        "voteCount": 2,
        "content": "The recommended way to refresh reference data is to:\nUse {date}/{time} in the path pattern. Box 1 should be {dat}/{time}/product.csv"
      },
      {
        "date": "2022-07-28T10:05:00.000Z",
        "voteCount": 3,
        "content": "given answer is correct"
      },
      {
        "date": "2022-07-25T06:27:00.000Z",
        "voteCount": 2,
        "content": "1. {date}/product.csv\n2. YYYY-MM-DD"
      },
      {
        "date": "2022-07-15T20:08:00.000Z",
        "voteCount": 2,
        "content": "Path pattern: This required property is used to locate your blobs within the specified container. Within the path, you might choose to specify one or more instances of the variables {date} and {time}.\nExample 1: products/{date}/{time}/product-list.csv\nExample 2: products/{date}/product-list.csv\nExample 3: product-list.csv\n\nIf the blob doesn't exist in the specified path, the Stream Analytics job waits indefinitely for the blob to become available.\n\n#####\nDate format [optional]:\tIf you used {date} within the path pattern you specified, select the date format in which your blobs are organized from the dropdown list of supported formats.\nExample: YYYY/MM/DD or MM/DD/YYYY\n\n##### \nTime format [optional]: If you used {time} within the path pattern you specified, select the time format in which your blobs are organized from the dropdown list of supported formats.\nExample: HH, HH/mm, or HH-mm\n\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data"
      },
      {
        "date": "2022-06-28T17:14:00.000Z",
        "voteCount": 3,
        "content": "I think the {date}/product.csv is correct, however it's formatted as YYYY/MM/DD, so why hyphenate it? In the example provided in the link the date was formatted with - instead of /, but in the question it's all /...."
      },
      {
        "date": "2022-06-28T17:17:00.000Z",
        "voteCount": 5,
        "content": "....and to answer my own question, the second exhibit shows the product.csv file in refdata/yyyy-mm-dd . So that'll be the path"
      },
      {
        "date": "2023-02-14T07:51:00.000Z",
        "voteCount": 1,
        "content": "Yes it through me at first because the first graphic refers to the file contents date format. When they looking for directory naming format"
      },
      {
        "date": "2023-08-04T08:24:00.000Z",
        "voteCount": 2,
        "content": "The prints don't look consistent with each other, the 2nd image the location is: refdata/2020-03-20/product.csv should be \"YYYY-DD-MM\".\nBut the 1st image shows a different format: YYYY/MM/DD."
      },
      {
        "date": "2022-05-30T14:03:00.000Z",
        "voteCount": 3,
        "content": "answers are correct"
      },
      {
        "date": "2022-05-02T03:18:00.000Z",
        "voteCount": 9,
        "content": "I should change box 2 to YYYY/MM/DD (as shows 1st exhibit). A bit confusing with time format in the box 1."
      },
      {
        "date": "2022-04-26T05:35:00.000Z",
        "voteCount": 5,
        "content": "The file is updated daily, i think `{date}/product.csv` is correct"
      },
      {
        "date": "2022-04-24T00:31:00.000Z",
        "voteCount": 3,
        "content": "Wrong! Path Pattern: {dat}/{time}/product.csv\nDat format: yyyy-mm-dd\nhttps://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-use-reference-data"
      },
      {
        "date": "2022-04-30T11:35:00.000Z",
        "voteCount": 8,
        "content": "See that the file is stored under the date folder, and there is no time folder. \nYour link does recommend the time part, but the the link also says it's optional, and ultimately you need to answer the question, which states the path without the time."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/microsoft/view/75015-exam-dp-203-topic-1-question-52-discussion/",
    "body": "HOTSPOT -<br>You have the following Azure Stream Analytics query.<br><img src=\"/assets/media/exam-media/04259/0010700001.png\" class=\"in-exam-image\"><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0010800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0010800002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: No -<br>Note: You can now use a new extension of Azure Stream Analytics SQL to specify the number of partitions of a stream when reshuffling the data.<br>The outcome is a stream that has the same partition scheme. Please see below for an example:<br>WITH step1 AS (SELECT * FROM [input1] PARTITION BY DeviceID INTO 10), step2 AS (SELECT * FROM [input2] PARTITION BY DeviceID INTO 10)<br>SELECT * INTO [output] FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID<br>Note: The new extension of Azure Stream Analytics SQL includes a keyword INTO that allows you to specify the number of partitions for a stream when performing reshuffling using a PARTITION BY statement.<br><br>Box 2: Yes -<br>When joining two streams of data explicitly repartitioned, these streams must have the same partition key and partition count.<br><br>Box 3: Yes -<br>Streaming Units (SUs) represents the computing resources that are allocated to execute a Stream Analytics job. The higher the number of SUs, the more CPU and memory resources are allocated for your job.<br>In general, the best practice is to start with 6 SUs for queries that don't use PARTITION BY.<br>Here there are 10 partitions, so 6x10 = 60 SUs is good.<br>Note: Remember, Streaming Unit (SU) count, which is the unit of scale for Azure Stream Analytics, must be adjusted so the number of physical resources available to the job can fit the partitioned flow. In general, six SUs is a good number to assign to each partition. In case there are insufficient resources assigned to the job, the system will only apply the repartition if it benefits the job.<br>Reference:<br>https://azure.microsoft.com/en-in/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/ https://docs.microsoft.com/en-us/azure/stream-analytics/stream-analytics-streaming-unit-consumption",
    "votes": [],
    "comments": [
      {
        "date": "2022-06-12T02:26:00.000Z",
        "voteCount": 37,
        "content": "I feel its all YES. Since it does use a UNION and UNION combines. No matter it repartitions the result is the combination of two sources, a UNION of two sources. Am I missing something here?"
      },
      {
        "date": "2023-10-02T07:23:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer to the first question heavily relies on creator's understanding \"what is query\". If  it is the last part only (without CTEs) than the answer should be \"yes\", because you have partitioned data that come from CTEs as input for the main query. But if creator's understaning that query is the whole thing, than probably answer should be \"no\", because you're receiving non-partitioned data from sensors."
      },
      {
        "date": "2023-06-27T13:13:00.000Z",
        "voteCount": 23,
        "content": "False, True, False.\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\" \nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False, \n\"In general, six SUs are needed for each partition.\"\n In the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60."
      },
      {
        "date": "2024-04-30T04:56:00.000Z",
        "voteCount": 2,
        "content": "they are unioned not joined"
      },
      {
        "date": "2024-04-14T23:52:00.000Z",
        "voteCount": 1,
        "content": "1) Y\n2) Y The output scheme should match the stream scheme key and count so that each substream can be flushed independently. https://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n3) Y"
      },
      {
        "date": "2024-02-07T15:52:00.000Z",
        "voteCount": 3,
        "content": "False, True, False.\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\"\nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False,\n\"In general, six SUs are needed for each partition.\"\nIn the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60."
      },
      {
        "date": "2024-02-03T03:32:00.000Z",
        "voteCount": 1,
        "content": "The first question should be yes. \"Partitioning lets you divide data into subsets based on a partition key\". https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization"
      },
      {
        "date": "2024-01-30T18:46:00.000Z",
        "voteCount": 1,
        "content": "Yes NO NO"
      },
      {
        "date": "2023-12-06T12:43:00.000Z",
        "voteCount": 4,
        "content": "Chatgpt say : yes tes no.\nBased on the provided information and additional documentation on Azure Stream Analytics:\n\n1. **Yes**: The query combines two streams of partitioned data [[\u275e]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/).\n2. **Yes**: The stream scheme key and count should match the output scheme for optimal independent processing of each substream [[\u275e]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/).\n3. **No**: The documentation does not specify that providing 60 streaming units will optimize the query's performance. The appropriate number of streaming units depends on experimentation and resource usage observation [[\u275e]](https://azure.microsoft.com/fr-fr/blog/maximize-throughput-with-repartitioning-in-azure-stream-analytics/)."
      },
      {
        "date": "2024-01-09T23:56:00.000Z",
        "voteCount": 5,
        "content": "You gice one reasoning to chatgpt it will change its answer so i dont think it is reliable resource"
      },
      {
        "date": "2023-10-08T04:55:00.000Z",
        "voteCount": 3,
        "content": "The answer you need for first and second questions is in Microsoft Documentation:\n\"The following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme. (...) The output scheme should match the stream scheme key and count so that each substream can be flushed independently. \" https://learn.microsoft.com/en-us/azure/stream-analytics/repartition#repartition-input-within-a-single-stream-analytics-job\n\nSo its not two streams of partitioned data, but two streams of REpartitioned data. \nAnd the output stream must have the same partition key and count.\n\nFor the third question, a bit lower in the same link, we get: In general, six SUs are needed for each partition. Therefore, if we have 10 partitions, 6*10 = 60."
      },
      {
        "date": "2023-10-08T05:19:00.000Z",
        "voteCount": 3,
        "content": "I will correct my previous message. After reading through SU calculation documentation, I concluded it should be 120 for SU V1 or 20 for SU V2. Therefore none of them would be 60. \nExplanation is this sentence here:\n\"All non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step. \nSo tecnically, it would be 21 SU V2. \nTherefore, 60 SU is not correct. \n\nShould be \nFALSE\nTRUE\nFALSE"
      },
      {
        "date": "2023-11-05T16:26:00.000Z",
        "voteCount": 1,
        "content": "Calculate the max streaming units for a job\nAll non-partitioned steps together can scale up to one streaming unit (SU V2s) for a Stream Analytics job. In addition, you can add 1 SU V2 for each partition in a partitioned step. You can see some examples in the table below.\n\nQuery\tMax SUs for the job\nThe query contains one step.\nThe step isn't partitioned.\n1 SU V2\n\n\nThe input data stream is partitioned by 16.\nThe query contains one step.\nThe step is partitioned.\n16 SU V2 (1 * 16 partitions)\n\n\nThe query contains two steps.\nNeither of the steps is partitioned.\n1 SU V2\n\n\nThe input data stream is partitioned by 3.\nThe query contains two steps. The input step is partitioned and the second step isn't.\nThe SELECT statement reads from the partitioned input.\n4 SU V2s (3 for partitioned steps + 1 for non-partitioned steps\n\nBase on the above from MS documentation, why do we need to multiply by 6SUs?"
      },
      {
        "date": "2023-10-04T04:00:00.000Z",
        "voteCount": 1,
        "content": "I think this is an older question since there is SU V2 now.\nAccording to this: https://learn.microsoft.com/en-us/azure/stream-analytics/stream-analytics-parallelization\nEach partition would consume 1 Streamiung Unit (SU) V2 and since we have 2 inputs with 10 partitions each it would add up to 20 SU V2, now we have 2 Select statements after the WITH Step which each consume 1 SU V2, so it should add up to 22 SU V2 which would equal 22*6=132 SU V1."
      },
      {
        "date": "2023-10-01T08:22:00.000Z",
        "voteCount": 1,
        "content": "Based on recommended Streaming units,\nStep 1: 10 partitions\nStep 2: 10 partitions\n\n(1*10+1*10) = 20 SU's is the optimal, if you have more, it's not ideal because some SU's are inactive and if you have less, it can cause a bottleneck\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/stre\nam-analytics-parallelization#calculate-the-maximum-streaming-units-of-a-job"
      },
      {
        "date": "2023-08-25T10:23:00.000Z",
        "voteCount": 1,
        "content": "Question 1 is false; the question says the union of streams and not data. The union combines 2 streams which are the same and thus, the output is the same stream."
      },
      {
        "date": "2023-06-21T00:41:00.000Z",
        "voteCount": 2,
        "content": "1. True\n2. False:\nIn the context of a UNION operation in Azure Stream Analytics, the stream scheme key and count do not need to match the output schema. The key and count of the output schema are determined based on the input streams being unioned.\n\nWhen performing a UNION operation, the input streams must have compatible schemas, which means that the data types and field names should align. However, the key and count are determined by the input streams themselves and do not need to match the output schema.\n3. True"
      },
      {
        "date": "2023-06-21T00:46:00.000Z",
        "voteCount": 1,
        "content": "Again for point 2-False:\nhttps://learn.microsoft.com/en-us/stream-analytics-query/union-azure-stream-analytics\nThe following are basic rules for combining the result sets of two queries by using UNION:\n- The number and the order of the columns must be the same in all queries.\n- The data types must be compatible.\n- Streams must have the same partition and partition count (not scheme key and count!&nbsp;:-)&nbsp;)"
      },
      {
        "date": "2023-09-04T01:37:00.000Z",
        "voteCount": 1,
        "content": "All answer is no."
      },
      {
        "date": "2023-09-04T01:45:00.000Z",
        "voteCount": 1,
        "content": "forgot it, no, yes, no"
      },
      {
        "date": "2023-06-27T13:14:00.000Z",
        "voteCount": 3,
        "content": "False, true, false.\nI've changed mind completely looking and reading accurately this official link where all 3 questions are answered:\n\nhttps://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n\nThe first is False, because this:\n\"The following example query joins two streams of repartitioned data.\" \nIt's extracted from the link above, and it's pointing to our query! Repartitioned and not partitioned.\nSecond is True, it's explicitly written\nThe output scheme should match the stream scheme key and count so that each substream can be flushed independently.\nThird is False, \n\"In general, six SUs are needed for each partition.\"\n In the example we have 10 positions for step 1 and 10 for step 2, it should be 120 and not 60."
      },
      {
        "date": "2023-01-09T08:29:00.000Z",
        "voteCount": 1,
        "content": "I believe the answer to the First Question is No because the execution of the last statement will result in an error because the second query has \"SELECT INTO output\" rather than a straight SELECT.  Can anyone confirm that you can UNION two data sets both directed to the same stream with SELECT INTO?  It's not functionality shown in any example I've been able to find (in the given answer, linked in other comments on here, and my own research)."
      },
      {
        "date": "2022-12-26T00:01:00.000Z",
        "voteCount": 5,
        "content": "Is the first option NO because it mentions partitioned data instead of repartitioned data?\n\nReference : https://learn.microsoft.com/en-us/azure/stream-analytics/repartition\n\nThe following example query joins two streams of repartitioned data. When joining two streams of repartitioned data, the streams must have the same partition key and count. The outcome is a stream that has the same partition scheme.\n\nWITH step1 AS (SELECT * FROM input1 PARTITION BY DeviceID),\nstep2 AS (SELECT * FROM input2 PARTITION BY DeviceID)\n\nSELECT * INTO output FROM step1 PARTITION BY DeviceID UNION step2 PARTITION BY DeviceID"
      },
      {
        "date": "2023-02-02T03:16:00.000Z",
        "voteCount": 2,
        "content": "yes, the only difference I see is this:\n\n1)this example:\nSELECT * \nINTO output \nFROM \nstep1 PARTITION BY StateID \nUNION  \nSELECT * INTO output FROM step2 PARTITION BY StateID #is using another select * on top of step2\n\n2)the doc example:\nSELECT * INTO output \nFROM \nstep1 PARTITION BY DeviceID \nUNION \nstep2 PARTITION BY DeviceID\n\nI think both query joins two streams of repartitioned/partitioned data, so first answer should be yes"
      },
      {
        "date": "2022-12-04T11:34:00.000Z",
        "voteCount": 3,
        "content": "All 3 are YES"
      },
      {
        "date": "2022-11-30T06:57:00.000Z",
        "voteCount": 1,
        "content": "At first I thought all 3 are Y but then; Strems are Input and UNION combines query results only so in that case first is No."
      },
      {
        "date": "2022-11-29T05:54:00.000Z",
        "voteCount": 2,
        "content": "The explanation of the second Yes indicates that the first one is also Yes"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/microsoft/view/75193-exam-dp-203-topic-1-question-53-discussion/",
    "body": "HOTSPOT -<br>You are building a database in an Azure Synapse Analytics serverless SQL pool.<br>You have data stored in Parquet files in an Azure Data Lake Storege Gen2 container.<br>Records are structured as shown in the following sample.<br>{<br>\"id\": 123,<br>\"address_housenumber\": \"19c\",<br>\"address_line\": \"Memory Lane\",<br>\"applicant1_name\": \"Jane\",<br>\"applicant2_name\": \"Dev\"<br>}<br>The records contain two applicants at most.<br>You need to build a table that includes only the address fields.<br>How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0011000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0011100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: CREATE EXTERNAL TABLE -<br>An external table points to data located in Hadoop, Azure Storage blob, or Azure Data Lake Storage. External tables are used to read data from files or write data to files in Azure Storage. With Synapse SQL, you can use external tables to read external data using dedicated SQL pool or serverless SQL pool.<br>Syntax:<br>CREATE EXTERNAL TABLE { database_name.schema_name.table_name | schema_name.table_name | table_name }<br>( &lt;column_definition&gt; [ ,...n ] )<br>WITH (<br>LOCATION = 'folder_or_filepath',<br>DATA_SOURCE = external_data_source_name,<br>FILE_FORMAT = external_file_format_name<br><br>Box 2. OPENROWSET -<br>When using serverless SQL pool, CETAS is used to create an external table and export query results to Azure Storage Blob or Azure Data Lake Storage Gen2.<br>Example:<br><br>AS -<br>SELECT decennialTime, stateName, SUM(population) AS population<br><br>FROM -<br>OPENROWSET(BULK 'https://azureopendatastorage.blob.core.windows.net/censusdatacontainer/release/us_population_county/year=*/*.parquet',<br>FORMAT='PARQUET') AS [r]<br>GROUP BY decennialTime, stateName<br><br>GO -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables",
    "votes": [],
    "comments": [
      {
        "date": "2022-07-28T10:25:00.000Z",
        "voteCount": 20,
        "content": "Correct answer"
      },
      {
        "date": "2024-04-11T03:27:00.000Z",
        "voteCount": 3,
        "content": "1) CREATE EXTERNAL TABLE (because this is SERVERLESS sql pool)\n2) OPENROWSET"
      },
      {
        "date": "2023-09-04T01:46:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-08-24T07:26:00.000Z",
        "voteCount": 2,
        "content": "Why is it External Table if it's a Serverless SQL pool?"
      },
      {
        "date": "2023-06-24T14:40:00.000Z",
        "voteCount": 2,
        "content": "\"You have data stored in Parquet files in an Azure Data Lake Storege Gen2 container.\""
      },
      {
        "date": "2022-09-23T03:22:00.000Z",
        "voteCount": 15,
        "content": "because serverless SQL pool does not have internal tables"
      },
      {
        "date": "2022-06-28T00:29:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-05-09T22:54:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-05-04T23:42:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74671-exam-dp-203-topic-1-question-54-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics dedicated SQL pool named Pool1 and an Azure Data Lake Storage Gen2 account named Account1.<br>You plan to access the files in Account1 by using an external table.<br>You need to create a data source in Pool1 that you can reference when you create the external table.<br>How should you complete the Transact-SQL statement? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0011200001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0011300001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: blob -<br>The following example creates an external data source for Azure Data Lake Gen2<br>CREATE EXTERNAL DATA SOURCE YellowTaxi<br>WITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',<br>TYPE = HADOOP)<br><br>Box 2: HADOOP -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables",
    "votes": [],
    "comments": [
      {
        "date": "2022-04-27T04:19:00.000Z",
        "voteCount": 62,
        "content": "1. dfs (for Azure Data Lake Storage Gen2)"
      },
      {
        "date": "2023-05-23T05:48:00.000Z",
        "voteCount": 4,
        "content": "Correct, https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated#location--prefixpath"
      },
      {
        "date": "2023-01-27T05:18:00.000Z",
        "voteCount": 1,
        "content": "dfs is not valid"
      },
      {
        "date": "2023-04-10T01:16:00.000Z",
        "voteCount": 7,
        "content": "dfs is valid \nData Lake Storage Gen2\nabfs[s] &lt;container&gt;@&lt;storage_account&gt;.dfs.core.windows.net\nhttp[s] &lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/subfolders\nwasb[s] &lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net"
      },
      {
        "date": "2023-03-29T02:43:00.000Z",
        "voteCount": 1,
        "content": "This table corroborates that \"dfs\" should be used for ADLS Gen 2:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#location"
      },
      {
        "date": "2023-01-31T04:43:00.000Z",
        "voteCount": 6,
        "content": "CREATE EXTERNAL DATA SOURCE mydatasource\nWITH (    LOCATION   = 'abfss://data@storageaccount.dfs.core.windows.net',\n          CREDENTIAL = AzureStorageCredential,\n          TYPE = HADOOP\n)"
      },
      {
        "date": "2022-11-23T14:53:00.000Z",
        "voteCount": 36,
        "content": "1. blob. Acoording with this article https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop we only use DFS (abfss endpoint) when your account has secure transfer enabled. \n\nOn the question the location starts with \"https://account1.\" not \"abfss://\""
      },
      {
        "date": "2024-04-16T05:57:00.000Z",
        "voteCount": 4,
        "content": "As it is written in this MSF example on how to create an External data source based on Azure storage account:\n\"\nTYPE = HADOOP,                 -- For dedicated SQL pool\n-- TYPE = BLOB_STORAGE, -- For serverless SQL pool\n\"\nhttps://learn.microsoft.com/en-us/training/modules/use-azure-synapse-serverless-sql-pools-for-transforming-data-lake/2-transform-data-using-create-external-table-select-statement"
      },
      {
        "date": "2023-03-24T03:56:00.000Z",
        "voteCount": 7,
        "content": "That's not correct, you use abfss:// if you have secure transfer enabled. There is nothing wrong with using https:// when you don't have secure transfer enabled. However, for DLSv2 you need to specify .dfs. ...\nThe correct answer is is:\ndfs\nhadoop"
      },
      {
        "date": "2023-04-10T01:16:00.000Z",
        "voteCount": 6,
        "content": "@kure87  DFS is valid \nData Lake Storage Gen2\nabfs[s] &lt;container&gt;@&lt;storage_account&gt;.dfs.core.windows.net\nhttp[s] &lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/subfolders\nwasb[s] &lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net"
      },
      {
        "date": "2022-12-19T22:47:00.000Z",
        "voteCount": 2,
        "content": "please upvote this"
      },
      {
        "date": "2024-06-30T08:18:00.000Z",
        "voteCount": 1,
        "content": "Dedicated SQL Pool: Use CREATE EXTERNAL DATA SOURCE with TYPE = HADOOP for accessing Azure Data Lake Storage Gen2.\nServerless SQL Pool: Use OPENROWSET for direct querying of the external data."
      },
      {
        "date": "2024-04-17T19:54:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL DATA SOURCE MyDataSource\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'abfss://&lt;container-name&gt;@&lt;storage-account-name&gt;.dfs.core.windows.net/',\n    CREDENTIAL = &lt;your-credential-name&gt;\n);"
      },
      {
        "date": "2024-04-15T00:00:00.000Z",
        "voteCount": 1,
        "content": "answer is correct\nsee the MS example \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source"
      },
      {
        "date": "2024-03-30T22:10:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer : DFS \nExplnation : If your source is ADLS Gen2 then it would be \"DFS\" and if your source is Azure Blob Storage then \"Blob\". \n\nPlease refer below table from Microsoft Documentation. \nExternal Data Source\tConnector location prefix\tLocation path\nData Lake Storage* Gen1\tadl\t&lt;storage_account&gt;.azuredatalake.net\nData Lake Storage Gen2\tabfs[s]\t&lt;container&gt;@&lt;storage_account&gt;.dfs.core.windows.net\nAzure Blob Storage\twasbs\t&lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net\nAzure Blob Storage\thttps\t&lt;storage_account&gt;.blob.core.windows.net/&lt;container&gt;/subfolders\nData Lake Storage Gen1\thttp[s]\t&lt;storage_account&gt;.azuredatalakestore.net/webhdfs/v1\nData Lake Storage Gen2\thttp[s]\t&lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/subfolders\nData Lake Storage Gen2\twasb[s]\t&lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net"
      },
      {
        "date": "2024-03-24T13:02:00.000Z",
        "voteCount": 1,
        "content": "Should be DFS for Datalake Gen2"
      },
      {
        "date": "2023-12-19T23:03:00.000Z",
        "voteCount": 1,
        "content": "Both `blob` and `dfs` endpoints work when connecting to Azure Data Lake Storage Gen2, but they serve different purposes. The `blob` endpoint is typically used for standard storage operations, while the `dfs` endpoint is optimized for hierarchical file system operations and is preferred for analytics workloads with Azure Synapse Analytics."
      },
      {
        "date": "2023-12-19T23:04:00.000Z",
        "voteCount": 2,
        "content": "To simply access files in Azure Data Lake Storage Gen2 for reading and analysis, without the need for Data Lake specific features like directory management or fine-grained ACLs, using the `blob` endpoint is sufficient. If your operations are primarily related to accessing files for reading, the `blob` endpoint can be used in the external data source definition within Azure Synapse Analytics."
      },
      {
        "date": "2023-09-27T03:14:00.000Z",
        "voteCount": 3,
        "content": "Answer is CORRECT: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#example-for-create-external-data-source\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n       TYPE = HADOOP)"
      },
      {
        "date": "2023-09-21T04:58:00.000Z",
        "voteCount": 1,
        "content": "Ans : dfs &amp; hadoop\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-data-source-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated"
      },
      {
        "date": "2023-09-04T01:49:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL DATA SOURCE AzureDataLakeStore\nWITH\n  -- Please note the abfss endpoint when your account has secure transfer enabled\n  ( LOCATION = 'abfss://data@newyorktaxidataset.dfs.core.windows.net' ,\n    CREDENTIAL = ADLS_credential ,\n    TYPE = HADOOP\n  ) ;\n\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n       TY\n\n\nHADOOP, blob"
      },
      {
        "date": "2023-08-25T08:53:00.000Z",
        "voteCount": 1,
        "content": "dfs should be the correct answer  (ADLS Gen2)"
      },
      {
        "date": "2023-06-21T01:57:00.000Z",
        "voteCount": 3,
        "content": "Confirmed HADOOP and DFS:\nExternal Data Source         | Connector | Location path\n----------------------------------------------------------------------------------------------------------------------------\nData Lake Storage Gen1   | adl       \t\t| &lt;storage_account&gt;.azuredatalake.net\nData Lake Storage Gen2   | abfs[s]   \t| &lt;container&gt;@&lt;storage_account&gt;.dfs.core.windows.net\nAzure Blob Storage            | wasbs     \t| &lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net\nAzure Blob Storage       \t   | https     \t\t| &lt;storage_account&gt;.blob.core.windows.net/&lt;container&gt;/subfolders\nData Lake Storage Gen1   | http[s]   \t\t| &lt;storage_account&gt;.azuredatalakestore.net/webhdfs/v1\nData Lake Storage Gen2   | http[s]   \t\t| &lt;storage_account&gt;.dfs.core.windows.net/&lt;container&gt;/subfolders\nData Lake Storage Gen2   | wasb[s]   \t| &lt;container&gt;@&lt;storage_account&gt;.blob.core.windows.net"
      },
      {
        "date": "2023-06-19T02:11:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL DATA SOURCE source1\nWITH (\n    LOCATION = 'https://account1.dfs.core.windows.net',\n    TYPE = HADOOP\n)"
      },
      {
        "date": "2023-06-08T02:22:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL DATA SOURCE DataSourceName\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'adl://Account1.dfs.core.windows.net/',\n    CREDENTIAL = SqlPoolCredential\n);"
      },
      {
        "date": "2023-05-23T11:46:00.000Z",
        "voteCount": 1,
        "content": "CREATE EXTERNAL DATA SOURCE &lt;datasource_name&gt;\nWITH (\n    TYPE = HADOOP,\n    LOCATION = 'adl://&lt;account_name&gt;.dfs.core.windows.net',\n    CREDENTIAL = &lt;credential_name&gt;\n);\n\nSo answer is dfs and Type = Hadoop"
      },
      {
        "date": "2023-05-13T20:23:00.000Z",
        "voteCount": 3,
        "content": "1. blob\n2. TYPE=HADOOP\nSource: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nThe following example creates an external data source for Azure Data Lake Gen2 pointing to the publicly available New York data set:\nCREATE EXTERNAL DATA SOURCE YellowTaxi\nWITH ( LOCATION = 'https://azureopendatastorage.blob.core.windows.net/nyctlc/yellow/',\n       TYPE = HADOOP)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/microsoft/view/75450-exam-dp-203-topic-1-question-55-discussion/",
    "body": "You have an Azure subscription that contains an Azure Blob Storage account named storage1 and an Azure Synapse Analytics dedicated SQL pool named<br>Pool1.<br>You need to store data in storage1. The data will be read by Pool1. The solution must meet the following requirements:<br>Enable Pool1 to skip columns and rows that are unnecessary in a query.<br><img src=\"/assets/media/exam-media/04259/0011300002.png\" class=\"in-exam-image\"><br>\u2711 Automatically create column statistics.<br>\u2711 Minimize the size of files.<br>Which type of file should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 30,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-06-30T23:34:00.000Z",
        "voteCount": 25,
        "content": "If the answer has Parquet. Of course, you should choose that answer. :D"
      },
      {
        "date": "2022-05-17T08:50:00.000Z",
        "voteCount": 15,
        "content": "Automatic creation of statistics is turned on for Parquet files. For CSV files, you need to create statistics manually until automatic creation of CSV files statistics is supported."
      },
      {
        "date": "2022-05-24T18:40:00.000Z",
        "voteCount": 3,
        "content": "Good point, also better cost"
      },
      {
        "date": "2024-04-11T03:42:00.000Z",
        "voteCount": 1,
        "content": "PARQUET"
      },
      {
        "date": "2023-10-29T07:50:00.000Z",
        "voteCount": 4,
        "content": "Parquet is always the answer ;-)"
      },
      {
        "date": "2023-09-04T01:50:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-08-07T01:20:00.000Z",
        "voteCount": 1,
        "content": "Parquet"
      },
      {
        "date": "2023-05-17T19:11:00.000Z",
        "voteCount": 7,
        "content": "When in doubt, select Parquet."
      },
      {
        "date": "2022-10-11T05:48:00.000Z",
        "voteCount": 5,
        "content": "When reading from Parquet files, you can specify only the columns you want to read and skip the rest."
      },
      {
        "date": "2022-08-29T20:51:00.000Z",
        "voteCount": 2,
        "content": "CORRECT"
      },
      {
        "date": "2022-08-24T23:29:00.000Z",
        "voteCount": 2,
        "content": "Question is bit contradictory:  it mentions reading blob storage data in dedicated sql , which could be done by External Tables, however, dedicated sql pool do NOT support automatic stats for external tables (as mentioned on \"automatic stats creation for dedicated sql pool\" section-  https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-statistics"
      },
      {
        "date": "2024-01-01T02:34:00.000Z",
        "voteCount": 1,
        "content": "@monibun : great point. I agree with you . I am not sure if anyone wants to advise on your point ."
      },
      {
        "date": "2022-07-28T10:34:00.000Z",
        "voteCount": 2,
        "content": "Parquet"
      },
      {
        "date": "2022-05-10T19:41:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74340-exam-dp-203-topic-1-question-56-discussion/",
    "body": "DRAG DROP -<br>You plan to create a table in an Azure Synapse Analytics dedicated SQL pool.<br>Data in the table will be retained for five years. Once a year, data that is older than five years will be deleted.<br>You need to ensure that the data is distributed evenly across partitions. The solution must minimize the amount of time required to delete old data.<br>How should you complete the Transact-SQL statement? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0011500001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0011600001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: HASH -<br><br>Box 2: OrderDateKey -<br>In most cases, table partitions are created on a date column.<br>A way to eliminate rollbacks is to use Metadata Only operations like partition switching for data management. For example, rather than execute a DELETE statement to delete all rows in a table where the order_date was in October of 2001, you could partition your data early. Then you can switch out the partition with data for an empty partition from another table.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/t-sql/statements/create-table-azure-sql-data-warehouse https://docs.microsoft.com/en-us/azure/synapse-analytics/sql/best-practices-dedicated-sql-pool",
    "votes": [],
    "comments": [
      {
        "date": "2022-05-17T09:04:00.000Z",
        "voteCount": 23,
        "content": "I think it is Hash because the question refer to a Fact table."
      },
      {
        "date": "2023-02-22T09:22:00.000Z",
        "voteCount": 17,
        "content": "1. Hash -&gt; Fact Table\n2. DateKey -&gt; for Partition"
      },
      {
        "date": "2024-04-11T04:03:00.000Z",
        "voteCount": 1,
        "content": "1) HASH, because this is a fact table\n2) OrderDateKey (simply see the partition values - these are dates)"
      },
      {
        "date": "2023-09-04T01:52:00.000Z",
        "voteCount": 3,
        "content": "the syntax is ok only for HASH &amp;\nDatekey"
      },
      {
        "date": "2023-08-31T04:04:00.000Z",
        "voteCount": 1,
        "content": "Why not 'Product Key' for partition? can anyone explain me please."
      },
      {
        "date": "2023-09-23T10:22:00.000Z",
        "voteCount": 4,
        "content": "Because partitioning on the date key will help in deleting older data quickly since the older records' partition can be moved to a different table and the table truncated."
      },
      {
        "date": "2023-06-24T07:52:00.000Z",
        "voteCount": 2,
        "content": "if it is round robin, there is no key to specify, so hash"
      },
      {
        "date": "2023-03-25T12:17:00.000Z",
        "voteCount": 5,
        "content": "It must be HASH because of syntax."
      },
      {
        "date": "2023-02-03T08:44:00.000Z",
        "voteCount": 3,
        "content": "The Answer is correct"
      },
      {
        "date": "2023-01-26T11:52:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct."
      },
      {
        "date": "2023-01-21T15:58:00.000Z",
        "voteCount": 2,
        "content": "Should be Round Robin as the requirement is to have the data evenly. the second one should be on the date"
      },
      {
        "date": "2023-03-18T01:50:00.000Z",
        "voteCount": 1,
        "content": "It should, but they have given attributes. So only hash supports attribute"
      },
      {
        "date": "2023-06-19T02:25:00.000Z",
        "voteCount": 3,
        "content": "You would use Round-Robin for staging table and not Fact table."
      },
      {
        "date": "2022-10-28T05:51:00.000Z",
        "voteCount": 4,
        "content": "&lt;distribution_option&gt; ::=\n    { \n        DISTRIBUTION = HASH ( distribution_column_name ) \n      | DISTRIBUTION = ROUND_ROBIN \n      | DISTRIBUTION = REPLICATE\n    }  \n\n+ fact table\nit's for sure ::: hash"
      },
      {
        "date": "2022-10-09T15:39:00.000Z",
        "voteCount": 3,
        "content": "data is distributed evenly across partitions and data is deleted once a year not frequently. So it should be Round-robin distribution."
      },
      {
        "date": "2022-08-23T18:06:00.000Z",
        "voteCount": 4,
        "content": "Cannot be Round Robin, the syntax of distribution for round robin don't mention/include Column Name. So it has to be HASH"
      },
      {
        "date": "2022-07-30T00:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct"
      },
      {
        "date": "2022-07-29T03:32:00.000Z",
        "voteCount": 1,
        "content": "should be round robin because of distributed evenly."
      },
      {
        "date": "2022-09-05T14:32:00.000Z",
        "voteCount": 4,
        "content": "syntax used is for HASH distribution."
      },
      {
        "date": "2022-07-25T06:34:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-04-24T09:36:00.000Z",
        "voteCount": 2,
        "content": "I think the first answer should be Round-Robin as it should be distributed evenly.\nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute"
      },
      {
        "date": "2022-04-28T04:26:00.000Z",
        "voteCount": 1,
        "content": "yes i think so too"
      },
      {
        "date": "2022-04-29T08:48:00.000Z",
        "voteCount": 6,
        "content": "the syntax is ok only for HASH"
      },
      {
        "date": "2022-04-25T07:06:00.000Z",
        "voteCount": 26,
        "content": "While you are right, that Round-Robin guarantees an even distribution, it is only recommended to use on small tables &lt; 2 GB (see your link). Using the Hash of the ProductKey will also allow for an even distribution but in a more efficient manner.\nAlso, the Syntax here would be wrong if you would insert Round-Robin. As in that case it would only say: \"DISTRIBUTION = ROUND-ROBIN\" (no ProductKey)"
      },
      {
        "date": "2022-05-07T21:33:00.000Z",
        "voteCount": 1,
        "content": "You are exactly righty"
      },
      {
        "date": "2022-07-16T09:27:00.000Z",
        "voteCount": 2,
        "content": "@Feljoud : Thanks for the clarification. Even I opted for Roundrobin, considering the keywords = \"distributed evenly\", but that's incorrect."
      },
      {
        "date": "2022-11-29T06:23:00.000Z",
        "voteCount": 1,
        "content": "For small tables is recommended replicated, not round robin"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/microsoft/view/73989-exam-dp-203-topic-1-question-57-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Data Lake Storage Gen2 service.<br>You need to design a data archiving solution that meets the following requirements:<br>\u2711 Data that is older than five years is accessed infrequently but must be available within one second when requested.<br>\u2711 Data that is older than seven years is NOT accessed.<br>\u2711 Costs must be minimized while maintaining the required availability.<br>How should you manage the data? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0011800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0011900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Move to cool storage -<br><br>Box 2: Move to archive storage -<br>Archive - Optimized for storing data that is rarely accessed and stored for at least 180 days with flexible latency requirements, on the order of hours.<br>The following table shows a comparison of premium performance block blob storage, and the hot, cool, and archive access tiers.<br><img src=\"/assets/media/exam-media/04259/0012000001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers",
    "votes": [],
    "comments": [
      {
        "date": "2022-04-21T01:55:00.000Z",
        "voteCount": 39,
        "content": "If \"Data that is older than seven years is NOT accessed\" then this data can be deleted to minimize the storage costs, right?"
      },
      {
        "date": "2022-04-25T07:53:00.000Z",
        "voteCount": 5,
        "content": "I agree, should be deleted"
      },
      {
        "date": "2022-04-30T12:10:00.000Z",
        "voteCount": 29,
        "content": "Deleting data older than 7 years is not an option available in the answer list. Becareful of the gotcha; 'Delete the blob' is an option but it would delete all the data, included the ones that are e.g. 5 years old. So you can't choose that answer. So the next best thing to do is to put it into archive."
      },
      {
        "date": "2022-05-10T07:59:00.000Z",
        "voteCount": 2,
        "content": "I'm confused by your comment. It clearly does state an option to delete the blob after 7 years."
      },
      {
        "date": "2022-06-07T02:38:00.000Z",
        "voteCount": 2,
        "content": "whet he meant to say is all the data be it in hot, cool or achieve resides in the blob. So if you delete the blob it will delete all the data be it 5 years or 7 years or more recent data in hot tier. Delete blob option is just to make it a tricky question"
      },
      {
        "date": "2022-07-15T20:30:00.000Z",
        "voteCount": 1,
        "content": "Why it will delete data in hot tier? I have a blob. Blob between 5 and 7 go to cool, more than 7 delete."
      },
      {
        "date": "2023-06-24T14:46:00.000Z",
        "voteCount": 2,
        "content": "God no... A single piece of data could be a BLOB (Binary Large OBJECT)."
      },
      {
        "date": "2023-01-20T08:25:00.000Z",
        "voteCount": 6,
        "content": "Be careful with wording. Any answer given must be an \"Archiving solution\" &amp; Delete the blob is not an archiving solution."
      },
      {
        "date": "2023-05-09T12:29:00.000Z",
        "voteCount": 4,
        "content": "of course it is."
      },
      {
        "date": "2022-04-25T10:21:00.000Z",
        "voteCount": 2,
        "content": "Makes sense to me"
      },
      {
        "date": "2024-01-14T04:03:00.000Z",
        "voteCount": 2,
        "content": "Most important part about that question is \"Costs must be minimized while maintaining the required availability.\". So deleting isn't an option, since it won't be available anymore. So ARCHIVE seems correct"
      },
      {
        "date": "2024-01-25T12:56:00.000Z",
        "voteCount": 1,
        "content": "The documentation, which is linked into the answer: \"Archive tier - An offline tier optimized for storing data that is rarely accessed, and ...\""
      },
      {
        "date": "2022-10-10T18:15:00.000Z",
        "voteCount": 9,
        "content": "Answer is correct. RE: Why is 'Delete the blob' not a valid option? Well I agree that 7 years may seem a long time to most who commented here BUT there is ABSOLUTELY NO mention to DELETE here,  In several context NOT ACCESSED can easily refer to be drawn OFFLINE in ARCHIVE IOW:\n\"Your data files may be stored in the archive access tier in Azure Blob storage based on different context needs. According to the Azure documentation: While a blob is in archive storage, the blob data is OFFLINE and CANNOT BE ACCESSED that is: read, copied, overwritten, or modified. You also can't take snapshots of a blob in archive storage. However, the blob METADA remains online and available, allowing you to list the blob and its properties. For blobs in archive, the only valid operations are GetBlobProperties, GetBlobMetadata, ListBlobs, SetBlobTier, and DeleteBlob. For more information about Azure Blob storage tiers, see the Azure documentation: \nhttps://docs.microsoft.com/en-us/azure/storage/blobs/storage-blob-storage-tiers."
      },
      {
        "date": "2024-08-03T14:13:00.000Z",
        "voteCount": 1,
        "content": "The question states that data older than 7 years is NOT accessed.  However, there is nothing stating that the company no longer needs the data.  It should be archived because the company may still have a regulatory or legal requirement to keep this data."
      },
      {
        "date": "2024-06-27T06:31:00.000Z",
        "voteCount": 2,
        "content": "In my opinion, the retention policy clearly states that any file or blob older than seven years is a violation. The policy specifies: \"Data that is older than seven years is NOT accessed.\" Therefore, any data older than seven years should be deleted to avoid unnecessary costs, as we should not pay for storage we don't need. This approach aligns with the requirement to \"minimize costs while maintaining the required availability.\"\n\nFor data retention:\n\nData older than five years should be moved to the cool storage tier.\nData older than seven years should be deleted."
      },
      {
        "date": "2024-06-05T01:03:00.000Z",
        "voteCount": 2,
        "content": "Why would keep any data in archive if it is NOT accessed? That would be just waste of money and possible violation of give data protection regulations."
      },
      {
        "date": "2024-04-11T04:06:00.000Z",
        "voteCount": 3,
        "content": "1) Cool, because it takes hours to access Archive data, but cool is cheaper than Hot\n2) archive. For those, who thinks, that \"Data that is older than seven years is NOT accessed\" is equal to \"the data can be deleted\", you're fired :)"
      },
      {
        "date": "2024-06-16T00:44:00.000Z",
        "voteCount": 2,
        "content": "Damn Right!! With my 10 years exp never ever delete the data until its being asked by business owners for cost reduction. It will end up reduction in work force in form of you."
      },
      {
        "date": "2024-03-26T07:19:00.000Z",
        "voteCount": 1,
        "content": "Data older than 7 years old is NOT accessed.\nArchived blobs are not accessible and deleting the blobs would be too aggressive so I'll go with archive for the second answer with the question as is."
      },
      {
        "date": "2024-03-24T13:11:00.000Z",
        "voteCount": 1,
        "content": "Cool Storage and Delete job, because in this way you can seriously minimize costs"
      },
      {
        "date": "2024-03-01T02:35:00.000Z",
        "voteCount": 1,
        "content": "i think you guys are really overthinking it. just f....g delete data that will never be accesed."
      },
      {
        "date": "2024-01-26T20:05:00.000Z",
        "voteCount": 1,
        "content": "Costs must be minimized while maintaining the required availability. -- &gt; DO NOT DELETE"
      },
      {
        "date": "2023-12-11T02:43:00.000Z",
        "voteCount": 1,
        "content": "correct. deleting files is not a type of archiving them (chatGPT)"
      },
      {
        "date": "2023-09-07T01:07:00.000Z",
        "voteCount": 1,
        "content": "Cool &amp; Archive\n\nNo deletion of data here because question require to perform a data archiving solution"
      },
      {
        "date": "2023-08-24T23:02:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-08-21T19:03:00.000Z",
        "voteCount": 1,
        "content": "The question says: \u2711 Provides the highest degree of data resiliency and this would be RA-GZRS with customer failover. Data is replicated to 3 zones in the primary region and async to a secondary region. If primary zone fails, there is read and write to other zones in the region. Microsoft only engages a failover if \"original primary region is deemed unrecoverable within a reasonable amount of time due to a major disaster.\", which is not the case here. The answer provided is correct"
      },
      {
        "date": "2023-06-21T04:08:00.000Z",
        "voteCount": 1,
        "content": "\"Costs must be minimized while maintaining the required availability.\" ==&gt; it means Move to archive storage\""
      },
      {
        "date": "2023-01-30T10:15:00.000Z",
        "voteCount": 5,
        "content": "The Given answer is correct,the big tip here is archiving solution,deleting is not archiving solution"
      },
      {
        "date": "2022-11-03T04:53:00.000Z",
        "voteCount": 1,
        "content": "Archive might be the right option as we need to retrive before we access the data. so we cannot access without retrieval."
      },
      {
        "date": "2022-09-27T23:34:00.000Z",
        "voteCount": 1,
        "content": "Why is 'Delete the blob' not a valid option? Given that seven years data is not accessed, why don't we delete the blob?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80300-exam-dp-203-topic-1-question-58-discussion/",
    "body": "HOTSPOT -<br>You plan to create an Azure Data Lake Storage Gen2 account.<br>You need to recommend a storage solution that meets the following requirements:<br>\u2711 Provides the highest degree of data resiliency<br>\u2711 Ensures that content remains available for writes if a primary data center fails<br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0012100003.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0012200001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=/azure/storage/blobs/toc.json https://docs.microsoft.com/en-us/answers/questions/32583/azure-data-lake-gen2-disaster-recoverystorage-acco.html",
    "votes": [],
    "comments": [
      {
        "date": "2022-11-08T15:08:00.000Z",
        "voteCount": 77,
        "content": "I am surprised you all missed this requirement 'Ensures that content remains available for writes if a primary data center fails'. RA-GRS and RAGZRS provide read access only after failover. The correct answer is ZRS as t=stated in the link below \"Microsoft recommends using ZRS in the primary region for Azure Data Lake Storage Gen2 workloads.\" https://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json"
      },
      {
        "date": "2024-04-11T04:15:00.000Z",
        "voteCount": 3,
        "content": "using your link find this\n\"If the primary region becomes unavailable, you can choose to fail over to the secondary region. After the failover has completed, the secondary region becomes the primary region, and you can again read and write data. \"\nSo, the answer is right, but it's a pity, not yours."
      },
      {
        "date": "2024-07-26T04:48:00.000Z",
        "voteCount": 3,
        "content": "Question talks about Data Center, not region (\"if a primary data center\"). When you chose ZRS you are replicating in 3 Data Centers. You don't need GRS or GZRS"
      },
      {
        "date": "2022-11-30T11:13:00.000Z",
        "voteCount": 1,
        "content": "you're right. \nhttps://bluexp.netapp.com/blog/azure-anf-blg-azure-storage-replication-explained-lrs-zrs-grs-ra-grs#h_h3"
      },
      {
        "date": "2023-06-24T14:48:00.000Z",
        "voteCount": 12,
        "content": "You can still write data to second region if first one fails. RA only allows you to read data in second region even if the first does not fail."
      },
      {
        "date": "2023-04-10T01:24:00.000Z",
        "voteCount": 10,
        "content": "What about the highest level of data resiliency that cant be in ZRS .... answer is RA-GZRS"
      },
      {
        "date": "2024-08-03T14:16:00.000Z",
        "voteCount": 1,
        "content": "Having \"Provides the highest degree of data resiliency\" as a requirement automatically sends you to the most redundant storage option."
      },
      {
        "date": "2022-09-05T04:31:00.000Z",
        "voteCount": 26,
        "content": "Failover initiated by Microsoft.\nCustomer-managed account failover is not yet supported in accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2). To learn more, see Blob storage features available in Azure Data Lake Storage Gen2."
      },
      {
        "date": "2022-09-10T10:55:00.000Z",
        "voteCount": 22,
        "content": "RA-GZRS\nFailover initiated by Microsoft."
      },
      {
        "date": "2023-10-26T09:00:00.000Z",
        "voteCount": 1,
        "content": "\"Until the Microsoft-managed failover has completed, you won't have write access to your storage account.\" Hence, I guess it should NOT be Microsoft-managed failover.\n\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json"
      },
      {
        "date": "2024-10-01T01:35:00.000Z",
        "voteCount": 1,
        "content": "Region wasn't mention in the question, so how did you guys come about it? \nAnswer: ZRS and Failover initiated by Microsoft."
      },
      {
        "date": "2024-06-30T08:27:00.000Z",
        "voteCount": 2,
        "content": "Failover Process:\n\nFailover initiated by Microsoft: In this scenario, Microsoft would initiate the failover process, which might not meet the requirement for maintaining availability for writes during a failure.\n\nCORRECT: Failover manually initiated by the customer: Allows the customer to control when the failover happens, ensuring that writes can be directed to the secondary region during a primary region failure.\n\nFailover automatically initiated by an Azure Automation job: This could provide automation for failover, but it may not give the same level of control as manual initiation by the customer."
      },
      {
        "date": "2024-04-30T06:00:00.000Z",
        "voteCount": 1,
        "content": "Zone redundant, microsoft initiated"
      },
      {
        "date": "2024-04-20T08:44:00.000Z",
        "voteCount": 1,
        "content": "Should be:\n- ZRS because the question talk about on a failure on the primary region for Azure Data Lake Storage Gen2 Workloads, so a Zone.\n- Failover initiated by Microsoft."
      },
      {
        "date": "2024-04-01T01:00:00.000Z",
        "voteCount": 1,
        "content": "Type\tFailover Scope\tUse case\tExpected data loss\tHNS supported\nCustomer-managed\tStorage account\tThe storage service endpoints for the primary region become unavailable, but the secondary region is available.\n\nYou received an Azure Advisory in which Microsoft advises you to perform a failover operation of storage accounts potentially affected by an outage.\tYes\tYes (In preview)\nMicrosoft-managed\tEntire region or scale unit\tThe primary region becomes completely unavailable due to a significant disaster, but the secondary region is available.\tYes\tYes"
      },
      {
        "date": "2024-02-22T00:59:00.000Z",
        "voteCount": 1,
        "content": "Your disaster recovery plan should be based on customer-managed failover. Do not rely on Microsoft-managed failover, which might only be used in extreme circumstances. A Microsoft-managed failover would be initiated for an entire physical unit, such as a region or scale unit. It can't be initiated for individual storage accounts, subscriptions, or tenants. For the ability to selectively failover your individual storage accounts, use customer-managed account failover.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance#microsoft-managed-failover"
      },
      {
        "date": "2024-02-21T19:49:00.000Z",
        "voteCount": 3,
        "content": "\"Provides the highest degree of data resiliency\" and it didn't meantioned about the cost.\nI think GZRS will be the one to go for and it is microsoft initiated failover"
      },
      {
        "date": "2024-01-30T18:55:00.000Z",
        "voteCount": 5,
        "content": "RA-GZRS\nFailover manually initiated by the customer"
      },
      {
        "date": "2024-01-12T05:09:00.000Z",
        "voteCount": 1,
        "content": "Why is the ET Azure community so divided for most of the Multiple Correct Questions? This was not the case for GCP ET community."
      },
      {
        "date": "2024-01-19T00:50:00.000Z",
        "voteCount": 2,
        "content": "Because the questions in Azure exams are written like crap. The wording in GCP questions is much more clearer"
      },
      {
        "date": "2024-01-10T14:20:00.000Z",
        "voteCount": 1,
        "content": "Failover initiated by Microsoft:\nIn general, automatic failover initiated by Microsoft is the best option for most organizations, as it is the most reliable and efficient. However, if you need more control over the failover process, then you can choose to use automatic failover initiated by an Azure Automation job.\n\nRA-GZRS\nAvailable using RA-GZRS. RA-GZRS replicates data asynchronously to a secondary data center in a different Azure region. This means that there is a longer latency for replication, but it also means that your data is protected from regional outages. Additionally, RA-GZRS supports data compression and encryption, which can further improve storage efficiency."
      },
      {
        "date": "2023-12-22T02:16:00.000Z",
        "voteCount": 1,
        "content": "accroding to microsoft Documentation: \nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance\n\nit mentioned\n1-Customer-managed -- scope--&gt;\tStorage account-- Case--&gt;\tThe storage service endpoints for the primary region become unavailable, but the secondary region is available.\n\nYou received an Azure Advisory in which Microsoft advises you to perform a failover operation of storage accounts potentially affected by an outage.\n\n2-Microsoft-managed-- Scope--&gt;\tEntire region or scale unit -- Case--&gt;\tThe primary region becomes completely unavailable due to a significant disaster, but the secondary region is available.\n\nin the question it talks about Storage i.e. Data Center not a region failure. so the anwer is  Customer managed"
      },
      {
        "date": "2023-11-23T13:16:00.000Z",
        "voteCount": 2,
        "content": "anser is correct.\nRA-GZRS (Read Access Geo Zone Redundancy) provides read AND WRITE and has the highest availability (16 9's).\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy#geo-zone-redundant-storage\nFor failover: iniciated by customer. In Microsofr case they MAY failover only if greater disaster happens.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-disaster-recovery-guidance?toc=%2Fazure%2Fstorage%2Fblobs%2Ftoc.json&amp;bc=%2Fazure%2Fstorage%2Fblobs%2Fbreadcrumb%2Ftoc.json#microsoft-managed-failover"
      },
      {
        "date": "2023-10-23T06:30:00.000Z",
        "voteCount": 2,
        "content": "GZRS Because it mentioned that when a data center failed, not the entire region. When a data centre failed, with ZRS, we can still read and write to the other two datacentres in the same region. So for the primary region, it must be ZRS. We still need a backup for the secondary region because it reqiures the highest degree of data resiliency. \nCustomer Managed Failover, No Doubt!!!"
      },
      {
        "date": "2023-10-08T06:50:00.000Z",
        "voteCount": 1,
        "content": "Careful as this question will change pretty soon:\n\"Customer-managed account failover for accounts that have a hierarchical namespace (Azure Data Lake Storage Gen2) is currently in PREVIEW and only supported in specific regions.\"\nTherefore it might soon be an option for ADLS2 as well."
      },
      {
        "date": "2023-09-14T22:43:00.000Z",
        "voteCount": 1,
        "content": "Since it should be available for Writing in the secondary region, ZRS should be the one that we should opt for, since others only provide read access."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79019-exam-dp-203-topic-1-question-59-discussion/",
    "body": "You need to implement a Type 3 slowly changing dimension (SCD) for product category data in an Azure Synapse Analytics dedicated SQL pool.<br>You have a table that was created by using the following Transact-SQL statement.<br><img src=\"/assets/media/exam-media/04259/0012300001.png\" class=\"in-exam-image\"><br>Which two columns should you add to the table? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>A.<br><img src=\"/assets/media/exam-media/04259/0012300002.png\" class=\"in-exam-image\"><br>B.<br><img src=\"/assets/media/exam-media/04259/0012300003.png\" class=\"in-exam-image\"><br>C.<br><img src=\"/assets/media/exam-media/04259/0012300004.png\" class=\"in-exam-image\"><br>D.<br><img src=\"/assets/media/exam-media/04259/0012300005.png\" class=\"in-exam-image\"><br>E.<br><img src=\"/assets/media/exam-media/04259/0012300006.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "BE",
    "answerDescription": "A Type 3 SCD supports storing two versions of a dimension member as separate columns. The table includes a column for the current value of a member plus either the original or previous value of the member. So Type 3 uses additional columns to track one key instance of history, rather than storing additional rows to track each change like in a Type 2 SCD.<br>This type of tracking may be used for one or two columns in a dimension table. It is not common to use it for many members of the same table. It is often used in combination with Type 1 or Type 2 members.<br><img src=\"/assets/media/exam-media/04259/0012400001.jpg\" class=\"in-exam-image\"><br>Reference:<br>https://k21academy.com/microsoft-azure/azure-data-engineer-dp203-q-a-day-2-live-session-review/",
    "votes": [],
    "comments": [
      {
        "date": "2022-12-04T09:22:00.000Z",
        "voteCount": 20,
        "content": "B &amp; E is correct answer"
      },
      {
        "date": "2022-11-12T10:51:00.000Z",
        "voteCount": 11,
        "content": "If BE is correct, then CE is also correct."
      },
      {
        "date": "2024-08-03T14:18:00.000Z",
        "voteCount": 1,
        "content": "Both BE and CE can be correct solutions.  However, you should choose the one that is most descriptive to any business user who is consuming this data.  So, describing each column as \"Current\" or \"Original\" is the better design."
      },
      {
        "date": "2024-08-24T12:29:00.000Z",
        "voteCount": 1,
        "content": "I agree with you; however, in the table's design, there's already a column called [RowInsertedDateTime], which implies that it is tracking the insertion of the ProductCategory values. So, it would make no sense for one of the answers to be Option B. Therefore, I believe the correct answers are C and E."
      },
      {
        "date": "2024-05-27T08:05:00.000Z",
        "voteCount": 2,
        "content": "BE is correct but CE is also correct"
      },
      {
        "date": "2024-04-15T00:14:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E is correct answer"
      },
      {
        "date": "2023-09-07T01:09:00.000Z",
        "voteCount": 1,
        "content": "B &amp; E is the correct answer"
      },
      {
        "date": "2023-09-04T02:02:00.000Z",
        "voteCount": 2,
        "content": "B &amp; E is correct answer for SCD 3"
      },
      {
        "date": "2022-09-05T19:45:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct"
      },
      {
        "date": "2022-09-04T05:23:00.000Z",
        "voteCount": 1,
        "content": "Correct but SCD is always type 2. Type 3 is not SCD."
      },
      {
        "date": "2022-10-23T08:49:00.000Z",
        "voteCount": 12,
        "content": "A SCD can be of type 0, 1, 2, 3 and so on, please read the documentation"
      },
      {
        "date": "2022-09-01T04:07:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80428-exam-dp-203-topic-1-question-60-discussion/",
    "body": "DRAG DROP -<br>You have an Azure subscription.<br>You plan to build a data warehouse in an Azure Synapse Analytics dedicated SQL pool named pool1 that will contain staging tables and a dimensional model.<br>Pool1 will contain the following tables.<br><img src=\"/assets/media/exam-media/04259/0012500001.png\" class=\"in-exam-image\"><br>You need to design the table storage for pool1. The solution must meet the following requirements:<br>\u2711 Maximize the performance of data loading operations to Staging.WebSessions.<br>\u2711 Minimize query times for reporting queries against the dimensional model.<br>Which type of table distribution should you use for each table? To answer, drag the appropriate table distribution types to the correct tables. Each table distribution type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>NOTE: Each correct selection is worth one point.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0012600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0012600002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Replicated -<br>The best table storage option for a small table is to replicate it across all the Compute nodes.<br><br>Box 2: Hash -<br>Hash-distribution improves query performance on large fact tables.<br><br>Box 3: Round-robin -<br>Round-robin distribution is useful for improving loading speed.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-05T14:45:00.000Z",
        "voteCount": 36,
        "content": "Replicated (Because its a Dimension table)\nHash (Fact table with High volume of data)\nRound-Robin (Staging table)"
      },
      {
        "date": "2024-04-15T13:51:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-04T02:02:00.000Z",
        "voteCount": 3,
        "content": "Replicated (Because its a Dimension table)\nHash (Fact table with High volume of data)\nRound-Robin (Staging table)"
      },
      {
        "date": "2023-06-25T00:07:00.000Z",
        "voteCount": 2,
        "content": "Shouldn't the fact table has round-robin as well since we need to prioritize data loading, hash will definitely improve the read query performance but will impact the data load speed?"
      },
      {
        "date": "2023-08-18T01:29:00.000Z",
        "voteCount": 1,
        "content": "The requirements stated are:\n- Maximize the performance of data loading operations to Staging.WebSessions.\n- Minimize query times for reporting queries against the dimensional model.\n\nSo only the staging table needs fast data loading."
      },
      {
        "date": "2023-02-03T09:09:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-01-21T16:04:00.000Z",
        "voteCount": 1,
        "content": "The dimension should be a replicated one. so that it will be available in all noes for a better performance \n\nfact table should be a HASH\nstaging table is always Round_ROBIN"
      },
      {
        "date": "2022-12-30T10:38:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-09-30T05:34:00.000Z",
        "voteCount": 4,
        "content": "the giving answer is correct, as the requirements"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80470-exam-dp-203-topic-1-question-61-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics dedicated SQL pool.<br>You need to create a table named FactInternetSales that will be a large fact table in a dimensional model. FactInternetSales will contain 100 million rows and two columns named SalesAmount and OrderQuantity. Queries executed on FactInternetSales will aggregate the values in SalesAmount and OrderQuantity from the last year for a specific product. The solution must minimize the data size and query execution time.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0012800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0012900001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: (CLUSTERED COLUMNSTORE INDEX<br><br>CLUSTERED COLUMNSTORE INDEX -<br>Columnstore indexes are the standard for storing and querying large data warehousing fact tables. This index uses column-based data storage and query processing to achieve gains up to 10 times the query performance in your data warehouse over traditional row-oriented storage. You can also achieve gains up to<br>10 times the data compression over the uncompressed data size. Beginning with SQL Server 2016 (13.x) SP1, columnstore indexes enable operational analytics: the ability to run performant real-time analytics on a transactional workload.<br>Note: Clustered columnstore index<br>A clustered columnstore index is the physical storage for the entire table.<br><img src=\"/assets/media/exam-media/04259/0013000001.jpg\" class=\"in-exam-image\"><br>To reduce fragmentation of the column segments and improve performance, the columnstore index might store some data temporarily into a clustered index called a deltastore and a B-tree list of IDs for deleted rows. The deltastore operations are handled behind the scenes. To return the correct query results, the clustered columnstore index combines query results from both the columnstore and the deltastore.<br>Box 2: HASH([ProductKey])<br>A hash distributed table distributes rows based on the value in the distribution column. A hash distributed table is designed to achieve high performance for queries on large tables.<br>Choose a distribution column with data that distributes evenly<br>Incorrect:<br>* Not HASH([OrderDateKey]). Is not a date column. All data for the same date lands in the same distribution. If several users are all filtering on the same date, then only 1 of the 60 distributions do all the processing work<br>* A replicated table has a full copy of the table available on every Compute node. Queries run fast on replicated tables since joins on replicated tables don't require data movement. Replication requires extra storage, though, and isn't practical for large tables.<br>* A round-robin table distributes table rows evenly across all distributions. The rows are distributed randomly. Loading data into a round-robin table is fast. Keep in mind that queries can require more data movement than the other distribution methods.<br>Reference:<br>https://docs.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-overview https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute",
    "votes": [],
    "comments": [
      {
        "date": "2022-10-30T12:14:00.000Z",
        "voteCount": 50,
        "content": "you don't hash the date.. never.."
      },
      {
        "date": "2024-04-16T06:30:00.000Z",
        "voteCount": 1,
        "content": "Check the question 39, which is quite similar:\n\nSELECT SupplierKey, StockItemKey, COUNT(*)\nFROM FactPurchase\nWHERE DateKey &gt;= 20210101 \n    AND DateKey &lt;= 20210131\nGROUP By SupplierKey, StockItemKey \n\nB. hash-distributed on PurchaseKey\nD. hash-distributed on DateKey\n\nBear in mind the type of the orderDateKey column, it's int, not date, not datetime... Doesn't it make a difference?\n\nIn this case I'd choose the ProductKey since it says queries will be done \"for specific products\", but in the previous case the WHERE clause uses the DateKey field."
      },
      {
        "date": "2024-04-30T06:09:00.000Z",
        "voteCount": 1,
        "content": "Date key is same as date,  it would have distributed by date and that is a no no"
      },
      {
        "date": "2023-10-06T09:02:00.000Z",
        "voteCount": 6,
        "content": "This sentence helped me soooo much  - Hash the date NEVER..  thank you"
      },
      {
        "date": "2023-10-11T06:20:00.000Z",
        "voteCount": 2,
        "content": "If you cant read don't bother commenting"
      },
      {
        "date": "2024-01-25T13:27:00.000Z",
        "voteCount": 2,
        "content": "Usually you don't use the hash date, because if the query is made daily, all record contains the same date, so the same node will do the job. In this example \" from the last year for a specific product\", so we have 365 separate node from date, but we have only one product!!"
      },
      {
        "date": "2023-01-25T10:34:00.000Z",
        "voteCount": 10,
        "content": "By using the product key as the distribution key, the data for a specific product will be stored on the same node, allowing for faster aggregation of the values in SalesAmount and OrderQuantity for that product."
      },
      {
        "date": "2024-04-30T06:12:00.000Z",
        "voteCount": 1,
        "content": "distriubtion hash(product key)\nclustered index (date key)"
      },
      {
        "date": "2024-04-11T20:09:00.000Z",
        "voteCount": 1,
        "content": "The answer is right"
      },
      {
        "date": "2023-09-04T02:08:00.000Z",
        "voteCount": 5,
        "content": "Clustered columnstore\nHash(ProductKey)"
      },
      {
        "date": "2022-09-26T01:18:00.000Z",
        "voteCount": 8,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute#choose-a-distribution-column-with-data-that-distributes-evenly\n\nTo balance the parallel processing, select a distribution column or set of columns that:\n\nHas many unique values. The distribution column(s) can have duplicate values. All rows with the same value are assigned to the same distribution. Since there are 60 distributions, some distributions can have &gt; 1 unique values while others may end with zero values.\nDoes not have NULLs, or has only a few NULLs. For an extreme example, if all values in the distribution column(s) are NULL, all the rows are assigned to the same distribution. As a result, query processing is skewed to one distribution, and does not benefit from parallel processing.\nIs not a date column. All data for the same date lands in the same distribution, or will cluster records by date. If several users are all filtering on the same date (such as today's date), then only 1 of the 60 distributions do all the processing work.\n\nAns: Hash(ProductKey)"
      },
      {
        "date": "2022-09-05T21:17:00.000Z",
        "voteCount": 7,
        "content": "must hash on OrderDateKey because that field was not a date and it was used for filter condition \"from the last year for a specific product\""
      },
      {
        "date": "2022-09-18T13:38:00.000Z",
        "voteCount": 14,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-distribute\nthe example in the sample-test is in this page and they used productkey for hashing, so yeah, the answer is productkey"
      },
      {
        "date": "2022-09-05T19:43:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80469-exam-dp-203-topic-1-question-62-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Table1. Table1 contains the following:<br>\u2711 One billion rows<br>\u2711 A clustered columnstore index<br>\u2711 A hash-distributed column named Product Key<br>\u2711 A column named Sales Date that is of the date data type and cannot be null<br>Thirty million rows will be added to Table1 each month.<br>You need to partition Table1 based on the Sales Date column. The solution must optimize query performance and data loading.<br>How often should you create a partition?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonce per month\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonce per year",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonce per day",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonce per week"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-20T03:21:00.000Z",
        "voteCount": 94,
        "content": "Remembering that we have data splitted in distribution (60 nodes) and considering that we Need a MINMIUM 1 million rows per distribution, we have:\n\nA. once per month = 30 milion / 60 = 500k record per partition\nB. once per year  = 360 milion / 60 = 6 milion record per partition\nC. once per day = about  1 milion / 60 = 16k record per partition\nD. once per week =about 7.5 milion / 60 = 125k  record per partition\n\ncorrect should be B"
      },
      {
        "date": "2023-02-08T11:21:00.000Z",
        "voteCount": 2,
        "content": "actually to be more accurate, I should have written record per distribution. We have 1 milion rows per distribution and 60 milion rows per partition.."
      },
      {
        "date": "2023-01-31T07:59:00.000Z",
        "voteCount": 9,
        "content": "I think you left out the fact that the table already has 1 billion records. This will change your calculations."
      },
      {
        "date": "2023-02-03T01:37:00.000Z",
        "voteCount": 5,
        "content": "You should consider the partition, not the table"
      },
      {
        "date": "2024-05-13T16:52:00.000Z",
        "voteCount": 1,
        "content": "no, we should consider everything including the table in order to come up with the most efficient solution"
      },
      {
        "date": "2023-08-10T19:39:00.000Z",
        "voteCount": 2,
        "content": "agree, the question ask about partition, not the table distribution"
      },
      {
        "date": "2023-08-10T19:38:00.000Z",
        "voteCount": 1,
        "content": "I think you mix up the concept of distribution and partition, we always do partitions with date, distribution with Product Key. I think you over think, no need to think about the calculation"
      },
      {
        "date": "2023-08-10T19:44:00.000Z",
        "voteCount": 1,
        "content": "sorry may be you are right"
      },
      {
        "date": "2023-08-11T04:49:00.000Z",
        "voteCount": 3,
        "content": "I have contributor access which is purchased for $47.99 but no downloadable PDF with questions and explanations received yet."
      },
      {
        "date": "2023-10-05T18:47:00.000Z",
        "voteCount": 1,
        "content": "Contact their Customer Care for answer with your complain and they will respond to you."
      },
      {
        "date": "2023-10-28T05:11:00.000Z",
        "voteCount": 3,
        "content": "Following this logic. Although A is less than 1m requirement, but it is the closest on to 1m. B met the requirement, but is is way too big hence loading to memory is slower than 500k one, already A may result in higher number of partitions, which is larger storage space."
      },
      {
        "date": "2022-09-05T19:43:00.000Z",
        "voteCount": 11,
        "content": "Correct, \nConsidering the high volume of data, for faster queries its recommended to create fewer partitions.\n\" If a table contains fewer than the recommended minimum number of rows per partition(i.e. 60 million rows per month for 60 distributed partitions, consider using fewer partitions in order to increase the number of rows per partition.\""
      },
      {
        "date": "2024-08-24T12:47:00.000Z",
        "voteCount": 1,
        "content": "The table currently has 1 billion rows. With 60 partitions, this means there are approximately 16,666,666 rows per partition.\n\nEach month, an additional 30 million rows will be added. After the first month, the table will have 1.03 billion rows, resulting in approximately 17,166,666 rows per partition.\n\nI recommend going with Option A: partitioning once per month."
      },
      {
        "date": "2024-05-13T16:50:00.000Z",
        "voteCount": 1,
        "content": "We have to consider all information given to us about Table1, including existing data, in order to optimize query performance and data loading for the specific table.\nSo, a year after Table1 will contain 1360M i.e. around 22.6M per distribution having specified one partition per year.\nInstead, if we specify one partition by month, then we will have around 1.8M per distribution and partition.\nTypical analytical queries involve filter predicates for a specific month.\nIsn't it more efficient to process 1.8M instead of 22.6M ? (optimize query performance).\nLoading data to a new empty or smaller partition isn't it faster than loading it to one with pre-existing data or larger partition (optimize data loading)?\nThe above leads us to \"A. once per month\"."
      },
      {
        "date": "2024-04-15T14:04:00.000Z",
        "voteCount": 1,
        "content": "It's B for that reason:\n12month * 30 mln / 60 nodes = 6 mln record per partition"
      },
      {
        "date": "2024-05-13T16:54:00.000Z",
        "voteCount": 1,
        "content": "you are totally ignoring existing table data"
      },
      {
        "date": "2024-04-15T00:21:00.000Z",
        "voteCount": 2,
        "content": "B) once per year, because your partition should contain more than 60 mln rows (1 mln for each of 60 nodes)"
      },
      {
        "date": "2024-04-11T20:11:00.000Z",
        "voteCount": 2,
        "content": "B. once per year = 360 milion / 60 = 6 milion record per partition"
      },
      {
        "date": "2024-03-17T23:32:00.000Z",
        "voteCount": 2,
        "content": "It is about partition, each distribution has several partitions."
      },
      {
        "date": "2024-03-02T15:18:00.000Z",
        "voteCount": 3,
        "content": "A partition is divided into distribution units.\n\nA) eleven per month =&gt; 30 million / 60 = 500k rows per partition\nB) eleven per year =&gt; 360 million / 60 = 6 million rows per partition\nC) eleven per day =&gt; 1 million / 60 = 15k rows per partition\nD) eleven per week =&gt; 7.5~ million/60 = 125k rows per partition\n\nBut since I already have 1 billion rows distributed, each distribution node would have 16.7 million rows, fulfilling more than the minimum, since I am interested in having a certain reasonable number of partitions to increase the speed of the queries, I would choose once per month, because once per year would create very few partitions.\n\nTherefore the answer is A)"
      },
      {
        "date": "2024-08-09T01:10:00.000Z",
        "voteCount": 1,
        "content": "you'll have new partitions whenever you update the table's partitions. if you create a new partition every month, this new partition will have only 500k rows per distribution. I agree that the previous partitions have more than enough records, but the problem is with the new partition."
      },
      {
        "date": "2024-02-08T16:33:00.000Z",
        "voteCount": 2,
        "content": "Given that you\u2019ll be adding 30 million rows per month, consider partitioning once per month based on the Sales Date column.\nThis approach balances data maintenance and query performance while avoiding excessive partitioning.\nTherefore, the correct answer is A. once per month"
      },
      {
        "date": "2024-02-08T16:31:00.000Z",
        "voteCount": 1,
        "content": "To optimize query performance and data loading for your Azure Synapse Analytics dedicated SQL pool table, you should create a partition based on the Sales Date column. \nPartitioning Frequency:\nThe decision on how often to create a partition depends on the data volume and your specific use case. Minimum Rows per Partition: For optimal compression and performance of clustered columnstore tables, it\u2019s recommended to have a minimum of 1 million rows per distribution and partition.\nAutomatic Distribution: Dedicated SQL pools already divide each table into 60 distributed databases.\nRecommendation:\nGiven that you\u2019ll be adding 30 million rows per month, consider partitioning once per month based on the Sales Date column.\nThis approach balances data maintenance and query performance while avoiding excessive partitioning.\nTherefore, the correct answer is A. once per month"
      },
      {
        "date": "2024-01-31T06:27:00.000Z",
        "voteCount": 1,
        "content": "I chose  D\nReason:\nFor optimal performance and compression the followings are true;\nThe minimum record to be load is 60M permonth.\nThis implies that for ;\n*Per week = 60/4= 15M records\n*Pe rday = 15/7 =2.1M records\n*Per year= 12 *60M = 740M\nand so on for hours ,minutes ,sec. in the right proportions.\nConsidering ,the case at hand.\nAdding 30M rows monthly will have negative impact on the performance and compression of the table .The reason is that the quantity of records to load is less than the required minimum value of 60M for monthly partition.\n\nIn other to correct this ,30M minimum  rows show be added every 2 weeks(not permonth) or  10M  minimum rows per week.\nSo ,the most reasonable advice is  to load the table perweek for optimal performance and compression."
      },
      {
        "date": "2024-05-13T17:00:00.000Z",
        "voteCount": 1,
        "content": "Your calculation logic is incorrect.\nA year after Table1 will contain 1360M i.e. around 22.6M per distribution having specified one partition per year. Instead, if we specify one partition by week, then we will have around 0.4M per distribution and partition, which is below than 1M Microsoft recommendation.\nThis makes your choice wrong."
      },
      {
        "date": "2024-01-26T20:39:00.000Z",
        "voteCount": 1,
        "content": "To partition Table1 based on the Sales Date column, you can create a partition for each month. Since thirty million rows will be added to Table1 each month, creating a partition once per month would be the most optimal solution for query performance and data loading 1. This way, the data can be easily queried and loaded, and the partitioning will not be too granular or too coarse 1.\n\nTherefore, the answer is A. once per month."
      },
      {
        "date": "2024-01-19T09:16:00.000Z",
        "voteCount": 1,
        "content": "The table already has 1 billion records (ie 100 Million) + an additional 30 Million per month. So going by the theory that a minimum of 1 million rows should be accommodated by each of the 60 distributions created in the background each distribution will end up with 130 Million / 60 = 2.6 millions. So A. Once per month is the right answer"
      },
      {
        "date": "2024-01-04T17:43:00.000Z",
        "voteCount": 1,
        "content": "Did we ever come to a consensus on this?  My thought would be once/month. You want to keep your records under 1 million, so the 60 million doesn't even come into it."
      },
      {
        "date": "2023-12-11T04:12:00.000Z",
        "voteCount": 1,
        "content": "Once per month is correct"
      },
      {
        "date": "2023-12-06T13:08:00.000Z",
        "voteCount": 3,
        "content": "Chatgpt say A\nConsidering the frequency of data loads and the typical access patterns, the best practice according to the Azure Synapse Analytics documentation would be:\n\nA. **once per month**\n\nThis aligns with the monthly data load pattern and would facilitate efficient data management and query optimization."
      },
      {
        "date": "2024-09-04T07:08:00.000Z",
        "voteCount": 1,
        "content": "man, don't trust fully in chat gpt, always rely on microsoft docs. for me, the bot said the best option would be D."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80467-exam-dp-203-topic-1-question-63-discussion/",
    "body": "You have an Azure Databricks workspace that contains a Delta Lake dimension table named Table1.<br>Table1 is a Type 2 slowly changing dimension (SCD) table.<br>You need to apply updates from a source table to Table1.<br>Which Apache Spark SQL operation should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCREATE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUPDATE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMERGE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-01-11T06:24:00.000Z",
        "voteCount": 11,
        "content": "When applying updates to a Type 2 slowly changing dimension (SCD) table in Azure Databricks, the best option is to use the MERGE operation in Apache Spark SQL. This operation allows you to combine the data from the source table with the data in the destination table, and then update or insert the appropriate r\necords. The MERGE operation provides a powerful and flexible way to handle updates for SCD tables, as it can handle both updates and inserts in a single operation. Additionally, this operation can be performed on Delta Lake tables, which can easily handle the ACID transactions needed for handling SCD updates."
      },
      {
        "date": "2022-09-16T10:40:00.000Z",
        "voteCount": 9,
        "content": "correct D"
      },
      {
        "date": "2024-04-11T20:13:00.000Z",
        "voteCount": 1,
        "content": "Type 2 SCD means you have a new row to insert for the changes. The only MERGE supports this operation among the possible answers"
      },
      {
        "date": "2024-02-08T16:36:00.000Z",
        "voteCount": 3,
        "content": "MERGE Operation:\nThe MERGE operation in Apache Spark SQL (and specifically in Delta Lake) combines the capabilities of both INSERT and UPDATE. It allows you to upsert data (insert new records or update existing records) based on a specified condition. When you have a slowly changing dimension (SCD) table like Table1, where historical data needs to be maintained. \nTherefore, the correct answer is D. MERGE"
      },
      {
        "date": "2023-09-04T02:10:00.000Z",
        "voteCount": 4,
        "content": "To update or upsert records in a delta lake in Databricks use the \"Merge\" command."
      },
      {
        "date": "2023-05-24T16:53:00.000Z",
        "voteCount": 4,
        "content": "To update or upsert records in a delta lake in Databricks use the \"Merge\" command.\nhttps://learn.microsoft.com/en-us/azure/databricks/delta/merge"
      },
      {
        "date": "2022-09-05T19:40:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80471-exam-dp-203-topic-1-question-64-discussion/",
    "body": "You are designing an Azure Data Lake Storage solution that will transform raw JSON files for use in an analytical workload.<br>You need to recommend a format for the transformed files. The solution must meet the following requirements:<br>\u2711 Contain information about the data types of each column in the files.<br>\u2711 Support querying a subset of columns in the files.<br>\u2711 Support read-heavy analytical workloads.<br>\u2711 Minimize the file size.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Avro",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Parquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-18T13:57:00.000Z",
        "voteCount": 10,
        "content": "Parquet has columnar format, best for reading a few columns. Also when in doubt, just select Parquet. More often than not, that is gonna be the correct answer if you don't know it."
      },
      {
        "date": "2022-09-05T19:44:00.000Z",
        "voteCount": 7,
        "content": "Correct as the requirement is Column based."
      },
      {
        "date": "2023-09-04T02:10:00.000Z",
        "voteCount": 2,
        "content": "D is correct"
      },
      {
        "date": "2023-05-13T20:53:00.000Z",
        "voteCount": 2,
        "content": "Parquet format satisfies all reqs"
      },
      {
        "date": "2023-01-30T10:46:00.000Z",
        "voteCount": 3,
        "content": "Parquet"
      },
      {
        "date": "2022-12-12T15:03:00.000Z",
        "voteCount": 4,
        "content": "Avro is totally a serialization format. It combines of JSON and Raw binary files. Why is the explanation like this? It's misleading."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79463-exam-dp-203-topic-1-question-65-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br>You have an Azure Storage account that contains 100 GB of files. The files contain rows of text and numerical values. 75% of the rows contain description data that has an average length of 1.1 MB.<br>You plan to copy the data from the storage account to an enterprise data warehouse in Azure Synapse Analytics.<br>You need to prepare the files to ensure that the data copies quickly.<br>Solution: You modify the files to ensure that each row is less than 1 MB.<br>Does this meet the goal?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T09:24:00.000Z",
        "voteCount": 32,
        "content": "I think we had this question in the previous pages and the correct answer was set as \" compress the files\""
      },
      {
        "date": "2022-09-04T06:14:00.000Z",
        "voteCount": 4,
        "content": "Exactly\ncompress because a lot of row have more than 1MB length"
      },
      {
        "date": "2023-05-08T05:16:00.000Z",
        "voteCount": 5,
        "content": "The question before was more than that 1 MB but here is less than 1 MB. since, it is less, then answer is Yes."
      },
      {
        "date": "2023-07-27T05:29:00.000Z",
        "voteCount": 3,
        "content": "More than 1 solution might be right. The question here is: if row size is reduced to 1MB, will loading go faster? The answer then is yes: whether compression is better or not, is not relevant."
      },
      {
        "date": "2022-09-05T21:53:00.000Z",
        "voteCount": 16,
        "content": "\"ensure that each row is less than 1 MB\" and the condition for polybase is &lt;1M, whatever method you used"
      },
      {
        "date": "2024-04-11T20:26:00.000Z",
        "voteCount": 4,
        "content": "If you modify the files to ensure that each row is less than 1 MB, you might end up truncating or losing data from those rows.\nTo achieve faster data copying, consider alternative approaches such as:\nCompression: Compress the files before transferring them to Azure Synapse Analytics. This can reduce the overall size of the data and improve transfer speed.\nParallelization: Split the data into smaller chunks and copy them in parallel to take advantage of multiple resources.\nOptimized Data Types: Ensure that numerical values are stored using appropriate data types (e.g., integers, floats) to minimize storage space.\nBatch Processing: Process the data in batches rather than row by row to optimize data transfer.\nThe question should be read carefully and attentively.\nYou need to prepare the files to ensure that the data copies QUICKLY.\nNobody asks if this improve...\nI agree with rocky48. You need do more to copy the data QUICKLY. \nSo, the answer is B (NO)"
      },
      {
        "date": "2024-06-27T07:46:00.000Z",
        "voteCount": 2,
        "content": "I am with this answer. It should never be a good practise modifying the data at loading time."
      },
      {
        "date": "2024-02-15T23:54:00.000Z",
        "voteCount": 1,
        "content": "Given the large size of the table, I will utilize PolyBase for data transfer. Additionally, considering PolyBase's constraint that it cannot load rows exceeding 1MB in size, I will compress rows to ensure compliance with this requirement, thereby making PolyBase the optimal choice for data transfer. =&gt; Option A: yes"
      },
      {
        "date": "2024-02-09T04:08:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't this be \"B: No\". Where does the question ask about Polybase? Modifying the rows of data could affect the integrity of the data"
      },
      {
        "date": "2024-02-08T16:42:00.000Z",
        "voteCount": 3,
        "content": "No, this solution does not meet the goal. While modifying the files to ensure that each row is less than 1 MB might help with individual row sizes, it won\u2019t necessarily improve the overall data transfer speed. The total size of the files in the storage account is still 100 GB, and copying large volumes of data can be time-consuming regardless of individual row sizes. To optimize data transfer speed, consider other strategies such as parallelizing the data transfer, optimizing network bandwidth, or using appropriate data loading techniques in Azure Synapse Analytics."
      },
      {
        "date": "2024-02-08T16:41:00.000Z",
        "voteCount": 2,
        "content": "No, this solution does not meet the goal. While modifying the files to ensure that each row is less than 1 MB might help with individual row sizes, it won\u2019t necessarily improve the overall data transfer speed. The total size of the files in the storage account is still 100 GB, and copying large volumes of data can be time-consuming regardless of individual row sizes. To optimize data transfer speed, consider other strategies such as parallelizing the data transfer, optimizing network bandwidth, or using appropriate data loading techniques in Azure Synapse Analytics."
      },
      {
        "date": "2024-01-27T21:57:00.000Z",
        "voteCount": 3,
        "content": "No, this solution does not meet the goal. The files contain rows of text and numerical values, and 75% of the rows contain description data that has an average length of 1.1 MB. If you modify the files to ensure that each row is less than 1 MB, you may end up splitting the description data into multiple rows, which could affect the integrity of the data"
      },
      {
        "date": "2024-01-04T17:38:00.000Z",
        "voteCount": 11,
        "content": "**Variations**\nSolution: You convert the files to compressed delimited text files.\nDoes this meet the goal? **YES**\nSolution: You copy the files to a table that has a columnstore index.\nDoes this meet the goal? **NO**\nSolution: You modify the files to ensure that each row is more than 1 MB.\nDoes this meet the goal? **NO**\nSolution: You modify the files to ensure that each row is less than 1 MB.\nDoes this meet the goal? **YES**"
      },
      {
        "date": "2023-11-19T22:21:00.000Z",
        "voteCount": 1,
        "content": "PolyBase enables Azure Synapse Analytics to import and export data from Azure Data Lake Store, and from Azure Blob Storage. And it supports row sizes up to 1MB.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-guide?view=sql-server-ver16#:~:text=Azure%20integration,and%20from%20Azure%20Blob%20Storage.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/polybase/polybase-versioned-feature-summary?view=sql-server-ver16"
      },
      {
        "date": "2023-09-04T02:16:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-06-28T00:03:00.000Z",
        "voteCount": 2,
        "content": "Yes, with less of 1Mb file we increase performance."
      },
      {
        "date": "2023-06-25T21:53:00.000Z",
        "voteCount": 2,
        "content": "i thought that polybase just query the tables and dont do any process of ETL or ELT."
      },
      {
        "date": "2023-05-11T21:27:00.000Z",
        "voteCount": 4,
        "content": "\"You modify the files to ensure that each row is more than 1 MB\" and the answer was \"No\". This particular question asks if \"You modify the files to ensure that each row is less than 1 MB\", and the answer given is \"Yes\"."
      },
      {
        "date": "2023-05-11T21:27:00.000Z",
        "voteCount": 1,
        "content": "\"You modify the files to ensure that each row is more than 1 MB\" and the answer was \"No\". This particular question asks if \"You modify the files to ensure that each row is less than 1 MB\", and the answer given is \"Yes\"."
      },
      {
        "date": "2023-03-06T21:38:00.000Z",
        "voteCount": 2,
        "content": "No, modifying the files to ensure that each row is less than 1 MB does not necessarily meet the goal of ensuring that the data copies quickly.\n\nWhile it is true that large row sizes can impact data copy performance, simply reducing the row size to less than 1 MB may not be enough to optimize the data copy process. The performance of the data copy process can also be affected by factors such as network bandwidth, database design, and the method used to copy the data.\n\nTo ensure that the data copies quickly, you could consider other techniques such as compressing the data, using parallel data copy processes, and optimizing the database schema for efficient data loading.\n\nTherefore, the correct answer is B. No."
      },
      {
        "date": "2023-01-30T21:30:00.000Z",
        "voteCount": 2,
        "content": "B. No\n\nModifying the files to ensure that each row is less than 1 MB may not be enough to ensure that the data copies quickly to Azure Synapse Analytics. Other factors such as network bandwidth, data compression, and parallel processing of data can also impact the speed of data transfer. To optimize data transfer, it may be necessary to implement data compression techniques, increase network bandwidth, or parallelize the data transfer process."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80472-exam-dp-203-topic-1-question-66-discussion/",
    "body": "You plan to create a dimension table in Azure Synapse Analytics that will be less than 1 GB.<br>You need to create the table to meet the following requirements:<br>\u2711 Provide the fastest query time.<br>\u2711 Minimize data movement during queries.<br>Which type of table should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\treplicated\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\thash distributed",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\theap",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tround-robin"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T19:51:00.000Z",
        "voteCount": 10,
        "content": "Given answer is correct !\nReplicated because \n-  Dimension table \n- Less than 2 GB ( less than 1 GB in this case)"
      },
      {
        "date": "2022-09-30T05:57:00.000Z",
        "voteCount": 5,
        "content": "correct answer"
      },
      {
        "date": "2024-09-29T05:27:00.000Z",
        "voteCount": 1,
        "content": "The question is: Which type of table should you use?\nActually, it should be Which type of distribution should you use?\n\nTable types = Clustered Columnstore, heap, clustered index\nDistribution types = hash, round robin and replicated"
      },
      {
        "date": "2024-02-08T16:46:00.000Z",
        "voteCount": 2,
        "content": "Replicated Table: Caches a full copy of the table on each compute node. While it eliminates data movement during joins or aggregations, it requires extra storage and is not suitable for large tables."
      },
      {
        "date": "2023-11-19T22:23:00.000Z",
        "voteCount": 2,
        "content": "Replicated"
      },
      {
        "date": "2023-09-04T02:17:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-08-14T18:05:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2023-08-07T01:48:00.000Z",
        "voteCount": 1,
        "content": "replicated"
      },
      {
        "date": "2023-05-11T04:57:00.000Z",
        "voteCount": 3,
        "content": "A is correct answer"
      },
      {
        "date": "2023-01-30T10:55:00.000Z",
        "voteCount": 3,
        "content": "answer is correct for Dimension tables and less than 2 GB microsoft recommends replicated tables"
      },
      {
        "date": "2022-12-17T07:14:00.000Z",
        "voteCount": 3,
        "content": "Since the dim table is under 1 GB size which is quite small, it should be replicated across all the partitions so that data movement is less and efficiency is more."
      },
      {
        "date": "2022-12-02T18:48:00.000Z",
        "voteCount": 3,
        "content": "correct answer"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80473-exam-dp-203-topic-1-question-67-discussion/",
    "body": "You are designing a dimension table in an Azure Synapse Analytics dedicated SQL pool.<br>You need to create a surrogate key for the table. The solution must provide the fastest query performance.<br>What should you use for the surrogate key?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta GUID column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta sequence object",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan IDENTITY column\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T19:52:00.000Z",
        "voteCount": 10,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-01-13T22:30:00.000Z",
        "voteCount": 6,
        "content": "C. an IDENTITY column\n\nWhen designing a dimension table in a data warehouse, it's important to consider the types of queries that will be run against it. IDENTITY columns are generally the best option for surrogate keys in dimension tables because they provide the fastest query performance. IDENTITY columns are auto-incremented and indexed by default, which makes them ideal for use as primary keys. They also require less storage space than GUID columns and are less likely to cause fragmentation in indexes."
      },
      {
        "date": "2024-02-08T16:49:00.000Z",
        "voteCount": 2,
        "content": "Remember that the IDENTITY value doesn\u2019t guarantee order due to the distributed architecture, but it ensures uniqueness within each distribution.The IDENTITY property generates unique values automatically.\nIt scales out across all distributions in the dedicated SQL pool.\nIt doesn\u2019t impact load performance.\nEach distribution maintains its own set of IDENTITY values.\nIdeal for creating surrogate keys without manual intervention."
      },
      {
        "date": "2023-09-04T02:18:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2023-08-14T18:05:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-08-07T01:49:00.000Z",
        "voteCount": 3,
        "content": "Identity column"
      },
      {
        "date": "2022-12-17T07:15:00.000Z",
        "voteCount": 2,
        "content": "Surrogate key is substitute key used if there is no natural or business key within the table. It should be always instantiated by IDENTITY(1,1)."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95005-exam-dp-203-topic-1-question-68-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Lake Storage Gen2 account that contains a container named container1. You have an Azure Synapse Analytics serverless SQL pool that contains a native external table named dbo.Table1. The source data for dbo.Table1 is stored in container1. The folder structure of container1 is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image262.png\"><br><br>The external data source is defined by using the following statement.<br><br><img src=\"https://img.examtopics.com/dp-203/image263.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image264.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image265.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-02-17T06:25:00.000Z",
        "voteCount": 79,
        "content": "I have just tested this on Synapse Serverless: ./Folder3 AND _mydata4.csv were ignored. Therefor; Yes, No, No"
      },
      {
        "date": "2024-04-16T06:54:00.000Z",
        "voteCount": 2,
        "content": "I think it should be: \n1 - NO: It would be needed to specify the \"/**\" option to traverse folders: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-folders-multiple-csv-files \n\n2 - NO: since it's a hidden folder: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver16&amp;tabs=dedicated Not sure if this documentation is well suited...\n\n3 - NO: It starts with \"_\": Same link as question 2"
      },
      {
        "date": "2023-04-07T10:54:00.000Z",
        "voteCount": 5,
        "content": "its not ./Folder3 . it is /.Folder3 still ignored?"
      },
      {
        "date": "2023-06-12T15:53:00.000Z",
        "voteCount": 22,
        "content": "Folder or file that starts with . or _\nReference documentation: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2023-09-05T23:49:00.000Z",
        "voteCount": 1,
        "content": "Thanks for the link"
      },
      {
        "date": "2023-09-23T10:24:00.000Z",
        "voteCount": 1,
        "content": "Its same for AD credz of team members / brass that get das-boot at work. Recent times seeing a whole lotta \" _  \" lol. Trying to exit hence here before catching \" _ \""
      },
      {
        "date": "2023-01-15T03:17:00.000Z",
        "voteCount": 47,
        "content": "1.Yes, 2.Yes:\n\"Unlike Hadoop external tables, native external tables don't return subfolders unless you specify /** at the end of path\" which is the case here.\n3. No:\n\"Both Hadoop and native external tables will skip the files with the names that begin with an underline (_) or a period (.)\", refers to files, not directories, so the last file with the underscore will be exluded.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#arguments-create-external-table"
      },
      {
        "date": "2023-06-09T08:39:00.000Z",
        "voteCount": 3,
        "content": "\u03ad\u03bd\u03b1 like \u03b3\u03b9\u03b1 \u03c4\u03bf nickname ;-)"
      },
      {
        "date": "2023-06-22T11:18:00.000Z",
        "voteCount": 7,
        "content": "@PGiagkoulas, read this link again : https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1\n\nSee this part in the link: \n\nnative external tables don't return subfolders unless you specify /** at the end of path. In this example, if LOCATION='/webdata/', a serverless SQL pool query, will return rows from mydata.txt. It won't return mydata2.txt and mydata3.txt because they're located in a subfolder.\n\nBased on this, the answer is Yes, No and No"
      },
      {
        "date": "2023-07-27T05:56:00.000Z",
        "voteCount": 4,
        "content": "But /** is specified at the end?"
      },
      {
        "date": "2024-01-08T01:35:00.000Z",
        "voteCount": 1,
        "content": "yes thats why it will return the all folders"
      },
      {
        "date": "2024-09-27T12:51:00.000Z",
        "voteCount": 1,
        "content": "yes-no-no -&gt;  https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql"
      },
      {
        "date": "2024-09-27T00:43:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer is correct:\n1) Yes\n2) Yes  \n3) No\n\nLink: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=sql-server-ver16&amp;tabs=dedicated\n\nLink"
      },
      {
        "date": "2024-07-27T01:40:00.000Z",
        "voteCount": 1,
        "content": "All should be No, Since we cannot provide ** in data source , it should be there in external table."
      },
      {
        "date": "2024-04-11T21:26:00.000Z",
        "voteCount": 1,
        "content": "you can use a wildcard character (*) in the path to reference multiple files within a directory. However, the wildcard character should not be used consecutively.\nYou will get an error message trying to select data from the external table:  \nConsecutive wildcard characters present in path '&lt;data_source_location&gt;/container1/*'.\nSo, I suppose, all the answers should be NO"
      },
      {
        "date": "2024-02-21T12:07:00.000Z",
        "voteCount": 7,
        "content": "YES,NO,NO \nThe \".\" and \"-\" are hidden. The native external tables and Hadoop external tables won't return anything from hidden folders. https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;preserve-view=true&amp;tabs=dedicated"
      },
      {
        "date": "2024-02-08T16:52:00.000Z",
        "voteCount": 1,
        "content": "Yes, No, No. /.Folder 3 2ill be ignored because it is a hidden folder and _mydata4.csv will also be ignored because it is a hidden file"
      },
      {
        "date": "2024-01-22T10:39:00.000Z",
        "voteCount": 2,
        "content": "Yes, No, No. /.Folder 3 2ill be ignored because it is a hidden folder and _mydata4.csv will also be ignored because it is a hidden file"
      },
      {
        "date": "2024-01-10T14:58:00.000Z",
        "voteCount": 2,
        "content": "Y - Visible / N - Hidden Folder / N - Hidden File\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/design-guidance-for-replicated-tables"
      },
      {
        "date": "2023-12-11T03:19:00.000Z",
        "voteCount": 2,
        "content": "yes, no, no  &gt;&gt;\nBoth Hadoop and native external tables \nwill skip the files with the names that begin with an \nunderline (_) or a period (.)."
      },
      {
        "date": "2023-12-01T19:12:00.000Z",
        "voteCount": 2,
        "content": "Yes / No / No\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2023-10-08T07:47:00.000Z",
        "voteCount": 5,
        "content": "In my opinion YES NO NO \nfrom this doc: https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2023-09-14T22:51:00.000Z",
        "voteCount": 4,
        "content": "Yes, No, No\nAnything that starts with '_' or '.' will be ignored.\nAnd if you are having a doubt that files are in the form of /.{name_of_folder}, Note that these are folders and '/' is used for representing the folders. Pretty basic thing, but hope it helps someone."
      },
      {
        "date": "2023-09-08T03:31:00.000Z",
        "voteCount": 1,
        "content": "Yes n n"
      },
      {
        "date": "2023-09-07T01:19:00.000Z",
        "voteCount": 1,
        "content": "YES /  NO / NO"
      },
      {
        "date": "2023-09-04T02:18:00.000Z",
        "voteCount": 1,
        "content": "Yes, No, No"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95064-exam-dp-203-topic-1-question-69-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to create a fact table named Table1 that will store sales data from the last three years. The solution must be optimized for the following query operations:<br><br>\u2022\tShow order counts by week.<br>\u2022\tCalculate sales totals by region.<br>\u2022\tCalculate sales totals by product.<br>\u2022\tFind all the orders from a given month.<br><br>Which data should you use to partition Table1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tproduct",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tmonth\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tweek",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tregion"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 24,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-29T01:01:00.000Z",
        "voteCount": 39,
        "content": "\u2022 Show order counts by week.\n\u2022 Calculate sales totals by region.\n\u2022 Calculate sales totals by product.\nFor these,  Group By is required while querying, hence cannot be a parition. But fourth one, requires you to use WHERE clause, so month is ideal for a partition"
      },
      {
        "date": "2023-10-08T07:53:00.000Z",
        "voteCount": 6,
        "content": "Thanks, this was helpful"
      },
      {
        "date": "2023-01-13T22:24:00.000Z",
        "voteCount": 17,
        "content": "When designing a fact table in a data warehouse, it is important to consider the types of queries that will be run against it. In this case, the queries that need to be optimized include: show order counts by week, calculate sales totals by region, calculate sales totals by product, and find all the orders from a given month.\n\nPartitioning the table by month would be the best option in this scenario as it would allow for efficient querying of data by month, which is necessary for the query operations described above. For example, it would be easy to find all the orders from a given month by only searching the partition for that specific month."
      },
      {
        "date": "2024-08-25T01:33:00.000Z",
        "voteCount": 1,
        "content": "I choose B, partitioning by month. This will make finding all orders for a specific month much faster. It also makes it easier to count orders by week. The other queries, like calculating sales totals by region and product, are simpler and don\u2019t need special partitioning."
      },
      {
        "date": "2024-02-21T12:17:00.000Z",
        "voteCount": 1,
        "content": "Partition by week.\nPartition by month sounds correct, but it is not.\nIf partition by week, then get the four weeks data from 4 partitions. If it is a month, retrieve the month partition and get the data for a week. The first one is simpler than the second one."
      },
      {
        "date": "2024-01-18T09:27:00.000Z",
        "voteCount": 3,
        "content": "Got this question on my exam on january 17, answer B is correct"
      },
      {
        "date": "2023-09-04T02:19:00.000Z",
        "voteCount": 1,
        "content": "Should be B month."
      },
      {
        "date": "2023-08-14T18:19:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-07-02T16:25:00.000Z",
        "voteCount": 4,
        "content": "Can someone please link documentation to where this is stated? I feel like any of these categories would be an effective partition strategy as there is a use case for each. I am confused"
      },
      {
        "date": "2023-07-27T06:01:00.000Z",
        "voteCount": 5,
        "content": "Same. Every question refers to a different category, so why is month dominant over the others?"
      },
      {
        "date": "2023-04-20T05:11:00.000Z",
        "voteCount": 2,
        "content": "Chat GPT: Based on the given usage patterns and requirements, the recommended folder structure would be option B: \n\n\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet\n\nThis structure allows for easy filtering of data by year and week, which aligns with the identified usage pattern of most queries filtering by the current year or week. It also organizes the data by data source and subject area, which simplifies folder security. By using a flat structure, with the data files directly under the year-week folder, query times can be minimized as the data is organized for efficient partition pruning.\n\nOption A is similar but includes an additional level of hierarchy for the year, which is unnecessary given the requirement to filter by year-week. Options C, D, and E do not follow a consistent hierarchy, making it difficult to navigate and locate specific data files."
      },
      {
        "date": "2023-04-19T21:29:00.000Z",
        "voteCount": 2,
        "content": "How can we partition by unique months unless we have the year too?"
      },
      {
        "date": "2023-12-11T07:32:00.000Z",
        "voteCount": 1,
        "content": "If the Month column is the format YYYYMM, this won't be a problem."
      },
      {
        "date": "2023-02-02T23:03:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-29T05:23:00.000Z",
        "voteCount": 3,
        "content": "Why B) month and not C) week?"
      },
      {
        "date": "2023-02-02T08:57:00.000Z",
        "voteCount": 4,
        "content": "because it's doing aggregation (like the others answer A and D), instead partitions are powerful for where clause query"
      },
      {
        "date": "2023-02-02T23:03:00.000Z",
        "voteCount": 1,
        "content": "Find all the orders for a given month.\nBecause of the above, monthly partitions are more efficient than weekly partitions.\nIf you have to read all the monthly data anyway, it is better to read one monthly partition than to read four to five weekly partitions."
      },
      {
        "date": "2023-01-21T16:19:00.000Z",
        "voteCount": 1,
        "content": "B should be the correct answer here"
      },
      {
        "date": "2023-01-14T03:37:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-13T10:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/microsoft/view/94993-exam-dp-203-topic-1-question-70-discussion/",
    "body": "You are designing the folder structure for an Azure Data Lake Storage Gen2 account.<br><br>You identify the following usage patterns:<br><br>\u2022\tUsers will query data by using Azure Synapse Analytics serverless SQL pools and Azure Synapse Analytics serverless Apache Spark pools.<br>\u2022\tMost queries will include a filter on the current year or week.<br>\u2022\tData will be secured by data source.<br><br>You need to recommend a folder structure that meets the following requirements:<br><br>\u2022\tSupports the usage patterns<br>\u2022\tSimplifies folder security<br>\u2022\tMinimizes query times<br><br>Which folder structure should you recommend?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataSource\\SubjectArea\\WW\\YYYY\\FileData_YYYY_MM_DD.parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\\YYYY\\WW\\DataSource\\SubjectArea\\FileData_YYYY_MM_DD.parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWW\\YYYY\\SubjectArea\\DataSource\\FileData_YYYY_MM_DD.parquet"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T22:22:00.000Z",
        "voteCount": 16,
        "content": "A. \\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet\n\nThe recommended folder structure that best meets the requirements is option A. It separates data by data source, year and week. It allows for easy filtering of data by year or week, which aligns with the usage pattern where most queries include a filter on the current year or week."
      },
      {
        "date": "2024-01-27T23:08:00.000Z",
        "voteCount": 2,
        "content": "Based on the usage patterns and requirements, I recommend the following folder structure:\n\n\\DataSource\\SubjectArea\\YYYY\\WW\\FileData_YYYY_MM_DD.parquet\n\nThis structure is designed to support the usage patterns by organizing data by data source and subject area. The year and week folders will help optimize queries that filter by current year or week. This structure also simplifies folder security by grouping data by data source and subject area, making it easier to apply security policies. Finally, this structure minimizes query times by organizing data in a way that is optimized for the most common queries."
      },
      {
        "date": "2023-12-18T07:39:00.000Z",
        "voteCount": 1,
        "content": "I believe it would be B. \n\n\"Minimises query time\" and \"Most queries will include a filter on the current year OR week.\"\n\nThe \"OR WEEK\" suggests that we may filter by only week and not the year.\n\nIn this event it would (to my knowledge) take longer to query through every single year to select the week you want as opposed to selecting all the folders containing the WW target value in their name.\n\nIf someone with query optimisation knowledge could confirm this below it would be appreciated."
      },
      {
        "date": "2023-09-04T02:23:00.000Z",
        "voteCount": 1,
        "content": "Minimizes query times &amp;&amp; Most queries will include a filter on the current year OR week.\nIt is B."
      },
      {
        "date": "2023-09-04T02:29:00.000Z",
        "voteCount": 2,
        "content": "forhot it ,Option A can set permissions at the year level or at the week, should be A."
      },
      {
        "date": "2023-08-14T18:26:00.000Z",
        "voteCount": 2,
        "content": "my opinion it should be A, as it can clearly filter on year or on week , both the options will be available."
      },
      {
        "date": "2023-06-28T00:16:00.000Z",
        "voteCount": 1,
        "content": "We need to query by week too, so better YYYY-WWW."
      },
      {
        "date": "2023-06-28T13:22:00.000Z",
        "voteCount": 1,
        "content": "I got another good point in favour of B from the question, look the requirement: \n\"Most queries will include a filter on the current year OR week.\"\n(OR) let's suppose they ask you to give back the fourth week for example, you need to go year by year (folders) instead of have it in 1 page. Definitely for me it's option B."
      },
      {
        "date": "2023-07-27T06:07:00.000Z",
        "voteCount": 2,
        "content": "But why is it YYY-WW for you then instead of YYYY\\WW?"
      },
      {
        "date": "2023-06-19T22:33:00.000Z",
        "voteCount": 2,
        "content": "Option B seems right answer as we can directly access the given yyyy-ww ."
      },
      {
        "date": "2023-04-20T05:14:00.000Z",
        "voteCount": 4,
        "content": "chat GPT: Based on the given usage patterns and requirements, the recommended folder structure would be option B: \n\n\\DataSource\\SubjectArea\\YYYY-WW\\FileData_YYYY_MM_DD.parquet\n\nThis structure allows for easy filtering of data by year and week, which aligns with the identified usage pattern of most queries filtering by the current year or week. It also organizes the data by data source and subject area, which simplifies folder security. By using a flat structure, with the data files directly under the year-week folder, query times can be minimized as the data is organized for efficient partition pruning.\n\nOption A is similar but includes an additional level of hierarchy for the year, which is unnecessary given the requirement to filter by year-week. Options C, D, and E do not follow a consistent hierarchy, making it difficult to navigate and locate specific data files."
      },
      {
        "date": "2023-05-23T07:28:00.000Z",
        "voteCount": 1,
        "content": "No, A it will give you additional separate column for week (WW)."
      },
      {
        "date": "2023-06-28T13:24:00.000Z",
        "voteCount": 1,
        "content": "\"Most queries will include a filter on the current year OR week.\""
      },
      {
        "date": "2023-06-14T07:22:00.000Z",
        "voteCount": 5,
        "content": "I would not trust using Chat GPT for studying a certification..."
      },
      {
        "date": "2023-02-04T02:19:00.000Z",
        "voteCount": 1,
        "content": "A is correct answer"
      },
      {
        "date": "2023-02-02T09:04:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-01-21T16:20:00.000Z",
        "voteCount": 1,
        "content": "The answer A is correct"
      },
      {
        "date": "2023-01-14T06:01:00.000Z",
        "voteCount": 1,
        "content": "Given answer is correct"
      },
      {
        "date": "2023-01-14T03:38:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-01-13T10:14:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct.\nThe reason is that this folder structure allows for the data to be organized by data source and subject area, which can help with securing the data by data source. Additionally, it organizes the data by year and week, which can minimize query times for the queries that include a filter on the current year or week. And also the file name format is consistent with the folder structure, which makes it easy to understand where the data comes from."
      },
      {
        "date": "2023-01-13T02:12:00.000Z",
        "voteCount": 1,
        "content": "My choice is A"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95066-exam-dp-203-topic-1-question-71-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a table named table1.<br><br>You load 5 TB of data into table1.<br><br>You need to ensure that columnstore compression is maximized for table1.<br><br>Which statement should you execute?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDBCC INDEXDEFRAG (pool1, table1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDBCC DBREINDEX (table1)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER INDEX ALL on table1 REORGANIZE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tALTER INDEX ALL on table1 REBUILD\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-13T10:23:00.000Z",
        "voteCount": 29,
        "content": "D. ALTER INDEX ALL on table1 REBUILD\n\nThis statement will rebuild all indexes on table1, which can help to maximize columnstore compression. The other options are not appropriate for this task.\nDBCC INDEXDEFRAG (pool1, table1) is for defragmenting the indexes and DBCC DBREINDEX (table1) is for recreating the indexes. ALTER INDEX ALL on table1 REORGANIZE is for reorganizing the indexes."
      },
      {
        "date": "2023-03-28T13:53:00.000Z",
        "voteCount": 7,
        "content": "Reorganizing an index is less resource intensive than rebuilding an index. For that reason it should be your preferred index maintenance method, unless there is a specific reason to use index rebuild.\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/reorganize-and-rebuild-indexes?view=sql-server-ver16"
      },
      {
        "date": "2023-05-10T05:53:00.000Z",
        "voteCount": 7,
        "content": "As far as I can see, your quoted article does not refer to Azure Synapse Analytics dedicated SQL pool. I think rebuild is the only supported option for dedicated SQL as can be found here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index"
      },
      {
        "date": "2023-05-13T05:19:00.000Z",
        "voteCount": 5,
        "content": "Yes, I agree with you, I haven't noticed that the article does not apply to Synapse Analystics. \nD seems to be only possible answer."
      },
      {
        "date": "2024-03-19T07:07:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-index\n\n// For a table with an ordered clustered columnstore index, ALTER INDEX REORGANIZE does not re-sort the data. To re-sort data, use ALTER INDEX REBUILD //\n\nThe question state \"columnstored\" table, so D"
      },
      {
        "date": "2024-08-25T06:19:00.000Z",
        "voteCount": 1,
        "content": "I saw all the documentations here and found another one that makes pretty clear the correct option is option D: \n\n\"By default, tables are defined as a clustered columnstore index. After a load completes, some of the data rows might not be compressed into the columnstore. There's a variety of reasons why this can happen. To learn more, see manage columnstore indexes.\nTo optimize query performance and columnstore compression after a load, rebuild the table to force the columnstore index to compress all the rows.\"\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-load-from-azure-data-lake-store#optimize-columnstore-compression"
      },
      {
        "date": "2024-08-22T01:51:00.000Z",
        "voteCount": 1,
        "content": "The answer is D. Why not C because ALTER INDEX ALL on table1 REORGANIZE is used for defragmenting the index, but it doesn\u2019t achieve the same level of compression as a full rebuild."
      },
      {
        "date": "2024-01-27T23:11:00.000Z",
        "voteCount": 2,
        "content": "To maximize columnstore compression for table1 in Azure Synapse Analytics dedicated SQL pool, you should execute the following statement:\n\nALTER INDEX ALL on table1 REBUILD\n\nThe REBUILD option is used to rebuild all indexes on the table, which will maximize columnstore compression for table1. This option is recommended when a large amount of data has been added to the table, as in this case where 5 TB of data has been loaded into table1"
      },
      {
        "date": "2023-10-29T04:53:00.000Z",
        "voteCount": 2,
        "content": "what is the different between \"Alter Index Rebuild\" or \"DBCC DBREINDEX\""
      },
      {
        "date": "2023-09-04T02:30:00.000Z",
        "voteCount": 1,
        "content": "D. ALTER INDEX ALL on table1 REBUILD"
      },
      {
        "date": "2023-08-15T11:15:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2023-06-24T15:21:00.000Z",
        "voteCount": 1,
        "content": "ALTER INDEX REORGANIZE is used for rebuilding or reorganizing indexes, but it does not maximize columnstore compression."
      },
      {
        "date": "2023-08-13T09:19:00.000Z",
        "voteCount": 1,
        "content": "I agree. A rebuild can compress the data more efficiently within each combination of distribution and partition: It can open such existing columnstore segments and shuffle data within them (and the deltastore) to maximize compression for the resulting compressed columnstore segments. That is not possible when reorganizing. That process only changes compressed columnstore segments by physically deleting logically deleted rows and \ncombining small columnstore segments into larger ones."
      },
      {
        "date": "2023-05-19T11:43:00.000Z",
        "voteCount": 2,
        "content": "Reorganize is for row store indexes. The question here clearly mentions column store indexes. Correct answer is D"
      },
      {
        "date": "2023-05-08T09:38:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer is C. \nreorganizing only help in optimizing compression and performance."
      },
      {
        "date": "2023-01-31T07:06:00.000Z",
        "voteCount": 1,
        "content": "Why not C?\nWhen reorganizing a columnstore index, the Database Engine compresses each closed row group in delta store into columnstore as a compressed row group. Starting with SQL Server 2016 (13.x) and in Azure SQL Database, the REORGANIZE command performs the following additional defragmentation optimizations online:\n\nPhysically removes rows from a row group when 10% or more of the rows have been logically deleted. For example, if a compressed row group of 1 million rows has 100,000 rows deleted, the Database Engine will remove the deleted rows and recompress the row group with 900,000 rows, reducing storage footprint."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105021-exam-dp-203-topic-1-question-72-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named pool1.<br><br>You plan to implement a star schema in pool and create a new table named DimCustomer by using the following code.<br><br><img src=\"https://img.examtopics.com/dp-203/image270.png\"><br><br>You need to ensure that DimCustomer has the necessary columns to support a Type 2 slowly changing dimension (SCD).<br><br>Which two columns should you add? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[HistoricalSalesPerson] [nvarchar] (256) NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveEndDate] [datetime] NOT NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[PreviousModifiedDate] [datetime] NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[RowID] [bigint] NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveStartDate] [datetime] NOT NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 32,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-07T06:24:00.000Z",
        "voteCount": 16,
        "content": "Surrogate is already there as customerkey column"
      },
      {
        "date": "2023-05-13T10:01:00.000Z",
        "voteCount": 8,
        "content": "that's the business key, not the surrogate key. If a new record is generated, there would be a duplicate key. SK is necessary to identify the record"
      },
      {
        "date": "2023-10-29T12:41:00.000Z",
        "voteCount": 2,
        "content": "No, the 'CustomerKey' is the Surrogate Key. Moreover, a Business Key also already exists in DimCustomer table by the name 'CustomerSourceID'. So, B&amp;E are the correct options."
      },
      {
        "date": "2023-04-24T23:59:00.000Z",
        "voteCount": 11,
        "content": "I think, there is already a column called InsertedDate, therefore E is not necessary. So we just need another column to track the end date, which is B. And RowID should be a surrogate key in this case."
      },
      {
        "date": "2023-10-08T07:26:00.000Z",
        "voteCount": 7,
        "content": "The date of insertion and the expiration date from when to when is something else.  You can insert data now, but either with future validity or with past validity (correcting errors, for example).\nSo options : BE"
      },
      {
        "date": "2024-09-05T06:11:00.000Z",
        "voteCount": 1,
        "content": "wrong answer by Xam topic correct answer with solution is-: [EffectiveStartDate] [datetime] NOT NULL: This column tracks when the current record started being valid, helping to define the validity period of the data.\n\n[EffectiveEndDate] [datetime] NOT NULL: This column marks the end of the record's validity period when a new version of the data is inserted.\n\nThese two columns will allow you to differentiate between active and historical records and handle Type 2 SCD effectively. BE"
      },
      {
        "date": "2024-07-25T12:44:00.000Z",
        "voteCount": 1,
        "content": "We need a surrogate key.\n\nThe Inserteddate is NOT NULL, so will act as EffectiveStartDate. However, ModifiedDate is also NOT NULL, so it cannot work as EffectiveEndDate because the current record must have NULL for its EffectiveEndDate. So, we need EffectiveEndDate.\n\nAnswer: D,E."
      },
      {
        "date": "2024-07-13T03:12:00.000Z",
        "voteCount": 1,
        "content": "the right answer depends on a few topics not clear:\n- CustumerKey may be a surrogate key?\n- InsertedDate may be a the \"EffectiveStartDate\"\n- modifiedDate may be the \"EffectiveEndDate\""
      },
      {
        "date": "2024-05-28T06:25:00.000Z",
        "voteCount": 1,
        "content": "The subrogatekey is already"
      },
      {
        "date": "2024-04-04T01:37:00.000Z",
        "voteCount": 5,
        "content": "B --&gt; end date is missing, and required by SCD2\nD --&gt; (not E), RowID is required since in SCD2 you're adding the same CustomerID twice, even with a different end date. So, you need a way to uniquely identify a row in the table, that's going no longer to be the customer identifier in general."
      },
      {
        "date": "2024-03-29T23:22:00.000Z",
        "voteCount": 2,
        "content": "It should be BD. If CustomerKey was surrogate key then IDENTITY should have been mentioned in Column definition."
      },
      {
        "date": "2024-01-04T18:00:00.000Z",
        "voteCount": 2,
        "content": "There is already a hash key that serves as the surrogate, if I'm not mistaken. Inserted and modified are probably dates from the source data, not from the work being done here, so you need to add the start/end dates."
      },
      {
        "date": "2023-10-08T07:29:00.000Z",
        "voteCount": 4,
        "content": "The date of insertion and the expiration date from when to when is something else. You can insert data now, but either with future validity or with past validity (correcting errors, for example).\nSo options : BE"
      },
      {
        "date": "2023-09-07T01:22:00.000Z",
        "voteCount": 1,
        "content": "B and E"
      },
      {
        "date": "2023-09-06T00:04:00.000Z",
        "voteCount": 2,
        "content": "B and D we need a unique row identifier"
      },
      {
        "date": "2023-08-25T01:13:00.000Z",
        "voteCount": 1,
        "content": "B and D ,its a star schema  on which has a fact table include a customerID property."
      },
      {
        "date": "2023-09-04T02:34:00.000Z",
        "voteCount": 1,
        "content": "after think twice ,B&amp;E"
      },
      {
        "date": "2023-08-15T11:24:00.000Z",
        "voteCount": 1,
        "content": "B and D makes more sense, since inserted date is there already"
      },
      {
        "date": "2023-07-02T16:39:00.000Z",
        "voteCount": 2,
        "content": "If RowID was the surrogate, wouldn't it be an IDENTITY column? Therefore, it has to be B and E. Right? Please explain if this doesn't make sense make sense"
      },
      {
        "date": "2023-06-28T00:57:00.000Z",
        "voteCount": 2,
        "content": "https://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/\n\n\"For the SCD Type 2, we need to include three more attributes such as StartDate, EndDate and IsCurrent\"\nIsCurrentRow is already present! ...&nbsp;;-)\nCustomerKey (in reality is the RowID that many guys wants to add here), \neffectiveEndDate will probably set to: 31.12.9999, (to justify the not null).\n\nMy final answer wil lbe :&nbsp;B and E."
      },
      {
        "date": "2023-06-27T23:18:00.000Z",
        "voteCount": 2,
        "content": "what is the answer ?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104994-exam-dp-203-topic-1-question-73-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription that contains an Azure Synapse Analytics dedicated SQL pool.<br>You plan to deploy a solution that will analyze sales data and include the following:<br><br>\u2022\tA table named Country that will contain 195 rows<br>\u2022\tA table named Sales that will contain 100 million rows<br>\u2022\tA query to identify total sales by country and customer from the past 30 days<br><br>You need to create the tables. The solution must maximize query performance.<br><br>How should you complete the script? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image271.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image272.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-03T10:12:00.000Z",
        "voteCount": 19,
        "content": "Correct! 1. Hash(CustomerID) 2. Replicate"
      },
      {
        "date": "2023-05-11T03:10:00.000Z",
        "voteCount": 2,
        "content": "Could you please explain why 1. Hash([CustomerID]) is correct, and 2. Hash([OrderDate]) is incorrect."
      },
      {
        "date": "2023-07-02T08:54:00.000Z",
        "voteCount": 15,
        "content": "Don't hash on date, only partition on date"
      },
      {
        "date": "2023-06-24T15:26:00.000Z",
        "voteCount": 5,
        "content": "Never distribute on Date."
      },
      {
        "date": "2023-05-19T08:02:00.000Z",
        "voteCount": 6,
        "content": "It is hash because it is a fact table (you can tell because there is the \"total\" column being created which is numerical). Rule of thumb, never hash on a date field, so in this case you would hash on 'CustomerID'. You want the hash to have as many unique values as possible."
      },
      {
        "date": "2024-07-17T09:32:00.000Z",
        "voteCount": 1,
        "content": "1. You would want hash distribution to improve query performance. You don't want to hash on Date column since it can cause bottlenecks if many people query on a same date (eg. getdate()), so hash on customerid is the way.\n2. You would want to replicate small tables across all distributions, so it can pick up any distribution and still have full data."
      },
      {
        "date": "2023-09-04T02:39:00.000Z",
        "voteCount": 1,
        "content": "1. Hash(CustomerID) \n2. Replicate"
      },
      {
        "date": "2023-08-15T11:30:00.000Z",
        "voteCount": 1,
        "content": "given answer is correct"
      },
      {
        "date": "2023-08-02T10:23:00.000Z",
        "voteCount": 2,
        "content": "Correct. Hash on Sales Table(Fact) and Replicate on Country table(Dimension)"
      },
      {
        "date": "2023-04-09T13:17:00.000Z",
        "voteCount": 4,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104995-exam-dp-203-topic-1-question-74-discussion/",
    "body": "You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account1 and an Azure Synapse Analytics workspace named workspace1.<br><br>You need to create an external table in a serverless SQL pool in workspace1. The external table will reference CSV files stored in account1. The solution must maximize performance.<br><br>How should you configure the external table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a native external table and authenticate by using a shared access signature (SAS).\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a native external table and authenticate by using a storage account key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Hadoop external table and authenticate by using a shared access signature (SAS).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Hadoop external table and authenticate by using a service principal in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 41,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-03T10:20:00.000Z",
        "voteCount": 33,
        "content": "Correct! Serverless SQL Pools cannot use Hadoop, Only Native. Access Key Auth is never best practice therefore leaving only A as a viable answer.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"
      },
      {
        "date": "2023-04-09T13:18:00.000Z",
        "voteCount": 5,
        "content": "thanks a lot for the good explanation"
      },
      {
        "date": "2023-05-23T07:42:00.000Z",
        "voteCount": 7,
        "content": "It's not about the best practice - there is no option to use storage keys... \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop#security"
      },
      {
        "date": "2023-06-24T15:28:00.000Z",
        "voteCount": 5,
        "content": "The other options provided (B, C, and D) are not the recommended configurations for maximizing performance in this scenario. Using a storage account key for authentication (option B) poses a security risk and should be avoided. Apache Hadoop external tables (options C and D) do not provide the same level of performance optimization as native external tables in Azure Synapse Analytics."
      },
      {
        "date": "2024-05-01T01:23:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      },
      {
        "date": "2024-01-31T14:13:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nNative external tables  perform better than hadoop external table.\nAlso ,storage account is more secure with the scheme of SAS \n\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
      },
      {
        "date": "2024-01-27T23:36:00.000Z",
        "voteCount": 1,
        "content": "To maximize performance when creating an external table in a serverless SQL pool in Azure Synapse Analytics workspace1 that references CSV files stored in account1, you should use a native external table and authenticate by using a shared access signature (SAS).\n\nNative external tables are designed to read and export data in various data formats such as CSV and Parquet. They are available in serverless SQL pools and are in public preview in dedicated SQL pools. Using an SAS to authenticate provides a secure way to access the data in account1 without exposing the storage account key."
      },
      {
        "date": "2023-09-08T03:34:00.000Z",
        "voteCount": 1,
        "content": "No support for storage acc key only ui, sas, sp, mi, apa"
      },
      {
        "date": "2023-09-04T02:41:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-08-15T11:35:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/microsoft/view/104974-exam-dp-203-topic-1-question-75-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics serverless SQL pool that contains a database named db1. The data model for db1 is shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image273.png\"><br><br>Use the drop-down menus to select the answer choice that completes each statement based on the information presented in the exhibit.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image274.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image275.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-03T07:49:00.000Z",
        "voteCount": 89,
        "content": "Correct answer should be join DimGeography and DimCustomer and 5 tables.\n\nYou also need to combine ProductLine and Product in order for the schema to be considered a star schema. This would result in 5 remaining tables: DimCustomer (DimCustomer JOIN DimGeography), DimStore, Date, Product (Product JOIN ProductLine) and FactOrders."
      },
      {
        "date": "2024-08-03T14:45:00.000Z",
        "voteCount": 1,
        "content": "This is true, but we are limited by what possible answers the question provides us.  So the ExamTopics selected answers are correct."
      },
      {
        "date": "2024-01-31T14:37:00.000Z",
        "voteCount": 2,
        "content": "It is a conversion of snow flake schema to star schema.All"
      },
      {
        "date": "2023-04-20T13:51:00.000Z",
        "voteCount": 20,
        "content": "Agree with explanation. It will still be snowflake if Product and ProductLine is not combined"
      },
      {
        "date": "2024-09-12T07:27:00.000Z",
        "voteCount": 2,
        "content": "I made the exam this week (09/09/2024) and this question was present but slightly different."
      },
      {
        "date": "2024-07-09T11:44:00.000Z",
        "voteCount": 2,
        "content": "We should join the FacOrders with the DimGeography\nIn a star schema we have a fact table and dimensions, and dimensions shouldn't be joined \nalso since we are making a join we are keeping the 6 dimensions + the fact table = 7"
      },
      {
        "date": "2024-05-03T07:53:00.000Z",
        "voteCount": 1,
        "content": "I found this question on my exam 30/04/2024, and I put \n- join dimGeography and dimCustomer\n- 5\nI passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-04-16T08:18:00.000Z",
        "voteCount": 1,
        "content": "If 2 tables are joined there are still 2 tables, we're not UNIONing them so still 7 tables"
      },
      {
        "date": "2024-04-12T11:39:00.000Z",
        "voteCount": 1,
        "content": "Provided answer is correct: why not 5 tables? Please, look at schema, and count once more, how many tables are at all including fact as well :)\nSo Join Geography with Customers = overall 6 tables"
      },
      {
        "date": "2024-04-12T11:48:00.000Z",
        "voteCount": 1,
        "content": "Sorry, the correct is 5, bcs question is tricky and we still have to union ProductLine with Product to have at the end star schema"
      },
      {
        "date": "2024-01-24T23:11:00.000Z",
        "voteCount": 2,
        "content": "If you design DimGeography as Role Playing Dimension (How its usually done), then join Geography and Fact. Afterwards you could merge the Product Group and Product Table which leads to 6 Tables. So I would argue correct would be \nA: Join DimGeography and Fact\nB: 6"
      },
      {
        "date": "2024-02-14T05:12:00.000Z",
        "voteCount": 2,
        "content": "Wrong and wrong"
      },
      {
        "date": "2024-01-04T18:07:00.000Z",
        "voteCount": 2,
        "content": "I think maybe we all missed something here. If it's star schema, there is nothing hanging off the outside of the outside tables. DimGeography should be joined to FactSales, with the geography placed in the FactSales Table. However, it doesn't solve the problem of Product and ProductCategory, which need to be combined.  So there is just part of the answer missing. Once those two items are done, then there are 5 tables remainin."
      },
      {
        "date": "2024-02-28T14:53:00.000Z",
        "voteCount": 2,
        "content": "There would still be 6 tables"
      },
      {
        "date": "2023-11-20T22:49:00.000Z",
        "voteCount": 2,
        "content": "it's a tricky question , it says once we join DimGeography and DimCustomer then how many tables will remain in data model. Answer is 6."
      },
      {
        "date": "2023-10-11T06:46:00.000Z",
        "voteCount": 4,
        "content": "num of tables ( dims + facts) == aka ==&gt; 6"
      },
      {
        "date": "2023-10-08T08:34:00.000Z",
        "voteCount": 6,
        "content": "Correct would be:\n1) join Geography with customer\n2) (then join productline and product - this is not in the question, but must be done to transform into a star schema)\n3) then we have 5 tables since Geography and ProductLine are no longer needed."
      },
      {
        "date": "2023-09-07T01:27:00.000Z",
        "voteCount": 3,
        "content": "join DimGeography and DimCustomer\n5 tables"
      },
      {
        "date": "2023-09-06T00:10:00.000Z",
        "voteCount": 3,
        "content": "shouldn't it be 5 tables?"
      },
      {
        "date": "2023-09-04T02:42:00.000Z",
        "voteCount": 3,
        "content": "1 DimGeography and DimCustomer\n\n 2. 5 tables."
      },
      {
        "date": "2023-08-16T19:29:00.000Z",
        "voteCount": 1,
        "content": "This question is really messy. It doesn't explicit say that by joining or unioning the tables this means they will be combined into a single table, to be it seems like we'll still have 2 tables (DimGeography and DimCustomer) in both options, besides the fact that just fixing DimGeography and DImCustomer won't generate a Star Schema"
      },
      {
        "date": "2023-08-15T11:40:00.000Z",
        "voteCount": 2,
        "content": "join DimGeography and DimCustomer and 5 tables"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105169-exam-dp-203-topic-1-question-76-discussion/",
    "body": "You have an Azure Databricks workspace and an Azure Data Lake Storage Gen2 account named storage1.<br><br>New files are uploaded daily to storage1.<br><br>You need to recommend a solution that configures storage1 as a structured streaming source. The solution must meet the following requirements:<br><br>\u2022\tIncrementally process new files as they are uploaded to storage1.<br>\u2022\tMinimize implementation and maintenance effort.<br>\u2022\tMinimize the cost of processing millions of files.<br>\u2022\tSupport schema inference and schema drift.<br><br>Which should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCOPY INTO",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAuto Loader\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Spark FileStreamSource"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-05T05:20:00.000Z",
        "voteCount": 17,
        "content": "Auto Loader provides a Structured Streaming source called cloudFiles. Plus, it supports schema drift. Hence, Auto Loader is the correct answer.\n https://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/"
      },
      {
        "date": "2023-05-10T10:17:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader does not support Azure Data Lake Storage Gen2"
      },
      {
        "date": "2023-05-14T19:21:00.000Z",
        "voteCount": 1,
        "content": "It does.  Refer this link\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/"
      },
      {
        "date": "2023-06-24T15:31:00.000Z",
        "voteCount": 3,
        "content": "Auto Loader can load data files from AWS S3 (s3://), Azure Data Lake Storage Gen2 (ADLS Gen2, abfss://), Google Cloud Storage (GCS, gs://), Azure Blob Storage (wasbs://), ADLS Gen1 (adl://), and Databricks File System (DBFS, dbfs:/)."
      },
      {
        "date": "2024-09-05T06:14:00.000Z",
        "voteCount": 1,
        "content": "autoloader is correct-:Incremental processing: Auto Loader can automatically detect and incrementally process new files as they are uploaded to Azure Data Lake Storage Gen2.\nMinimize implementation and maintenance effort: Auto Loader is designed for simplicity, requiring minimal setup and automatically handling file management. It reduces operational overhead by automating many of the tasks required to manage a streaming source.\nMinimize the cost of processing millions of files: Auto Loader efficiently scales to handle millions of files and minimizes costs by using a directory listing mode or a more optimized file notification mode with Azure Event Grid.\nSupport schema inference and schema drift: Auto Loader automatically infers schema and can handle schema drift, which allows it to dynamically adapt to changes in the file structure without requiring constant updates to the processing logic."
      },
      {
        "date": "2024-03-25T02:49:00.000Z",
        "voteCount": 3,
        "content": "Auto Loader is correct"
      },
      {
        "date": "2024-03-19T09:32:00.000Z",
        "voteCount": 1,
        "content": "Reference:\nhttps://learn.microsoft.com/en-us/azure/databricks/ingestion/auto-loader/#incremental-ingestion-using-auto-loader-with-delta-live-tables"
      },
      {
        "date": "2024-02-01T09:28:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader seems more correct.\nCopy Into focuses on loading from storage to a Delta table"
      },
      {
        "date": "2024-01-28T00:10:00.000Z",
        "voteCount": 1,
        "content": "To configure storage1 as a structured streaming source that incrementally processes new files as they are uploaded minimizes implementation and maintenance effort, minimizes the cost of processing millions of files, and supports schema inference and schema drift, you should use Apache Spark FileStreamSource"
      },
      {
        "date": "2023-10-08T08:44:00.000Z",
        "voteCount": 4,
        "content": "Bing explains the following:\nThe best option is C. Auto Loader.\n\nAuto Loader is a feature in Azure Databricks that uses a cloudFiles data source to incrementally and efficiently process new data files as they arrive in Azure Data Lake Storage Gen2. It supports schema inference and schema evolution (drift). It also minimizes implementation and maintenance effort, as it simplifies the ETL pipeline by reducing the complexity of identifying new files for processing.\n\nOther options do not meet the requirements because: \nA. COPY INTO: does not incrementally process new files as they are uploaded, which is one of your requirements.\n\nB. Azure Data Factory: does not natively support schema inference and schema drift. The incremental processing of new files would need to be manually implemented, which could increase implementation and maintenance effort.\n\nD. Apache Spark FileStreamSource: requires manual setup and does not natively support schema inference or schema drift. It also may not minimize the cost of processing millions of files as efficiently as Auto Loader."
      },
      {
        "date": "2023-09-04T02:43:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader"
      },
      {
        "date": "2023-08-15T11:42:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-06-11T09:57:00.000Z",
        "voteCount": 4,
        "content": "To configure Azure Data Lake Storage Gen2 account (storage1) as a structured streaming source in Azure Databricks workspace, while meeting the given requirements, you should include the following in the recommendation:\n\nC. Auto Loader\n\nAuto Loader is a feature provided by Azure Databricks that automatically discovers and processes new files as they are uploaded to a specified directory in Azure Data Lake Storage Gen2. It provides an efficient and cost-effective way to incrementally process new files without the need for manual intervention. Auto Loader also supports schema inference and schema drift, allowing you to handle changes in the file schema over time.\n\nBy using Auto Loader, you can minimize implementation and maintenance effort as it takes care of monitoring the storage directory for new files and processing them in an optimized manner. It also helps to minimize the cost of processing millions of files as it leverages the efficient processing capabilities of Databricks.\n\nTherefore, the correct answer is C. Auto Loader."
      },
      {
        "date": "2023-05-14T22:53:00.000Z",
        "voteCount": 1,
        "content": "Auto Loader"
      },
      {
        "date": "2024-02-10T13:33:00.000Z",
        "voteCount": 1,
        "content": "I recommend using Auto Loader. Here\u2019s why:\nIncremental Processing: Auto Loader in Azure Databricks allows you to process new files incrementally as they are uploaded to your storage account. It efficiently identifies and processes only the new data, reducing the need to reprocess entire datasets.\nLow Implementation and Maintenance Effort: Auto Loader simplifies the setup process. You can configure it easily within your Databricks workspace, and it automatically handles file discovery, partitioning, and schema inference.\nCost-Effective: Auto Loader optimizes resource usage by processing only the necessary data. It avoids unnecessary scans of existing files, which helps minimize costs when dealing with millions of files.\nSchema Inference and Schema Drift Support: Auto Loader automatically infers the schema from the data and adapts to schema changes over time (schema drift). This flexibility ensures smooth processing even when the structure of incoming files evolves.\nTherefore, choose C."
      },
      {
        "date": "2023-05-08T05:07:00.000Z",
        "voteCount": 1,
        "content": "D according to ChatGPT"
      },
      {
        "date": "2023-04-04T12:05:00.000Z",
        "voteCount": 1,
        "content": "Ans : B\n\nDF supports Schema Drift - \nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-data-flow-schema-drift"
      },
      {
        "date": "2023-04-15T04:49:00.000Z",
        "voteCount": 2,
        "content": "Auto Loader is lower cost."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108481-exam-dp-203-topic-1-question-77-discussion/",
    "body": "You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image287.png\"><br><br>You need to read the TSV files by using ad-hoc queries and the OPENROWSET function. The solution must assign a name and override the inferred data type of each column.<br><br>What should you include in the OPENROWSET function?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe WITH clause\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe ROWSET_OPTIONS bulk option",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe DATAFILETYPE bulk option",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tthe DATA_SOURCE parameter"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 40,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-05T10:45:00.000Z",
        "voteCount": 15,
        "content": "In the Question \"The solution must assign a name and override the inferred data type of each column\", so we must need a WITH Clause to define the column names and data types."
      },
      {
        "date": "2023-05-04T06:58:00.000Z",
        "voteCount": 9,
        "content": "I think it's A. WITH CLAUSE\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset"
      },
      {
        "date": "2023-05-05T01:28:00.000Z",
        "voteCount": 4,
        "content": "Agreed - Should be A. \"To specify explicit column names and data types, you can override the default column names and inferred data types by providing a schema definition in a WITH clause\" (https://learn.microsoft.com/en-us/training/modules/query-data-lake-using-azure-synapse-serverless-sql-pools/3-query-files)"
      },
      {
        "date": "2024-09-05T06:20:00.000Z",
        "voteCount": 2,
        "content": "A is correct because-:SELECT * \nFROM OPENROWSET(\n    BULK 'https://storage1.blob.core.windows.net/container/file.tsv',\n    FORMAT='CSV', \n    PARSER_VERSION='2.0', \n    FIELDTERMINATOR = '\\t', \n    ROWTERMINATOR = '\\n'\n) \nWITH (\n    Column1Name datatype1,\n    Column2Name datatype2,\n    Column3Name datatype3\n) AS result;      \nB. ROWSET_OPTIONS bulk option: Configures aspects of data processing like buffering or parallelism, but it doesn\u2019t handle column names or types.\nC. DATAFILETYPE bulk option: Used to specify the file type but not for defining column names or types.\nD. DATA_SOURCE parameter: Points to an external data source, but it doesn't help with naming or overriding data types.\nThus, the WITH clause is the correct option to include in the OPENROWSET function to assign column names and override inferred data types."
      },
      {
        "date": "2024-07-13T03:50:00.000Z",
        "voteCount": 1,
        "content": "DATA_SOURCE is not even an existing parameter option in th OPENROWSET function, the right existing option is  DATASOURCE, without underscore, but it is not needed here, so A is the right option, \n\nsee doc:\nhttps://learn.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver16"
      },
      {
        "date": "2024-04-11T11:42:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-04-04T01:28:00.000Z",
        "voteCount": 1,
        "content": "Question insists on the point that the file has not the header row! specifying a datasource is not strictly required, since OPENROWSET can connect to storage even with a simple link and managed identity. The WITH clause is the most reasonable recommendation to include in the solution."
      },
      {
        "date": "2024-03-25T02:54:00.000Z",
        "voteCount": 2,
        "content": "There isn't header, so you have to specify columns name manually in WITH clause"
      },
      {
        "date": "2024-03-24T10:31:00.000Z",
        "voteCount": 1,
        "content": "With clause can be omitted for csv files see automatic schema discovery at https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#automatic-schema-discovery\nGeneric colnames will be assigned"
      },
      {
        "date": "2024-03-27T02:43:00.000Z",
        "voteCount": 1,
        "content": "on secodn thought, even if the types CAN be inferred, the question clearly states that the inferred types MUST be overwritten. Therefore WITH"
      },
      {
        "date": "2024-02-28T14:58:00.000Z",
        "voteCount": 1,
        "content": "There is no header row, therefore you should define one using the WITH Clause"
      },
      {
        "date": "2024-01-28T00:13:00.000Z",
        "voteCount": 1,
        "content": "The WITH clause allows you to specify the schema of the data source and override the inferred data type of each column. You can also use the WITH clause to specify the name of the external table and the location of the data source"
      },
      {
        "date": "2024-01-27T02:35:00.000Z",
        "voteCount": 1,
        "content": "WITH CLAUSE"
      },
      {
        "date": "2023-12-06T13:45:00.000Z",
        "voteCount": 1,
        "content": "To read TSV files without a header row using the `OPENROWSET` function and to assign a name and specify the data type for each column, you should use:\n\nA. the WITH clause\n\nThe WITH clause is used in the `OPENROWSET` function to define the format file or to directly define the structure of the file by specifying the column names and data types."
      },
      {
        "date": "2023-11-22T02:15:00.000Z",
        "voteCount": 1,
        "content": "A - WITH Clause is the correct answer."
      },
      {
        "date": "2023-10-30T12:08:00.000Z",
        "voteCount": 1,
        "content": "D is right."
      },
      {
        "date": "2023-10-28T11:29:00.000Z",
        "voteCount": 2,
        "content": "To read TSV (Tab-Separated Values) files using ad-hoc queries and the OPENROWSET function in Azure Synapse Analytics, and to assign a name and override the inferred data type of each column, you should include the following in the OPENROWSET function:\n\nA. the WITH clause\n\nThe WITH clause allows you to specify options for reading the data, including defining the column names and data types. You can use the WITH clause to provide column definitions and specify the data type for each column in the TSV file, which allows you to override the inferred data types."
      },
      {
        "date": "2023-10-26T23:02:00.000Z",
        "voteCount": 5,
        "content": "They ask for \"in the function\", not \"in the query\""
      },
      {
        "date": "2023-10-08T09:23:00.000Z",
        "voteCount": 1,
        "content": "Correct is A (explained in another comment)"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108483-exam-dp-203-topic-1-question-78-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br><br>You plan to create a fact table named Table1 that will contain a clustered columnstore index.<br><br>You need to optimize data compression and query performance for Table1.<br><br>What is the minimum number of rows that Table1 should contain before you create partitions?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t100,000",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t600,000",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 million",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t60 million\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 47,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T04:39:00.000Z",
        "voteCount": 23,
        "content": "Clustered Column Store will by default have 60 partitions. And to achieve best compression we need at least 1 Million rows per partition, hence Option D 60 Millions (1M per partition)"
      },
      {
        "date": "2023-10-04T08:13:00.000Z",
        "voteCount": 2,
        "content": "You mean the dedicated SQL pool has 60 distributions \"by default\"?"
      },
      {
        "date": "2024-03-25T04:40:00.000Z",
        "voteCount": 1,
        "content": "Not by default, but always"
      },
      {
        "date": "2023-12-04T18:28:00.000Z",
        "voteCount": 2,
        "content": "60 Million is correct"
      },
      {
        "date": "2024-09-05T06:22:00.000Z",
        "voteCount": 1,
        "content": "D is correct-:Partitioning in a dedicated SQL pool in Azure Synapse Analytics is typically used to manage very large tables, and the recommendation is to start considering partitions when the table contains 60 million rows or more.\n\nPartitioning helps optimize both data compression and query performance by allowing the system to process smaller subsets of data more efficiently. However, partitioning comes with overhead, and if you partition tables that are too small, it can actually degrade performance.\n\nClustered columnstore indexes are designed to provide efficient compression and query performance for large datasets. Partitioning further helps with large fact tables, but for smaller tables (e.g., fewer than 60 million rows), partitioning is usually not necessary and might even be counterproductive."
      },
      {
        "date": "2024-08-26T01:53:00.000Z",
        "voteCount": 1,
        "content": "\"When creating partitions on clustered columnstore tables, it is important to consider how many rows belong to each partition. For optimal compression and performance of clustered columnstore tables, a minimum of 1 million rows per distribution and partition is needed. Before partitions are created, dedicated SQL pool already divides each table into 60 distributions.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition#partition-sizing"
      },
      {
        "date": "2024-07-21T18:55:00.000Z",
        "voteCount": 1,
        "content": "1M * 60 = 60M"
      },
      {
        "date": "2024-06-30T21:24:00.000Z",
        "voteCount": 2,
        "content": "it is 1 Million. the question is clearly asking for minimum number of rows before creating partitions. and in Microsoft document they stated that 1 million rows is the minimum number of rows before partitioning a table"
      },
      {
        "date": "2024-08-26T01:50:00.000Z",
        "voteCount": 1,
        "content": "The question is asking the minimum number of rows in the whole table and not the number per partition. So, in order to have at least 1 million rows per partition, you must have 60 million rows in the fact table."
      },
      {
        "date": "2024-06-14T04:52:00.000Z",
        "voteCount": 4,
        "content": "1 Million (Option C) is correct. You need the minimum number of rows to create a optimized partition. A single optimal partition requires 1 Million rows."
      },
      {
        "date": "2024-05-15T11:52:00.000Z",
        "voteCount": 1,
        "content": "I think the correct answer should be 120 million rows. Since splitting the data up into 2 partitions would result in 1 million rows per distribution and partition for 120 million rows"
      },
      {
        "date": "2024-04-04T01:26:00.000Z",
        "voteCount": 1,
        "content": "even without knowing how many partitions you're going to create, you know that each partition should contain at least 1million rows. 60million rows are the only case enabling to use partitions."
      },
      {
        "date": "2024-02-28T15:04:00.000Z",
        "voteCount": 3,
        "content": "Cluster columnstore tables begin to achieve optimal compression once there is more than 60 million rows. For small lookup tables, less than 60 million rows, consider using HEAP or clustered index for faster query performance. -- Microsoft"
      },
      {
        "date": "2024-01-28T00:16:00.000Z",
        "voteCount": 2,
        "content": "To optimize data compression and query performance for Table in Azure Synapse Analytics dedicated SQL pool, you should create partitions when the table contains at least 60 million rows.\n\nPartitioning tables can improve query performance by reducing the amount of data that needs to be scanned. It can also improve data compression by allowing each partition to be compressed separately.\n\nIn general, you should consider partitioning a table when it contains a large amount of data and queries frequently filter on a specific column or set of columns"
      },
      {
        "date": "2023-12-29T03:11:00.000Z",
        "voteCount": 3,
        "content": "Question says \"What is the minimum number of rows that Table1 should contain before you create (add/new/extra) partitions?\"\nAs per microsoft documentation, each partition will contain 1Million records. So, if there atleast 1million records, we can go for partitioning.\n\nHere is the link for documentation\nhttps://learn.microsoft.com/en-us/sql/relational-databases/indexes/columnstore-indexes-overview?view=sql-server-ver16"
      },
      {
        "date": "2023-12-27T09:58:00.000Z",
        "voteCount": 2,
        "content": "60m, see https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-tables-partition"
      },
      {
        "date": "2023-12-12T02:43:00.000Z",
        "voteCount": 1,
        "content": "A  100mil  is correct &gt;&gt;\nFrom the answers to this Q, I see that MS has done a bad job because people don't understand what distributions or partitions are. My explanation: Each table with column store index is auto divided into 60 distributions, on each of these distributions there is auto 1 partition. For good performance (with column store) each partition must have at least 1Mil rows.\nThe question was: \"What is the minimum number of rows that Table1 should contain before you create (add/new/extra) partitions?\"\nSo there is no point in creating partitions with 60M rows,\nbecause then you divide this into 0.5Mil per partition. At least 120Mil would be ideal, but 100Mil already starts."
      },
      {
        "date": "2023-09-07T01:29:00.000Z",
        "voteCount": 2,
        "content": "60 million"
      },
      {
        "date": "2023-09-04T02:47:00.000Z",
        "voteCount": 2,
        "content": "is correct"
      },
      {
        "date": "2023-08-15T12:23:00.000Z",
        "voteCount": 3,
        "content": "should be D"
      },
      {
        "date": "2023-08-07T04:14:00.000Z",
        "voteCount": 2,
        "content": "WHY People mentioned option D..please explain how?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108530-exam-dp-203-topic-1-question-79-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named DimSalesPerson. DimSalesPerson contains the following columns:<br><br>\u2022\tRepSourceID<br>\u2022\tSalesRepID<br>\u2022\tFirstName<br>\u2022\tLastName<br>\u2022\tStartDate<br>\u2022\tEndDate<br>\u2022\tRegion<br><br>You are developing an Azure Synapse Analytics pipeline that includes a mapping data flow named Dataflow1. Dataflow1 will read sales team data from an external source and use a Type 2 slowly changing dimension (SCD) when loading the data into DimSalesPerson.<br><br>You need to update the last name of a salesperson in DimSalesPerson.<br><br>Which two actions should you perform? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate three columns of an existing row.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate two columns of an existing row.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert an extra row.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate one column of an existing row.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 66,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 18,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-27T04:41:00.000Z",
        "voteCount": 19,
        "content": "CD is correct"
      },
      {
        "date": "2023-05-23T21:54:00.000Z",
        "voteCount": 18,
        "content": "1) Insert an extra row with the updated last name and the current date as the StartDate.\n2) Update two columns of an existing row: set the EndDate of the previous row for that salesperson to the current date and set the current value of the SalesRepID column to inactive."
      },
      {
        "date": "2023-05-23T21:59:00.000Z",
        "voteCount": 1,
        "content": "This will preserve the history of changes to the salesperson\u2019s last name while keeping the most current information in the table"
      },
      {
        "date": "2024-04-05T09:07:00.000Z",
        "voteCount": 2,
        "content": "Exactly.\n\nSCD TYPE2 Needs three columns: StartDate, EndDate and isActive flag."
      },
      {
        "date": "2024-05-01T01:56:00.000Z",
        "voteCount": 1,
        "content": "active flag is optional for type 2 and in this scenario does not exist"
      },
      {
        "date": "2024-05-01T01:55:00.000Z",
        "voteCount": 1,
        "content": "C - extra row with new surname and new start date\nD-set end date in the existing column\nThat makes it type 2 - one of the variations off"
      },
      {
        "date": "2024-02-28T15:08:00.000Z",
        "voteCount": 4,
        "content": "I originally thought it would be BC, but then I realised you WOULDN'T update the Last Name, you want to retain the history of the LastName. You would update the EndDate column for that record to mark it as historic and then insert a new row with the new LastName."
      },
      {
        "date": "2024-01-08T01:48:00.000Z",
        "voteCount": 1,
        "content": "CD is correct as in scd2 we need startdate,enddate that is already present \nwhat we need is \"ISActive/flag\" and one more row thats all its takes to make scd2"
      },
      {
        "date": "2023-09-07T01:31:00.000Z",
        "voteCount": 3,
        "content": "C &amp; D are correct"
      },
      {
        "date": "2023-09-04T21:16:00.000Z",
        "voteCount": 10,
        "content": "- Update one column: EndDate to the change date\n- Insert a new record with the new value of LastName, StartDate as the change date."
      },
      {
        "date": "2023-09-04T02:50:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-08-18T04:17:00.000Z",
        "voteCount": 2,
        "content": "cd is correct"
      },
      {
        "date": "2023-08-15T16:08:00.000Z",
        "voteCount": 8,
        "content": "answer should be CD, since activeRow flag is not present, we need to update only end date."
      },
      {
        "date": "2023-08-10T18:19:00.000Z",
        "voteCount": 2,
        "content": "CD is correct"
      },
      {
        "date": "2023-05-23T07:55:00.000Z",
        "voteCount": 1,
        "content": "It's SCD Type 2 - you need to update at least three columns in the original raw:\nSurname, StartDate and EndDate. (IsActive if one exists). Then insert new record.\nA and C"
      },
      {
        "date": "2023-05-23T07:56:00.000Z",
        "voteCount": 1,
        "content": "* \"original row\""
      },
      {
        "date": "2023-05-30T07:24:00.000Z",
        "voteCount": 7,
        "content": "but if you update the surname on the original row, don't you lose the previous value?"
      },
      {
        "date": "2023-10-04T08:21:00.000Z",
        "voteCount": 3,
        "content": "No the StartDate stays, you only need to update the EndDate in the original row, the old name also stays to track which names he had, only the new row should have the new name. So you would only need to edit the EndDate column on the old row and since there is no \"IsActive\" flag you ignore it, maybe it's just queried by date and sorted by date and you take the last row which is the newest."
      },
      {
        "date": "2023-05-13T10:32:00.000Z",
        "voteCount": 4,
        "content": "For me this is a little dubious since besides the end date update for the record we could have flg_is_active as well. Making B a possible answer in my opinion"
      },
      {
        "date": "2023-05-19T18:40:00.000Z",
        "voteCount": 1,
        "content": "It's saying \"update on column of an EXISTING row\". AKA you're just changing the IsCurrent part of the existing row, that's it."
      },
      {
        "date": "2023-05-05T10:50:00.000Z",
        "voteCount": 4,
        "content": "The answer is correct"
      },
      {
        "date": "2023-05-05T05:35:00.000Z",
        "voteCount": 4,
        "content": "Ans is correct"
      },
      {
        "date": "2023-05-05T04:27:00.000Z",
        "voteCount": 6,
        "content": "SCD Type 2 will have historical changes hence we will have new row and we need to update the existing row's end date. Hence - CD\n\nhttps://www.sqlshack.com/implementing-slowly-changing-dimensions-scds-in-data-warehouses/"
      },
      {
        "date": "2023-05-05T02:53:00.000Z",
        "voteCount": 7,
        "content": "Correct. You need to insert a new row with the updated data and update the EndDate of the old row"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108497-exam-dp-203-topic-1-question-80-discussion/",
    "body": "HOTSPOT<br> -<br><br>You plan to use an Azure Data Lake Storage Gen2 account to implement a Data Lake development environment that meets the following requirements:<br><br>\u2022\tRead and write access to data must be maintained if an availability zone becomes unavailable.<br>\u2022\tData that was last modified more than two years ago must be deleted automatically.<br>\u2022\tCosts must be minimized.<br><br>What should you configure? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image288.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image289.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-23T22:03:00.000Z",
        "voteCount": 22,
        "content": "Statement 1: For Storage redundancy, you should select ZRS (Zone-redundant storage). This will maintain read and write access to data even if an availability zone becomes unavailable.\n\nStatement 2: For data deletion, you should select A lifecycle management policy. This will allow you to automatically delete data that was last modified more than two years ago"
      },
      {
        "date": "2023-05-05T10:53:00.000Z",
        "voteCount": 6,
        "content": "Zone-redundant storage (ZRS) synchronously replicates your Azure managed disk across three Azure availability zones in the region you select. Each availability zone is a separate physical location with independent power, cooling, and networking"
      },
      {
        "date": "2024-04-20T08:57:00.000Z",
        "voteCount": 1,
        "content": "Correct guys"
      },
      {
        "date": "2024-02-22T03:57:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-06T00:20:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-04T02:52:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-25T01:56:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-15T16:10:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      },
      {
        "date": "2023-05-19T13:34:00.000Z",
        "voteCount": 1,
        "content": "Confusion is how the write access will be maintained in case primary zone failure?"
      },
      {
        "date": "2023-05-22T08:11:00.000Z",
        "voteCount": 2,
        "content": "With ZRS, your data is still accessible for both read and write operations even if a zone becomes unavailable.\nhttps://learn.microsoft.com/en-us/azure/storage/common/storage-redundancy"
      },
      {
        "date": "2023-05-16T15:56:00.000Z",
        "voteCount": 4,
        "content": "Zone-redundant storage (ZRS) &amp; Lifecycle Policy"
      },
      {
        "date": "2023-05-05T09:57:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-05-04T13:12:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108498-exam-dp-203-topic-1-question-81-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are designing an Azure Data Lake Storage Gen2 container to store data for the human resources (HR) department and the operations department at your company.<br><br>You have the following data access requirements:<br><br>\u2022\tAfter initial processing, the HR department data will be retained for seven years and rarely accessed.<br>\u2022\tThe operations department data will be accessed frequently for the first six months, and then accessed once per month.<br><br>You need to design a data retention solution to meet the access requirements. The solution must minimize storage costs.<br><br>What should you include in the storage policy for each department? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image290.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image291.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-05T06:18:00.000Z",
        "voteCount": 15,
        "content": "The answer for HR depends on the meaning of \"rarely\" and the duration of \"initial processing\". If rarely is like once a year and initial processing is complete within 24 h the answer is correct. If rarely is like on a weekly basis, archiv might be the wrong way"
      },
      {
        "date": "2023-07-28T00:23:00.000Z",
        "voteCount": 3,
        "content": "I agree, I also felt like I was missing information. In this case however, I'd say go for 'minimizing costs'. So the lowest cost option possible."
      },
      {
        "date": "2023-05-04T13:14:00.000Z",
        "voteCount": 13,
        "content": "correct"
      },
      {
        "date": "2024-03-25T09:30:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-28T07:52:00.000Z",
        "voteCount": 1,
        "content": "After initial processing, the HR department data will be retained for seven years and rarely accessed\nThat means 365*7  = 2,555\n\nThe operations department data will be accessed frequently for the first six months, and then accessed once per month.\n\naccessed frequently for the first six months - Cool 30*6 =180"
      },
      {
        "date": "2023-09-06T00:27:00.000Z",
        "voteCount": 1,
        "content": "I had to reread the question but the answer is correct, it would have been better if they mentioned what \"rarely\" means. Issue will arise if data needs to be accessed within 180 days of moving to archive."
      },
      {
        "date": "2023-09-04T02:56:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-05-27T05:15:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-05-23T08:15:00.000Z",
        "voteCount": 1,
        "content": "You can't access data that was archived without rehydration. Rehydration requires either amending blob tier to hot or cold and is likely to incur a fee if stored less than 180 day or copying blob to another location... therefore \"rarely\" is unlikely a good option..."
      },
      {
        "date": "2023-05-16T15:59:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      },
      {
        "date": "2024-02-12T21:57:00.000Z",
        "voteCount": 1,
        "content": "After initial processing, the HR department data will be retained for seven years and rarely accessed\nThat means 365*7 = 2,555\n\nThe operations department data will be accessed frequently for the first six months, and then accessed once per month.\n\naccessed frequently for the first six months - Cool 30*6 =180"
      },
      {
        "date": "2023-05-05T11:00:00.000Z",
        "voteCount": 4,
        "content": "the answer is correct"
      },
      {
        "date": "2023-05-05T02:56:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108544-exam-dp-203-topic-1-question-82-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are developing an Azure Synapse Analytics pipeline that will include a mapping data flow named Dataflow1. Dataflow1 will read customer data from an external source and use a Type 1 slowly changing dimension (SCD) when loading the data into a table named DimCustomer in an Azure Synapse Analytics dedicated SQL pool.<br><br>You need to ensure that Dataflow1 can perform the following tasks:<br><br>\u2022 Detect whether the data of a given customer has changed in the DimCustomer table.<br>\u2022 Perform an upsert to the DimCustomer table.<br><br>Which type of transformation should you use for each task? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image292.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image293.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-10T10:58:00.000Z",
        "voteCount": 25,
        "content": "The answer is correct. Check \"Exercise - Design and implement a Type 1 slowly changing dimension with mapping data flows\", there is described implementation of the dataflow mentioned in this question.\n\nhttps://learn.microsoft.com/en-us/training/modules/populate-slowly-changing-dimensions-azure-synapse-analytics-pipelines/4-exercise-design-implement-type-1-dimension\n\n\nIn the exercise 'Derived column' transformation is used to add InsertedDate and ModifiedDate columns. ModifiedDate column can be used to detect whether the customer data has changed.  For Upsert 'Alter row' tranformation is used. The answer is definitely correct."
      },
      {
        "date": "2023-05-23T22:48:00.000Z",
        "voteCount": 7,
        "content": "Answer is correct..\n1) we don't need a surrogate key in SCD type 1. you can use a Derived Column transformation to compare the incoming data with the existing data in the DimCustomer table and detect changes..\n\nStatement 2: To perform an upsert to the DimCustomer table, you should use an Alter Row transformation. This transformation can be used to specify the actions to take for each row of data, such as inserting new rows or updating existing rows. The current web page context is empty."
      },
      {
        "date": "2024-05-04T04:26:00.000Z",
        "voteCount": 5,
        "content": "I found this question on my exam 30/04/2024, and I put:\n- derived column\n- alter row\n I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-02-01T08:32:00.000Z",
        "voteCount": 2,
        "content": "Aggregate and alter row.\nAggregate will give table statistics which reveals information on max.minimum,sum ,average etc.\n\nWhile alter row encompasses ,update &amp; insert which are upset operations"
      },
      {
        "date": "2023-09-04T03:06:00.000Z",
        "voteCount": 1,
        "content": "'Derived column'  &amp;'Alter row'"
      },
      {
        "date": "2023-08-15T16:18:00.000Z",
        "voteCount": 1,
        "content": "answer is correct"
      },
      {
        "date": "2023-05-16T16:12:00.000Z",
        "voteCount": 2,
        "content": "'Derived column' &amp; 'Alter row'"
      },
      {
        "date": "2023-05-10T14:20:00.000Z",
        "voteCount": 1,
        "content": "surrogate key and assert"
      },
      {
        "date": "2023-05-08T05:26:00.000Z",
        "voteCount": 1,
        "content": "surrogate key and alter row according to chatgpt"
      },
      {
        "date": "2023-05-05T06:35:00.000Z",
        "voteCount": 1,
        "content": "It should be aggregate and alter row\n\nAs we talking Type 1 slowly changing dimension, we want to replace the current row with the updated one. This can be achieved by aggregate.\n\n\"A common use of the aggregate transformation is removing or identifying duplicate entries in source data. This process is known as deduplication. Based upon a set of group by keys, use a heuristic of your choosing to determine which duplicate row to keep. Common heuristics are first(), last(), max(), and min(). Use column patterns to apply the rule to every column except for the group by columns.\"\n\nAs in https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate\n\nAlter row is correct:\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-alter-row#merges-and-upserts-with-azure-sql-database-and-azure-synapse"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108532-exam-dp-203-topic-1-question-83-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Synapse Analytics serverless SQL pool.<br><br>You have an Azure Data Lake Storage account named adls1 that contains a public container named container1. The container1 container contains a folder named folder1.<br><br>You need to query the top 100 rows of all the CSV files in folder1.<br><br>How should you complete the query? To answer, drag the appropriate values to the correct targets. Each value may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point<br><br><img src=\"https://img.examtopics.com/dp-203/image294.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image295.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-16T16:21:00.000Z",
        "voteCount": 8,
        "content": "The provided query is correct for Azure Synapse Analytics serverless SQL pool. It selects the top 100 rows from the data in CSV format located at the specified URL: https://adls1.dfs.core.windows.net/container1/folde1/*.csv. The results are returned under the alias rows. Answer is correct."
      },
      {
        "date": "2023-09-07T01:33:00.000Z",
        "voteCount": 6,
        "content": "OPENROWSET &amp; Bulk"
      },
      {
        "date": "2024-05-04T04:27:00.000Z",
        "voteCount": 2,
        "content": "I found this question on my exam 30/04/2024, and I put \n- openrowset\n- bulk\n I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-03-25T09:55:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-04T03:09:00.000Z",
        "voteCount": 2,
        "content": "openrowset..bulk"
      },
      {
        "date": "2023-08-15T16:20:00.000Z",
        "voteCount": 2,
        "content": "ans is correct"
      },
      {
        "date": "2023-05-27T05:19:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-05-05T11:08:00.000Z",
        "voteCount": 3,
        "content": "The answer is correct"
      },
      {
        "date": "2023-05-05T06:41:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-05-05T03:13:00.000Z",
        "voteCount": 3,
        "content": "Answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108552-exam-dp-203-topic-1-question-84-discussion/",
    "body": "You have an Azure Synapse Analytics workspace named WS1 that contains an Apache Spark pool named Pool1.<br><br>You plan to create a database named DB1 in Pool1.<br><br>You need to ensure that when tables are created in DB1, the tables are available automatically as external tables to the built-in serverless SQL pool.<br><br>Which format should you use for the tables in DB1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tParquet\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tORC",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHIVE"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-08T18:29:00.000Z",
        "voteCount": 5,
        "content": "always pick parquet first"
      },
      {
        "date": "2023-09-04T03:10:00.000Z",
        "voteCount": 1,
        "content": "is correct"
      },
      {
        "date": "2023-08-15T16:23:00.000Z",
        "voteCount": 2,
        "content": "parquet.  CSV , delta also possible but not an option here."
      },
      {
        "date": "2023-08-07T04:29:00.000Z",
        "voteCount": 1,
        "content": "parquet"
      },
      {
        "date": "2023-05-27T05:20:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-05-16T16:22:00.000Z",
        "voteCount": 1,
        "content": "Parquet is the correct answer"
      },
      {
        "date": "2023-05-10T11:12:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n\nSupported formats for serverless pool: Delimited/CSV, Parquet,  Delta Lake \nSo Parquet is the correct answer"
      },
      {
        "date": "2023-05-05T11:10:00.000Z",
        "voteCount": 2,
        "content": "Parquet is supported by serverless SQL pool\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-parquet-files"
      },
      {
        "date": "2023-05-05T10:11:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-05T10:10:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111548-exam-dp-203-topic-1-question-85-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account named storage1.<br><br>You plan to implement query acceleration for storage1.<br><br>Which two file types support query acceleration? Each correct answer presents a complete solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Parquet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tXML",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAvro"
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-06-20T20:24:00.000Z",
        "voteCount": 11,
        "content": "Correct.\nQuery acceleration supports CSV and JSON formatted data as input to each request.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration"
      },
      {
        "date": "2023-06-11T10:32:00.000Z",
        "voteCount": 6,
        "content": "Query acceleration supports CSV and JSON formatted data as input to each request."
      },
      {
        "date": "2024-02-28T16:27:00.000Z",
        "voteCount": 1,
        "content": "A and D"
      },
      {
        "date": "2024-01-06T12:17:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration"
      },
      {
        "date": "2023-09-22T04:38:00.000Z",
        "voteCount": 1,
        "content": "Query acceleration supports CSV and JSON formatted data as input to each request."
      },
      {
        "date": "2023-09-04T03:10:00.000Z",
        "voteCount": 1,
        "content": "CSV and JSON"
      },
      {
        "date": "2023-06-12T02:27:00.000Z",
        "voteCount": 1,
        "content": "Parquet and CSV"
      },
      {
        "date": "2023-06-08T06:37:00.000Z",
        "voteCount": 4,
        "content": "Correct. https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-query-acceleration"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111553-exam-dp-203-topic-1-question-86-discussion/",
    "body": "You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image308.png\"><br><br>You need to read the files in storage1 by using ad-hoc queries and the OPENROWSET function. The solution must ensure that each rowset contains a single JSON record.<br><br>To what should you set the FORMAT option of the OPENROWSET function?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJSON",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDELTA",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPARQUET",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCSV\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 53,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-19T12:58:00.000Z",
        "voteCount": 11,
        "content": "It should be D normally. \nI'll appreciate it if the one who curates the right answer and when the majority doesn't agree with his choice, brings some explanation so we can discuss and understand why he chooses it. \nIt's not the first time I see a total disagreement without any explanation from the ones who select the right answers."
      },
      {
        "date": "2023-10-31T07:17:00.000Z",
        "voteCount": 11,
        "content": "Was on my exam today (31.10.2023)."
      },
      {
        "date": "2024-09-05T06:41:00.000Z",
        "voteCount": 1,
        "content": "This allows the SQL pool to treat each JSON object as a row and parse the file accordingly.\n\nThe correct answer is:\nA. JSON"
      },
      {
        "date": "2024-08-11T14:10:00.000Z",
        "voteCount": 1,
        "content": "A \n\nOPENROWSET support JSON format in Synapse. In traditional SQL Server on-prem JSON format wasn't supported so we used CSV method. Synapse support JSON OPENROWSET, json object is treated as separate row, no need for CSV workaround, it would complicate the process requiring additional parsing record"
      },
      {
        "date": "2024-07-28T12:10:00.000Z",
        "voteCount": 1,
        "content": "A.\nOPENROWSET(BULK) is a table-valued function that can read data from any file on the local drive or network, if SQL Server has read access to that location. It returns a table with a single column that contains the contents of the file.\nEg..\nSELECT BulkColumn\nFROM OPENROWSET(BULK 'C:\\JSON\\Books\\book.json', SINGLE_CLOB) as j;"
      },
      {
        "date": "2024-06-22T20:31:00.000Z",
        "voteCount": 1,
        "content": "You have three choices for input files that contain the target data for querying. Valid values are:\n\n    'CSV' - Includes any delimited text file with row/column separators. Any character can be used as a field separator, such as TSV: FIELDTERMINATOR = tab.\n\n    'PARQUET' - Binary file in Parquet format\n\n    'DELTA' - A set of Parquet files organized in Delta Lake (preview) format\n\nValues with blank spaces are not valid, e.g. 'CSV ' is not a valid value.\n\nSo, its D"
      },
      {
        "date": "2024-06-21T09:06:00.000Z",
        "voteCount": 4,
        "content": "Gemini says it is a CSV:\nBased on the information provided in the image, the format option of the OPENROWSET function should be set to D. CSV.\n\nHere's why:\n\nThe data files in Azure Blob storage are JSON files.\nHowever, SQL Server doesn't natively support the JSON format for the OPENROWSET function.\nAs a workaround, you can specify the format option as CSV and configure the field terminator and fieldquote options appropriately to process each line of the JSON file as a single record.\nEven though the data is in JSON format, choosing CSV as the format option allows you to read the data into a SQL table using OPENROWSET."
      },
      {
        "date": "2024-05-19T22:22:00.000Z",
        "voteCount": 1,
        "content": "chatgpt says option A"
      },
      {
        "date": "2024-04-01T05:37:00.000Z",
        "voteCount": 1,
        "content": "The easiest way to see to the content of your JSON file is to provide the file URL to the OPENROWSET function, specify csv FORMAT, and set values 0x0b for fieldterminator and fieldquote. If you need to read line-delimited JSON files, then this is enough."
      },
      {
        "date": "2024-04-01T05:34:00.000Z",
        "voteCount": 1,
        "content": "select top 10 *\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.jsonl',\n        format = 'csv',\n        fieldterminator ='0x0b',\n        fieldquote = '0x0b'\n    ) with (doc nvarchar(max)) as rows\ngo\nselect top 10 *\nfrom openrowset(\n        bulk 'https://pandemicdatalake.blob.core.windows.net/public/curated/covid-19/ecdc_cases/latest/ecdc_cases.json',\n        format = 'csv',\n        fieldterminator ='0x0b',\n        fieldquote = '0x0b',\n        rowterminator = '0x0b' --&gt; You need to override rowterminator to read classic JSON\n    ) with (doc nvarchar(max)) as rows"
      },
      {
        "date": "2024-02-29T12:02:00.000Z",
        "voteCount": 1,
        "content": "OPENROWSET doesn't have a JSON option"
      },
      {
        "date": "2024-02-06T08:00:00.000Z",
        "voteCount": 1,
        "content": "I checked this on another dump site and chatgpt. Both said the answer is A."
      },
      {
        "date": "2024-01-09T10:56:00.000Z",
        "voteCount": 1,
        "content": "Bonne r\u00e9ponse est D. Se r\u00e9f\u00e9rer au lien : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files"
      },
      {
        "date": "2023-09-24T10:52:00.000Z",
        "voteCount": 3,
        "content": "Please refer: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-documents"
      },
      {
        "date": "2023-09-04T03:12:00.000Z",
        "voteCount": 7,
        "content": "no json format, using CSV"
      },
      {
        "date": "2023-08-28T10:52:00.000Z",
        "voteCount": 4,
        "content": "Ignore my previous comment.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files"
      },
      {
        "date": "2023-08-28T10:52:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-json-files#read-json-files"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111867-exam-dp-203-topic-1-question-87-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription that contains the Azure Synapse Analytics workspaces shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image309.png\"><br><br>Each workspace must read and write data to datalake1.<br><br>Each workspace contains an unused Apache Spark pool.<br><br>You plan to configure each Spark pool to share catalog objects that reference datalake1.<br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image310.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image311.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-28T05:07:00.000Z",
        "voteCount": 15,
        "content": "Provided answers are correct:\n1. Yes:\nAzure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog. When customers want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines outside of the workspace, such as HDInsight and Azure Databricks, they can connect to an external Hive Metastore. Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. \n\n2. Yes:\nAnd currently we only support User-Password authentication. \n\n3. No:\n And currently we only support User-Password authentication.  ==&gt;&nbsp;STORAGE BLOB CONTRIBUTOR is an Azure RBAC (Role-Based Access Control) ==&gt;&nbsp;NOT&nbsp;COMPATIBLE&nbsp;(it is supported User-Password authentication&nbsp;ONLY).\n\nref.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2023-06-12T02:42:00.000Z",
        "voteCount": 11,
        "content": "No-Yes-Yes"
      },
      {
        "date": "2023-06-21T06:44:00.000Z",
        "voteCount": 3,
        "content": "I&nbsp;confirm the first No:\nThe first statement, \"The shared catalog objects can be stored in Azure Database for MySQL,\" is not true because Azure Database for MySQL is not the appropriate storage option for shared catalog objects in Azure Synapse Analytics. The shared catalog objects, which include metadata and schema information, are typically stored in a centralized metadata store such as the Apache Hive Metastore. Azure Synapse Analytics supports using an Azure SQL Database or an Azure SQL Data Warehouse (now called Azure Synapse SQL) as the metadata store, but Azure Database for MySQL is not a supported option for this purpose."
      },
      {
        "date": "2023-06-28T04:43:00.000Z",
        "voteCount": 8,
        "content": "Sorry I&nbsp;was wrong."
      },
      {
        "date": "2024-02-12T22:12:00.000Z",
        "voteCount": 1,
        "content": "Yes, Yes , No\n1. Yes:\nAzure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog. When customers want to persist the Hive catalog metadata outside of the workspace, and share catalog objects with other computational engines outside of the workspace, such as HDInsight and Azure Databricks, they can connect to an external Hive Metastore. Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore.\n\n2. Yes:\nAnd currently we only support User-Password authentication.\n\n3. No:\nAnd currently we only support User-Password authentication. ==&gt; STORAGE BLOB CONTRIBUTOR is an Azure RBAC (Role-Based Access Control) ==&gt; NOT COMPATIBLE (it is supported User-Password authentication ONLY).\n\nref.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2024-02-08T07:48:00.000Z",
        "voteCount": 2,
        "content": "I got this question yesterday on my exam"
      },
      {
        "date": "2024-02-01T10:34:00.000Z",
        "voteCount": 1,
        "content": "Looks like the Storage Blob Data Contributor role does not need to be assigned.\n\nWorkspace primary storage account\nIf the underlying data of your Hive tables is stored in the workspace primary storage account, you don't need to do extra settings. It will just work as long as you followed storage setting up instructions during workspace creation.\n\nOther ADLS Gen 2 account\nIf the underlying data of your Hive catalogs is stored in another ADLS Gen 2 account, you need to make sure the users who run Spark queries have Storage Blob Data Contributor role on the ADLS Gen2 storage account."
      },
      {
        "date": "2024-01-18T09:30:00.000Z",
        "voteCount": 5,
        "content": "Got this question on my exam on january 17, answers are correct."
      },
      {
        "date": "2023-12-20T11:52:00.000Z",
        "voteCount": 3,
        "content": "Confusing discussion section, no consensus"
      },
      {
        "date": "2023-09-04T03:14:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-11-01T19:20:00.000Z",
        "voteCount": 1,
        "content": "Shouldn't WS1 have blob data contributor access?"
      },
      {
        "date": "2023-08-15T16:33:00.000Z",
        "voteCount": 2,
        "content": "given answer is correct \nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2023-08-10T18:40:00.000Z",
        "voteCount": 1,
        "content": "Yes , yes , no https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2023-07-05T15:46:00.000Z",
        "voteCount": 2,
        "content": "Correct order should be Yes, No, Yes"
      },
      {
        "date": "2023-07-02T09:13:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2023-06-30T05:08:00.000Z",
        "voteCount": 1,
        "content": "What are the correct answers???"
      },
      {
        "date": "2023-08-11T02:13:00.000Z",
        "voteCount": 2,
        "content": "Yes, Yes , No you can check the document link"
      },
      {
        "date": "2023-06-22T14:20:00.000Z",
        "voteCount": 6,
        "content": "Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. And currently we only support User-Password authentication. https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      },
      {
        "date": "2023-06-10T22:45:00.000Z",
        "voteCount": 6,
        "content": "A ''Blob Data Contributor\" role must be assigned to user in order to access the files in blob storage. So it's a \"Yes\"\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/assign-azure-role-data-access?tabs=portal"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111554-exam-dp-203-topic-1-question-88-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have a data warehouse.<br><br>You need to implement a slowly changing dimension (SCD) named Product that will include three columns named ProductName, ProductColor, and ProductSize. The solution must meet the following requirements:<br><br>\u2022\tPrevent changes to the values stored in ProductName.<br>\u2022\tRetain only the current and the last values in ProductSize.<br>\u2022\tRetain all the current and previous values in ProductColor.<br><br>Which type of SCD should you implement for each column? To answer, drag the appropriate types to the correct columns. Each type may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image312.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image313.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-10T20:52:00.000Z",
        "voteCount": 95,
        "content": "Product name -type 0\ncolor -type 2\nsize -type 3"
      },
      {
        "date": "2023-06-15T02:40:00.000Z",
        "voteCount": 33,
        "content": "ProductName - type 0, as no changes are done. Color - type 3, as with type 3 we have one column for the current value and one for the previous so only these two are preserved. Size - type 2, as it inserts a new row for every change, so we get all historical values."
      },
      {
        "date": "2023-07-18T07:28:00.000Z",
        "voteCount": 19,
        "content": "Agree. beware that the order of ProductSize, ProductColor in the question, not same as in the graph.\nProduct name -type 0\ncolor -type 3\nsize -type 2"
      },
      {
        "date": "2024-02-13T02:17:00.000Z",
        "voteCount": 1,
        "content": "Type 1: Overwrite the existing data with new data, without maintaining a history. This is suitable when you want to prevent changes to the values stored in a column.\n\nType 2: Keep a history of changes by adding new rows for each change. This is suitable for retaining all the current and previous values in a column.\n\nType 3: Keep a limited history, typically by adding new columns to the dimension table to store both the current and previous values. This allows you to track changes but in a more compact way compared to Type 2.\nTherefore  I believe the answer is 0 3 2"
      },
      {
        "date": "2024-02-13T02:28:00.000Z",
        "voteCount": 5,
        "content": "I made a typo. \nMy answer should be as follow\nProductName: 0\nColor: 2\nSize: 3"
      },
      {
        "date": "2024-02-12T22:16:00.000Z",
        "voteCount": 5,
        "content": "ProductName: To prevent changes to the values stored in ProductName, we should use Type 0. This means that the value remains static and does not change over time.\nProductColor: Since we need to retain all the current and previous values in ProductColor, we should use Type 2. This type allows us to track historical changes by creating new rows for each change and maintaining a history of values.\nProductSize: To retain only the current and last values in ProductSize, we should use Type 3. This type keeps track of the current value and the previous value in separate columns.\nTherefore, the correct SCD types for the three columns are as follows:\n\nProductName: Type 0\nProductColor: Type 2\nProductSize: Type 3"
      },
      {
        "date": "2024-02-02T08:28:00.000Z",
        "voteCount": 1,
        "content": "Product : Type 0 :No change (This eliminates possibilities of Type  1,2 and other higher dimensions \nColour : Type  1 or 2 ;There is a change which can be rowise(type 1) or column wise(Type 2)\nSize :Type 3 ; All changes means both rowise and column wise"
      },
      {
        "date": "2023-12-06T14:12:00.000Z",
        "voteCount": 6,
        "content": "For the given requirements:\n\n- **Product Name**: Since changes must be prevented, this would be a Type 0 SCD, as it maintains the original value without any changes.\n- **Product Size**: To retain only the current and the last values, you would use a Type 3 SCD, which keeps the original value and adds a new column for the current value.\n- **Product Color**: To retain all the current and previous values, a Type 2 SCD is used, as it tracks historical data by creating a new record for each change.\n\nSo, you would apply:\n\n- Type 0 for ProductName\n- Type 3 for ProductSize\n- Type 2 for ProductColor"
      },
      {
        "date": "2023-10-04T10:02:00.000Z",
        "voteCount": 5,
        "content": "Name: 0 Fixed Dimension\nColor: 2 Row Versioning (current + last value)\nSize: 3 Previous Value column (current value + all previous values in extra column)"
      },
      {
        "date": "2023-10-04T10:03:00.000Z",
        "voteCount": 5,
        "content": "I meant Name: 0\nColor: 3\nSize: 2"
      },
      {
        "date": "2023-09-10T05:27:00.000Z",
        "voteCount": 2,
        "content": "SCD0-1-2"
      },
      {
        "date": "2023-09-07T01:38:00.000Z",
        "voteCount": 2,
        "content": "Product Name - Type 0\nColor - Type 2\nSize - Type 3"
      },
      {
        "date": "2023-09-06T00:37:00.000Z",
        "voteCount": 2,
        "content": "The answers are 0, 3, 2"
      },
      {
        "date": "2023-09-06T00:38:00.000Z",
        "voteCount": 2,
        "content": "ProductName - 0\nProductColor - 3\nProductSize - 2"
      },
      {
        "date": "2023-09-06T00:39:00.000Z",
        "voteCount": 2,
        "content": "The requirements and blanks are out of order in the question"
      },
      {
        "date": "2023-12-01T01:02:00.000Z",
        "voteCount": 4,
        "content": "ProductColor cannot be Type3 since it indicates that it has to preserve all current and previous values \u200b\u200bso Type3\nIt only preserves the current and previous value without maintaining a complete history.\nFor me it is:\nProductName: Type0\nColor: Type2\nSize: Type3"
      },
      {
        "date": "2023-08-15T16:37:00.000Z",
        "voteCount": 4,
        "content": "correct answer is - type 0, type 3,type 2"
      },
      {
        "date": "2023-08-07T04:33:00.000Z",
        "voteCount": 3,
        "content": "type 0\ntype 3\ntype 2"
      },
      {
        "date": "2023-07-01T07:24:00.000Z",
        "voteCount": 9,
        "content": "Answer is \nProduct name -type 0\ncolor -type 2\nsize -type 3\n\nType 0 \u2013 Fixed Dimension\nNo changes allowed, dimension never changes\n\nA Type 1 SCD always reflects the latest values, and when changes in source data are detected, the dimension table data is overwritten.\nType 2 SCD\nA Type 2 SCD supports versioning of dimension members. Often the source system doesn't store versions, so the data warehouse load process detects and manages changes in a dimension table. \nType 3 SCD\nA Type 3 SCD supports storing two versions of a dimension member as separate columns. The table includes a column for the current value of a member plus either the original or previous value of the member. So Type 3 uses additional columns to track one key instance of history, rather than storing additional rows to track each change like in a Type 2 SCD."
      },
      {
        "date": "2023-06-21T08:17:00.000Z",
        "voteCount": 2,
        "content": "Answer are corrects as reported in the solution:&nbsp;0, 1, and 2."
      },
      {
        "date": "2023-06-21T08:02:00.000Z",
        "voteCount": 3,
        "content": "\u2022 Prevent changes to the values stored in ProductName. =&gt; TYPE 0\n\u2022 Retain only the current and the last values in ProductSize. =&gt; TYPE 2 (current and last, menas all the history)\n\u2022 Retain all the current and previous values in ProductColor. =&gt; TYPE 3 (it includes a column for the previous value)"
      },
      {
        "date": "2023-06-21T08:14:00.000Z",
        "voteCount": 2,
        "content": "\u2022 Retain only the current and the last values in ProductSize. =&gt; TYPE 1 (current and last, DOESN'T means all the history, but as written only the last ... meaning the current).\n\nhttps://learn.microsoft.com/en-us/training/modules/load-optimize-data-into-relational-data-warehouse/5-load-slowly-changing-dimensions"
      },
      {
        "date": "2023-06-21T08:17:00.000Z",
        "voteCount": 2,
        "content": "Retain all the current and previous values in ProductColor.  =&gt;&nbsp;TYPE 2 (because the plural in the requirement: previous values, probably it means all the history). Concluding the answers provided are correct :)"
      },
      {
        "date": "2023-06-17T07:49:00.000Z",
        "voteCount": 4,
        "content": "my answer is 0,2,3"
      },
      {
        "date": "2023-06-10T23:00:00.000Z",
        "voteCount": 2,
        "content": "ProductSize is Type 2 since it maintains current and last record. Type 1 can only have current value."
      },
      {
        "date": "2023-06-13T08:16:00.000Z",
        "voteCount": 5,
        "content": "Retain ONLY the current and the last values in ProductSize. type2 will include all changes. type 3 is correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/microsoft/view/111555-exam-dp-203-topic-1-question-89-discussion/",
    "body": "HOTSPOT<br> -<br><br>You are incrementally loading data into fact tables in an Azure Synapse Analytics dedicated SQL pool.<br><br>Each batch of incoming data is staged before being loaded into the fact tables.<br><br>You need to ensure that the incoming data is staged as quickly as possible.<br><br>How should you configure the staging tables? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image314.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image315.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-06-20T20:45:00.000Z",
        "voteCount": 23,
        "content": "Correct!\nThe ROUND_ROBIN distribution distributes the data evenly across all distribution nodes in the SQL pool. This distribution type is suitable for loading data quickly into the staging tables because it minimizes the data movement during the loading process.\n\nUse a HEAP table: Instead of creating a clustered index on the staging table, it is recommended to create a HEAP table. A HEAP table does not have a clustered index, which eliminates the need for maintaining the index and improves the data loading performance. It allows for faster insert operations."
      },
      {
        "date": "2023-09-04T03:14:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-09-04T03:16:00.000Z",
        "voteCount": 1,
        "content": "For the staging data"
      },
      {
        "date": "2023-08-15T16:39:00.000Z",
        "voteCount": 2,
        "content": "ans is correct"
      },
      {
        "date": "2023-06-10T16:20:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-06-09T06:11:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2023-06-08T07:08:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117304-exam-dp-203-topic-1-question-90-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics workspace named ws1 and an Azure Cosmos DB database account named Cosmos1. Cosmos1 contains a container named container1 and ws1 contains a serverless SQL pool.<br><br>You need to ensure that you can query the data in container1 by using the serverless SQL pool.<br><br>Which three actions should you perform? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Azure Synapse Link for Cosmos1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable the analytical store for container1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn ws1, create a linked service that references Cosmos1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the analytical store for container1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable indexing for container1."
    ],
    "answer": "ACD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACD",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-04T01:34:00.000Z",
        "voteCount": 5,
        "content": "The answer is correct. We need to enable an analytical store in container1."
      },
      {
        "date": "2024-02-12T22:22:00.000Z",
        "voteCount": 1,
        "content": "To enable querying data in container1 using the serverless SQL pool, you should perform the following actions:\n\nEnable Azure Synapse Link for Cosmos1: This allows seamless integration between Azure Synapse Analytics and Cosmos DB, enabling efficient querying of data stored in Cosmos DB containers 1.\nCreate a linked service in ws1 that references Cosmos1: By creating a linked service, you establish a connection to your Cosmos DB account, allowing the serverless SQL pool to access data from Cosmos1 2.\nDisable the analytical store for container1: Since we want to query data using the serverless SQL pool, we don\u2019t need the analytical store feature for this specific scenario 3.\nTherefore, the correct actions are A, C, and B."
      },
      {
        "date": "2024-03-30T20:22:00.000Z",
        "voteCount": 2,
        "content": "\"Make sure that you have prepared Analytical store\" is a prerequisite.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-cosmos-db-analytical-store?tabs=openrowset-key#prerequisites"
      },
      {
        "date": "2024-01-24T23:37:00.000Z",
        "voteCount": 1,
        "content": "There are only 2 Correct answers.\nA &amp; D. You don't need a linked Service to query Cosmos DB from Serverless Pool.\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-cosmos-db-analytical-store?tabs=openrowset-key#prerequisites"
      },
      {
        "date": "2024-04-03T01:52:00.000Z",
        "voteCount": 1,
        "content": "If you need also synapse pipelines, adding a linked service is a requirement. It is a little trap, I think."
      },
      {
        "date": "2023-09-06T00:41:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-04T03:17:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-23T13:19:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-08-15T16:41:00.000Z",
        "voteCount": 2,
        "content": "The answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117107-exam-dp-203-topic-1-question-91-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image324.png\"><br><br>The storage1 account contains a container named container1. The container1 container contains the following files.<br><br><img src=\"https://img.examtopics.com/dp-203/image325.png\"><br><br>In Pool1, you run the following script.<br><br><img src=\"https://img.examtopics.com/dp-203/image326.png\"><br><br>In the Built-in serverless SQL pool, you run the following script.<br><br><img src=\"https://img.examtopics.com/dp-203/image327.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image328.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image329.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-04T01:39:00.000Z",
        "voteCount": 44,
        "content": "The answer is No, Yes, No\nIt will ignore \"_\" and \".\""
      },
      {
        "date": "2023-08-02T23:54:00.000Z",
        "voteCount": 10,
        "content": "Both Hadoop(dedicated) and native(serverless) external tables will skip the files with the names that begin with an underline (_) or a period (.)."
      },
      {
        "date": "2024-06-26T05:54:00.000Z",
        "voteCount": 1,
        "content": "Both Hadoop and native external tables will skip the files with the names that begin with an underline (_) or a period (.)"
      },
      {
        "date": "2024-04-24T06:16:00.000Z",
        "voteCount": 2,
        "content": "No No No"
      },
      {
        "date": "2024-02-13T02:15:00.000Z",
        "voteCount": 6,
        "content": "No, No, No\nBecause names that begin with an underline (_) or a period (.) are ignored and in the mentioned location, it will not be going to read subfolders, until '/**' is mentioned."
      },
      {
        "date": "2024-07-21T22:41:00.000Z",
        "voteCount": 2,
        "content": "No, Yes, No\nIn serverless SQL pools must be specified /** at the end of the location path. In Dedicated pool the folders are always scanned recursively."
      },
      {
        "date": "2024-03-30T20:42:00.000Z",
        "voteCount": 1,
        "content": "when you use DATA SOURCE to create an External Table you also use LOCATION (where you specified the relative path e.g. '/directory/**')\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#b-import-data-from-parquet-into-"
      },
      {
        "date": "2024-01-09T11:28:00.000Z",
        "voteCount": 1,
        "content": "R\u00e9ponse trouv\u00e9e : Non - Oui - Non. Lien de r\u00e9f\u00e9rence est : https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2023-12-06T23:44:00.000Z",
        "voteCount": 1,
        "content": "No, Yes, No"
      },
      {
        "date": "2023-09-06T00:49:00.000Z",
        "voteCount": 5,
        "content": "NO, YES, NO\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      },
      {
        "date": "2023-09-04T03:24:00.000Z",
        "voteCount": 4,
        "content": "NO\nYES\nNO\nsee previous quizs."
      },
      {
        "date": "2023-08-29T19:28:00.000Z",
        "voteCount": 3,
        "content": "NO\nYES\nNO"
      },
      {
        "date": "2023-08-02T05:59:00.000Z",
        "voteCount": 4,
        "content": "The last one is No.  File is prefxied with a period and therefore can't be returned.\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#location--folder_or_filepath-1"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117108-exam-dp-203-topic-1-question-92-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named account1 and a user named User1.<br><br>In account1, you create a container named container1. In container1, you create a folder named folder1.<br><br>You need to ensure that User1 can list and read all the files in folder1. The solution must use the principle of least privilege.<br><br>How should you configure the permissions for each folder? To answer, drag the appropriate permissions to the correct folders. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image330.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image331.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-02T06:06:00.000Z",
        "voteCount": 19,
        "content": "correct!\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control#levels-of-permission"
      },
      {
        "date": "2024-02-02T02:30:00.000Z",
        "voteCount": 6,
        "content": "Execute permission to container1 to navigate to folder1 and Read &amp; Execute permission to folder1 to list and read all files present in there."
      },
      {
        "date": "2024-03-25T14:33:00.000Z",
        "voteCount": 1,
        "content": "Correct, execute is to navigate folders"
      },
      {
        "date": "2024-01-06T15:10:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-04T03:27:00.000Z",
        "voteCount": 2,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117526-exam-dp-203-topic-1-question-93-discussion/",
    "body": "You have an Azure Data Factory pipeline named pipeline1.<br><br>You need to execute pipeline1 at 2 AM every day. The solution must ensure that if the trigger for pipeline1 stops, the next pipeline execution will occur at 2 AM, following a restart of the trigger.<br><br>Which type of trigger should you create?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tschedule\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttumbling",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tstorage event",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcustom event"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-08-25T01:46:00.000Z",
        "voteCount": 5,
        "content": "Answer is Tumbling : \nLink : https://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison\n\"Retry capability\tSupported. Failed pipeline runs have a default retry policy of 0, or a policy that's specified by the user in the trigger definition. Automatically retries when the pipeline runs fail due to concurrency/server/throttling limits (that is, status codes 400: User Error, 429: Too many requests, and 500: Internal Server error)\"\n\nRetry capability is not supported on Schedule trigger"
      },
      {
        "date": "2023-09-04T07:25:00.000Z",
        "voteCount": 4,
        "content": "I don't see 'Retry' as a requirement, they mentioned only about the next pipeline execution.  'A' seems to be more appropriate."
      },
      {
        "date": "2024-10-18T07:05:00.000Z",
        "voteCount": 1,
        "content": "A tumbling window trigger runs at a specified interval, and if the trigger stops, it maintains its schedule when restarted. This ensures the pipeline executes exactly at 2 AM daily, even after a trigger restart. Tumbling windows are useful for consistent execution without overlap or gaps, which makes them ideal for daily fixed scheduling like in this scenario."
      },
      {
        "date": "2024-10-18T07:04:00.000Z",
        "voteCount": 1,
        "content": "A tumbling window trigger runs at a specified interval, and if the trigger stops, it maintains its schedule when restarted. This ensures the pipeline executes exactly at 2 AM daily, even after a trigger restart. Tumbling windows are useful for consistent execution without overlap or gaps, which makes them ideal for daily fixed scheduling like in your scenario."
      },
      {
        "date": "2024-05-20T18:01:00.000Z",
        "voteCount": 2,
        "content": "This question is not clear enough. \"If a pipeline stops\" can mean many different things."
      },
      {
        "date": "2024-04-24T06:20:00.000Z",
        "voteCount": 1,
        "content": "Schedule at 1 , repeat once, delay 60 minutes"
      },
      {
        "date": "2024-03-30T21:57:00.000Z",
        "voteCount": 1,
        "content": "I'll go for A based on the use cases provided Microsoft Data Platform MVP:\nhttps://www.cathrinewilhelmsen.net/triggers-azure-data-factory/"
      },
      {
        "date": "2024-02-16T11:54:00.000Z",
        "voteCount": 1,
        "content": "You set up a tumbling window trigger with a window size of 1 hour and a start time of 2 AM.\nThe trigger fires automatically at 2 AM every hour.\nIf the trigger encounters an outage and misses a window (e.g., cannot execute at 2 AM), it automatically resumes at the beginning of the next window (i.e., at 3 AM).\nThis ensures that the pipeline executes at 2 AM after the restart, effectively catching up for the missed execution."
      },
      {
        "date": "2024-02-06T01:37:00.000Z",
        "voteCount": 1,
        "content": "in this scenario, the requirement is to execute the pipeline at a specific time every day (2 AM) and ensure that if the trigger stops, the next pipeline execution occurs at 2 AM following a restart of the trigger. A tumbling window trigger might not guarantee that the pipeline always runs at exactly 2 AM daily, especially if the trigger stops and restarts at different times.\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#tumbling-window-trigger\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#schedule-trigger-with-json"
      },
      {
        "date": "2024-02-02T18:37:00.000Z",
        "voteCount": 1,
        "content": "The answer is B\nTumbling window trigger is more specific .The pipeline is trigger every 24hrs which is how a tumbling window funtion works.\nScheduler is general .\nOne can say that a  schedule trigger is called Tumbling trigger when it works at non overlapping equal intervals of time just as in the pipeline case study"
      },
      {
        "date": "2024-02-02T18:30:00.000Z",
        "voteCount": 1,
        "content": "A\nTumbling widow trigger  is more specific. \nScheduler is general .\nI can say this Schedule trigger is called Tumbling  trigger.."
      },
      {
        "date": "2023-12-27T03:16:00.000Z",
        "voteCount": 3,
        "content": "The tumbling window trigger run waits for the triggered pipeline run to finish. Its run state reflects the state of the triggered pipeline run. For example, if a triggered pipeline run is cancelled, the corresponding tumbling window trigger run is marked cancelled. This is different from the \"fire and forget\" behavior of the schedule trigger, which is marked successful as long as a pipeline run started.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"
      },
      {
        "date": "2023-12-18T05:46:00.000Z",
        "voteCount": 1,
        "content": "I believe it's tumbling, It doesn't say that if the trigger fails from the get-go, but if it stops, and in schedule, the state of a pipe is successful if the pipeline ran at first\n\n\"The tumbling window trigger run waits for the triggered pipeline run to finish. Its run state reflects the state of the triggered pipeline run. For example, if a triggered pipeline run is cancelled, the corresponding tumbling window trigger run is marked cancelled. This is different from the \"fire and forget\" behavior of the schedule trigger, which is marked successful as long as a pipeline run started.\"\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/concepts-pipeline-execution-triggers#trigger-type-comparison"
      },
      {
        "date": "2023-11-10T09:05:00.000Z",
        "voteCount": 3,
        "content": "Looks to me like Tumbling"
      },
      {
        "date": "2023-10-09T05:56:00.000Z",
        "voteCount": 4,
        "content": "Schedule triggers allow you to schedule pipelines to run at specific times and intervals. They are also idempotent, which means that if a pipeline execution fails due to a trigger failure, the next pipeline execution will still occur at the scheduled time."
      },
      {
        "date": "2023-10-04T11:06:00.000Z",
        "voteCount": 1,
        "content": "\"following a restart of the trigger\" sounds a lot like a \"retry\" and only tumbling would offer a retry not schedule."
      },
      {
        "date": "2023-09-23T11:41:00.000Z",
        "voteCount": 4,
        "content": "I would say tumbling with a max concurrency = 1. in this way if the first run doesn't stop the second one will not start"
      },
      {
        "date": "2023-09-08T03:49:00.000Z",
        "voteCount": 2,
        "content": "Schedule"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117659-exam-dp-203-topic-1-question-94-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure data factory named adf1 that contains a pipeline named ExecProduct. ExecProduct contains a data flow named Product.<br><br>The Product data flow contains the following transformations:<br><br>1. WeeklyData: A source that points to a CSV file in an Azure Data Lake Storage Gen2 account with 20 columns<br>2. ProductColumns: A select transformation that selects from WeeklyData six columns named ProductID, ProductDescr, ProductSubCategory, ProductCategory, ProductStatus, and ProductLastUpdated<br>3. ProductRows: An aggregate transformation<br>4. ProductList: A sink that outputs data to an Azure Synapse Analytics dedicated SQL pool<br><br>The Aggregate settings for ProductRows are configured as shown in the following exhibit.<br><br><img src=\"https://img.examtopics.com/dp-203/image332.png\"><br><br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image333.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image334.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-10T19:33:00.000Z",
        "voteCount": 25,
        "content": "Yes , no, yes - https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"
      },
      {
        "date": "2023-08-09T05:15:00.000Z",
        "voteCount": 5,
        "content": "How can one know that the answer to the third question is either yes or no? In my understanding, you'd have to assume that there is no duplicates in the source table, i.e. there are no rows that share the same values in all columns except for productid"
      },
      {
        "date": "2024-07-25T00:50:00.000Z",
        "voteCount": 1,
        "content": "No, No, Yes."
      },
      {
        "date": "2024-05-01T02:59:00.000Z",
        "voteCount": 1,
        "content": "Yes, No, Yes"
      },
      {
        "date": "2024-03-26T02:53:00.000Z",
        "voteCount": 2,
        "content": "Yes, no, yes. You can take a look on https://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"
      },
      {
        "date": "2024-02-16T00:37:00.000Z",
        "voteCount": 4,
        "content": "How come we might have multiple product description for unique product id if we group by product id ? It must be yes yes yes"
      },
      {
        "date": "2024-02-17T05:22:00.000Z",
        "voteCount": 2,
        "content": "I also think it's yes, yes, yes. We're grouping by ProductID and select the first/max ProductDescr per ProductID, like so:\nselect ProductID, first/max(ProductDescr) as FirstProductDescr\nfrom ProductColumns\ngroup by ProductID"
      },
      {
        "date": "2024-02-10T23:38:00.000Z",
        "voteCount": 1,
        "content": "What if product id is duplicate i.e. for different combinations of the other columns we have the same product ID.. in that case, we will have multiple records for same product id even after aggregation and so answer to third Q would be No"
      },
      {
        "date": "2024-02-02T19:40:00.000Z",
        "voteCount": 1,
        "content": "Yes / No / Yes"
      },
      {
        "date": "2024-01-30T05:55:00.000Z",
        "voteCount": 2,
        "content": "YES. NO. YES"
      },
      {
        "date": "2024-01-22T14:12:00.000Z",
        "voteCount": 2,
        "content": "No - Input Schema is same as output schema, since there is no derived column to hold the aggregate value ( Function \"first\" -returns the fist row among duplicate grouping key ie thge first unique row Product Id in this example ). Input Schema will be same as Output Schema as the aggregator in this case is only used for removing duplicate rows by Product Id\nNo - Product Description is not a group by column, hence there is no guarantee it will be unique.\nYes - Product Id is the grouping Key hence the out will only have 1 unique row per Product Id"
      },
      {
        "date": "2024-04-24T06:35:00.000Z",
        "voteCount": 1,
        "content": "Your explanation is correct for the first but you  misscounted the columns - there are 6; Y N Y"
      },
      {
        "date": "2024-04-30T10:50:00.000Z",
        "voteCount": 1,
        "content": "What about the column qualifier !=ProductID  ?  I think that makes it 5 in the return"
      },
      {
        "date": "2023-12-27T03:32:00.000Z",
        "voteCount": 1,
        "content": "1 - Yes, group by productId and then column pattern match the others results in 1 + 5 = 6 columns\n2 - No, for aggregation using a group by clause we get one row for each unique value that we group by.\n3 - Yes, the opposite compared to (2), since we are actually grouping by productId now."
      },
      {
        "date": "2023-12-07T02:25:00.000Z",
        "voteCount": 2,
        "content": "1- Yes \n2- No \n3- Yes \nwe are not creating new column, using aggregate function for  deduplication\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"
      },
      {
        "date": "2023-11-27T08:15:00.000Z",
        "voteCount": 1,
        "content": "The syntax mentions name!= which means \"not equal to\" ProductID, so my answer would be: yes, yes, no"
      },
      {
        "date": "2023-12-17T04:49:00.000Z",
        "voteCount": 2,
        "content": "Upon second glance my assumption was incorrect; the \"first\" pattern applies to all rows who's name is not ProductID. ProductID itself is the grouping column, so will return one output row for each unique value. ProductDescr., however, is part of the other columns so here it's the combination of these columns that has to be unique (first row of this combination of values is returned, the rest is dropped). The ProductDescr. column itself can generate more rows per unique value. So answer should be Yes, No, Yes."
      },
      {
        "date": "2023-09-22T05:12:00.000Z",
        "voteCount": 3,
        "content": "1. No There will be 7 in output (6 are at input)\n2. No We haven't tested anything by ProductDescr\n3. Yes We have grouped by ProductID and added new column"
      },
      {
        "date": "2023-10-04T11:30:00.000Z",
        "voteCount": 3,
        "content": "Did we add a new column tho? $$ -&gt; first($$) would just remove duplicate rows in ProductID and only keep the first value that is encountered. So it should be yes, no, yes.\nhttps://learn.microsoft.com/en-us/azure/data-factory/data-flow-aggregate"
      },
      {
        "date": "2023-09-04T03:29:00.000Z",
        "voteCount": 4,
        "content": "yes, no, yes"
      },
      {
        "date": "2023-08-26T00:19:00.000Z",
        "voteCount": 3,
        "content": "Yes , no, yes"
      },
      {
        "date": "2023-08-18T04:13:00.000Z",
        "voteCount": 4,
        "content": "I have notice that some answers might be wrong. What does this mean? who is confirming the correct answers ?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117702-exam-dp-203-topic-1-question-95-discussion/",
    "body": "You manage an enterprise data warehouse in Azure Synapse Analytics.<br><br>Users report slow performance when they run commonly used queries. Users do not report performance changes for infrequently used queries.<br><br>You need to monitor resource utilization to determine the source of the performance issues.<br><br>Which metric should you monitor?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDWU limit",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCache hit percentage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLocal tempdb percentage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tData IO percentage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-08-09T17:30:00.000Z",
        "voteCount": 5,
        "content": "Correct"
      },
      {
        "date": "2024-01-18T09:32:00.000Z",
        "voteCount": 1,
        "content": "Got this question on my exam on january 17, answer B is correct."
      },
      {
        "date": "2023-10-16T16:08:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-how-to-monitor-cache"
      },
      {
        "date": "2023-10-04T11:35:00.000Z",
        "voteCount": 3,
        "content": "B. Cache hit percentage should be correct since it only affects common used queries, which should be saved and loaded from cache."
      },
      {
        "date": "2023-09-04T03:30:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/microsoft/view/117112-exam-dp-203-topic-1-question-96-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics serverless SQL pool.<br><br>You have an Apache Parquet file that contains 10 columns.<br><br>You need to query data from the file. The solution must return only two columns.<br><br>How should you complete the query? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image335.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image336.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-02T06:25:00.000Z",
        "voteCount": 15,
        "content": "correct!\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset#data-source"
      },
      {
        "date": "2023-09-04T03:31:00.000Z",
        "voteCount": 4,
        "content": "SELECT *\nFROM OPENROWSET(BULK 'http://&lt;storage account&gt;.dfs.core.windows.net/container/folder/*.parquet',\n                FORMAT = 'PARQUET') AS [file]"
      },
      {
        "date": "2023-08-04T01:46:00.000Z",
        "voteCount": 4,
        "content": "Correct, Serverless SQL pool uses BULK."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125496-exam-dp-203-topic-1-question-97-discussion/",
    "body": "You have an Azure Synapse Analytics workspace that contains an Apache Spark pool named SparkPool1. SparkPool1 contains a Delta Lake table named SparkTable1.<br><br>You need to recommend a solution that supports Transact-SQL queries against the data referenced by SparkTable1. The solution must ensure that the queries can use partition elimination.<br><br>What should you include in the recommendation?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta partitioned table in a dedicated SQL pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta partitioned view in a dedicated SQL pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta partitioned index in a dedicated SQL pool",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta partitioned view in a serverless SQL pool\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T02:53:00.000Z",
        "voteCount": 2,
        "content": "from this link : https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\nPartition elimination is available only in the partitioned tables created on Parquet or CSV formats that are synchronized from Apache Spark pools. You might create external tables on Parquet partitioned folders, but the partitioning columns are inaccessible and ignored, while the partition elimination won't be applied. Don't create external tables on Delta Lake folders because they aren't supported. Use Delta partitioned views if you need to query partitioned Delta Lake data."
      },
      {
        "date": "2024-05-01T03:21:00.000Z",
        "voteCount": 1,
        "content": "D is correct\nthe keyword here is: \"Delta Lake table named SparkTable1\"\nhere is how partitioned view works:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-views#partitioned-views\nAnd this link shows that dedicated pool does not talk to delta table:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"
      },
      {
        "date": "2024-04-13T21:56:00.000Z",
        "voteCount": 1,
        "content": "Partition elimination is available only in the partitioned tables created on Parquet or CSV formats that are synchronized from Apache Spark pools.\"https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n\""
      },
      {
        "date": "2024-03-26T03:15:00.000Z",
        "voteCount": 4,
        "content": "Only partitioned Table can support partition elimination, so the answer should be A"
      },
      {
        "date": "2024-02-21T04:37:00.000Z",
        "voteCount": 1,
        "content": "The folder partition elimination is available in the native external tables that are synchronized from the Synapse Spark pools. If you have partitioned data set and you would like to leverage the partition elimination with the external tables that you create, use the partitioned views instead of the external tables."
      },
      {
        "date": "2024-02-03T19:33:00.000Z",
        "voteCount": 4,
        "content": "D is correct. \nThe solution is achieved by storing the delta lake  table  in sql spool ,so that transact-SQL query can be used   on it..The choice of sql pool to use is determined by the format of the data .which is Delta.This format (Delta) is only supported by severless sql pool,hence it will be  chosen for the job.Yes,it is important to state thay it can use the transct sql query through the openrowset function.\nThe next case in the solution is the partitioning part.\nAs a precaution from azure documentation  ,external tables in severless sql pool with delta format should not be partitioned .If a partition must be used for query optimization,then use , a partition view .This view also contain partition folders or rather support partition folder  elimination which is need for query optimization"
      },
      {
        "date": "2023-11-25T03:44:00.000Z",
        "voteCount": 2,
        "content": "Everything is mentioned here:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop"
      },
      {
        "date": "2023-12-20T13:57:00.000Z",
        "voteCount": 1,
        "content": "Quote from link provided suggesting D is correct\n\u201cuse the partitioned views instead of the external tables.\u201d"
      },
      {
        "date": "2023-11-05T23:55:00.000Z",
        "voteCount": 4,
        "content": "D is correct.\n\"The OPENROWSET function is not supported in dedicated SQL pools in Azure Synapse.\" so it eliminates A,B and C.\nRef: https://learn.microsoft.com/en-us/sql/t-sql/functions/openrowset-transact-sql?view=sql-server-ver16\nOnly the partitioned view in the serverless sql pool is correct since \"External tables in serverless SQL pools do not support partitioning on Delta Lake format. Use Delta partitioned views instead of tables if you have partitioned Delta Lake data sets.\"\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables#delta-tables-on-partitioned-folders"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125112-exam-dp-203-topic-1-question-98-discussion/",
    "body": "You are designing a sales transactions table in an Azure Synapse Analytics dedicated SQL pool. The table will contain approximately 60 million rows per month and will be partitioned by month. The table will use a clustered column store index and round-robin distribution.<br><br>Approximately how many rows will there be for each combination of distribution and partition?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1 million\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t5 million",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t20 million",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t60 million"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-11-01T08:30:00.000Z",
        "voteCount": 13,
        "content": "Partitioned by month and with 60 nodes, means it\u2019s 1M per combination"
      },
      {
        "date": "2024-04-14T07:14:00.000Z",
        "voteCount": 2,
        "content": "Should be A"
      },
      {
        "date": "2024-02-03T20:33:00.000Z",
        "voteCount": 4,
        "content": "Answer is A.\nEach partition has one million rows( minimum).\n*A partition with 1 distribution will have 1M rows \n* Partition with 2 distribution will have 2M rows\n----'\nPartition with 60 distributions will have 60M rows\n....Partition with n-distribution will have  n \u00d71M rows = n M rows  \nSo in general, if there is p-partitions and  d--distributions .then the combination of P and d describes the total rows or records (Q) which can be mathematical represented below \nQ = 1000000 * p *d \nFor each combination of partition  and  distribution, \nP = 1 and d = 1 \nTherefore ,\nThe total rows,Q = 1000000*1*1= 1M.\nN.B .1M rows is the minimum  per partion and distribution  required for a table compression and optimization in table  partitioning"
      },
      {
        "date": "2024-02-02T03:10:00.000Z",
        "voteCount": 2,
        "content": "Azure Synapse Analytics dedicated SQL pools distribute data across multiple distributions - by default, there are 60 distributions. Since the sales transactions table will be partitioned by month and will contain approximately 60 million rows per month, and considering round-robin distribution is used (which distributes rows evenly across all distributions), we can estimate the number of rows per combination of distribution and partition by dividing the total number of rows by the number of distributions.\n\nHere's the calculation:\n\n60 million rows per month / 60 distributions = 1 million rows per distribution per month.\n\nSo, the correct answer is:\n\nA. 1 million\n\nEach combination of distribution and partition (monthly in this case) would have approximately 1 million rows."
      },
      {
        "date": "2023-12-12T13:07:00.000Z",
        "voteCount": 1,
        "content": "60 nodes so 1M per distribiution and partition"
      },
      {
        "date": "2023-12-01T03:36:00.000Z",
        "voteCount": 1,
        "content": "1 Mio per combination"
      },
      {
        "date": "2023-11-28T02:52:00.000Z",
        "voteCount": 1,
        "content": "same as Blablatest123 said"
      },
      {
        "date": "2023-11-01T08:00:00.000Z",
        "voteCount": 2,
        "content": "Duplicate Question"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125515-exam-dp-203-topic-1-question-99-discussion/",
    "body": "You have an Azure Synapse Analytics workspace.<br><br>You plan to deploy a lake database by using a database template in Azure Synapse.<br><br>Which two elements are included in the template? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trelationships\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata formats",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tlinked services",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttable permissions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttable definitions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "AE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AE",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-11-06T04:45:00.000Z",
        "voteCount": 8,
        "content": "Correct, AE. Only table definition and their relationship is included in the template. The rest of the options should be configured\nRef: https://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/create-lake-database-from-lake-database-templates"
      },
      {
        "date": "2024-02-04T14:43:00.000Z",
        "voteCount": 1,
        "content": "Please can you explain this part of the template requirements.\nPrerequisites\nAt least Synapse User role permissions are required for exploring a lake database template from Gallery.\nSynapse Administrator, or Synapse Contributor permissions are required on the Synapse workspace for creating a lake database.\nStorage Blob Data Contributor permissions are required on data lake when using the create table From data lake option.\n\n\n.I copied directly from the link you dropped ."
      },
      {
        "date": "2024-05-01T03:28:00.000Z",
        "voteCount": 1,
        "content": "B and E 100%\nRelationships you have to define"
      },
      {
        "date": "2024-04-24T09:22:00.000Z",
        "voteCount": 1,
        "content": "B E ; relationships have to be added in the designer after tables are added"
      },
      {
        "date": "2024-02-02T20:39:00.000Z",
        "voteCount": 1,
        "content": "According to the Microsoft Learn documentation1, a lake database template in Azure Synapse includes the following elements:\n\nTable definitions: These are the schemas for the tables that store the data in the lake database. They include the table name, column names, data types, and descriptions.\nRelationships: These are the associations between the tables that define how they are related to each other. They include the primary and foreign keys, cardinality, and referential integrity rules.\nData formats: These are the specifications for how the data is stored and accessed in the lake database. They include the file format, compression type, delimiter, encoding, and schema inference options.\nTherefore, the correct answers are A and E."
      },
      {
        "date": "2024-01-16T06:51:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/microsoft/view/125516-exam-dp-203-topic-1-question-100-discussion/",
    "body": "You are implementing a star schema in an Azure Synapse Analytics dedicated SQL pool.<br><br>You plan to create a table named DimProduct.<br><br>DimProduct must be a Type 3 slowly changing dimension (SCD) table that meets the following requirements:<br><br>\u2022\tThe values in two columns named ProductKey and ProductSourceID will remain the same.<br>\u2022\tThe values in three columns named ProductName, ProductDescription, and Color can change.<br><br>You need to add additional columns to complete the following table definition.<br><br><img src=\"https://img.examtopics.com/dp-203/image347.png\"><br><br>Which three columns should you add? Each correct answer presents part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveStartDate] [datetime] NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[EffectiveEndDate] [datetime] NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[OriginalProductDescription] NVARCHAR(2000) NOT NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[IsCurrentRow] [bit] NOT NULL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[OriginalColor] NVARCHAR(50) NOT NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t[OriginalProductName] NVARCHAR(100) NULL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CEF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CEF",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-22T05:11:00.000Z",
        "voteCount": 5,
        "content": "CEF IS CORRECT"
      },
      {
        "date": "2024-06-21T02:44:00.000Z",
        "voteCount": 2,
        "content": "The key here is that the 3 columns can change. In Type SCD tables, we need to track the old and the new values by adding a new column that explicitly says it is the new value. For example, we will leave column Color and will add additional column with name NewColor."
      },
      {
        "date": "2024-04-24T09:26:00.000Z",
        "voteCount": 1,
        "content": "C D F is correct for type 3\nA B D would be type 2"
      },
      {
        "date": "2024-02-05T08:44:00.000Z",
        "voteCount": 2,
        "content": "correct answer"
      },
      {
        "date": "2024-01-22T05:08:00.000Z",
        "voteCount": 1,
        "content": "yes cef!"
      },
      {
        "date": "2023-12-12T13:11:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2023-12-04T02:20:00.000Z",
        "voteCount": 4,
        "content": "The proposed solution is incorrect and corresponds to a type 2 scd, not 3. For it to be a type 3 scd, the start and end date of the change is needed along with the current value."
      },
      {
        "date": "2023-12-12T10:03:00.000Z",
        "voteCount": 7,
        "content": "You dont need start and end date for a type 3 scd \n\nIn Type 3 Slowly Changing Dimension, there will be two columns to indicate the particular attribute of interest, one indicating the original value, and one indicating the current value.\n\nCEF is correct"
      },
      {
        "date": "2023-11-06T04:48:00.000Z",
        "voteCount": 3,
        "content": "Correct. The other three options are needed for a scd type 2 table."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130441-exam-dp-203-topic-1-question-101-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool named Pool1. Pool1 contains a fact table named Table1. Table1 contains sales data. Sixty-five million rows of data are added to Table1 monthly.<br><br>At the end of each month, you need to remove data that is older than 36 months. The solution must minimize how long it takes to remove the data.<br><br>How should you partition Table1, and how should you remove the old data? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image354.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image355.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-06T00:51:00.000Z",
        "voteCount": 10,
        "content": "Correct, dedicated SQL pool divides partitions into 60 databases and should aim for at least 1M rows per distribution (although not mentioned in the question, clustered columnstore compression becomes efficient at this scale). Therefore, partitioning by day would result in too small partitions.\n\nHaving partitioned by months, we can use switch to move it using metadata operations only, which makes the operation extremely efficient."
      },
      {
        "date": "2024-09-15T08:27:00.000Z",
        "voteCount": 1,
        "content": "why not just truncate the oldest partition"
      },
      {
        "date": "2024-07-08T16:41:00.000Z",
        "voteCount": 1,
        "content": "Should you migrate the partition another table, delete the data from the main table, migrate again the partition and the drop table2?"
      },
      {
        "date": "2024-04-24T09:28:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-04-23T06:32:00.000Z",
        "voteCount": 1,
        "content": "correct\n!"
      },
      {
        "date": "2024-03-27T14:24:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2024-01-28T03:15:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130442-exam-dp-203-topic-1-question-102-discussion/",
    "body": "You have an Azure subscription that contains an Azure Synapse Analytics serverless SQL pool.<br><br>You execute the following query.<br><br><img src=\"https://img.examtopics.com/dp-203/image356.png\"><br><br>Where will the rows returned by the query be stored?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tin a file in a data lake\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tin a relational database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tin a global temporary table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tin a session temporary table"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T01:01:00.000Z",
        "voteCount": 5,
        "content": "Correct:\n\nServerless SQL pools can query, import, and store data from Azure Blob Storage, Azure Data Lake Storage Gen1 and Gen2. Serverless does not support TYPE=Hadoop.\n\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#overview-azure-synapse-analytics"
      },
      {
        "date": "2024-05-21T13:13:00.000Z",
        "voteCount": 1,
        "content": "How do we know this is ADLS (data lake) storage? Can't it be plain blob storage as well?"
      },
      {
        "date": "2024-04-23T06:39:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-02-29T06:07:00.000Z",
        "voteCount": 2,
        "content": "their is no Tables with servless pool"
      },
      {
        "date": "2024-04-09T05:35:00.000Z",
        "voteCount": 1,
        "content": "You are right. We can't create the table in serverless SQL pool."
      },
      {
        "date": "2024-02-18T04:50:00.000Z",
        "voteCount": 3,
        "content": "Correct\n\n\"DATA_SOURCE = external_data_source_name\nSpecifies the name of the external data source that contains the location of the external data. This location is in Azure Data Lake\"\n\nhttps://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#overview-azure-synapse-analytics"
      },
      {
        "date": "2024-01-28T03:16:00.000Z",
        "voteCount": 1,
        "content": "A is the correct one"
      },
      {
        "date": "2024-01-14T18:33:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-01-13T05:37:00.000Z",
        "voteCount": 1,
        "content": "Bonne r\u00e9ponse : A. Lien est : https://learn.microsoft.com/en-us/sql/t-sql/statements/create-external-table-transact-sql?view=azure-sqldw-latest&amp;tabs=dedicated#overview-azure-synapse-analytics"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130335-exam-dp-203-topic-1-question-103-discussion/",
    "body": "You are deploying a lake database by using an Azure Synapse database template.<br><br>You need to add additional tables to the database. The solution must use the same grouping method as the template tables.<br><br>Which grouping method should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tpartition style",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbusiness area\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsize",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tfacts and dimensions"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-28T03:19:00.000Z",
        "voteCount": 14,
        "content": "Azure Synapse database templates are organized by \"business areas,\" which are broader groupings that include various tables related to specific aspects of a business or industry\u200b. \n\nWhile \"facts and dimensions\" are a part of the overall data modeling process within these business areas, the question specifically asks about the grouping method used in the template tables. Therefore, the grouping method for adding additional tables to an Azure Synapse database template would align with the business areas defined within the template, making option B, \"business area,\" the most appropriate choice given the context of the question and the structure of the Synapse database templates."
      },
      {
        "date": "2024-03-20T14:10:00.000Z",
        "voteCount": 3,
        "content": "top explanation, thanks"
      },
      {
        "date": "2024-01-16T06:58:00.000Z",
        "voteCount": 6,
        "content": "The answer is right below the first image in the documentation:\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/overview-database-templates"
      },
      {
        "date": "2024-04-24T09:31:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-14T07:31:00.000Z",
        "voteCount": 1,
        "content": "It's B"
      },
      {
        "date": "2024-01-16T10:54:00.000Z",
        "voteCount": 1,
        "content": "I think it should be D also."
      },
      {
        "date": "2024-01-13T05:45:00.000Z",
        "voteCount": 1,
        "content": "R\u00e9ponse : B Ref : https://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/overview-database-templates"
      },
      {
        "date": "2024-01-11T05:19:00.000Z",
        "voteCount": 3,
        "content": "jongert is right, should be B"
      },
      {
        "date": "2024-01-10T00:36:00.000Z",
        "voteCount": 2,
        "content": "B. business area\n\nThis is a common approach where tables are grouped based on the business areas or domains they belong to, making it easier to manage, organize, and understand the structure of the database."
      },
      {
        "date": "2024-01-06T01:05:00.000Z",
        "voteCount": 4,
        "content": "Correct\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/database-designer/overview-database-templates"
      },
      {
        "date": "2024-01-04T07:33:00.000Z",
        "voteCount": 1,
        "content": "Why? Shouldn't it be D?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130444-exam-dp-203-topic-1-question-104-discussion/",
    "body": "You have an Azure data factory connected to a Git repository that contains the following branches:<br><br>\u2022\tmain: Collaboration branch<br>\u2022\tabc: Feature branch<br>\u2022\txyz: Feature branch<br><br>You save changes to a pipeline in the xyz branch.<br><br>You need to publish the changes to the live service.<br><br>What should you do first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublish the data factory.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull request to merge the changes into the main branch.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pull request to merge the changes into the abc branch.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPush the code to a remote origin."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-09T04:23:00.000Z",
        "voteCount": 1,
        "content": "Correct . C'est la premi\u00e8re \u00e9tape appropri\u00e9e. Vous devez cr\u00e9er une pull request pour fusionner vos modifications de la branche xyz dans la branche main. Une fois que cette PR est approuv\u00e9e et fusionn\u00e9e, vous pourrez publier les changements dans le service en direct."
      },
      {
        "date": "2024-07-26T14:59:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-07-13T08:49:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2024-04-16T11:10:00.000Z",
        "voteCount": 1,
        "content": "Save changes to a pipeline in the xyz branch means that the changes are Published in the data factory"
      },
      {
        "date": "2024-04-14T07:35:00.000Z",
        "voteCount": 1,
        "content": "Why not A? You should Publish the save changes before every pull requests"
      },
      {
        "date": "2024-02-02T13:07:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-01-14T18:37:00.000Z",
        "voteCount": 3,
        "content": "Correct, always merge the feature into to the main branch and then publish the main branch"
      },
      {
        "date": "2024-01-13T05:52:00.000Z",
        "voteCount": 1,
        "content": "Bonne r\u00e9ponse : B."
      },
      {
        "date": "2024-01-06T01:08:00.000Z",
        "voteCount": 3,
        "content": "Correct, simply best practices for version control. Each feature branch develops changes, then should create pull requests to merge with main branch before publishing."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130446-exam-dp-203-topic-1-question-105-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have an Azure subscription that contains an Azure data factory named ADF1.<br><br>From Azure Data Factory Studio, you build a complex data pipeline in ADF1.<br><br>You discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.<br><br>You need to ensure that you can save the logic of the pipeline.<br><br>Solution: You enable Git integration for ADF1.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T09:11:00.000Z",
        "voteCount": 9,
        "content": "Partial saves: \"Whether your pipelines are not finished or you simply don't want to lose changes if your computer crashes, git integration allows for incremental changes of data factory resources regardless of what state they are in.\"\nhttps://learn.microsoft.com/en-us/azure/data-factory/source-control"
      },
      {
        "date": "2024-01-17T03:02:00.000Z",
        "voteCount": 6,
        "content": "Git configuration is already enabled otherwise the \"Save\" option would be a \"Publish\" option."
      },
      {
        "date": "2024-05-19T22:06:00.000Z",
        "voteCount": 1,
        "content": "It says the Save option is unavailable, which can mean its not present. In that case, it implies that Git is not configured"
      },
      {
        "date": "2024-05-24T06:51:00.000Z",
        "voteCount": 1,
        "content": "Man, I feel like this is a trick question with the wording lol. Depends on how you interpret the word \"unavailable\". I think_Ahan_ might be right with the word \"unavailable\" meaning not present. Anyways, gonna do some more research on it!"
      },
      {
        "date": "2024-09-05T04:36:00.000Z",
        "voteCount": 1,
        "content": "The answer is YES: \"When authoring against the data factory service, you can't save changes as a draft and all publishes must pass data factory validation. Whether your pipelines are not finished or you simply don't want to lose changes if your computer crashes, git integration allows for incremental changes of data factory resources regardless of what state they are in. Configuring a git repository allows you to save changes, letting you only publish when you have tested your changes to your satisfaction.\"\n\nhttps://learn.microsoft.com/en-us/answers/questions/391324/save-option-not-enabled"
      },
      {
        "date": "2024-07-13T08:51:00.000Z",
        "voteCount": 1,
        "content": "Don't overthink it, you enable git so you can save it. This is not about fixing the problem just making sure we can save what we have so far."
      },
      {
        "date": "2024-06-16T13:01:00.000Z",
        "voteCount": 4,
        "content": "Answer is A 100%.\nReason: Save button is unavailable means git is not enabled or I say you are in live mode you will see only Publish icon. In that case there is no way you can Publish the code. So you need to enable the GIT to save the code, but switching to git mode from live mode with remove the changes in live mode, so go to json view and copy the code. Discard changes switch to git mode, create new pipeline and paste the copied json in the json view and you can save the code now"
      },
      {
        "date": "2024-07-02T03:47:00.000Z",
        "voteCount": 1,
        "content": "Explained very well."
      },
      {
        "date": "2024-05-17T13:23:00.000Z",
        "voteCount": 1,
        "content": "Enabling Git Integration is just the first step. Doesn't necessarily allow to have a backup. JSON file on the other hand its different"
      },
      {
        "date": "2024-04-26T13:20:00.000Z",
        "voteCount": 3,
        "content": "B. No\n\nEnabling Git integration for ADF1 wouldn't directly address the immediate need to save the logic of the pipeline if the Save button is unavailable due to validation errors. Git integration allows for version control and collaboration but doesn't inherently resolve the issue of saving the pipeline's logic when there are validation errors preventing it from being published. You'd still need to address the validation errors before being able to successfully save the pipeline."
      },
      {
        "date": "2024-04-14T07:37:00.000Z",
        "voteCount": 1,
        "content": "It's A"
      },
      {
        "date": "2024-03-27T00:24:00.000Z",
        "voteCount": 1,
        "content": "Enabling Git integration for Azure Data Factory (ADF) does not directly address the issue of validation errors preventing the pipeline from being published. Git integration allows you to manage your ADF resources using source control and enables collaboration among multiple developers. It doesn't inherently fix validation errors."
      },
      {
        "date": "2024-03-05T05:25:00.000Z",
        "voteCount": 2,
        "content": "As stated, this question is a bit misleading. \"You need to ensure that you can save the logic of the pipeline\" means \"you have to prevent the situation in which already working pipelines are damaged by buggy implementations\". If you have the working, not buggy, code already inside the repository, you can create a new branch and do the updates there. If there's a error, it is contained inside the feature branch and not inside the main branch, which keeps working fine."
      },
      {
        "date": "2024-03-02T00:20:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A"
      },
      {
        "date": "2024-02-02T03:24:00.000Z",
        "voteCount": 2,
        "content": "No, enabling Git integration alone does not directly address the immediate goal of saving the logic of the pipeline and resolving validation errors in Azure Data Factory Studio.\nYou need to fix the validation errors and save the changes locally within the studio before enabling Git integration for long-term version control."
      },
      {
        "date": "2024-01-16T07:00:00.000Z",
        "voteCount": 4,
        "content": "Although the validation might be wrong, enablng the git integration allows you to save your datasets, pipelines and everything else as json files in your git repo, so A) is correct."
      },
      {
        "date": "2024-01-06T01:13:00.000Z",
        "voteCount": 3,
        "content": "Safe to assume that enabling git integration also means setting up the repo and branches, then it would allow saving. Would answer yes."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130336-exam-dp-203-topic-1-question-106-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have an Azure subscription that contains an Azure data factory named ADF1.<br><br>From Azure Data Factory Studio, you build a complex data pipeline in ADF1.<br><br>You discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.<br><br>You need to ensure that you can save the logic of the pipeline.<br><br>Solution: You view the JSON code representation of the resource and copy the JSON to a file.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-08T03:38:00.000Z",
        "voteCount": 6,
        "content": "B. No\n\nThe Save button being unavailable and validation errors preventing the pipeline from being published indicate issues with the current configuration or logic of the pipeline within Azure Data Factory Studio. Copying the JSON code to a file won't resolve the validation errors or allow you to save the pipeline."
      },
      {
        "date": "2024-01-10T00:39:00.000Z",
        "voteCount": 7,
        "content": "Please ignore my comment its wrong"
      },
      {
        "date": "2024-07-13T08:55:00.000Z",
        "voteCount": 1,
        "content": "Horrible question, but yes you technically can store it as JSON to keep the logic and you're work."
      },
      {
        "date": "2024-03-27T16:58:00.000Z",
        "voteCount": 1,
        "content": "\"You view the JSON code representation of the RESOURCE and copy the JSON to a file\".\nwhat \"resource\" are we talking about? If the JSON of the \"pipeline resource\" was specified, I would be 50/50% as saving a JSON achives this but isn't best practice. \nHowever here the proposed solution is intentionally vague, doesn't mention that the full json pipeline will be saved, could be another \"resource\" within the pipeline. Going for Nope"
      },
      {
        "date": "2024-03-27T17:01:00.000Z",
        "voteCount": 1,
        "content": "Moreover, the first correct variation is much more specific in its solution, mentioning the correct name of the resource: \n\"Solution: You enable Git integration for ADF1\"."
      },
      {
        "date": "2024-03-27T00:25:00.000Z",
        "voteCount": 2,
        "content": "Viewing the JSON code representation of the pipeline and copying it to a file can help preserve the logic of the pipeline, even if the Save button is unavailable due to validation errors. This allows you to retain the pipeline configuration and logic for future reference or for manual editing to address the validation errors. While it doesn't directly fix the validation errors, it ensures that you have a backup of the pipeline definition."
      },
      {
        "date": "2024-02-07T02:33:00.000Z",
        "voteCount": 1,
        "content": "A is right answer\n\nGet up-to-date https://www.pinterest.com/pin/937522847419270211/"
      },
      {
        "date": "2024-02-06T17:37:00.000Z",
        "voteCount": 1,
        "content": "A is correct.\nThe solution only aims at preserving the logic of the code .So viewing and copying the JSON code  to another file will support  versioning through partial saves which is required for securing the logic. of the code.\nN.B\nThe acceptable solution in Azure is through the provisioning of the git repository which helps in source control,versioning ,collaboration etc."
      },
      {
        "date": "2024-02-02T03:26:00.000Z",
        "voteCount": 1,
        "content": "Yes, it works fine"
      },
      {
        "date": "2024-01-27T00:31:00.000Z",
        "voteCount": 1,
        "content": "I'm going with A. Yes you can capture the logic using JSON but the validation errors will still persist. Question did not state if it should be error-free or not after capturing the logic, just whether it would do the job of saving the logic."
      },
      {
        "date": "2024-01-25T22:19:00.000Z",
        "voteCount": 1,
        "content": "Question asks how to save logic. This would work."
      },
      {
        "date": "2024-01-16T07:04:00.000Z",
        "voteCount": 2,
        "content": "Maybe you can save manually your json by copying the content to your local machine or something like that, but in an Azure context, I think that the question implies that we are using a solution involving Azure technology.\nCan you copy your json contents to a file? Yes of course.\nDoes that enable the Save button? No, it doesn't. It is not the best practice, so I'm going with a No."
      },
      {
        "date": "2024-01-14T18:44:00.000Z",
        "voteCount": 1,
        "content": "Yes, it would theoretically work, but it is not a good idea."
      },
      {
        "date": "2024-01-14T04:42:00.000Z",
        "voteCount": 3,
        "content": "Anwser should be \"Yes\", and because of the phrase: \"You need to ensure that you can save the logic of the pipeline.\". This means you have to save the logic of the pipeline, and not the pipeline itself. This won't, however, resolve the issues and errors, but it will provide you with a back-up of your work so far."
      },
      {
        "date": "2024-01-13T06:01:00.000Z",
        "voteCount": 1,
        "content": "Bonne r\u00e9ponse : B-----&gt; NON"
      },
      {
        "date": "2024-01-10T00:39:00.000Z",
        "voteCount": 1,
        "content": "Yes, viewing the JSON code representation of the resource and copying the JSON to a file is a valid approach to save the logic of the pipeline. The JSON code represents the configuration and structure of the data pipeline, and you can store it in a file for future use or reference. While it doesn't directly address the validation errors preventing publishing, it allows you to preserve the logic of the pipeline."
      },
      {
        "date": "2024-01-06T01:33:00.000Z",
        "voteCount": 3,
        "content": "The JSON file contains the logic of the pipeline and configurations such as paths. It should achieve the goal, although it would not be best practice."
      },
      {
        "date": "2024-01-04T07:36:00.000Z",
        "voteCount": 2,
        "content": "Yes it should work"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130337-exam-dp-203-topic-1-question-107-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have an Azure subscription that contains an Azure data factory named ADF1.<br><br>From Azure Data Factory Studio, you build a complex data pipeline in ADF1.<br><br>You discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.<br><br>You need to ensure that you can save the logic of the pipeline.<br><br>Solution: You export ADF1 as an Azure Resource Manager (ARM) template.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-27T00:37:00.000Z",
        "voteCount": 8,
        "content": "This has to be B. If you have experience working with Azure before you will know that it is impossible to export anything as an ARM template with validation errors.\n\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/troubleshooting/error-invalid-template?tabs=bicep\n\nScroll down to \"Solution 3\"."
      },
      {
        "date": "2024-02-17T09:56:00.000Z",
        "voteCount": 2,
        "content": "Solution 3 is about an error during deployment. The question is about a validation error so, I don't see how this comment applies to the question."
      },
      {
        "date": "2024-01-25T22:27:00.000Z",
        "voteCount": 7,
        "content": "You can't export an ARM Template with validation errors. I don't know why some people voted yes on this one."
      },
      {
        "date": "2024-07-13T08:58:00.000Z",
        "voteCount": 2,
        "content": "From Azure documentation:\nWhen exporting from a resource group or resource, the exported template is generated from the published schema\nWe can't publish with errors so this cannot be done with ARM template export.\nhttps://learn.microsoft.com/en-us/azure/azure-resource-manager/templates/export-template-portal"
      },
      {
        "date": "2024-06-09T01:26:00.000Z",
        "voteCount": 1,
        "content": "B is correct. ARM template will only have latest Pipeline work only when published. Since the pipeline has validation errors it can't be published."
      },
      {
        "date": "2024-04-12T14:19:00.000Z",
        "voteCount": 1,
        "content": "Yes, you can export Azure Data Factory (ADF1) pipelines, datasets, linked services, and other artifacts as Azure Resource Manager (ARM) templates. This process allows you to capture the configuration of your data factory in a JSON-based template format. You can then use this template to automate deployment, manage version control, or replicate your data factory across different environments.\n\nExporting an ADF1 data factory as an ARM template can be done from the Azure portal. Simply navigate to your data factory, select the \"Author\" tab, and then click on \"Export ARM template\" from the menu. This will generate an ARM template containing the definition of your data factory and its components, which you can download and use as needed."
      },
      {
        "date": "2024-03-27T00:26:00.000Z",
        "voteCount": 1,
        "content": "Exporting the Azure Data Factory (ADF) as an Azure Resource Manager (ARM) template would capture the logic of the pipeline along with other ADF resources. This allows you to save the configuration and logic of the pipeline in a structured format, even if the Save button is unavailable due to validation errors. While it doesn't directly fix the validation errors, it ensures that you have a backup of the pipeline definition in an ARM template, which can be modified and redeployed later."
      },
      {
        "date": "2024-02-06T17:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is B.\nThe ARM template is disabled or rather it is not available in ADF ,hence there is no template to export .This means that there no template to hold the logic of the program. \nHowever, when it is connected to the git,the template is enabled in there\n(lol..They are good marketers)"
      },
      {
        "date": "2024-02-03T06:27:00.000Z",
        "voteCount": 2,
        "content": "This will allow you to save the logic of the pipeline and make changes to it as needed"
      },
      {
        "date": "2024-01-28T18:11:00.000Z",
        "voteCount": 1,
        "content": "No, exporting ADF1 as an Azure Resource Manager (ARM) template does not meet the goal of ensuring that you can save the logic of the pipeline. ARM templates are used to deploy resources, not to save the logic of a data pipeline. To save the logic of the pipeline, you need to resolve the validation errors that are preventing the pipeline from being published. Once the errors are resolved, the Save button will become available, and you can save the pipeline."
      },
      {
        "date": "2024-01-14T18:49:00.000Z",
        "voteCount": 1,
        "content": "answer is A as it is possible to use an ARM template to save the logic, but it's not necessarily best practice when you could use a git repo instead"
      },
      {
        "date": "2024-01-08T03:40:00.000Z",
        "voteCount": 3,
        "content": "A. Yes\n\nExporting ADF1 as an Azure Resource Manager (ARM) template will capture the logic of the pipeline in JSON format. Even if the Save button is unavailable in the Azure Data Factory Studio due to validation errors, exporting the ARM template allows you to save the pipeline logic in a file. You can then review and edit the JSON code to correct the validation errors and redeploy the updated ARM template to resolve the issues."
      },
      {
        "date": "2024-01-04T07:36:00.000Z",
        "voteCount": 4,
        "content": "From chat gpt: \nYes, exporting the Azure Data Factory (ADF1) as an Azure Resource Manager (ARM) template can meet the goal of ensuring that you can save the logic of the pipeline, even when the Save button is unavailable due to validation errors.\n\nWhen you export the Azure Data Factory as an ARM template, it captures the entire structure and configuration of the Data Factory, including pipelines, datasets, linked services, triggers, and other artifacts in JSON format. This exported ARM template serves as a backup or snapshot of your Data Factory configuration.\n\nTherefore, by exporting ADF1 as an ARM template, you create a backup of the entire Data Factory structure, including the complex data pipeline that you built. This allows you to save the logic of the pipeline, despite the Save button being unavailable due to validation errors. Later, you can rectify the issues causing validation errors and re-import the updated ARM template to restore the logic of the pipeline."
      },
      {
        "date": "2024-01-06T01:37:00.000Z",
        "voteCount": 3,
        "content": "Agree, also the MS documentation for it.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/continuous-integration-delivery-manual-promotion"
      },
      {
        "date": "2024-01-10T00:42:00.000Z",
        "voteCount": 1,
        "content": "chatgpt \nB. No\n\nExporting ADF1 as an Azure Resource Manager (ARM) template is not a direct solution to saving the logic of the pipeline in the Azure Data Factory Studio when the Save button is unavailable due to validation errors. Exporting as an ARM template is typically done for versioning, source control, or deployment purposes, and it does not directly address the issue of saving the pipeline logic within the Data Factory Studio interface.\n\nThe suggested approach in the scenario would be to address and resolve the validation errors preventing the pipeline from being published, allowing you to save the changes within the Azure Data Factory Studio. Once the validation errors are fixed, you should be able to save and publish the pipeline without exporting it as an ARM template."
      },
      {
        "date": "2024-02-18T05:19:00.000Z",
        "voteCount": 2,
        "content": "It's correct, you shouldn't be using ARM, but it works if you want to save the state of the pipeline. Gpt is saying that it's not the optimal way, and it should be done with Git Integration"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130450-exam-dp-203-topic-1-question-108-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Databricks workspace.<br><br>You read data from a CSV file by using a notebook, and then load the data to a DataFrame.<br><br>You need to add rows from the DataFrame to an existing Delta table by using Python code.<br><br>How should you complete the code? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image357.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image358.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-06T01:39:00.000Z",
        "voteCount": 8,
        "content": "Correct:\nAdd to a delta table =&gt; format is delta\nAdd the rows to existing table =&gt; append"
      },
      {
        "date": "2024-01-28T03:22:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      },
      {
        "date": "2024-03-27T14:39:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-02-06T06:14:00.000Z",
        "voteCount": 3,
        "content": "correct\nhttps://learn.microsoft.com/en-us/training/modules/use-delta-lake-azure-synapse-analytics/3-create-delta-tables"
      },
      {
        "date": "2024-02-02T03:28:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130452-exam-dp-203-topic-1-question-109-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure subscription that contains an Azure Cosmos DB for NoSQL account named account1. The account1 account contains a container named Container1 that has the following configurations:<br>\u2022\tAnalytical store: On<br>\u2022\tTTL: 3600<br><br>You need to remove analytical store support from Container1. The solution must meet the following requirements:<br>\u2022\tMinimize the impact on the apps that reference Container1.<br>\u2022\tMinimize storage usage.<br><br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-203/image359.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image360.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-06T01:49:00.000Z",
        "voteCount": 8,
        "content": "Correct"
      },
      {
        "date": "2024-01-28T03:23:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2024-02-22T11:40:00.000Z",
        "voteCount": 2,
        "content": "agreed"
      },
      {
        "date": "2024-03-01T22:58:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2024-04-10T07:19:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2024-08-06T09:37:00.000Z",
        "voteCount": 1,
        "content": "agreed"
      },
      {
        "date": "2024-05-25T15:26:00.000Z",
        "voteCount": 2,
        "content": "Why couldn't we just simply turn off the analytical store on Container1?"
      },
      {
        "date": "2024-07-13T09:11:00.000Z",
        "voteCount": 1,
        "content": "Analytical store cannot be turned off after it has been enabled. A new container is needed to transfer"
      },
      {
        "date": "2024-03-27T14:51:00.000Z",
        "voteCount": 1,
        "content": "Correct way"
      },
      {
        "date": "2024-03-19T12:12:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/microsoft/view/131140-exam-dp-203-topic-1-question-110-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool named SQL1 that contains a hash-distributed fact table named Table1.<br><br>You need to recreate Table1 and add a new distribution column. The solution must maximize the availability of data.<br><br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br><br><img src=\"https://img.examtopics.com/dp-203/image361.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image362.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-01-13T19:37:00.000Z",
        "voteCount": 8,
        "content": "Answers are correct.\nDropping the index or running pdw_showspaceused is not required."
      },
      {
        "date": "2024-01-28T03:24:00.000Z",
        "voteCount": 2,
        "content": "Agreed!"
      },
      {
        "date": "2024-02-06T18:55:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/search/search-howto-reindex"
      },
      {
        "date": "2024-02-29T01:11:00.000Z",
        "voteCount": 1,
        "content": "Agreed!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130453-exam-dp-203-topic-1-question-111-discussion/",
    "body": "Note: This question is part of a series of questions that present the same scenario. Each question in the series contains a unique solution that might meet the stated goals. Some question sets might have more than one correct solution, while others might not have a correct solution.<br><br>After you answer a question in this section, you will NOT be able to return to it. As a result, these questions will not appear in the review screen.<br><br>You have an Azure subscription that contains an Azure data factory named ADF1.<br><br>From Azure Data Factory Studio, you build a complex data pipeline in ADF1.<br><br>You discover that the Save button is unavailable, and there are validation errors that prevent the pipeline from being published.<br><br>You need to ensure that you can save the logic of the pipeline.<br><br>Solution: You disable all the triggers for ADF1.<br><br>Does this meet the goal?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNo\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-28T03:26:00.000Z",
        "voteCount": 3,
        "content": "Agreed with the others: Git is needed to save the changes, and triggers have nothing to do with saving the pipeline logic. Answer is NO."
      },
      {
        "date": "2024-01-08T03:55:00.000Z",
        "voteCount": 1,
        "content": "IT NEED GIT CONFIG TO SAVE THE CHANGES"
      },
      {
        "date": "2024-01-06T01:53:00.000Z",
        "voteCount": 2,
        "content": "Correct, triggers have nothing to do with saving the pipeline logic."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141754-exam-dp-203-topic-1-question-112-discussion/",
    "body": "You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image382.png\"><br><br>You need to implement Azure Synapse Link for Azure SQL Database.<br><br>Which two actions should you perform on sql1? Each correct answer presents a part of the solution.<br><br>NOTE: Each correct selection is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the firewall rules to allow Azure services to access sql1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the system-assigned managed identity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Access control (IAM) settings, assign the Contributor role to the system-assigned managed identity of workspace1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable Transparent Data Encryption (TDE)."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-02T08:31:00.000Z",
        "voteCount": 7,
        "content": "I think the given answers are correct, Please refer the below link\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/synapse-link/connect-synapse-link-sql-database"
      },
      {
        "date": "2024-06-16T13:27:00.000Z",
        "voteCount": 4,
        "content": "Indeed it will be AB."
      },
      {
        "date": "2024-09-15T02:38:00.000Z",
        "voteCount": 1,
        "content": "To implement Azure Synapse Link for Azure SQL Database, you'll need to take the following actions on sql1:\n\nA. Update the firewall rules to allow Azure services to access sql1.\nThis is necessary because Azure Synapse Link requires the Azure Synapse workspace to be able to connect to the Azure SQL Database. Updating the firewall rules ensures that Azure services can access sql1.\n\nB. Enable the system-assigned managed identity.\nThis is important for security and authentication. The system-assigned managed identity allows Azure Synapse to authenticate to Azure SQL Database securely without needing to store credentials.\n\nSo, the correct actions are A and B."
      },
      {
        "date": "2024-09-03T22:00:00.000Z",
        "voteCount": 1,
        "content": "To implement Azure Synapse Link for Azure SQL Database, you need to perform the following actions on sql1:\n\nUpdate the firewall rules to allow Azure services to access sql1.\nAssign the Contributor role to the system-assigned managed identity of workspace1 from the Access control (IAM) settings."
      },
      {
        "date": "2024-08-13T23:54:00.000Z",
        "voteCount": 1,
        "content": "i think the answer is :\n A. Update the firewall rules to allow Azure services to access sql1.\nC. From the Access control (IAM) settings, assign the Contributor role to the system-assigned managed identity of workspace1."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141793-exam-dp-203-topic-1-question-113-discussion/",
    "body": "You have an Azure subscription that contains an Azure Cosmos DB database. Azure Synapse Link is implemented on the database.<br><br>You configure a full fidelity schema for the analytical store.<br><br>You perform the following actions:<br><br>\u2022\tInsert {\"customerID\": 12, \"customer\": \u201cTailspin Toys\"} as the first document in the container.<br>\u2022\tInsert {\"customerID\": \"14\", \"customer\": \"Contoso\"} as the second document in the container.<br><br>How many columns will the analytical store contain?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t3\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t4"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-02T23:47:00.000Z",
        "voteCount": 7,
        "content": "C - With a full fidelity schema, the analytical store will track both the data and their types accurately. This means different types for the same field will be stored in separate columns. Specifically:\n\nThere will be one column for customerID as an integer.\nThere will be another column for customerID as a string.\nThere will be one column for customer as a string."
      },
      {
        "date": "2024-06-16T13:32:00.000Z",
        "voteCount": 1,
        "content": "Answer C"
      },
      {
        "date": "2024-06-06T00:28:00.000Z",
        "voteCount": 3,
        "content": "The above explained is correct answer must be C. 3.\n12 and \"14\" being different datatypes will have separate columns in full fidelity schema."
      },
      {
        "date": "2024-08-14T02:30:00.000Z",
        "voteCount": 1,
        "content": "Because the customerID field appears with different data types (integer in the first and string in the second, the analytical store will treat these as separate columns to maintain the schema's fidelity."
      },
      {
        "date": "2024-07-13T09:24:00.000Z",
        "voteCount": 1,
        "content": "C, a third column will be added for the change in datatype."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141999-exam-dp-203-topic-1-question-114-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Lake Storage account that contains CSV files. The CSV files contain sales order data and are partitioned by using the following format.<br><br>/data/salesorders/year=xxxx/month=y<br><br>You need to retrieve only the sales orders from January 2023 and February 2023.<br><br>How should you complete the query? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image383.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image384.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2024-06-26T12:18:00.000Z",
        "voteCount": 1,
        "content": "SELECT\n    r.filepath() AS filepath\n    ,r.filepath(1) AS [year]\n    ,r.filepath(2) AS [month]\n    ,COUNT_BIG(*) AS [rows]\nFROM OPENROWSET(\n        BULK 'csv/taxi/yellow_tripdata_*-*.csv',\n        DATA_SOURCE = 'SqlOnDemandDemo',\n        FORMAT = 'CSV',\n        PARSER_VERSION = '2.0',        \n        FIRSTROW = 2\n    )\nWITH (\n    vendor_id INT\n) AS [r]\nWHERE\n    r.filepath(1) IN ('2017')\n    AND r.filepath(2) IN ('10', '11', '12')\nGROUP BY\n    r.filepath()\n    ,r.filepath(1)\n    ,r.filepath(2)\nORDER BY\n    filepath;"
      },
      {
        "date": "2024-06-06T00:41:00.000Z",
        "voteCount": 4,
        "content": "Answer is correct see below link for more details on filepath() function\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/query-specific-files#filepath"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "1"
  }
]