[
  {
    "topic": 3,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52952-exam-dp-203-topic-3-question-1-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Active Directory (Azure AD) tenant that contains a security group named Group1. You have an Azure Synapse Analytics dedicated SQL pool named dw1 that contains a schema named schema1.<br>You need to grant Group1 read-only permissions to all the tables and views in schema1. The solution must use the principle of least privilege.<br>Which three actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0028800001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0028900001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create a database user named dw1 that represents Group1 and use the FROM EXTERNAL PROVIDER clause.<br>Step 2: Create a database role named Role1 and grant Role1 SELECT permissions to schema1.<br>Step 3: Assign Role1 to the Group1 database user.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-share/how-to-share-from-sql",
    "votes": [],
    "comments": [
      {
        "date": "2021-05-17T07:45:00.000Z",
        "voteCount": 91,
        "content": "1. create user from external provider for Group1\n2. create Role1 with select on schema1\n3. add user to the Role1"
      },
      {
        "date": "2022-01-31T00:11:00.000Z",
        "voteCount": 2,
        "content": "add user to the role1 option isnt available in the given choices not sure why this answer is suggested then?  what is the need for creating external provider for Group1 can you explain?"
      },
      {
        "date": "2022-04-23T00:17:00.000Z",
        "voteCount": 1,
        "content": "UDEMY says this as well. So correct"
      },
      {
        "date": "2022-01-31T06:16:00.000Z",
        "voteCount": 4,
        "content": "Sorry, but \"add user to the Role1\" is not part of the answers. Or, which option is that?"
      },
      {
        "date": "2022-02-16T01:57:00.000Z",
        "voteCount": 4,
        "content": "Assign Role1 to the Group1 database user"
      },
      {
        "date": "2022-07-13T09:20:00.000Z",
        "voteCount": 9,
        "content": "Step 1: Create a database user named dw1 that represents Group1 and use the FROM EXTERNAL PROVIDER clause.\nStep 2: Create a database role named Role1 and grant Role1 SELECT permissions to schema1.\nStep 3: Assign Role1 to the Group1 database user."
      },
      {
        "date": "2021-07-17T06:07:00.000Z",
        "voteCount": 12,
        "content": "The suggested answer is wrong. As others have identified, the correct steps are;\n1. create user &lt;&gt; from external provider\n2. create role &lt;&gt; with select permission on schema\n3. add user to role"
      },
      {
        "date": "2021-09-23T08:24:00.000Z",
        "voteCount": 3,
        "content": "Agreed 100%"
      },
      {
        "date": "2022-06-16T00:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is not wrong. Read the question properly"
      },
      {
        "date": "2022-01-06T00:52:00.000Z",
        "voteCount": 3,
        "content": "Can somebody explain why we have to create the user first and not the role?"
      },
      {
        "date": "2022-03-15T23:16:00.000Z",
        "voteCount": 6,
        "content": "There is a note in the question that says \"More than one order of answer choices is correct\". Create role and create user can be interchanged."
      },
      {
        "date": "2022-03-18T04:03:00.000Z",
        "voteCount": 4,
        "content": "They do mention that: : More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.\n\nCreating user or role first does not matter. As long as you assign the role to the user in the end."
      },
      {
        "date": "2023-08-30T04:12:00.000Z",
        "voteCount": 3,
        "content": "1. create database user in dw1 that represent Group1 and uses From External Provider clause\n2. create database role named Role1 with grant Role1 select permission on dw1\n3. add Role1 to Group1 database user"
      },
      {
        "date": "2023-07-18T10:14:00.000Z",
        "voteCount": 1,
        "content": "CREATE USER security_group_lk FROM EXTERNAL PROVIDER;\nCREATE ROLE security_group_role;\nGRANT SELECT ON SCHEMA::app TO security_group_role;\nALTER ROLE security_group_role ADD MEMBER security_group_lk;"
      },
      {
        "date": "2023-04-04T04:32:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-08-07T03:52:00.000Z",
        "voteCount": 2,
        "content": "given answer is correct"
      },
      {
        "date": "2021-12-30T14:30:00.000Z",
        "voteCount": 6,
        "content": "1. create database user in dw1 that represent Group1 and uses From External Provider clause\n 2. create database role named Role1 with grant Role1 select permission on dw1 \n3. add Role1 to Group1 database user"
      },
      {
        "date": "2022-04-11T03:53:00.000Z",
        "voteCount": 2,
        "content": "it should be least privileged so select on schema is correct not on dw1"
      },
      {
        "date": "2021-06-17T22:31:00.000Z",
        "voteCount": 1,
        "content": "It should be D-E-A"
      },
      {
        "date": "2021-06-19T22:00:00.000Z",
        "voteCount": 18,
        "content": "Please ignore my previous answer, it should be\nD: Create a database user in dw1 that represents Group1 and uses FROM EXTERNAL PROVIDE clause\nA: Create a database role named Role1 and grant Role1 SELECT permissions to schema1\nE: Assign Rol1 to the Group1 database user"
      },
      {
        "date": "2021-06-17T22:30:00.000Z",
        "voteCount": 1,
        "content": "It should be C-A-E"
      },
      {
        "date": "2021-06-14T01:40:00.000Z",
        "voteCount": 1,
        "content": "Is the answer correct ??"
      },
      {
        "date": "2021-06-29T06:55:00.000Z",
        "voteCount": 6,
        "content": "No, in my opinion it is D, A, E. If you give a reader role to the group, the users will have the possibility to query all the tables, not only the selected schema."
      },
      {
        "date": "2022-06-23T21:19:00.000Z",
        "voteCount": 2,
        "content": "But, the answer shown in solution es D,A,E...."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52954-exam-dp-203-topic-3-question-2-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains a logical Microsoft SQL server named Server1. Server1 hosts an Azure Synapse Analytics SQL dedicated pool named Pool1.<br>You need to recommend a Transparent Data Encryption (TDE) solution for Server1. The solution must meet the following requirements:<br>\u2711 Track the usage of encryption keys.<br>Maintain the access of client apps to Pool1 in the event of an Azure datacenter outage that affects the availability of the encryption keys.<br><img src=\"/assets/media/exam-media/04259/0028900003.png\" class=\"in-exam-image\"><br>What should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0029000001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0029100001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: TDE with customer-managed keys<br>Customer-managed keys are stored in the Azure Key Vault. You can monitor how and when your key vaults are accessed, and by whom. You can do this by enabling logging for Azure Key Vault, which saves information in an Azure storage account that you provide.<br>Box 2: Create and configure Azure key vaults in two Azure regions<br>The contents of your key vault are replicated within the region and to a secondary region at least 150 miles away, but within the same geography to maintain high durability of your keys and secrets.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption https://docs.microsoft.com/en-us/azure/key-vault/general/logging",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-16T08:01:00.000Z",
        "voteCount": 63,
        "content": "Guys the aswers are correct: https://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview"
      },
      {
        "date": "2021-10-09T21:44:00.000Z",
        "voteCount": 5,
        "content": "Agreed. \"Link each server with two key vaults that reside in different regions and hold the same key material, to ensure high availability of encrypted databases. Mark only the key from the key vault in the same region as a TDE protector. System will automatically switch to the key vault in the remote region if there is an outage affecting the key vault in the same region.\"\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview"
      },
      {
        "date": "2021-09-26T05:08:00.000Z",
        "voteCount": 7,
        "content": "First answer is correct.\n2nd box answer should be \" Implement the client apps by using .NET framework data provider\" as key vault is by default replicated in two or more regions for HA."
      },
      {
        "date": "2021-09-26T05:17:00.000Z",
        "voteCount": 1,
        "content": "Link from Microsoft docs : https://docs.microsoft.com/en-us/azure/key-vault/general/disaster-recovery-guidance#:~:text=The%20contents%20of%20your%20key%20vault%20are%20replicated%20within%20the%20region%20and%20to%20a%20secondary%20region%20at%20least%20150%20miles%20away%2C%20but%20within%20the%20same%20geography%20to%20maintain%20high%20durability%20of%20your%20keys%20and%20secrets"
      },
      {
        "date": "2024-04-25T19:24:00.000Z",
        "voteCount": 1,
        "content": "why not platform managed key?"
      },
      {
        "date": "2023-08-30T04:16:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-07-06T12:10:00.000Z",
        "voteCount": 3,
        "content": "why \"two\" azure regions? the requirement never mentioned how many regions?"
      },
      {
        "date": "2022-08-07T03:57:00.000Z",
        "voteCount": 1,
        "content": "correct answer"
      },
      {
        "date": "2021-12-30T14:48:00.000Z",
        "voteCount": 2,
        "content": "Both answers Correct 1) Transparent Data Encryption with customer-managed key 2) key vault in 2 regions"
      },
      {
        "date": "2021-12-28T12:50:00.000Z",
        "voteCount": 1,
        "content": "Correct.\nRecommendations when configuring customer-managed TDE: Recommendations when configuring AKV:\n- Enable auditing and reporting on all encryption keys: Key vault provides logs that are easy to inject into other security information and event management tools. Operations Management Suite Log Analytics is one example of a service that is already integrated.\n\n- Link each server with two key vaults that reside in different regions and hold the same key material, to ensure high availability of encrypted databases. Mark the key from one of the key vaults as the TDE protector. System will automatically switch to the key vault in the second region with the same key material, if there's an outage affecting the key vault in the first region."
      },
      {
        "date": "2021-09-17T12:02:00.000Z",
        "voteCount": 1,
        "content": "Transparent Data Encryption with customer-managed key\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview"
      },
      {
        "date": "2021-06-11T01:55:00.000Z",
        "voteCount": 1,
        "content": "TDE doesn't use client managed keys\n\nanswer therefore is\n1) always encrypted\n2) key vault in 2 regions"
      },
      {
        "date": "2021-06-13T05:19:00.000Z",
        "voteCount": 5,
        "content": "TDE can be configured with Customer Managed keys:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?tabs=azure-portal#customer-managed-transparent-data-encryption---bring-your-own-key\n\nKey vault is configured in multiple regions by microsoft itself. I also double-checked by creating a key vault and there are no geo-redundancy options. Also see here:\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/disaster-recovery-guidance"
      },
      {
        "date": "2021-06-13T05:21:00.000Z",
        "voteCount": 3,
        "content": "Moreover, always encrypted is NOT TDE option. The question asks to enable TDE."
      },
      {
        "date": "2022-11-16T22:38:00.000Z",
        "voteCount": 1,
        "content": "you need to create key vault separately on two regions and then linked it together\n\"Even in cases when there's no configured geo-redundancy for server, it's highly recommended to configure the server to use two different key vaults in two different regions with the same key material.\"\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-overview?view=azuresql#high-availability-with-customer-managed-tde"
      },
      {
        "date": "2021-06-05T02:00:00.000Z",
        "voteCount": 2,
        "content": "The first answer is correct. You need to enable TDE with customer keys in order to track the key usage in Azure key vault. \nThe second answer seems wrong, as pointed out by Rob77. AKV does have replication it 2 additional regions by default. So I guess that it makes more sense to use a Microsoft .NET framwork data provider https://docs.microsoft.com/en-us/dotnet/framework/data/adonet/data-providers"
      },
      {
        "date": "2021-06-11T01:50:00.000Z",
        "voteCount": 1,
        "content": "TDE doesn't operate with customer keys but always encrypted does"
      },
      {
        "date": "2021-05-17T07:57:00.000Z",
        "voteCount": 3,
        "content": "second answer does not seem to be correct - AKV is already replicated within the region locally (and also 2 pair regions). Therefore if the datacentre fails (or even whole region) the traffic will be redirected. https://docs.microsoft.com/en-us/azure/key-vault/general/disaster-recovery-guidance"
      },
      {
        "date": "2021-12-28T15:24:00.000Z",
        "voteCount": 1,
        "content": "\"The contents of your key vault are replicated within the region and to a secondary region at least 150 miles away, but within the same geography to maintain high durability of your keys and secrets.\"\n\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/disaster-recovery-guidance"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55525-exam-dp-203-topic-3-question-3-discussion/",
    "body": "You plan to create an Azure Synapse Analytics dedicated SQL pool.<br>You need to minimize the time it takes to identify queries that return confidential information as defined by the company's data privacy regulations and the users who executed the queues.<br>Which two components should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsensitivity-classification labels applied to columns that contain confidential information\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tresource tags for databases that contain confidential information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taudit logs sent to a Log Analytics workspace\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking for columns that contain confidential information"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-17T11:41:00.000Z",
        "voteCount": 27,
        "content": "Correct!"
      },
      {
        "date": "2021-06-17T10:02:00.000Z",
        "voteCount": 11,
        "content": "Answer is correct. Dynamic data masking will limit the exposure of sensitive data."
      },
      {
        "date": "2024-05-02T02:22:00.000Z",
        "voteCount": 1,
        "content": "B and C\nnobody mentions purview there"
      },
      {
        "date": "2024-04-25T19:37:00.000Z",
        "voteCount": 1,
        "content": "The answer is B and C\nSensitivity classification tags are part of Purview. Purview is a separate service, and an expensive one. The question does not mention Purview."
      },
      {
        "date": "2023-08-30T04:21:00.000Z",
        "voteCount": 2,
        "content": "audit aand sensitivity-classification labels"
      },
      {
        "date": "2022-09-06T17:40:00.000Z",
        "voteCount": 4,
        "content": "Given Answers are correct !"
      },
      {
        "date": "2022-08-07T04:04:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2022-06-28T12:16:00.000Z",
        "voteCount": 2,
        "content": "Also for me is correct"
      },
      {
        "date": "2022-04-04T18:17:00.000Z",
        "voteCount": 2,
        "content": "log auditing &amp; tracing is important for data governance, therefore necessary for any data solution."
      },
      {
        "date": "2021-12-07T08:17:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2021-12-02T01:11:00.000Z",
        "voteCount": 8,
        "content": "Correct: \"The solution needs to identify the users who executed queries, not to hide confidental information.\" thanks @DirectX from this discussion: https://www.examtopics.com/discussions/microsoft/view/51257-exam-dp-201-topic-3-question-32-discussion/"
      },
      {
        "date": "2021-09-14T19:58:00.000Z",
        "voteCount": 1,
        "content": "Is it really C correct?"
      },
      {
        "date": "2021-12-02T01:12:00.000Z",
        "voteCount": 3,
        "content": "Yes, the logs are used to identify the user who executed the query."
      },
      {
        "date": "2021-10-30T12:07:00.000Z",
        "voteCount": 1,
        "content": "wondering the same thing."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52626-exam-dp-203-topic-3-question-4-discussion/",
    "body": "You are designing an enterprise data warehouse in Azure Synapse Analytics that will contain a table named Customers. Customers will contain credit card information.<br>You need to recommend a solution to provide salespeople with the ability to view all the entries in Customers. The solution must prevent all the salespeople from viewing or inferring the credit card information.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdata masking",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAlways Encrypted",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn-level security\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-06-13T05:29:00.000Z",
        "voteCount": 32,
        "content": "C is the right answer. Check the discussion here:\nhttps://www.examtopics.com/discussions/microsoft/view/18788-exam-dp-201-topic-3-question-12-discussion/"
      },
      {
        "date": "2022-01-30T22:32:00.000Z",
        "voteCount": 3,
        "content": "yeah, from ms docs: \"ensuring that specific users can access only certain columns of a table pertinent to their department\""
      },
      {
        "date": "2021-07-22T03:47:00.000Z",
        "voteCount": 3,
        "content": "The link below show how you can infer a column that is data masked. It is also referenced in the 201 topic, https://docs.microsoft.com/nl-nl/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver15"
      },
      {
        "date": "2024-02-02T02:08:00.000Z",
        "voteCount": 1,
        "content": "The link that you put in says that \"data masking\" is the right answer! :)"
      },
      {
        "date": "2021-06-24T22:19:00.000Z",
        "voteCount": 11,
        "content": "the key word is 'infer'. as listed in the below documentation, data masking is not used to protect against malicious intent to infer the underlying data. I would therefore choose C"
      },
      {
        "date": "2022-08-07T04:08:00.000Z",
        "voteCount": 3,
        "content": "I agree with the logic provided"
      },
      {
        "date": "2021-09-16T07:33:00.000Z",
        "voteCount": 4,
        "content": "I agree with mikerss, the key word is 'infer'. Data masking is a kind of column-level security but it is only partial. A malicious person could infer the credit card number. The good answer is C"
      },
      {
        "date": "2021-11-19T05:33:00.000Z",
        "voteCount": 10,
        "content": "Data masking does not protect against inferrring with the data"
      },
      {
        "date": "2024-01-25T16:12:00.000Z",
        "voteCount": 2,
        "content": "Column-level security allows you to restrict access to specific columns in a table based on user permissions. This means that salespeople would only be able to see the columns that they need to see, such as the customer's name, address, and email address. They would not be able to see the credit card information, which would be encrypted and hidden from them.\n\nAlways Encrypted is a good option for protecting sensitive data that needs to be accessed by multiple users. However, it is not as suitable for this scenario because it encrypts the entire table, including the columns that salespeople need to see. This would make it difficult for salespeople to do their jobs.\n\nHere is a summary of why column-level security is the better choice in this case:\n\nColumn-level security is more granular and allows you to control access to specific columns.\nColumn-level security does not impact the performance of queries as much as Always Encrypted.\nColumn-level security is easier to implement and manage."
      },
      {
        "date": "2023-08-30T04:23:00.000Z",
        "voteCount": 1,
        "content": "Data masking does not protect against inferrring with the data"
      },
      {
        "date": "2023-09-08T21:26:00.000Z",
        "voteCount": 1,
        "content": "go to A"
      },
      {
        "date": "2023-06-19T15:12:00.000Z",
        "voteCount": 1,
        "content": "Infer data means:\n\nSELECT ID, Name, Salary FROM Employees\nWHERE Salary &gt; 99999 and Salary &lt; 100001;\n+------------+---------------------+--------+\n|Id\t        |Name\t        |Salary|\n+------------+---------------------+--------+\n|62543\t|Jane Doe\t|0         |\n|91245\t|John Smith\t|0         |\n+------------+--------------------+----------+"
      },
      {
        "date": "2023-05-20T15:55:00.000Z",
        "voteCount": 1,
        "content": "Only with DDM, you can guess with trying some queries"
      },
      {
        "date": "2023-01-30T13:53:00.000Z",
        "voteCount": 1,
        "content": "C is the answer\nAs an example, consider a database principal that has sufficient privileges to run ad-hoc queries on the database, and tries to 'guess' the underlying data and ultimately infer the actual values. Assume that we have a mask defined on the [Employee].[Salary] column, and this user connects directly to the database and starts guessing values, eventually inferring the [Salary] value of a set of Employees:\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/dynamic-data-masking?view=sql-server-ver16#security-note-bypassing-masking-using-inference-or-brute-force-techniques"
      },
      {
        "date": "2022-09-06T17:41:00.000Z",
        "voteCount": 2,
        "content": "Column level security is the correct answer !!"
      },
      {
        "date": "2022-08-07T04:07:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-05-13T08:57:00.000Z",
        "voteCount": 1,
        "content": "There is nothing that says that you must use the credit card masking rule, you can use another one. This way, the sales persons has access to all entries but cannot infer the credit card. The answer is A"
      },
      {
        "date": "2022-06-13T00:22:00.000Z",
        "voteCount": 1,
        "content": "data masking will only help in not viewing the credit card information however it won't help in inferring the column so column level security is required. In this way you can view all the rows(entries) without using the credit card column"
      },
      {
        "date": "2022-04-25T06:35:00.000Z",
        "voteCount": 1,
        "content": "Column-level security prevent get \"credit card\" column, you not be able to infer the credit card information contrary to \"masking\"."
      },
      {
        "date": "2022-01-29T12:30:00.000Z",
        "voteCount": 3,
        "content": "There are 2 parts to it:\n1. provide salespeople with the ability to &lt;b&gt;view all the entries&lt;/b&gt; in Customers.\n2. should not be able to infer.\n\nDDM is the only solution if you have to comply with both requirements"
      },
      {
        "date": "2022-01-25T22:23:00.000Z",
        "voteCount": 1,
        "content": "C is correct. The requirement is to put restriction on viewing or inferring. In other words, don't allow to access the column. My previous choice A was wrong."
      },
      {
        "date": "2022-01-25T22:20:00.000Z",
        "voteCount": 1,
        "content": "You get 'The SELECT permission was denied on the colum...' error if you use column level security. You need to allow to query the column with protection which is acheived using data masking. So A is correct"
      },
      {
        "date": "2021-12-30T15:10:00.000Z",
        "voteCount": 1,
        "content": "to provide salespeople with the ability to view all the entries in Customers. (Column level security prevents that) The solution must prevent all the salespeople from viewing or inferring the credit card information. (Data masking helps infer information even when you can view the column)"
      },
      {
        "date": "2021-12-20T07:42:00.000Z",
        "voteCount": 2,
        "content": "Data Masking is the correct Answer, it is not necessarily he need to use credit card masking. we can even use Default or Random and avoid users from inferring the data.\nHence A is the Right Answer."
      },
      {
        "date": "2021-12-12T04:22:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: C"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62048-exam-dp-203-topic-3-question-5-discussion/",
    "body": "You develop data engineering solutions for a company.<br>A project requires the deployment of data to Azure Data Lake Storage.<br>You need to implement role-based access control (RBAC) so that project members can manage the Azure Data Lake Storage resources.<br>Which three actions should you perform? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate security groups in Azure Active Directory (Azure AD) and add project members.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure end-user authentication for the Azure Data Lake Storage account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign Azure AD security groups to Azure Data Lake Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure Service-to-service authentication for the Azure Data Lake Storage account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure access control lists (ACL) for the Azure Data Lake Storage account.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 33,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-07T08:17:00.000Z",
        "voteCount": 17,
        "content": "correct"
      },
      {
        "date": "2021-09-14T20:41:00.000Z",
        "voteCount": 7,
        "content": "nice question!"
      },
      {
        "date": "2024-09-17T22:03:00.000Z",
        "voteCount": 1,
        "content": "Notes:\n- ACL is not a requirement. It's just there for \"the best third option\".\n\nWhy \"nots\":\n- Set user authentication: because it's already there.\n- Set service authentication: because the question is not about services."
      },
      {
        "date": "2024-01-09T15:46:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-30T04:25:00.000Z",
        "voteCount": 1,
        "content": "ACE is correct method."
      },
      {
        "date": "2023-06-19T15:15:00.000Z",
        "voteCount": 6,
        "content": "Create security groups in Azure Active Directory (Azure AD) and add project members: Start by creating the necessary security groups in Azure AD and adding the project members to these groups. This step allows you to organize users and manage their permissions collectively.\n\nConfigure access control lists (ACL) for the Azure Data Lake Storage account: Next, configure the access control lists (ACL) for the Azure Data Lake Storage account. ACLs provide granular control over permissions at the individual file or folder level within the storage. By setting up ACLs, you can define specific access rights for different data assets.\n\nAssign Azure AD security groups to Azure Data Lake Storage: Once the security groups and ACLs are set up, assign the Azure AD security groups to the Azure Data Lake Storage account. This step associates the security groups with the storage resources and enables you to grant permissions based on group membership rather than individually managing permissions for each user."
      },
      {
        "date": "2023-05-13T11:55:00.000Z",
        "voteCount": 2,
        "content": "Isn't this a ACL model instead of RBAC?"
      },
      {
        "date": "2023-01-23T13:04:00.000Z",
        "voteCount": 1,
        "content": "1.Create security group.\n2. Assign the Group/users to data lake.\n3. Assign ACL (access control on the data which is stored inside the lake)"
      },
      {
        "date": "2022-09-06T17:43:00.000Z",
        "voteCount": 3,
        "content": "CORRECT !!"
      },
      {
        "date": "2022-09-04T01:59:00.000Z",
        "voteCount": 2,
        "content": "E-&gt; A-&gt;C \nis the order right ?"
      },
      {
        "date": "2022-08-07T04:13:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-04-25T06:40:00.000Z",
        "voteCount": 1,
        "content": "Is correct!"
      },
      {
        "date": "2022-02-10T04:05:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2021-09-14T11:38:00.000Z",
        "voteCount": 4,
        "content": "Correct answer!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61695-exam-dp-203-topic-3-question-6-discussion/",
    "body": "You have an Azure Data Factory version 2 (V2) resource named Df1. Df1 contains a linked service.<br>You have an Azure Key vault named vault1 that contains an encryption key named key1.<br>You need to encrypt Df1 by using key1.<br>What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a private endpoint connection to vault1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Azure role-based access control on vault1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the linked service from Df1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a self-hosted integration runtime."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-09-11T00:00:00.000Z",
        "voteCount": 41,
        "content": "I believe this is correct, based on the question: What should you do FIRST?\nA DF needs to be empty to be encrypted: https://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key#post-factory-creation-in-data-factory-ui\nSo FIRST we need to empty the DF - then we can move on."
      },
      {
        "date": "2023-01-03T04:04:00.000Z",
        "voteCount": 1,
        "content": "B!!!\nEnable Azure RBAC permissions on Key Vault:\nhttps://learn.microsoft.com/en-us/azure/key-vault/general/rbac-guide?tabs=azure-cli"
      },
      {
        "date": "2023-06-25T22:55:00.000Z",
        "voteCount": 6,
        "content": "Correct answer:\nA customer-managed key can only be configured on an empty data Factory. The data factory can't contain any resources such as linked services, pipelines and data flows.\nhttps://learn.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key#post-factory-creation-in-data-factory-ui"
      },
      {
        "date": "2023-08-30T04:28:00.000Z",
        "voteCount": 1,
        "content": "is the first step."
      },
      {
        "date": "2023-07-05T00:59:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-06-19T15:23:00.000Z",
        "voteCount": 1,
        "content": "A customer-managed key can only be configured on an empty data Factory. The data factory can't contain any resources such as linked services, pipelines and data flows.\n\nhttps://learn.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key#post-factory-creation-in-data-factory-ui"
      },
      {
        "date": "2022-10-28T06:15:00.000Z",
        "voteCount": 1,
        "content": "so you need to encrypt the df, you need to remove the bonded service first , answer is correct"
      },
      {
        "date": "2022-08-22T22:52:00.000Z",
        "voteCount": 3,
        "content": "Its C:\nYour ADF should be empty during encryption process using a KEY"
      },
      {
        "date": "2022-08-07T04:16:00.000Z",
        "voteCount": 3,
        "content": "correct answer"
      },
      {
        "date": "2022-04-25T06:54:00.000Z",
        "voteCount": 1,
        "content": "You don't need to enable \"RBAC\", access policies is a default and more simple way to assign permissions, so B option is not necesary, but it is a requirement to delete the linked services to configure customer-managed key. So the correct answer is C - Delete linked services first.\n\nhttps://docs.microsoft.com/en-us/azure/key-vault/general/assign-access-policy?tabs=azure-portal\nhttps://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key#enable-customer-managed-keys"
      },
      {
        "date": "2022-02-04T03:00:00.000Z",
        "voteCount": 1,
        "content": "Correct. \"A customer-managed key can only be configured on an empty data Factory. The data factory can't contain any resources such as linked services, pipelines and data flows.\""
      },
      {
        "date": "2021-12-28T09:46:00.000Z",
        "voteCount": 3,
        "content": "A customer-managed key can only be configured on an empty data Factory. The data factory can\u2019t contain any resources such as linked services, pipelines and data flows. It is recommended to enable customer-managed key right after factory creation.\n\nNote: Azure Data Factory encrypts data at rest, including entity definitions and any data cached while runs are in progress. By default, data is encrypted with a randomly generated Microsoft-managed key that is uniquely assigned to your data factory.\n\nReference: https://docs.microsoft.com/en-us/azure/data-factory/enable-customer-managed-key"
      },
      {
        "date": "2021-12-26T11:36:00.000Z",
        "voteCount": 1,
        "content": "B should be the correct answer. \nhttps://docs.microsoft.com/en-us/azure/key-vault/general/rbac-guide?tabs=azure-cli"
      },
      {
        "date": "2021-12-07T20:13:00.000Z",
        "voteCount": 1,
        "content": "Should it be D?\nhttps://docs.microsoft.com/en-us/powershell/module/az.datafactory/new-azdatafactoryv2linkedserviceencryptedcredential?view=azps-7.0.0"
      },
      {
        "date": "2021-11-02T02:23:00.000Z",
        "voteCount": 2,
        "content": "I thin k it's B. I recently changed a linked service pwf to key vault. I didn't delete the service and just added the managed Identity access to the vault with all the desired rules."
      },
      {
        "date": "2021-09-08T04:11:00.000Z",
        "voteCount": 2,
        "content": "Isn't B Correct ?"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62049-exam-dp-203-topic-3-question-7-discussion/",
    "body": "You are designing an Azure Synapse Analytics dedicated SQL pool.<br>You need to ensure that you can audit access to Personally Identifiable Information (PII).<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn-level security",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security (RLS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tsensitivity classifications\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-14T11:53:00.000Z",
        "voteCount": 27,
        "content": "Correct answer!"
      },
      {
        "date": "2022-08-09T11:15:00.000Z",
        "voteCount": 6,
        "content": "An important aspect of the classification is the ability to monitor access to sensitive data. Azure SQL Auditing has been enhanced to include a new field in the audit log called data_sensitivity_information. This field logs the sensitivity classifications (labels) of the data that was returned by a query.\n\nRef -  https://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql"
      },
      {
        "date": "2023-08-30T04:29:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-19T15:27:00.000Z",
        "voteCount": 1,
        "content": "By implementing Data Discovery &amp; Classification we can Audit access to sensitive data.\n\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview?view=azuresql#audit-sensitive-data"
      },
      {
        "date": "2023-04-04T05:43:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-10-19T00:23:00.000Z",
        "voteCount": 2,
        "content": "Why not dynamic data masking?"
      },
      {
        "date": "2023-06-19T15:25:00.000Z",
        "voteCount": 3,
        "content": "Data mask you hide the data, but can't audit who read it."
      },
      {
        "date": "2022-04-25T06:57:00.000Z",
        "voteCount": 3,
        "content": "Is correct!\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/data-discovery-and-classification-overview#audit-sensitive-data"
      },
      {
        "date": "2022-04-05T17:32:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/microsoft/view/118064-exam-dp-203-topic-3-question-40-discussion/",
    "body": "HOTSPOT -<br><br>You have an Azure subscription that contains an Azure Data Lake Storage account. The storage account contains a data lake named DataLake1.<br><br>You plan to use an Azure data factory to ingest data from a folder in DataLake1, transform the data, and land the data in another folder.<br><br>You need to ensure that the data factory can read and write data from any folder in the DataLake1 container. The solution must meet the following requirements:<br><br>\u2022\tMinimize the risk of unauthorized user access.<br>\u2022\tUse the principle of least privilege.<br>\u2022\tMinimize maintenance effort.<br><br>How should you configure access to the storage account for the data factory? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image337.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image338.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-08-31T00:25:00.000Z",
        "voteCount": 6,
        "content": "correcr"
      },
      {
        "date": "2024-02-26T09:39:00.000Z",
        "voteCount": 1,
        "content": "It's correct"
      },
      {
        "date": "2024-02-20T09:10:00.000Z",
        "voteCount": 1,
        "content": "Correct, first and first"
      },
      {
        "date": "2023-10-09T02:47:00.000Z",
        "voteCount": 2,
        "content": "should be B SAS"
      },
      {
        "date": "2023-12-07T18:05:00.000Z",
        "voteCount": 4,
        "content": "No, because of the requirement \"Minimize maintenance effort\""
      },
      {
        "date": "2023-08-13T20:24:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60567-exam-dp-203-topic-3-question-9-discussion/",
    "body": "HOTSPOT -<br>You are designing an Azure Synapse Analytics dedicated SQL pool.<br>Groups will have access to sensitive data in the pool as shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0029800004.png\" class=\"in-exam-image\"><br>You have policies for the sensitive data. The policies vary be region as shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0029900001.png\" class=\"in-exam-image\"><br>You have a table of patients for each region. The tables contain the following potentially sensitive columns.<br><img src=\"/assets/media/exam-media/04259/0029900002.png\" class=\"in-exam-image\"><br>You are designing dynamic data masking to maintain compliance.<br>For each of the following statements, select Yes if the statement is true. Otherwise, select No.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0030000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0030000002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-08-24T19:52:00.000Z",
        "voteCount": 146,
        "content": "The Answer should be No, No, No. Analysts have access to in-region sensitive data, so the first one should be No. Engineers have access to all numeric sensitive data, Height is patient\u2019s height in CM, so the second and third one should also No."
      },
      {
        "date": "2024-04-25T20:06:00.000Z",
        "voteCount": 1,
        "content": "Either that or the question is poorly formulated.\nIt may be that those roles require access to tables containing those columns but should not see the columns in which case the answers are correct"
      },
      {
        "date": "2024-04-25T20:10:00.000Z",
        "voteCount": 2,
        "content": "actually then it would be all Y, because engineers have access to all regions to numeric data (hight)"
      },
      {
        "date": "2022-07-10T10:24:00.000Z",
        "voteCount": 1,
        "content": "100 % Agreed."
      },
      {
        "date": "2022-01-31T22:12:00.000Z",
        "voteCount": 3,
        "content": "Agree with you and I've to say that this question very bad formulated"
      },
      {
        "date": "2022-06-13T00:39:00.000Z",
        "voteCount": 1,
        "content": "No it is not. It's a proper question and very neatly formulated. This is to test if you know how the masking rules are applied"
      },
      {
        "date": "2021-10-09T22:07:00.000Z",
        "voteCount": 5,
        "content": "I think you are right because question is about masking rule required, and masking rule is to mask data. In all 3 questions, the data that is mentioned should be visible to the group of users that is mentioned so therefore a masking rule is not required."
      },
      {
        "date": "2021-09-19T15:06:00.000Z",
        "voteCount": 86,
        "content": "the solution is correct: Yes, no, yes. Just because somebody has access, doesnt mean that they dont need any dynamic masking. It just means that they have access and a policy is required. If they had no access, then obviously no data masking is required.\nStatement 1: Analysts in Region A have access to (all) the following sensitive data in region A: CardOnFile, Heigth and ContactEmail. Since financial (CardOnFike) and PII (ContactEmail) are considered sensitive data you need dynamic data masking: so Yes.\nStatement 2 &amp; 3: Engineers have access to all numeric sensitive data (which means in every region). So they have access to height. Height is medical and therefore only sensitive in Region B according to the second table, but not in Region A. So Statement 2 is \u201cNo\u201d and Statement 3 is \u201cYes\u201d"
      },
      {
        "date": "2024-09-11T00:14:00.000Z",
        "voteCount": 1,
        "content": "I don't think this is the correct answer, because that would mean that the information in the first box (stating the access that should be given to each employee role) would be irrelevant. If you are ignoring that information, you could just replace the roles in the answer statement with \"employee\". For example \"Employees in RegionA require...\""
      },
      {
        "date": "2024-06-28T08:25:00.000Z",
        "voteCount": 1,
        "content": "This explanation is nice"
      },
      {
        "date": "2021-09-22T08:58:00.000Z",
        "voteCount": 6,
        "content": "I think You are correct"
      },
      {
        "date": "2021-10-28T14:26:00.000Z",
        "voteCount": 2,
        "content": "I would go for this answer as well.. otherwise the double question 2 and 3 would be useless.."
      },
      {
        "date": "2024-07-05T21:36:00.000Z",
        "voteCount": 1,
        "content": "the answer is : No, No,NO"
      },
      {
        "date": "2024-05-07T17:17:00.000Z",
        "voteCount": 1,
        "content": "This is the worst question I have come across."
      },
      {
        "date": "2024-05-05T23:49:00.000Z",
        "voteCount": 2,
        "content": "I found this question on my exam 30/04/2024, and I put Yes/no/Yes. I passed the exam with a high score, but I'm not sure if the answer is correct."
      },
      {
        "date": "2024-04-14T07:56:00.000Z",
        "voteCount": 2,
        "content": "What the hell? It's a very confusing question!"
      },
      {
        "date": "2024-03-21T16:28:00.000Z",
        "voteCount": 1,
        "content": "The question is poorly written. The problem is that you define dynamic data masking directly on the column and its enabled for every one (except admins, db_owner and etc...)\n\nThen you GRANT UNMASK permission for those that needed access to the original content.\nIf you look that way (who needs grant unmask) the provided answers are correctly.\n\nIf you think of enable or not enable masking its No, No and No. (but for me doesn\u00b4t make sense)"
      },
      {
        "date": "2023-08-30T04:45:00.000Z",
        "voteCount": 1,
        "content": "in oder, y,n,y"
      },
      {
        "date": "2023-06-25T23:27:00.000Z",
        "voteCount": 2,
        "content": "First: NO, because there a no medical data in the region A. Second and Third, NO, because data engineers can see numeric data in all regions (heigth is number)."
      },
      {
        "date": "2024-01-08T23:17:00.000Z",
        "voteCount": 1,
        "content": "You have a table of patients for each region. The tables contain the following potentially sensitive columns. they have specify that each region have patient table"
      },
      {
        "date": "2023-06-01T23:16:00.000Z",
        "voteCount": 1,
        "content": "Answer should be NO, NO, NO. Analyst have access to in-region sensitive data, Engineers have access to all numeric sensitive data."
      },
      {
        "date": "2023-10-13T10:50:00.000Z",
        "voteCount": 1,
        "content": "last one is yes... in region b, financial, pii and medical are sensitive data. but engineers have access to all numeric sensitive data.  pii is sensitive data."
      },
      {
        "date": "2023-04-26T20:08:00.000Z",
        "voteCount": 1,
        "content": "Q1: Yes, these users need to see past any default masking.\nAnalysts have access to in-region sensitive data. So, since they're in RegionA looking at RegionA data, the default masking should be dynamically removed for them.\n\nQ2: No, these users should see data with default masking.\nYou have to assume that Enhanced Access only apply to users when they are in their own region. Since the Engineers are outside of the region, they are treated as regular users, with default masking. Perhaps there's some documentation in Azure that says you can't enhance access for users outside of a given region, but I'm not aware of any. Personally, I feel the wording of the Enhanced Access makes me assume it's \"region agnostic\". However, the given answer (of No) seems to imply otherwise. \n\nQ3: Yes, these users need to see past SOME default masking.\nThere's a lot to consider, but I assume because the Engineers need to see numeric data, and both Financial and Medical data is numeric, they need to SOME data unmasked."
      },
      {
        "date": "2023-04-28T15:59:00.000Z",
        "voteCount": 2,
        "content": "This is a poorly worded question, in my opinion. I eventually came to accept the given answer of Yes, No, Yes. However, my gut would have had me say No (no masking), Yes (mask e-mail), Yes (mask e-mail).\n\nThese were the questions I had when trying to sort through this one.\n\n1. Is Enhanced Access truly defined as only applicable should the user be in the same region as the data? (I didn't want to.)\n2. Should we only be considering the Height field for Q2, Q3? (Hard to say, with that comma....)\n3. If we're meant to consider the full table, then (a) is it a \"Yes\" if ANY data needs to be unmasked, or (b) is it only a \"Yes\" if ALL data needs to be unmasked? (I'd assume A.)\n4. Does the region of the Engineer matter at all? (I doubt it.)\n\nNot fun to sort through before committing to an answer. (I spent way too long typing this up too.)"
      },
      {
        "date": "2023-04-26T20:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: Yes, No, Yes.\n\nThis is a poorly worded question, in my opinion. I eventually came to accept the given answer of Yes, No, Yes. However, my gut would have had me say No (no masking), Yes (mask e-mail), Yes (mask e-mail).\n\nI initially assumed that \"Yes\" meant the user should have the data masked/treated for them. Based on the given answers (of Yes, No, Yes) it seems like it's the opposite"
      },
      {
        "date": "2023-04-26T20:04:00.000Z",
        "voteCount": 1,
        "content": "Answer: Yes, No, Yes.\n\nThis is a poorly worded question, in my opinion. I eventually came to accept the given answer of Yes, No, Yes. However, my gut would have had me say No (no masking), Yes (mask e-mail), Yes (mask e-mail)."
      },
      {
        "date": "2023-04-28T16:01:00.000Z",
        "voteCount": 1,
        "content": "Sorry for the spam. The site was throwing an error when I would try to submit my full comment...."
      },
      {
        "date": "2023-03-21T13:18:00.000Z",
        "voteCount": 7,
        "content": "after reading discussion very confused. What could be the answer."
      },
      {
        "date": "2023-05-24T00:17:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is YES, NO &amp; YES, look at the explanation from essade underneath. The fact that the data should be unmasked for certain group, these are masked by some rules. After masking, some are unmasked for required group- this holds for Q1 &amp; Q3. Q2 does not have height on it and hence no rule is needed."
      },
      {
        "date": "2023-03-12T23:38:00.000Z",
        "voteCount": 5,
        "content": "Analysts in RegionA require dynamic data masking rules for [Patients RegionA].\n\nYes. Since analysts in RegionA have access to in-region sensitive data, which includes PII, dynamic data masking rules should be implemented for the [Patients RegionA] table to mask the [ContactEmail] column which contains PII.\nEngineers in RegionC require a dynamic data masking rule for [Patients RegionA], [Height].\n\nNo. Engineers in RegionC have access to all numeric sensitive data, but [Height] is not considered sensitive data in RegionC, only in RegionB. Therefore, there is no need to implement a dynamic data masking rule for [Height] in RegionC.\nEngineers in RegionB require a dynamic data masking rule for [Patients RegionB], [Height].\n\nYes. Engineers in RegionB have access to sensitive data, including medical data, which includes the [Height] column in the [Patients RegionB] table. Therefore, dynamic data masking should be implemented for the [Height] column in the [Patients RegionB] table."
      },
      {
        "date": "2022-12-06T10:38:00.000Z",
        "voteCount": 1,
        "content": "This answer is clearly NO, NO, NO"
      },
      {
        "date": "2022-12-05T10:19:00.000Z",
        "voteCount": 1,
        "content": "The answer is No for all questions. Engineers have full access to all data so no need for data masking. Analysts have access to in region data already."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/microsoft/view/63170-exam-dp-203-topic-3-question-10-discussion/",
    "body": "DRAG DROP -<br>You have an Azure Synapse Analytics SQL pool named Pool1 on a logical Microsoft SQL server named Server1.<br>You need to implement Transparent Data Encryption (TDE) on Pool1 by using a custom key named key1.<br>Which five actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0030100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0030200001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Assign a managed identity to Server1<br>You will need an existing Managed Instance as a prerequisite.<br>Step 2: Create an Azure key vault and grant the managed identity permissions to the vault<br>Create Resource and setup Azure Key Vault.<br>Step 3: Add key1 to the Azure key vault<br>The recommended way is to import an existing key from a .pfx file or get an existing key from the vault. Alternatively, generate a new key directly in Azure Key<br>Vault.<br>Step 4: Configure key1 as the TDE protector for Server1<br><br>Provide TDE Protector key -<br><br>Step 5: Enable TDE on Pool1 -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/managed-instance/scripts/transparent-data-encryption-byok-powershell",
    "votes": [],
    "comments": [
      {
        "date": "2021-09-28T22:28:00.000Z",
        "voteCount": 19,
        "content": "Answer is right!"
      },
      {
        "date": "2021-10-12T14:57:00.000Z",
        "voteCount": 7,
        "content": "Shouldn\u2019t the last two be switched? Enable TDE then configure the key?"
      },
      {
        "date": "2024-04-25T20:14:00.000Z",
        "voteCount": 1,
        "content": "yes should be switched"
      },
      {
        "date": "2022-01-31T23:03:00.000Z",
        "voteCount": 1,
        "content": "I also think so, but not sure"
      },
      {
        "date": "2021-10-28T14:41:00.000Z",
        "voteCount": 10,
        "content": "I think the correct answer is the one provided.\nPlease see the link below:\nhttps://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-byok-configure?tabs=azure-powershell"
      },
      {
        "date": "2023-03-01T23:24:00.000Z",
        "voteCount": 2,
        "content": "Checked this link and it supports the answer given"
      },
      {
        "date": "2023-08-30T04:52:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-01-03T04:11:00.000Z",
        "voteCount": 4,
        "content": "1. Get a KV\n2. Add key to KV\n3. Assign MI to server\n4. Enable TDE\n5. Config TDE"
      },
      {
        "date": "2023-01-30T14:31:00.000Z",
        "voteCount": 2,
        "content": "4 and 5 should be swapped"
      },
      {
        "date": "2022-10-28T06:44:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-08-16T04:32:00.000Z",
        "voteCount": 1,
        "content": "given ans is correct"
      },
      {
        "date": "2022-02-04T21:01:00.000Z",
        "voteCount": 1,
        "content": "options looks correct but. i am bit lost. I dont see tde settings for the logical server it creates by default while creating synapse analytics ws. and there is no option to create synapse anlaytics pool when I create logic server and then try to create database."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/microsoft/view/61988-exam-dp-203-topic-3-question-11-discussion/",
    "body": "You have a data warehouse in Azure Synapse Analytics.<br>You need to ensure that the data in the data warehouse is encrypted at rest.<br>What should you enable?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdvanced Data Security for this database",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransparent Data Encryption (TDE)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSecure transfer required",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDynamic Data Masking"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-09-13T21:06:00.000Z",
        "voteCount": 22,
        "content": "Correct!"
      },
      {
        "date": "2022-04-25T07:08:00.000Z",
        "voteCount": 5,
        "content": "Correct!"
      },
      {
        "date": "2024-01-26T12:18:00.000Z",
        "voteCount": 1,
        "content": "Transparent Data Encryption (TDE) is a built-in security feature of SQL Server that automatically encrypts data at rest."
      },
      {
        "date": "2023-09-08T21:34:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-09-06T20:37:00.000Z",
        "voteCount": 3,
        "content": "Correct !"
      },
      {
        "date": "2022-08-09T11:36:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/microsoft/view/62215-exam-dp-203-topic-3-question-12-discussion/",
    "body": "You are designing a streaming data solution that will ingest variable volumes of data.<br>You need to ensure that you can change the partition count after creation.<br>Which service should you use to ingest the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Event Hubs Dedicated\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Stream Analytics",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Data Factory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Synapse Analytics"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-26T13:27:00.000Z",
        "voteCount": 13,
        "content": "A is the correct Answer.\nYou can specify the number of partitions at the time of creating an event hub. In some scenarios, you may need to add partitions after the event hub has been created. This article describes how to dynamically add partitions to an existing event hub.\n\nDynamic additions of partitions is available only in premium and dedicated tiers of Event Hubs.\n\nhttps://docs.microsoft.com/en-us/azure/event-hubs/dynamically-add-partitions"
      },
      {
        "date": "2021-09-16T08:39:00.000Z",
        "voteCount": 12,
        "content": "Answer is Correct according to given link"
      },
      {
        "date": "2023-08-30T05:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is Correct"
      },
      {
        "date": "2023-03-13T00:25:00.000Z",
        "voteCount": 3,
        "content": "A. Azure Event Hubs Dedicated would be the best choice to ingest the variable volumes of data and change the partition count after creation.\n\nAzure Event Hubs Dedicated is a highly scalable and fully managed event hub service that can ingest millions of events per second. It allows you to create and manage partitions, and you can dynamically increase or decrease the number of partitions to accommodate changes in data volume or throughput requirements.\n\nAzure Stream Analytics, Azure Data Factory, and Azure Synapse Analytics are not specifically designed to manage the partition count after creation. Although they can be used to ingest streaming data, they may not provide the flexibility to change the partition count dynamically."
      },
      {
        "date": "2023-03-01T23:25:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      },
      {
        "date": "2022-08-09T11:37:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2022-05-12T03:58:00.000Z",
        "voteCount": 3,
        "content": "As A is correct"
      },
      {
        "date": "2021-10-07T01:01:00.000Z",
        "voteCount": 5,
        "content": "From the provided link: \"We recommend that you choose at least as many partitions as you expect that are required during the peak load of your application for that particular event hub. You can't change the partition count for an event hub after its creation except for the event hub in a dedicated cluster. The partition count for an event hub in a dedicated Event Hubs cluster can be increased after the event hub has been created, but the distribution of streams across partitions will change when it's done as the mapping of partition keys to partitions changes, so you should try hard to avoid such changes if the relative order of events matters in your application.\""
      },
      {
        "date": "2021-10-24T14:59:00.000Z",
        "voteCount": 3,
        "content": "I think you're focusing on the wrong part. It says that the partition count can be increased in a dedicated event hubs cluster. And this question is about event hubs dedicated (cluster?), so I think event hubs is the correct answer."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79440-exam-dp-203-topic-3-question-13-discussion/",
    "body": "You are designing a date dimension table in an Azure Synapse Analytics dedicated SQL pool. The date dimension table will be used by all the fact tables.<br>Which distribution type should you recommend to minimize data movement during queries?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHASH",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREPLICATE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tROUND_ROBIN"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-28T22:53:00.000Z",
        "voteCount": 7,
        "content": "correct B"
      },
      {
        "date": "2023-06-19T16:02:00.000Z",
        "voteCount": 6,
        "content": "HASH = Fact/2+Gb table\nREPLICATE = Dimensionn\nROUND_ROBIN = Staging"
      },
      {
        "date": "2024-07-05T21:47:00.000Z",
        "voteCount": 1,
        "content": "date dimension table is small so choose replicate"
      },
      {
        "date": "2023-09-07T06:03:00.000Z",
        "voteCount": 1,
        "content": "Replicate"
      },
      {
        "date": "2023-08-30T05:07:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-09-06T20:39:00.000Z",
        "voteCount": 5,
        "content": "REPLICATE"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/microsoft/view/60600-exam-dp-203-topic-3-question-14-discussion/",
    "body": "HOTSPOT -<br>You develop a dataset named DBTBL1 by using Azure Databricks.<br>DBTBL1 contains the following columns:<br>\u2711 SensorTypeID<br>\u2711 GeographyRegionID<br>\u2711 Year<br>\u2711 Month<br>\u2711 Day<br>\u2711 Hour<br>\u2711 Minute<br>\u2711 Temperature<br>\u2711 WindSpeed<br>\u2711 Other<br>You need to store the data to support daily incremental load pipelines that vary for each GeographyRegionID. The solution must minimize storage costs.<br>How should you complete the code? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0030600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0030700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: .partitionBy -<br>Incorrect Answers:<br>\u2711 .format:<br>Method: format():<br>Arguments: \"parquet\", \"csv\", \"txt\", \"json\", \"jdbc\", \"orc\", \"avro\", etc.<br>\u2711 .bucketBy:<br>Method: bucketBy()<br>Arguments: (numBuckets, col, col..., coln)<br>The number of buckets and names of columns to bucket by. Uses Hive's bucketing scheme on a filesystem.<br>Box 2: (\"Year\", \"Month\", \"Day\",\"GeographyRegionID\")<br>Specify the columns on which to do the partition. Use the date columns followed by the GeographyRegionID column.<br>Box 3: .saveAsTable(\"/DBTBL1\")<br>Method: saveAsTable()<br>Argument: \"table_name\"<br>The table to save to.<br>Reference:<br>https://www.oreilly.com/library/view/learning-spark-2nd/9781492050032/ch04.html https://docs.microsoft.com/en-us/azure/databricks/delta/delta-batch",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-06T00:17:00.000Z",
        "voteCount": 100,
        "content": "1. Partition by\n2. GeographyRegionID, Year, Month, Day as the pipelines are per region this seems right choice\n3. Parquet"
      },
      {
        "date": "2024-05-18T07:51:00.000Z",
        "voteCount": 1,
        "content": "Partition on date prerably, and hash on non date fields so that it is balanced..."
      },
      {
        "date": "2022-06-26T09:03:00.000Z",
        "voteCount": 7,
        "content": "regarding point 2 Solution needs to support daily incremental load so having Year, Month, Day first would be more useful"
      },
      {
        "date": "2021-08-25T08:26:00.000Z",
        "voteCount": 52,
        "content": "I suggest storing the data in parquet"
      },
      {
        "date": "2024-07-10T07:17:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nGiven the requirement to support daily incremental loads for each GeographyRegionID, the optimal partitioning strategy would be:\n\nPartition by (GeographyRegionID, Year, Month, Day)\n\nThis strategy allows the system to efficiently access the data based on GeographyRegionID first, and then further narrows down the partitions based on Year, Month, and Day. This approach is particularly useful for managing and querying large datasets in a time-series fashion, which is typical for sensor data."
      },
      {
        "date": "2024-07-04T02:34:00.000Z",
        "voteCount": 1,
        "content": "The second option given in the answer is correct which is Year, Month, Day, GeographyRegionId\nWhy? The question states that the \"solution must minimize storage cost\". One of the way to do that is to have lesser number of folders getting created.\n\nHere is an example:\nLet's says we go with the option of Year, Month, Day, GeographyRegionId where we consider 1 year of data and say we have 10 different region. So the number of folders that would get created would be 1(Year) + 12(Month) + 365 (Days) + 3650 (No of days multiplied by 10 regions as each day's folder would contain 10 sub folders for 10 regions) which gives 3663 as count.\n\nIf we go with the option of GeographyRegionId, Year, Month, Day then 1(Year) + 12(Month) + 365 (Days) i.e, 378 folders would be repeated 10 times inside 10 different region folder which gives total count as 3780(378*10)  which is definitely higher than 3663."
      },
      {
        "date": "2024-01-26T13:18:00.000Z",
        "voteCount": 4,
        "content": "1. Partition by\n2. GeographyRegionID, Year, Month, Day\n3. Parquet"
      },
      {
        "date": "2024-02-19T02:41:00.000Z",
        "voteCount": 1,
        "content": "Agreed with Geographyregionid first before the Year, month and day"
      },
      {
        "date": "2024-01-02T12:56:00.000Z",
        "voteCount": 8,
        "content": "1. Patition by\n2.Year,Month, Day, GeographyRegionID (it said to minimize storage cost, not query performance. if GeographyRegionID goes first, each regionID will have repeated folders for different dates)\n3. Parquet"
      },
      {
        "date": "2024-05-18T07:41:00.000Z",
        "voteCount": 1,
        "content": "Completely true. The amount of date folders per RegionID would be huge"
      },
      {
        "date": "2023-10-04T03:39:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-best-practices"
      },
      {
        "date": "2023-07-14T06:27:00.000Z",
        "voteCount": 4,
        "content": "This question is similar to #36Topic 1, if you reference that question, the answer should be 1. Partitioned By, 2. GeographyRegionID, Year, Month, Day, 3. Parquet"
      },
      {
        "date": "2022-12-16T11:00:00.000Z",
        "voteCount": 1,
        "content": "For 3.) if parquet with partitions, then it should \"overwrite\" mode instead of \"append\". Since, it is \"append\" mode, I think saveAsTable sis more appropriate."
      },
      {
        "date": "2022-08-10T07:50:00.000Z",
        "voteCount": 8,
        "content": "Agree with Pallavi\n1. Partition by\n2. GeographyRegionID, Year, Month, Day \n3. Parquet"
      },
      {
        "date": "2022-11-22T08:49:00.000Z",
        "voteCount": 2,
        "content": "Agree on 1) &amp; 3) but for 2) it should be year/month/day/GeographyRegionId and for each day we would generate several GeographyRegionId.parquet files"
      },
      {
        "date": "2022-11-28T07:42:00.000Z",
        "voteCount": 2,
        "content": "Disregard my comment on 2). Provided answer is the correct one."
      },
      {
        "date": "2022-07-10T11:29:00.000Z",
        "voteCount": 3,
        "content": "Parquet is must (offer higher compression rates)- \"The solution must minimize storage costs.\""
      },
      {
        "date": "2022-06-30T05:29:00.000Z",
        "voteCount": 10,
        "content": "it is the same question on Topic 1 Question 36.\nThen \n1. Partition by\n2. GeographyRegionID, Year, Month, Day\n3. Parquet"
      },
      {
        "date": "2022-05-14T07:06:00.000Z",
        "voteCount": 4,
        "content": "// the correct answer is\n\ndf.write.partitionBy(\"GeographyRegionID\").mode(\"append\").parquet(\"/DBTBL1\")\n\n// or\n\ndf.write.partitionBy(\"GeographyRegionID\",\"Year\",\"Month\",\"Day\").mode(\"append\").parquet(\"/DBTBL1\")\n\n\n// Question says \"minimize storage costs\" so I would select the first one"
      },
      {
        "date": "2022-06-25T10:11:00.000Z",
        "voteCount": 1,
        "content": "Agree, but if you choose the first one, you won't have the daily data"
      },
      {
        "date": "2022-10-28T22:56:00.000Z",
        "voteCount": 2,
        "content": "no mentionning for daily data in the question"
      },
      {
        "date": "2023-05-15T17:53:00.000Z",
        "voteCount": 3,
        "content": "daily incremental load pipelines"
      },
      {
        "date": "2022-04-13T01:55:00.000Z",
        "voteCount": 2,
        "content": "I was wondering if the incremental load is supported for parquet, but since \"append\" mode is used, this should be alright. The question asks to minimize costs, so I go for parquet (not saveAsTable).\npartitionBy\nGeopgraphyRegionID, Year,Month,Day (pipelines per region; daily load)\nparquet"
      },
      {
        "date": "2022-01-25T23:53:00.000Z",
        "voteCount": 9,
        "content": "its recommend to use partitions first before Y/M/D so that they can be managed eazily such as assigning security, or processing by business unit such as zone/country/area etc., GeographyRegionId/Year/Month/Day and Paraquet are answers"
      },
      {
        "date": "2021-12-28T14:18:00.000Z",
        "voteCount": 8,
        "content": "Mes chers amis: \n1.Sortby\n2.GeographyRegionId, Year, Month, Day\n3.Parquet"
      },
      {
        "date": "2021-12-22T20:50:00.000Z",
        "voteCount": 2,
        "content": "only reason for using .parquet is option seems to be dataset path not table else saveastable is right."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67667-exam-dp-203-topic-3-question-15-discussion/",
    "body": "You are designing a security model for an Azure Synapse Analytics dedicated SQL pool that will support multiple companies.<br>You need to ensure that users from each company can view only the data of their respective company.<br>Which two objects should you include in the solution? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta security policy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta custom role-based access control (RBAC) role",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta predicate function\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta column encryption key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tasymmetric keys"
    ],
    "answer": "AC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AC",
        "count": 51,
        "isMostVoted": true
      },
      {
        "answer": "AB",
        "count": 41,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-06T03:35:00.000Z",
        "voteCount": 20,
        "content": "A and B"
      },
      {
        "date": "2021-12-11T14:19:00.000Z",
        "voteCount": 18,
        "content": "Respuesta A/C"
      },
      {
        "date": "2021-12-15T13:24:00.000Z",
        "voteCount": 6,
        "content": "why not RBAC?"
      },
      {
        "date": "2023-01-02T05:13:00.000Z",
        "voteCount": 1,
        "content": "Assuming RBAC is already in place, predicate function for row-level security would be next step. However, it's not clearly stated in question which makes it confusing."
      },
      {
        "date": "2023-03-12T16:42:00.000Z",
        "voteCount": 6,
        "content": "That's why I went with AB instead because it wasn't mentioned. Therefore, we should assume that the system does not already have the RBAC already in place."
      },
      {
        "date": "2024-01-02T03:28:00.000Z",
        "voteCount": 1,
        "content": "see you can not add even row level security bcz you are saying  some company will have access to some of its rows even that is not allowed \n\nAB"
      },
      {
        "date": "2024-01-11T16:34:00.000Z",
        "voteCount": 1,
        "content": "RBAC on storage, no impact on dedicated pool."
      },
      {
        "date": "2024-08-01T10:19:00.000Z",
        "voteCount": 3,
        "content": "Question is malformed.  It does not indicate if all data resides in a single table or if each company has their own tables in the same database."
      },
      {
        "date": "2024-06-24T10:27:00.000Z",
        "voteCount": 1,
        "content": "both needed for RLS"
      },
      {
        "date": "2024-05-27T08:29:00.000Z",
        "voteCount": 1,
        "content": "both are needed for RLS"
      },
      {
        "date": "2024-05-25T21:28:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o:\n\nTo ensure that users from each company can view only the data of their respective company in an Azure Synapse Analytics dedicated SQL pool, you should include the following objects in your solution:\n\nA. a security policy\nC. a predicate function\n\nSecurity Policy (A): A security policy in Azure Synapse Analytics is used to define the conditions under which access to data is granted. This can include row-level security (RLS) policies that control access to rows in a table based on the characteristics of the user executing a query.\n\nPredicate Function (C): A predicate function is used in conjunction with a security policy to enforce row-level security. The predicate function specifies the logic that determines whether a given row should be visible to a particular user. This function is often written as an inline table-valued function that checks user-specific attributes, such as their company affiliation, against the data in the table."
      },
      {
        "date": "2024-05-02T02:50:00.000Z",
        "voteCount": 2,
        "content": "A and C"
      },
      {
        "date": "2024-04-17T13:25:00.000Z",
        "voteCount": 2,
        "content": "Sec Policy &amp; Predicate Function"
      },
      {
        "date": "2024-02-25T05:21:00.000Z",
        "voteCount": 2,
        "content": "Implement RLS by using the&nbsp;CREATE SECURITY POLICY Transact-SQL statement, and predicates created as&nbsp;inline table-valued functions\n\ntherefore, answers are A and C\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=azure-sqldw-latest&amp;preserve-view=true"
      },
      {
        "date": "2024-01-26T14:03:00.000Z",
        "voteCount": 2,
        "content": "Despite their different purposes, security policies and custom RBAC roles share some common elements:\n\nBoth are designed to protect data from unauthorized access.\nBoth can be used to define permissions for users or groups of users.\nBoth can be managed by administrators."
      },
      {
        "date": "2024-01-25T10:02:00.000Z",
        "voteCount": 2,
        "content": "AC - for sure"
      },
      {
        "date": "2024-01-09T20:09:00.000Z",
        "voteCount": 1,
        "content": "A and B"
      },
      {
        "date": "2023-12-24T16:32:00.000Z",
        "voteCount": 2,
        "content": "Chatgpt:\n\nIf only two responses must be selected from the given options, based on the question asked, the two most relevant objects to ensure that users can view only the data of their respective company would be:\n\nA. **A security policy**: This would define the rules and conditions for data access based on company affiliation.\n\nB. **A custom role-based access control (RBAC) role**: This would allow for the assignment of specific access rights depending on the user's company.\n\nEven though a predicate function could be used as part of a security policy implementation, it is typically a component of such a policy, rather than a standalone object. Options D and E are related to encryption and are not directly used to control data views based on the user's company.\n\nTherefore, the two most appropriate answers, according to the question, would be A and B."
      },
      {
        "date": "2023-12-08T13:54:00.000Z",
        "voteCount": 5,
        "content": "Answer is A &amp; C.  Although as many have indicated, the steps are \n\u2022\tCreate the users or groups you want to isolate access.\n\u2022\tCreate the inline table-valued function that will filter the results based on the predicate defined.\n\u2022\tCreate a security policy for the table, assigning the function created above\n\nThe first step may look like \"objects\"/option B but option B says \"A custom role-based access control (RBAC) role.\nIn reality, you would want to create a domain table with companyId and RoleName and create one Role per companyId. (Or maybe a set of roles per companyId depending on what the requirements are).  Then the predicate function would use the meta data driven companyIdRoleName table."
      },
      {
        "date": "2023-11-18T23:55:00.000Z",
        "voteCount": 2,
        "content": "Based on this MS doc, A&amp;C is the right answer\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=azure-sqldw-latest&amp;preserve-view=true"
      },
      {
        "date": "2023-11-05T07:59:00.000Z",
        "voteCount": 1,
        "content": "Question says: \"Which two objects should you include in the solution?\". It seems that answers A, B and C should be part of the solution, so any combination of the 3 should be ok in terms of a valid answer. If the question would asked for \"the sequence of the first 2 steps required to achieve the goal\" then the answer would be B =&gt; C =&gt; A."
      },
      {
        "date": "2023-10-04T04:35:00.000Z",
        "voteCount": 1,
        "content": "It's A &amp; C"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68438-exam-dp-203-topic-3-question-16-discussion/",
    "body": "You have a SQL pool in Azure Synapse that contains a table named dbo.Customers. The table contains a column name Email.<br>You need to prevent nonadministrative users from seeing the full email addresses in the Email column. The users must see values in a format of aXXX@XXXX.com instead.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Microsoft SQL Server Management Studio, set an email mask on the Email column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure portal, set a mask on the Email column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Microsoft SQL Server Management Studio, grant the SELECT permission to the users for all the columns in the dbo.Customers table except Email.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom the Azure portal, set a sensitivity classification of Confidential for the Email column."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-29T08:32:00.000Z",
        "voteCount": 23,
        "content": "I think it's a terrible question, both A(using T-SQL) and B (via GUI) can do the job."
      },
      {
        "date": "2024-03-04T13:19:00.000Z",
        "voteCount": 1,
        "content": "You can do it on Azure Portal if it is a SQL service, on Synapse dedicated SQL pool you can only do it on Synapse Studio. I think only A is 100% correct in either scenario."
      },
      {
        "date": "2022-10-29T05:11:00.000Z",
        "voteCount": 13,
        "content": "Go with A, reason for not B, if email column is string type ,default masking will make it as xxxxxxxx, so here I go with email mask on email column.\nhttps://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql"
      },
      {
        "date": "2023-08-01T23:21:00.000Z",
        "voteCount": 2,
        "content": "but you don't have to opt for the default and the add masking rule step from the link shows the exact same format as in the task. Therefore I would go with B to avoid overthinking :D"
      },
      {
        "date": "2024-09-17T08:08:00.000Z",
        "voteCount": 1,
        "content": "I'm going with A. I asked ChatGPT and it didn't provice documentation source, but explained this:\n\nYou're right to ask for clarification. Unfortunately, Azure documentation does not always explicitly state which features are supported across different services, leading to confusion. However, based on current feature availability, here's the relevant context:\n\nAzure SQL Database supports dynamic data masking (DDM) through both the Azure portal UI and SQL commands.\n\nAzure Synapse Analytics dedicated SQL pools support DDM, but only through T-SQL commands like those executed via SQL Server Management Studio (SSMS) or other query tools.\n\nAzure Synapse Analytics documentation and features do not include portal-based support for dynamic data masking. Instead, the Synapse dedicated SQL pools only allow DDM configuration via T-SQL. You won\u2019t find a corresponding section in the Synapse portal for dynamic data masking, as you would for Azure SQL Database."
      },
      {
        "date": "2024-08-31T01:26:00.000Z",
        "voteCount": 1,
        "content": "In the provided documentation link, if you check the step-by-step guide at number 7, where the documentation instructs you to go to the Azure Portal, you'll see the email DDM format exactly as the question asks: 'aXXX@XXXX.com'. So, I'll go with option B.\n\nYou can view the guide here: https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-configure-portal?view=azuresql"
      },
      {
        "date": "2024-07-05T22:00:00.000Z",
        "voteCount": 1,
        "content": "A is better than B"
      },
      {
        "date": "2024-04-13T03:18:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2023-12-24T16:38:00.000Z",
        "voteCount": 1,
        "content": "Its A ! B can work but not by default.\nCgatgpt :\nBased solely on the information provided in the prompt and considering that any unspecified option would use a default value, the appropriate response would be:\n\nA. From Microsoft SQL Server Management Studio, set an email mask on the Email column.\n\nThis is because option A specifically mentions setting an email mask, which is the type of masking required by the scenario. The other options do not mention configuring a custom masking format for email addresses."
      },
      {
        "date": "2023-10-04T05:06:00.000Z",
        "voteCount": 1,
        "content": "Both A &amp; B are correct capable of achieving the same. But let's go for A."
      },
      {
        "date": "2023-08-30T05:25:00.000Z",
        "voteCount": 2,
        "content": "A or B"
      },
      {
        "date": "2023-09-08T21:42:00.000Z",
        "voteCount": 1,
        "content": "agree with @auwia ,B"
      },
      {
        "date": "2023-08-13T23:16:00.000Z",
        "voteCount": 1,
        "content": "default masking will make it as xxxxxxxx,"
      },
      {
        "date": "2023-07-05T23:43:00.000Z",
        "voteCount": 4,
        "content": "B says just mask and not email mask"
      },
      {
        "date": "2023-06-26T01:44:00.000Z",
        "voteCount": 5,
        "content": "The link provided in the solution is correctly pointing to the solution:&nbsp;Dynamic Data Masking, that is done from the Azure Portal, so the correct answer is B!&nbsp;:)"
      },
      {
        "date": "2022-12-07T23:04:00.000Z",
        "voteCount": 2,
        "content": "email masking option via ssms"
      },
      {
        "date": "2022-11-15T08:50:00.000Z",
        "voteCount": 2,
        "content": "Vote for B becouse of \"You set up a dynamic data masking policy in the Azure portal by selecting the Dynamic Data Masking blade under Security in your SQL Database configuration pane.\"\nSource: https://learn.microsoft.com/en-us/azure/azure-sql/database/dynamic-data-masking-overview?view=azuresql#:~:text=You%20set%20up%20a%20dynamic%20data%20masking%20policy%20in%20the%20Azure%20portal%20by%20selecting%20the%20Dynamic%20Data%20Masking%20blade%20under%20Security%20in%20your%20SQL%20Database%20configuration%20pane."
      },
      {
        "date": "2022-09-21T00:47:00.000Z",
        "voteCount": 1,
        "content": "B correct"
      },
      {
        "date": "2022-08-10T08:20:00.000Z",
        "voteCount": 1,
        "content": "both A and B are correct"
      },
      {
        "date": "2022-07-29T14:06:00.000Z",
        "voteCount": 2,
        "content": "Occams razor with this one"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68572-exam-dp-203-topic-3-question-17-discussion/",
    "body": "You have an Azure Data Lake Storage Gen2 account named adls2 that is protected by a virtual network.<br>You are designing a SQL pool in Azure Synapse that will use adls2 as a source.<br>What should you use to authenticate to adls2?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan Azure Active Directory (Azure AD) user",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared key",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared access signature (SAS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed identity\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-02-04T05:22:00.000Z",
        "voteCount": 10,
        "content": "D is the way we do it in our company. So it works at least."
      },
      {
        "date": "2022-01-06T00:20:00.000Z",
        "voteCount": 8,
        "content": "the answer and explanation given is correct."
      },
      {
        "date": "2024-09-17T08:10:00.000Z",
        "voteCount": 1,
        "content": "\"Managed Identity authentication is required when your storage account is attached to a VNet.\"\n\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/quickstart-bulk-load-copy-tsql-examples#c-managed-identity"
      },
      {
        "date": "2023-08-30T05:27:00.000Z",
        "voteCount": 2,
        "content": "Vnet = managed identity"
      },
      {
        "date": "2023-06-19T16:33:00.000Z",
        "voteCount": 4,
        "content": "VNet = managed identity"
      },
      {
        "date": "2023-05-27T09:47:00.000Z",
        "voteCount": 3,
        "content": "Vnet = managed identity"
      },
      {
        "date": "2023-01-11T16:03:00.000Z",
        "voteCount": 2,
        "content": "Answer is correct.\nThe blow link has more detains.\n\"Analytic capabilities such as Dedicated SQL pool and Serverless SQL pool use multi-tenant infrastructure that is not deployed into the managed virtual network. In order for traffic from these capabilities to access the secured storage account, you must configure access to your storage account based on the workspace's system-assigned managed identity by following the steps below.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/security/connect-to-a-secure-storage-account#grant-your-azure-synapse-workspace-access-to-your-secure-storage-account-as-a-trusted-azure-service"
      },
      {
        "date": "2022-08-11T10:53:00.000Z",
        "voteCount": 3,
        "content": "yes, correct"
      },
      {
        "date": "2022-04-14T17:50:00.000Z",
        "voteCount": 3,
        "content": "Managed identity is correct"
      },
      {
        "date": "2022-02-09T22:21:00.000Z",
        "voteCount": 4,
        "content": "I too I think is correct, anyway for sure it's possible"
      },
      {
        "date": "2021-12-25T11:33:00.000Z",
        "voteCount": 4,
        "content": "I believe so"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67744-exam-dp-203-topic-3-question-18-discussion/",
    "body": "HOTSPOT -<br>You have an Azure Synapse Analytics SQL pool named Pool1. In Azure Active Directory (Azure AD), you have a security group named Group1.<br>You need to control the access of Group1 to specific columns and rows in a table in Pool1.<br>Which Transact-SQL commands should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0031100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0031200001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: GRANT -<br>You can implement column-level security with the GRANT T-SQL statement. With this mechanism, both SQL and Azure Active Directory (Azure AD) authentication are supported.<br><br>Box 2: CREATE SECURITY POLICY -<br>Implement RLS by using the CREATE SECURITY POLICY Transact-SQL statement, and predicates created as inline table-valued functions.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security https://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-12T08:38:00.000Z",
        "voteCount": 21,
        "content": "Correct Answer"
      },
      {
        "date": "2022-01-02T00:03:00.000Z",
        "voteCount": 13,
        "content": "Answer is correct.\nfor Row LEvel Security: https://docs.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver15\n\nFor Column Level Security: https://docs.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/column-level-security"
      },
      {
        "date": "2023-05-27T09:46:00.000Z",
        "voteCount": 2,
        "content": "You are correct! :-)"
      },
      {
        "date": "2024-04-13T03:21:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2024-04-06T13:16:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-08-30T05:30:00.000Z",
        "voteCount": 3,
        "content": "Row = CREATE SECURITY POLICY\nColumn = GRANT"
      },
      {
        "date": "2023-06-19T16:41:00.000Z",
        "voteCount": 3,
        "content": "Row = CREATE SECURITY POLICY \nColumn = GRANT"
      },
      {
        "date": "2023-02-12T05:37:00.000Z",
        "voteCount": 3,
        "content": "correct, as documentation claims:\n\nto control access to the columns)--&gt;Implement RLS by using the CREATE SECURITY POLICY Transact-SQL statement, and predicates created as inline table-valued functions.\n\nto control access to the rows) --&gt;You can implement column-level security with the GRANT T-SQL statement. With this mechanism, both SQL and Azure Active Directory (Azure AD) authentication are supported."
      },
      {
        "date": "2023-06-28T15:05:00.000Z",
        "voteCount": 5,
        "content": "It should be swaped.\nRow = CREATE SECURITY POLICY\nColumn = GRANT"
      },
      {
        "date": "2022-08-11T11:00:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-29T09:20:00.000Z",
        "voteCount": 1,
        "content": "Correct!"
      },
      {
        "date": "2022-04-25T10:06:00.000Z",
        "voteCount": 1,
        "content": "Totally correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68958-exam-dp-203-topic-3-question-19-discussion/",
    "body": "HOTSPOT -<br>You need to implement an Azure Databricks cluster that automatically connects to Azure Data Lake Storage Gen2 by using Azure Active Directory (Azure AD) integration.<br>How should you configure the new cluster? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0031300001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0031300002.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Premium -<br>Credential passthrough requires an Azure Databricks Premium Plan<br>Box 2: Azure Data Lake Storage credential passthrough<br>You can access Azure Data Lake Storage using Azure Active Directory credential passthrough.<br>When you enable your cluster for Azure Data Lake Storage credential passthrough, commands that you run on that cluster can read and write data in Azure Data<br>Lake Storage without requiring you to configure service principal credentials for access to storage.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough",
    "votes": [],
    "comments": [
      {
        "date": "2022-01-24T09:12:00.000Z",
        "voteCount": 14,
        "content": "Correct"
      },
      {
        "date": "2022-01-02T00:13:00.000Z",
        "voteCount": 9,
        "content": "Provided answer is correct\nhttps://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2024-04-17T13:31:00.000Z",
        "voteCount": 1,
        "content": "Yeeeeah"
      },
      {
        "date": "2024-04-13T03:26:00.000Z",
        "voteCount": 1,
        "content": "Premium Tier is better for managing VNet"
      },
      {
        "date": "2023-09-08T21:44:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-09T10:39:00.000Z",
        "voteCount": 3,
        "content": "The question seems outdated. Credential passthrough is a legacy data governance model. Use unity catalogue instead. \nRef: https://learn.microsoft.com/en-us/azure/databricks/data-governance/credential-passthrough/adls-passthrough"
      },
      {
        "date": "2023-06-28T15:10:00.000Z",
        "voteCount": 2,
        "content": "Azure Active Directory credential passthrough requires a Premium plan.\n\nhttps://learn.microsoft.com/en-us/azure/databricks/data-governance/credential-passthrough/adls-passthrough#--requirements"
      },
      {
        "date": "2022-09-02T09:21:00.000Z",
        "voteCount": 3,
        "content": "Given Answer is correct"
      },
      {
        "date": "2022-08-11T11:06:00.000Z",
        "voteCount": 4,
        "content": "correct"
      },
      {
        "date": "2021-12-29T09:02:00.000Z",
        "voteCount": 3,
        "content": "I think answer is correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69575-exam-dp-203-topic-3-question-20-discussion/",
    "body": "You are designing an Azure Synapse solution that will provide a query interface for the data stored in an Azure Storage account. The storage account is only accessible from a virtual network.<br>You need to recommend an authentication mechanism to ensure that the solution can access the source data.<br>What should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta managed identity\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tanonymous public read access",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta shared key"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-01-06T02:14:00.000Z",
        "voteCount": 13,
        "content": "correct"
      },
      {
        "date": "2023-01-24T17:52:00.000Z",
        "voteCount": 7,
        "content": "Whenever you see Vnet , answer is usually managed Identity"
      },
      {
        "date": "2024-06-22T12:54:00.000Z",
        "voteCount": 1,
        "content": "very true"
      },
      {
        "date": "2023-08-30T21:16:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-06-26T02:02:00.000Z",
        "voteCount": 3,
        "content": "Managed Idendity =&nbsp;VNET"
      },
      {
        "date": "2023-06-23T22:01:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-19T16:24:00.000Z",
        "voteCount": 2,
        "content": "A. A managed identity: By assigning a managed identity to the Azure Synapse solution, you can enable it to authenticate and access the Azure Storage account securely. The managed identity acts as a service principal and provides a way to authenticate to Azure services without the need for explicit credentials. By granting the managed identity appropriate permissions on the Azure Storage account, the solution can access the data while ensuring security and avoiding the need for storing and managing explicit credentials.\n\nB. Anonymous public read access is not recommended in this scenario as it would expose the data publicly without any authentication, which can lead to unauthorized access.\n\nC. A shared key is not recommended in this scenario as it involves managing and distributing the storage account's access keys, which can be cumbersome, less secure, and not ideal for scenarios where the storage account is only accessible from a virtual network."
      },
      {
        "date": "2022-09-02T09:23:00.000Z",
        "voteCount": 4,
        "content": "Correct, Managed Identity authentication is required when your storage account is attached to a VNet."
      },
      {
        "date": "2022-08-11T11:10:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-04-06T10:42:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-03-22T11:50:00.000Z",
        "voteCount": 1,
        "content": "the key here is virtual network. Correct!"
      },
      {
        "date": "2022-01-24T09:13:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68573-exam-dp-203-topic-3-question-21-discussion/",
    "body": "You are developing an application that uses Azure Data Lake Storage Gen2.<br>You need to recommend a solution to grant permissions to a specific application for a limited time period.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trole assignments",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tshared access signatures (SAS)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) identities",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\taccount keys"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-12-25T11:45:00.000Z",
        "voteCount": 18,
        "content": "Agree with the answer =&gt; B"
      },
      {
        "date": "2024-04-17T13:34:00.000Z",
        "voteCount": 1,
        "content": "SAS for limited time"
      },
      {
        "date": "2023-08-30T21:20:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-26T02:03:00.000Z",
        "voteCount": 1,
        "content": "Correct."
      },
      {
        "date": "2023-06-19T16:22:00.000Z",
        "voteCount": 2,
        "content": "You are developing an application that uses Azure Data Lake Storage Gen2.\nYou need to recommend a solution to grant permissions to a specific application for a limited time period.\nWhat should you include in the recommendation?\nA. role assignments\nB. shared access signatures (SAS)\nC. Azure Active Directory (Azure AD) identities\nD. account keys"
      },
      {
        "date": "2022-08-11T11:13:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-06-29T09:26:00.000Z",
        "voteCount": 3,
        "content": "the key here is \"limited time period\", so SAS."
      },
      {
        "date": "2022-04-25T10:08:00.000Z",
        "voteCount": 4,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/microsoft/view/67845-exam-dp-203-topic-3-question-22-discussion/",
    "body": "HOTSPOT -<br>You use Azure Data Lake Storage Gen2 to store data that data scientists and data engineers will query by using Azure Databricks interactive notebooks. Users will have access only to the Data Lake Storage folders that relate to the projects on which they work.<br>You need to recommend which authentication methods to use for Databricks and Data Lake Storage to provide the users with the appropriate access. The solution must minimize administrative effort and development effort.<br>Which authentication method should you recommend for each Azure service? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0031600001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0031700001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Personal access tokens -<br>You can use storage shared access signatures (SAS) to access an Azure Data Lake Storage Gen2 storage account directly. With SAS, you can restrict access to a storage account using temporary tokens with fine-grained access control.<br>You can add multiple storage accounts and configure respective SAS token providers in the same Spark session.<br>Box 2: Azure Active Directory credential passthrough<br>You can authenticate automatically to Azure Data Lake Storage Gen1 (ADLS Gen1) and Azure Data Lake Storage Gen2 (ADLS Gen2) from Azure Databricks clusters using the same Azure Active Directory (Azure AD) identity that you use to log into Azure Databricks. When you enable your cluster for Azure Data Lake<br>Storage credential passthrough, commands that you run on that cluster can read and write data in Azure Data Lake Storage without requiring you to configure service principal credentials for access to storage.<br>After configuring Azure Data Lake Storage credential passthrough and creating storage containers, you can access data directly in Azure Data Lake Storage<br>Gen1 using an adl:// path and Azure Data Lake Storage Gen2 using an abfss:// path:<br>Reference:<br>https://docs.microsoft.com/en-us/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sas-access https://docs.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-13T11:16:00.000Z",
        "voteCount": 63,
        "content": "Accessing the ADLS via Databricks should be using Azure Active Directory with Passthrough. Accessing the files in ADLS should be SAS, based on the options provided.\n\nThe explanation provided for this question is incorrect."
      },
      {
        "date": "2022-01-16T20:39:00.000Z",
        "voteCount": 2,
        "content": "To be more clear, for box it shall be user delegation SAS which is secured with ADD credentials."
      },
      {
        "date": "2022-12-15T11:32:00.000Z",
        "voteCount": 4,
        "content": "This is it. Correct"
      },
      {
        "date": "2022-01-11T10:08:00.000Z",
        "voteCount": 15,
        "content": "1. Accessing the Databricks should be using Personal Tokens\n2. Accessing the ADLS should be using Shared Access Signatures. (Because of controlled access to project folders they work)."
      },
      {
        "date": "2024-07-09T08:45:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT 4o\nAzure Active Directory (Azure AD) passthrough for both"
      },
      {
        "date": "2024-07-03T03:16:00.000Z",
        "voteCount": 1,
        "content": "The whole community is confused. Everyone has their own answers and explanations. No consensus whatsoever :("
      },
      {
        "date": "2024-06-22T04:30:00.000Z",
        "voteCount": 1,
        "content": "Personal Token\nShared Access Signatures"
      },
      {
        "date": "2024-04-13T03:32:00.000Z",
        "voteCount": 1,
        "content": "1. Personal Token\n2. Passthrough"
      },
      {
        "date": "2024-04-06T13:30:00.000Z",
        "voteCount": 1,
        "content": "Personal access token\nCredential Pass through"
      },
      {
        "date": "2023-08-30T21:29:00.000Z",
        "voteCount": 4,
        "content": "box1 Azure Active Directory with Passthrough\nbox2 SAS"
      },
      {
        "date": "2023-07-06T00:37:00.000Z",
        "voteCount": 4,
        "content": "Box 1 - Pass through Databricks\nBox 2 - SAS - DL Gen 2"
      },
      {
        "date": "2023-06-26T02:14:00.000Z",
        "voteCount": 4,
        "content": "Databricks: Azure Active Directory credential passthrough or personal access tokens.\nData Lake Storage: Azure Active Directory credential passthrough.\nPlease note that while shared access keys and shared access signatures are valid authentication methods for Data Lake Storage, they do not meet the requirement of minimizing administrative effort and providing granular access control based on projects in this scenario."
      },
      {
        "date": "2023-05-05T08:04:00.000Z",
        "voteCount": 1,
        "content": "I think the answers given are correct. The question is which authentication to use \"for\" Databricks and Gen2. So we look at authenticating for (or \"into\")  either of them. The question then becomes which authentication can you use to access databricks and then through that which authentication can you use to authenticate for gen2?"
      },
      {
        "date": "2023-06-20T13:03:00.000Z",
        "voteCount": 1,
        "content": "Personal Access Tokens are an alternative authentication method for Azure Databricks that can be used to authenticate to the Databricks REST API and to access Databricks resources. While PATs can provide a high level of security, they require more administrative effort to manage and maintain than Azure Active Directory Credential Passthrough."
      },
      {
        "date": "2022-12-04T05:52:00.000Z",
        "voteCount": 1,
        "content": "As we need to access Databricks via ADLS use Azure Databricks access tokens or AAD tokens as explained here: https://learn.microsoft.com/en-us/azure/databricks/dev-tools/api/latest/aad/\nData Lake Storage with Passtrough"
      },
      {
        "date": "2022-11-30T06:54:00.000Z",
        "voteCount": 6,
        "content": "Both should be Azure Active Directory with Passthrough\n1. Shared Key and SAS authorization grants access to a user (or application) without requiring them to have an identity in Azure Active Directory (Azure AD). With these two forms of authentication, Azure RBAC and ACLs have no effect.\nACLs let you grant \"fine-grained\" access, such as write access to a specific directory or file.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model\nAzure AD provides superior security and ease of use over Shared Key for authorizing requests to Blob storage. For more information, see Authorize access to data in Azure Storage.\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/security-recommendations\n\n2. Azure AD Passthrough will ensure a user can only access the data that they have previously been granted access to via Azure AD in ADLS Gen2.\nhttps://www.databricks.com/blog/2019/10/24/simplify-data-lake-access-with-azure-ad-credential-passthrough.html"
      },
      {
        "date": "2022-10-19T10:02:00.000Z",
        "voteCount": 4,
        "content": "Databricks- Azure Active Directory with Passthrough\nhttps://learn.microsoft.com/en-us/azure/databricks/security/credential-passthrough/adls-passthrough\nData Lake Storage - SAS\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model"
      },
      {
        "date": "2022-09-17T23:41:00.000Z",
        "voteCount": 5,
        "content": "the question is about how to authenticate the ADLS gen2 dataset both in Databricks and ADLSGen2... Its not about how you authenticate the Databricks.\n\n1)  Credential Pass through\n2) SAS"
      },
      {
        "date": "2023-01-30T07:42:00.000Z",
        "voteCount": 1,
        "content": "I agree with you, plus looking at the definitions here:\n\n-) SAS = A shared access signature provides secure delegated access to resources in your storage account. With a SAS, you have granular control over how a client can access your data\n-) Azure Active Directory with Passthrough = Credential passthrough allows you to authenticate automatically to Azure Data Lake Storage from Azure Databricks clusters using the identity that you use to log in to Azure Databricks.\n-) Shared Access Key = Access keys give you full rights to everything in your storage account\n\nThe more explicit question will be:\nWhich authentication method should you recommend for each Azure service to provide the users with the appropriate access? \n1) how to authenticate the ADLS gen2 dataset using databricks? ---&gt; Credential Pass through\n2) how to authenticate the ADLS gen2 dataset using Data Lake Storage? ---&gt; SAS"
      },
      {
        "date": "2023-01-30T08:05:00.000Z",
        "voteCount": 1,
        "content": "Sorry but I missed completely one definition:\n-) personal acces token =  Personal Access Tokens (PATs) can be used to authenticate to the Databricks REST API, allowing for programmatic access to your Databricks workspace\n\nSo by using a PAT, you can automate data movements between Databricks and Data Lake Storage Gen 2 and control user permission to appropriate access\n\nCorrect answer should be:\n1) how to authenticate the ADLS gen2 dataset using databricks? ---&gt; personal acces token\n2) how to authenticate the ADLS gen2 dataset using Data Lake Storage? ---&gt; SAS"
      },
      {
        "date": "2022-08-12T07:50:00.000Z",
        "voteCount": 1,
        "content": "Given answer seems correct, agree with HaBroNounen's explanation"
      },
      {
        "date": "2022-07-29T09:28:00.000Z",
        "voteCount": 2,
        "content": "Azure Data Lake Storage Gen2 also supports Shared Key and SAS methods for authentication.\nTo authenticate to and access Databricks REST APIs, you can use Azure Databricks personal access tokens or Azure Active Directory (Azure AD) tokens"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/microsoft/view/69577-exam-dp-203-topic-3-question-23-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named Contacts. Contacts contains a column named Phone.<br>You need to ensure that users in a specific role only see the last four digits of a phone number when querying the Phone column.<br>What should you include in the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ttable partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta default value",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security (RLS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn encryption",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-06T02:30:00.000Z",
        "voteCount": 14,
        "content": "correct"
      },
      {
        "date": "2022-01-24T09:17:00.000Z",
        "voteCount": 5,
        "content": "Correct"
      },
      {
        "date": "2024-01-14T22:23:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-09-12T09:19:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is row level security (\"to allow specific roles\")\n\nSee https://learn.microsoft.com/en-us/azure/data-explorer/kusto/management/rowlevelsecuritypolicy\n\nMore use cases\n\n    A call center support person may identify callers by several digits of their social security number. This number shouldn't be fully exposed to the support person. An RLS policy can be applied on the table to mask all but the last four digits of the social security number in the result set of any query."
      },
      {
        "date": "2023-08-30T21:30:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2022-11-20T13:00:00.000Z",
        "voteCount": 2,
        "content": "And Topic 3 question 24 is column-level encryption?"
      },
      {
        "date": "2023-02-12T05:59:00.000Z",
        "voteCount": 3,
        "content": "I think the key is \"when querying the Phone column\". Column encryption encrypts individual columns of database on db level, instead Dynamic data masking  does not store masked data, only display it."
      },
      {
        "date": "2022-08-12T07:56:00.000Z",
        "voteCount": 2,
        "content": "correct!"
      },
      {
        "date": "2022-04-25T10:13:00.000Z",
        "voteCount": 4,
        "content": "Correct!"
      },
      {
        "date": "2022-02-17T13:40:00.000Z",
        "voteCount": 4,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74500-exam-dp-203-topic-3-question-24-discussion/",
    "body": "You are designing database for an Azure Synapse Analytics dedicated SQL pool to support workloads for detecting ecommerce transaction fraud.<br>Data will be combined from multiple ecommerce sites and can include sensitive financial information such as credit card numbers.<br>You need to recommend a solution that meets the following requirements:<br>Users must be able to identify potentially fraudulent transactions.<br><img src=\"/assets/media/exam-media/04259/0031800001.png\" class=\"in-exam-image\"><br>\u2711 Users must be able to use credit cards as a potential feature in models.<br>\u2711 Users must NOT be able to access the actual credit card numbers.<br>What should you include in the recommendation?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransparent Data Encryption (TDE)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security (RLS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn-level encryption\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAzure Active Directory (Azure AD) pass-through authentication"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-04-25T10:25:00.000Z",
        "voteCount": 15,
        "content": "By discard, is C, you can create a symetric key to encript a data, for example one column, and then use this data as feature of the model\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/encryption/encrypt-a-column-of-data?view=sql-server-ver15\nThe other options that not meet the requeriments:\n- TDE encript data, but decrypt when you query https://docs.microsoft.com/en-us/azure/azure-sql/database/transparent-data-encryption-tde-overview?tabs=azure-portal\n- RLS is for row restriction, not meet the requeriment\n- Azure AD pass-through is for authentication"
      },
      {
        "date": "2023-08-30T21:33:00.000Z",
        "voteCount": 1,
        "content": "C. Column-level encryption"
      },
      {
        "date": "2023-07-20T21:55:00.000Z",
        "voteCount": 3,
        "content": "C. Column-level encryption\n\nExplanation:\nThe given requirement is to enable users to utilize credit card data for model features but to not have access to the actual credit card numbers. Column-level encryption serves this purpose best as it allows for specific columns (in this case, the credit card number column) to be encrypted, while still enabling operations on the data.\n\nA. Transparent Data Encryption (TDE): This encrypts the physical files of the database, but not specific columns. It doesn't fit the requirement here.\n\nB. Row-level security (RLS): This restricts data access at the row level based on certain filters. It doesn't offer column-specific security, and thus isn't the best choice here.\n\nD. Azure Active Directory (Azure AD) pass-through authentication: This is an authentication method, not an encryption method. It would not be applicable for protecting specific data within the database."
      },
      {
        "date": "2023-01-12T16:42:00.000Z",
        "voteCount": 2,
        "content": "Looks like the column level encryption is still in preview.\nhttps://azure.microsoft.com/en-us/updates/columnlevel-encryption-for-azure-synapse-analytics/"
      },
      {
        "date": "2023-01-12T16:41:00.000Z",
        "voteCount": 1,
        "content": "IS column level encryption supported on Dedicated SQL Pools? The question is relate to Dedicated Pool?"
      },
      {
        "date": "2022-08-12T08:02:00.000Z",
        "voteCount": 1,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/microsoft/view/74501-exam-dp-203-topic-3-question-25-discussion/",
    "body": "You have an Azure subscription linked to an Azure Active Directory (Azure AD) tenant that contains a service principal named ServicePrincipal1. The subscription contains an Azure Data Lake Storage account named adls1. Adls1 contains a folder named Folder2 that has a URI of https://adls1.dfs.core.windows.net/ container1/Folder1/Folder2/.<br>ServicePrincipal1 has the access control list (ACL) permissions shown in the following table.<br><img src=\"/assets/media/exam-media/04259/0031900003.png\" class=\"in-exam-image\"><br>You need to ensure that ServicePrincipal1 can perform the following actions:<br>\u2711 Traverse child items that are created in Folder2.<br>\u2711 Read files that are created in Folder2.<br>The solution must use the principle of least privilege.<br>Which two permissions should you grant to ServicePrincipal1 for Folder2? Each correct answer presents part of the solution.<br>NOTE: Each correct selection is worth one point.<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess \u05d2\u20ac\" Read",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess \u05d2\u20ac\" Write",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess \u05d2\u20ac\" Execute\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefault \u05d2\u20ac\" Read\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefault \u05d2\u20ac\" Write",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDefault \u05d2\u20ac\" Execute"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "DF",
        "count": 27,
        "isMostVoted": false
      },
      {
        "answer": "AF",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-14T08:00:00.000Z",
        "voteCount": 21,
        "content": "Phrased different, the question for me says: if you create \"Folder3\" inside Folder2, you should be able to read files created in Folder3.\n\nThis means that you for sure need Executive and Read premissions to Folder2 (Executive to traverse child folder, read to read the files).\n\nNow, starting from the least privilege, suppose you give \"Access\" permission both for read and execute. In this case, you can't read files created in Folder3. This is a requirement (\"child items that are created in Folder2\"), so you need Default Read access.\n\nYou don't need Default Execute, otherwise you would have access to a Folder created in Folder3 (say Folder 4) and this is not required so for the least privilege you must give Access Execute and not Defualt Execute."
      },
      {
        "date": "2024-06-22T14:08:00.000Z",
        "voteCount": 3,
        "content": "Given Answers (D&amp;F) are correct....Reason is basic difference between Access and Default ACLs\nAccess ACL: is for existing items.\nDefault ACL: is template ACL for new Items to be created.\nHere question says traverse and read child items that created in folder2. So Access ACLs will fail to provide access to new files so we need to add Default ACL's for new files"
      },
      {
        "date": "2023-01-13T07:51:00.000Z",
        "voteCount": 3,
        "content": "Requirement 1 says Traverse child items that are created in Folder2. Means that you need to be able to travers the subFolders under Folder2. So Defaut:Execute is a required permission."
      },
      {
        "date": "2022-10-27T00:54:00.000Z",
        "voteCount": 14,
        "content": "C - You need to traverse the FOlder2 only and no potential children folders - Principals of least privelage.\nD- You need to pass on the READ access to the files in Folder2. Default ACLs are not passed to files but we are not setting the permission on a file level, we are setting it on Folder2."
      },
      {
        "date": "2024-01-26T15:41:00.000Z",
        "voteCount": 1,
        "content": "cannot agree more, and do not need to over think :)"
      },
      {
        "date": "2024-07-04T08:06:00.000Z",
        "voteCount": 1,
        "content": "The link https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control explains that \"Default ACLs are templates of ACLs associated with a directory that determine the access ACLs for any child items that are created under that directory. Files do not have default ACLs.\"\nNow the requirement here is to \n1) Traverse the child items that will be created within folder2 \n2) Read the files that will be created within folder2. \nThe question states that the child items(both folders and files) within the folder2 will get created i.e, IT IS NOT YET CREATED and WILL GET CREATED IN THE FUTURE which means the access has to be at the root directory level which is folder2 here. And as per the Microsoft documentation, only Default ACLs will work because Access ACLs control access to an object(file or directory). Choosing Access ACL would mean each time a new child item getting created with folder2, the Access ACL has to explicitly set for that child item at that time."
      },
      {
        "date": "2024-07-03T05:15:00.000Z",
        "voteCount": 2,
        "content": "Same problem. Everyone has different answers. No one knows which answer is correct. Worst part is even Gemini disagrees with ChatGPT :("
      },
      {
        "date": "2024-04-25T21:28:00.000Z",
        "voteCount": 1,
        "content": "A and F"
      },
      {
        "date": "2024-04-17T14:06:00.000Z",
        "voteCount": 1,
        "content": "Access ACLs control access to an object. Files and directories both have access ACLs.\nDefault ACLs are templates of ACLs associated with a directory that determine the access\nACLs for any child items that are created under that directory. Files do not have default ACLs."
      },
      {
        "date": "2024-02-24T17:51:00.000Z",
        "voteCount": 2,
        "content": "Traverse child items that are created in Folder2 --&gt; Default Execute\nRead files that are created in Folder2 --&gt; Access Read"
      },
      {
        "date": "2024-01-03T14:39:00.000Z",
        "voteCount": 6,
        "content": "I'm going with AF and here is why.\nThe requirement \"Traverse child items that are created in Folder2\" -&gt; This requires default execute so that if any child folders under folder2 get created, the user can list those folders and files.\nNow, because of principle of least privilege, it does NOT say that if a file is created under a subfolder (like folder2/folder2/file1.json) that they need access to it.\nSo, it should be Access Read on folder2 so that the users only get read access to the files in folder2 and not in /folder2/folder3/*.json, for instance."
      },
      {
        "date": "2023-12-30T23:49:00.000Z",
        "voteCount": 1,
        "content": "Default Execute and Default Read as you don\u00b4t know in advance the files/folder to be created, and you need to access to all of them."
      },
      {
        "date": "2023-08-30T21:47:00.000Z",
        "voteCount": 2,
        "content": "\"Default - Read\" and \"Default - Execute\""
      },
      {
        "date": "2023-08-14T03:00:00.000Z",
        "voteCount": 1,
        "content": "Traverse require access execute, file reads need default read"
      },
      {
        "date": "2023-07-06T01:25:00.000Z",
        "voteCount": 4,
        "content": "Default Execute is mandatory to traverse child items through cascade.. Default Read by  process of elimination"
      },
      {
        "date": "2023-06-26T02:41:00.000Z",
        "voteCount": 4,
        "content": "\u2711 Traverse child items that are created in Folder2. =&gt;&nbsp;DEFAULT EXECUTE\n\u2711 Read files that are created in Folder2. =&gt; ACCESS READ (that was already given)."
      },
      {
        "date": "2023-03-13T02:49:00.000Z",
        "voteCount": 6,
        "content": "Based on the permissions table provided, the ServicePrincipal1 has \"Access - Execute\" permission on container1, \"Access - Execute\" permission on Folder1, and \"Access - Read\" permission on Folder2. To allow ServicePrincipal1 to traverse child items that are created in Folder2 and read files created in Folder2, you should grant the \"Default - Read\" and \"Default - Execute\" permissions on Folder2. The \"Default - Read\" permission allows ServicePrincipal1 to read files created in Folder2, and the \"Default - Execute\" permission allows ServicePrincipal1 to traverse child items that are created in Folder2.\n\nTherefore, the correct answer is:\nD. Default - Read\nF. Default - Execute"
      },
      {
        "date": "2023-01-13T07:49:00.000Z",
        "voteCount": 4,
        "content": "Traverse child items that are created in Folder2.\nThis needs Default:Execute Because user needs to traverse any child Items(Sub Folders) created under under Folder2. \nRead files that are created in Folder2.\nSince the The Access:read ACL is already set on Folder2.Any files that are created under Folder2 can be access by User. But to see (or list) the items/files under Folder2 we need Access:Execute .\nSO the answer is Access: Execute and Default: Execute"
      },
      {
        "date": "2022-12-03T16:32:00.000Z",
        "voteCount": 6,
        "content": "Default Read and Execute are required. The reason is as below.\n\nIn the POSIX-style model that's used by Data Lake Storage Gen2, permissions for an item are stored on the item itself. In other words, permissions for an item cannot be inherited from the parent items if the permissions are set after the child item has already been created. Permissions are only inherited if default permissions have been set on the parent items before the child items have been created.\nReference: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2022-08-12T08:38:00.000Z",
        "voteCount": 4,
        "content": "so the answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/microsoft/view/68731-exam-dp-203-topic-3-question-26-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that is linked to a hybrid Azure Active Directory (Azure AD) tenant. The subscription contains an Azure Synapse Analytics SQL pool named Pool1.<br>You need to recommend an authentication solution for Pool1. The solution must support multi-factor authentication (MFA) and database-level authentication.<br>Which authentication solution or solutions should you include in the recommendation? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0032100001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0032200001.png\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure AD authentication -<br>Azure AD authentication has the option to include MFA.<br><br>Box 2: Contained database users -<br>Azure AD authentication uses contained database users to authenticate identities at the database level.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-mfa-ssms-overview https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview",
    "votes": [],
    "comments": [
      {
        "date": "2021-12-28T08:16:00.000Z",
        "voteCount": 22,
        "content": "Correct"
      },
      {
        "date": "2022-04-13T00:26:00.000Z",
        "voteCount": 8,
        "content": "\"SQL Database and Azure Synapse Analytics support Azure Active Directory identities as contained database users\"\nhttps://docs.microsoft.com/en-us/sql/relational-databases/security/contained-database-users-making-your-database-portable?view=sql-server-ver15#contained-database-user-model"
      },
      {
        "date": "2024-09-17T05:30:00.000Z",
        "voteCount": 1,
        "content": "Azure AD will enforce MFA\nContainer Users can map to AD and enforce MFA through that"
      },
      {
        "date": "2023-08-30T21:51:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2023-06-20T13:54:00.000Z",
        "voteCount": 2,
        "content": "Azure Synapse Analytics supports two types of database-level authentication:\n\nAzure Active Directory (Azure AD) authentication: This uses your Azure AD identity to authenticate to Synapse SQL. This is the recommended authentication method, as it provides a single sign-on experience and allows you to manage permissions using Azure AD groups.\nSQL Server authentication: This uses a traditional SQL Server username and password to authenticate to Synapse SQL. This authentication method is less secure than Azure AD authentication, but it may be necessary if you are using legacy applications that do not support Azure AD."
      },
      {
        "date": "2022-08-07T03:30:00.000Z",
        "voteCount": 4,
        "content": "answer is correct"
      },
      {
        "date": "2022-01-25T01:39:00.000Z",
        "voteCount": 4,
        "content": "B is wrong. Contained users not supported by synapse analytics. D is correct ('MS SQL Server logins')"
      },
      {
        "date": "2022-01-30T23:57:00.000Z",
        "voteCount": 18,
        "content": "https://docs.microsoft.com/en-us/azure/azure-sql/database/authentication-aad-overview this document says contained users are supported by synapse analytics, so this is correct answer."
      },
      {
        "date": "2022-02-03T06:02:00.000Z",
        "voteCount": 3,
        "content": "correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/microsoft/view/52742-exam-dp-203-topic-3-question-27-discussion/",
    "body": "DRAG DROP -<br>You have an Azure data factory.<br>You need to ensure that pipeline-run data is retained for 120 days. The solution must ensure that you can query the data by using the Kusto query language.<br>Which four actions should you perform in sequence? To answer, move the appropriate actions from the list of actions to the answer area and arrange them in the correct order.<br>NOTE: More than one order of answer choices is correct. You will receive credit for any of the correct orders you select.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0032300001.png\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0032400001.png\" class=\"in-exam-image\">",
    "answerDescription": "Step 1: Create an Azure Storage account that has a lifecycle policy<br>To automate common data management tasks, Microsoft created a solution based on Azure Data Factory. The service, Data Lifecycle Management, makes frequently accessed data available and archives or purges other data according to retention policies. Teams across the company use the service to reduce storage costs, improve app performance, and comply with data retention policies.<br>Step 2: Create a Log Analytics workspace that has Data Retention set to 120 days.<br>Data Factory stores pipeline-run data for only 45 days. Use Azure Monitor if you want to keep that data for a longer time. With Monitor, you can route diagnostic logs for analysis to multiple different targets, such as a Storage Account: Save your diagnostic logs to a storage account for auditing or manual inspection. You can use the diagnostic settings to specify the retention time in days.<br>Step 3:  From Azure Portal, add a diagnostic setting.<br>Step 4: Send the data to a log Analytics workspace,<br>Event Hub: A pipeline that transfers events from services to Azure Data Explorer.<br>Keeping Azure Data Factory metrics and pipeline-run data.<br>Configure diagnostic settings and workspace.<br>Create or add diagnostic settings for your data factory.<br>1. In the portal, go to Monitor. Select Settings &gt; Diagnostic settings.<br>2. Select the data factory for which you want to set a diagnostic setting.<br>3. If no settings exist on the selected data factory, you're prompted to create a setting. Select Turn on diagnostics.<br>4. Give your setting a name, select Send to Log Analytics, and then select a workspace from Log Analytics Workspace.<br>5. Select Save.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor",
    "votes": [],
    "comments": [
      {
        "date": "2021-06-06T12:18:00.000Z",
        "voteCount": 187,
        "content": "Step 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace."
      },
      {
        "date": "2022-01-28T23:02:00.000Z",
        "voteCount": 5,
        "content": "Shouldn't it need to swap step 3 &amp; 4?"
      },
      {
        "date": "2022-08-07T03:38:00.000Z",
        "voteCount": 1,
        "content": "seems correct to me"
      },
      {
        "date": "2023-08-30T21:59:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2022-08-22T22:36:00.000Z",
        "voteCount": 5,
        "content": "This is correct order, I have tried this on Azure portal."
      },
      {
        "date": "2021-07-12T16:54:00.000Z",
        "voteCount": 39,
        "content": "step 1. From Azure Portal, add a diagnostic setting.\nstep 2. Send data to a Log analytics workspace.\nstep 3. Create a Log Analytics workspace that has Data Retention set to 120 days.\nstep 4. Select the PipelineRuns Category.\n\nThe video in the below link walks you through the process step by step, start watching at 2min 30sec mark\n\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data"
      },
      {
        "date": "2021-08-02T04:08:00.000Z",
        "voteCount": 1,
        "content": "This is the correct answer"
      },
      {
        "date": "2022-01-05T13:01:00.000Z",
        "voteCount": 1,
        "content": "Don't you have to select PipelineRuns Category while adding a diagnostic setting?"
      },
      {
        "date": "2022-02-13T14:50:00.000Z",
        "voteCount": 2,
        "content": "Video matches the steps above. Thanks for sharing."
      },
      {
        "date": "2022-05-22T23:56:00.000Z",
        "voteCount": 4,
        "content": "I don't agree with you, why \"Send data to a Log analytics workspace\" is step2, but \"Create the Log Analytics workspace\" is step3 ? how to use Log analytics workspace if the Log analytics workspace hasn't been created?"
      },
      {
        "date": "2024-02-06T15:39:00.000Z",
        "voteCount": 1,
        "content": "Sunnyb is correct"
      },
      {
        "date": "2022-07-26T08:46:00.000Z",
        "voteCount": 2,
        "content": "Can see multiple answers that are correct in the discussion!\nAlso note the question states : \"More than one order of answer choices is correct\""
      },
      {
        "date": "2022-06-19T04:00:00.000Z",
        "voteCount": 1,
        "content": "Output is either SA, LA or Eventhub\nRetention is configured during setting up the diag on any Azure resource , so take out option 1 which says configure SA retention.\nJust stick to LA solution and include all the points related to it."
      },
      {
        "date": "2021-08-24T16:51:00.000Z",
        "voteCount": 12,
        "content": "I am not very familiar with this topic, but follow the link below, we can know With Monitor, you can route diagnostic logs for analysis to multiple different targets: Storage account, Event Hub and Log Analytics. It also needs to query the data by use Kusto query language, so we can know we should use Log Analytics for this scenario. With this in mind, we can exclude anything related with storage account and Event Hub. Then the question talks about Pipeline runs log, so we can also exclude the Trigger run log one. Then there are 4 options left there as listed in the solution raised by @Sunnyb.\nhttps://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data"
      },
      {
        "date": "2021-08-17T07:28:00.000Z",
        "voteCount": 9,
        "content": "in this case we will not use a storage Account to save the diagnostic logs to a storage account, but we will send them to Log Analytics:\n1: Create a Log Analytics workspace that has Data Retention set to 120 days.\n2: From Azure Portal, add a diagnostic setting.\n3: Select the PipelineRuns Category\n4: Send the data to a Log Analytics workspace"
      },
      {
        "date": "2021-08-05T01:37:00.000Z",
        "voteCount": 2,
        "content": "If you create diagnostics from the Datafactory you wil notice that you can only set the retentiondays when you select a storage account for the PipelineRuns. So you need a storage account first. You do not have an option in the selection to create a diagnostic from the datafactory and thus the option \"select the pipelineruns\" is not an option. I agree with the current selection."
      },
      {
        "date": "2021-08-07T04:49:00.000Z",
        "voteCount": 2,
        "content": "To complete my answer. I also agree with \"Sunnyb\". There are more solutions to this question."
      },
      {
        "date": "2021-09-15T08:31:00.000Z",
        "voteCount": 1,
        "content": "When you create diagnostic, you have to select \"Log Analytics\" as destination target. Log Analytics Workspace has it own Data Retention Properties under General/Usage and Estimated Cost/Data Retention. So the good answer is:Step 1: Create a Log Analytics workspace that has Data Retention set to 120 days.\nStep 2: From Azure Portal, add a diagnostic setting.\nStep 3: Select the PipelineRuns Category\nStep 4: Send the data to a Log Analytics workspace."
      },
      {
        "date": "2021-06-25T01:08:00.000Z",
        "voteCount": 4,
        "content": "According to the linked article, it's: first Storage Account, then Event Hub, and finally Log Analytics.\nSo I would say:\n1- Create an Azure Storage Account with a lifecycle policy\n2- Stream to an Azure Event Hub\n3- Create a Log Analytics workspace that has a Data Retention set to 120 days\n4- Send the data to a Log Analytics Workspace\nSource: https://docs.microsoft.com/en-us/azure/data-factory/monitor-using-azure-monitor#keeping-azure-data-factory-metrics-and-pipeline-run-data"
      },
      {
        "date": "2021-05-27T13:14:00.000Z",
        "voteCount": 2,
        "content": "Take off the storage account and After add diagnostic setting it would be select pipelineruns then send to log analytics"
      },
      {
        "date": "2021-05-15T06:30:00.000Z",
        "voteCount": 1,
        "content": "regarding the storage account, what is it for?!"
      },
      {
        "date": "2021-05-15T14:59:00.000Z",
        "voteCount": 2,
        "content": "I don't know if you need to, see this discussion: https://www.examtopics.com/discussions/microsoft/view/49811-exam-dp-200-topic-3-question-19-discussion/"
      },
      {
        "date": "2022-04-13T00:37:00.000Z",
        "voteCount": 1,
        "content": "In this case, not needed (imo). MS advises to store log data in a storage account (if needed) since Data Factory only retains it for 45 days. However, in this case you don't have to store it longer than 2 years and you want to use Kusto, so Log Analytics makes more sense."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/microsoft/view/55467-exam-dp-203-topic-3-question-28-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool.<br>You need to ensure that data in the pool is encrypted at rest. The solution must NOT require modifying applications that query the data.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable encryption at rest for the Azure Data Lake Storage Gen2 account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Transparent Data Encryption (TDE) for the pool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a customer-managed key to enable double encryption for the Azure Synapse workspace.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Azure key vault in the Azure subscription grant access to the pool."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-06-16T14:26:00.000Z",
        "voteCount": 39,
        "content": "Correct!"
      },
      {
        "date": "2024-01-14T22:50:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-08-30T22:01:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-06-28T15:31:00.000Z",
        "voteCount": 1,
        "content": "Transparent Data Encryption (TDE) is a feature provided by Azure SQL Database and Azure Synapse Analytics that encrypts the database files at rest. It performs real-time I/O encryption and decryption of the database files, ensuring that the data is encrypted on disk. TDE operates transparently and does not require any changes to the application code or queries.\n\nBy enabling TDE for the dedicated SQL pool in Azure Synapse Analytics, you can achieve encryption at rest for the data stored in the pool without impacting the applications that access the data."
      },
      {
        "date": "2022-08-07T03:42:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-05-09T02:59:00.000Z",
        "voteCount": 1,
        "content": "B is right, however using CMK configed at workspace level to achieve double encryption is also right. \nhttps://docs.microsoft.com/en-us/azure/synapse-analytics/security/workspaces-encryption"
      },
      {
        "date": "2022-12-08T23:13:00.000Z",
        "voteCount": 2,
        "content": "you can only enable double encryption when you are creating a new workspace."
      },
      {
        "date": "2022-02-18T08:37:00.000Z",
        "voteCount": 3,
        "content": "TDE is used for encrypting data at rest."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80798-exam-dp-203-topic-3-question-29-discussion/",
    "body": "DRAG DROP -<br>You have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named storage1. Storage1 contains a container named container1.<br>Container1 contains a directory named directory1. Directory1 contains a file named file1.<br>You have an Azure Active Directory (Azure AD) user named User1 that is assigned the Storage Blob Data Reader role for storage1.<br>You need to ensure that User1 can append data to file1. The solution must use the principle of least privilege.<br>Which permissions should you grant? To answer, drag the appropriate permissions to the correct resources. Each permission may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br>Select and Place:<br><img src=\"/assets/media/exam-media/04259/0032600001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0032700001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Execute -<br>If you are granting permissions by using only ACLs (no Azure RBAC), then to grant a security principal read or write access to a file, you'll need to give the security principal Execute permissions to the root folder of the container, and to each folder in the hierarchy of folders that lead to the file.<br><br>Box 2: Execute -<br>On Directory: Execute (X): Required to traverse the child items of a directory<br><br>Box 3: Write -<br>On file: Write (W): Can write or append to a file.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-06T21:18:00.000Z",
        "voteCount": 13,
        "content": "-Execute\n-Execute\n-Write"
      },
      {
        "date": "2023-08-09T23:48:00.000Z",
        "voteCount": 2,
        "content": "Supported by the following two references:\n\nwithout additional permissions: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nwith additional permissions such as storage blob data reader: https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model#permissions-table-combining-azure-rbac-abac-and-acls"
      },
      {
        "date": "2023-08-29T12:35:00.000Z",
        "voteCount": 5,
        "content": "In the above link, the use case is given for appending to Data.txt file, then the answers would be \n-Execute\n-Execute\n-Read and Write"
      },
      {
        "date": "2024-02-26T10:51:00.000Z",
        "voteCount": 2,
        "content": "you don't need to read a file to be able to append data to it"
      },
      {
        "date": "2022-09-11T00:58:00.000Z",
        "voteCount": 6,
        "content": "Correct : Execute to traverse the folders and Write to append the file"
      },
      {
        "date": "2024-04-07T08:41:00.000Z",
        "voteCount": 1,
        "content": "Exe; Exe; Write"
      },
      {
        "date": "2024-01-10T14:57:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nmust be rw- for the file"
      },
      {
        "date": "2023-09-08T18:25:00.000Z",
        "voteCount": 3,
        "content": "X X RW need both rw for append"
      },
      {
        "date": "2023-09-07T06:21:00.000Z",
        "voteCount": 1,
        "content": "Execute\nExecute\nWrite\n\nThe provided answer is correct!"
      },
      {
        "date": "2023-08-30T22:12:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-27T10:02:00.000Z",
        "voteCount": 1,
        "content": "container1 : Read access [ by default because User1 that is assigned the Storage Blob Data Reader role for storage1 ]\n\ndirectory1: Execute [ since requirement is only to append file1 so traverse (execute) permission will be enough for it ]\n\nfile1 : Write [ because execute cannot append the file in Azure Data Lake Storage Gen2 ]\nonly write permission can append a file."
      },
      {
        "date": "2022-12-06T06:12:00.000Z",
        "voteCount": 5,
        "content": "Can't remember if the wording on actual exam was the same or very similar but instead of Append was Delete and the Q was like this:\nYou have an Azure subscription that contains an Azure Data Lake Storage Gen2 account named storage1. Storage1 contains a container named container1.\nContainer1 contains a directory named directory1. Directory1 contains a file named file1.\nYou have an Azure Active Directory (Azure AD) user named User1 that is assigned the Storage Blob Data Reader role for storage1.\nYou need to ensure that User1 can delete file1. The solution must use the principle of least privilege.\nPermission:\n----\n--WX\n---X\nAnswer Area and my answers:\ncontainer1            ---X\ndirectory1             ---X\nfile1                       --WX"
      },
      {
        "date": "2023-04-28T04:54:00.000Z",
        "voteCount": 1,
        "content": "i dont think you gave correct answers;\nsee this doc: https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions\nto delete a file you dont need any permissions on the file itself; only on the folder where it resides (read + execute)"
      },
      {
        "date": "2023-06-28T15:34:00.000Z",
        "voteCount": 2,
        "content": "The solution must use the principle of least privilege. You shouldn't do -WX on folder, only on file."
      },
      {
        "date": "2023-07-20T22:55:00.000Z",
        "voteCount": 1,
        "content": "if you give write access to entire folder , the user can delete/modify other folders , whihc is not correct"
      },
      {
        "date": "2023-08-09T23:49:00.000Z",
        "voteCount": 1,
        "content": "mamahani is correct. See the following references:\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control-model#permissions-table-combining-azure-rbac-abac-and-acls"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/microsoft/view/80883-exam-dp-203-topic-3-question-30-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription that contains an Azure Databricks workspace named databricks1 and an Azure Synapse Analytics workspace named synapse1.<br>The synapse1 workspace contains an Apache Spark pool named pool1.<br>You need to share an Apache Hive catalog of pool1 with databricks1.<br>What should you do? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0032800001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0032800002.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Azure SQL Database -<br>Use external Hive Metastore for Synapse Spark Pool<br>Azure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog.<br>Set up linked service to Hive Metastore<br>Follow below steps to set up a linked service to the external Hive Metastore in Synapse workspace.<br>1. Open Synapse Studio, go to Manage &gt; Linked services at left, click New to create a new linked service.<br>2. Set up Hive Metastore linked service<br>3. Choose Azure SQL Database or Azure Database for MySQL based on your database type, click Continue.<br>4. Provide Name of the linked service. Record the name of the linked service, this info will be used to configure Spark shortly.<br>5. You can either select Azure SQL Database/Azure Database for MySQL for the external Hive Metastore from Azure subscription list, or enter the info manually.<br>6. Provide User name and Password to set up the connection.<br>7. Test connection to verify the username and password.<br>8. Click Create to create the linked service.<br><br>Box 2: A Hive Metastore -<br>Reference:<br>https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-07T04:55:00.000Z",
        "voteCount": 10,
        "content": "I would say:\n1. sql - this is correct\n2. managed hive metastore"
      },
      {
        "date": "2024-03-20T01:23:00.000Z",
        "voteCount": 2,
        "content": "Only Azure SQL Database and Azure Database for MySQL are supported as an external Hive Metastore. And currently we only support User-Password authentication."
      },
      {
        "date": "2022-09-07T04:58:00.000Z",
        "voteCount": 20,
        "content": "scrath that, given anwers are correct. sql + hive metastore"
      },
      {
        "date": "2024-04-25T22:18:00.000Z",
        "voteCount": 1,
        "content": "It seems that it is Managed Hive Metastore; se the second green note in here:\nhttps://learn.microsoft.com/en-us/azure/hdinsight/share-hive-metastore-with-synapse"
      },
      {
        "date": "2024-04-18T02:43:00.000Z",
        "voteCount": 1,
        "content": "Correct for me"
      },
      {
        "date": "2023-11-08T02:03:00.000Z",
        "voteCount": 1,
        "content": "b1-sql\nb2. managed hive metastore\nWhy b2 a managed ? \n\nA Hive metastore is a central repository that stores metadata about the data stored in a Hive warehouse. A managed Hive metastore is a type of Hive metastore that is fully managed by Azure Databricks. It provides the following benefits over a self-managed Hive metastore:\n\nIt is automatically created and configured when you create a Databricks workspace.\nIt is automatically backed up and restored by Databricks.\nIt is automatically scaled and optimized by Databricks.\nIt is compatible with all Databricks features, such as Delta Lake, SQL Analytics, and Unity Catalog.\nA managed Hive metastore is recommended for most use cases, unless you have specific requirements that need a self-managed Hive metastore, such as:\n\nYou want to use an external metastore service, such as AWS Glue or Azure SQL Database.\nYou want to share the same metastore across multiple Databricks workspaces or other applications."
      },
      {
        "date": "2023-11-08T02:07:00.000Z",
        "voteCount": 1,
        "content": "b2. hive metastore, not a managed ! ( sorry )\nWhy b2 have to be \" A Hive metastore\" and not a managed one"
      },
      {
        "date": "2023-08-30T23:13:00.000Z",
        "voteCount": 2,
        "content": "1. sql db\n2. hive metastore"
      },
      {
        "date": "2023-07-11T00:22:00.000Z",
        "voteCount": 3,
        "content": "1 - definitely correct per documentation TestingCRM provided.\n2 - I think the devil's in the detail here :/ documentation says \"Azure Synapse Analytics allows Apache Spark pools in the same workspace to share a managed HMS (Hive Metastore) compatible metastore as their catalog\". \nThe word managed may sway you towards the answer managed hive metasotre SERVICE but the docs don't mention \"service\" at all,  which is why I would go with Hive metastore"
      },
      {
        "date": "2023-06-02T23:25:00.000Z",
        "voteCount": 1,
        "content": "1. sql - this is correct\n2. managed hive metastore\n\nSee https://learn.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-external-metastore"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/microsoft/view/81625-exam-dp-203-topic-3-question-31-discussion/",
    "body": "HOTSPOT -<br>You have an Azure subscription.<br>You need to deploy an Azure Data Lake Storage Gen2 Premium account. The solution must meet the following requirements:<br>* Blobs that are older than 365 days must be deleted.<br>* Administrative effort must be minimized.<br>* Costs must be minimized.<br>What should you use? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0033000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0033100001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: The Archive access tier -<br>Archive tier - An offline tier optimized for storing data that is rarely accessed, and that has flexible latency requirements, on the order of hours. Data in the Archive tier should be stored for a minimum of 180 days.<br>Box 2: Azure Storage lifecycle management<br>With the lifecycle management policy, you can:<br>* Delete current versions of a blob, previous versions of a blob, or blob snapshots at the end of their lifecycles.<br>Transition blobs from cool to hot immediately when they're accessed, to optimize for performance.<br>Transition current versions of a blob, previous versions of a blob, or blob snapshots to a cooler storage tier if these objects haven't been accessed or modified for a period of time, to optimize for cost. In this scenario, the lifecycle management policy can move objects from hot to cool, from hot to archive, or from cool to archive.<br>Etc.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview",
    "votes": [],
    "comments": [
      {
        "date": "2022-09-21T12:33:00.000Z",
        "voteCount": 44,
        "content": "If u choose premium storage account, there is no possibility to choose tiers (hot, cool, archive), its always hot, sa LRS and lifecycle storage mngt"
      },
      {
        "date": "2022-10-28T23:41:00.000Z",
        "voteCount": 4,
        "content": "Agree no mention for tiering in the quetion so LRS is the best option to minimize the cost"
      },
      {
        "date": "2023-04-28T07:26:00.000Z",
        "voteCount": 1,
        "content": "its not the same as hot; see this microsoft article: https://azure.microsoft.com/nl-nl/blog/azure-premium-block-blob-storage-is-now-generally-available/\n\"'Premium Blob Storage is a new performance tier in Azure Blob Storage for block blobs and append blobs, complimenting the existing Hot, Cool, and Archive access tiers. \"'"
      },
      {
        "date": "2022-09-11T01:44:00.000Z",
        "voteCount": 17,
        "content": "The statement doesn't mention requirement for a tiercing storage archive nor cool nor hot before deletion.\nThen I think it is LRS and lifecycle storage mngt"
      },
      {
        "date": "2024-09-17T07:00:00.000Z",
        "voteCount": 1,
        "content": "1. Premium does not support other than Hot. Azure does recommend LRS for lower costs.\n2. Lifecycle can be set in Premium and can manage the removal.\n\nNotes:\n- Soft delete is about recovery, not [programmed] removal.\n- I just tested what I wrote.\n- ChatGPT can be challenged if you feel it provided a wrong answer. And it does.\n\nRef.: https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview#known-issues-and-limitations"
      },
      {
        "date": "2024-01-27T16:09:00.000Z",
        "voteCount": 1,
        "content": "LRS and LCM"
      },
      {
        "date": "2023-12-24T17:15:00.000Z",
        "voteCount": 1,
        "content": "Chatgpt :\nTo minimize costs, select **The Archive access tier** since it is optimized for data that is rarely accessed and offers the lowest storage cost. For the deletion of blobs older than 365 days, you would use **Azure Storage lifecycle management** to automate the deletion process, reducing administrative effort."
      },
      {
        "date": "2023-12-16T04:05:00.000Z",
        "voteCount": 1,
        "content": "For your Azure Data Lake Storage Gen2 Premium account, considering the requirements:\n\nTo minimize costs: Locally-redundant storage (LRS). This is cost-effective and provides high durability within a single region.\n\nTo delete blobs older than 365 days: Azure Automation runbooks. Since Azure Storage lifecycle management isn't applicable to Premium tier, automation runbooks can be used to programmatically delete older blobs, minimizing administrative effort."
      },
      {
        "date": "2023-09-07T06:31:00.000Z",
        "voteCount": 3,
        "content": "LRS &amp; Lifecycle"
      },
      {
        "date": "2023-08-30T23:18:00.000Z",
        "voteCount": 3,
        "content": "LRS and LCM"
      },
      {
        "date": "2023-08-14T03:35:00.000Z",
        "voteCount": 1,
        "content": "As per the response from the Microsoft https://github.com/MicrosoftDocs/azure-docs/issues/100695 tiering is not supported for premium but delete through LCM is supported.. but still not clearly mentioned in this document https://learn.microsoft.com/en-us/azure/storage/blobs/lifecycle-management-overview\n\nAnswer LRS and LCM"
      },
      {
        "date": "2023-07-06T15:58:00.000Z",
        "voteCount": 1,
        "content": "Why you want to \"Archive\"???"
      },
      {
        "date": "2023-06-28T15:40:00.000Z",
        "voteCount": 2,
        "content": "LRS and data lifecycle. Even tho you can't switch data from tier-to-tier, you can still apply a rule to delete the BLOB once it reaches 365 days."
      },
      {
        "date": "2023-05-20T22:50:00.000Z",
        "voteCount": 2,
        "content": "According to \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview?tabs=azure-portal\n\"Data stored in a premium block blob storage account cannot be tiered to hot, cool, cold or archive by using Set Blob Tier or using Azure Blob Storage lifecycle management.\"\nSo answers are LRS and Soft delete\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-blob-overview\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/soft-delete-container-enable?tabs=azure-portal"
      },
      {
        "date": "2023-06-20T18:54:00.000Z",
        "voteCount": 1,
        "content": "Azure Blob Storage Lifecycle Management allows you to create rules to automatically delete blobs based on their age, reducing administrative effort and minimizing costs. This makes it a better option for meeting the requirements specified in your scenario.\nSoft delete is an option for protecting against accidental deletion of blobs, but it is not the best option for automatically deleting blobs that are older than 365 days. Soft delete works by retaining deleted blobs for a specified period of time, allowing you to recover them if needed. However, it does not automatically delete blobs based on their age."
      },
      {
        "date": "2023-04-28T07:27:00.000Z",
        "voteCount": 1,
        "content": "\u00e1ccording to microsoft: '\"Premium Blob Storage is a new performance tier in Azure Blob Storage for block blobs and append blobs, complimenting the existing Hot, Cool, and Archive access tiers. \"\"\nhttps://azure.microsoft.com/nl-nl/blog/azure-premium-block-blob-storage-is-now-generally-available/\nso the only two other options left are LRS and ZRS; LRS is cheaper; so it must be this one;"
      },
      {
        "date": "2023-04-28T07:30:00.000Z",
        "voteCount": 1,
        "content": "also in the documentation all the three tiers are greyed out for premium\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/storage-feature-support-in-storage-accounts#premium-block-blob-accounts\nso you cannot possibly choose this as an answer;"
      },
      {
        "date": "2022-12-25T18:04:00.000Z",
        "voteCount": 1,
        "content": "I strongly doubt they didn't offer the whole question. The question is not clear."
      },
      {
        "date": "2022-12-03T17:59:00.000Z",
        "voteCount": 7,
        "content": "Box1: Locally-redundant storage (LRS)\nIn the question, it specifically states that \"You need to deploy an Azure Data Lake Storage Gen2 Premium account\", and Azure Data Lake Storage Gen2 premium tier is neither an Archive access tier nor a Cool Access tier, and so those two options are out.  Locally-redundant storage (LRS) is less expensive than Zone-redundant storage (ZRS), so we choose LRS. \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/premium-tier-for-data-lake-storage\n\nBox2: Azure Storage Lifecycle management\nWell explained in the answer already."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/microsoft/view/79373-exam-dp-203-topic-3-question-32-discussion/",
    "body": "HOTSPOT -<br>You are designing an application that will use an Azure Data Lake Storage Gen 2 account to store petabytes of license plate photos from toll booths. The account will use zone-redundant storage (ZRS).<br>You identify the following usage patterns:<br>* The data will be accessed several times a day during the first 30 days after the data is created. The data must meet an availability SLA of 99.9%.<br>* After 90 days, the data will be accessed infrequently but must be available within 30 seconds.<br>* After 365 days, the data will be accessed infrequently but must be available within five minutes.<br>You need to recommend a data retention solution. The solution must minimize costs.<br>Which access tier should you recommend for each time frame? To answer, select the appropriate options in the answer area.<br>NOTE: Each correct selection is worth one point.<br>Hot Area:<br><img src=\"/assets/media/exam-media/04259/0033300001.jpg\" class=\"in-exam-image\"><br>",
    "options": [],
    "answer": "<img src=\"/assets/media/exam-media/04259/0033500001.jpg\" class=\"in-exam-image\">",
    "answerDescription": "Box 1: Hot -<br>The data will be accessed several times a day during the first 30 days after the data is created. The data must meet an availability SLA of 99.9%.<br><br>Box 2: Cool -<br>After 90 days, the data will be accessed infrequently but must be available within 30 seconds.<br>Data in the Cool tier should be stored for a minimum of 30 days.<br>When your data is stored in an online access tier (either Hot or Cool), users can access it immediately. The Hot tier is the best choice for data that is in active use, while the Cool tier is ideal for data that is accessed less frequently, but that still must be available for reading and writing.<br><br>Box 3: Cool -<br>After 365 days, the data will be accessed infrequently but must be available within five minutes.<br>Incorrect:<br>Not Archive:<br>While a blob is in the Archive access tier, it's considered to be offline and can't be read or modified. In order to read or modify data in an archived blob, you must first rehydrate the blob to an online tier, either the Hot or Cool tier.<br><br>Rehydration priority -<br>When you rehydrate a blob, you can set the priority for the rehydration operation via the optional x-ms-rehydrate-priority header on a Set Blob Tier or Copy Blob operation. Rehydration priority options include:<br>Standard priority: The rehydration request will be processed in the order it was received and may take up to 15 hours.<br>High priority: The rehydration request will be prioritized over standard priority requests and may complete in less than one hour for objects under 10 GB in size.<br>Reference:<br>https://docs.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview https://docs.microsoft.com/en-us/azure/storage/blobs/archive-rehydrate-overview",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-21T16:18:00.000Z",
        "voteCount": 10,
        "content": "Hot, Cool, Cool is correct.\nRef: https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview"
      },
      {
        "date": "2023-12-16T04:09:00.000Z",
        "voteCount": 1,
        "content": "First 30 Days: Use the Hot tier for frequent access and meeting the 99.9% availability SLA.\n\nAfter 90 Days: Shift to the Cool tier, suitable for infrequent access with availability within 30 seconds.\n\nAfter 365 Days: Transition to the Archive tier for rare access and longer retrieval time."
      },
      {
        "date": "2024-08-02T10:55:00.000Z",
        "voteCount": 1,
        "content": "Archive requires several hours to retrieve a file, so it would not be a good choice for \"After 365\" which need to have a 5 minute response time."
      },
      {
        "date": "2023-09-07T06:32:00.000Z",
        "voteCount": 2,
        "content": "Hot\nCool\nCool"
      },
      {
        "date": "2023-08-30T23:21:00.000Z",
        "voteCount": 1,
        "content": "Hot, Cool, Cool is correct."
      },
      {
        "date": "2023-01-17T14:52:00.000Z",
        "voteCount": 2,
        "content": "1. Hot - because of the 99.9% availability. \n2. Hot - because Cool tier needs several minutes to give back an answer (but 30 sec. is asked for).\n3. Cool - because the answer is needed within 5 minutes. Thats what cool tier does."
      },
      {
        "date": "2023-03-01T23:56:00.000Z",
        "voteCount": 8,
        "content": "Cool has a respone time of Milliseconds. So Hot, Cool, Cool"
      },
      {
        "date": "2023-08-30T23:22:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#summary-of-access-tier-options"
      },
      {
        "date": "2022-09-23T09:42:00.000Z",
        "voteCount": 2,
        "content": "I think that 'cool' tier is just enough, it provides availability on 99.9%"
      },
      {
        "date": "2022-10-10T23:32:00.000Z",
        "voteCount": 4,
        "content": "Cool Tier provides 99.9% availability only on RA-GRS. For ZRS, it should be 99% .\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/access-tiers-overview#summary-of-access-tier-options"
      },
      {
        "date": "2023-01-03T11:04:00.000Z",
        "voteCount": 4,
        "content": "Keep this in mind --&gt; \"The data will be accessed several times a day during the first 30 days\". Cool tier is more expensive to read from.\nhot, cool, cool looks correct."
      },
      {
        "date": "2022-09-02T04:14:00.000Z",
        "voteCount": 2,
        "content": "Correct!"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/microsoft/view/95364-exam-dp-203-topic-3-question-33-discussion/",
    "body": "DRAG DROP<br> -<br><br>You have an Azure Data Lake Storage Gen 2 account named storage1.<br><br>You need to recommend a solution for accessing the content in storage1. The solution must meet the following requirements:<br><br>\u2022\tList and read permissions must be granted at the storage account level.<br>\u2022\tAdditional permissions can be applied to individual objects in storage1.<br>\u2022\tSecurity principals from Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra, must be used for authentication.<br><br>What should you use? To answer, drag the appropriate components to the correct requirements. Each component may be used once, more than once, or not at all. You may need to drag the split bar between panes or scroll to view content.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image268.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image269.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-01-28T08:07:00.000Z",
        "voteCount": 8,
        "content": "Correct"
      },
      {
        "date": "2023-04-19T09:16:00.000Z",
        "voteCount": 7,
        "content": "Correct. \n\nAzure Data Lake Storage Gen2 implements an access control model that supports both Azure role-based access control (Azure RBAC) and POSIX-like access control lists (ACLs). \nAzure RBAC scope are storage accounts and  containers.\nACL\tscope are directories and files.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2024-02-29T03:18:00.000Z",
        "voteCount": 1,
        "content": "Correct!\nRBAC and ACL"
      },
      {
        "date": "2023-09-12T01:13:00.000Z",
        "voteCount": 2,
        "content": "1. Role-based access control (RBAC) rules\n2. Access control lists (ACLs)"
      },
      {
        "date": "2023-08-30T23:26:00.000Z",
        "voteCount": 2,
        "content": "correct"
      },
      {
        "date": "2023-01-14T20:22:00.000Z",
        "voteCount": 4,
        "content": "Given answer is correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105221-exam-dp-203-topic-3-question-34-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named Pool1 that contains a table named Sales.<br><br>Sales has row-level security (RLS) applied. RLS uses the following predicate filter.<br><br><img src=\"https://img.examtopics.com/dp-203/image281.png\"><br><br>A user named SalesUser1 is assigned the db_datareader role for Pool1.<br><br>Which rows in the Sales table are returned when SalesUser1 queries the table?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonly the rows for which the value in the User_Name column is SalesUser1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tall the rows",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonly the rows for which the value in the SalesRep column is Manager",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tonly the rows for which the value in the SalesRep column is SalesUser1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-28T07:47:00.000Z",
        "voteCount": 6,
        "content": "here is the same example directly from microsoft docs:\n|https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#Typical\nits definitely D"
      },
      {
        "date": "2024-02-03T01:55:00.000Z",
        "voteCount": 3,
        "content": "I am really confused. The function compares the input parameter to logged in user. No filtering in rows in where cause. \nSo if the function called with a fake user it returns nothing. \nIf parameter is the same az logged in user returns all rows. \nIf the parameter is Manager, then returns all rows."
      },
      {
        "date": "2023-08-30T23:36:00.000Z",
        "voteCount": 1,
        "content": "D is correct , see link\nhttps://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/6-exercise-manage-authorization-through-column-row-level-security"
      },
      {
        "date": "2023-08-30T23:35:00.000Z",
        "voteCount": 2,
        "content": "https://learn.microsoft.com/en-us/training/modules/secure-data-warehouse-azure-synapse-analytics/6-exercise-manage-authorization-through-column-row-level-security"
      },
      {
        "date": "2023-04-04T16:23:00.000Z",
        "voteCount": 2,
        "content": "Ans is C. \nThe function returns 1 when a row in the SalesRep column is the same as the user executing the query (@SalesRep = USER_NAME()) or if the user executing the query is the Manager user (USER_NAME() = 'Manager'). \nref: https://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16"
      },
      {
        "date": "2023-04-12T03:46:00.000Z",
        "voteCount": 2,
        "content": "I have looked it up too. Answer is C"
      },
      {
        "date": "2023-04-19T09:31:00.000Z",
        "voteCount": 6,
        "content": "In the  \"Scenario for users who authenticate to the database\" there is the same code snippet and it's clearly stated that after applying security policy adding the function as a filter predicate \"the manager should see all  rows. The Sales1 and Sales2 users should only see their own sales.\"\n\nSo the answer is D."
      },
      {
        "date": "2023-08-07T05:18:00.000Z",
        "voteCount": 1,
        "content": "If you really looked up then what did you understand from this?\n\nEXECUTE AS USER = 'SalesRep1';\nSELECT * FROM Sales.Orders;\nREVERT;\n  \nEXECUTE AS USER = 'SalesRep2';\nSELECT * FROM Sales.Orders;\nREVERT;\n  \nEXECUTE AS USER = 'Manager';\nSELECT * FROM Sales.Orders;\nREVERT;\n\nThe manager should see all six rows. The Sales1 and Sales2 users should only see their own sales.\n\nhttps://learn.microsoft.com/en-us/sql/relational-databases/security/row-level-security?view=sql-server-ver16#Typical\n\nIt's clearly D"
      },
      {
        "date": "2023-04-16T07:53:00.000Z",
        "voteCount": 8,
        "content": "It's D. It clearly states that the user querying the table is SalesUser1. I feel they should have mentioned it being a manager if it's C."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/microsoft/view/105649-exam-dp-203-topic-3-question-35-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Data Lake Storage Gen2 account named account1 that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image282.png\"><br><br>You need to configure access control lists (ACLs) to allow a user named User1 to delete File1. User1 is NOT assigned any role-based access control (RBAC) roles for account1. The solution must use the principle of least privilege.<br><br>Which type of ACL should you configure for each resource? To answer select the appropriate options in the answer area.<br><br><img src=\"https://img.examtopics.com/dp-203/image283.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image284.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-04-19T00:11:00.000Z",
        "voteCount": 42,
        "content": "Answer is \n--x/ -wx/ ---\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2023-04-09T03:02:00.000Z",
        "voteCount": 7,
        "content": "last box file1 should be --- (Frist option)\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control"
      },
      {
        "date": "2023-04-19T04:23:00.000Z",
        "voteCount": 1,
        "content": "According to this your Link, the Directory should have \"-WX\" and the File \"- - -\""
      },
      {
        "date": "2023-06-20T14:51:00.000Z",
        "voteCount": 1,
        "content": "\"So long as the previous two conditions are true.\""
      },
      {
        "date": "2024-09-16T09:59:00.000Z",
        "voteCount": 1,
        "content": "As the general response confirms, it's X, XW and whatever. The reason for this is that when you add or remove a file from a directory, you actually change the directory, so the file's permission means little. You will need X permission to transverse (navigate) until the directory containing the file, so X is required on all directories."
      },
      {
        "date": "2024-08-31T08:17:00.000Z",
        "voteCount": 1,
        "content": "The correct answers are --X, --WX, and --- permissions, according to the documentation in the link below. The example is the same, just with different names. Refer to the table titled \"Common scenarios related to ACL permissions\" and look for the delete data operation.\n\nBelow the table, there's a note that states: \"As shown in the previous table, write permissions on the file are not required to delete it as long as the directory permissions are set properly. However, to delete a directory and all of its contents, the parent directory must have Write + Execute permissions.\"\n\nSince the question asks about permissions for deleting a file and not the directory, no additional permissions are needed on the file as long as the directory has the appropriate Write and Execute permissions.\n\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control#common-scenarios-related-to-acl-permissions"
      },
      {
        "date": "2024-08-01T11:05:00.000Z",
        "voteCount": 3,
        "content": "Guys, the question wants \"least privileged\".  The user must be able to delete only this file.  They should not be able to delete ANY file in this directory, so answer should be --x / --x / -wx"
      },
      {
        "date": "2024-04-07T09:46:00.000Z",
        "voteCount": 2,
        "content": "--X\n-WX\n---\n\nobviously!"
      },
      {
        "date": "2024-02-06T16:29:00.000Z",
        "voteCount": 1,
        "content": "X,WX,--"
      },
      {
        "date": "2024-01-29T16:01:00.000Z",
        "voteCount": 2,
        "content": "Write permissions on the file are not required to delete it as long as the previous two conditions are true."
      },
      {
        "date": "2024-01-29T16:00:00.000Z",
        "voteCount": 1,
        "content": "Delete\tFile1\n--X / -WX / ---"
      },
      {
        "date": "2023-11-08T02:24:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: --X, -WX, ---\nIN general : X until the last folder, the last forder needs WX, and on the file needs nothing( --- )\nR:\nCommon scenarios related to permissions\nhttps://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions"
      },
      {
        "date": "2023-12-16T04:19:00.000Z",
        "voteCount": 1,
        "content": "please clarify why should  Directory1 be --WX? why write access for a directory? shouldn't it be just --X?"
      },
      {
        "date": "2023-08-30T23:50:00.000Z",
        "voteCount": 2,
        "content": "--x/ --x/ ---"
      },
      {
        "date": "2023-09-08T22:28:00.000Z",
        "voteCount": 1,
        "content": "https://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions"
      },
      {
        "date": "2023-09-25T04:48:00.000Z",
        "voteCount": 2,
        "content": "--x/ -wx/ ---"
      },
      {
        "date": "2023-07-06T23:56:00.000Z",
        "voteCount": 4,
        "content": "--x/ -wx/ ---"
      },
      {
        "date": "2023-06-20T14:50:00.000Z",
        "voteCount": 2,
        "content": "The solution must use the principle of least privilege!!!\nYou shouldn't grant -WX to the entire Directory1. Instead, do -x / --w"
      },
      {
        "date": "2023-04-28T07:51:00.000Z",
        "voteCount": 6,
        "content": "you do not need any permissions on a file itself to delete it; you only need permissions on the folder where the file resides; \nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control#common-scenarios-related-to-acl-permissions\nso answer -x / -wx / ---"
      },
      {
        "date": "2023-07-11T01:15:00.000Z",
        "voteCount": 1,
        "content": "I agree with everything except the write&amp;execute permission for directory. According to the \"famous\" link on ACLs the directory permissions should be only execute for deleting actions."
      },
      {
        "date": "2023-09-04T02:11:00.000Z",
        "voteCount": 2,
        "content": "You need write and execute in order to create child items in a directory. And for deleting you dont need permissions so ---."
      },
      {
        "date": "2023-09-25T04:51:00.000Z",
        "voteCount": 1,
        "content": "Not according to this link\nhttps://learn.microsoft.com/en-us/azure/data-lake-store/data-lake-store-access-control#common-scenarios-related-to-permissions\n\nThe directory on which the file resides have to be -wx in order to delete the file."
      },
      {
        "date": "2023-09-25T04:53:00.000Z",
        "voteCount": 1,
        "content": "The link should be this instead:\nhttps://learn.microsoft.com/en-us/azure/storage/blobs/data-lake-storage-access-control#common-scenarios-related-to-acl-permissions"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/microsoft/view/107823-exam-dp-203-topic-3-question-36-discussion/",
    "body": "You have an Azure subscription that is linked to a tenant in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra. The tenant that contains a security group named Group1. The subscription contains an Azure Data Lake Storage account named myaccount1. The myaccount1 account contains two containers named container1 and container2.<br><br>You need to grant Group1 read access to container1. The solution must use the principle of least privilege.<br><br>Which role should you assign to Group1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Table Data Reader for myaccount1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Reader for container1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Reader for myaccount1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Table Data Reader for container1"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-28T14:22:00.000Z",
        "voteCount": 11,
        "content": "The appropriate role to assign to Group1 to grant read access to container1 with the principle of least privilege is option B, Storage Blob Data Reader for container1.\n\nOption A, Storage Table Data Reader for myaccount1, is incorrect because it grants read access to all tables in the storage account, not just container1.\n\nOption C, Storage Blob Data Reader for myaccount1, is incorrect because it grants read access to all containers in the storage account, not just container1.\n\nOption D, Storage Table Data Reader for container1, is incorrect because it grants read access to tables in the specified container only, not blobs in container1.\n\nTherefore, option B, Storage Blob Data Reader for container1, is the most appropriate role to assign Group1 to grant read access to container1 with the principle of least privilege."
      },
      {
        "date": "2024-01-10T07:36:00.000Z",
        "voteCount": 1,
        "content": "B. Storage Blob Data Reader for container1"
      },
      {
        "date": "2023-09-08T22:30:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-04-29T01:00:00.000Z",
        "voteCount": 3,
        "content": "Correct."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/microsoft/view/106024-exam-dp-203-topic-3-question-37-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool that contains a table named dbo.Users.<br><br>You need to prevent a group of users from reading user email addresses from dbo.Users.<br><br>What should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcolumn-level security\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\trow-level security (RLS)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransparent Data Encryption (TOE)",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tdynamic data masking"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-04-28T14:21:00.000Z",
        "voteCount": 14,
        "content": "The appropriate feature to use to prevent a group of users from reading user email addresses from dbo.Users in an Azure Synapse Analytics dedicated SQL pool is option A, column-level security.\n\nOption B, row-level security (RLS), is used to filter rows in a table based on the user executing a query, but it cannot prevent certain columns from being read by a group of users.\n\nOption C, Transparent Data Encryption (TDE), encrypts data at rest and does not prevent a group of users from reading specific columns in a table.\n\nOption D, dynamic data masking, is used to mask sensitive data in query results, but it does not prevent a group of users from reading the actual values in a column.\n\nTherefore, option A, column-level security, is the most appropriate feature to use to prevent a group of users from reading user email addresses from dbo.Users in an Azure Synapse Analytics dedicated SQL pool. Column-level security can be used to deny read access to specific columns in a table based on a user or group's permissions."
      },
      {
        "date": "2023-04-12T04:32:00.000Z",
        "voteCount": 6,
        "content": "I think it should be D: \ndynamic data masking"
      },
      {
        "date": "2023-08-30T23:51:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-05-24T10:35:00.000Z",
        "voteCount": 1,
        "content": "I guess i missed reading about it, but how do you implement column-level security?  If via view, folks still have access to the underlying table.    Let me know."
      },
      {
        "date": "2023-06-11T14:49:00.000Z",
        "voteCount": 1,
        "content": "sorry, pls ignore my comment here"
      },
      {
        "date": "2023-04-12T07:21:00.000Z",
        "voteCount": 5,
        "content": "A is correct. Column-level security simplifies the design and coding of security in your application, allowing you to restrict column access to protect sensitive data. For example, ensuring that specific users can access only certain columns of a table pertinent to their department."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108852-exam-dp-203-topic-3-question-38-discussion/",
    "body": "HOTSPOT<br> -<br><br>You have an Azure Synapse Analytics dedicated SQL pool that hosts a database named DB1.<br><br>You need to ensure that DB1 meets the following security requirements:<br><br>\u2022\tWhen credit card numbers show in applications, only the last four digits must be visible.<br>\u2022\tTax numbers must be visible only to specific users.<br><br>What should you use for each requirement? To answer, select the appropriate options in the answer area.<br><br>NOTE: Each correct selection is worth one point.<br><br><img src=\"https://img.examtopics.com/dp-203/image304.png\">",
    "options": [],
    "answer": "<img src=\"https://img.examtopics.com/dp-203/image305.png\">",
    "answerDescription": "",
    "votes": [],
    "comments": [
      {
        "date": "2023-05-10T02:04:00.000Z",
        "voteCount": 10,
        "content": "Correct"
      },
      {
        "date": "2023-05-12T13:51:00.000Z",
        "voteCount": 10,
        "content": "It should be Row Level security not column since limited for some users"
      },
      {
        "date": "2023-08-10T00:57:00.000Z",
        "voteCount": 1,
        "content": "yes. that is what row-level security is designed for."
      },
      {
        "date": "2023-05-14T08:54:00.000Z",
        "voteCount": 18,
        "content": "I think the answer is correct. Imagine a team where all have access to the table, but just one person needs access to the tax numbers, you can use column-level to disable access for all the other people except the one that needs it"
      },
      {
        "date": "2024-08-01T11:12:00.000Z",
        "voteCount": 2,
        "content": "The question describes fields named credit card and tax, which hints that this is a purchase type of record, so the \"Tax\" value would likely be a single column.  The assumption would dictate that column level security should be used here."
      },
      {
        "date": "2024-04-03T06:18:00.000Z",
        "voteCount": 1,
        "content": "Correct awnser\n\nWith Row level security, will remove rows and we don't want that, we just want hide the column, so currect awnser"
      },
      {
        "date": "2024-03-04T13:19:00.000Z",
        "voteCount": 1,
        "content": "Row level security   \nColumn level security primarily controls access to specific columns within a table. It restricts users' ability to read or write data in certain columns based on their permissions. However, it does not control which rows of data users can access.\n\nFor the requirement that tax numbers must be visible only to specific users, it's not about restricting access to a specific column (as would be the case with column level security). Instead, it's about controlling access to certain rows of data based on specific criteria, such as user roles or permissions.\n\nRow level security allows you to restrict access to rows of data based on conditions defined in security policies"
      },
      {
        "date": "2023-08-30T23:53:00.000Z",
        "voteCount": 1,
        "content": "masking ,row Level"
      },
      {
        "date": "2023-08-31T00:00:00.000Z",
        "voteCount": 5,
        "content": "should be Column-level security"
      },
      {
        "date": "2023-08-18T00:22:00.000Z",
        "voteCount": 3,
        "content": "It should be Column-level security as it ensures those specific users can access only certain columns of a table. \nWhereas, RLS can help you to create a group membership or execution context in order to control not just columns in a database table, but actually, the rows."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/microsoft/view/108729-exam-dp-203-topic-3-question-39-discussion/",
    "body": "You have an Azure subscription that contains a storage account named storage1 and an Azure Synapse Analytics dedicated SQL pool. The storage1 account contains a CSV file that requires an account key for access.<br><br>You plan to read the contents of the CSV file by using an external table.<br><br>You need to create an external data source for the external table.<br><br>What should you create first?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta database role",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta database scoped credential\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\ta database view",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tan external file format"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-05-08T03:26:00.000Z",
        "voteCount": 12,
        "content": "Given answer is correct.\nRefer this link - https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
      },
      {
        "date": "2024-04-18T03:02:00.000Z",
        "voteCount": 1,
        "content": "It's correct\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/create-use-external-tables"
      },
      {
        "date": "2024-01-11T08:33:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2023-08-31T00:04:00.000Z",
        "voteCount": 3,
        "content": "correct"
      },
      {
        "date": "2023-08-13T20:25:00.000Z",
        "voteCount": 2,
        "content": "Correct"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/microsoft/view/126131-exam-dp-203-topic-3-question-40-discussion/",
    "body": "You have a tenant in Microsoft Azure Active Directory (Azure AD), part of Microsoft Entra. The tenant contains a group named Group1.<br><br>You have an Azure subscription that contains the resources shown in the following table.<br><br><img src=\"https://img.examtopics.com/dp-203/image350.png\"><br><br>You need to ensure that members of Group1 can read CSV files from storage1 by using the OPENROWSET function. The solution must meet the following requirements:<br><br>\u2022\tThe members of Group1 must use credential1 to access storage1.<br>\u2022\tThe principle of least privilege must be followed.<br><br>Which permission should you grant to Group1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEXECUTE",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONTROL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tREFERENCES\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSELECT"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-16T15:04:00.000Z",
        "voteCount": 13,
        "content": "\"Caller must have REFERENCES permission on credential to use it to authenticate to storage.\"\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-openrowset"
      },
      {
        "date": "2023-11-14T11:34:00.000Z",
        "voteCount": 7,
        "content": "When you're using the OPENROWSET function to read data from the storage account, you're actually performing a read operation, not an execute operation. The credential is used implicitly by Azure Synapse to authenticate the session with the storage account and does not require the EXECUTE permission for the user or group accessing it. Instead, you grant permissions that are appropriate for data access. In this case, the SELECT permission is the correct one to use because it allows the members of Group1 to read or select the data."
      },
      {
        "date": "2024-09-17T00:06:00.000Z",
        "voteCount": 1,
        "content": "Although SELECT may be required *IF* the groups need to query the external table, the question says \"ensure that members of Group1 can read CSV\" [using the credential].\n\nBecause of this, the group must have REFERENCES permission to reference the credential in the OPENROWSET setting.\n\nRemember that in the real world, a group can create a table for another group to access.\n\nNote: control would give access, but breaks the \"least privilege\" principle."
      },
      {
        "date": "2024-02-08T01:05:00.000Z",
        "voteCount": 1,
        "content": "Yes both require execute and reference.\nI am starting to feel this is a trick question.\nI think I will choose 'execute' due to the fact of the following wording 'The principle of least privilege must be followed."
      },
      {
        "date": "2024-02-06T16:54:00.000Z",
        "voteCount": 1,
        "content": "REFERENCES"
      },
      {
        "date": "2024-01-12T13:56:00.000Z",
        "voteCount": 2,
        "content": "the table give it out, reference https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-storage-files-overview?tabs=impersonation"
      },
      {
        "date": "2024-01-05T03:20:00.000Z",
        "voteCount": 1,
        "content": "Database users who access external storage must have permission to use credentials. To use the credential, a user must have the REFERENCES permission on a specific credential.\n\nTo grant the REFERENCES permission on a server-level credential for a login, use the following T-SQL query in the master database:\n\nGRANT REFERENCES ON CREDENTIAL::[server-level_credential] TO [login_name];\nTo grant a REFERENCES permission on a database-scoped credential for a database user, use the following T-SQL query in the user database:\n\nGRANT REFERENCES ON DATABASE SCOPED CREDENTIAL::[database-scoped_credential] TO [user_name];"
      },
      {
        "date": "2024-01-05T04:58:00.000Z",
        "voteCount": 2,
        "content": "To be fair, I know I answered with both answers, but the context isn't at all that clear. So depending on the context it could be D or C really."
      },
      {
        "date": "2023-12-31T01:17:00.000Z",
        "voteCount": 3,
        "content": "Caller must have REFERENCES permission on credential to use it to authenticate to storage."
      },
      {
        "date": "2023-12-15T03:13:00.000Z",
        "voteCount": 1,
        "content": "I am new this topic, however to read from a file you should grant Execute on the above level. so Storage1 should grant Execute on file level should be read."
      },
      {
        "date": "2024-08-01T11:21:00.000Z",
        "voteCount": 1,
        "content": "That would only be true if the user is accessing the storage file directly.  They are accessing this through Synapse Workspace, which already has credential1 as authentication mechanism.  So they really need to \"REFERENCE\" the ability to use credential1 as access control."
      },
      {
        "date": "2023-12-02T14:35:00.000Z",
        "voteCount": 2,
        "content": "D , SELECT"
      },
      {
        "date": "2024-04-16T20:28:00.000Z",
        "voteCount": 3,
        "content": "https://learn.microsoft.com/en-us/azure/synapse-analytics/sql/develop-tables-external-tables?tabs=hadoop\n\nIt is clearly mentioned in Security Section"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/microsoft/view/130465-exam-dp-203-topic-3-question-41-discussion/",
    "body": "You have an Azure subscription that contains an Azure Data Lake Storage account named dl1 and an Azure Analytics Synapse workspace named workspace1.<br><br>You need to query the data in dl1 by using an Apache Spark pool named Pool1 in workspace1. The solution must ensure that the data is accessible Pool1.<br><br>Which two actions achieve the goal? Each correct answer presents a complete solution.<br><br>NOTE: Each correct answer is worth one point.",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Azure Synapse Link.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data to the primary storage account of workspace1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom workspace1, create a linked service for the dl1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFrom Microsoft Purview, register dl1 as a data source."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T07:50:00.000Z",
        "voteCount": 11,
        "content": "Would say Purview only registers the data source to track lineage, should not have anything to do with access. Synapse Link is not concerned with data lake.\n\nSeemingly, data lake storage has to be configured as primary storage as per:\nhttps://learn.microsoft.com/en-us/troubleshoot/azure/synapse-analytics/spark/spark-jobexec-storage-access#common-issues-and-solutions"
      },
      {
        "date": "2024-07-18T05:10:00.000Z",
        "voteCount": 1,
        "content": "B and C, Purview doesnt give access"
      },
      {
        "date": "2024-07-09T11:01:00.000Z",
        "voteCount": 1,
        "content": "B works, C also works"
      },
      {
        "date": "2024-02-15T12:26:00.000Z",
        "voteCount": 1,
        "content": "A, C ?"
      },
      {
        "date": "2024-02-06T17:09:00.000Z",
        "voteCount": 1,
        "content": "C,D is the correct answer"
      },
      {
        "date": "2024-01-10T07:44:00.000Z",
        "voteCount": 1,
        "content": "chatgpt(although not sure)\nC. From workspace1, create a linked service for dl1.\nD. From Microsoft Purview, register dl1 as a data source.\n\nExplanation:\n\nCreate a linked service (C): You need to create a linked service in workspace1 that establishes a connection to dl1. This linked service allows your Spark pool (Pool1) to access the data in dl1.\n\nRegister dl1 as a data source (D): Registering dl1 as a data source in Microsoft Purview helps in tracking lineage and metadata. While it is not directly related to enabling access from Spark, it is a good practice for governance and understanding the data landscape within your organization."
      },
      {
        "date": "2024-01-10T07:45:00.000Z",
        "voteCount": 1,
        "content": "wait ignore my comment its CD"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/microsoft/view/141996-exam-dp-203-topic-3-question-42-discussion/",
    "body": "You have an Azure Synapse Analytics dedicated SQL pool named SQL1 and a user named User1.<br><br>You need to ensure that User1 can view requests associated with SQL1 by querying the sys.dm_pdw_exec_requests dynamic management view. The solution must follow the principle of least privilege.<br><br>Which permission should you grant to User1?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVIEW DATABASE STATE\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSHOWPLAN",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCONTROL SERVER",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVIEW ANY DATABASE"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-17T02:45:00.000Z",
        "voteCount": 1,
        "content": "A - Correct\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
      },
      {
        "date": "2024-06-06T11:02:00.000Z",
        "voteCount": 1,
        "content": "correct"
      },
      {
        "date": "2024-06-05T23:47:00.000Z",
        "voteCount": 4,
        "content": "A - Correct\nhttps://learn.microsoft.com/en-us/azure/synapse-analytics/sql-data-warehouse/sql-data-warehouse-manage-monitor"
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  },
  {
    "topic": 3,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/microsoft/view/142058-exam-dp-203-topic-3-question-43-discussion/",
    "body": "You have a Microsoft Entra tenant.<br><br>The tenant contains an Azure Data Lake Storage Gen2 account named storage1 that has two containers named fs1 and fs2.<br><br>You have a Microsoft Entra group named DepartmentA.<br><br>You need to meet the following requirements:<br><br>\u2022\tDepartmentA must be able to read, write, and list all the files in fs1.<br>\u2022\tDepartmentA must be prevented from accessing any files in fs2.<br>\u2022\tThe solution must use the principle of least privilege.<br><br>Which role should you assign to DepartmentA?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContributor for fs1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Owner for fs1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Contributor for storage1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Blob Data Contributor for fs1\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-06-06T11:06:00.000Z",
        "voteCount": 3,
        "content": "correct!\nStorage Blob Data Contributor for fs1: This role grants the necessary read, write, and list permissions on fs1 only, adhering to the principle of least privilege and preventing access to fs2"
      },
      {
        "date": "2024-06-28T21:37:00.000Z",
        "voteCount": 1,
        "content": "I think A."
      }
    ],
    "examNameCode": "dp-203",
    "topicNumber": "3"
  }
]