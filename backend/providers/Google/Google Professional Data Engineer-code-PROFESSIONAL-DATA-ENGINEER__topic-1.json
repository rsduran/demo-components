[
  {
    "topic": 1,
    "index": 1,
    "url": "https://www.examtopics.com/discussions/google/view/79414-exam-professional-data-engineer-topic-1-question-1/",
    "body": "Your company built a TensorFlow neutral-network model with a large number of neurons and layers. The model fits well for the training data. However, when tested against new data, it performs poorly. What method can you employ to address this?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThreading",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSerialization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDropout Methods\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDimensionality Reduction"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T06:46:00.000Z",
        "voteCount": 25,
        "content": "Answer is C.\nBad performance of a model is either due to lack of relationship between dependent and independent variables used, or just overfit due to having used too many features and/or bad features.\n\nA: Threading parallelisation can reduce training time, but if the selected featuers are the same then the resulting performance won't have changed\nB: Serialization is only changing data into byte streams. This won't be useful.\nC: This can show which features are bad. E.g. if it is one feature causing bad performance, then the dropout method will show it, so you can remove it from the model and retrain it.\nD: This would become clear if the model did not fit the training data well. But the question says that the model fits the training data well, so D is not the answer."
      },
      {
        "date": "2024-09-23T23:01:00.000Z",
        "voteCount": 1,
        "content": "C. Dropout Methods\nDropout is a regularization technique commonly used in neural networks to prevent overfitting. It helps improve the generalization of the model by randomly setting a fraction of the neurons to zero during each training iteration, which prevents the network from relying too heavily on specific neurons. This, in turn, can lead to better performance on new, unseen data."
      },
      {
        "date": "2024-09-23T23:01:00.000Z",
        "voteCount": 1,
        "content": "A: Threading parallelisation can reduce training time, but if the selected featuers are the same then the resulting performance won't have changed\nB: Serialization is only changing data into byte streams. This won't be useful.\nC: This can show which features are bad. E.g. if it is one feature causing bad performance, then the dropout method will show it, so you can remove it from the model and retrain it.\nD: This would become clear if the model did not fit the training data well. But the question says that the model fits the training data well.\n\nSo, C is the answer."
      },
      {
        "date": "2024-05-06T18:12:00.000Z",
        "voteCount": 1,
        "content": "Dropout Methods are useful to prevent a TensorFlow model from overfitting"
      },
      {
        "date": "2023-07-11T03:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. Dropout methods are used to mitigate overfitting. Hence, it is commonly used in training phase and it's beneficial for test-time performance."
      },
      {
        "date": "2023-06-09T03:48:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-05-23T04:24:00.000Z",
        "voteCount": 1,
        "content": "Dropout is a regularization technique commonly used in model training with TensorFlow and other deep learning frameworks. It is employed to prevent overfitting, a phenomenon where a model learns to perform well on the training data but fails to generalize well to new, unseen data."
      },
      {
        "date": "2023-03-14T05:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is C"
      },
      {
        "date": "2023-02-15T18:16:00.000Z",
        "voteCount": 1,
        "content": "Dropout is a regularization method to remove random selection of fixed number of unit in a neural network layer. So pick C for this question."
      },
      {
        "date": "2023-02-15T11:06:00.000Z",
        "voteCount": 1,
        "content": "becouse  it is a regularization technique for reducing overfitting in neural networks by preventing complex co-adaptations on training data."
      },
      {
        "date": "2023-01-22T16:37:00.000Z",
        "voteCount": 4,
        "content": "C. Dropout Methods\n\nDropout is a regularization technique that can be used to prevent overfitting of the model to the training data. It works by randomly dropping out a certain percentage of neurons during training, which helps to reduce the complexity of the model and prevent it from memorizing the training data. This can improve the model's ability to generalize to new data and reduce the risk of poor performance when tested against new data."
      },
      {
        "date": "2023-01-22T16:37:00.000Z",
        "voteCount": 4,
        "content": "A. Threading: it's not a method to address overfitting, it's a technique to improve the performance of the model by parallelizing the computations using multiple threads.\n\nB. Serialization: it's a technique to save the model's architecture and trained parameters to a file, it's helpful when you want to reuse the model later, but it doesn't address overfitting problem.\n\nD. Dimensionality Reduction: it's a technique that can be used to reduce the number of features in the data, it's helpful when the data contains redundant or irrelevant features, but it doesn't address overfitting problem directly."
      },
      {
        "date": "2023-01-06T21:07:00.000Z",
        "voteCount": 2,
        "content": "answer is likely to be C, but D (dimensionality reduction) can also be used to mitigate overfitting too! Not sure which one is the correct answer."
      },
      {
        "date": "2022-12-24T10:38:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-12-10T03:34:00.000Z",
        "voteCount": 1,
        "content": "Answer is C.\nWe are in an over feting problem with nerual-net model.\nRead at least the abstract of this.\nhttps://jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf"
      },
      {
        "date": "2022-11-17T11:23:00.000Z",
        "voteCount": 3,
        "content": "Correct"
      },
      {
        "date": "2022-10-23T23:04:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer. \nDropouts: are special layers that you can add to control some operations or add functionalities. If training has an issue you can take them out. \nIn the question, it fits training but not new data, i.e., overfitting.\nSolution for overfitting is dropout"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 2,
    "url": "https://www.examtopics.com/discussions/google/view/15911-exam-professional-data-engineer-topic-1-question-2/",
    "body": "You are building a model to make clothing recommendations. You know a user's fashion preference is likely to change over time, so you build a data pipeline to stream new data back to the model as it becomes available. How should you use this data to train the model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContinuously retrain the model on just the new data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContinuously retrain the model on a combination of existing data and the new data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain on the existing data while using the new data as your test set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain on the new data while using the existing data as your test set."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-05-29T18:30:00.000Z",
        "voteCount": 36,
        "content": "I think it should be B because we have to use a combination of old and new test data as well as training data"
      },
      {
        "date": "2020-06-21T19:32:00.000Z",
        "voteCount": 4,
        "content": "Yes - The training set should be shuffled well to represent data across all scenarios"
      },
      {
        "date": "2020-06-28T06:59:00.000Z",
        "voteCount": 11,
        "content": "B, as we need to train the data with new data, so that it will keep learning, and as well as used for test"
      },
      {
        "date": "2023-11-02T13:18:00.000Z",
        "voteCount": 1,
        "content": "Option A is not recommended because retraining the model on just new data will cause the model to lose the information it has learned from the historical data.\n\nOption C and D are not recommended because they are using the new data as test set and this approach will lead to a model that is overfitting and not generalize well to new users.\n\nSo answer is B"
      },
      {
        "date": "2023-10-30T08:50:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. It is time sensitive data so latest data should be used for testing.\nReference: https://cloud.google.com/automl-tables/docs/prepare#ml-use"
      },
      {
        "date": "2023-10-22T04:47:00.000Z",
        "voteCount": 1,
        "content": "This approach allows the model to benefit from both the historical data (existing data) and the new data, ensuring that it adapts to changing preferences while retaining knowledge from the past. By combining both types of data, the model can learn to make recommendations that are up-to-date and relevant to users' evolving preferences."
      },
      {
        "date": "2023-08-08T04:49:00.000Z",
        "voteCount": 1,
        "content": "train on old and new data"
      },
      {
        "date": "2023-05-23T04:26:00.000Z",
        "voteCount": 1,
        "content": "Option B is the right answer. Since the questions states the models needs to be updated since the clothing preference changes. Hence we need the new data to be utilized for training/ updating model."
      },
      {
        "date": "2023-03-10T21:13:00.000Z",
        "voteCount": 1,
        "content": "Have verified this"
      },
      {
        "date": "2023-02-21T09:20:00.000Z",
        "voteCount": 1,
        "content": "there are two point first when retraining second what data. I think retraining should be occur when the model could not predict well in this case there is monitoring metric should be needed first but no one said, second what data? in this case I think the answer is A. because when the model could not predict well it means the data variance and bias are changed so, it's no make sense what is combination new data with old data because the data being not be changed is not necessary anymore.."
      },
      {
        "date": "2023-02-21T09:28:00.000Z",
        "voteCount": 1,
        "content": "And the questions should explain in detail.. whether it's deep learning or tree based machine learning model.. and how large of new dataset is.. I think"
      },
      {
        "date": "2023-02-15T18:24:00.000Z",
        "voteCount": 1,
        "content": "The trend keep changing, so must mix new and old data..."
      },
      {
        "date": "2023-01-22T16:38:00.000Z",
        "voteCount": 5,
        "content": "B. Continuously retrain the model on a combination of existing data and the new data.\n\nThis approach will help to ensure that the model remains up-to-date with the latest fashion preferences of the users, while also leveraging the historical data to provide context and improve the accuracy of the recommendations. Retraining the model on a combination of existing and new data will help to prevent the model from being overly influenced by the new data and losing its ability to generalize to users with different preferences.\n\nOption A is not recommended because retraining the model on just new data will cause the model to lose the information it has learned from the historical data.\n\nOption C and D are not recommended because they are using the new data as test set and this approach will lead to a model that is overfitting and not generalize well to new users."
      },
      {
        "date": "2023-11-02T13:16:00.000Z",
        "voteCount": 1,
        "content": "Nice explanation bro."
      },
      {
        "date": "2023-01-06T21:11:00.000Z",
        "voteCount": 1,
        "content": "The answer can be A, if we implement online learning! But for regular model which can't implement online learning (everything with no gradient descent) the answer should be B."
      },
      {
        "date": "2023-01-06T03:42:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2022-11-08T02:47:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-10-18T20:23:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer. Model training using existing data + new data will allow model to learn new behaviour of users while retaining some of the older behaviour to make model."
      },
      {
        "date": "2022-06-06T09:52:00.000Z",
        "voteCount": 1,
        "content": "B seems like the best option because we need to make the prediction based on the latest preferences by an individual also we need not forget how the user used to prefer before .It's highly unlikely that a person's fashion sense changes a lot."
      },
      {
        "date": "2022-06-06T05:42:00.000Z",
        "voteCount": 1,
        "content": "\"You know a user's fashion preference is likely to change over time\" -  It is necessary to understand old data and new inputs for a good model - B is the best option"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 3,
    "url": "https://www.examtopics.com/discussions/google/view/16635-exam-professional-data-engineer-topic-1-question-3/",
    "body": "You designed a database for patient records as a pilot project to cover a few hundred patients in three clinics. Your design used a single database table to represent all patients and their visits, and you used self-joins to generate reports. The server resource utilization was at 50%. Since then, the scope of the project has expanded. The database must now store 100 times more patient records. You can no longer run the reports, because they either take too long or they encounter errors with insufficient compute resources. How should you adjust the database design?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd capacity (memory and disk space) to the database server by the order of 200.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShard the tables into smaller ones based on date ranges, and only generate reports with prespecified date ranges.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tNormalize the master patient-record table into the patient table and the visits table, and create other necessary tables to avoid self-join.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the table into smaller tables, with one for each clinic. Run queries against the smaller table pairs, and use unions for consolidated reports."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-07T10:37:00.000Z",
        "voteCount": 6,
        "content": "C is correct because this option provides the least amount of inconvenience over using pre-specified date ranges or one table per clinic while also increasing performance due to avoiding self-joins. \nA is not correct because adding additional compute resources is not a recommended way to resolve database schema problems.\nB is not correct because this will reduce the functionality of the database and make running reports more difficult.\nD is not correct because this will likely increase the number of tables so much that it will be more difficult to generate reports vs. the correct option. \nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/query-syntax#explicit-alias-visibility"
      },
      {
        "date": "2021-01-31T06:24:00.000Z",
        "voteCount": 5,
        "content": "A is incorrect because adding space won't solve the problem of query performance.\nB is incorrect because there is nothing related to the report generation which is specified and sharding tables on date ranges is not a good option as it will create many tables.\nC is CORRECT because the statement says \"the scope of the project has expanded. The database must now store 100 times more patient records\". As the data increases there would be difficulty in managing the tables and querying it. Hence creating different table is correct as per the need.\nD is Incorrect as it Partitions on each clinic. We have to adjust the database design so that it performs optimally when generating reports.\nAlso nothing is specified for generation of reports in the required statement."
      },
      {
        "date": "2023-11-02T13:24:00.000Z",
        "voteCount": 3,
        "content": "Normalization is a technique used to organize data in a relational database to reduce data redundancy and improve data integrity. Breaking the patient records into separate tables (patient and visits) and eliminating self-joins will make the database more scalable and improve query performance. It also helps maintain data integrity and makes it easier to manage large datasets efficiently.\n\nOptions A, B, and D may provide some benefits in specific cases, but for a scenario where the project scope has expanded significantly and there are performance issues with self-joins, normalization (Option C) is the most robust and scalable solution."
      },
      {
        "date": "2023-10-22T04:50:00.000Z",
        "voteCount": 3,
        "content": "Normalization is a technique used to organize data in a relational database to reduce data redundancy and improve data integrity. Breaking the patient records into separate tables (patient and visits) and eliminating self-joins will make the database more scalable and improve query performance. It also helps maintain data integrity and makes it easier to manage large datasets efficiently.\n\nOptions A, B, and D may provide some benefits in specific cases, but for a scenario where the project scope has expanded significantly and there are performance issues with self-joins, normalization (Option C) is the most robust and scalable solution."
      },
      {
        "date": "2023-05-19T07:08:00.000Z",
        "voteCount": 1,
        "content": "\"100 times more patient records\"immediately brings to create a patient dimensional table to save space on disk if a generical relational database is mentioned."
      },
      {
        "date": "2023-03-14T11:45:00.000Z",
        "voteCount": 1,
        "content": "C - https://cloud.google.com/bigquery/docs/best-practices-performance-patterns"
      },
      {
        "date": "2023-03-10T21:13:00.000Z",
        "voteCount": 1,
        "content": "C- This is correct have verified from different sources"
      },
      {
        "date": "2023-02-15T18:26:00.000Z",
        "voteCount": 1,
        "content": "Should be C. Basic ER design..."
      },
      {
        "date": "2023-01-09T22:13:00.000Z",
        "voteCount": 1,
        "content": "c - is the correct one."
      },
      {
        "date": "2023-01-06T03:43:00.000Z",
        "voteCount": 1,
        "content": "C should be the correct answer"
      },
      {
        "date": "2022-12-24T10:53:00.000Z",
        "voteCount": 1,
        "content": "C- Is the correct answer!"
      },
      {
        "date": "2022-03-02T01:11:00.000Z",
        "voteCount": 2,
        "content": "C - based on Google documentation, self-join is an anti-pattern: \nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns"
      },
      {
        "date": "2022-02-10T04:35:00.000Z",
        "voteCount": 1,
        "content": "Correct"
      },
      {
        "date": "2022-01-21T12:13:00.000Z",
        "voteCount": 3,
        "content": "correct answer -&gt; Normalize the master patient-record table into the patient table and the visits table, and create other necessary tables to avoid self-join.\n\nAvoid self-join at all cost because that's what google says.\n\nReference:\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns"
      },
      {
        "date": "2023-01-22T16:42:00.000Z",
        "voteCount": 2,
        "content": "Normalizing the database design will help to minimize data redundancy and improve the efficiency of the queries. By separating the patient and visit information into separate tables, the database will be able to handle the increased number of records and generate reports more efficiently, because the self-joins will no longer be required.\n\nOption A is not a good solution because adding more capacity to the server will not address the underlying problem of the database design, and it may not be sufficient to handle the increased data volume.\n\nOption B is not a good solution because it limits the flexibility of the queries and reports, and it may not be sufficient to handle the increased data volume.\n\nOption D is not a good solution because partitioning the table into smaller tables may lead to data redundancy and it may not be sufficient to handle the increased data volume."
      },
      {
        "date": "2021-10-14T06:27:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-07-07T04:22:00.000Z",
        "voteCount": 3,
        "content": "Vote for C. I read this question in  'google 'sample question' of PDE."
      },
      {
        "date": "2021-03-24T21:02:00.000Z",
        "voteCount": 2,
        "content": "Correct: C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 4,
    "url": "https://www.examtopics.com/discussions/google/view/79677-exam-professional-data-engineer-topic-1-question-4/",
    "body": "You create an important report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. You notice that visualizations are not showing data that is less than 1 hour old. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable caching by editing the report settings.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable caching in BigQuery by editing table details.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRefresh your browser tab showing the visualizations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClear your browser history for the past hour then reload the tab showing the virtualizations."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-25T06:43:00.000Z",
        "voteCount": 9,
        "content": "A. Disable caching by editing the report settings.\n\nBy default, Google Data Studio 360 caches data to improve performance and reduce the amount of queries made to the data source. However, this can cause visualizations to not show data that is less than 1 hour old, as the cached data is not up-to-date.\n\nTo resolve this, you should disable caching by editing the report settings. This can be done by following these steps:\n\nOpen the report in Google Data Studio 360.\nClick on the \"File\" menu in the top left corner of the screen.\nSelect \"Report settings\" from the dropdown menu.\nIn the \"Report settings\" window, scroll down to the \"Data\" section.\nToggle off the \"Enable cache\" option.\nClick the \"Save\" button to apply the changes.\nDisabling caching ensures that the data shown in the visualizations is always up-to-date, but it may increase the query load on the data source and affect the report's performance. Therefore, it's important to consider the trade-off between performance and data accuracy when making this change."
      },
      {
        "date": "2023-11-02T13:28:00.000Z",
        "voteCount": 3,
        "content": "A. Disable caching by editing the report settings.\n\nBy default, Google Data Studio 360 caches data to improve performance and reduce the amount of queries made to the data source. However, this can cause visualizations to not show data that is less than 1 hour old, as the cached data is not up-to-date."
      },
      {
        "date": "2023-10-22T04:51:00.000Z",
        "voteCount": 1,
        "content": "Disabling caching in the report settings will ensure that the visualizations are not using cached data and will reflect the most up-to-date information from your Google BigQuery data source. This will allow your report to show data that is less than 1 hour old. Caching is often used for performance optimization, but it can result in delays in displaying real-time or near-real-time data, so disabling it is the appropriate action in this case."
      },
      {
        "date": "2023-08-08T04:52:00.000Z",
        "voteCount": 1,
        "content": "disable caching in report setting will get the issue resolved"
      },
      {
        "date": "2023-02-15T18:35:00.000Z",
        "voteCount": 1,
        "content": "The solution from the site is perfect."
      },
      {
        "date": "2023-01-24T07:32:00.000Z",
        "voteCount": 1,
        "content": "what is relevant here is to uncache Data Studio"
      },
      {
        "date": "2023-01-22T16:45:00.000Z",
        "voteCount": 3,
        "content": "A. Disable caching by editing the report settings.\n\nData Studio 360 uses caching to speed up report loading times. When caching is enabled, Data Studio 360 will only show the data that was present in the data source at the time the report was loaded. To ensure that the visualizations in your report are always up-to-date, you should disable caching by editing the report settings. This will force Data Studio 360 to retrieve the latest data from the data source (in this case BigQuery) every time the report is loaded.\n\nOption B is incorrect as it would only disable caching in BigQuery, but it wouldn't affect the caching in Data Studio 360, so the visualizations would still not show the latest data.\n\nOption C and D will not help as the data is not being updated in Data Studio 360, it's just the cache that needs to be updated."
      },
      {
        "date": "2023-01-06T21:24:00.000Z",
        "voteCount": 1,
        "content": "I'm confused, is it possible that the cache is in BigQuery level? and the looker just get the cache from bigquery"
      },
      {
        "date": "2022-11-21T17:52:00.000Z",
        "voteCount": 1,
        "content": "Based on the doc, you can refresh the report using refresh button on the report, not the browser's refresh button. So the answer is A."
      },
      {
        "date": "2022-11-09T07:22:00.000Z",
        "voteCount": 1,
        "content": "On my opinion it's C, because Data Studio doesn't support the Real-Time dashboard updates, so it means that if caching will be disabled, user will be forced to update the dashboard manually, otherwise report will be stuck on the data from the last update. According to documentation https://support.google.com/looker-studio/answer/7020039?hl=en#zippy=%2Cin-this-article, if we want to keep the data fresh we need to setup caching with minimum value of 15 minites - it means that data in the report will be updated automatically wvery 15 minutes, if cache will be disabled completely then the report will be stuck until we will manually update it. So, tbh for me it doesn't make sense to disable cache."
      },
      {
        "date": "2022-10-31T21:24:00.000Z",
        "voteCount": 1,
        "content": "C. \nSince the data is not always stale. When it is, click on refresh button. Document also says the same\nRefresh report data manually\nReport editors can refresh the cache at any time:\n\nView or edit the report.\nIn the top right, click More options. and then click RefreshRefresh data .\nThis refreshes the cache for every data source added to the report."
      },
      {
        "date": "2022-11-01T07:28:00.000Z",
        "voteCount": 3,
        "content": "Refreshing the web browser does not refresh the data behind the viz's in Data Studio. You have to click the 'refresh data source' button."
      },
      {
        "date": "2022-10-26T05:14:00.000Z",
        "voteCount": 1,
        "content": "https://support.google.com/looker-studio/answer/7020039?hl=en#zippy=%2Cin-this-article"
      },
      {
        "date": "2022-10-20T01:18:00.000Z",
        "voteCount": 1,
        "content": "A. should be correct. after disabled the cache, it will retrieve data every time."
      },
      {
        "date": "2022-10-04T02:56:00.000Z",
        "voteCount": 3,
        "content": "Same question from a Cloud guru and answer was C. The wording is slightly different in the documentation but still, the idea is that you can trigger a manual refresh\n\n"
      },
      {
        "date": "2023-01-28T00:56:00.000Z",
        "voteCount": 1,
        "content": "The option is to refresh the browser tab not inside data studio itself. incorrect."
      },
      {
        "date": "2022-09-02T22:43:00.000Z",
        "voteCount": 4,
        "content": "A. Disable caching by editing the report settings."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 5,
    "url": "https://www.examtopics.com/discussions/google/view/16637-exam-professional-data-engineer-topic-1-question-5/",
    "body": "An external customer provides you with a daily dump of data from their database. The data flows into Google Cloud Storage GCS as comma-separated values<br>(CSV) files. You want to analyze this data in Google BigQuery, but the data could have rows that are formatted incorrectly or corrupted. How should you build this pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse federated data sources, and check data in the SQL query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable BigQuery monitoring in Google Stackdriver and create an alert.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the data into BigQuery using the gcloud CLI and set max_bad_records to 0.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-04T20:13:00.000Z",
        "voteCount": 14,
        "content": "The answer is D. An ETL pipeline will be implemented for this scenario. Check out handling invalid inputs in cloud data flow\n\nhttps://cloud.google.com/blog/products/gcp/handling-invalid-inputs-in-dataflow\n\nParDos . . . and don\u2019ts: handling invalid inputs in Dataflow using Side Outputs as a \u201cDead Letter\u201d file"
      },
      {
        "date": "2022-12-04T00:11:00.000Z",
        "voteCount": 5,
        "content": "The sources you've provided cannot be accessed. Here is an updated best practice. https://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#use_dead_letter_queues"
      },
      {
        "date": "2024-08-11T01:13:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dataflow/docs/guides/write-to-bigquery#:~:text=It%27s%20a%20good%20practice%20to%20send%20the%20errors%20to%20a%20dead%2Dletter%20queue%20or%20table%2C%20for%20later%20processing.%20For%20more%20information%20about%20this%20pattern%2C%20see%20BigQueryIO%20dead%20letter%20pattern.\n\nIt's a good practice to send the errors to a dead-letter queue or table, for later processing. For more information about this pattern, see BigQueryIO dead letter pattern."
      },
      {
        "date": "2021-08-16T08:25:00.000Z",
        "voteCount": 5,
        "content": "Disagree a bit here. Could well be A. In one Coursera video course (https://www.coursera.org/learn/batch-data-pipelines-gcp/lecture/SkDus/how-to-carry-out-operations-in-bigquery), they do have a video about when to just use an SQL query to find wrong data without creating a Dataflow pipeline. The question says \"SQL\" as a language, not Cloud SQL as a service. Federated Sources is great because you can federate a CSV file in GCS with BigQuery. From the video: \"In this section, we'll take a look at exactly how BigQuery can help with some of those data quality issues we just described. Let's start with validity, what do we mean by invalid? It can mean things like corrupted data maybe data that is missing a timestamp\""
      },
      {
        "date": "2021-08-28T05:19:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/external-data-sources\nUse cases for external data sources include:\n\nFor ETL workloads, loading and cleaning your data in one pass and writing the cleaned result into BigQuery storage.\nJoining BigQuery tables with frequently changing data from an external data source. By querying the external data source directly, you don't need to reload the data into BigQuery storage every time it changes."
      },
      {
        "date": "2024-09-23T23:05:00.000Z",
        "voteCount": 2,
        "content": "Option A is incorrect because federated data sources do not provide any data validation or cleaning capabilities and you'll have to do it on the SQL query, which could slow down the performance.\n\nOption B is incorrect because Stackdriver monitoring can only monitor the performance of the pipeline, but it can't handle corrupted or incorrectly formatted data.\n\nOption C is incorrect because using gcloud CLI and setting max_bad_records to 0 will ignore the corrupted or incorrectly formatted data and continue the load process, this will lead to incorrect analysis.\n\nAnswer D. Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis."
      },
      {
        "date": "2024-09-23T23:05:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud Dataflow allows you to create a data pipeline that can preprocess and transform data before loading it into BigQuery. This approach will enable you to handle problematic rows, push them to a dead-letter table for later analysis, and load the valid data into BigQuery.\n\nOption A (using federated data sources and checking data in the SQL query) can be used but doesn't directly address the issue of handling corrupted or incorrectly formatted rows.\n\nOptions B and C are not the best choices for handling data quality and error issues. Enabling monitoring and setting max_bad_records to 0 in BigQuery may help identify errors but won't store the problematic rows for further analysis, and it might prevent loading any data with issues, which may not be ideal."
      },
      {
        "date": "2024-09-23T23:05:00.000Z",
        "voteCount": 2,
        "content": "D. Run a Google Cloud Dataflow batch pipeline to import the data into BigQuery, and push errors to another dead-letter table for analysis.\n\nBy running a Cloud Dataflow pipeline to import the data, you can perform data validation, cleaning and transformation before it gets loaded into BigQuery. Dataflow allows you to handle corrupted or incorrectly formatted rows by pushing them to another dead-letter table for analysis. This way, you can ensure that only clean and correctly formatted data is loaded into BigQuery for analysis."
      },
      {
        "date": "2023-01-22T16:54:00.000Z",
        "voteCount": 5,
        "content": "Option A is incorrect because federated data sources do not provide any data validation or cleaning capabilities and you'll have to do it on the SQL query, which could slow down the performance.\n\nOption B is incorrect because Stackdriver monitoring can only monitor the performance of the pipeline, but it can't handle corrupted or incorrectly formatted data.\n\nOption C is incorrect because using gcloud CLI and setting max_bad_records to 0 will ignore the corrupted or incorrectly formatted data and continue the load process, this will lead to incorrect analysis."
      },
      {
        "date": "2023-07-25T08:49:00.000Z",
        "voteCount": 1,
        "content": "for Option C i think when setting max_bad_records to 0  this will prevent the loading to be achieved since the condition will cut off the loading if we have at least 1 corrupted row"
      },
      {
        "date": "2024-09-23T23:04:00.000Z",
        "voteCount": 1,
        "content": "All other options only alert or error out bad data. As the question requires, option D sends bad data to the dead letter table for further analysis while valid data is loaded to the table"
      },
      {
        "date": "2023-05-19T07:19:00.000Z",
        "voteCount": 1,
        "content": "Agreed: D"
      },
      {
        "date": "2023-03-24T11:40:00.000Z",
        "voteCount": 1,
        "content": "D because you need Transform the data"
      },
      {
        "date": "2023-02-15T18:33:00.000Z",
        "voteCount": 3,
        "content": "D. The question is asking pipeline, then let\u2019s build a pipeline."
      },
      {
        "date": "2022-10-16T10:33:00.000Z",
        "voteCount": 1,
        "content": "Agreed: D"
      },
      {
        "date": "2022-08-02T21:35:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is D"
      },
      {
        "date": "2022-03-02T01:33:00.000Z",
        "voteCount": 1,
        "content": "Correct - D (as we need to create Pipeline) which possible via 'D'"
      },
      {
        "date": "2021-11-07T10:51:00.000Z",
        "voteCount": 3,
        "content": "Looks like D, with C you will not import anything, stackdriver alerts will not help you with this and with federated resources you won\u2019t know what happened with those bad records. D is the most complete one.\nhttps://cloud.google.com/blog/products/gcp/handling-invalid-inputs-in-dataflow"
      },
      {
        "date": "2021-10-14T06:40:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-09-26T01:31:00.000Z",
        "voteCount": 1,
        "content": "D seems to be correct. explained here how combined wth Pub/Sub, this can be achieved. https://cloud.google.com/pubsub/docs/handling-failures"
      },
      {
        "date": "2021-08-22T23:48:00.000Z",
        "voteCount": 2,
        "content": "D\nthis structure is known as \"dead letter pattern\".\n\n\nhttps://medium.com/@rako/error-rate-monitoring-in-dead-letter-pattern-using-apache-beam-193fd03f8970"
      },
      {
        "date": "2021-05-31T20:17:00.000Z",
        "voteCount": 1,
        "content": "D seems the correct answer: https://cloud.google.com/bigquery/docs/loading-data#dataflow"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 6,
    "url": "https://www.examtopics.com/discussions/google/view/16639-exam-professional-data-engineer-topic-1-question-6/",
    "body": "Your weather app queries a database every 15 minutes to get the current temperature. The frontend is powered by Google App Engine and server millions of users. How should you design the frontend to respond to a database failure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIssue a command to restart the database servers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry the query with exponential backoff, up to a cap of 15 minutes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry the query every second until it comes back online to minimize staleness of data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the query frequency to once every hour until the database comes back online."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-04T20:24:00.000Z",
        "voteCount": 53,
        "content": "Correct answer is B. App engine create applications that use Cloud SQL database connections effectively. Below is what is written in google cloud documnetation.\n\nIf your application attempts to connect to the database and does not succeed, the database could be temporarily unavailable. In this case, sending too many simultaneous connection requests might waste additional database resources and increase the time needed to recover. Using exponential backoff prevents your application from sending an unresponsive number of connection requests when it can't connect to the database.\n\nThis retry only makes sense when first connecting, or when first grabbing a connection from the pool. If errors happen in the middle of a transaction, the application must do the retrying, and it must retry from the beginning of a transaction. So even if your pool is configured properly, the application might still see errors if connections are lost.\n\nreference link is https://cloud.google.com/sql/docs/mysql/manage-connections"
      },
      {
        "date": "2020-07-17T08:53:00.000Z",
        "voteCount": 12,
        "content": "https://cloud.google.com/sql/docs/mysql/manage-connections#backoff"
      },
      {
        "date": "2024-09-23T23:07:00.000Z",
        "voteCount": 3,
        "content": "Exponential backoff is a commonly used technique to handle temporary failures, such as a database server becoming temporarily unavailable. This approach retries the query, initially with a short delay and then with increasingly longer intervals between retries. Setting a cap of 15 minutes ensures that you don't excessively burden your system with constant retries.\n\nOption C (retrying the query every second) can be too aggressive and may lead to excessive load on the server when it comes back online.\n\nOption D (reducing the query frequency to once every hour) would result in significantly stale data and a poor user experience, which is generally not desirable for a weather app.\n\nOption A (issuing a command to restart the database servers) is not a suitable action for a frontend component and might not address the issue effectively. Database server restarts should be managed as a part of the infrastructure and not initiated by the frontend."
      },
      {
        "date": "2024-09-23T23:06:00.000Z",
        "voteCount": 2,
        "content": "correct answer -&gt; Retry the query with exponential backoff, up to a cap of 15 minutes.\n\nIf your application attempts to connect to the database and does not succeed, the database could be temporarily unavailable. In this case, sending too many simultaneous connection requests might waste additional database resources and increase the time needed to recover. Using exponential backoff prevents your application from sending an unresponsive number of connection requests when it can't connect to the database.\n\n"
      },
      {
        "date": "2023-01-22T16:56:00.000Z",
        "voteCount": 2,
        "content": "Exponential backoff with a cap is a common technique used to handle temporary failures, such as database outages. In this approach, the frontend will retry the query with increasing intervals (e.g., 1s, 2s, 4s, 8s, etc.) up to a maximum interval (in this case, 15 minutes), this will help to avoid overwhelming the database servers with too many requests at once, and minimize the impact of the failure on the users.\n\nOption A, is not recommended because it's not guaranteed that restarting the database servers will fix the problem, it could be a network or a configuration problem and it could cause more downtime.\n\nOption C is not recommended because it could cause too many requests to be sent to the server, overwhelming the database and causing more downtime.\n\nOption D is not recommended because reducing the query frequency too much would result in stale data, and users will not receive the most up-to-date information."
      },
      {
        "date": "2023-11-07T11:11:00.000Z",
        "voteCount": 1,
        "content": "Retries with exponential backoff seems like the most efficient option in this scenario"
      },
      {
        "date": "2023-11-02T20:16:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is B"
      },
      {
        "date": "2023-09-05T07:06:00.000Z",
        "voteCount": 1,
        "content": "good answer, good answer @radhika7983."
      },
      {
        "date": "2023-06-06T05:42:00.000Z",
        "voteCount": 1,
        "content": "B is anser"
      },
      {
        "date": "2023-05-19T07:27:00.000Z",
        "voteCount": 1,
        "content": "I agree with the exponential backoff technique, even thoght I do not see why 15 minutes should be a desired choice."
      },
      {
        "date": "2023-05-19T07:30:00.000Z",
        "voteCount": 1,
        "content": "I guess that when u have failed after 15 minutes, your app must go through a serious review before being used again, since it is not able to provide the updated results as quickly as desired."
      },
      {
        "date": "2022-12-15T05:24:00.000Z",
        "voteCount": 4,
        "content": "Truncated exponential backoff is a standard error-handling strategy for network applications. In this approach, a client periodically retries a failed request with increasing delays between requests"
      },
      {
        "date": "2022-11-18T05:45:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-02-13T05:51:00.000Z",
        "voteCount": 1,
        "content": "According to the documentation"
      },
      {
        "date": "2022-01-14T05:45:00.000Z",
        "voteCount": 1,
        "content": "B is Correct; this question appeared in Cloud Architect exam also"
      },
      {
        "date": "2021-11-07T11:27:00.000Z",
        "voteCount": 2,
        "content": "B, \nbackoff is a standard error handling strategy for network applications in which a client periodically retries a failed request with increasing delays between requests. Clients should use truncated exponential backoff for all requests to Cloud Storage that return HTTP 5xx and 429 response codes, including uploads and downloads of data or metadata."
      },
      {
        "date": "2021-10-14T06:40:00.000Z",
        "voteCount": 1,
        "content": "Ans: B"
      },
      {
        "date": "2021-04-04T16:46:00.000Z",
        "voteCount": 3,
        "content": "Vote for B.\n\nhttps://cloud.google.com/iot/docs/how-tos/exponential-backoff"
      },
      {
        "date": "2021-03-24T21:48:00.000Z",
        "voteCount": 1,
        "content": "Correct : B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 7,
    "url": "https://www.examtopics.com/discussions/google/view/16640-exam-professional-data-engineer-topic-1-question-7/",
    "body": "You are creating a model to predict housing prices. Due to budget constraints, you must run it on a single resource-constrained virtual machine. Which learning algorithm should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLinear regression\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLogistic classification",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecurrent neural network",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeedforward neural network"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-10-27T20:28:00.000Z",
        "voteCount": 53,
        "content": "Correct answer is A. A tip here to decide when a liner regression should be used or logistics regression needs to be used. If you are forecasting that is the values in the column that you are predicting is numeric, it is always liner regression. If you are classifying, that is buy or no buy, yes or no, you will be using logistics regression."
      },
      {
        "date": "2021-01-04T19:37:00.000Z",
        "voteCount": 7,
        "content": "Liner Regression is correct but this is one aspect of the question, how does it relates to resource constrained machines? or that could be just a distraction?"
      },
      {
        "date": "2021-02-03T09:31:00.000Z",
        "voteCount": 25,
        "content": "Neural Networks(Feed Forward or Recurrent) require resource intensive machines(i.e GPU's) whereas Linear regression can be done on ordinary CPU's"
      },
      {
        "date": "2020-03-15T00:43:00.000Z",
        "voteCount": 7,
        "content": "Correct A"
      },
      {
        "date": "2024-09-23T23:08:00.000Z",
        "voteCount": 2,
        "content": "Linear regression is a simple and resource-efficient algorithm for predicting continuous values like housing prices. It's computationally lightweight and well-suited for single machines with limited resources. It doesn't require the extensive computational power or specialized hardware that more complex algorithms like neural networks (options C and D) might need.\n\nOption B (Logistic classification) is used for binary classification tasks, not for predicting continuous values like housing prices, so it's not the right choice in this context."
      },
      {
        "date": "2024-02-19T17:50:00.000Z",
        "voteCount": 1,
        "content": "Linear regression is used for continous distribution."
      },
      {
        "date": "2024-01-08T06:08:00.000Z",
        "voteCount": 1,
        "content": "Here, due to budget constraints, we're utilizing a single resource-constrained virtual machine, operating in a minimal resource environment. Linear regression emerges as the appropriate algorithm. It's a lightweight predictive model that suits our resource limitations"
      },
      {
        "date": "2023-11-07T11:15:00.000Z",
        "voteCount": 1,
        "content": "Linear regression will be used since the prediction requires forecasting prices involving numeric values and is computationally less resource intensive"
      },
      {
        "date": "2023-11-02T20:20:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A"
      },
      {
        "date": "2023-05-23T04:48:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer is A. Since linear regression is used to predict a numeric value. While logistic regression is used to classify among the binary scenario.\nFurther option C and D are advance ML options and not cost and resource effective for the current situation."
      },
      {
        "date": "2023-02-24T00:40:00.000Z",
        "voteCount": 2,
        "content": "predict housing prices = linear regresssion"
      },
      {
        "date": "2023-02-15T17:48:00.000Z",
        "voteCount": 1,
        "content": "must be A.\nThough C can do it, linear regression is the better practice."
      },
      {
        "date": "2022-12-11T00:58:00.000Z",
        "voteCount": 1,
        "content": "Must be A"
      },
      {
        "date": "2022-08-15T20:19:00.000Z",
        "voteCount": 2,
        "content": "A for sure. B is for classification. Neural nets can accomplish the task but they take WAY too many resources"
      },
      {
        "date": "2022-01-22T08:00:00.000Z",
        "voteCount": 4,
        "content": "correct answer -&gt; Linear Regression\n\nLinear regression is a statistical method that allows to summarize and study relationships between two continuous (quantitative) variables: One variable, denoted X, is regarded as the independent variable. The other variable denoted y is regarded as the dependent variable. Linear regression uses one independent variable X to explain or predict the outcome of the dependent variable y.\n\nWhenever you are told to predict some future value of a process which is currently running, you can go with a regression algorithm.\n\n"
      },
      {
        "date": "2023-01-22T16:57:00.000Z",
        "voteCount": 2,
        "content": "Linear regression is a simple and computationally efficient algorithm that can be used to predict a continuous target variable based on one or more input variables. It is particularly well-suited for resource-constrained environments, as it requires minimal computational resources and can be run on a single virtual machine.\nLinear regression is a good fit for this problem as it is a supervised learning algorithm that can be used for regression problems, and it's not computationally expensive.\n\nOption B is not recommended as Logistic classification is a supervised learning algorithm that is used for classification problems, not regression problems.\n\nOption C and D are not recommended as Recurrent Neural Network (RNN) and Feedforward Neural Network (FNN) are computationally expensive and may require significant computational resources and memory to run on a single virtual machine."
      },
      {
        "date": "2021-11-07T11:29:00.000Z",
        "voteCount": 3,
        "content": "A as Supervised learning using Regression can help build a model to predict house prices.\nOption B is wrong as Classification would not help to solve the problem.\nOptions C &amp; D are wrong as they would need more resources."
      },
      {
        "date": "2021-10-14T06:41:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      },
      {
        "date": "2021-09-03T05:47:00.000Z",
        "voteCount": 2,
        "content": "Ok the right answer is A, but the question is why? Then:\n- B not because we are make forecasting  and not classifying\n- C and D not because this solution need more nodes, then more VM.\nRight?"
      },
      {
        "date": "2021-07-07T04:47:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 8,
    "url": "https://www.examtopics.com/discussions/google/view/16641-exam-professional-data-engineer-topic-1-question-8/",
    "body": "You are building new real-time data warehouse for your company and will use Google BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInclude ORDER BY DESK on timestamp column and LIMIT to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GROUP BY on the unique ID column and timestamp column and SUM on the values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the LAG window function with PARTITION by unique ID along with WHERE LAG IS NOT NULL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-27T12:31:00.000Z",
        "voteCount": 9,
        "content": "I personally don't think any answer is correct, \n\nD is the closest one but it's missing a \"ORDER BY timestamp DESC\" to ensure to get only the latest record based in the timestamp"
      },
      {
        "date": "2023-01-15T08:56:00.000Z",
        "voteCount": 3,
        "content": "The question mention only duplicated data and nothing about taking only the latest ones. Therefore I assume there is no need to always take the latest, we should ensure we take only one record for each ID."
      },
      {
        "date": "2021-03-07T19:16:00.000Z",
        "voteCount": 9,
        "content": "D:\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery#manually_removing_duplicates"
      },
      {
        "date": "2024-09-23T23:10:00.000Z",
        "voteCount": 1,
        "content": "D ensures data is partitioned by the unique id and only one record is picked thereby ensuring results are de-duplicated"
      },
      {
        "date": "2024-09-23T23:10:00.000Z",
        "voteCount": 2,
        "content": "This approach will assign a row number to each row within a unique ID partition, and by selecting only rows with a row number of 1, you will ensure that duplicates are excluded in your query results. It allows you to filter out redundant rows while retaining the latest or earliest records based on your timestamp column.\n\nOptions A, B, and C do not address the issue of duplicates effectively or interactively as they do not explicitly remove duplicates based on the unique ID and event timestamp."
      },
      {
        "date": "2024-09-23T23:10:00.000Z",
        "voteCount": 8,
        "content": "Correct answer is D. Group by column us used to check for the duplicates where you can have the count(*) for each of the unique id column. If the count is greater than 1, we will know duplicate exists.The easiest way to remove duplicates while streaming inserts is to use row_number. Use GROUP BY on the unique ID column and timestamp column and SUM on the values will not remove duplicates.\nI also executed LAG function and LAG function will return NULL on unique id when no previous records with same unique id exist. Hence LAG is also not an option here."
      },
      {
        "date": "2024-09-23T23:09:00.000Z",
        "voteCount": 6,
        "content": "D is correct because it will just pick out a single row for each set of duplicates.\nA is not correct because this will just return one row.\nB is not correct because this doesn\u2019t get you the latest value, but will get you a sum of the same event over time which doesn\u2019t make too much sense if you have duplicates.\nC is not correct because if you have events that are not duplicated, it will be excluded."
      },
      {
        "date": "2023-02-23T07:50:00.000Z",
        "voteCount": 1,
        "content": "Correct D"
      },
      {
        "date": "2023-02-16T19:12:00.000Z",
        "voteCount": 3,
        "content": "Row number gives the unique number ranking based on target column."
      },
      {
        "date": "2022-12-10T04:34:00.000Z",
        "voteCount": 1,
        "content": "It's the only valid option, try it your self with examples in QB."
      },
      {
        "date": "2022-06-19T03:43:00.000Z",
        "voteCount": 1,
        "content": "Ans is D as Row number is the clause to fetch unique record from duplicate"
      },
      {
        "date": "2022-03-02T01:44:00.000Z",
        "voteCount": 1,
        "content": "Answer: D"
      },
      {
        "date": "2022-01-22T08:58:00.000Z",
        "voteCount": 3,
        "content": "correct answer -&gt; Use the ROW_NUMBER window function with PARTITION by unique ID along with WHERE row equals 1.\n\nYou can use the ROW_NUMBER() to turn non-unique rows into unique rows and then delete the duplicate rows.\n\nReference:\nhttps://www.mysqltutorial.org/mysql-window-functions/mysql-row_number-function/"
      },
      {
        "date": "2024-09-23T23:09:00.000Z",
        "voteCount": 4,
        "content": "When you are using BigQuery streaming inserts, there is no guarantee that data will only be sent once. However, you can use the ROW_NUMBER window function to ensure that duplicates are not included while interactively querying data. By using a PARTITION BY clause on the unique ID column, you can assign a unique number to each row within a result set, based on the order specified in the timestamp column. Then, a WHERE clause can be used to select only the row with the number 1. This will return the first row for each unique ID based on the timestamp column, which will ensure that duplicates are not included in your query results."
      },
      {
        "date": "2023-01-22T17:00:00.000Z",
        "voteCount": 2,
        "content": "Option A is not recommended because it will only return the first row based on the timestamp column, it doesn't consider the unique ID, so you could have multiple rows with the same timestamp, and you will get one of them arbitrarily.\n\nOption B is not recommended because it's used for aggregation, it doesn't return the first row for each unique ID based on the timestamp column.\n\nOption C is not recommended because it's used for comparing rows, it doesn't return the first row for each unique ID based on the timestamp column."
      },
      {
        "date": "2021-11-23T23:30:00.000Z",
        "voteCount": 1,
        "content": "Sorry, but IMHO no response is correct, because, in addition to making the ID field unique, it occurs consider the record with most recent timestamp"
      },
      {
        "date": "2021-10-14T06:42:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-03-24T22:19:00.000Z",
        "voteCount": 1,
        "content": "Correct : D"
      },
      {
        "date": "2021-02-28T06:53:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2021-02-07T11:17:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 9,
    "url": "https://www.examtopics.com/discussions/google/view/79679-exam-professional-data-engineer-topic-1-question-9/",
    "body": "Your company is using WILDCARD tables to query data across multiple tables with similar names. The SQL statement is currently failing with the following error:<br><img src=\"/assets/media/exam-media/04341/0000600001.png\" class=\"in-exam-image\"><br>Which table name will make the SQL statement work correctly?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t'bigquery-public-data.noaa_gsod.gsod'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tbigquery-public-data.noaa_gsod.gsod*",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t'bigquery-public-data.noaa_gsod.gsod'*",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t'bigquery-public-data.noaa_gsod.gsod*`\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-23T08:28:00.000Z",
        "voteCount": 29,
        "content": "None, the actual `bigquery-public-data.noaa_gsod.gsod*`\nwith back ticks at the beginning and at the end."
      },
      {
        "date": "2023-01-15T08:58:00.000Z",
        "voteCount": 10,
        "content": "I suspect there has been some typo with copy-paste of the option D"
      },
      {
        "date": "2023-12-27T00:56:00.000Z",
        "voteCount": 1,
        "content": "yes, I see from another source that actually ans D has to be backtick.  Probably a problem when this web do data ingestion."
      },
      {
        "date": "2024-09-23T23:11:00.000Z",
        "voteCount": 1,
        "content": "let's forget the fact that in BQ is used ` instead than ' which retrieves an error in any case. ` is called backquote, backtick, or left quote while ' is simply an apostrophe. Let's consider ' to be ` in every answer, since moderators could have not been aware of such when they had received the question."
      },
      {
        "date": "2023-05-19T08:12:00.000Z",
        "voteCount": 1,
        "content": "Who used BQ knows that the backquote is necessary only for the project name, while it can be used for the whole string, and necessary only when the project name contains special (special in this specific context) characters.\n\n- is a special character. so\n`bigquery-public-data`.noaa_gsod.gsod1940 \nwould have worked too.\n\nThe question now turns out to be\n`bigquery-public-data`.noaa_gsod.gsod* \nstill works or due to the * presence we need to write \n`bigquery-public-data.noaa_gsod.gsod*`\n?\n\nI personally do not remember, and I do not have a BQ at my disposal at the moment. \nBut I know for sure that \n`bigquery-public-data.noaa_gsod.gsod*`\nworks while \n`bigquery-public-data`.noaa_gsod.gsod*\nis not in the options."
      },
      {
        "date": "2024-09-23T23:11:00.000Z",
        "voteCount": 1,
        "content": "Option D (assuming to have backticks)\n\nRefer: https://cloud.google.com/bigquery/docs/querying-wildcard-tables\nThe following query is NOT valid because it isn't properly quoted with backticks:\n```\n#standardSQL\n/* Syntax error: Expected end of statement but got \"-\" at [4:11] */\nSELECT\n  max\nFROM\n  # missing backticks\n  bigquery-public-data.noaa_gsod.gsod*\nWHERE\n  max != 9999.9 # code for missing data\n  AND _TABLE_SUFFIX = '1929'\nORDER BY\n  max DESC\n```"
      },
      {
        "date": "2024-09-23T23:11:00.000Z",
        "voteCount": 1,
        "content": "Reference: https://cloud.google.com/bigquery/docs/querying-wildcard-tables\nThe wildcard table name contains the special character (*), which means that you must enclose the wildcard table name in backtick (`) characters. For example, the following query is valid because it uses backticks:\n\n\n#standardSQL\n/* Valid SQL query */\nSELECT\n  max\nFROM\n  `bigquery-public-data.noaa_gsod.gsod*`\nWHERE\n  max != 9999.9 # code for missing data\n  AND _TABLE_SUFFIX = '1929'\nORDER BY\n  max DESC"
      },
      {
        "date": "2024-09-23T23:11:00.000Z",
        "voteCount": 4,
        "content": "Few might go with the Option B which will be a blunder because of the below reason.\n\nWhile querying the tables or views with the name, it is optional to surround with the backticks. But while querying the list of tables with Wild card character, it is must to surround with the backticks. \n\nWe can get the Syntax error: Expected end of input but got \"*\" with the below query \n\nSELECT * FROM bigquery-public-data.noaa_gsod.gsod*\nWHERE _TABLE_SUFFIX = \"2024\"\n\nSo, option D might be the correct one, provided if there is a typo."
      },
      {
        "date": "2024-06-25T10:23:00.000Z",
        "voteCount": 1,
        "content": "IN option D, there is differert ' ` on first and last. That's why right option is second."
      },
      {
        "date": "2024-05-21T05:33:00.000Z",
        "voteCount": 3,
        "content": "bigquery-public-data.noaa_gsod.gsod*  also works"
      },
      {
        "date": "2023-11-07T11:22:00.000Z",
        "voteCount": 1,
        "content": "Agree with others - Option D"
      },
      {
        "date": "2023-10-29T08:08:00.000Z",
        "voteCount": 1,
        "content": "D. 'bigquery-public-data.noaa_gsod.gsod*` is the right answer with 1 typo"
      },
      {
        "date": "2023-05-12T06:43:00.000Z",
        "voteCount": 3,
        "content": "Answer is 'D'\nReference : https://cloud.google.com/bigquery/docs/wildcard-table-reference\n\nEnclose table names with wildcards in backticks\nThe wildcard table name contains the special character (*), which means that you must enclose the wildcard table name in backtick (`) characters."
      },
      {
        "date": "2023-04-26T14:04:00.000Z",
        "voteCount": 2,
        "content": "bigquery-public-data.noaa_gsod.gsod* works"
      },
      {
        "date": "2023-03-20T00:35:00.000Z",
        "voteCount": 1,
        "content": "should be B, the backtick at D answer is wrong ' instead of `"
      },
      {
        "date": "2023-03-20T00:16:00.000Z",
        "voteCount": 1,
        "content": "should be B, the backtick at D answer is wrong ' instead of `"
      },
      {
        "date": "2023-02-23T07:49:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-06T13:45:00.000Z",
        "voteCount": 2,
        "content": "D. 'bigquery-public-data.noaa_gsod.gsod*`"
      },
      {
        "date": "2022-09-02T22:48:00.000Z",
        "voteCount": 2,
        "content": "D. 'bigquery-public-data.noaa_gsod.gsod*`"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 10,
    "url": "https://www.examtopics.com/discussions/google/view/16642-exam-professional-data-engineer-topic-1-question-10/",
    "body": "Your company is in a highly regulated industry. One of your requirements is to ensure individual users have access only to the minimum amount of information required to do their jobs. You want to enforce this requirement with Google BigQuery. Which three approaches can you take? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDisable writes to certain tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestrict access to tables by role.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the data is encrypted at all times.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestrict BigQuery API access to approved users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSegregate data across multiple tables or databases.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Stackdriver Audit Logging to determine policy violations."
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "BDF",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "DEF",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-01-22T10:57:00.000Z",
        "voteCount": 37,
        "content": "correct option -&gt; B. Restrict access to tables by role. \nReference: https://cloud.google.com/bigquery/docs/table-access-controls-intro\n\ncorrect option -&gt; D. Restrict BigQuery API access to approved users. \n***Only approved users will have access which means other users will have minimum amount of information required to do their job.***\nReference: https://cloud.google.com/bigquery/docs/access-control\n\ncorrect option -&gt; F. Use Google Stackdriver Audit Logging to determine policy violations.\nReference: https://cloud.google.com/bigquery/docs/table-access-controls-intro#logging\n\nA. Disable writes to certain tables. ---&gt; Read is still available(not minimal access)\nC. Ensure that the data is encrypted at all times. ---&gt; Data is encrypted by default.\nE. Segregate data across multiple tables or databases. ---&gt; Normalization is of no help here."
      },
      {
        "date": "2024-09-23T23:12:00.000Z",
        "voteCount": 24,
        "content": "I was WRONG. I am not sure why s o many upvotes lol.\n\nI think this is the correct answer:\nB. Restrict access to tables by role.\nD. Restrict BigQuery API access to approved users.\nE. Segregate data across multiple tables or databases.\n\nRestrict access to tables by role: You can use BigQuery's access controls to restrict access to specific tables based on user roles. This allows you to ensure that users can only access the data they need to do their job.\nRestrict BigQuery API access to approved users: By using Cloud Identity and Access Management (IAM) you can control who has access to the BigQuery API, and what actions they are allowed to perform. This will help to ensure that only authorized users can access the data.\nSegregate data across multiple tables or databases: You can use multiple tables or databases to separate different types of data, so that users can only access the data they need. This will prevent users from seeing data they shouldn't have access to."
      },
      {
        "date": "2023-01-22T18:00:00.000Z",
        "voteCount": 5,
        "content": "Option A is incorrect because disabling writes to certain tables would prevent users from updating the data which is not in line with the goal of providing access to the minimum amount of information required to do their jobs.\nOption C is incorrect because while data encryption is important for security it doesn't specifically help with providing users access to the minimum amount of information required to do their jobs.\nOption F is incorrect because while Google Stackdriver Audit Logging can help to determine policy violations it does not help to enforce the access controls and segregation of data."
      },
      {
        "date": "2023-02-17T08:20:00.000Z",
        "voteCount": 4,
        "content": "There is no database in Bigquery, only datasets. I would pick it if it says \"tables and datasets\"."
      },
      {
        "date": "2024-05-15T11:19:00.000Z",
        "voteCount": 1,
        "content": "What about cloud SQL and Spanner"
      },
      {
        "date": "2020-09-03T08:25:00.000Z",
        "voteCount": 15,
        "content": "Yes. Access control on table level is now possible in BigQuery : https://cloud.google.com/bigquery/docs/table-access-controls-intro"
      },
      {
        "date": "2023-10-29T08:20:00.000Z",
        "voteCount": 1,
        "content": "Thanks. For me, this type of answer is more valuable because even as time passes, I can revisit existing solutions and ideas and refresh the concepts of the initial question. It helped me on the ACE exam"
      },
      {
        "date": "2024-09-23T23:13:00.000Z",
        "voteCount": 3,
        "content": "I disagree with [F]. It's too late for a \"highly regulated industry\" to detect access violations by audit logs.\n[E] is a more reasonable answer, since it is a kind of row-level security, especially the times when BigQuery row-level security wasn't available.\nIt is a practice still recommended (even with row-level sec. available) for the extreme scenario that:\n(Through repeated observation of query duration when querying tables with row-level access policies,) \"a user could infer the values of rows that otherwise might be protected by row-level access policies\"\n\"If you are sensitive to this level of protection, we recommend using separate tables to isolate rows with different access control requirements, instead.\"\nSource: \nhttps://cloud.google.com/bigquery/docs/best-practices-row-level-security#limit-side-channel-attacks"
      },
      {
        "date": "2024-09-23T23:13:00.000Z",
        "voteCount": 3,
        "content": "E seems more sensible to be as the question concentrates more on table access restriction than access violation, policy violations can only be determined through stackdriver how ever we cant restrict the access to tables.Probably option E should be considered as, by segregating the data into diferrent tables , we can restrict access to tables."
      },
      {
        "date": "2022-12-10T04:27:00.000Z",
        "voteCount": 1,
        "content": "E says segregate across multiple tables or databases, this is not the pattern of BigQuery, in BQ there is only one database, and you can organize your data in datasets..."
      },
      {
        "date": "2024-09-23T23:12:00.000Z",
        "voteCount": 4,
        "content": "B. Restrict access to tables by role: You can define roles in BigQuery and grant specific permissions to these roles to control who can access particular tables.\n\nD. Restrict BigQuery API access to approved users: You can control access to the BigQuery API and, consequently, to the underlying data by ensuring that only approved users or services can make API requests.\n\nE. Segregate data across multiple tables or databases: You can separate data into different tables or databases based on user access requirements, which allows you to limit users' access to specific data sets.\n\nThese approaches, when used together, can help you enforce data access controls in a regulated environment. Options A, C, and F are also important considerations but are not direct methods for enforcing fine-grained access control to specific data."
      },
      {
        "date": "2024-09-23T23:12:00.000Z",
        "voteCount": 1,
        "content": "B. Restrict access to tables by role.\n\nUse IAM roles and permissions to control access to specific datasets or tables based on the user\u2019s role.\nD. Restrict BigQuery API access to approved users.\n\nLimit API access to only those users or services that need it, ensuring that unauthorized users cannot interact with the data.\nE. Segregate data across multiple tables or databases.\n\nOrganize data in a way that separates sensitive information, allowing more granular control over who has access to specific datasets.\nThese options directly contribute to enforcing the principle of least privilege, ensuring users can only access the data necessary for their roles."
      },
      {
        "date": "2023-12-15T02:58:00.000Z",
        "voteCount": 1,
        "content": "You want to enforce this requirement with Google BigQuery -&gt; BDE"
      },
      {
        "date": "2023-11-07T11:35:00.000Z",
        "voteCount": 1,
        "content": "BDF. We are fairly unanimous with options B and D. I'm going with F because it does help identifying policy violations which is also one aspect to be considered when designing access controls. Option D only indicates segregating into multiple tables and databases which may or may not help with controlling access leaving it open-ended for the architect to decide."
      },
      {
        "date": "2023-11-02T20:46:00.000Z",
        "voteCount": 1,
        "content": "In Google BigQuery, you can organize and segregate data across multiple tables within the same dataset, but you cannot directly segregate data into separate databases. BigQuery uses a flat namespace structure where data is organized into datasets and tables within those datasets. Datasets are the highest level of organization within BigQuery.\n\nSo i'm sticking with BDF"
      },
      {
        "date": "2023-10-20T04:35:00.000Z",
        "voteCount": 1,
        "content": "B. Restrict access to tables by role.\nD. Restrict BigQuery API access to approved users.\nE. Segregate data across multiple tables or databases."
      },
      {
        "date": "2023-08-21T23:03:00.000Z",
        "voteCount": 2,
        "content": "BDF as per other answers"
      },
      {
        "date": "2023-07-29T23:50:00.000Z",
        "voteCount": 4,
        "content": "Regarding E or F, opinions seem to be divided into two parts.\n\nI think E is insufficient because it seems that appropriate conditions must be additionally described for table or dataset separation.\n\nF is also emphasized in Google's official textbook. You need to ensure that it is operating well as set up through monitoring.\n\nSo, BDF!"
      },
      {
        "date": "2023-07-24T09:11:00.000Z",
        "voteCount": 3,
        "content": "Why B is correct? Access control can only be applied on dataset and views, not on partitions and tables. =&gt; So it is not possible to restrict access to table, but only to dataset. Can someone help me understand why in this scenario B is correct?"
      },
      {
        "date": "2023-07-27T12:21:00.000Z",
        "voteCount": 1,
        "content": "I was thinking the same thing. I thought dataset access gave you access to all tables within it, and that you couldn't restrict access on the table level."
      },
      {
        "date": "2023-06-29T19:55:00.000Z",
        "voteCount": 2,
        "content": "Option E says \"...or databases\". The data housing service in question in BigQuery and the context is to design that support BigQuery access delegation. Seems random to include moving to another database as an option. If it did not mention databases and stopped at just tables, then E would also be the right option"
      },
      {
        "date": "2023-04-24T02:42:00.000Z",
        "voteCount": 2,
        "content": "B. Restrict access to tables by role: This approach can be used to control access to tables based on user roles. Access controls can be set at the project, dataset, and table level, and roles can be customized to provide granular access controls to different groups of users.\n\nD. Restrict BigQuery API access to approved users: This approach involves using IAM (Identity and Access Management) to control access to the BigQuery API. Access can be granted or revoked at the project or dataset level, and policies can be customized to control access based on user roles, IP addresses, and other factors.\n\nE. Segregate data across multiple tables or databases: This approach involves breaking down large datasets into smaller, more manageable tables or databases. This helps to ensure that individual users have access only to the minimum amount of information required to do their jobs, and reduces the risk of data breaches or policy violations."
      },
      {
        "date": "2023-03-16T07:28:00.000Z",
        "voteCount": 5,
        "content": "F won't avoid undesired access, only detect after it already happened.\nE makes it easier to control access."
      },
      {
        "date": "2023-02-21T22:55:00.000Z",
        "voteCount": 1,
        "content": "I think BDF.\nSegregating the table is needed to try to distribute into a few of dataset not only I said but it looks like complicated."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 11,
    "url": "https://www.examtopics.com/discussions/google/view/79682-exam-professional-data-engineer-topic-1-question-11/",
    "body": "You are designing a basket abandonment system for an ecommerce company. The system will send a message to a user based on these rules:<br>\u2711 No interaction by the user on the site for 1 hour<br>Has added more than $30 worth of products to the basket<br><img src=\"/assets/media/exam-media/04341/0000700003.png\" class=\"in-exam-image\"><br>\u2711 Has not completed a transaction<br>You use Google Cloud Dataflow to process the data and decide if a message should be sent. How should you design the pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a fixed-time window with a duration of 60 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a sliding time window with a duration of 60 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a session window with a gap time duration of 60 minutes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a global window with a time based trigger with a delay of 60 minutes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-05T17:06:00.000Z",
        "voteCount": 27,
        "content": "There are 3 windowing concepts in dataflow and each can be used for below use case\n1) Fixed window\n2) Sliding window and\n3) Session window.\n\nFixed window = any aggregation use cases, any batch analysis of data, relatively simple use cases.\n\nSliding window = Moving averages of data\nSession window = user session data, click data and real time gaming analysis.\n\nThe question here is about user session data and hence session window.\n\nReference:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines"
      },
      {
        "date": "2024-09-23T23:14:00.000Z",
        "voteCount": 3,
        "content": "C. Use a session window with a gap time duration of 60 minutes.\n\nA session window with a gap time duration of 60 minutes is appropriate for capturing user sessions where there has been no interaction on the site for 1 hour. It allows you to group user activity within a session, and when the session becomes inactive for the defined gap time, you can evaluate whether the user added more than $30 worth of products to the basket and has not completed a transaction.\n\nOptions A and B (fixed-time window and sliding time window) might not capture the specific session-based criteria of inactivity and user interaction effectively.\n\nOption D (global window with a time-based trigger) is not suitable for capturing user sessions and checking inactivity based on a specific time duration. It's more appropriate for cases where you need a single global view of the data."
      },
      {
        "date": "2023-11-07T11:39:00.000Z",
        "voteCount": 1,
        "content": "Session window since the question specifically talks about a specific user for a fixed duration."
      },
      {
        "date": "2023-11-03T16:30:00.000Z",
        "voteCount": 1,
        "content": "Session window = user session data, click data and real time gaming analysis."
      },
      {
        "date": "2023-10-06T20:37:00.000Z",
        "voteCount": 2,
        "content": "The basket abandonment system needs to determine if a user hasn't interacted with the site for 1 hour, has added products worth more than $30, and hasn't completed a transaction. Therefore, the pipeline should account for periods of user activity and inactivity. A session-based windowing approach is appropriate here.\n\nThe right choice is:\n\nC. Use a session window with a gap time duration of 60 minutes.\n\nSession windows group data based on periods of activity and inactivity. If there's no interaction for the duration of the gap time (in this case, 60 minutes), a new window is started. This would help identify users who haven't interacted with the site for the specified duration, fulfilling the requirement for the basket abandonment system."
      },
      {
        "date": "2023-09-25T08:34:00.000Z",
        "voteCount": 1,
        "content": "session windows can divide a data stream representing user activity\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows"
      },
      {
        "date": "2023-03-14T13:05:00.000Z",
        "voteCount": 2,
        "content": "C - The best option for this use case."
      },
      {
        "date": "2023-03-10T21:25:00.000Z",
        "voteCount": 2,
        "content": "Session window is used for these type of scenario"
      },
      {
        "date": "2023-02-05T13:25:00.000Z",
        "voteCount": 2,
        "content": "C. Use a session window with a gap time duration of 60 minutes.\n\nA session window would be the most appropriate option to use in this case, as it would allow you to group events into sessions based on time gaps. In this case, the gap time of 60 minutes could be used to define a session, and if there is no interaction from the user for 60 minutes, a new session would be created. By using a session window, you can track the behavior of the user during each session, including the products added to the basket, and determine if the conditions for sending a message have been met (i.e., the user has added more than $30 worth of products to the basket and has not completed a transaction)."
      },
      {
        "date": "2022-10-20T01:20:00.000Z",
        "voteCount": 1,
        "content": "Only C is feasible for this question"
      },
      {
        "date": "2022-09-02T22:50:00.000Z",
        "voteCount": 1,
        "content": "C. Use a session window with a gap time duration of 60 minutes."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 12,
    "url": "https://www.examtopics.com/discussions/google/view/16644-exam-professional-data-engineer-topic-1-question-12/",
    "body": "Your company handles data processing for a number of different clients. Each client prefers to use their own suite of analytics tools, with some allowing direct query access via Google BigQuery. You need to secure the data so that clients cannot see each other's data. You want to ensure appropriate access to the data.<br>Which three steps should you take? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data into different partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad data into a different dataset for each client.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut each client's BigQuery dataset into a different table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestrict a client's dataset to approved users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly allow a service account to access the datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the appropriate identity and access management (IAM) roles for each client's users.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BDF",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDF",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-08-17T11:18:00.000Z",
        "voteCount": 12,
        "content": "My vota also goes for B,D,F"
      },
      {
        "date": "2021-04-07T17:03:00.000Z",
        "voteCount": 9,
        "content": "Some voted for 'E' i.e. E. Only allow a service account to access the datasets.\nNot sure why ?\n\nif we gave access ONLY to service account - Does not it mean - we need to access BigQuery using Some Code (by mentioning Service account credentials there) OR using some other resource like VM)\nIn this case - i think person can't even access the Big Query Service via UI (if we give access only to Service account). Correct me if there is option on UI as well"
      },
      {
        "date": "2021-07-02T09:31:00.000Z",
        "voteCount": 5,
        "content": "yes, that is precisely why we need to eliminate E."
      },
      {
        "date": "2024-09-23T23:14:00.000Z",
        "voteCount": 2,
        "content": "B, D, F!\nC - is technically wrong . tables are being logically stored in a single dataset.\nA - Partitioning data is for improving performance. once you SQL (select) the table, you can not control the data being selected for the developer."
      },
      {
        "date": "2023-02-21T23:13:00.000Z",
        "voteCount": 1,
        "content": "For C. What if thinking about that there are tables by clients? such as customer_clients_a table and giving IAM from each table to users??.."
      },
      {
        "date": "2024-09-23T23:14:00.000Z",
        "voteCount": 4,
        "content": "B. Load data into a different dataset for each client.\nD. Restrict a client's dataset to approved users.\nF. Use the appropriate identity and access management (IAM) roles for each client's users.\n\nBy loading each client's data into a separate dataset, you ensure that each client's data is isolated from the data of other clients. Restricting access to each client's dataset to only approved users, as specified in D, further enhances data security by ensuring that only authorized users can access the data. By using appropriate IAM roles for each client's users, as specified in F, you can grant different levels of access to different clients and their users, ensuring that each client has only the level of access required for their specific needs."
      },
      {
        "date": "2024-09-23T23:14:00.000Z",
        "voteCount": 3,
        "content": "B. Load data into a different dataset for each client.\nD. Restrict a client's dataset to approved users.\nF. Use the appropriate identity and access management (IAM) roles for each client's users."
      },
      {
        "date": "2024-09-23T23:14:00.000Z",
        "voteCount": 5,
        "content": "B. Load data into a different dataset for each client: Organize the data into separate datasets for each client. This ensures data isolation and simplifies access control.\n\nD. Restrict a client's dataset to approved users: Implement access controls by specifying which users or groups are allowed to access each client's dataset. This restricts data access to approved users only.\n\nF. Use the appropriate identity and access management (IAM) roles for each client's users: Assign IAM roles based on client-specific requirements to manage permissions effectively. IAM roles help control access at a more granular level, allowing you to tailor access to specific users or groups within each client's dataset.\n\nThese steps ensure that each client's data is separated, and access is controlled based on client-specific requirements. Options A, C, and E, while important in other contexts, are not sufficient on their own to ensure client data isolation and access control in a multi-client environment."
      },
      {
        "date": "2024-01-25T04:09:00.000Z",
        "voteCount": 1,
        "content": "My Vote is BDF.\nI was thinking BEF but the question shows that the Big Query warehouse will be accessed by both direct users and other applications, as preferred by each customer."
      },
      {
        "date": "2024-01-25T03:24:00.000Z",
        "voteCount": 1,
        "content": "agreed B,D,F"
      },
      {
        "date": "2023-11-07T11:42:00.000Z",
        "voteCount": 1,
        "content": "Agree with others"
      },
      {
        "date": "2023-10-06T20:40:00.000Z",
        "voteCount": 2,
        "content": "the answers are B, D, and F.\nTo ensure that clients cannot see each other's data and have appropriate access, you would want to:\n\n    Segregate the data by client.\n    Restrict access to each client's data.\n    Use proper identity and access management techniques."
      },
      {
        "date": "2023-09-13T07:17:00.000Z",
        "voteCount": 2,
        "content": "B,D,F is the answer"
      },
      {
        "date": "2023-03-11T12:17:00.000Z",
        "voteCount": 4,
        "content": "BDF is right"
      },
      {
        "date": "2023-01-10T04:38:00.000Z",
        "voteCount": 1,
        "content": "B, D, F! \nC - is technically wrong . tables are being logically stored in a single dataset. \n A -  Partitioning data is for improving performance. once you SQL (select) the table, you can not control the data being selected for the developer."
      },
      {
        "date": "2022-12-27T19:40:00.000Z",
        "voteCount": 2,
        "content": "Please why is DEF not correct?"
      },
      {
        "date": "2022-12-26T03:08:00.000Z",
        "voteCount": 1,
        "content": "Agree BDF"
      },
      {
        "date": "2022-12-10T04:48:00.000Z",
        "voteCount": 4,
        "content": "Why no E? E has a lot a sense to me, they have external analytical tools, and the best practice is to give access to external trow service account, and not throw user level."
      },
      {
        "date": "2022-12-23T01:43:00.000Z",
        "voteCount": 2,
        "content": "Yes, I also wonder why not E instead of D"
      },
      {
        "date": "2023-07-31T06:25:00.000Z",
        "voteCount": 1,
        "content": "Because the client might want a mixture of SAs and user accounts. Maybe they have a Big Data Team that wants to run queries and access  the data with their account. Also SAs do not help with segregating the data"
      },
      {
        "date": "2023-02-26T12:57:00.000Z",
        "voteCount": 4,
        "content": "I hesitated over this too, but the question talks about direct access query so that's the reason for not choosing E."
      },
      {
        "date": "2022-09-12T12:19:00.000Z",
        "voteCount": 1,
        "content": "agreed, B,D,F"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 13,
    "url": "https://www.examtopics.com/discussions/google/view/16279-exam-professional-data-engineer-topic-1-question-13/",
    "body": "You want to process payment transactions in a point-of-sale application that will run on Google Cloud Platform. Your user base could grow exponentially, but you do not want to manage infrastructure scaling.<br>Which Google database service should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 55,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 34,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-02T18:57:00.000Z",
        "voteCount": 77,
        "content": "Initially, thinking D is the best answer but when question is re-re-read, A seems to be correct answer for following reasons\n1. Is payment TRANSACTION -- DB should able to perform full blown transaction (updating inventory, sales info etc, though not specified) , not just ATOMIC which DataStore provides\n2. Its point-of-sale application, not ONLINE STORE where HIGH number of concurrent users ordering stuff. \n3. User Base could grow exponentially - again more users does mot mean concurrent users and more processing power. Its only about storage.\n4. Do not want to Manage infrastructure scaling. - Cloud SQL can scale in terms of storage.\n5. CloudStore is poor selection for OLTP application \n   - Each property is index - so higher latency\n   \nNot sure, during exam 2 min is enough  to think on various point.. \nI may be wrong or wrong path ... lets brainstrom.."
      },
      {
        "date": "2021-11-06T08:15:00.000Z",
        "voteCount": 10,
        "content": "CloudSql does not auto scale."
      },
      {
        "date": "2021-12-05T21:51:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/architecture/elastically-scaling-your-mysql-environment#objectives\n\nPlease read. It can be configured for autoscaling."
      },
      {
        "date": "2021-12-16T19:29:00.000Z",
        "voteCount": 4,
        "content": "That link explains how to set MySQL autoscaling with Google Compute Engine instances (you install and manage MySQL on the VM). This can not be applied to Cloud SQL (managed service). In Cloud SQL, only the storage can be automatically increased, and changing the Cloud SQL instance size requires a manual edit of the instance type."
      },
      {
        "date": "2022-10-28T10:21:00.000Z",
        "voteCount": 3,
        "content": "yes. and that is ok since this is a point of sale. an exponential increase in number of clients still means reduced parallel processing (how many customers can buy in the very same time) so an increase in memory and CPU is very unlikely to be necessary. yes, an exponential increase in the number of customers means more memory, and more storage, which in cloud SQL increases automatically."
      },
      {
        "date": "2022-09-27T21:43:00.000Z",
        "voteCount": 2,
        "content": "C SQL doesn't AUTO SCALE, you need to manually edit , Please show where does it says AUTO SCALING"
      },
      {
        "date": "2021-08-25T00:01:00.000Z",
        "voteCount": 3,
        "content": "Can't online be considered PoS? CloudSQL does have constraints for scaling and Google seem to specifically be selling Datastore for transactional use cases so going with D: \nhttps://cloud.google.com/datastore/docs/concepts/transactions"
      },
      {
        "date": "2021-09-14T23:09:00.000Z",
        "voteCount": 5,
        "content": "Based on a re-read of the above comments and other later questions agree with A. \npls ignore my first answer."
      },
      {
        "date": "2020-03-11T10:22:00.000Z",
        "voteCount": 37,
        "content": "D seems to be the right one. Cloud SQL doesn't automatically scale"
      },
      {
        "date": "2021-12-05T21:47:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL does scale automatically. THERE IS A SETTING WHERE YOU DEFINE INCREASE MEMORY SPACE WHEN IT REACHED 70%. \n\nhttps://cloud.google.com/sql/docs/features#features_3\n\nHere it say's \n-&gt; Fully managed SQL Server databases in the cloud.\n-&gt; Custom machine types with up to 624 GB of RAM and 96 CPUs.\n-&gt; Up to 64 TB of storage available, with the ability to automatically increase storage size as needed."
      },
      {
        "date": "2021-12-16T19:33:00.000Z",
        "voteCount": 5,
        "content": "Storage scale is automatic (e.g. you begin with a 50GB disk and it grows automatically as needed), but the instance size (CPU/memory) will be the same. The questions states that the user base may increase exponentially. Even if you have enough disk space to store all your user data, the increase in users will cause problems if your instance (CPU/memory) is too small, since the instance will not be able to process all the queries at the required speed."
      },
      {
        "date": "2022-12-08T15:57:00.000Z",
        "voteCount": 2,
        "content": "I believe the key point is it's a POS, not an e-commerce. Keeping that in mind, exponential user increase in POS might not mean concurrent user increase, which could be a huge consideration in case of it is being e-commerce.\n\nI would rather go with 'Cloud SQL' as the best answer."
      },
      {
        "date": "2024-09-23T23:20:00.000Z",
        "voteCount": 9,
        "content": "D seems to be the answer. This is what I think based on my analysis below.\nPOS is OLTP system but now a days NOSQL with ACID properties also are used for OLTP,\nCloud sql is good for relational database and it would have been an option here but it clearly says that \"you do not want to manage infrastructure scaling\". In cloud SQL, which is managed service and not server less, you need to manually do vertical scaling(scale up and scale down). \nHence I believe CLOUD SQL is not the option here. \nI also tried creating a datastore using google cloud console and it gives 2 options now that is cloud firestore in native mode and cloud firestore in data store mode. automatic scaling is available in both where there is no manual scaling up or down is required. Also, both firestore in native and datastore provides ACID properties. Also, firestore is now optimized for OLTP. Please see below\nhttps://cloud.google.com/solutions/building-scalable-apps-with-cloud-firestore\nThough the question only talks about datastore, I am just providing additional information. \nConsidering all what I read through, D is the answer."
      },
      {
        "date": "2021-07-02T11:52:00.000Z",
        "voteCount": 1,
        "content": "I agree. I think people are missing the part of the question that mentions they don't want to maintain the DB."
      },
      {
        "date": "2021-07-02T11:53:00.000Z",
        "voteCount": 1,
        "content": "This should be accepted and the highest voted answer."
      },
      {
        "date": "2024-09-23T23:19:00.000Z",
        "voteCount": 2,
        "content": "D is the hero here.\nThough Cloud SQL has an upper hand when it comes to transactions(OLTP), it does not autoscale its computing capabilities as compared to datastore.\nDo visit: https://cloud.google.com/datastore/docs/concepts/overview#what_its_good_for"
      },
      {
        "date": "2024-09-23T23:19:00.000Z",
        "voteCount": 4,
        "content": "D is correct: Datastore (currently Firestore in native or datastore mode). It is a fully managed and serverless solution that allows for transactions and will autoscale (storage and compute) without the need to manage any infrastructure. \nA is wrong: Cloud SQL is fully a managed transactional DB, but only the storage grows automatically. As your user base increases, you will need to increase the CPU/memory of the instance, and to do that you must edit the instance manually (and the questions specifically says \"you do not want to manage infrastructure scaling\")\nB is wrong: Bigquery is OLAP (for analytics). NoOps, fully managed, autoscales and allows transactions, but it is not designed for this use case.\nC is wrong: Bigtable is a NoSQL database for massive writes, and to scale (storage and CPU) you must add nodes, so it is completely out of this use case."
      },
      {
        "date": "2021-12-27T16:49:00.000Z",
        "voteCount": 2,
        "content": "May be some history can help to decide which is best answer.\nDatastore built by Google uses BigTable as it's storage, while the company who built FireStore uses Cloud Spanner as it's storage.  Google decided that they like the FireStore technology and acquired it.\nIf Cloud Spanner is an option I would choose it.  So, D for me, although it's json storage format, but the Cloud Spanner it uses as storage fits all the requirements."
      },
      {
        "date": "2024-09-23T23:16:00.000Z",
        "voteCount": 1,
        "content": "B - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability \nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough"
      },
      {
        "date": "2024-09-23T23:16:00.000Z",
        "voteCount": 2,
        "content": "B - not an option\nC - lack of ACID transactions\nA - lack of resource automatic scalability\nD - (correct, IMHO) support ACID, suitable for OLPT and scalable enough"
      },
      {
        "date": "2024-09-23T23:16:00.000Z",
        "voteCount": 1,
        "content": "Cloud Datastore (now part of Google Cloud Firestore in Datastore mode) is designed for high scalability and ease of management for applications. It is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It's serverless, meaning it handles the scaling, performance, and management automatically, fitting your requirement of not wanting to manage infrastructure scaling.\n\nCloud SQL, while a fully-managed relational database service that makes it easy to set up, manage, and administer your SQL databases, is not as automatically scalable as Datastore. It's better suited for applications that require a traditional relational database."
      },
      {
        "date": "2024-09-20T07:29:00.000Z",
        "voteCount": 1,
        "content": "This actually is A. I was initially in the D camp, and have spent considerable time reading about it now (circa 1 hour). D is explicitly not suitable for payment transactions, as Datastore supports ACID transactions, but only within entity groups, which are small, localized sets of data. This restriction means that transactions are not suitable for scenarios requiring multi-entity consistency across the entire database.\nThe only two products recommended for payments in the Google ecosystem are Cloud Spanner and Cloud SQL. Is Cloud SQL managed? I'd say it wasn't really, due to the need to configure instances, but that is trumped by the fact it is the only choice suitable for a payment transaction system."
      },
      {
        "date": "2024-08-11T03:39:00.000Z",
        "voteCount": 1,
        "content": "Cloud Datastore is a fully managed, NoSQL document database that is highly scalable and designed to automatically handle large increases in traffic without requiring manual intervention. It's well-suited for applications with a rapidly growing user base."
      },
      {
        "date": "2024-07-30T11:21:00.000Z",
        "voteCount": 1,
        "content": "Firestore extension of Datastore can handle acid transactions and allows autoscaling"
      },
      {
        "date": "2024-06-27T07:14:00.000Z",
        "voteCount": 1,
        "content": "A. Requires Transactions."
      },
      {
        "date": "2024-05-31T06:38:00.000Z",
        "voteCount": 1,
        "content": "Transactional = SQL. Anything with payment processors should NOT be NoSQL"
      },
      {
        "date": "2024-05-27T18:57:00.000Z",
        "voteCount": 1,
        "content": "Transactional DB"
      },
      {
        "date": "2024-05-18T18:50:00.000Z",
        "voteCount": 2,
        "content": "The best choice for this scenario is D. Cloud Datastore. Here's why:\n\nScalability: Cloud Datastore is a NoSQL database designed for automatic scaling. It can handle exponential growth without requiring manual intervention for infrastructure management.\nNoSQL: This type of database is well-suited for transactional data like payment processing, where the schema can be flexible.\nFully Managed: Cloud Datastore is a fully managed service, meaning Google takes care of the underlying infrastructure, maintenance, and updates.\nHere's why the other options aren't the best fit:\n\nCloud SQL: While suitable for relational data, it might not be the most scalable or cost-effective option for handling massive growth in a point-of-sale application.\nBigQuery: Primarily designed for data warehousing and analytics, not transactional processing.\nCloud Bigtable: A high-performance NoSQL database, but it's more suited for large-scale, low-latency applications and might be overkill for a point-of-sale system."
      },
      {
        "date": "2024-04-28T10:29:00.000Z",
        "voteCount": 1,
        "content": "auto-scale is the key word to unfold the correct answer. D is the the correct answer"
      },
      {
        "date": "2023-11-07T11:55:00.000Z",
        "voteCount": 1,
        "content": "D - Because DataStore supports massive scaling and ACID transactions which are two primary considerations in this scenario."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 14,
    "url": "https://www.examtopics.com/discussions/google/view/16281-exam-professional-data-engineer-topic-1-question-14/",
    "body": "You want to use a database of information about tissue samples to classify future tissue samples as either normal or mutated. You are evaluating an unsupervised anomaly detection method for classifying the tissue samples. Which two characteristic support this method? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are very few occurrences of mutations relative to normal samples.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThere are roughly equal occurrences of both normal and mutated samples in the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou expect future mutations to have different features from the mutated samples in the database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou expect future mutations to have similar features to the mutated samples in the database.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYou already have labels for which samples are mutated and which are normal in the database."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 60,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 53,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T04:28:00.000Z",
        "voteCount": 73,
        "content": "I think that AD makes more sense. D is the explanation you gave. In the rest, A makes more sense, in any anomaly detection algorithm it is assumed a priori that you have much more \"normal\" samples than mutated ones, so that you can model normal patterns and detect patterns that are \"off\" that normal pattern. For that you will always need the no. of normal samples to be much bigger than the no. of mutated samples."
      },
      {
        "date": "2021-12-05T21:55:00.000Z",
        "voteCount": 19,
        "content": "Guys its A &amp; C.\nAnomaly detection has two basic assumptions: \n-&gt;Anomalies only occur very rarely in the data.  (a)\n-&gt;Their features differ from the normal instances significantly. (c)\n\nlink -&gt; https://towardsdatascience.com/anomaly-detection-for-dummies-15f148e559c1#:~:text=Unsupervised%20Anomaly%20Detection%20for%20Univariate%20%26%20Multivariate%20Data.&amp;text=Anomaly%20detection%20has%20two%20basic,from%20the%20normal%20instances%20significantly."
      },
      {
        "date": "2021-12-15T13:59:00.000Z",
        "voteCount": 27,
        "content": "I don't agree on C. Anomaly detection assumes \"Their features differ from the NORMAL INSTANCES significantly\" and in the C  option you have: \n\"You expect future mutations to have different features from the MUTATED SAMPLES IN THE DATABASE\". \n\nIMHO Answer D fits better: \"D. You expect future mutations to have similar features to the mutated samples in the database.\" - in other words: Expect future anomalies to be similar to the anomalies that we already have in database"
      },
      {
        "date": "2020-03-11T10:24:00.000Z",
        "voteCount": 21,
        "content": "A instead of B:\n\"anomaly detection (also outlier detection[1]) is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data"
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 4,
        "content": "AD: to use unsupervised anomaly detection the anomalies a) must be rare b) they must differ from the NORMAL. So...\nA: mutated samples must be scarce compared to normal tissue.\nD: yes, we expect the future mutated samples to have similar features to the mutated samples currently in the database.\nWhy not C? If I train my model with mutated samples with specific characteristics, I do not expect it to find different mutations. In the future, when new mutations appear, I would retrain my model including those new samples."
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 4,
        "content": "Anomaly detection has two basic assumptions:\n*Anomalies only occur very rarely in the data.\n*Their features differ from the normal instances significantly.\nAnomaly detection involves identifying rare data instances (anomalies) that come from a different class or distribution than the majority (which are simply called \u201cnormal\u201d instances). Given a training set of only normal data, the semi-supervised anomaly detection task is to identify anomalies in the future. Good solutions to this task have applications in fraud and intrusion detection.\nThe unsupervised anomaly detection task is different: Given unlabeled, mostly-normal data, identify the anomalies among them.\nhttps://www.science.gov/topicpages/u/unsupervised+anomaly+detection\nA because \u201cUnsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal\u201d, B is for Supervised anomaly detection https://en.wikipedia.org/wiki/Anomaly_detection"
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 2,
        "content": "A - anomaly detection is used for detecting rare events, meaning it is expected that there are much less of those than of normal ones.\nD - you expect the future mutations to be similar to the mutations you already have, so that you can detect them (pattern recognition)"
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 3,
        "content": "A makes sense\n\nC and D compares future mutations to mutated samples in database\n\nThe question is pretty badly worded\u2026 If we were to run a full unsupervised anomaly detection over the entire dataset, C and D will be true, since some future mutations may be similar to current mutations and some will be significantly different to current mutations.\n\nThe question is suggesting \"labelling\" tissue samples using unsupervised anomaly detection, and subsequently using the labels with a supervised algorithm to classify future samples. If this interpretation of the question is correct, then D makes sense"
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 2,
        "content": "The answer should be AD.\n\nA, anomaly should have a little amount, if there are many samples then we should do classification instead, because unsupervised will give a lot of false positive.\n\nD, the future anomaly should be of the same distribution as present anomaly! or else our anomaly detection will not be generalize to the future feature."
      },
      {
        "date": "2024-09-23T23:22:00.000Z",
        "voteCount": 2,
        "content": "A. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated."
      },
      {
        "date": "2024-09-23T23:21:00.000Z",
        "voteCount": 5,
        "content": "D should be correct. You expect future samples will correlate with the training samples. That's the whole point of learning procedure. If you do not expect that they have similar features, then why would you use features in the training samples in the first place? A is also correct, since anomaly labels would be seen rarely."
      },
      {
        "date": "2024-09-23T23:21:00.000Z",
        "voteCount": 2,
        "content": "A. There are very few occurrences of mutations relative to normal samples. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying rare events or anomalies in large amounts of data. By training the algorithm on the normal tissue samples in the database, it can then identify new tissue samples that have different features from the normal samples and classify them as mutated.\n\nD. You expect future mutations to have similar features to the mutated samples in the database. This characteristic is supportive of using an unsupervised anomaly detection method, as it is well suited for identifying patterns or anomalies in the data. By training the algorithm on the mutated tissue samples in the database, it can then identify new tissue samples that have similar features and classify them as mutated."
      },
      {
        "date": "2024-08-08T01:14:00.000Z",
        "voteCount": 2,
        "content": "A. There are very few occurrences of mutations relative to normal samples.\n\nAnomaly detection is well-suited for situations where anomalies (in this case, mutations) are rare compared to the normal cases. When the dataset is highly imbalanced, with far fewer mutated samples than normal samples, anomaly detection can be used to identify these rare cases as outliers or anomalies.\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection works under the assumption that anomalies (mutations) will differ significantly from the majority of the data (normal samples). If future mutations are expected to exhibit different features, this method can help detect those anomalies as deviations from the normal samples."
      },
      {
        "date": "2024-07-30T11:37:00.000Z",
        "voteCount": 2,
        "content": "A. There are very few occurrences of mutations relative to normal samples.\nAnomaly detection is particularly useful in scenarios where anomalies (mutations, in this case) are rare compared to normal instances. This aligns with the nature of anomaly detection, which focuses on identifying rare events that deviate significantly from the majority (normal) data.\n\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nUnsupervised anomaly detection methods do not rely on prior knowledge of anomalies. They work on the assumption that anomalies will be different from normal instances in a significant way. If future mutations have different features from known mutations, it supports using an unsupervised method as it can detect novel anomalies not seen during training"
      },
      {
        "date": "2024-07-09T06:51:00.000Z",
        "voteCount": 1,
        "content": "That's A and D. The aim of unsupervised classification of anomalies is to identify sub-groups with characteristics in common that may resemble anomalies. So, when a new mutation appears, we can determine whether it shares characteristics with previously discovered anomaly subgroups. If this mutation is an anomaly and has very different characteristics from our detected anomaly subgroup, it is likely to be associated with an incorrect group."
      },
      {
        "date": "2024-06-06T00:41:00.000Z",
        "voteCount": 1,
        "content": "A. There are very few occurrences of mutations relative to normal samples.\n\nUnsupervised anomaly detection is particularly useful in situations where anomalies (mutations) are rare compared to the normal instances. This characteristic aligns well with unsupervised methods that can detect outliers or rare events in a dataset dominated by normal samples.\nC. You expect future mutations to have different features from the mutated samples in the database.\n\nAnomaly detection methods are effective when future anomalies do not follow the same patterns as the known anomalies. These methods aim to identify instances that significantly deviate from the norm, which suits the scenario where future mutations might exhibit different characteristics from those currently known."
      },
      {
        "date": "2023-12-19T06:32:00.000Z",
        "voteCount": 2,
        "content": "As A is a good answer, i'd like to give my point of view on the second right answer. I initially thought D was the correct one, as you would normally train your model to detect mutations seen in the training dataset. But the goal of unsupervised learning is to detect unidentified patterns. If you were sure the mutations would always look the same, you'd rather use supervised learning and labels the \"normal\" and \"mutated\" tissues, which would result in better performances in my point of view."
      },
      {
        "date": "2023-11-09T13:26:00.000Z",
        "voteCount": 2,
        "content": "Unsupervised anomaly detection is best for scenarios without labels or when the anomalies are unknown or ever-changing"
      },
      {
        "date": "2023-10-29T10:13:00.000Z",
        "voteCount": 2,
        "content": "AC; might also be interesting - https://towardsdatascience.com/unsupervised-learning-for-anomaly-detection-44c55a96b8c1 as comments below"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 15,
    "url": "https://www.examtopics.com/discussions/google/view/16723-exam-professional-data-engineer-topic-1-question-15/",
    "body": "You need to store and analyze social media postings in Google BigQuery at a rate of 10,000 messages per minute in near real-time. Initially, design the application to use streaming inserts for individual postings. Your application also performs data aggregations right after the streaming inserts. You discover that the queries after streaming inserts do not exhibit strong consistency, and reports from the queries might miss in-flight data. How can you adjust your application design?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-write the application to load accumulated data every 2 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the streaming insert code to batch load for individual messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the original message to Google Cloud SQL, and export the table every hour to BigQuery via streaming inserts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstimate the average latency for data availability after streaming inserts, and always run queries after waiting twice as long.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-11-08T11:09:00.000Z",
        "voteCount": 7,
        "content": "B. Streams data into BigQuery one record at a time without needing to run a load job: https://cloud.google.com/bigquery/docs/reference/rest/v2/tabledata/insertAll\nInstead of using a job to load data into BigQuery, you can choose to stream your data into BigQuery one record at a time by using the tabledata.insertAll method. This approach enables querying data without the delay of running a load job:\nhttps://cloud.google.com/bigquery/streaming-data-into-bigquery\nThe BigQuery Storage Write API is a unified data-ingestion API for BigQuery. It combines the functionality of streaming ingestion and batch loading into a single high-performance API. You can use the Storage Write API to stream records into BigQuery that become available for query as they are written, or to batch process an arbitrarily large number of records and commit them in a single atomic operation.\nCommitted mode. Records are available for reading immediately as you write them to the stream. Use this mode for streaming workloads that need minimal read latency.\nhttps://cloud.google.com/bigquery/docs/write-api"
      },
      {
        "date": "2021-11-18T18:08:00.000Z",
        "voteCount": 1,
        "content": "IN THIS ALSO BIGQUERY HAS A BUFFER  WHICH IT TAKES SLOWLY ANS INSERTS INTO REAL THING, WHAT YOU SAID IS HELPFULL IN REMOVING THE APPLICATION PART"
      },
      {
        "date": "2021-12-15T15:04:00.000Z",
        "voteCount": 1,
        "content": "could you please argue?"
      },
      {
        "date": "2024-09-23T23:26:00.000Z",
        "voteCount": 6,
        "content": "Answer: D. The only that describe a way to resolve the problem, with buffering the data. \n\n(the question is possible old, the best approach would be Pub/Sub + Dataflow Streaming + Bigquery for streaming data instead near-real time)"
      },
      {
        "date": "2024-09-23T23:26:00.000Z",
        "voteCount": 5,
        "content": "\"D\" seems to use the typical approximate terminology of a wrong answer. \"estimate the time\" (how do you do that? do you do that over different times of the day?) and \"wait twice as long\" (who tells you that there are not a lot of cases when lag is twice as long?). Instead, \"A\" seems good. You don't need to show the exact results, but an approximation thereof, but you still want consistency. So an aggregation of the data every 2 minutes is a viable thing."
      },
      {
        "date": "2024-09-23T23:26:00.000Z",
        "voteCount": 2,
        "content": "D is correct. The problem requirement is doing analytics on real-time data. You cannot do batch processing because the business requires it to be real-time even if it makes your job simpler, so B is incorrect. Other options are not streaming."
      },
      {
        "date": "2024-09-23T23:25:00.000Z",
        "voteCount": 2,
        "content": "There are assumptions over the quality of data acceptable. If slight variations of the analytics against actual can be accepted, then D would be a good choice.\n\nMany people chose B, but this also requires some form of waiting for the late data to arrive.\n\nI think a combination of D and B can be applied, but for an intial fix, delaying the aggregation queries with D seems to make more sense. If the variance is small and the some late data leakage is acceptable, and we can remain as D.\n\nIf problems arise, we can always proceed to attempt B"
      },
      {
        "date": "2024-09-23T23:25:00.000Z",
        "voteCount": 2,
        "content": "The streaming mode may be in pending mode or buffered mode where the streaming data is not immediately available before committing or flushing. Thus, we need to wait before the data will be available. Or else we need to switch to commited mode (which is not present in the choices)."
      },
      {
        "date": "2024-09-23T23:24:00.000Z",
        "voteCount": 5,
        "content": "Answer: D\nWhat to learn or look for\n1. In-Flight data = (Real Time data, i.e still in streaming pipeline and not landed in BigQuery)\n2. Dataflow (assume in best case) streaming pipeline is running to send data to Bigquery. \nWhy not option B: change streaming to batch upload is not business requirement, we have to stuck to streaming and real time analysis. \n\nOption D: make bigquery run after waiting for sometime (twice here), How will you do it? \n- there is not setting in BQ to do it, right!. So, adjust it in your pipeline (dataflow)\n- For example, add Fixed window, and you want to execute aggregation query after 2 min. \nCode\n```pipeline.apply(...)\n   .apply(Window.&lt;TableRow&gt;into(FixedWindows.of(Duration.standardMinutes(2))))\n    .apply(BigQueryIO.writeTableRows()\n        .to(\"my_dataset.my_table\")\n```"
      },
      {
        "date": "2024-01-25T05:19:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nI agree with the first part of the D answer, but for the second part, I don't know how they came about the 2 mins, is it from a calculation?"
      },
      {
        "date": "2023-10-06T20:54:00.000Z",
        "voteCount": 3,
        "content": "A. Re-write the application to load accumulated data every 2 minutes.\n\nBy accumulating data and performing a batch load every 2 minutes, you can reduce the potential inconsistency caused by streaming inserts. While this introduces a slight delay, it provides a more consistent approach than streaming each individual message. This method can still meet the near real-time requirement, and the slight delay is often acceptable in scenarios where data consistency is paramount."
      },
      {
        "date": "2023-09-27T06:35:00.000Z",
        "voteCount": 1,
        "content": "BBBBB is the only option"
      },
      {
        "date": "2023-09-18T11:40:00.000Z",
        "voteCount": 4,
        "content": "I'd argue that this question became outdated with the introduction of the BigQuery Storage Write API: https://cloud.google.com/bigquery/docs/write-api"
      },
      {
        "date": "2023-10-29T10:18:00.000Z",
        "voteCount": 1,
        "content": "Good point"
      },
      {
        "date": "2023-08-01T10:43:00.000Z",
        "voteCount": 3,
        "content": "Streaming inserts in BigQuery are not immediately available to be queried, which is causing the weak consistency you're observing. A better approach is to batch the data and load it at regular intervals. Loading the data every two minutes is still relatively real-time, and it should help solve the consistency problem.\nAnswer A."
      },
      {
        "date": "2023-07-29T20:11:00.000Z",
        "voteCount": 1,
        "content": "All the options aim to address the challenge of strong consistency in the data and potential missing data that may occur with streaming inserts. Each approach has its pros and cons, so the best choice depends on the specific needs and requirements of the application. It's like having different strategies for keeping track of all the fun things the kids do and say on the playground, making sure nothing gets left behind!"
      },
      {
        "date": "2023-06-09T01:59:00.000Z",
        "voteCount": 3,
        "content": "Streaming Inserts is marked as Legacy now.\nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery#dataavailability\n\nThe documentation is hinting on it can take up to 90 minutes to process the buffered data. \nThis question is testing if you are aware of the possible long times the buffer can build up."
      },
      {
        "date": "2023-04-22T18:08:00.000Z",
        "voteCount": 3,
        "content": "In my experience, estimation in D is not a technical solution. it is just a guess solution.\nYou might still get caught when loading get higher and easily take twice as long latency, then problem occur again.\nSo for a more permanent solution, you should definitely go with B"
      },
      {
        "date": "2023-03-10T21:47:00.000Z",
        "voteCount": 3,
        "content": "1s tline of question requires near real time queries so D is the best option as batch load is never near real time"
      },
      {
        "date": "2023-02-19T07:53:00.000Z",
        "voteCount": 3,
        "content": "near realtime"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 16,
    "url": "https://www.examtopics.com/discussions/google/view/16729-exam-professional-data-engineer-topic-1-question-16/",
    "body": "Your startup has never implemented a formal security policy. Currently, everyone in the company has access to the datasets stored in Google BigQuery. Teams have freedom to use the service as they see fit, and they have not documented their use cases. You have been asked to secure the data warehouse. You need to discover what everyone is doing. What should you do first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Stackdriver Audit Logs to review data access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet the identity and access management IIAM) policy of each table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring to see the usage of BigQuery query slots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Google Cloud Billing API to see what account the warehouse is being billed to."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-11-06T12:37:00.000Z",
        "voteCount": 6,
        "content": "Table access control is now possible in big query. However, before even checking table access control permission which is not set by the company as a formal security policy yet, we need to first understand by looking at the big query immutable audit logs as who is accessing what DAT sets and tables. Based on the information, access control policy at dataset and table level can be set. \n\nSo the correct answer is A"
      },
      {
        "date": "2020-07-16T06:06:00.000Z",
        "voteCount": 5,
        "content": "A - need to check first who is accessing which table"
      },
      {
        "date": "2024-09-23T23:28:00.000Z",
        "voteCount": 2,
        "content": "A is correct because this is the best way to get granular access to data showing which users are accessing which data. \nB is not correct because we already know that all users already have access to all data, so this information is unlikely to be useful. It will also not show what users have done, just what they can do.\nC is not correct because slot usage will not inform security policy.\nD is not correct because a billing account is typically shared among many people and will only show the amount of data queried and stored\nhttps://cloud.google.com/bigquery/docs/reference/auditlogs/#mapping-audit-entries-to-log-streams\nhttps://cloud.google.com/bigquery/docs/monitoring#slots-available"
      },
      {
        "date": "2024-09-23T23:27:00.000Z",
        "voteCount": 2,
        "content": "A. Use Google Stackdriver Audit Logs to review data access.\n\nReviewing the audit logs provides visibility into who is accessing your data, when they are doing so, and what actions they are taking within BigQuery. This is crucial for understanding current data usage and potential security risks.\n\nOption B (getting the IAM policy of each table) is important but more focused on controlling access rather than discovering what everyone is currently doing.\n\nOption C (using Stackdriver Monitoring to see query slots usage) can help with monitoring and optimizing your BigQuery usage but doesn't provide a comprehensive view of what users are doing with the data.\n\nOption D (using the Google Cloud Billing API) is more related to tracking billing information rather than understanding what users are doing with the data."
      },
      {
        "date": "2024-09-23T23:27:00.000Z",
        "voteCount": 4,
        "content": "To begin securing your data warehouse in Google BigQuery and gain insights into what everyone is doing with the datasets, the first step you should take is:\n\nA. Use Google Stackdriver Audit Logs to review data access.\n\nReviewing the audit logs provides visibility into who is accessing your data, when they are doing so, and what actions they are taking within BigQuery. This is crucial for understanding current data usage and potential security risks.\n\nOption B (getting the IAM policy of each table) is important but more focused on controlling access rather than discovering what everyone is currently doing.\n\nOption C (using Stackdriver Monitoring to see query slots usage) can help with monitoring and optimizing your BigQuery usage but doesn't provide a comprehensive view of what users are doing with the data.\n\nOption D (using the Google Cloud Billing API) is more related to tracking billing information rather than understanding what users are doing with the data."
      },
      {
        "date": "2024-09-23T23:27:00.000Z",
        "voteCount": 1,
        "content": "A. Use Google Stackdriver Audit Logs to review data access.\n\nIn this scenario, you have been asked to secure the data warehouse in Google BigQuery. To do that, you first need to understand what everyone is doing with the data, i.e., who is accessing it and what actions they are performing. Google Stackdriver Audit Logs can provide you with a detailed record of all the data access and actions taken by users in Google BigQuery. It's like having a logbook that keeps track of who enters the library, which books they read, and what they do with the books.\n\nC just give how many people accessing the same dataset at given time \nC. Another tool you have is called \"Stackdriver Monitoring.\" It helps you see how many people are using the library at the same time. It's like knowing how many readers are in the library at any given moment."
      },
      {
        "date": "2024-09-23T23:27:00.000Z",
        "voteCount": 1,
        "content": "A - Since the question is to discover what everyone is doing. Also the question has indicated that no security policies have been implemented."
      },
      {
        "date": "2024-09-23T23:27:00.000Z",
        "voteCount": 1,
        "content": "A. Use Google Stackdriver Audit Logs to review data access.\n\nReviewing the audit logs provides visibility into who is accessing your data when they are doing so, and what actions they are taking within BigQuery. This is crucial for understanding current data usage and potential security risks."
      },
      {
        "date": "2024-01-25T06:04:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\nBut recently, I think Dataplex is used for data governance ."
      },
      {
        "date": "2023-10-06T20:56:00.000Z",
        "voteCount": 1,
        "content": "A. Use Google Stackdriver Audit Logs to review data access.\n\nStackdriver Audit Logs provide detailed logs on who accessed what resources and when, including data in BigQuery. Reviewing these logs will give you insight into which users and service accounts are accessing datasets, what operations they are performing, and when these accesses occur. This would be a crucial first step in understanding current usage and subsequently in crafting a security policy."
      },
      {
        "date": "2023-09-15T10:21:00.000Z",
        "voteCount": 1,
        "content": "Stackdriver audit logs is where we will view which datasets are being accessed by whom"
      },
      {
        "date": "2023-03-10T21:48:00.000Z",
        "voteCount": 1,
        "content": "In order to take a decision you need to analyze the access lofs"
      },
      {
        "date": "2023-02-13T07:30:00.000Z",
        "voteCount": 1,
        "content": "\"Discover what everyone is doing\" will happen through Audit logs, hence correct answer is A"
      },
      {
        "date": "2022-12-11T05:37:00.000Z",
        "voteCount": 1,
        "content": "\"...to secure the data warehouse\"  is to list all tables/views/Mviews VS. who is accessing these objects. Slot info is not relevant."
      },
      {
        "date": "2022-11-13T09:46:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-09-28T00:28:00.000Z",
        "voteCount": 1,
        "content": "Audit log ..logs activities against resources , is the best place to discover about activities against BQ"
      },
      {
        "date": "2022-09-18T07:14:00.000Z",
        "voteCount": 1,
        "content": "A for sure"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 17,
    "url": "https://www.examtopics.com/discussions/google/view/16730-exam-professional-data-engineer-topic-1-question-17/",
    "body": "Your company is migrating their 30-node Apache Hadoop cluster to the cloud. They want to re-use Hadoop jobs they have already created and minimize the management of the cluster as much as possible. They also want to be able to persist data beyond the life of the cluster. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Cloud Dataflow job to process the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Cloud Dataproc cluster that uses persistent disks for HDFS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Hadoop cluster on Google Compute Engine that uses persistent disks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Hadoop cluster on Google Compute Engine that uses Local SSD disks."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-11-09T12:11:00.000Z",
        "voteCount": 10,
        "content": "D is correct because it uses managed services, and also allows for the data to persist on GCS beyond the life of the cluster.\nA is not correct because the goal is to re-use their Hadoop jobs and MapReduce and/or Spark jobs cannot simply be moved to Dataflow.\nB is not correct because the goal is to persist the data beyond the life of the ephemeral clusters, and if HDFS is used as the primary attached storage mechanism, it will also disappear at the end of the cluster\u2019s life.\nC is not correct because the goal is to use managed services as much as possible, and this is the opposite.\nE is not correct because the goal is to use managed services as much as possible, and this is the opposite."
      },
      {
        "date": "2020-11-06T12:50:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is D. Here is the explanation to why Data proc and why not Data flow. \nWhen a company wants to move their existing Hadoop jobs  on premise to cloud, we can simply move the jobs in cloud data prod and replace hdfs with gs:// which is google storage. This way you are keeping compute and storage separately. Hence the correct answer is D. However, if the company wants to complete create a new jobs and don\u2019t want to use the existing Hadoop jobs running on premise, the option is to create new data flow jobs."
      },
      {
        "date": "2024-09-23T23:28:00.000Z",
        "voteCount": 1,
        "content": "D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\nDataproc clusters can be created to lift and shift existing Hadoop jobs\nData stored in Google Cloud Storage extends beyond the life of a Dataproc cluster."
      },
      {
        "date": "2024-09-23T23:28:00.000Z",
        "voteCount": 1,
        "content": "D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n\nHere's why:\n\n    Cloud Dataproc allows you to run Apache Hadoop jobs with minimal management. It is a managed Hadoop service.\n\n    Using the Google Cloud Storage (GCS) connector, Dataproc can access data stored in GCS, which allows data persistence beyond the life of the cluster. This means that even if the cluster is deleted, the data in GCS remains intact. Moreover, using GCS is often cheaper and more durable than using HDFS on persistent disks."
      },
      {
        "date": "2024-09-23T23:28:00.000Z",
        "voteCount": 3,
        "content": "D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector.\n\nGoogle Cloud Dataproc is a managed Hadoop and Spark service that allows you to easily create and manage Hadoop clusters in the cloud. By using the Google Cloud Storage connector, you can persist data in Google Cloud Storage, which provides durable storage beyond the cluster's lifecycle. This approach ensures data is retained even if the cluster is terminated, and it allows you to reuse your existing Hadoop jobs.\n\nOption B (Creating a Dataproc cluster that uses persistent disks for HDFS) is another valid choice. However, using Google Cloud Storage for data storage and processing is often more cost-effective and scalable, especially when migrating to the cloud.\n\nOptions A, C, and E do not take full advantage of Google Cloud's services and the benefits of cloud-native data storage and processing with Google Cloud Storage and Dataproc."
      },
      {
        "date": "2024-06-28T06:18:00.000Z",
        "voteCount": 1,
        "content": "Option D is incorrect, as it would not provide persistent HDFS storage within cluster itself. Rather B should be the correct answer."
      },
      {
        "date": "2023-09-15T03:52:00.000Z",
        "voteCount": 1,
        "content": "Correct D"
      },
      {
        "date": "2023-03-10T21:49:00.000Z",
        "voteCount": 2,
        "content": "Hadoop --&gt; Dataproc   Persistent storage after the processing --&gt; GCS"
      },
      {
        "date": "2023-01-11T08:39:00.000Z",
        "voteCount": 1,
        "content": "D Seems right. Cloud storage can be used to achieve data storage even after the life of cluster."
      },
      {
        "date": "2023-01-06T22:43:00.000Z",
        "voteCount": 1,
        "content": "The answer is D! Dataproc have no need for use to manage the infra and cloudstorage also no need for us to manage too!"
      },
      {
        "date": "2022-12-11T09:38:00.000Z",
        "voteCount": 1,
        "content": "D. Create a Cloud Dataproc cluster that uses the Google Cloud Storage connector."
      },
      {
        "date": "2022-11-05T15:27:00.000Z",
        "voteCount": 1,
        "content": "Seems like it is D. https://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs\nNever saw they mentioned persistent disks, although they are not deleted with the clusters..."
      },
      {
        "date": "2022-11-05T15:29:00.000Z",
        "voteCount": 1,
        "content": "although:\nBy default, when no local SSDs are provided, HDFS data and intermediate shuffle data is stored on VM boot disks, which are Persistent Disks."
      },
      {
        "date": "2022-11-05T15:35:00.000Z",
        "voteCount": 2,
        "content": "and it says that only VM Boot disks are deleted when the cluster is deleted."
      },
      {
        "date": "2022-10-17T00:25:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer : D"
      },
      {
        "date": "2022-09-28T00:33:00.000Z",
        "voteCount": 1,
        "content": "Dataproc cluster set up will be ephemeral to run HDFS Jobs and can be killed after Job execution killing persistent storage with cluster"
      },
      {
        "date": "2022-08-18T20:35:00.000Z",
        "voteCount": 1,
        "content": "Anwer: D"
      },
      {
        "date": "2022-06-06T02:33:00.000Z",
        "voteCount": 1,
        "content": "Isn't it A and D both dataflow for reusable jobs and gcs for data peraistance?"
      },
      {
        "date": "2022-04-11T02:04:00.000Z",
        "voteCount": 2,
        "content": "Two key points:\nManaged hadoop cluster - dataproc\nPersistent storage:  GCS (dataproc uses gcs connector to connect to gcs)"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 18,
    "url": "https://www.examtopics.com/discussions/google/view/16654-exam-professional-data-engineer-topic-1-question-18/",
    "body": "Business owners at your company have given you a database of bank transactions. Each row contains the user ID, transaction type, transaction location, and transaction amount. They ask you to investigate what type of machine learning can be applied to the data. Which three machine learning applications can you use? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSupervised learning to determine which transactions are most likely to be fraudulent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnsupervised learning to determine which transactions are most likely to be fraudulent.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tClustering to divide the transactions into N categories based on feature similarity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSupervised learning to predict the location of a transaction.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReinforcement learning to predict the location of a transaction.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUnsupervised learning to predict the location of a transaction."
    ],
    "answer": "BCD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BCD",
        "count": 25,
        "isMostVoted": true
      },
      {
        "answer": "ABC",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BCE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "ACD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T04:32:00.000Z",
        "voteCount": 69,
        "content": "BCD makes more sense to me. Its for sure not unsupervised, since locations are in the data already. Reinforcement also doesn't fit, as there no AI and no interactions with data from the observer."
      },
      {
        "date": "2021-08-30T08:38:00.000Z",
        "voteCount": 4,
        "content": "D make sense, but i have a doubt: location is a discrete value (no regression), so a multiclass classification model should be applied ... to predict locations?"
      },
      {
        "date": "2021-10-08T18:53:00.000Z",
        "voteCount": 5,
        "content": "yes. multiclass classification model should be applied"
      },
      {
        "date": "2020-03-16T05:58:00.000Z",
        "voteCount": 8,
        "content": "BCD looks more appropriate"
      },
      {
        "date": "2024-09-23T23:30:00.000Z",
        "voteCount": 7,
        "content": "BCD makes sense, but I now agree that BCE is the correct answer. \nSay the model predict a location, guessing US or Sweden are both wrong when the answer is Canada. But US is closer, the distance from the correct location can be used to calculate a reward. Through reinforcement learning (E) the model could guess a location with better accuracy than supervised (D)."
      },
      {
        "date": "2024-09-23T23:30:00.000Z",
        "voteCount": 2,
        "content": "Ans: B, C and D\ni) Fraudulent transaction, is nothing but anomaly detection which falls under Unsupervised.\nii) All transactions can be categorized using type etc - clustering algorithm.\niii) Using location as a label, supervised classification can be developed to predict location."
      },
      {
        "date": "2024-09-23T23:29:00.000Z",
        "voteCount": 7,
        "content": "As wrote by RP123\nB - Not labelled as Fraud or not. So Unsupervised.\nC - Clustering can be done based on location, amount etc.\nD - Location is already given. So labelled. Hence supervised."
      },
      {
        "date": "2024-09-23T23:29:00.000Z",
        "voteCount": 1,
        "content": "BCD makes more sense. B, C should not be controversial. For D vs E, in this use case D fits better than usage of reinfocement learning"
      },
      {
        "date": "2024-09-23T23:29:00.000Z",
        "voteCount": 1,
        "content": "BCD makes more sens to me"
      },
      {
        "date": "2024-09-23T23:29:00.000Z",
        "voteCount": 5,
        "content": "Anwer: BCD\nThings to understand:\nSupervised learning will only predict the column that is labeled. In this case, there is not Fraud or not Fraud column inside which he will train on. So Option A, wrong. \noption D: Supervised learning for column (transaction location) is possible as column exist to train on. \nOption C: Custering N-type is possible and also an unsupervised learning to make cluster of similar pattern. \nOption B: Its a weaker point here, User should be able to know which clusters are fraud in history. As it doesn't give enough information about past analysis whether user knows potential frauds or not. Ignore this option, if question asked for 2 right options only."
      },
      {
        "date": "2024-07-30T11:55:00.000Z",
        "voteCount": 2,
        "content": "Why would you need to predict a location..."
      },
      {
        "date": "2024-07-09T04:16:00.000Z",
        "voteCount": 1,
        "content": "C and D are good for sureCliquez pour utiliser cette solution et E, F wrong for sure.\n \nThen, to choose between A and B. Both options indicate that we know which transactions are fraudulent and which are not. Indeed, in order to use unsupervised classification to determine the characteristics of fraudulent transactions, we must already know which ones are fraudulent, either because all transactions in the dataset are fraudulent, or because a variable allows us to identify them. If all transactions were fraudulent, this would probably have been specified in the statement. It is therefore more likely that the \"type of transaction\" variable can be used to distinguish fraudulent transactions from others.\n\nIn this case, we have a target variable to predict, enabling us to build interpretable supervised models to understand the typology of fraudulent transactions. I therefore opt for A, C and D"
      },
      {
        "date": "2023-11-23T02:19:00.000Z",
        "voteCount": 3,
        "content": "Options B, E, and F are not as suitable for the given scenario:\n\nB. Unsupervised learning to determine which transactions are most likely to be fraudulent.\n\nUnsupervised learning, while useful for anomaly detection, might not be as effective for fraud detection without labeled data indicating which transactions are fraudulent.\nE. Reinforcement learning to predict the location of a transaction.\n\nReinforcement learning is more suitable for scenarios where an agent learns to make decisions through trial and error, which doesn't seem to align with predicting transaction locations.\nF. Unsupervised learning to predict the location of a transaction.\n\nUnsupervised learning typically doesn't involve predicting specific values (like location) without labeled data for training.\nIn summary, A, C, and D are the most appropriate machine learning applications for investigating the provided bank transactions dataset."
      },
      {
        "date": "2023-11-05T13:37:00.000Z",
        "voteCount": 1,
        "content": "Answer: BCD"
      },
      {
        "date": "2023-09-13T03:41:00.000Z",
        "voteCount": 2,
        "content": "Location is already given as attribite so what value is served with predicting location?"
      },
      {
        "date": "2023-08-11T01:20:00.000Z",
        "voteCount": 1,
        "content": "A, B: Data features without the definition of fraudulent, so we can not obtain the answer even if using the unsupervise learning.\nC: Kmeans solve this.\nD: logistic regression. Just put the location into target.\nE: Give the positive reward when the model predicts correct location.\nF: Same as C. Use all features but locations, and use similarity to predict new data."
      },
      {
        "date": "2023-08-07T00:43:00.000Z",
        "voteCount": 1,
        "content": "Absolutely"
      },
      {
        "date": "2023-08-03T23:16:00.000Z",
        "voteCount": 1,
        "content": "makes more sense"
      },
      {
        "date": "2023-07-26T01:16:00.000Z",
        "voteCount": 1,
        "content": "BCD make sense and does not require anything that is not given in the question data."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 19,
    "url": "https://www.examtopics.com/discussions/google/view/16870-exam-professional-data-engineer-topic-1-question-19/",
    "body": "Your company's on-premises Apache Hadoop servers are approaching end-of-life, and IT has decided to migrate the cluster to Google Cloud Dataproc. A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node. The CIO is concerned about the cost of using that much block storage. You want to minimize the storage cost of the migration. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPut the data into Google Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse preemptible virtual machines (VMs) for the Cloud Dataproc cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTune the Cloud Dataproc cluster so that there is just enough disk for all data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate some of the cold data into Google Cloud Storage, and keep only the hot data in Persistent Disk."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-23T23:30:00.000Z",
        "voteCount": 6,
        "content": "Ans: A\nB: Wrong eVM wont solve the problem of larger storage prices.\nC: May be, but nothing mentioned in terms of what to tune in the question, also this is like-for-like migration so tuning may not be part of the migration.\nD: Again, this is like-for-like so need to define what is hot data and which is cold data, also persistent disk costlier than cloud storage."
      },
      {
        "date": "2024-01-05T18:26:00.000Z",
        "voteCount": 1,
        "content": "You are most of the people looking at like for like migration would require 50TB persistent storage but missing to look at CIO concern about cost of block storage...considering CIO concern the option here is cloud storage... moreover that is recommended as well .."
      },
      {
        "date": "2023-10-06T21:06:00.000Z",
        "voteCount": 1,
        "content": "Option A: Put the data into Google Cloud Storage.\n\n    This is the best option. Google Cloud Dataproc is designed to work well with Google Cloud Storage. Using GCS instead of Persistent Disk can save money, and GCS offers advantages such as higher durability and the ability to share data across multiple clusters."
      },
      {
        "date": "2023-10-03T10:39:00.000Z",
        "voteCount": 1,
        "content": "I have seen this question in other places and I believe that you store the older data in Cloud Storage and retain processing data in persistent disk. D"
      },
      {
        "date": "2023-09-04T14:31:00.000Z",
        "voteCount": 1,
        "content": "Answer: D\nQuestion: A like-for- like migration of the cluster would require 50 TB of Google Persistent Disk per node.\nwhich means Persistent  is still required."
      },
      {
        "date": "2023-09-15T10:35:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Storage is designed for 11 9's availability. So it is also kind of persistent storage. Also, it is a Google product, hence recommended.\nhttps://cloud.google.com/storage/docs/availability-durability#key-concepts"
      },
      {
        "date": "2023-08-16T08:14:00.000Z",
        "voteCount": 1,
        "content": "the question is talking about block storage , GCS is object storage  !"
      },
      {
        "date": "2023-08-03T00:57:00.000Z",
        "voteCount": 1,
        "content": "GCS is cost-effective and also Google's recommendation!"
      },
      {
        "date": "2023-03-10T21:53:00.000Z",
        "voteCount": 1,
        "content": "Minimize cost then GCS"
      },
      {
        "date": "2023-01-10T10:37:00.000Z",
        "voteCount": 1,
        "content": "A - is the right answer."
      },
      {
        "date": "2022-12-12T15:57:00.000Z",
        "voteCount": 1,
        "content": "A - dataproc - storage -  cost effective is cloud storage"
      },
      {
        "date": "2022-10-07T11:24:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage"
      },
      {
        "date": "2022-05-30T09:45:00.000Z",
        "voteCount": 1,
        "content": "Cloud Storage is google recommended one"
      },
      {
        "date": "2021-06-25T12:43:00.000Z",
        "voteCount": 2,
        "content": "Vote for 'A\""
      },
      {
        "date": "2024-09-23T23:31:00.000Z",
        "voteCount": 6,
        "content": "A is correct because Google recommends using Cloud Storage instead of HDFS as it is much more cost effective especially when jobs aren\u2019t running.\nB is not correct because this will decrease the compute cost but not the storage cost.\nC is not correct because while this will reduce cost somewhat, it will not be as cost effective as using Cloud Storage.\nD is not correct because while this will reduce cost somewhat, it will not be as cost effective as using Cloud Storage."
      },
      {
        "date": "2021-05-04T00:12:00.000Z",
        "voteCount": 1,
        "content": "A, Moving the data to GCS will reduce the cost of running the dataproc clusters all the time \\"
      },
      {
        "date": "2021-02-06T10:02:00.000Z",
        "voteCount": 2,
        "content": "Correct A"
      },
      {
        "date": "2020-11-02T06:48:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is A - Put the data into google cloud storage. This is what Google always recommend."
      },
      {
        "date": "2020-09-04T17:42:00.000Z",
        "voteCount": 4,
        "content": "Because the cluster is going to end-of-life. So maybe don't need to put hot data in the disk. All can be put in the Cloud Storage. The answer is A."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 20,
    "url": "https://www.examtopics.com/discussions/google/view/16282-exam-professional-data-engineer-topic-1-question-20/",
    "body": "You work for a car manufacturer and have set up a data pipeline using Google Cloud Pub/Sub to capture anomalous sensor events. You are using a push subscription in Cloud Pub/Sub that calls a custom HTTPS endpoint that you have created to take action of these anomalous events as they occur. Your custom<br>HTTPS endpoint keeps getting an inordinate amount of duplicate messages. What is the most likely cause of these duplicate messages?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe message body for the sensor event is too large.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYour custom endpoint has an out-of-date SSL certificate.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe Cloud Pub/Sub topic has too many messages published to it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tYour custom endpoint is not acknowledging messages within the acknowledgement deadline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-11T10:37:00.000Z",
        "voteCount": 86,
        "content": "The Answer should be D. The custom endpoint is not acknowledging the message, that is the reason for Pub/Sub to send the message again and again. Not B."
      },
      {
        "date": "2020-04-05T01:44:00.000Z",
        "voteCount": 9,
        "content": "D :  Doubt should be only between B &amp; D. But B is not possible because if SSL is expired then endpoint URL will not receive any messages forget about duplicates. So It should be D for Duplicates."
      },
      {
        "date": "2024-09-23T23:32:00.000Z",
        "voteCount": 5,
        "content": "D\nWhy are there too many duplicate messages?\n\nPub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Pub/Sub is retrying the message delivery. This can be observed in the monitoring metrics pubsub.googleapis.com/subscription/pull_ack_message_operation_count for pull subscriptions, and pubsub.googleapis.com/subscription/push_request_count for push subscriptions. Look for elevated expired or webhook_timeout values in the /response_code. This is particularly likely if there are many small messages, since Pub/Sub may batch messages internally and a partially acknowledged batch will be fully redelivered."
      },
      {
        "date": "2024-09-23T23:32:00.000Z",
        "voteCount": 3,
        "content": "D is correct\nAs per google docs- When you do not acknowledge a message before its acknowledgement deadline has expired, Pub/Sub resends the message. As a result, Pub/Sub can send duplicate messages. Use Google Cloud's operations suite to monitor acknowledge operations with the expired response code to detect this condition"
      },
      {
        "date": "2024-09-23T23:32:00.000Z",
        "voteCount": 6,
        "content": "The correct answer is D. Look for the link\nhttps://cloud.google.com/pubsub/docs/faq\n\nWhy are there too many duplicate messages?\nPub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Pub/Sub is retrying the message delivery. This can be observed in the monitoring metrics pubsub.googleapis.com/subscription/pull_ack_message_operation_count for pull subscriptions, and pubsub.googleapis.com/subscription/push_request_count for push subscriptions. Look for elevated expired or webhook_timeout values in the /response_code. This is particularly likely if there are many small messages, since Pub/Sub may batch messages internally and a partially acknowledged batch will be fully redelivered.\n\nAnother possibility is that the subscriber is not acknowledging some messages because the code path processing those specific messages fails, and the Acknowledge call is never made; or the push endpoint never responds or responds with an error."
      },
      {
        "date": "2024-09-23T23:32:00.000Z",
        "voteCount": 2,
        "content": "D as the Cloud Pub/Sub will deliver duplicate messages only if there has been no acknowledgement from the subscriber.\nRefer GCP documentation - Cloud Pub/Sub FAQs - Duplicates:\nhttps://cloud.google.com/pubsub/docs/faq#duplicates\nWhy are there too many duplicate messages?\nCloud Pub/Sub guarantees at-least-once message delivery, which means that occasional duplicates are to be expected. However, a high rate of duplicates may indicate that the client is not acknowledging messages within the configured ack_deadline_seconds, and Cloud Pub/Sub is retrying the message delivery."
      },
      {
        "date": "2024-09-23T23:31:00.000Z",
        "voteCount": 1,
        "content": "If the answer had been B the messages had not reached the HTTP page since the push subscription requires an SSL certificate (https://cloud.google.com/pubsub/docs/push#:~:text=Endpoint%20URL%20(required)), so the answer must be D.\n\nI think it is due to a bad response due to overcharge in the HTTPS page, which is sending a non correct status code (https://cloud.google.com/pubsub/docs/push#:~:text=VPC%20Service%20Controls.-,Receive%20messages,-When%20Pub/Sub)"
      },
      {
        "date": "2024-09-23T23:31:00.000Z",
        "voteCount": 2,
        "content": "In Google Cloud Pub/Sub, when you use a push subscription, messages are delivered to the specified endpoint (in this case, your custom HTTPS endpoint). The acknowledgment deadline is the time given to the endpoint to acknowledge that it has received and processed the message. If the acknowledgment is not received within the deadline, Pub/Sub may consider the message as unacknowledged and may attempt redelivery, which can lead to duplicate messages.\n\nYou should ensure that your custom HTTPS endpoint acknowledges messages within the acknowledgment deadline to prevent duplicate messages from being sent. Additionally, it's essential to handle messages in an idempotent way, so even if duplicates do occur, the action taken by your endpoint doesn't have unintended consequences."
      },
      {
        "date": "2024-09-23T23:31:00.000Z",
        "voteCount": 1,
        "content": "After re-reading the question it seem to me that it is asking for a root cause. It is possible that the most common cause of this symptom is and expired certificate. Once expired duplicates would be received for every message."
      },
      {
        "date": "2024-05-23T20:39:00.000Z",
        "voteCount": 1,
        "content": "Agree with previous explanations regarding validity of D"
      },
      {
        "date": "2024-03-01T02:05:00.000Z",
        "voteCount": 1,
        "content": "D -  If a message has not been acknowledged within its acknowledgement deadline, Dataflow attempts to maintain the lease on the message by repeatedly extending the acknowledgement deadline to prevent redelivery from Pub/Sub. However this is best effort and there is a possibility that messages may be redelivered. This can be monitored using metrics listed here. https://cloud.google.com/blog/products/data-analytics/handling-duplicate-data-in-streaming-pipeline-using-pubsub-dataflow"
      },
      {
        "date": "2024-01-25T06:39:00.000Z",
        "voteCount": 1,
        "content": "D should be the answer. If acknowlegement is not received back to pub/sub , pub/sub may resend meassages."
      },
      {
        "date": "2023-10-06T21:08:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is:\nD. Your custom endpoint is not acknowledging messages within the acknowledgement deadline."
      },
      {
        "date": "2023-10-03T10:41:00.000Z",
        "voteCount": 1,
        "content": "if there were an out of date certificate then nothing would get through. D"
      },
      {
        "date": "2023-07-28T02:19:00.000Z",
        "voteCount": 3,
        "content": "It should be D\nhttps://cloud.google.com/pubsub/docs/troubleshooting#dupes"
      },
      {
        "date": "2023-06-09T05:36:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D, because it's how Pub/Sub works.\nDocumentation here: https://cloud.google.com/pubsub/docs/troubleshooting#dupes"
      },
      {
        "date": "2023-05-01T08:57:00.000Z",
        "voteCount": 2,
        "content": "D for sure"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 21,
    "url": "https://www.examtopics.com/discussions/google/view/16929-exam-professional-data-engineer-topic-1-question-21/",
    "body": "Your company uses a proprietary system to send inventory data every 6 hours to a data ingestion service in the cloud. Transmitted data includes a payload of several fields and the timestamp of the transmission. If there are any concerns about a transmission, the system re-transmits the data. How should you deduplicate the data most efficiency?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign global unique identifiers (GUID) to each data entry.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompute the hash value of each data entry, and compare it with all historical data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore each data entry as the primary key in a separate database and apply an index.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaintain a database table to store the hash value and other metadata for each data entry."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 42,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-04T05:24:00.000Z",
        "voteCount": 66,
        "content": "The best answer is \"A\". \nAnswer \"D\" is not as efficient or error-proof due to two reasons\n1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power.\n2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash. \nif timestamp is used as message creation timestamp than that can also be used as a UUID."
      },
      {
        "date": "2023-10-03T10:45:00.000Z",
        "voteCount": 3,
        "content": "If you add a unique ID aren't you by definition not getting a duplicate record. Honestly I hate  all these answers."
      },
      {
        "date": "2024-05-16T06:04:00.000Z",
        "voteCount": 1,
        "content": "You can add a function or condition that verifies if the global unique id already exists or just do a deduplication later"
      },
      {
        "date": "2020-10-19T19:06:00.000Z",
        "voteCount": 13,
        "content": "If the goal is to ensure at least ONE of each pair of entries is inserted into the db, then how is assigning a GUID to each entry resolving the duplicates?  Keep in mind if the 1st entry fails, then hopefully the 2nd (duplicate) is successful."
      },
      {
        "date": "2021-06-28T05:21:00.000Z",
        "voteCount": 12,
        "content": "A - In D, same message with different timestamp will have different hash, though the message content is the same."
      },
      {
        "date": "2022-01-20T10:48:00.000Z",
        "voteCount": 2,
        "content": "agreed, the key here is \"payload of several fields and the timestamp\""
      },
      {
        "date": "2022-01-20T10:49:00.000Z",
        "voteCount": 2,
        "content": "\"payload of several fields and the timestamp of the transmission\""
      },
      {
        "date": "2022-02-01T02:03:00.000Z",
        "voteCount": 1,
        "content": "Hi Max, I also think that the hash value would be worng because the timestamp is part of payload and is not written that the hash value is generated without the ts; but it also not written if GUID is linked or not with sending. Often this is a point where the answer is vague. Because don't specify if the GUID is related to the data or to the send."
      },
      {
        "date": "2021-07-15T17:57:00.000Z",
        "voteCount": 8,
        "content": "Strong Answer is A - in another question on the gcp sample questions: the correct answer to that particular question was \"You are building a new real-time data warehouse for your company and will use BigQuery streaming inserts. There is no guarantee that data will only be sent in once but you do have a unique ID for each row of data and an event timestamp. You want to ensure that duplicates are not included while interactively querying data. Which query type should you use?\"\nThis means you need a \"uniqueid\" and timestamps to properly dedupe a data."
      },
      {
        "date": "2022-01-23T02:05:00.000Z",
        "voteCount": 1,
        "content": "U need a uniqueid but in this scenario, there is none. So u have to calculate by hashing w/ some of the fields in the dataset. \n\nA is assigning guid in processing side will not solve the issue. Cause u will assign diff. ids..."
      },
      {
        "date": "2023-01-24T07:36:00.000Z",
        "voteCount": 4,
        "content": "Answer - D\nKey statement is  \"Transmitted data includes a payload of several fields and the timestamp of the transmission.\" \n\nSo the timestamp is appended to message while sending, in other words that field is subject to change if message is retransmitted. However, adding a GUID doesn't help much because if message is transmitted twice you will have different GUID for both messages but they will be the same/duplicate data.\n\nYou can simply calculate a hash based on not all data but from a select of columns (with the payload of several fields AND definitely by excluding the timestamp). By doing so, you can assure a different hash for each message."
      },
      {
        "date": "2021-12-15T03:35:00.000Z",
        "voteCount": 5,
        "content": "Answer is D. Using Hash values we can remove duplicate values from a database. Hash values will be same for duplicate data and thus can be easily rejected. Obviously you won't check hash for timestmp.\nD is better thatn B because maintaning a different table will reduce cost for hash computation for all historical data"
      },
      {
        "date": "2023-07-18T04:13:00.000Z",
        "voteCount": 1,
        "content": "Why can't it be A, where the GUID is a hash value? Why do we need to store the hash with the metadata in a separate database to do the deduplication?"
      },
      {
        "date": "2020-03-27T00:07:00.000Z",
        "voteCount": 24,
        "content": "Answer: D\nDescription: Using Hash values we can remove duplicate values from a database. Hashvalues will be same for duplicate data and thus can be easily rejected."
      },
      {
        "date": "2022-04-27T00:17:00.000Z",
        "voteCount": 2,
        "content": "Hash values for same data will be the same, but in this case data contains also the timestamp"
      },
      {
        "date": "2022-12-12T16:13:00.000Z",
        "voteCount": 1,
        "content": "While calculating Hash value we exclude the timestamp."
      },
      {
        "date": "2024-06-16T19:37:00.000Z",
        "voteCount": 1,
        "content": "1. My original vote was 'B'. I chose it over 'D' because option 'D' does not explicitly say anything about how that table will be used for deduplication. In hindsight, explicit usage of table should not be given much weightage so after review and seeing other comments, I thought of 'D' as the correct answer.\n\n2. Now looking more clearly at option 'D' (and 'B' also), it's a little ambiguous of what keys will be used to create the hash. So, if you use the payload PLUS the timestamp, the hash is of no use. This is a little confusing\n\n3. Finally, although I never thought this is the right option, 'A' seems to be the correct option. The GUID is created at Data entry, NOT at the transmission stage. So, the GUID should be representative of the payload only and NOT the timestamp which will make it unique per payload, not per transmission of the same payload. So, in the end, I feel like 'A' is the correct choice."
      },
      {
        "date": "2023-12-21T00:48:00.000Z",
        "voteCount": 1,
        "content": "To deduplicate the data most efficiently, especially in a cloud environment where the data is sent periodically and re-transmissions can occur, the recommended approach would be:\n\nD. Maintain a database table to store the hash value and other metadata for each data entry.\n\nThis approach allows you to quickly check if an incoming data entry is a duplicate by comparing hash values, which is much faster than comparing all fields of a data entry. The metadata, which includes the timestamp and possibly other relevant information, can help resolve any ambiguities that may arise if the hash function ever produces collisions."
      },
      {
        "date": "2023-11-20T09:41:00.000Z",
        "voteCount": 3,
        "content": "B. Compute the hash value of each data entry, and compare it with all historical data.\n\nExplanation:\n\nEfficiency: Hashing is a fast and efficient operation, and comparing hash values is generally quicker than comparing the entire payload or using other methods.\n\nSpace Efficiency: Storing hash values requires less storage space compared to storing entire payloads or using global unique identifiers (GUIDs).\n\nDeduplication: By computing the hash value of each data entry and comparing it with historical data, you can easily identify duplicate transmissions. If the hash value matches an existing one, it indicates that the payload is the same."
      },
      {
        "date": "2023-11-09T00:07:00.000Z",
        "voteCount": 2,
        "content": "I though the answer was A 'cos it's more efficient. But I read the answer with more attention: GUID is given \"at each data entry\". It's not said that GUID was given from publisher. If GUID is given in data entry (subscriber), two equal messages can have different GUID.\n D is not complete 'cos it's not so precise about hash field that are used.\nI'm in doubt on this answer :-("
      },
      {
        "date": "2024-03-18T21:24:00.000Z",
        "voteCount": 1,
        "content": "Data entry means record, it is not an action. that means that each record will have a unique id. so assuming our sink will not accept duplicates based on a key, the GUID will work."
      },
      {
        "date": "2023-11-06T11:19:00.000Z",
        "voteCount": 1,
        "content": "Answer : A\n\"D\" is not as efficient or error-proof due to two reasons\n1. You need to calculate hash at sender as well as at receiver end to do the comparison. Waste of computing power.\n2. Even if we discount the computing power, we should note that the system is sending inventory information. Two messages sent at different can denote same inventory level (and thus have same hash). Adding sender time stamp to hash will defeat the purpose of using hash as now retried messages will have different timestamp and a different hash.\nif timestamp is used as message creation timestamp than that can also be used as a UUID."
      },
      {
        "date": "2023-10-22T05:57:00.000Z",
        "voteCount": 1,
        "content": "D. Maintain a database table to store the hash value and other metadata for each data entry.\n\nStoring a database table with hash values and metadata is an efficient way to deduplicate data. When new data is transmitted, you can calculate the hash of the payload and check whether it already exists in the database. This approach allows for efficient duplicate detection without the need to compare the new data with all historical data. It's a common and scalable technique used to ensure data consistency and avoid processing the same data multiple times.\n\nOptions A (assigning GUIDs to each data entry) and C (storing each data entry as the primary key) can work, but they might be less efficient than using hash values when dealing with a large volume of data. Option B (computing the hash value of each data entry and comparing it with all historical data) can be computationally expensive and slow, especially if there's a significant amount of historical data to compare against. Storing hash values in a table allows for fast and efficient deduplication."
      },
      {
        "date": "2023-08-05T00:09:00.000Z",
        "voteCount": 2,
        "content": "Why not D ? Generate a Hash for payload entry and maintain the value as metadata. Do the validation check on Dataflow..... A GUID will generate 2 different entries for same payload entry, it will not tackle duplication check"
      },
      {
        "date": "2023-08-04T09:51:00.000Z",
        "voteCount": 3,
        "content": "Answer is B - although the time stamp is diff for each transmission - the hash value is computed for the payload, not for the timestamp - which is just an added field for transmission. So, has val remains the same for all transmissions of the same data - which is what we can use for comparision.\n\nSo, much more efficient to just directly compare the hash values with the historical data - to check and remove duplicates - instead of again wasting space storing stuff - in option D"
      },
      {
        "date": "2023-07-26T06:55:00.000Z",
        "voteCount": 2,
        "content": "This question is formulated very badly.\nFrom the way that A is formulated, you would not deduplicate but rather the duplicates would have the same GUID.\nThen we have D, which is storing the information (assuming the hash is created without the timestamp). B is doing it right away. D only alludes to the actual deduplication. But it would be more efficient."
      },
      {
        "date": "2023-05-01T09:00:00.000Z",
        "voteCount": 2,
        "content": "A is best choice. D doesn't make sense."
      },
      {
        "date": "2023-08-25T13:11:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect. how can you find duplicates if you assign a unique id to every record? The answer is either B or D. I first selected B, but reading through the answers D may be better."
      },
      {
        "date": "2023-04-26T14:37:00.000Z",
        "voteCount": 1,
        "content": "you cannot deduplicate data adding a random guid, with guid row is distinct than others"
      },
      {
        "date": "2023-03-17T12:06:00.000Z",
        "voteCount": 4,
        "content": "Hard question.\nIt's a *proprietary* system. Who guarantees we can even add a GUID?\nBut if you can, it's definitely more efficient than calculating hashes (ignoring timestamp)."
      },
      {
        "date": "2023-03-01T02:36:00.000Z",
        "voteCount": 2,
        "content": "As Dg63 wrote."
      },
      {
        "date": "2023-02-23T10:08:00.000Z",
        "voteCount": 1,
        "content": "Just asked Chatgpt, it gave me option D"
      },
      {
        "date": "2023-02-23T05:10:00.000Z",
        "voteCount": 1,
        "content": "Answer B:\nOption A: GUIDs can deduplicate the data but is expensive and good for multiple data processing. \nOption B: Using hash function to authenticate the unique rows, this function can be applied directly in bigquery. \nOption D, is complex and more expensive. \n\n``\n`CREATE TEMP FUNCTION hashValue(input STRING) AS (\n  CAST(FARM_FINGERPRINT(input) AS STRING)\n);\n``"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 22,
    "url": "https://www.examtopics.com/discussions/google/view/16624-exam-professional-data-engineer-topic-1-question-22/",
    "body": "Your company has hired a new data scientist who wants to perform complicated analyses across very large datasets stored in Google Cloud Storage and in a<br>Cassandra cluster on Google Compute Engine. The scientist primarily wants to create labelled data sets for machine learning projects, along with some visualization tasks. She reports that her laptop is not powerful enough to perform her tasks and it is slowing her down. You want to help her perform her tasks.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a local version of Jupiter on the laptop.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the user access to Google Cloud Shell.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHost a visualization tool on a VM on Google Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 50,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-14T19:36:00.000Z",
        "voteCount": 48,
        "content": "Answer should be D."
      },
      {
        "date": "2020-03-27T00:09:00.000Z",
        "voteCount": 14,
        "content": "Answer: D \nDescription: Datalab provides Jupyter for this kind of work"
      },
      {
        "date": "2024-05-23T20:46:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Datalab is a powerful interactive tool for data exploration, analysis, and machine learning."
      },
      {
        "date": "2024-05-08T22:08:00.000Z",
        "voteCount": 3,
        "content": "My answer is Google Cloud Datalab, but since that service has already been discontinued, I question whether a problem like this would actually be asked on the actual exam."
      },
      {
        "date": "2024-01-17T05:05:00.000Z",
        "voteCount": 1,
        "content": "D sounds good for me"
      },
      {
        "date": "2023-12-14T03:22:00.000Z",
        "voteCount": 1,
        "content": "Hash Value for Deduplication: By computing a hash value for each data entry, you create a unique identifier based on the content of the data. This allows you to efficiently identify duplicates, as entries with identical content will have the same hash value.\n\nStoring Hash Value and Metadata: Maintaining a database table that includes the hash value and other relevant metadata (like the timestamp of transmission) allows for quick lookups and comparisons. This way, when new data is received, you can check if an entry with the same hash value already exists.\n Assign global unique identifiers (GUID) to each data entry: While GUIDs are unique, they do not inherently identify duplicate content. Two transmissions of the same data would have different GUIDs."
      },
      {
        "date": "2023-11-21T06:51:00.000Z",
        "voteCount": 1,
        "content": "D sounds good for me"
      },
      {
        "date": "2023-11-07T14:51:00.000Z",
        "voteCount": 1,
        "content": "Agree with D"
      },
      {
        "date": "2023-10-22T05:58:00.000Z",
        "voteCount": 3,
        "content": "D. Deploy Google Cloud Datalab to a virtual machine (VM) on Google Compute Engine.\n\nGoogle Cloud Datalab is a powerful interactive tool for data exploration, analysis, and machine learning. By deploying it to a VM on Google Compute Engine, you can provide her with a robust and scalable environment where she can work with large datasets, create labeled datasets, and perform data analyses efficiently.\n\nOption A (running a local version of Jupyter on her laptop) might not be sufficient for very large datasets, and her laptop's limited power could still be a bottleneck.\n\nOption B (granting access to Google Cloud Shell) is useful for running command-line tools but may not provide the interactive and visual capabilities she needs.\n\nOption C (hosting a visualization tool on a VM on Google Compute Engine) is beneficial for visualization tasks but does not cover the full spectrum of data analysis and machine learning tasks that Google Cloud Datalab offers."
      },
      {
        "date": "2023-09-05T07:32:00.000Z",
        "voteCount": 1,
        "content": "D - as it is a FULL set up, not a shell that is needed..."
      },
      {
        "date": "2023-08-30T20:54:00.000Z",
        "voteCount": 2,
        "content": "Nowadays it should be similar to D, deploy a Vertex workbench"
      },
      {
        "date": "2023-08-18T00:45:00.000Z",
        "voteCount": 1,
        "content": "As per Options , Correct Answer should be D. ie Datalab\nHowever Datalab is no longer used in GCP (Deprecated in Sep2022), It is Vertex AI or Deep Learning VM Images"
      },
      {
        "date": "2023-08-17T22:19:00.000Z",
        "voteCount": 1,
        "content": "I think.\nAnswer is D"
      },
      {
        "date": "2023-07-22T22:56:00.000Z",
        "voteCount": 2,
        "content": "Datalab is deprecated. This question should appear in the exam."
      },
      {
        "date": "2023-07-22T22:56:00.000Z",
        "voteCount": 6,
        "content": "typo- should NOT appear in the exam"
      },
      {
        "date": "2023-10-29T12:44:00.000Z",
        "voteCount": 1,
        "content": "Good point - https://cloud.google.com/datalab/deprecation-notice. Google recommends using Vertex AI Workbench instead"
      },
      {
        "date": "2023-06-09T05:44:00.000Z",
        "voteCount": 2,
        "content": "Should be D, because Cloud shell alone does not provide access to what they need.\nNowadays is Vertex AI, but still, correct answer is D"
      },
      {
        "date": "2023-05-29T16:02:00.000Z",
        "voteCount": 3,
        "content": "Google Cloud Datalab is now Vertex AI. So, letter D make more sense."
      },
      {
        "date": "2023-05-01T09:06:00.000Z",
        "voteCount": 1,
        "content": "B doesn't make sense at all"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 23,
    "url": "https://www.examtopics.com/discussions/google/view/16931-exam-professional-data-engineer-topic-1-question-23/",
    "body": "You are deploying 10,000 new Internet of Things devices to collect temperature data in your warehouses globally. You need to process, store and analyze these very large datasets in real time. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Google Cloud Datastore and then export to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSend the data to Cloud Storage and then spin up an Apache Hadoop cluster as needed in Google Cloud Dataproc whenever analysis is required.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport logs in batch to Google Cloud Storage and then spin up a Google Cloud SQL instance, import the data from Cloud Storage, and run an analysis as needed."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-18T08:38:00.000Z",
        "voteCount": 30,
        "content": "Answer: B"
      },
      {
        "date": "2020-03-18T08:45:00.000Z",
        "voteCount": 9,
        "content": "https://cloud.google.com/blog/products/iot-devices/quick-and-easy-way-set-end-end-iot-solution-google-cloud-platform"
      },
      {
        "date": "2020-03-27T00:11:00.000Z",
        "voteCount": 25,
        "content": "Answer: B\nDescription: Pubsub for realtime, Dataflow for pipeline, Bigquery for analytics"
      },
      {
        "date": "2024-04-14T07:20:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2023-11-21T06:57:00.000Z",
        "voteCount": 1,
        "content": "In short, B is less complex and more recommended other than D"
      },
      {
        "date": "2023-10-22T06:00:00.000Z",
        "voteCount": 1,
        "content": "B. Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery.\n\nHere's why this approach is preferred:\n\nGoogle Cloud Pub/Sub allows for efficient ingestion and real-time data streaming.\nGoogle Cloud Dataflow can process and transform the streaming data in real-time.\nGoogle BigQuery is a fully managed, highly scalable data warehouse that is well-suited for real-time analysis and querying of large datasets."
      },
      {
        "date": "2023-08-08T03:47:00.000Z",
        "voteCount": 1,
        "content": "Obviously B."
      },
      {
        "date": "2023-05-29T16:05:00.000Z",
        "voteCount": 2,
        "content": "PubSub for queue in real time, Dataflow for processing (pipeline) and Bigquery for analyses."
      },
      {
        "date": "2023-03-10T22:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2022-12-12T16:30:00.000Z",
        "voteCount": 1,
        "content": "GCP recommend best practice for streaming data pipeline as option B - pub/sub, dataflow &amp; Bigquery"
      },
      {
        "date": "2022-12-11T09:54:00.000Z",
        "voteCount": 1,
        "content": "B. Send the data to Google Cloud Pub/Sub, stream Cloud Pub/Sub to Google Cloud Dataflow, and store the data in Google BigQuery."
      },
      {
        "date": "2022-11-30T06:48:00.000Z",
        "voteCount": 1,
        "content": "B aris tqve yleebo"
      },
      {
        "date": "2022-10-07T11:35:00.000Z",
        "voteCount": 1,
        "content": "B of course"
      },
      {
        "date": "2022-08-03T04:15:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-06-15T13:58:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\n\nDeafult ETL streaming process: Pub/Sub + Dataflow + Bigquery."
      },
      {
        "date": "2022-06-12T11:09:00.000Z",
        "voteCount": 1,
        "content": "Definitely B"
      },
      {
        "date": "2022-04-15T10:55:00.000Z",
        "voteCount": 1,
        "content": "B is the only option for real time process &amp; analysis"
      },
      {
        "date": "2022-04-04T14:37:00.000Z",
        "voteCount": 1,
        "content": "The most appropriate is B but BQ can't solve Analyzing data in RT."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 24,
    "url": "https://www.examtopics.com/discussions/google/view/16285-exam-professional-data-engineer-topic-1-question-24/",
    "body": "You have spent a few days loading data from comma-separated values (CSV) files into the Google BigQuery table CLICK_STREAM. The column DT stores the epoch time of click events. For convenience, you chose a simple schema where every field is treated as the STRING type. Now, you want to compute web session durations of users who visit your site, and you want to change its data type to the TIMESTAMP. You want to minimize the migration effort without making future queries computationally expensive. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelete the table CLICK_STREAM, and then re-create it such that the column DT is of the TIMESTAMP type. Reload the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a column TS of the TIMESTAMP type to the table CLICK_STREAM, and populate the numeric values from the column TS for each row. Reference the column TS instead of the column DT from now on.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view CLICK_STREAM_V, where strings from the column DT are cast into TIMESTAMP values. Reference the view CLICK_STREAM_V instead of the table CLICK_STREAM from now on.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd two columns to the table CLICK STREAM: TS of the TIMESTAMP type and IS_NEW of the BOOLEAN type. Reload all data in append mode. For each appended row, set the value of IS_NEW to true. For future queries, reference the column TS instead of the column DT, with the WHERE clause ensuring that the value of IS_NEW must be true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConstruct a query to return every row of the table CLICK_STREAM, while using the built-in function to cast strings from the column DT into TIMESTAMP values. Run the query into a destination table NEW_CLICK_STREAM, in which the column TS is the TIMESTAMP type. Reference the table NEW_CLICK_STREAM instead of the table CLICK_STREAM from now on. In the future, new data is loaded into the table NEW_CLICK_STREAM.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-11T11:13:00.000Z",
        "voteCount": 31,
        "content": "\"E\" looks better. For D, the database will be double in size (which increases the storage price) and the user has to spend some more days reloading all the data."
      },
      {
        "date": "2022-12-15T08:43:00.000Z",
        "voteCount": 5,
        "content": "Also D doesn't make sense since we're filtering IS_NEW to true to only consider future data, which disregards our previously loaded data"
      },
      {
        "date": "2022-11-17T02:18:00.000Z",
        "voteCount": 4,
        "content": "\"You want to minimize the migration effort without making future queries computationally expensive.\" Nothing about storage price."
      },
      {
        "date": "2020-03-19T02:34:00.000Z",
        "voteCount": 22,
        "content": "E - more simple and reasonable. Also recommended if not concerned about cost but simplicity. \nhttps://cloud.google.com/bigquery/docs/manually-changing-schemas#changing_a_columns_data_type"
      },
      {
        "date": "2022-01-23T04:34:00.000Z",
        "voteCount": 1,
        "content": "Due to the hard limitations of bq, Not E is the simple answer by the way!"
      },
      {
        "date": "2024-08-08T01:54:00.000Z",
        "voteCount": 1,
        "content": "Create a view no data migration easy to do but computational efficient queries not sure (?)"
      },
      {
        "date": "2024-05-16T22:28:00.000Z",
        "voteCount": 1,
        "content": "E. It recreates the table one time and everything is fixed. Next time you load, load to the new table, you can delete the previous one.\nDefinitely not C. The question says I have to minimize future query effort, which literally means \"don't create a view that converts from STR to TIMESTAMP for every row.\""
      },
      {
        "date": "2024-05-16T06:02:00.000Z",
        "voteCount": 1,
        "content": "Option E is correct.\nThe question is asking to consider the Query cost for future.\nThis is a one time job to fix the Timestamp column. no views were created."
      },
      {
        "date": "2024-04-22T06:44:00.000Z",
        "voteCount": 1,
        "content": "Why Option E is the best choice:\n\nIt modifies the schema with minimal data movement.\nThe original table remains untouched for potential future needs.\nFuture data loads can directly go to the new table with the desired schema.\nQueries referencing the new table (NEW_CLICK_STREAM) will benefit from the optimized data type for timestamp operations."
      },
      {
        "date": "2024-03-12T01:39:00.000Z",
        "voteCount": 1,
        "content": "minimizing effort is key."
      },
      {
        "date": "2023-12-14T03:25:00.000Z",
        "voteCount": 3,
        "content": "A view in Google BigQuery is a virtual table defined by a SQL query. By creating a view that casts the DT column as a TIMESTAMP, you can transform the data format without altering the underlying data in the CLICK_STREAM table. This means you don't have to reload any data, thereby minimizing migration effort."
      },
      {
        "date": "2023-11-21T07:03:00.000Z",
        "voteCount": 1,
        "content": "Good point about the logical views and the desire to reduce costs. I would vote for E"
      },
      {
        "date": "2023-10-24T14:43:00.000Z",
        "voteCount": 1,
        "content": "The best way to minimize the migration effort without making future queries computationally expensive is to create a view and reference it instead of the table. This is because views are materialized when they are queried, so they do not incur any additional overhead.\nSo the answer is (C)."
      },
      {
        "date": "2023-11-09T11:00:00.000Z",
        "voteCount": 1,
        "content": "C doesn't say materialized view, there's a difference with a regular view so it'll be slower and more expensive on every call to that view."
      },
      {
        "date": "2023-10-22T06:02:00.000Z",
        "voteCount": 2,
        "content": "Option \"E\"\nIt avoids the need to delete and recreate the entire CLICK_STREAM table, which is time-consuming and requires reloading all data.\n\nIt allows you to use a simple query to cast the existing DT column as TIMESTAMP values and store the results in a new table, NEW_CLICK_STREAM.\n\nYou can gradually migrate to the new data format, and your future queries will be able to utilize the TIMESTAMP data type for more efficient processing."
      },
      {
        "date": "2023-08-30T20:58:00.000Z",
        "voteCount": 1,
        "content": "Option D duplicates, not a good solution"
      },
      {
        "date": "2023-07-29T22:11:00.000Z",
        "voteCount": 1,
        "content": "E.     E. You can use a special command to change the time on the old cards to the better type \"TIMESTAMP\" and create a new box called \"NEW_CLICK_STREAM.\" From now on, you'll look at the new box whenever you want to know the time. It's like having a new and better box to keep things tidy and organized.\n\nSo, the best way to change the time on the little cards to the better type \"TIMESTAMP\" is option E. It's like using magic to create a new box and making sure everything is still easy to find and work with. It's a clever way to keep track of time and make your website even better!"
      },
      {
        "date": "2023-06-14T02:05:00.000Z",
        "voteCount": 1,
        "content": "they asked to \"change its data type\""
      },
      {
        "date": "2023-05-07T08:46:00.000Z",
        "voteCount": 1,
        "content": "it's E. computationally less expensive than running a view every time"
      },
      {
        "date": "2023-05-01T09:15:00.000Z",
        "voteCount": 1,
        "content": "E is best option"
      },
      {
        "date": "2023-02-19T08:21:00.000Z",
        "voteCount": 5,
        "content": "its C vs E. E is better because C will try to do a cast operation everytime query is run making it computationally expensive."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 25,
    "url": "https://www.examtopics.com/discussions/google/view/16286-exam-professional-data-engineer-topic-1-question-25/",
    "body": "You want to use Google Stackdriver Logging to monitor Google BigQuery usage. You need an instant notification to be sent to your monitoring tool when new data is appended to a certain table using an insert job, but you do not want to receive notifications for other tables. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a call to the Stackdriver API to list all logs, and apply an advanced filter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Stackdriver logging admin interface, and enable a log sink export to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn the Stackdriver logging admin interface, enable a log sink export to Google Cloud Pub/Sub, and subscribe to the topic from your monitoring tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsing the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-11T11:16:00.000Z",
        "voteCount": 49,
        "content": "I would choose D.\nA and B are wrong since don't notify anything to the monitoring tool.\nC has no filter on what will be notified. We want only some tables."
      },
      {
        "date": "2021-11-10T08:21:00.000Z",
        "voteCount": 15,
        "content": "D as the key requirement is to have notification on a particular table. It can be achieved using advanced log filter to filter only the table logs and create a project sink to Cloud Pub/Sub for notification.\nRefer GCP documentation - Advanced Logs Filters: https://cloud.google.com/logging/docs/view/advanced-queries\nA is wrong as advanced filter will help in filtering. However, there is no notification sends.\nB is wrong as it would send all the logs and BigQuery does not provide notifications.\nC is wrong as it would send all the logs."
      },
      {
        "date": "2024-05-16T05:49:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer because: \n- we need to advance filtering to filter the logs for the specific table\n- we need to use monitoring tool for notification."
      },
      {
        "date": "2023-11-21T07:09:00.000Z",
        "voteCount": 2,
        "content": "Good point by MaxNRG about reducing the number of logs sending to Pub/Sub"
      },
      {
        "date": "2023-10-31T02:42:00.000Z",
        "voteCount": 1,
        "content": "Theorically Pub/Sub could filters log to forward the right ones to the correct topic. https://cloud.google.com/pubsub/docs/subscription-message-filter\nSo C could be accepted, but It's better if filtering is performed earlier, so in this case D is more performing"
      },
      {
        "date": "2023-10-22T06:04:00.000Z",
        "voteCount": 2,
        "content": "D. Using the Stackdriver API, create a project sink with an advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.\n\nThis approach allows you to set up a custom log sink with an advanced filter that targets the specific table and then export the log entries to Google Cloud Pub/Sub. Your monitoring tool can subscribe to the Pub/Sub topic, providing you with instant notifications when relevant events occur without being inundated with notifications from other tables.\n\nOptions A and B do not offer the same level of customization and specificity in targeting notifications for a particular table.\n\nOption C is almost correct but doesn't mention the use of an advanced log filter in the sink configuration, which is typically needed to filter the logs to a specific table effectively. Using the Stackdriver API for more advanced configuration is often necessary for fine-grained control over log filtering."
      },
      {
        "date": "2023-09-15T18:00:00.000Z",
        "voteCount": 1,
        "content": "D makes sense."
      },
      {
        "date": "2023-08-08T03:50:00.000Z",
        "voteCount": 1,
        "content": "D should be the answer"
      },
      {
        "date": "2023-07-24T01:07:00.000Z",
        "voteCount": 1,
        "content": "A and B mention nothing about notifications and C would push all data. It's D."
      },
      {
        "date": "2023-03-10T22:09:00.000Z",
        "voteCount": 1,
        "content": "D makes sense"
      },
      {
        "date": "2022-12-28T09:47:00.000Z",
        "voteCount": 2,
        "content": "\"advanced log filter\" is the key word here, all other options push all data ..."
      },
      {
        "date": "2022-11-10T12:13:00.000Z",
        "voteCount": 1,
        "content": "D is the best choice"
      },
      {
        "date": "2022-04-20T05:24:00.000Z",
        "voteCount": 4,
        "content": "Using the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool."
      },
      {
        "date": "2022-04-04T14:46:00.000Z",
        "voteCount": 2,
        "content": "D. Option B doesn't make sense"
      },
      {
        "date": "2022-01-23T09:24:00.000Z",
        "voteCount": 3,
        "content": "correct answer -&gt; Using the Stackdriver API, create a project sink with advanced log filter to export to Pub/Sub, and subscribe to the topic from your monitoring tool.\n\nOption C is also most likely right answer but it doesn't have the filter. We don't want all the tables. We only want one. So the correct answer is D.\n\nLogging sink - Using a Logging sink, you can direct specific log entries to your business logic. In this example, you can use Cloud Audit logs for Compute Engine which use the resource type gce_firewall_rule to filter for the logs of interest. You can also add an event type GCE_OPERATION_DONE to the filter to capture only the completed log events. Here is the Logging filter used to identify the logs. You can try out the query in the Logs Viewer.\n\nPub/Sub topic \u2013 In Pub/Sub, you can create a topic to which to direct the log sink and use the Pub/Sub message to trigger a cloud function. \n\nReference: https://cloud.google.com/blog/products/management-tools/automate-your-response-to-a-cloud-logging-event"
      },
      {
        "date": "2022-01-10T14:55:00.000Z",
        "voteCount": 3,
        "content": "explained by MaxNRG"
      },
      {
        "date": "2022-01-04T04:34:00.000Z",
        "voteCount": 3,
        "content": "as explained by MaxNRG"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 26,
    "url": "https://www.examtopics.com/discussions/google/view/16288-exam-professional-data-engineer-topic-1-question-26/",
    "body": "You are working on a sensitive project involving private user data. You have set up a project on Google Cloud Platform to house your work internally. An external consultant is going to assist with coding a complex transformation in a Google Cloud Dataflow pipeline for your project. How should you maintain users' privacy?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the consultant the Viewer role on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the consultant the Cloud Dataflow Developer role on the project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and allow the consultant to log on with it.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an anonymized sample of the data for the consultant to work with in a different project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 67,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-11T11:18:00.000Z",
        "voteCount": 75,
        "content": "The Answer should be B. The Dataflow developer role will not provide access to the underlying data."
      },
      {
        "date": "2020-03-17T04:53:00.000Z",
        "voteCount": 5,
        "content": "Remember he's an external consultant. You need to create a service account for him, you can't grant before that... I think C is correct in this case."
      },
      {
        "date": "2020-07-09T14:16:00.000Z",
        "voteCount": 35,
        "content": "Service account is between applications and non human entry."
      },
      {
        "date": "2022-01-23T11:16:00.000Z",
        "voteCount": 1,
        "content": "u can enable a service account as user so that externals can use to login"
      },
      {
        "date": "2022-01-23T11:26:00.000Z",
        "voteCount": 3,
        "content": "u can enable a service account as user so that externals can use to login.\n\nbut the problem is service account is about login. not the minimum resources to do the dataflow related staffs. so C is not enough !. \n\nso the answer should be B. \n\nif the question was about \"doing the 1st thing\", then yeah may be creating a service account could be the 1st thing."
      },
      {
        "date": "2022-06-09T19:35:00.000Z",
        "voteCount": 2,
        "content": "The answer should be D.\nYou do not need any DataFlow permission to implement a pipeline. \nIf needed, you can test using the DirectRunner which runs locally:\n\nttps://cloud.google.com/dataflow/docs/concepts/access-control#example_role_assignment"
      },
      {
        "date": "2022-06-09T19:40:00.000Z",
        "voteCount": 2,
        "content": "Sorry I did a wrong copy/paste on the link, I wanted to send:\n\nhttps://cloud.google.com/dataflow/docs/concepts/security-and-permissions#security_and_permissions_for_local_pipelines\n\nhttps://cloud.google.com/dataflow/docs/guides/setting-pipeline-options#LocalExecution"
      },
      {
        "date": "2023-08-01T04:22:00.000Z",
        "voteCount": 3,
        "content": "Im not sure how you expect the consultant to implement a pipeline without having access to any data that is being processed. Having test data is a prerequisite."
      },
      {
        "date": "2023-04-11T02:50:00.000Z",
        "voteCount": 5,
        "content": "and now? For seeing test data, (D) would be right. And the system tells me (C) is the right answer. What shall I click in the exam?"
      },
      {
        "date": "2020-03-27T02:31:00.000Z",
        "voteCount": 18,
        "content": "Answer: B\nDescription: Provides the permissions necessary to execute and manipulate Dataflow jobs."
      },
      {
        "date": "2024-09-21T02:53:00.000Z",
        "voteCount": 1,
        "content": "The answer cannot be B, because B is too retrictive, it can only create and manage dataflow jobs, but cannot view data. I acknowledge that is secure, but no consultant can do the job without seeing representative test data. D is the only one that provides enough to do the job, while still remaining totally private."
      },
      {
        "date": "2024-09-18T13:59:00.000Z",
        "voteCount": 1,
        "content": "D cannot be the answer because the question clearly states the developer has to work in your project. Creating another project is not in scope and is a waste of time. Correct answer is B. Developer role has developer rights only, no view rights."
      },
      {
        "date": "2024-07-31T11:20:00.000Z",
        "voteCount": 2,
        "content": "A.&nbsp;Grant the consultant the Viewer role on the project. \n    This role provides read-only access to all resources in the project, which could expose sensitive data to the consultant, violating privacy principles.\n    \nB.&nbsp;Grant the consultant the Cloud Dataflow Developer role on the project.\n    This role allows the consultant to create and manage Dataflow jobs but does not give them access the underlying data, it is not sufficient, the developer still needs data.\n    \n- C.&nbsp;Create a service account and allow the consultant to log on with it.\n    Allowing the consultant to log on with a service account could grant them access to sensitive data if the service account has broad permissions. This approach does not address the need to limit data exposure.\n    \n- D.&nbsp;Create an anonymized sample of the data for the consultant to work with in a different project.\n- this fits the requrements"
      },
      {
        "date": "2024-03-12T20:49:00.000Z",
        "voteCount": 1,
        "content": "D\nB is a good option to maintain privacy of sensitive data, but he also need some test data to validate the transformation logic right, so creating sample data and allow him to test in another project seems good."
      },
      {
        "date": "2024-02-11T18:27:00.000Z",
        "voteCount": 1,
        "content": "Data flow data privacy rules cant allow the developer to see what the data, He/she just designs the pipelines and the flow as the interdependent tasks for the composer"
      },
      {
        "date": "2023-12-16T02:41:00.000Z",
        "voteCount": 3,
        "content": "B as the Dataflow developer role would help provide the third-party consultant access to create and work on the Dataflow pipeline. However, it does not provide access to view the data, thus maintaining user's privacy.\nRefer GCP documentation - Dataflow roles:\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#roles\nOption A is wrong as it would not allow the consultant to work on the pipeline.\nOption C is wrong as the consultant cannot use the service account to login.\nOption D is wrong as it does not enable collaboration."
      },
      {
        "date": "2023-12-04T12:29:00.000Z",
        "voteCount": 1,
        "content": "C and A will not maintain user's privacy so out. B without data will be enough. D will give a good sample data, maintain privacy and the consultant will help creating the dataflow pipe for the project as requested. so D."
      },
      {
        "date": "2023-11-21T07:22:00.000Z",
        "voteCount": 1,
        "content": "I follow the corresponding logic choosing between B and D:\n\nYes, with the Dataflow Developer role it is possible to execute and manipulate Dataflow jobs, but do we need to execute it? Based on my understanding we only need to ask for help to write it down. Is it possible without having access to test the data? I don't think so. At the same time, we need to perform an anonymization on it. So the answer D is more appropriate for me"
      },
      {
        "date": "2023-11-06T11:53:00.000Z",
        "voteCount": 1,
        "content": "By creating an anonymized sample of the data, you can provide the consultant with a realistic dataset that doesn't contain sensitive or private information. This way, the consultant can work on the project without direct access to sensitive data, reducing privacy risks.\n\nOptions A and B involve granting the consultant access to the project, which may expose sensitive data, even if they have limited permissions.\n\nOption C involves creating a service account, but it doesn't address the need to anonymize the data or provide a separate, safe environment for the consultant to work with.\n\nOption D provides a controlled environment that allows the consultant to work effectively while maintaining data privacy."
      },
      {
        "date": "2023-10-22T06:06:00.000Z",
        "voteCount": 2,
        "content": "D. Create an anonymized sample of the data for the consultant to work within a different project.\n\nBy creating an anonymized sample of the data, you can provide the consultant with a realistic dataset that doesn't contain sensitive or private information. This way, the consultant can work on the project without direct access to sensitive data, reducing privacy risks.\n\nOptions A and B involve granting the consultant access to the project, which may expose sensitive data, even if they have limited permissions.\n\nOption C involves creating a service account, but it doesn't address the need to anonymize the data or provide a separate, safe environment for the consultant to work with.\n\nOption D provides a controlled environment that allows the consultant to work effectively while maintaining data privacy."
      },
      {
        "date": "2023-10-07T02:11:00.000Z",
        "voteCount": 1,
        "content": "D. Creating an anonymized sample of the data for the consultant to work with in a different project is the safest option. This way, the consultant can develop and test the transformation logic without accessing the real, sensitive data."
      },
      {
        "date": "2023-10-31T02:50:00.000Z",
        "voteCount": 1,
        "content": "The question says \" with coding a complex transformation\", so I don't' think that a sample of data is enough. I thiknk that the most suitable way is C, 'cos with a service account you can handle access fine-grained"
      },
      {
        "date": "2023-09-20T04:31:00.000Z",
        "voteCount": 2,
        "content": "I think C would be correct, as the question says external consultants want to do some work and how we can maintain the 'external consultant' user privacy. Question didn't mention about the company user data or customer information."
      },
      {
        "date": "2023-09-04T14:44:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2023-08-17T21:55:00.000Z",
        "voteCount": 1,
        "content": "Dataflow Developer \n(roles/dataflow.developer)\n\nProvides the permissions necessary to execute and manipulate Dataflow jobs."
      },
      {
        "date": "2023-07-12T10:12:00.000Z",
        "voteCount": 2,
        "content": "Unfortunately it's the Service Account answer: \"The developer who creates and examines jobs needs the roles/iam.serviceAccountUser role.\" - https://cloud.google.com/dataflow/docs/concepts/access-control#example"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 27,
    "url": "https://www.examtopics.com/discussions/google/view/16969-exam-professional-data-engineer-topic-1-question-27/",
    "body": "You are building a model to predict whether or not it will rain on a given day. You have thousands of input features and want to see if you can improve training speed by removing some features while having a minimum effect on model accuracy. What can you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEliminate features that are highly correlated to the output labels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCombine highly co-dependent features into one representative feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstead of feeding in each feature individually, average their values in batches of 3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the features that have null values for more than 50% of the training records."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T02:41:00.000Z",
        "voteCount": 33,
        "content": "Answer: B\nDescription: Best Choice out of given options."
      },
      {
        "date": "2020-03-19T02:54:00.000Z",
        "voteCount": 17,
        "content": "Should be B"
      },
      {
        "date": "2024-05-16T22:43:00.000Z",
        "voteCount": 1,
        "content": "B: combine the dependent features. It is more like PCA (principal component analysis). \nD: could be the answer, but what if that feature is very important, or how often do you get a feature with more than 50% NULL values?"
      },
      {
        "date": "2023-11-21T07:25:00.000Z",
        "voteCount": 2,
        "content": "I am not into ML, to be honest, so I will rely on community opinion and choose B"
      },
      {
        "date": "2023-10-22T06:09:00.000Z",
        "voteCount": 3,
        "content": "B. Combine highly co-dependent features into one representative feature.\n\nCombining highly correlated features into a single representative feature can reduce the dimensionality of your dataset, making the training process faster while preserving relevant information. This approach often helps eliminate redundancy in the input data.\n\nOption A (eliminating features that are highly correlated to the output labels) can be counterproductive, as you want to maintain features that are informative for your prediction task. Removing features that are correlated with the output may reduce model accuracy.\n\nOption C (averaging feature values in batches of 3) is not a common technique for reducing dimensionality, and it could lead to loss of important information.\n\nOption D (removing features with null values for more than 50% of training records) can help reduce the dimensionality and may be useful if you have a large number of features with missing data, but it may not necessarily address co-dependency among features."
      },
      {
        "date": "2023-09-15T18:11:00.000Z",
        "voteCount": 1,
        "content": "B. Combine highly co-dependent features into one representative feature.\nThis is the best choice."
      },
      {
        "date": "2023-05-09T00:05:00.000Z",
        "voteCount": 1,
        "content": "\"D\" is wrong, and very dangerous. For instance, it might represent modern measurements only installed in &lt;50% of weather stations, but very very precise and valuable. \n\nNulls are not a problem for models, out-of-the-box or with transformations models can handle nulls just fine."
      },
      {
        "date": "2023-02-20T22:27:00.000Z",
        "voteCount": 1,
        "content": "wrong question. there are two answers B, D"
      },
      {
        "date": "2023-02-20T22:38:00.000Z",
        "voteCount": 2,
        "content": "B. Combine highly co-dependent features into one representative feature.\n-&gt; Explainable feature should be dependent from each other feature. expecially not deep leanring. so, in this case normally eliminated or combined\n\nD. Remove the features that have null values for more than 50% of the training records.\n-&gt; it's too large null data in the feature. normally the feature should be removed because it's too hard to fill up replacing data"
      },
      {
        "date": "2023-01-04T13:57:00.000Z",
        "voteCount": 1,
        "content": "Answer is Combine highly co-dependent features into one representative feature.\n\nA: correlated to output means that feature can contribute a lot to the model. so not a good idea.\nC: you need to run with almost same number, but you will iterate twice, once for averaging and second time to feed the averaged value.\nD: removing features even if it 50% nulls is not good idea, unless you prove that it is not at all correlated to output. But this is nowhere so can remove."
      },
      {
        "date": "2023-02-20T22:17:00.000Z",
        "voteCount": 1,
        "content": "But, if there are null datas more than 50% then, it should be eliminated because there are two ways to train the model. first, remove records containing having null but in this case there are too many records should be removed and second, replace null to other data but in this case cause of it's too large data having null  then It's literally hard to replace. so normally the feature having too many null data should be removed. So, there are two answer in this question B, D I think"
      },
      {
        "date": "2022-11-21T01:00:00.000Z",
        "voteCount": 2,
        "content": "I have a doubt, instead of combining highly corelated features why cant we remove corelated features which may give much more simplified dataset?"
      },
      {
        "date": "2022-06-15T14:21:00.000Z",
        "voteCount": 2,
        "content": "Answer: B\n\nData that is co-dependent is high corelated is some kind of reduldant information in some cases. If the features x1, x2 and x3 are x2 = x1 + 1 and x3 = 2*x1, for example, x2 and x3 are reduldant because can be explained with x1 feature, so can be excluded of the the model. Other option is to group this features. There is a lot of ways to resolve, but the main ideia is to use data engineer in co-depedent features to reduce the number of features in the model"
      },
      {
        "date": "2022-06-07T06:37:00.000Z",
        "voteCount": 1,
        "content": "This method is called Data Engineering, that you combine two or more values to get a custom info, this will avoid that the model read an extra column on the training and probably increase it's accuracy."
      },
      {
        "date": "2022-05-13T06:23:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-04-20T05:31:00.000Z",
        "voteCount": 2,
        "content": "Co-dependent -&gt; correlated -&gt; correlated info = already present info in other variable."
      },
      {
        "date": "2022-02-10T05:53:00.000Z",
        "voteCount": 6,
        "content": "Trying to find a reason why it is B and not D, found this and it seems the answer is D.\nhttps://cloud.google.com/architecture/data-preprocessing-for-ml-with-tf-transform-pt1\nFeature selection. Selecting a subset of the input features for training the model, and ignoring the irrelevant or redundant ones, using filter or wrapper methods. This can also involve simply dropping features if the features are missing a large number of values."
      },
      {
        "date": "2022-05-23T20:12:00.000Z",
        "voteCount": 1,
        "content": "Yes. But nearly 50% of the non-null data still seems to be a lot to ignore."
      },
      {
        "date": "2022-01-22T03:56:00.000Z",
        "voteCount": 4,
        "content": "B\nnull values can have many meanings and need different approach to handle, otherwise it causes inaccurate model, so not D"
      },
      {
        "date": "2022-01-16T12:24:00.000Z",
        "voteCount": 1,
        "content": "For me the best option is B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 28,
    "url": "https://www.examtopics.com/discussions/google/view/79388-exam-professional-data-engineer-topic-1-question-28/",
    "body": "Your company is performing data preprocessing for a learning algorithm in Google Cloud Dataflow. Numerous data logs are being are being generated during this step, and the team wants to analyze them. Due to the dynamic nature of the campaign, the data is growing exponentially every hour.<br>The data scientists have written the following code to read the data for a new key features in the logs.<br><img src=\"/assets/media/exam-media/04341/0001600001.png\" class=\"in-exam-image\"><br>You want to improve the performance of this data read. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify the TableReference object in the code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse .fromQuery operation to read specific fields from the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse of both the Google BigQuery TableSchema and TableFieldSchema classes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall a transform that returns TableRow objects, where each element in the PCollection represents a single row in the table."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T05:45:00.000Z",
        "voteCount": 12,
        "content": "B BigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs)."
      },
      {
        "date": "2022-10-12T06:21:00.000Z",
        "voteCount": 6,
        "content": "reading only relevant cols"
      },
      {
        "date": "2023-12-16T02:46:00.000Z",
        "voteCount": 1,
        "content": "B as BigQueryIO.read.from() directly reads the whole table from BigQuery. \nThis function exports the whole table to temporary files in Google Cloud Storage, where it will later be read from. \nThis requires almost no computation, as it only performs an export job, and later Dataflow reads from GCS (not from BigQuery).\nBigQueryIO.read.fromQuery() executes a query and then reads the results received after the query execution. Therefore, this function is more time-consuming, given that it requires that a query is first executed (which will incur in the corresponding economic and computational costs).\nhttps://stackoverflow.com/questions/54413681/bigqueryio-read-vs-fromquery"
      },
      {
        "date": "2023-11-21T07:31:00.000Z",
        "voteCount": 1,
        "content": "B works for me"
      },
      {
        "date": "2023-11-20T06:33:00.000Z",
        "voteCount": 1,
        "content": "We are trying to optimize reading each row is not optimal, we want columns"
      },
      {
        "date": "2023-10-22T06:13:00.000Z",
        "voteCount": 5,
        "content": "B. Use the .fromQuery operation to read specific fields from the table.\n\nUsing the .fromQuery operation allows you to specify the exact fields you need to read from the table, which can significantly improve performance by reducing the amount of data that needs to be processed. This is particularly important when dealing with large and growing datasets.\n\nOption A (specifying the TableReference object) provides information about the table but doesn't inherently improve the performance of reading specific fields.\n\nOption C (using Google BigQuery TableSchema and TableFieldSchema classes) is related to specifying the schema of the data but doesn't directly address improving the performance of reading specific fields.\n\nOption D (calling a transform that returns TableRow objects) is more about how the data is processed after it's read, not how it's initially read from BigQuery."
      },
      {
        "date": "2023-09-21T11:27:00.000Z",
        "voteCount": 2,
        "content": "When I have a different answer then the \"Correct Answer\", I run it through AI and it keeps saying ExamTopics is wrong. Is there any way to know if I am going to pass or fail this exam?"
      },
      {
        "date": "2023-11-21T07:27:00.000Z",
        "voteCount": 1,
        "content": "AI is just a LLM model, not a silver bullet at all"
      },
      {
        "date": "2023-09-15T18:23:00.000Z",
        "voteCount": 1,
        "content": "Since the requirement is to read the data for a *new* key features in the logs, it makes sense to select limited columns, which are required rather than using .from() method which exports the entire BigQuery table.\nB makes sense here."
      },
      {
        "date": "2023-09-05T11:46:00.000Z",
        "voteCount": 1,
        "content": "SHOULD be B.\nNot quite sure how D is the correct answer (Red herring....?) when you want to improve the query, which is .fromQuery and NOT transform and PCollection...."
      },
      {
        "date": "2023-07-29T19:26:00.000Z",
        "voteCount": 1,
        "content": "Answer Is D, imagine that you dont have permission on BQ AND you cant see the table info or anything else about the table you only ar\u00e9 working whit dataflow the only way Is transform the data using apache beam"
      },
      {
        "date": "2023-07-24T01:57:00.000Z",
        "voteCount": 1,
        "content": "I have seen people explain why B is not right because it doesn't optimize performance but only cost, which is not true, or because fromQuery is still not performant.\n\nI think it's B because no other option is more performant, even if you claim it's not good. \n\nAs for option D, the transform given by the description is already a transform that provides as output a PCollection of TableRow objects. So how would that be any different?\n\nhttps://beam.apache.org/releases/javadoc/2.1.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html"
      },
      {
        "date": "2023-07-17T07:34:00.000Z",
        "voteCount": 1,
        "content": "Why should it be D? \n\"fromQuery()\" allows us to read only the columns we want, I see no point in using a Transform for each row of a \"SELECT *\", which, moreover, is a bad BQ Practice."
      },
      {
        "date": "2023-05-23T07:42:00.000Z",
        "voteCount": 1,
        "content": "Correct answer should be B. BigQuery is a columnar storage, so reducing the number of fields being selected should improve performance."
      },
      {
        "date": "2023-01-17T02:19:00.000Z",
        "voteCount": 1,
        "content": "Does BigQuery have a pCollections? I thought it's unique to Apache Beam i.e. Cloud Dataflow"
      },
      {
        "date": "2022-11-30T07:15:00.000Z",
        "voteCount": 4,
        "content": "Guys, how is B the answer? Like all the justifications given here, BigQueryIO.read.fromQuery() is time consuming and the question asked for a better performance solution."
      },
      {
        "date": "2023-01-29T06:36:00.000Z",
        "voteCount": 2,
        "content": "That part is the docs trying to explain the side effects of using it, however, the part that is important to us is the fact that it reads from a query. \"Read\" reads the whole table. If we specify a query we can say select col1 only, which makes it all more efficient."
      },
      {
        "date": "2022-10-08T05:00:00.000Z",
        "voteCount": 6,
        "content": "Since we want to be able to analyze data from a new ML feature (column) we only need to check values from that column. By doing a fromQuery(SELECT featueColum FROM table)\nwe are optimizing costs and performance since we are not checking all columns.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-costs#avoid_select_"
      },
      {
        "date": "2022-10-08T05:01:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      },
      {
        "date": "2023-05-17T06:54:00.000Z",
        "voteCount": 3,
        "content": "According to Chat GPT, it is also B\nIn general, if your \"primary goal is to reduce the amount of data read and transferred\", and the downstream processing mainly focuses on a subset of fields, using .fromQuery to select specific fields would be a good choice. \n\nOn the other hand, if you need to simplify downstream processing and optimize resource utilization, transforming data into TableRow objects might be more suitable."
      },
      {
        "date": "2022-10-07T17:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is D, apparently."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 29,
    "url": "https://www.examtopics.com/discussions/google/view/17029-exam-professional-data-engineer-topic-1-question-29/",
    "body": "Your company is streaming real-time sensor data from their factory floor into Bigtable and they have noticed extremely poor performance. How should the row key be redesigned to improve Bigtable performance on queries that populate real-time dashboards?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a row key of the form &lt;timestamp&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a row key of the form &lt;sensorid&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a row key of the form &lt;timestamp&gt;#&lt;sensorid&gt;.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a row key of the form &gt;#&lt;sensorid&gt;#&lt;timestamp&gt;.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T02:50:00.000Z",
        "voteCount": 33,
        "content": "Description: Best practices of bigtable states that rowkey should not be only timestamp or have timestamp at starting. It\u2019s better to have sensorid and timestamp as rowkey"
      },
      {
        "date": "2020-03-20T00:08:00.000Z",
        "voteCount": 19,
        "content": "Answer D"
      },
      {
        "date": "2023-11-21T07:35:00.000Z",
        "voteCount": 2,
        "content": "Looks like D is the best option\nReference: https://cloud.google.com/bigtable/docs/schema-design#time-based"
      },
      {
        "date": "2024-05-16T23:04:00.000Z",
        "voteCount": 1,
        "content": "Thank you that is right."
      },
      {
        "date": "2023-10-22T06:14:00.000Z",
        "voteCount": 2,
        "content": "D. Use a row key of the form &lt;sensorid&gt;#&lt;timestamp&gt;.\n\nBy using the sensor ID as the prefix in the row key, you can achieve better distribution of data across Bigtable tablets. This can help balance the workload and prevent hotspots in the table. Additionally, placing the timestamp after the sensor ID allows you to perform range scans for a specific sensor and retrieve data efficiently within a time frame.\n\nOption C (using a row key of the form &lt;timestamp&gt;#&lt;sensorid&gt;) can work for some use cases but may not be as efficient for range scans when you want to retrieve data for a specific sensor within a time range.\n\nOption A (using a row key of the form &lt;timestamp&gt;) may lead to hotspots and inefficient range scans because it doesn't consider sensor IDs.\n\nOption B (using a row key of the form &lt;sensorid&gt;) is not optimal because it doesn't allow for efficient time-based filtering and could lead to uneven data distribution in Bigtable."
      },
      {
        "date": "2023-01-04T14:00:00.000Z",
        "voteCount": 1,
        "content": "D is right\nBest practices of bigtable states that rowkey should not be only timestamp or have timestamp at starting. It\u2019s better to have sensorid and timestamp as rowkey.\n\nReference:\nhttps://cloud.google.com/bigtable/docs/schema-design"
      },
      {
        "date": "2022-12-13T05:54:00.000Z",
        "voteCount": 5,
        "content": "#&lt;sensorid&gt;#&lt;timestamp&gt;    ------&gt; low cardinality # high cardinality \nThis is current Bigtable Best Practice (to avoid Hotspots on the inserts)"
      },
      {
        "date": "2022-10-08T05:15:00.000Z",
        "voteCount": 1,
        "content": "Discard:\nA -&gt; timestamp unique id could not be unique in the case that sensors transmit data at the same time.\nB -&gt; sensorId repeated id for messages coming from the same sensor\nC -&gt; a bad performance choice\n\nD -&gt; BEST CHOICE. Each time BigTable looks for data in a table it does a scan and sort operations. By starting each unique id by sensorId it will make it easier to group and sort data since it has the lowest cardinality\nhttps://cloud.google.com/bigtable/docs/schema-design#general-concepts"
      },
      {
        "date": "2022-09-07T23:29:00.000Z",
        "voteCount": 2,
        "content": "as I look at https://cloud.google.com/bigtable/docs/schema-design#row-keys\n asia#india#bangalore\n  asia#india#mumbai\nthey didn't have # ahead of this first value.\nasia#india#bangalore  OR #asia#india#bangalore\nAre both valid?"
      },
      {
        "date": "2022-08-20T09:24:00.000Z",
        "voteCount": 1,
        "content": "ANSWER: D"
      },
      {
        "date": "2022-06-16T03:36:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-01-23T15:52:00.000Z",
        "voteCount": 9,
        "content": "A. Use a row key of the form &lt;timestamp&gt;. \n---&gt; Incorrect, because google says don't use a timestamp by itself or at the beginning of a row key.\nB. Use a row key of the form &lt;sensorid&gt;.\n---&gt;Incorrect, because google says Include a timestamp as part of your row key.\nC. Use a row key of the form &lt;timestamp&gt;#&lt;sensorid&gt;.\n---&gt; Incorrect, because google says don't use a timestamp by itself or at the beginning of a row key.\nD. Use a row key of the form &gt;#&lt;sensorid&gt;#&lt;timestamp&gt;.\n---&gt; Correct answer, because of option A,B,C reasons.\n       - Timestamp isn't by itself, neither at the beginning.\n       - Timestamp is included.\n\nReference: https://cloud.google.com/bigtable/docs/schema-design#row-keys"
      },
      {
        "date": "2021-10-12T09:57:00.000Z",
        "voteCount": 2,
        "content": "Ans: D"
      },
      {
        "date": "2021-06-26T10:23:00.000Z",
        "voteCount": 9,
        "content": "Vote for 'D'  - Store multiple delimited values in each row key.  (But avoid starting with Timestamp)\n\n\"Row keys to avoid\"\nhttps://cloud.google.com/bigtable/docs/schema-design"
      },
      {
        "date": "2021-07-08T05:30:00.000Z",
        "voteCount": 7,
        "content": "A is not correct because this will cause most writes to be pushed to a single node (known as hotspotting)\nB is not correct because this will not allow for multiple readings from the same sensor as new readings will overwrite old ones.\nC is not correct because this will cause most writes to be pushed to a single node (known as hotspotting)\nD is correct because it will allow for retrieval of data based on both sensor id and timestamp but without causing hotspotting."
      },
      {
        "date": "2021-02-06T11:05:00.000Z",
        "voteCount": 2,
        "content": "Correct D"
      },
      {
        "date": "2020-12-14T00:37:00.000Z",
        "voteCount": 3,
        "content": "Should be D\nReverse of timestamp even better but no options for that.\nAlso changing sensor ID if they are in sequential to hash or changing data to bits even better.\nIdea is not to use timestamp or sequential ID as first key."
      },
      {
        "date": "2022-01-24T08:51:00.000Z",
        "voteCount": 1,
        "content": "reverse TS or hashing is not always first choice or better. never."
      },
      {
        "date": "2020-11-16T03:50:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. \nRefer to the link https://cloud.google.com/bigtable/docs/schema-design for Big table schema design.\n\nC is not the right answer becuase\nTimestamps\nIf you often need to retrieve data based on the time when it was recorded, it's a good idea to include a timestamp as part of your row key. Using the timestamp by itself as the row key is not recommended, as most writes would be pushed onto a single node. For the same reason, avoid placing a timestamp at the start of the row key.\n\nFor example, your application might need to record performance-related data, such as CPU and memory usage, once per second for a large number of machines. Your row key for this data could combine an identifier for the machine with a timestamp for the data (for example, machine_4223421#1425330757685)."
      },
      {
        "date": "2020-11-01T08:42:00.000Z",
        "voteCount": 2,
        "content": "answer would be D to avoid hotspoting.."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 30,
    "url": "https://www.examtopics.com/discussions/google/view/16655-exam-professional-data-engineer-topic-1-question-30/",
    "body": "Your company's customer and order databases are often under heavy load. This makes performing analytics against them difficult without harming operations.<br>The databases are in a MySQL cluster, with nightly backups taken using mysqldump. You want to perform analytics with minimal impact on operations. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a node to the MySQL cluster and build an OLAP cube there.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an ETL tool to load the data from MySQL into Google BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConnect an on-premises Apache Hadoop cluster to MySQL and perform ETL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 45,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-07T19:14:00.000Z",
        "voteCount": 113,
        "content": "It is a GOOGLE exam. The answer won't be on-premise or OLAP cubes even if it is the easiest. The answer is B"
      },
      {
        "date": "2022-01-24T08:56:00.000Z",
        "voteCount": 9,
        "content": "choose dataproc over hadoop cluster\nchose bigquery over all..\n\nthere is no special customer requirement that gonna drive us to hadoop or dataproc."
      },
      {
        "date": "2023-01-24T09:14:00.000Z",
        "voteCount": 2,
        "content": "Answer - B\nmysql dump: This utility creates a logical backup and a flat file containing the SQL statements that can be run again to bring back the database to the state when this file was created. So this file can easily be processed by an ETL tool and loaded into BQ."
      },
      {
        "date": "2023-04-11T03:05:00.000Z",
        "voteCount": 2,
        "content": "So, you are saying that B takes the backup data from the nightly dumps? How can you be sure?"
      },
      {
        "date": "2023-05-17T07:07:00.000Z",
        "voteCount": 1,
        "content": "I agree that B sounds like running ETL directly on the database. It doesn't say anything explicitly about using dumps.\n\nHowever, by leveraging the Dataproc JDBC Connector, one can perform various operations such as querying, joining, filtering, and aggregating data from your SQL databases within your Dataproc jobs. This can be particularly useful when you want to combine data from multiple sources or perform complex data transformations before processing the data further.\n\nSo with D, you can run your analysis from a separate cloud-sql instance created from the dump and without affecting the production database."
      },
      {
        "date": "2024-02-20T18:42:00.000Z",
        "voteCount": 1,
        "content": "That\u2019s so true! This should be the first logic for elimination"
      },
      {
        "date": "2020-03-27T03:02:00.000Z",
        "voteCount": 39,
        "content": "Answer: D\nDescription: Easy and it won\u2019t affect processing"
      },
      {
        "date": "2020-06-26T06:18:00.000Z",
        "voteCount": 5,
        "content": "Agreed- Option[D] is most appropriate in this scenario"
      },
      {
        "date": "2021-09-13T03:15:00.000Z",
        "voteCount": 6,
        "content": "So I vote for B"
      },
      {
        "date": "2021-01-03T04:45:00.000Z",
        "voteCount": 15,
        "content": "I think it is B and not D:\n1) There are no info regarding date freshness required for analytics. So nightly backup might be not enough as a source because it will only provide info one tie a day.\n2) Dataproc is recommended as easiest way for migration of hadoop processes. SO no reason to use Dataproc for designing a new analytics processes.\n3) The solution is really very limited if you will extend it in the future and add new data sources or create new aggregate tables. Where they should be created?\n4) There is no info on which version is on prem MySQL database (I am not an expert in MySql)  but I can imagine there might be compartibility issue for backup / restore between different releases"
      },
      {
        "date": "2021-09-08T06:45:00.000Z",
        "voteCount": 3,
        "content": "I also think the answer is D, because on B it is not written that the source is the backup but (directly) MYSQL. So wit this solution we add requests on MySQL and so, mpacting the operations-"
      },
      {
        "date": "2021-01-23T17:14:00.000Z",
        "voteCount": 1,
        "content": "but how about the impact on operation? D seems better match."
      },
      {
        "date": "2021-10-09T06:56:00.000Z",
        "voteCount": 1,
        "content": "\" Dataproc makes open source data and analytics processing fast, easy, and more secure in the cloud \". Please refer this google link.\nhttps://cloud.google.com/blog/products/data-analytics/genomics-data-analytics-with-cloud-pt2"
      },
      {
        "date": "2021-10-30T02:10:00.000Z",
        "voteCount": 1,
        "content": "The link titles \"Genomics analysis with Hail, BIGQUERY, and Data Proc\", the solution describes the use of Bigquery to do analytics"
      },
      {
        "date": "2021-09-13T03:14:00.000Z",
        "voteCount": 10,
        "content": "Google Cloud Dataproc is not an analytic tool"
      },
      {
        "date": "2024-08-11T22:43:00.000Z",
        "voteCount": 1,
        "content": "Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page."
      },
      {
        "date": "2024-08-16T04:12:00.000Z",
        "voteCount": 1,
        "content": "How you accessed questions after question number 70 it is asking for pro subscription ?"
      },
      {
        "date": "2024-05-16T23:08:00.000Z",
        "voteCount": 1,
        "content": "Answer B:\nI don't know why people are choosing D. It is two steps, first cloudsql and then dataproc, a lot of overhead. BigQuery is just perfect fit."
      },
      {
        "date": "2024-03-06T00:16:00.000Z",
        "voteCount": 1,
        "content": "This won\u2019t affect processing"
      },
      {
        "date": "2023-11-23T02:52:00.000Z",
        "voteCount": 2,
        "content": "Based on these considerations, option B is likely the best approach. By using an ETL tool to load data from MySQL into Google BigQuery, you're leveraging BigQuery's strengths in handling large-scale analytics workloads without impacting the performance of the operational databases. This option provides a clear separation of operational and analytical workloads and takes advantage of BigQuery's fast analytics capabilities."
      },
      {
        "date": "2023-11-21T07:37:00.000Z",
        "voteCount": 1,
        "content": "Do not spend much time on in - just B"
      },
      {
        "date": "2023-11-06T12:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is B - Use an ETL tool to load the data from MySQL into Google BigQuery.\n\n* Google BigQuery is a serverless, highly scalable data warehouse that can handle large-scale analytics workloads without impacting your MySQL cluster's performance.\n* Using an ETL (Extract, Transform, Load) tool to transfer data from MySQL to BigQuery allows you to maintain a separate analytics environment, ensuring that your operational database remains unaffected.\n\nOption C (connecting an on-premises Apache Hadoop cluster to MySQL and performing ETL) introduces complexity and may not be as scalable as a cloud-based solution.\n\nOption D (mounting backups to Google Cloud SQL and processing the data using Google Cloud Dataproc) could be an option for historical data analysis but might not be the best choice for real-time analytics while the MySQL cluster is under heavy load. Additionally, the backups need to be restored and processed, which might introduce some delay."
      },
      {
        "date": "2023-10-24T15:02:00.000Z",
        "voteCount": 2,
        "content": "It's GOOGLE exam where choosing the GCP service shall be first preference.\nNow notice the problem statement \"perform analytics with minimal impact on operations\"\nBigQuery is right option for analytic as well as Cloud SQL does provide easy export to GCS where we can query from BigQuery without loading into BQ to save storage cost."
      },
      {
        "date": "2023-10-22T06:17:00.000Z",
        "voteCount": 3,
        "content": "B. Use an ETL tool to load the data from MySQL into Google BigQuery.\n* Google BigQuery is a serverless, highly scalable data warehouse that can handle large-scale analytics workloads without impacting your MySQL cluster's performance.\n* Using an ETL (Extract, Transform, Load) tool to transfer data from MySQL to BigQuery allows you to maintain a separate analytics environment, ensuring that your operational database remains unaffected.\n\nOption C (connecting an on-premises Apache Hadoop cluster to MySQL and performing ETL) introduces complexity and may not be as scalable as a cloud-based solution.\n\nOption D (mounting backups to Google Cloud SQL and processing the data using Google Cloud Dataproc) could be an option for historical data analysis but might not be the best choice for real-time analytics while the MySQL cluster is under heavy load. Additionally, the backups need to be restored and processed, which might introduce some delay."
      },
      {
        "date": "2023-10-20T19:33:00.000Z",
        "voteCount": 1,
        "content": "The question clearly says there is load on MYSQL already so doing analytics on it is bad idea. Its bad to run analytics on MYSQL but still a better option to run etl with it to load it to BigQuery."
      },
      {
        "date": "2023-10-07T02:23:00.000Z",
        "voteCount": 2,
        "content": "B. Use an ETL tool to load the data from MySQL into Google BigQuery. This way, analytics is entirely separated from the operational database, and BigQuery is well-suited for large-scale analytics."
      },
      {
        "date": "2023-10-04T06:15:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is to build a read replica :-) but since we can't do that then migrating to BigQuery will have to suffice."
      },
      {
        "date": "2023-10-10T08:01:00.000Z",
        "voteCount": 1,
        "content": "thanks! :3"
      },
      {
        "date": "2023-09-27T11:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc.\n\nA: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question)."
      },
      {
        "date": "2023-09-21T11:31:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't the correct answer be to create read replica and do analytics off of that?"
      },
      {
        "date": "2023-09-06T00:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is Mount the backups to Google Cloud SQL, and then process the data using Google Cloud Dataproc. \n\nA: OLAP on MySQL performs poorly.\nB: ETL consumes lot of MySQL resources, to read the data, as per question MySQL is under pressure already.\nC: Similar to B.\nD: By mounting backup can avoid reading from MySQL, data freshness is not an issue as per the question (and is not mention in the question). \n\nReference:\nhttps://cloud.google.com/blog/products/data-analytics/genomics-data-analytics-with-cloud-pt2"
      },
      {
        "date": "2023-07-28T03:12:00.000Z",
        "voteCount": 3,
        "content": "Why overcomplicate things by using Dataproc? I choose B"
      },
      {
        "date": "2023-07-28T03:21:00.000Z",
        "voteCount": 1,
        "content": "I meant hadoop cluster"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 31,
    "url": "https://www.examtopics.com/discussions/google/view/17051-exam-professional-data-engineer-topic-1-question-31/",
    "body": "You have Google Cloud Dataflow streaming pipeline running with a Google Cloud Pub/Sub subscription as the source. You need to make an update to the code that will make the new Cloud Dataflow pipeline incompatible with the current version. You do not want to lose any data when making this update. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the current pipeline and use the drain flag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the current pipeline and provide the transform mapping JSON object.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline that has the same Cloud Pub/Sub subscription and cancel the old pipeline.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-26T00:38:00.000Z",
        "voteCount": 79,
        "content": "Correct Option : A\nExplanation:-This option is correct as the key requirement is not to lose\nthe data, the Dataflow pipeline can be stopped using the Drain option.\nDrain options would cause Dataflow to stop any new processing, but would\nalso allow the existing processing to complete"
      },
      {
        "date": "2021-09-02T07:30:00.000Z",
        "voteCount": 7,
        "content": "C and D are incorrect because canceling the old pipeline can cause data loss \nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline  \nA is incorrect because updating pipeline does not include any drain flag\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline"
      },
      {
        "date": "2022-01-25T07:48:00.000Z",
        "voteCount": 2,
        "content": "drain is in the guide ...stopping-a-pipeline. Just ...updating-a-pipeline  is not enough to evaluate this question.\n\nthat's why drainnin is not a flag in a pipeline update. it is a process about how to stop a pipeline w/o data loss !\n\ndata in dataflow is in 3 stages. ingestion data, buffered data and in-flight data which is processing by old pipeline."
      },
      {
        "date": "2021-10-30T07:47:00.000Z",
        "voteCount": 5,
        "content": "B is correct: Update the current pipeline and provide the transform mapping JSON object.\n Dataflow always performs a compatibility check between the old and new job and without the mapping (necessary as old and new are incompatible) it would give an error and the old job would continue to be executed\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Mapping\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#CCheck"
      },
      {
        "date": "2022-01-25T07:50:00.000Z",
        "voteCount": 2,
        "content": "new pipeline is incompatible means, compatibility check will fail. so you wil not be able to update as new pipeline.\n\nthat's why B cannot be valid answer here in this context."
      },
      {
        "date": "2022-10-09T00:34:00.000Z",
        "voteCount": 2,
        "content": "B is a way to solve compatibility issues"
      },
      {
        "date": "2020-07-26T00:44:00.000Z",
        "voteCount": 3,
        "content": "Option C &amp; D are incorrect as Cancel Option will lead to loose the data\nOption B is very Close, since the new Code make pipeline incompatible by providing transform mapping JSON file you can handle this"
      },
      {
        "date": "2021-10-30T07:49:00.000Z",
        "voteCount": 2,
        "content": "A is incorrect because updating pipeline does not include any drain flag"
      },
      {
        "date": "2022-01-25T07:54:00.000Z",
        "voteCount": 2,
        "content": "two steps.. 1st drain the job w/ sdk or console. then, update the pipeline. cause it is OK to update a job  while in draining"
      },
      {
        "date": "2022-10-09T00:33:00.000Z",
        "voteCount": 1,
        "content": "Yes but the compatibility problem will still be there, stopping the pipeline does not solve that"
      },
      {
        "date": "2022-01-25T08:01:00.000Z",
        "voteCount": 6,
        "content": "There are 5 update scenarios in a job in update-a-pipeline context. \na- changing transform name (requires mapping) , adding a new step (no need for mapping)\nb- windowing or triggering (only for minor changes, otherwise don't do that)\nc- coders (don't do that\nd- schema (adding or required to nullable is possible) other scenarios not possible\ne- stateful operations\n\nnone of them are relevant, here. cause there is no specific detail, secondly incompatible w. new pipeline. \n\nand mostly if in compatible only a has a solve. but not for all cases.\nso, drain == no data loss (ingesting, buffered and in-flight data) is the only scenario."
      },
      {
        "date": "2022-10-09T00:32:00.000Z",
        "voteCount": 3,
        "content": "As you said, Drain stops the pipeline but it does not solve the compatibility issue. The pipeline will not be able to be updated which is the core problem of the question."
      },
      {
        "date": "2022-11-19T07:23:00.000Z",
        "voteCount": 2,
        "content": "You do not want to lose any data when making this update - is the core problem. You are doing it ANYWAY."
      },
      {
        "date": "2021-12-05T07:28:00.000Z",
        "voteCount": 30,
        "content": "To all the New Guys Here. Please don't get confused with all the people's fight over here. Just google the question and you will get the correct ans in many website. Still I recommend to refer this website for question. for this Particular problem ans is A. Reason is here --&gt; https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python\nhave time to read the full page when to use Update using Json mapping and when to use Drain. (you will have question following for Drain option though).\nThumb rule is this,\n# If any major change  to windowing transformation (like completely changing window fn from fixed to sliding) in Beam/Dataflow/you want to stop pipeline but want inflight data --&gt; use Drain option.\n# For all other use cases and Minor changing to windowing fn (like just changing window time of sliding window) --&gt; Use Update with Json mapping.\n\nIn this case it is Code change to new version. so, Update with Json mapping. Simple as that.\n\nAll the Best Guys."
      },
      {
        "date": "2021-12-05T22:23:00.000Z",
        "voteCount": 6,
        "content": "SORRY I MEANT TO SAY ANS IS 'B'. In this case it is Code change to new version. so, Update with Json mapping."
      },
      {
        "date": "2022-03-21T10:44:00.000Z",
        "voteCount": 6,
        "content": "Its clearly mentioned in the question that pipeline in compatible, if it is so you can not update with JSON mapping. Only way is to stop the pipeline with Drain and replace it with a new one. So the closest answer is A only."
      },
      {
        "date": "2022-10-09T00:35:00.000Z",
        "voteCount": 1,
        "content": "JSON Mapping is a way to solve compatibility issues when updating"
      },
      {
        "date": "2020-03-20T06:13:00.000Z",
        "voteCount": 24,
        "content": "Correct B - https://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#preventing_compatibility_breaks"
      },
      {
        "date": "2020-03-20T06:13:00.000Z",
        "voteCount": 4,
        "content": "Changing the pipeline graph without providing a mapping. When you update a job, the Dataflow service attempts to match the transforms in your prior job to the transforms in the replacement job in order to transfer intermediate state data for each step. If you've renamed or removed any steps, you'll need to provide a transform mapping so that Dataflow can match state data accordingly."
      },
      {
        "date": "2020-04-15T16:41:00.000Z",
        "voteCount": 9,
        "content": "The job can be incompatible for reasons other than transformation changes. Since it is clearly mentioned that the change job is incompatible, I think we have to create a new job and D should be correct."
      },
      {
        "date": "2021-10-30T07:51:00.000Z",
        "voteCount": 3,
        "content": "Canceling the job will cause data loss, against the requirement"
      },
      {
        "date": "2024-01-26T04:28:00.000Z",
        "voteCount": 2,
        "content": "Option: C\nusing draining will stop the subscription totally while allowing the existing data to complete processing. While the pipeline is stopped, will lose streaming data. The best option is to create a new pipeline that is connected to the same subscription, then we can apply drain to the old pipeline and end it. That way we will capture all the streaming data."
      },
      {
        "date": "2023-12-16T03:06:00.000Z",
        "voteCount": 1,
        "content": "Drain flag: This flag allows the pipeline to finish processing all existing data in the Pub/Sub subscription before shutting down. This ensures no data is lost during the update.\nCurrent pipeline: Updating the current pipeline minimizes disruption and avoids setting up entirely new infrastructure.\nIncompatible changes: Even with incompatible changes, the drain flag ensures existing data is processed correctly."
      },
      {
        "date": "2023-12-16T03:06:00.000Z",
        "voteCount": 1,
        "content": "While other options might work in some cases, they have drawbacks:\n\nB. Transform mapping JSON: This option is mainly for schema changes and doesn't guarantee data completion before shutdown.\nC. New pipeline, same subscription: This risks duplicate processing of data if both pipelines run concurrently.\nD. New pipeline, new subscription: This loses the current pipeline's state and potentially data, making it impractical for incompatible changes.\nTherefore, the most reliable and data-safe approach is to update the current pipeline with the drain flag for seamless transition and data integrity.\n\nRemember, always test updates in a staging environment before deploying to production."
      },
      {
        "date": "2023-12-14T04:36:00.000Z",
        "voteCount": 2,
        "content": "Same Cloud Pub/Sub Subscription: By using the same Cloud Pub/Sub subscription for the new pipeline, you ensure that no messages are lost during the transition. Pub/Sub manages message delivery, ensuring that unacknowledged messages (those that haven't been processed by your old pipeline) will be available for the new pipeline to process.\n\nCreating a New Pipeline: Since the update makes the new pipeline incompatible with the current version, it's necessary to create a new pipeline. Attempting to update the current pipeline in place (options A and B) would not be feasible due to compatibility issues.\n\nCancel the Old Pipeline: Once the new pipeline is up and running and processing messages, you can safely cancel the old pipeline. This ensures a smooth transition with no data loss."
      },
      {
        "date": "2023-12-04T07:51:00.000Z",
        "voteCount": 1,
        "content": "In order to make an update to a Google Cloud Dataflow streaming pipeline without losing any data, the recommended approach is:\n\nA. Update the current pipeline and use the drain flag.\n\nExplanation:\n\nThe drain flag is designed to allow the current pipeline to finish processing any remaining data before shutting down. This helps ensure that no data is lost during the update process.\nBy updating the current pipeline and using the drain flag, you allow the pipeline to complete its current processing before the update takes effect, minimizing the risk of data loss.\nThis approach is a safe way to transition from the old version to the new version without interrupting data processing."
      },
      {
        "date": "2023-11-21T07:46:00.000Z",
        "voteCount": 1,
        "content": "I would vote for A because of the structure of the exam, but there are other options worth considering as well"
      },
      {
        "date": "2023-11-07T15:42:00.000Z",
        "voteCount": 1,
        "content": "My answer is C. Chatted with ChatGPT and narrowed down on this option. Let me know your thoughts on this perspective.\nOption C - By using the existing subscription, you can ensure that the data flow remains uninterrupted, and there is no loss of data during the transition from the old pipeline to the new one.\n\nCreating a new pipeline that uses the same Cloud Pub/Sub subscription allows for a seamless transition without any interruptions to the data flow. This approach ensures that the new pipeline can continue to consume data from the same subscription as the old pipeline, thereby maintaining data continuity throughout the update process."
      },
      {
        "date": "2023-11-07T08:24:00.000Z",
        "voteCount": 1,
        "content": "Correct Option : A\nExplanation:-This option is correct as the key requirement is not to lose\nthe data, the Dataflow pipeline can be stopped using the Drain option."
      },
      {
        "date": "2023-10-24T15:10:00.000Z",
        "voteCount": 1,
        "content": "It should be B\nDrain will stop the existing job only and it does not suffice the updated schema.\nIn order to bring updated schema into effect, updated JSON mapping need to be applied."
      },
      {
        "date": "2023-10-12T12:23:00.000Z",
        "voteCount": 1,
        "content": "The two cores of this question are: 1- Don't lose data \u2190 Drain, is perfect for this because you process all buffer data and stop reviving messages; normally this message is alive for 7 days of retry, so when you start a new job you will receive all without lose any data. 2- Incompatible new code \u2190 mapping solve some incompatibilities like change name of a ParDO but no a version issue. So launch a new job with the new code, it's the only option.\nSo, option is A."
      },
      {
        "date": "2023-10-07T02:30:00.000Z",
        "voteCount": 1,
        "content": "The best choice is D. Create a new pipeline that has a new Cloud Pub/Sub subscription and cancel the old pipeline."
      },
      {
        "date": "2023-07-18T05:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is A \n\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain"
      },
      {
        "date": "2023-07-10T05:07:00.000Z",
        "voteCount": 1,
        "content": "You don't want to lose data, so drain dude, drain!"
      },
      {
        "date": "2023-04-24T03:58:00.000Z",
        "voteCount": 1,
        "content": "Option C is the correct answer because creating a new pipeline with the same Cloud Pub/Sub subscription and canceling the old pipeline allows for a seamless transition without any data loss. The new pipeline can consume from the same subscription as the old pipeline and continue processing data, while the old pipeline can be safely stopped without impacting data ingestion. This approach ensures continuity of data processing while updating the pipeline code."
      },
      {
        "date": "2023-04-11T04:55:00.000Z",
        "voteCount": 1,
        "content": "A is correct since it is the first step to prevent data loss"
      },
      {
        "date": "2023-03-10T05:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/products/gcp/managing-streaming-pipelines-in-google-cloud-dataflow-just-got-better"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 32,
    "url": "https://www.examtopics.com/discussions/google/view/17052-exam-professional-data-engineer-topic-1-question-32/",
    "body": "Your company is running their first dynamic campaign, serving different offers by analyzing real-time data during the holiday season. The data scientists are collecting terabytes of data that rapidly grows every hour during their 30-day campaign. They are using Google Cloud Dataflow to preprocess the data and collect the feature (signals) data that is needed for the machine learning model in Google Cloud Bigtable. The team is observing suboptimal performance with reads and writes of their initial load of 10 TB of data. They want to improve this performance while minimizing cost. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedefine the schema by evenly distributing reads and writes across the row space of the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe performance issue should be resolved over time as the site of the BigDate cluster is increased.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedesign the schema to use a single row key to identify values that need to be updated frequently in the cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedesign the schema to use row keys based on numeric IDs that increase sequentially per user viewing the offers."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-09-14T06:32:00.000Z",
        "voteCount": 53,
        "content": "I hate it when I read the question, than I think oh easy and I KNOW the answer, then I look at the choices and the answer I thought of is just not there at all... and I realize I absolutely have no idea :'D"
      },
      {
        "date": "2020-03-20T06:15:00.000Z",
        "voteCount": 23,
        "content": "Correct A"
      },
      {
        "date": "2020-03-20T06:24:00.000Z",
        "voteCount": 20,
        "content": "https://cloud.google.com/bigtable/docs/performance#troubleshooting\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed."
      },
      {
        "date": "2024-08-11T22:38:00.000Z",
        "voteCount": 2,
        "content": "Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page."
      },
      {
        "date": "2024-07-25T13:02:00.000Z",
        "voteCount": 1,
        "content": "B is a Lie\nC and D are actually not recommended \nA is correct as it will help in even distribution of load and avoid hotspots"
      },
      {
        "date": "2023-12-04T07:46:00.000Z",
        "voteCount": 3,
        "content": "Improving performance in Google Cloud Bigtable involves optimizing the schema design to distribute the load efficiently across the clusters. Given the scenario, the best option would be:\n\nA. Redefine the schema by evenly distributing reads and writes across the row space of the table.\n\nExplanation:\n\nDistributing reads and writes evenly across the row space helps prevent hotspots and ensures that the load is spread evenly, avoiding performance bottlenecks.\nGoogle Cloud Bigtable's performance is influenced by how well the data is distributed across the tablet servers, and evenly distributing the load can lead to better performance.\nThis approach aligns with best practices for designing scalable and performant Bigtable schemas."
      },
      {
        "date": "2023-11-21T09:32:00.000Z",
        "voteCount": 1,
        "content": "The comment from hilel_eth totally makes sense to me. I would go with A"
      },
      {
        "date": "2023-08-13T21:23:00.000Z",
        "voteCount": 7,
        "content": "Guys, how relevant are these questions, as of Aug 14, 2023  Could we clear the PDE exam with these set of questions?"
      },
      {
        "date": "2023-12-11T03:25:00.000Z",
        "voteCount": 2,
        "content": "HEY DID U CLEAR THE EXAM"
      },
      {
        "date": "2023-07-29T03:13:00.000Z",
        "voteCount": 1,
        "content": "A is the only one that makes sense and is correct"
      },
      {
        "date": "2023-07-24T02:08:00.000Z",
        "voteCount": 1,
        "content": "I understand why it could be A. But why not B also? Is it because of the typo saying BigDate instead of BigTable?"
      },
      {
        "date": "2023-04-10T16:29:00.000Z",
        "voteCount": 2,
        "content": "A to avoid hot-spotting https://cloud.google.com/bigtable/docs/schema-design"
      },
      {
        "date": "2022-12-25T11:23:00.000Z",
        "voteCount": 6,
        "content": "A - \nMake sure you're reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.\n\nhttps://cloud.google.com/bigtable/docs/performance#troubleshooting"
      },
      {
        "date": "2022-12-10T15:00:00.000Z",
        "voteCount": 3,
        "content": "A good way to improve read and write performance in a database system like Google Cloud Bigtable is to redefine the schema of the table so that reads and writes are evenly distributed across the row space of the table. This can help reduce bottlenecks in processing capacity and improve efficiency in table management. In addition, by evenly distributing read and write operations, it can prevent the accumulation of operations in one part of the table, which can improve the overall performance of the system."
      },
      {
        "date": "2022-07-29T05:17:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/bigtable/docs/keyvis-overview#what-is-keyvis\n\nTo accomplish these goals, Key Visualizer can help you complete the following tasks:\n\nCheck whether your reads or writes are creating hotspots on specific rows"
      },
      {
        "date": "2022-03-03T08:43:00.000Z",
        "voteCount": 3,
        "content": "A is correct\nhttps://cloud.google.com/bigtable/docs/performance#troubleshooting\n\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed."
      },
      {
        "date": "2022-01-23T17:21:00.000Z",
        "voteCount": 2,
        "content": "correct answer -&gt; Redefine the schema by evenly distributing reads and writes across the row space of the table.\n\nMake sure you're reading and writing many different rows in your table. Bigtable performs best when reads and writes are evenly distributed throughout your table, which helps Bigtable distribute the workload across all of the nodes in your cluster. If reads and writes cannot be spread across all of your Bigtable nodes, performance will suffer.\nIf you find that you're reading and writing only a small number of rows, you might need to redesign your schema so that reads and writes are more evenly distributed.\n\nReference: https://cloud.google.com/bigtable/docs/performance#troubleshooting"
      },
      {
        "date": "2021-11-14T07:33:00.000Z",
        "voteCount": 11,
        "content": "A as the schema needs to be redesigned to distribute the reads and writes evenly across each table.\nRefer GCP documentation - Bigtable Performance:\nhttps://cloud.google.com/bigtable/docs/performance\nThe table's schema is not designed correctly. To get good performance from Cloud Bigtable, it's essential to design a schema that makes it possible to distribute reads and writes evenly across each table. See Designing Your Schema for more information. \nhttps://cloud.google.com/bigtable/docs/schema-design\nOption B is wrong as increasing the size of cluster would increase the cost.\nOption C is wrong as single row key for frequently updated identifiers reduces performance\nOption D is wrong as sequential IDs would degrade the performance.\nA safer approach is to use a reversed version of the user's numeric ID, which spreads traffic more evenly across all of the nodes for your Cloud Bigtable table."
      },
      {
        "date": "2021-10-12T09:52:00.000Z",
        "voteCount": 1,
        "content": "Ans: A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 33,
    "url": "https://www.examtopics.com/discussions/google/view/17054-exam-professional-data-engineer-topic-1-question-33/",
    "body": "Your software uses a simple JSON format for all messages. These messages are published to Google Cloud Pub/Sub, then processed with Google Cloud<br>Dataflow to create a real-time dashboard for the CFO. During testing, you notice that some messages are missing in the dashboard. You check the logs, and all messages are being published to Cloud Pub/Sub successfully. What should you do next?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCheck the dashboard application to see if it is not displaying correctly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Google Stackdriver Monitoring on Cloud Pub/Sub to find the missing messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch Cloud Dataflow to pull messages from Cloud Pub/Sub instead of Cloud Pub/Sub pushing messages to Cloud Dataflow."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T05:50:00.000Z",
        "voteCount": 35,
        "content": "Answer: C\nDescription: Stackdriver can be used to check the error like number of unack messages, publisher pushing messages faster"
      },
      {
        "date": "2020-07-12T16:46:00.000Z",
        "voteCount": 25,
        "content": "B.\nStack driver monitoring is for performance, not logging of missing data."
      },
      {
        "date": "2020-07-23T06:37:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/pubsub/docs/monitoring"
      },
      {
        "date": "2022-06-14T12:37:00.000Z",
        "voteCount": 1,
        "content": "Exactly!"
      },
      {
        "date": "2022-12-07T09:02:00.000Z",
        "voteCount": 1,
        "content": "Please refer to this PubSub specific Monitoring metrics https://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog"
      },
      {
        "date": "2020-03-27T05:51:00.000Z",
        "voteCount": 12,
        "content": "this will help us understand the reason, when we know that the data is not reaching subscriber then there is no point in checking it with dummy data"
      },
      {
        "date": "2020-11-12T17:48:00.000Z",
        "voteCount": 10,
        "content": "All messages are being published to Cloud Pub/Sub successfully. so Stackdriver might not help."
      },
      {
        "date": "2021-09-10T22:03:00.000Z",
        "voteCount": 12,
        "content": "messages sent successfully to Topic, but not Subscription.\nin this case, if Dataflow cannot handle messages correctly it might not return acknowledgments to the Pub/Sub, and these errors can be seen from Monitoring.\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_exp"
      },
      {
        "date": "2022-01-25T12:36:00.000Z",
        "voteCount": 1,
        "content": "to be more precise,  first to publisher, \n- then forwards to topic, and persistance for a while\n- then forwards to subscribe, \n- then to subscription..\n- then acknowledgement happens\n\nso in every steps, there is possibly for errors."
      },
      {
        "date": "2022-12-07T09:01:00.000Z",
        "voteCount": 2,
        "content": "PubSub doesn't forward from subscriber to subscription. A topic sends it over to subscription first, then to subscriber"
      },
      {
        "date": "2020-03-20T07:42:00.000Z",
        "voteCount": 25,
        "content": "Should be B"
      },
      {
        "date": "2020-03-20T07:58:00.000Z",
        "voteCount": 1,
        "content": "confused with D as well."
      },
      {
        "date": "2020-03-21T11:23:00.000Z",
        "voteCount": 6,
        "content": "Push or Pull guarantees the message to be delivered at-least once. So it doesn't make any difference."
      },
      {
        "date": "2021-03-11T22:35:00.000Z",
        "voteCount": 3,
        "content": "Push needs Https endpoint"
      },
      {
        "date": "2020-03-22T14:23:00.000Z",
        "voteCount": 4,
        "content": "pushing is only for a https endpoint. So Dataflow just can pull messages"
      },
      {
        "date": "2022-01-25T12:49:00.000Z",
        "voteCount": 3,
        "content": "push or pull is about how target will handle the messages.  pull mode gives flexibility when to get messages , so considering if target (or client) is slow, then it can make predictable choices.\n\ndataflow is serverless. so if you need to awake it when necessary, you should use push mechanism. or leverage cloud composer/airflow and listen to pub/sub  to trigger the dataflow."
      },
      {
        "date": "2023-10-22T06:35:00.000Z",
        "voteCount": 1,
        "content": "B. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n* By running a fixed dataset through the Cloud Dataflow pipeline, you can determine if the problem lies within the data processing stage. This allows you to identify any issues with data transformation, filtering, or processing in your pipeline.\n* Analyzing the output from this fixed dataset will help you isolate the problem and confirm whether it's related to data processing or the dashboard application."
      },
      {
        "date": "2023-10-31T03:50:00.000Z",
        "voteCount": 1,
        "content": "You must know what kind of data causes errors. I think, the first step is to get erroneous data and then test with sample of it."
      },
      {
        "date": "2023-10-07T02:34:00.000Z",
        "voteCount": 1,
        "content": "B. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output. If this results in the expected output, then the problem might be with the dashboard application (Option A), and that should be checked next."
      },
      {
        "date": "2023-06-09T03:27:00.000Z",
        "voteCount": 3,
        "content": "\"...to find the missing messages\"\nUp to that remark, Monitoring was a valid option as well. But missing messages cannot be found with monitoring.\nIt is simply not possible to find the exact missing message. I read this remark as a test if you know what is, and what isn't possible with monitoring."
      },
      {
        "date": "2023-04-11T05:16:00.000Z",
        "voteCount": 2,
        "content": "here is to determine next step. Not better way to optimize the workload. So B is the correct next step"
      },
      {
        "date": "2023-05-02T11:06:00.000Z",
        "voteCount": 1,
        "content": "B is not the next step. The next step is between pub/sub ands dataflow(C). B will not help with it at all. However it could show the issue if it is the pipeline or the view. But also it could not show it - you have no idea why some messages are not shown, so most probably it wouldnt get you any info. Definitely next step is to chek if the issue is between pubsub and dataflow. Then you coud go with B."
      },
      {
        "date": "2023-04-10T16:37:00.000Z",
        "voteCount": 1,
        "content": "Pull subscription is the correct one. Push subscription means Dataflow cannot keep up with the topic."
      },
      {
        "date": "2023-05-02T11:11:00.000Z",
        "voteCount": 1,
        "content": "It could be the issue. But C would reveal it if this is the real issue - if you will not check stackdriver, you cannot be sure if you really resolved the issue, as even if it seems to be working properly after switch to pull you cannot be sure if it is because of some other temporal factor."
      },
      {
        "date": "2023-02-22T19:44:00.000Z",
        "voteCount": 4,
        "content": "If the Dataflow does not have the expected output, it is either wrong at the input or at the pipelines. The chance that the issue is at the input (PubSub) is very low. For this case, it is likely the pipelines got some mistakes (e.g. JSON parsing failed). So we should follow B to debug the pipelines (using snapshot as test dataset for example)"
      },
      {
        "date": "2023-02-03T04:21:00.000Z",
        "voteCount": 7,
        "content": "The most efficient solution would be to run a fixed dataset through the Cloud Dataflow pipeline and analyze the output (Option B). This will allow you to determine if the issue is with the pipeline or with the dashboard application. By analyzing the output, you can see if the messages are being processed correctly and determine if there are any discrepancies or missing messages. If the issue is with the pipeline, you can then debug and make any necessary updates to ensure that all messages are processed correctly. If the issue is with the dashboard application, you can then focus on resolving that issue. This approach allows you to isolate and identify the root cause of the missing messages in a controlled and efficient manner."
      },
      {
        "date": "2023-01-29T07:21:00.000Z",
        "voteCount": 7,
        "content": "I've just skimmed over the Stackdriver docs, yes guys, it helps you check the number and age of messages that were not received/acknowledged, excellent, hurray.\n\nSo first off, c will not give us the missing messages, it will give us the count and age.\nthat means that c is inherently incorrect. \n\nAdditionally, will knowledge of the number of messages make resolving the problem any easier? No, it is just confirming what we already know.\n\nMeanwhile, approach B, will allow us to see HOW and WHY it is missing some messages, which is the step that proceeds the fix."
      },
      {
        "date": "2023-01-26T03:38:00.000Z",
        "voteCount": 2,
        "content": "Here is ChatGPT answer : \nIt's always a good practice to start by checking the logs and monitoring tools to see if there is any indication of an issue with the messages being published to Cloud Pub/Sub. In this case, you should use Google Stackdriver Monitoring to investigate if the missing messages have been published or not. You can also run a fixed dataset through the Cloud Dataflow pipeline to see if the pipeline is processing the messages correctly. If there is no issue found on the Cloud Pub/Sub and Cloud Dataflow, then you can check the dashboard application to see if it is not displaying the messages correctly. As a last resort, you can switch Cloud Dataflow to pull messages from Cloud Pub/Sub instead of Cloud Pub/Sub pushing messages to Cloud Dataflow."
      },
      {
        "date": "2023-01-29T07:24:00.000Z",
        "voteCount": 4,
        "content": "I provided it with the question as input but added the metrics available in Stackdriver, here is the response:\n\nB. Run a fixed dataset through the Cloud Dataflow pipeline and analyze the output.\n\nIf messages are being published successfully to Cloud Pub/Sub but are missing in the dashboard, the issue is likely to be with the Cloud Dataflow pipeline that processes the messages. To find the root cause of the problem, you should run a fixed dataset through the pipeline and analyze the output. This will allow you to see if the pipeline is correctly processing all messages, and identify any processing errors that might be causing messages to be lost. The output can be compared to the expected results to identify any discrepancies and resolve the issue."
      },
      {
        "date": "2023-05-02T11:19:00.000Z",
        "voteCount": 2,
        "content": "I'm tired with these responses about what chatGPT says. Most probably you've used the free 3.5 version which is absolute disaster regarding being all knowing oracle. BTW in this case I wouldn't believe even GPT4. It is a difficult question that needs a specific knowledge and experience which might be not available in the GPT training data. You cannot use any GPT up to 4 as an argument in such cases."
      },
      {
        "date": "2023-11-21T09:55:00.000Z",
        "voteCount": 1,
        "content": "Exactly. Sometimes it is total garbage"
      },
      {
        "date": "2023-01-18T10:18:00.000Z",
        "voteCount": 4,
        "content": "The question is really not asking for a solution to the problem, per se - but more of what would the next step in the situation to triage the issue.... \n\nAnswer would be B over D.  Answer D would be the recommended solution IF the question asked to rectify/fixed the issue.\n\nThoughts?"
      },
      {
        "date": "2023-04-11T05:17:00.000Z",
        "voteCount": 1,
        "content": "Agree with u"
      },
      {
        "date": "2022-12-19T18:24:00.000Z",
        "voteCount": 3,
        "content": "D. Dataflow must PULL the data to process it in real-time. Missing messages in the dashboard, means that the Pub/Sub to Dataflow was misconfigured as PUSH."
      },
      {
        "date": "2023-01-20T08:27:00.000Z",
        "voteCount": 1,
        "content": "Pull will lead to latency as new data will not be streamed upon arrival, but instead will only be passed on when Dataflow makes a pull request. So if data comes in at time 0:01 but pull requests are only happening every 10 seconds, we have 9 second delay. Push will automatically push the data to any subscribers as soon as the data comes, and thus is closer to real-time."
      },
      {
        "date": "2022-12-19T15:39:00.000Z",
        "voteCount": 3,
        "content": "To me, B sounds more logical for the below reason.\nOption C would have been ideal because any debugging starts with checking the logs, however the option says, check stackdriver for missing messages. Had it been, check stackdriver to figure out the number of undelivered messages, C would have been more suitable. Given the slight bit of dodginess in option c, I would go with B"
      },
      {
        "date": "2022-12-13T08:03:00.000Z",
        "voteCount": 1,
        "content": "Why checking Pub/Sub again when this is already verified to be fine according to the question. Shouldn't you be checking the next stage in the flow which is Dataflow? \nOption - B"
      },
      {
        "date": "2022-12-12T19:18:00.000Z",
        "voteCount": 1,
        "content": "Answer - B. Because already we know message is missing so better to test with fixed dataset and check code ."
      },
      {
        "date": "2022-11-29T12:53:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://cloud.google.com/pubsub/docs/monitoring#:~:text=the%20specific%20metrics.-,Monitor%20message%20backlog,information%20about%20this%20metric%2C%20see%20the%20relevant%20section%20of%20this%20document.,-Create%20alerting%20policies"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 34,
    "url": "https://www.examtopics.com/discussions/google/view/79622-exam-professional-data-engineer-topic-1-question-34/",
    "body": "Flowlogistic Case Study -<br><br>Company Overview -<br>Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.<br><br>Company Background -<br>The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.<br><br>Solution Concept -<br>Flowlogistic wants to implement two concepts using the cloud:<br>\u2711 Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads<br>\u2711 Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.<br><br>Existing Technical Environment -<br>Flowlogistic architecture resides in a single data center:<br>\u2711 Databases<br>8 physical servers in 2 clusters<br>- SQL Server `\" user data, inventory, static data<br>3 physical servers<br>- Cassandra `\" metadata, tracking messages<br>10 Kafka servers `\" tracking message aggregation and batch insert<br>\u2711 Application servers `\" customer front end, middleware for order/customs<br>60 virtual machines across 20 physical servers<br>- Tomcat `\" Java services<br>- Nginx `\" static content<br>- Batch servers<br>\u2711 Storage appliances<br>-  iSCSI for virtual machine (VM) hosts<br>-  Fibre Channel storage area network (FC SAN) `\" SQL server storage<br>-  Network-attached storage (NAS) image storage, logs, backups<br>\u2711 10 Apache Hadoop /Spark servers<br>- Core Data Lake<br>- Data analysis workloads<br>\u2711 20 miscellaneous servers<br>- Jenkins, monitoring, bastion hosts,<br><br>Business Requirements -<br>Build a reliable and reproducible environment with scaled panty of production.<br><img src=\"/assets/media/exam-media/04341/0001900008.png\" class=\"in-exam-image\"><br>\u2711 Aggregate data in a centralized Data Lake for analysis<br>\u2711 Use historical data to perform predictive analytics on future shipments<br>\u2711 Accurately track every shipment worldwide using proprietary technology<br>\u2711 Improve business agility and speed of innovation through rapid provisioning of new resources<br>\u2711 Analyze and optimize architecture for performance in the cloud<br>\u2711 Migrate fully to the cloud if all other requirements are met<br><br>Technical Requirements -<br>\u2711 Handle both streaming and batch data<br>\u2711 Migrate existing Hadoop workloads<br>\u2711 Ensure architecture is scalable and elastic to meet the changing demands of the company.<br>\u2711 Use managed services whenever possible<br>\u2711 Encrypt data flight and at rest<br>\u2711 Connect a VPN between the production data center and cloud environment<br><br>SEO Statement -<br>We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.<br>We need to organize our information so we can more easily understand where our customers are and what they are shipping.<br><br>CTO Statement -<br>IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.<br><br>CFO Statement -<br>Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.<br>Flowlogistic wants to use Google BigQuery as their primary analysis system, but they still have Apache Hadoop and Spark workloads that they cannot move to<br>BigQuery. Flowlogistic does not know how to store the data that is common to both workloads. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the common data in BigQuery as partitioned tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the common data in BigQuery and expose authorized views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the common data encoded as Avro in Google Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore he common data in the HDFS storage for a Google Cloud Dataproc cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-10-22T09:20:00.000Z",
        "voteCount": 6,
        "content": "C. Store the common data encoded as Avro in Google Cloud Storage.\n\nThis approach allows for interoperability between BigQuery and Hadoop/Spark as Avro is a commonly used data serialization format that can be read by both systems. Data stored in Google Cloud Storage can be accessed by both BigQuery and Dataproc, providing a bridge between the two environments. Additionally, you can set up data transformation pipelines in Dataproc to work with this data."
      },
      {
        "date": "2024-07-31T11:54:00.000Z",
        "voteCount": 1,
        "content": "in BigQuery we can use BigLake tables based on Avro for historical data, and Spark stored procedures"
      },
      {
        "date": "2024-06-13T14:28:00.000Z",
        "voteCount": 1,
        "content": "Data lake,fully managed, data analytics. Stores structured and unstructured data are keywords,so answer is GCS, OPTION C"
      },
      {
        "date": "2023-12-04T07:59:00.000Z",
        "voteCount": 4,
        "content": "Given the scenario described for Flowlogistic's requirements and technical environment, the most suitable option for storing common data that is used by both Google BigQuery and Apache Hadoop/Spark workloads is:\n\nC. Store the common data encoded as Avro in Google Cloud Storage."
      },
      {
        "date": "2023-07-30T16:57:00.000Z",
        "voteCount": 3,
        "content": "To simplify the question, Apache Hadoop and Spark workloads that cannot be moved to BigQuery can be handled by DataProc. So the correct answer is D."
      },
      {
        "date": "2023-07-24T02:12:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer. Common data will lie in BigQuery but will be accessible via the views with SQL in Hadoop workloads."
      },
      {
        "date": "2023-02-22T23:11:00.000Z",
        "voteCount": 3,
        "content": "C should be the correct answer. However, please note that Google just released the BigQuery Connector for Hadoop, so if they ask the same question today, B will be the correct answer.\nA could be correct too, but I cannot see why it has to be partitioned"
      },
      {
        "date": "2023-07-04T05:31:00.000Z",
        "voteCount": 3,
        "content": "If you check the https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery, it unloads the BQ data to GCS, utilizes it, and then deletes it from the GCS. Storing common data twice (at BQ and GCS) will not be the best option compared to 'C' (using GCS as the main common dataset)."
      },
      {
        "date": "2023-01-07T18:12:00.000Z",
        "voteCount": 1,
        "content": "I would vote for C as it can be used for analysis with Bigquery. Furthermore, Hadoop workload can also be transferred to dataproc connected to GCS."
      },
      {
        "date": "2022-12-12T19:31:00.000Z",
        "voteCount": 1,
        "content": "Answer B look ok , because in question they want to store common data which can use by both workload, and using big query and primary analytical tool that would be best option and easy to analysis common data."
      },
      {
        "date": "2022-11-30T07:48:00.000Z",
        "voteCount": 3,
        "content": "\"Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data\" - BigQuery cant take unstructured data so A and B are out.\nStoring data in HDFS storage is never recommended unless latency is a requirement, so D is out.\n\nThat leaves us with GCS. Answer is C"
      },
      {
        "date": "2023-01-01T08:03:00.000Z",
        "voteCount": 1,
        "content": "I thought you can now store unstructured data in BigQuery via the object tables announced during Google NEXT 2022... If that's possib;e, does that make B a better choice?"
      },
      {
        "date": "2022-11-18T06:26:00.000Z",
        "voteCount": 2,
        "content": "BigQuery can use federated queries to connect to the avro data in GCS while running spark jobs on it.  If you duplicate the date you have to manage both data sets."
      },
      {
        "date": "2022-11-16T09:13:00.000Z",
        "voteCount": 1,
        "content": "A\nThey wanted BigQuery. And connector is all you need to perform Hadoop or spark. Hadoop migration can be done using dataproc."
      },
      {
        "date": "2022-11-16T09:14:00.000Z",
        "voteCount": 2,
        "content": "Also apparently they want all data at one place and want bigQ"
      },
      {
        "date": "2022-11-16T07:28:00.000Z",
        "voteCount": 1,
        "content": "C as it can be used as an external table from BigQuery and with the Cloud Storage Connector it can be used by the Spark workloads (running in Dataproc)"
      },
      {
        "date": "2022-11-14T07:30:00.000Z",
        "voteCount": 1,
        "content": "C, as both capable of AVRO, but the customer does not know what they want to do with the data yet."
      },
      {
        "date": "2022-11-03T21:56:00.000Z",
        "voteCount": 1,
        "content": "In Technical requirements it Was clearly mentioned that they need to Migrate existing Hadoop Cluster for which Data Proc Cluster is a replacement."
      },
      {
        "date": "2022-09-21T05:49:00.000Z",
        "voteCount": 4,
        "content": "C is ans...avro data can be accessed by spark as well"
      },
      {
        "date": "2022-09-02T16:44:00.000Z",
        "voteCount": 3,
        "content": "The answer is C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 35,
    "url": "https://www.examtopics.com/discussions/google/view/16658-exam-professional-data-engineer-topic-1-question-35/",
    "body": "Flowlogistic Case Study -<br><br>Company Overview -<br>Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.<br><br>Company Background -<br>The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.<br><br>Solution Concept -<br>Flowlogistic wants to implement two concepts using the cloud:<br>\u2711 Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads<br>\u2711 Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.<br><br>Existing Technical Environment -<br>Flowlogistic architecture resides in a single data center:<br>\u2711 Databases<br>8 physical servers in 2 clusters<br>- SQL Server `\" user data, inventory, static data<br>3 physical servers<br>- Cassandra `\" metadata, tracking messages<br>10 Kafka servers `\" tracking message aggregation and batch insert<br>\u2711 Application servers `\" customer front end, middleware for order/customs<br>60 virtual machines across 20 physical servers<br>- Tomcat `\" Java services<br>- Nginx `\" static content<br>- Batch servers<br>\u2711 Storage appliances<br>- iSCSI for virtual machine (VM) hosts<br>- Fibre Channel storage area network (FC SAN) `\" SQL server storage<br>- Network-attached storage (NAS) image storage, logs, backups<br>\u2711 10 Apache Hadoop /Spark servers<br>- Core Data Lake<br>- Data analysis workloads<br>\u2711 20 miscellaneous servers<br>- Jenkins, monitoring, bastion hosts,<br><br>Business Requirements -<br>\u2711 Build a reliable and reproducible environment with scaled panty of production.<br>\u2711 Aggregate data in a centralized Data Lake for analysis<br>\u2711 Use historical data to perform predictive analytics on future shipments<br>\u2711 Accurately track every shipment worldwide using proprietary technology<br>\u2711 Improve business agility and speed of innovation through rapid provisioning of new resources<br>\u2711 Analyze and optimize architecture for performance in the cloud<br>\u2711 Migrate fully to the cloud if all other requirements are met<br><br>Technical Requirements -<br>\u2711 Handle both streaming and batch data<br>\u2711 Migrate existing Hadoop workloads<br>\u2711 Ensure architecture is scalable and elastic to meet the changing demands of the company.<br>\u2711 Use managed services whenever possible<br>\u2711 Encrypt data flight and at rest<br>\u2711 Connect a VPN between the production data center and cloud environment<br><br>SEO Statement -<br>We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.<br>We need to organize our information so we can more easily understand where our customers are and what they are shipping.<br><br>CTO Statement -<br>IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.<br><br>CFO Statement -<br>Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.<br>Flowlogistic's management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system.<br>You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, and Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, and Local SSD",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud SQL, and Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Load Balancing, Cloud Dataflow, and Cloud Storage"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T05:18:00.000Z",
        "voteCount": 38,
        "content": "I would say A.\nI think Pub/Sub can't directly send data to Cloud SQL."
      },
      {
        "date": "2020-03-20T08:33:00.000Z",
        "voteCount": 15,
        "content": "Answer: A"
      },
      {
        "date": "2024-05-16T15:17:00.000Z",
        "voteCount": 1,
        "content": "A is right answer"
      },
      {
        "date": "2023-12-04T08:03:00.000Z",
        "voteCount": 1,
        "content": "Given the requirements for ingesting data from global sources, processing and querying in real-time, and storing the data reliably for the real-time inventory tracking system, the most suitable combination of Google Cloud Platform (GCP) products is:\nA. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\nExplanation:\nCloud Pub/Sub: It is a messaging service that allows you to asynchronously send and receive messages between independent applications.\nCloud Dataflow: It can handle both streaming and batch data, making it suitable for real-time processing of data from various sources.\nCloud Storage: Cloud Storage can be used to store the processed and analyzed data reliably. It provides scalable, durable, and globally accessible object storage, making it suitable for storing large volumes of data."
      },
      {
        "date": "2023-10-22T09:21:00.000Z",
        "voteCount": 2,
        "content": "A. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage\n\nHere's why this combination is suitable:\n\nCloud Pub/Sub: It is used for ingesting real-time data from various global sources. It's a messaging service that can handle large volumes of data and is highly scalable.\n\nCloud Dataflow: It's a stream and batch data processing service that allows you to process and analyze the data in real-time. It can take data from Pub/Sub and perform transformations or aggregations as needed.\n\nCloud Storage: It provides reliable storage for the data. You can store the processed data in Cloud Storage for further analysis, and it is a scalable and durable storage solution.\n\nOption B is not ideal because Local SSDs are not a suitable storage option for persisting data that needs to be reliably stored. Option C includes Cloud SQL, which is not typically used for ingesting and processing real-time data. Option D includes Cloud Load Balancing, which is not relevant to the use case of ingesting and processing data for the inventory tracking system."
      },
      {
        "date": "2023-07-26T06:40:00.000Z",
        "voteCount": 1,
        "content": "Since Cloud SQL is fully managed service &amp; Dataflow is serverless hence we should opt for dataflow as it is thumb rule for google that we should choose serverless product over fully managed service."
      },
      {
        "date": "2023-07-18T05:41:00.000Z",
        "voteCount": 2,
        "content": "The technical requirements mention that the pipeline should handle both streaming and batch data. The solution should include DataFlow and not Cloud SQL. the answer is A."
      },
      {
        "date": "2023-02-14T04:27:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub to scale streaming data, Dataflow to processes both structured and unstructured data and cloud storage to store common data"
      },
      {
        "date": "2023-01-26T03:50:00.000Z",
        "voteCount": 1,
        "content": "Option B. Cloud Pub/Sub, Cloud Dataflow, and Local SSD is not a good option as Local SSD is not a scalable solution and could not handle large amount of data\nOption C. Cloud Pub/Sub, Cloud SQL, and Cloud Storage is not a good option as Cloud SQL is a relational database and is not suitable for real-time processing and querying large amounts of data\nOption D. Cloud Load Balancing, Cloud Dataflow, and Cloud Storage is not a good option as Cloud Load Balancing is used for distributing traffic across multiple instances, it doesn't handle data processing and storage."
      },
      {
        "date": "2023-01-26T03:49:00.000Z",
        "voteCount": 1,
        "content": "A. Cloud Pub/Sub, Cloud Dataflow, and Cloud Storage is the best combination of GCP products for the use case described.\nCloud Pub/Sub can be used to ingest data from a variety of global sources, as it allows for easy integration with external systems through its publish-subscribe messaging model.\nCloud Dataflow can be used to process and query the data in real-time, as it is a fully managed service for creating data pipelines that can handle both batch and streaming data.\nCloud Storage can be used to store the data reliably, as it is a fully managed object storage service that can handle large amounts of data and is highly durable and available."
      },
      {
        "date": "2023-01-17T02:53:00.000Z",
        "voteCount": 1,
        "content": "Answer is A. Cloud Dataflow for batch + streaming, Cloud Pub/Sub for streaming ingestion, Cloud Storage for long term data storage."
      },
      {
        "date": "2022-11-19T12:53:00.000Z",
        "voteCount": 2,
        "content": "Are scenario based questions still in the latest exam?? Are these still relevant?"
      },
      {
        "date": "2022-10-18T12:33:00.000Z",
        "voteCount": 2,
        "content": "Existing inventory data is in SQL, data ingested from Kafka will need to update inventory at some point. Existence of SQL in current estate indicates SQL must be present in the Cloud estate"
      },
      {
        "date": "2022-09-14T14:51:00.000Z",
        "voteCount": 9,
        "content": "This site make me feel that it intends to make users to be involved in discussion by suggesting wrong answer"
      },
      {
        "date": "2022-08-10T21:07:00.000Z",
        "voteCount": 3,
        "content": "Answer is clearly option A."
      },
      {
        "date": "2022-07-13T14:34:00.000Z",
        "voteCount": 6,
        "content": "why are there so many incorrect answers? it's so hard to study this way"
      },
      {
        "date": "2022-07-06T01:57:00.000Z",
        "voteCount": 1,
        "content": "Answer A : because Cloud Sql not suitable for Global"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 36,
    "url": "https://www.examtopics.com/discussions/google/view/17058-exam-professional-data-engineer-topic-1-question-36/",
    "body": "Flowlogistic Case Study -<br><br>Company Overview -<br>Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.<br><br>Company Background -<br>The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.<br><br>Solution Concept -<br>Flowlogistic wants to implement two concepts using the cloud:<br>Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads<br><img src=\"/assets/media/exam-media/04341/0002300001.png\" class=\"in-exam-image\"><br>\u2711 Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.<br><br>Existing Technical Environment -<br>Flowlogistic architecture resides in a single data center:<br>\u2711 Databases<br>8 physical servers in 2 clusters<br>- SQL Server `\" user data, inventory, static data<br>3 physical servers<br>- Cassandra `\" metadata, tracking messages<br>10 Kafka servers `\" tracking message aggregation and batch insert<br>\u2711 Application servers `\" customer front end, middleware for order/customs<br>60 virtual machines across 20 physical servers<br>- Tomcat `\" Java services<br>- Nginx `\" static content<br>- Batch servers<br>\u2711 Storage appliances<br>- iSCSI for virtual machine (VM) hosts<br>- Fibre Channel storage area network (FC SAN) `\" SQL server storage<br>- Network-attached storage (NAS) image storage, logs, backups<br>\u2711 10 Apache Hadoop /Spark servers<br>- Core Data Lake<br>- Data analysis workloads<br>\u2711 20 miscellaneous servers<br>- Jenkins, monitoring, bastion hosts,<br><br>Business Requirements -<br>\u2711 Build a reliable and reproducible environment with scaled panty of production.<br>\u2711 Aggregate data in a centralized Data Lake for analysis<br>\u2711 Use historical data to perform predictive analytics on future shipments<br>\u2711 Accurately track every shipment worldwide using proprietary technology<br>\u2711 Improve business agility and speed of innovation through rapid provisioning of new resources<br>\u2711 Analyze and optimize architecture for performance in the cloud<br>\u2711 Migrate fully to the cloud if all other requirements are met<br><br>Technical Requirements -<br>Handle both streaming and batch data<br><img src=\"/assets/media/exam-media/04341/0002400014.png\" class=\"in-exam-image\"><br>\u2711 Migrate existing Hadoop workloads<br>\u2711 Ensure architecture is scalable and elastic to meet the changing demands of the company.<br>\u2711 Use managed services whenever possible<br>\u2711 Encrypt data flight and at rest<br>\u2711 Connect a VPN between the production data center and cloud environment<br><br>SEO Statement -<br>We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.<br>We need to organize our information so we can more easily understand where our customers are and what they are shipping.<br><br>CTO Statement -<br>IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.<br><br>CFO Statement -<br>Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.<br>Flowlogistic's CEO wants to gain rapid insight into their customer base so his sales team can be better informed in the field. This team is not very technical, so they've purchased a visualization tool to simplify the creation of BigQuery reports. However, they've been overwhelmed by all the data in the table, and are spending a lot of money on queries trying to find the data they need. You want to solve their problem in the most cost-effective way. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data into a Google Sheet for virtualization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional table with only the necessary columns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view on the table to present to the virtualization tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate identity and access management (IAM) roles on the appropriate columns, so only they appear in a query."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-20T08:36:00.000Z",
        "voteCount": 39,
        "content": "Answer: C"
      },
      {
        "date": "2020-11-17T03:51:00.000Z",
        "voteCount": 24,
        "content": "Answer is C. A logical view can be created with only the required columns which is required for visualization. B is not the right option as you will create a table and make it static. What happens when the original data is updated. This new table will not have the latest data and hence view is the best possible option here."
      },
      {
        "date": "2023-02-21T03:45:00.000Z",
        "voteCount": 4,
        "content": "I don't think so because in question they worried about spending money for query but, using view could not make money safe because logical view scan all of the data in the table. so, for saving money for query then Answer B is more suitable"
      },
      {
        "date": "2024-05-17T01:08:00.000Z",
        "voteCount": 1,
        "content": "The point is reducing the number of columns, not caching. Yes, it will query the table, but with only the necessary columns the view has."
      },
      {
        "date": "2023-11-07T21:20:00.000Z",
        "voteCount": 2,
        "content": "Answer: C"
      },
      {
        "date": "2023-10-22T09:23:00.000Z",
        "voteCount": 4,
        "content": "C. Create a view on the table to present to the virtualization tool.\n\nCreating a view in BigQuery allows you to define a virtual table that is a subset of the original data, containing only the necessary columns or filtered data that the sales team requires for their reports. This approach is cost-effective because it doesn't involve exporting data to external tools or creating additional tables, and it ensures that the sales team is working with the specific data they need without running expensive queries on the full dataset. It simplifies the data for non-technical users while keeping the data in BigQuery, which is a powerful and cost-efficient data warehousing solution.\n\nOptions A (exporting to Google Sheet) and B (creating an additional table) might introduce data redundancy and maintenance overhead, and they don't provide the same level of control and security as creating a view. Option D (IAM roles) doesn't address the issue of simplifying the data for the sales team; it's more focused on access control."
      },
      {
        "date": "2023-07-24T02:26:00.000Z",
        "voteCount": 2,
        "content": "C. You won't pay for storage for the view, and it will only include the necessary columns. Even if we assume that we don't talk about a materialized view, a logical view query can use the cache as much as a table query. So a new table does not have any benefit over a view, even if the view is logical."
      },
      {
        "date": "2023-04-20T19:12:00.000Z",
        "voteCount": 2,
        "content": "The answer is C"
      },
      {
        "date": "2023-04-19T01:17:00.000Z",
        "voteCount": 3,
        "content": "Answer is C"
      },
      {
        "date": "2023-03-26T23:26:00.000Z",
        "voteCount": 1,
        "content": "B it is more cost-effective and efficient approach to handle reports"
      },
      {
        "date": "2023-03-10T23:35:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2023-03-06T04:14:00.000Z",
        "voteCount": 3,
        "content": "C \u2014\u2014 view is better than another table to keep data consistent"
      },
      {
        "date": "2023-02-08T08:26:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, creating view tables can easy and flexible to do the most cost-effetive way."
      },
      {
        "date": "2023-01-25T08:29:00.000Z",
        "voteCount": 2,
        "content": "The appropriate solution is C, creating a view on the table, by selecting the relevant columns only (and not by creating another, static, table)"
      },
      {
        "date": "2023-01-21T09:53:00.000Z",
        "voteCount": 1,
        "content": "Providing that the question was explicit in B and D about the selection of the appropriate columns it quite  intriguing that it did not mention the selection of the appropriate column for the view. We can definitely build a view which might just present the same data or something much complex, thus i vote for B"
      },
      {
        "date": "2023-01-17T22:26:00.000Z",
        "voteCount": 1,
        "content": "C is the correct answer"
      },
      {
        "date": "2023-01-08T16:42:00.000Z",
        "voteCount": 1,
        "content": "I mean C"
      },
      {
        "date": "2023-01-05T07:22:00.000Z",
        "voteCount": 1,
        "content": "The answer is B. Because it is not specified as a materialized view"
      },
      {
        "date": "2023-01-10T01:08:00.000Z",
        "voteCount": 1,
        "content": "But you would want a logical view for this, not materialized view"
      },
      {
        "date": "2022-12-27T05:47:00.000Z",
        "voteCount": 2,
        "content": "A materialized view would be the best choice since it contains real-time streaming data and is also cost-effective since it only changes the delta from query updates. This is possible by using smart caching."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 37,
    "url": "https://www.examtopics.com/discussions/google/view/17059-exam-professional-data-engineer-topic-1-question-37/",
    "body": "Flowlogistic Case Study -<br><br>Company Overview -<br>Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.<br><br>Company Background -<br>The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.<br><br>Solution Concept -<br>Flowlogistic wants to implement two concepts using the cloud:<br>\u2711 Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads<br>\u2711 Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.<br><br>Existing Technical Environment -<br>Flowlogistic architecture resides in a single data center:<br>\u2711 Databases<br>8 physical servers in 2 clusters<br>- SQL Server `\" user data, inventory, static data<br>3 physical servers<br>- Cassandra `\" metadata, tracking messages<br>10 Kafka servers `\" tracking message aggregation and batch insert<br>\u2711 Application servers `\" customer front end, middleware for order/customs<br>60 virtual machines across 20 physical servers<br>- Tomcat `\" Java services<br>- Nginx `\" static content<br>- Batch servers<br>\u2711 Storage appliances<br>- iSCSI for virtual machine (VM) hosts<br>- Fibre Channel storage area network (FC SAN) `\" SQL server storage<br>- Network-attached storage (NAS) image storage, logs, backups<br>\u2711 10 Apache Hadoop /Spark servers<br>- Core Data Lake<br>- Data analysis workloads<br>\u2711 20 miscellaneous servers<br>- Jenkins, monitoring, bastion hosts,<br><br>Business Requirements -<br>\u2711 Build a reliable and reproducible environment with scaled panty of production.<br>\u2711 Aggregate data in a centralized Data Lake for analysis<br>\u2711 Use historical data to perform predictive analytics on future shipments<br>\u2711 Accurately track every shipment worldwide using proprietary technology<br>\u2711 Improve business agility and speed of innovation through rapid provisioning of new resources<br>\u2711 Analyze and optimize architecture for performance in the cloud<br>\u2711 Migrate fully to the cloud if all other requirements are met<br><br>Technical Requirements -<br>\u2711 Handle both streaming and batch data<br>\u2711 Migrate existing Hadoop workloads<br>\u2711 Ensure architecture is scalable and elastic to meet the changing demands of the company.<br>\u2711 Use managed services whenever possible<br>\u2711 Encrypt data flight and at rest<br>\u2711 Connect a VPN between the production data center and cloud environment<br><br>SEO Statement -<br>We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.<br>We need to organize our information so we can more easily understand where our customers are and what they are shipping.<br><br>CTO Statement -<br>IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.<br><br>CFO Statement -<br>Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.<br>Flowlogistic is rolling out their real-time inventory tracking system. The tracking devices will all send package-tracking messages, which will now go to a single<br>Google Cloud Pub/Sub topic instead of the Apache Kafka cluster. A subscriber application will then process the messages for real-time reporting and store them in<br>Google BigQuery for historical analysis. You want to ensure the package data can be analyzed over time.<br>Which approach should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the timestamp on each message in the Cloud Pub/Sub subscriber application as they are received.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAttach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Clod Pub/Sub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the NOW () function in BigQuery to record the event's time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the automatically generated timestamp from Cloud Pub/Sub to order the data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-04-14T00:18:00.000Z",
        "voteCount": 36,
        "content": "\"However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume.\"  \n\nSure man, Kafka is not performing, let's use PubSub instead hahaha..."
      },
      {
        "date": "2021-06-29T07:07:00.000Z",
        "voteCount": 7,
        "content": "lol this is a vendor exam..."
      },
      {
        "date": "2022-11-29T00:03:00.000Z",
        "voteCount": 2,
        "content": "google send via pub sub web indexes\ntwice a day a whole internet is being sent via pub sub"
      },
      {
        "date": "2020-03-20T08:42:00.000Z",
        "voteCount": 23,
        "content": "Answer: B"
      },
      {
        "date": "2023-10-22T09:24:00.000Z",
        "voteCount": 5,
        "content": "B. Attach the timestamp and Package ID on the outbound message from each publisher device as they are sent to Cloud Pub/Sub.\n\nHere's why this approach is the most suitable:\n\nBy attaching a timestamp and Package ID at the point of origin (publisher device), you ensure that each message has a clear and consistent timestamp associated with it from the moment it is generated. This provides a reliable and accurate record of when each package-tracking message was created, which is crucial for analyzing the data over time.\n\nThis approach allows you to maintain the chronological order of events as they occurred at the source, which is important for real-time reporting and historical analysis.\n\nOption A suggests attaching the timestamp in the Cloud Pub/Sub subscriber application. While this can work, it introduces a potential delay and the risk of timestamps not being accurate if there are issues with message processing.\n\nOption C, using the NOW() function in BigQuery, records the time when the data is ingested into BigQuery, which may not reflect the actual time of the event."
      },
      {
        "date": "2023-02-08T08:31:00.000Z",
        "voteCount": 1,
        "content": "Answer is B, attach the timestamp and ID is necessary to analyze data easily."
      },
      {
        "date": "2022-04-25T23:41:00.000Z",
        "voteCount": 4,
        "content": "Answer: B"
      },
      {
        "date": "2022-03-03T09:48:00.000Z",
        "voteCount": 1,
        "content": "we need package ID + Timestamp so B"
      },
      {
        "date": "2022-01-23T07:34:00.000Z",
        "voteCount": 1,
        "content": "Answer: B"
      },
      {
        "date": "2022-01-22T18:51:00.000Z",
        "voteCount": 1,
        "content": "agree with humza"
      },
      {
        "date": "2022-01-13T17:28:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage"
      },
      {
        "date": "2021-10-14T13:29:00.000Z",
        "voteCount": 1,
        "content": "D is enough.. we have publish timestamp which is enough for this requirement"
      },
      {
        "date": "2022-01-26T03:55:00.000Z",
        "voteCount": 3,
        "content": "there are 2 requirements\n1- is about ordering due to historical data analysis \n2- what it means  to write a single topic and its impact.. why some sentence added here.\n\n1st is primary, 2nd is secondary req. in this context.\n\nSo, \n- in pub/sub, processTime is filled by server, not publisher. but that does not guarantee the ordering due to latency, pub/sub handling, sensors or any other reasons..\n- you need to populate orderingKey field too, so that subscribers can get in ordered."
      },
      {
        "date": "2022-04-11T00:47:00.000Z",
        "voteCount": 1,
        "content": "Also, since this is a International company, adding timestamp on message receiving would  help catch local time."
      },
      {
        "date": "2021-10-12T19:25:00.000Z",
        "voteCount": 1,
        "content": "It is about processing time and event time.. Answer is B."
      },
      {
        "date": "2022-01-26T03:45:00.000Z",
        "voteCount": 1,
        "content": "not just timing, but also package-id .. cause they are sending 1 topic in gcp instead of to many in kafka. that means there must be added some additional critical data too."
      },
      {
        "date": "2021-10-12T10:26:00.000Z",
        "voteCount": 4,
        "content": "Ans: B\nA: Adding timestamp as they received is not a better option, messages may not arrive in order at the receiver/ subscriber, could be due to connectivity or network.\nB: Timestamp should be added here.\nC: Doesn't make sense at all.\nD: Ordering should be based on the order how messages are generated at the publisher but not as per order they reach the pub/sub."
      },
      {
        "date": "2021-07-10T06:41:00.000Z",
        "voteCount": 7,
        "content": "Answer: B\nA. There is no indication that the application can do this. Moreover, due to networking problems, it is possible that Pub/Sub doesn't receive messages in order. This will analysis difficult.\nB. This makes sure that you have access to publishing timestamp which provides you with the correct ordering of messages.\nC. If timestamps are already messed up, BigQuery will get wrong results anyways.\nD. The timestamp we are interested in is when the data was produced by the publisher, not when it was received by Pub/Sub."
      },
      {
        "date": "2021-07-08T07:22:00.000Z",
        "voteCount": 2,
        "content": "Vote for B"
      },
      {
        "date": "2021-02-19T09:51:00.000Z",
        "voteCount": 3,
        "content": "Better if the publisher attached the package ID and Timestamp as packages can come in an Asynchronous fashion."
      },
      {
        "date": "2021-02-07T09:36:00.000Z",
        "voteCount": 3,
        "content": "Correct B"
      },
      {
        "date": "2020-11-17T05:56:00.000Z",
        "voteCount": 6,
        "content": "The answer is B. \n\nJSON representation\n{\n  \"data\": string,\n  \"attributes\": {\n    string: string,\n    ...\n  },\n  \"messageId\": string,\n  \"publishTime\": string,\n  \"orderingKey\": string\n}\n\nIn the attribute, we can have package id and timestamp."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 38,
    "url": "https://www.examtopics.com/discussions/google/view/16657-exam-professional-data-engineer-topic-1-question-38/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>\u2711 Ensure secure and efficient transport and storage of telemetry data<br>\u2711 Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>\u2711 Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>\u2711 Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>MJTelco's Google Cloud Dataflow pipeline is now ready to start receiving data from the 50,000 installations. You want to allow Cloud Dataflow to scale its compute power up as required. Which Cloud Dataflow pipeline configuration setting should you update?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe zone",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe number of workers",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe disk size per worker",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe maximum number of workers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-11-17T23:25:00.000Z",
        "voteCount": 27,
        "content": "The correct answer is D. Please look for the details in below\nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params\nWe need to specify and set execution parameters for cloud data flow .\n\nAlso, to enable autoscaling, set the following execution parameters when you start your pipeline:\n\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=N\nThe objective of autoscaling streaming pipelines is to minimize backlog while maximizing worker utilization and throughput, and quickly react to spikes in load. By enabling autoscaling, you don't have to choose between provisioning for peak load and fresh results. Workers are added as CPU utilization and backlog increase and are removed as these metrics come down. This way, you\u2019re paying only for what you need, and the job is processed as efficiently as possible."
      },
      {
        "date": "2020-03-15T05:16:00.000Z",
        "voteCount": 26,
        "content": "D. The maximum number of workers answers to the scale question"
      },
      {
        "date": "2024-03-13T22:50:00.000Z",
        "voteCount": 1,
        "content": "Cloud Dataflow dynamically scales the number of workers based on the amount of data being processed and the processing requirements. By updating the maximum number of workers, you allow Dataflow to scale up the compute power as needed to handle the workload efficiently. This ensures that the pipeline can adapt to changes in data volume and processing demands."
      },
      {
        "date": "2023-10-22T09:26:00.000Z",
        "voteCount": 2,
        "content": "D. The maximum number of workers\n\nBy increasing the maximum number of workers, you ensure that Cloud Dataflow can scale its compute power to handle the increased data processing load efficiently."
      },
      {
        "date": "2023-05-21T06:51:00.000Z",
        "voteCount": 2,
        "content": "dataflow auto-scales, then if it is not scaling is because it has reached the maximum number of workers that have been set."
      },
      {
        "date": "2023-04-20T19:48:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer.  Dataflow is Serverless. Specify your Region, autoscaling and other 'knobing' activities that are 'under the hood' will be taken care for you. Remember the company cannot afford to staff an Operations team to monitor data feeds so rely on ..."
      },
      {
        "date": "2023-03-10T23:37:00.000Z",
        "voteCount": 2,
        "content": "this is correct"
      },
      {
        "date": "2023-01-17T22:38:00.000Z",
        "voteCount": 1,
        "content": "D . is the correct answer"
      },
      {
        "date": "2023-01-17T03:02:00.000Z",
        "voteCount": 1,
        "content": "Answer A provided is definitely wrong. Who comes up with these answers?"
      },
      {
        "date": "2022-09-27T12:09:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: D\n\n\u274c A: The zone has nothing to do with scaling computer power.\n\n\u274c B: The key word here is, \"Scale its compute power up AS REQUIRED\", with this answer, the number of workers would immediately scale the computer power.\n\n\u274c C: we need to scale compute power, not storage\n\n\u2705 D: is the correct answer, changing the Number of Maximum workers will allow Dataflow to add up to that number of workers if required.\n\nhttps://cloud.google.com/dataflow/docs/reference/pipeline-options#resource_utilization"
      },
      {
        "date": "2022-01-13T17:36:00.000Z",
        "voteCount": 2,
        "content": "@Radhika7983"
      },
      {
        "date": "2022-01-04T05:14:00.000Z",
        "voteCount": 2,
        "content": "The correct answer is D. \nhttps://cloud.google.com/dataflow/docs/guides/specifying-exec-params\nWe need to specify and set execution parameters for cloud data flow .\n\nAlso, to enable autoscaling, set the following execution parameters when you start your pipeline:\n\n--autoscaling_algorithm=THROUGHPUT_BASED\n--max_num_workers=N"
      },
      {
        "date": "2021-11-29T06:58:00.000Z",
        "voteCount": 5,
        "content": "Answer is A: Dataflow is serverless, so no need to specify neither the number of workers, nor the max number of workers. https://cloud.google.com/dataflow"
      },
      {
        "date": "2023-05-02T12:02:00.000Z",
        "voteCount": 1,
        "content": "Have you ever use it? You pay for workers processing, so you specify max number of workers. Here is the doc: https://cloud.google.com/sdk/gcloud/reference/dataflow/jobs/run"
      },
      {
        "date": "2021-10-12T10:29:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-07-08T07:27:00.000Z",
        "voteCount": 3,
        "content": "Vote for D"
      },
      {
        "date": "2021-03-17T10:48:00.000Z",
        "voteCount": 3,
        "content": "D, thats because scalability is directly corerlated to max number of workers, size determines the speed of functioning"
      },
      {
        "date": "2021-02-07T09:48:00.000Z",
        "voteCount": 4,
        "content": "Correct D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 39,
    "url": "https://www.examtopics.com/discussions/google/view/17075-exam-professional-data-engineer-topic-1-question-39/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><img src=\"/assets/media/exam-media/04341/0003000006.png\" class=\"in-exam-image\"><br><br>Technical Requirements -<br>\u2711 Ensure secure and efficient transport and storage of telemetry data<br>\u2711 Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>\u2711 Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>\u2711 Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>You need to compose visualizations for operations teams with the following requirements:<br>\u2711 The report must include telemetry data from all 50,000 installations for the most resent 6 weeks (sampling once every minute).<br>\u2711 The report must not be more than 3 hours delayed from live data.<br>\u2711 The actionable report should only show suboptimal links.<br>\u2711 Most suboptimal links should be sorted to the top.<br>\u2711 Suboptimal links can be grouped and filtered by regional geography.<br>\u2711 User response time to load the report must be &lt;5 seconds.<br>Which approach meets the requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Google Sheets, use formulas to calculate a metric, and use filters/sorting to show only suboptimal links in a table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Google BigQuery tables, write Google Apps Script that queries the data, calculates the metric, and shows only suboptimal rows in a table in Google Sheets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Google Cloud Datastore tables, write a Google App Engine Application that queries all rows, applies a function to derive the metric, and then renders results in a table using the Google charts and visualization API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-20T20:22:00.000Z",
        "voteCount": 28,
        "content": "Answer: D"
      },
      {
        "date": "2020-04-18T09:51:00.000Z",
        "voteCount": 13,
        "content": "D; dataflow doesn't connect to datastore, and not really for reporting. BQ, and data studio is a better choice."
      },
      {
        "date": "2023-09-19T06:29:00.000Z",
        "voteCount": 1,
        "content": "Dataflow does connect to Datastore, D is still the right answer though."
      },
      {
        "date": "2023-10-22T09:27:00.000Z",
        "voteCount": 2,
        "content": "D. Load the data into Google BigQuery tables, write a Google Data Studio 360 report that connects to your data, calculates a metric, and then uses a filter expression to show only suboptimal rows in a table.\n\nHere's why this option is the most suitable:\n\nGoogle BigQuery is a powerful data warehouse for processing and analyzing large datasets. It can efficiently handle the telemetry data from all 50,000 installations.\nGoogle Data Studio 360 is designed for creating interactive and visually appealing reports and dashboards.\nUsing Google Data Studio allows you to connect to BigQuery, calculate the required metrics, and apply filters to show only suboptimal links.\nIt can provide real-time or near-real-time data updates, ensuring that the report is not more than 3 hours delayed from live data.\nGoogle Data Studio can also be used to sort and group suboptimal links and display them based on regional geography.\nWith the right design, you can ensure that user response time to load the report is less than 5 seconds.\nThis approach leverages Google's cloud services effectively to meet the specified requirements."
      },
      {
        "date": "2024-05-17T01:31:00.000Z",
        "voteCount": 1,
        "content": "Is Google Data studio 360  a product now?"
      },
      {
        "date": "2023-07-17T12:03:00.000Z",
        "voteCount": 3,
        "content": "Why bother with a custom GAE app when you have Data Studio?"
      },
      {
        "date": "2022-12-13T14:48:00.000Z",
        "voteCount": 2,
        "content": "Its think answer would be C because of telemetry data and response time is &lt;5 second that force me to think about datastore,"
      },
      {
        "date": "2022-06-09T21:28:00.000Z",
        "voteCount": 4,
        "content": "I believe the answer is C.\nFirst requirement is that it must be a visualisation with, so A and B do not work (create a table and a spreadsheet).\nNow the second constraint which I believe is important is that the report MUST load in less than 5 seconds. But we do not know how complex the metric computation is, thus I cannot assume that we can compute it when we want to load the report, making me think that it be must be pre-computed. Thus option D cannot work as it create the metric AFTER querying the data (we are also not sure if we can really compute it in a query)."
      },
      {
        "date": "2023-09-05T12:09:00.000Z",
        "voteCount": 1,
        "content": "Ummm, sorry @willymac2, but you have to account for size and growth which datastore cannot scale to. \nThen, you have to worry about sub-second response time and datastore cannot do that as well as BigQuery..."
      },
      {
        "date": "2022-05-07T05:05:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-04-13T05:15:00.000Z",
        "voteCount": 1,
        "content": "DataStudio and BQ are the simpliest way to do it"
      },
      {
        "date": "2022-04-04T17:46:00.000Z",
        "voteCount": 1,
        "content": "They also can activate BI Engine feature to improve the response time."
      },
      {
        "date": "2022-01-13T17:39:00.000Z",
        "voteCount": 2,
        "content": "D: Usually when a reporting tool is involved for GCP, DataStudio mostly goes by default due to it's no cost analytics and BigQuery joins it due to it's OLAP nature and the wonderful integration provided by GCP for these 2"
      },
      {
        "date": "2022-01-04T05:16:00.000Z",
        "voteCount": 1,
        "content": "as explained by JayZeeLee"
      },
      {
        "date": "2021-11-02T17:34:00.000Z",
        "voteCount": 4,
        "content": "D. \nA and B are incorrect, because Google Sheets are not the best fit to handle large amount of data. \nC may work, but it requires building an application which equates to more work. \nD is more efficient, therefore a better option."
      },
      {
        "date": "2022-11-14T05:10:00.000Z",
        "voteCount": 1,
        "content": "I can't think of a single compelling reason to go with anything but D, given the scope definition in the question brief."
      },
      {
        "date": "2021-10-12T19:28:00.000Z",
        "voteCount": 1,
        "content": "Visualization = Data Studio 360"
      },
      {
        "date": "2021-10-12T19:29:00.000Z",
        "voteCount": 1,
        "content": "Next question give you the answer: Question #40. They using Data Studio 360 and Bigquery as source"
      },
      {
        "date": "2021-10-12T10:33:00.000Z",
        "voteCount": 1,
        "content": "Ans: D"
      },
      {
        "date": "2021-07-08T07:42:00.000Z",
        "voteCount": 3,
        "content": "Vote for D"
      },
      {
        "date": "2021-05-18T06:56:00.000Z",
        "voteCount": 3,
        "content": "just check the next question (#40) to get an idea about correct answer"
      },
      {
        "date": "2021-03-12T16:06:00.000Z",
        "voteCount": 2,
        "content": "D\nCorrect"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 40,
    "url": "https://www.examtopics.com/discussions/google/view/17076-exam-professional-data-engineer-topic-1-question-40/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>Provide reliable and timely access to data for analysis from distributed research workers<br><img src=\"/assets/media/exam-media/04341/0003200005.png\" class=\"in-exam-image\"><br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>\u2711 Ensure secure and efficient transport and storage of telemetry data<br>\u2711 Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>\u2711 Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>\u2711 Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>You create a new report for your large team in Google Data Studio 360. The report uses Google BigQuery as its data source. It is company policy to ensure employees can view only the data associated with their region, so you create and populate a table for each region. You need to enforce the regional access policy to the data.<br>Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure all the tables are included in global dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure each table is included in a dataset for a region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the settings for each table to allow a related region-based security group view access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the settings for each view to allow a related region-based security group view access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdjust the settings for each dataset to allow a related region-based security group view access.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "BE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BE",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "BC",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-20T20:25:00.000Z",
        "voteCount": 48,
        "content": "Answer: B E"
      },
      {
        "date": "2021-05-22T03:18:00.000Z",
        "voteCount": 35,
        "content": "C is correct starting 2020, as BigQuery come with table level access control\nhttps://cloud.google.com/blog/products/data-analytics/introducing-table-level-access-controls-in-bigquery"
      },
      {
        "date": "2021-08-21T09:17:00.000Z",
        "voteCount": 11,
        "content": "Yes, the correct answer should be BC - since we can have table-level access and each region represents a table."
      },
      {
        "date": "2024-07-31T13:09:00.000Z",
        "voteCount": 1,
        "content": "C doesn't make sense, if you have already selected B"
      },
      {
        "date": "2024-05-19T22:13:00.000Z",
        "voteCount": 3,
        "content": "If I choose Option E, then Option C and D are eliminated because, Once you provide the access level in dataset for the use group, it applies for both Table and view.\n\nNow for remaining options A and B.\nThe Question itself has stated to include the table region wise in dataset, So Option A is eliminated.\nSo, B and E are Correct answers."
      },
      {
        "date": "2024-02-29T01:28:00.000Z",
        "voteCount": 6,
        "content": "BE... If we've already split the tables into regions via datasets then why give regional access through tables again.. If not, then AC but definitely not BC.. Please help"
      },
      {
        "date": "2023-11-07T21:35:00.000Z",
        "voteCount": 3,
        "content": "Answer : BE\n\nB. Ensure each table is included in a dataset for a region.\nThis means that you should organize your data in BigQuery into separate datasets, one for each region. Each dataset contains the tables specific to that region. This ensures that data is segregated by region.\nE. Adjust the settings for each dataset to allow a related region-based security group view access."
      },
      {
        "date": "2023-10-22T09:28:00.000Z",
        "voteCount": 1,
        "content": "B. Ensure each table is included in a dataset for a region.\n\nThis means that you should organize your data in BigQuery into separate datasets, one for each region. Each dataset contains the tables specific to that region. This ensures that data is segregated by region.\nE. Adjust the settings for each dataset to allow a related region-based security group view access.\n\nYou should set the access controls at the dataset level in BigQuery. This means configuring access permissions for each dataset based on regional security groups. This way, you can enforce the regional access policy to the data, ensuring that users from different regions can only access the data associated with their region.\nOption A is not necessary because you don't need to include all the tables in a global dataset. Segregating data into region-specific datasets is a better approach for enforcing access controls.\n\nOptions C and D are not typical actions in BigQuery. Access control and permissions are usually managed at the dataset level, and you can grant access to specific groups at that level."
      },
      {
        "date": "2023-07-24T02:36:00.000Z",
        "voteCount": 4,
        "content": "It's B and E or B and C. However, B and E makes some more sense because if you have one dataset for each region and they need to access the data for each region then why not allow them access to the whole dataset? What if you want to add other supplementary tables later? If you did that on a table level you would have to add access to every table separately. \n\nStill, I think both are valid because we don't have any extra requirement, but B E makes more sense."
      },
      {
        "date": "2023-05-02T12:16:00.000Z",
        "voteCount": 4,
        "content": "The intended answer was for sure BE. If C or D would be the right answers there is absolutely no reason to do B, right? Why should you put each table into separate dataset if you then set the accesss on table/view level? What is more the question is about tables not views, so I have no idea why would anybody take D. \nThe issue is that this question is out of date and now the right answer would be sole C."
      },
      {
        "date": "2023-04-24T05:38:00.000Z",
        "voteCount": 2,
        "content": "The two actions that should be taken are B and E.\nB. Ensure each table is included in a dataset for a region: By creating separate datasets for each region and including only the tables associated with that region, you can enforce the regional access policy.\n\nE. Adjust the settings for each dataset to allow a related region-based security group view access: By adjusting the settings for each dataset to allow only the related region-based security group view access, you can ensure that employees can only view data associated with their region.\n\nA is incorrect because including all tables in a global dataset would not enforce the regional access policy.\n\nC is incorrect because adjusting the settings for each table is not a scalable solution, especially as the number of tables grows.\n\nD is incorrect because adjusting the settings for each view does not ensure that employees can only view data associated with their region."
      },
      {
        "date": "2023-04-15T05:17:00.000Z",
        "voteCount": 2,
        "content": "B: Location is on dataset level: https://cloud.google.com/bigquery/docs/datasets#dataset_limitations\nC: IAM can be set on table level"
      },
      {
        "date": "2023-01-29T07:53:00.000Z",
        "voteCount": 3,
        "content": "Guys, \nthere are 2 possible combinations\nIf you think that each table represents a region, then they should all be in a global dataset and you should apply table access control to them.\nSo A+C\n\nOtherwise you would put each table in a regional dataset, and apply access control to the dataset. Why would you create a dataset for the purpose of controlling regional access, and then only apply the controls to a table inside it? that is not extensible in the future.\n Anyway create dataset+access control for dataset (B+E) is also valid.\n\nWhich to choose? I dont know."
      },
      {
        "date": "2023-01-26T04:41:00.000Z",
        "voteCount": 2,
        "content": "First put tables in region-dedicated dataset (B)\nThen, ensure access control at dataset level (by creating region-based security groups) (E)"
      },
      {
        "date": "2023-01-07T18:45:00.000Z",
        "voteCount": 3,
        "content": "I would vote for AC. As we already split the table for each region, why do we need to split the dataset per region? Furthermore, the access control will be provided to the users based on table level anyway."
      },
      {
        "date": "2023-01-07T18:50:00.000Z",
        "voteCount": 1,
        "content": "Oh, the location should be specified in the dataset level! Then, the dataset should be splitted by region, my bad!"
      },
      {
        "date": "2022-10-31T05:37:00.000Z",
        "voteCount": 7,
        "content": "if you create table-level access control and grant it to different groups for different tables, what is the point of putting tables in different datasets and different regions?\nSo i choose BE"
      },
      {
        "date": "2022-09-25T07:02:00.000Z",
        "voteCount": 2,
        "content": "BigQuery come with table level access control. Since we can have table-level access and each region represents a table, B &amp; C is correct answer."
      },
      {
        "date": "2022-08-28T16:03:00.000Z",
        "voteCount": 1,
        "content": "I voted for BC"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 41,
    "url": "https://www.examtopics.com/discussions/google/view/16659-exam-professional-data-engineer-topic-1-question-41/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>Ensure secure and efficient transport and storage of telemetry data<br><img src=\"/assets/media/exam-media/04341/0003400007.png\" class=\"in-exam-image\"><br>\u2711 Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>\u2711 Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>\u2711 Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>MJTelco needs you to create a schema in Google Bigtable that will allow for the historical analysis of the last 2 years of records. Each record that comes in is sent every 15 minutes, and contains a unique identifier of the device and a data record. The most common query is for all the data for a given device for a given day.<br>Which schema should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRowkey: date#device_id Column data: data_point\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRowkey: date Column data: device_id, data_point",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRowkey: device_id Column data: date, data_point",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRowkey: data_point Column data: device_id, date",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRowkey: date#data_point Column data: device_id"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-18T09:59:00.000Z",
        "voteCount": 89,
        "content": "None, rowkey should be Device_Id+Date(reverse)"
      },
      {
        "date": "2021-12-09T11:24:00.000Z",
        "voteCount": 21,
        "content": "A - \"Date#Device_Id\" is not the same that \"Timestamp#Device_Id\". If you want to query historical data, rowkey as \"2021-12-09#12345device\" is optimal design. Nevertheless, \"2021-12-09:09:10:47:2000#12345device\" isn't it. Each record has a date (2021-12-09) and unique devide id (12345, 12346, 12347...)."
      },
      {
        "date": "2020-07-13T23:02:00.000Z",
        "voteCount": 5,
        "content": "A is a better option then other ..though not perfect as you mentioned."
      },
      {
        "date": "2022-01-13T18:11:00.000Z",
        "voteCount": 1,
        "content": "Totally agree if we have to avoid hotspotting! , but, incase we need to choose one of the options below, would you be going for A?"
      },
      {
        "date": "2021-07-08T07:52:00.000Z",
        "voteCount": 4,
        "content": "For READ operation it's is correct. i.e. Date#Device (so that data read from single node) -\nFor write operation it should be DeviceID#Date (so that data write via multiple nodes)"
      },
      {
        "date": "2020-03-15T05:42:00.000Z",
        "voteCount": 19,
        "content": "think is A, since \u201cThe most common query is for all the data for a given device for a given day\u201d, rowkey should have info for both devcie and date."
      },
      {
        "date": "2021-09-09T05:56:00.000Z",
        "voteCount": 12,
        "content": "Google specifically mentions that it's a bad idea to use a timestamp at the start of a rowkey\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nThe answer really should be Device_id#Timestamp but with the answers we were given you would be better off leaving the timestamp out all together"
      },
      {
        "date": "2022-12-22T11:24:00.000Z",
        "voteCount": 2,
        "content": "I remember seeing it as well. the answer should be A. (reversed)"
      },
      {
        "date": "2022-11-20T06:07:00.000Z",
        "voteCount": 5,
        "content": "but it didnt say cant use date, date and timestamp are different"
      },
      {
        "date": "2023-08-26T02:34:00.000Z",
        "voteCount": 1,
        "content": "The date is even worse than timestamp for the problem of hot-spotting"
      },
      {
        "date": "2024-08-12T06:50:00.000Z",
        "voteCount": 1,
        "content": "A-https://cloud.google.com/bigtable/docs/schema-design-time-series?hl=es-419#use_tall_and_narrow_tables"
      },
      {
        "date": "2024-07-06T07:19:00.000Z",
        "voteCount": 1,
        "content": "showed up in my exam. picked A. passed the exam. still not sure it's correct though"
      },
      {
        "date": "2024-05-19T10:11:00.000Z",
        "voteCount": 1,
        "content": "A. Rowkey: date#device_id Column data: data_point\n\nExplanation:\n\nOptimized for Most Common Query: The most common query is for all data for a given device on a given day. This schema directly matches the query pattern by including both date and device_id in the row key. This enables efficient retrieval of the required data using a single row key prefix scan.\nScalability: As the number of devices and data points increases, this schema distributes the data evenly across nodes in the Bigtable cluster, avoiding hotspots and ensuring scalability.\nData Organization: By storing data points as column values within each row, you can easily add new data points or timestamps without modifying the table structure."
      },
      {
        "date": "2024-05-17T02:50:00.000Z",
        "voteCount": 1,
        "content": "Answer C:\nhttps://cloud.google.com/bigtable/docs/schema-design#time-based:~:text=Don%27t%20use%20a%20timestamp%20by%20itself%20or%20at%20the%20beginning%20of%20a%20row%20key%2C"
      },
      {
        "date": "2024-03-05T10:20:00.000Z",
        "voteCount": 2,
        "content": "c without any doubt"
      },
      {
        "date": "2024-02-04T10:24:00.000Z",
        "voteCount": 1,
        "content": "The right answer should be Reverse A, but since we don't have that, the best answer is C."
      },
      {
        "date": "2024-01-19T05:32:00.000Z",
        "voteCount": 3,
        "content": "C. This schema is best suited for historical analysis of device data over time when the most common query is to retrieve all data for a **specific device** on a **given day**.\n\n* **Row Key as `device_id`:** This allows for efficient retrieval of all data points related to a particular device in a single operation. Bigtable sorts data lexicographically by row key, so all data for a single device will be stored together.\n\n* **Column with `date` and `data_point`:** \n    - Using `date` as a column name or part of the column qualifier allows you to quickly filter and retrieve data for specific date ranges. \n    - Storing `data_point` as the column value provides the actual data associated with each timestamp.\n\n**Example:**\n\nWith this schema, a query to get all data for `device_12345` on `2023-12-20` would efficiently target the specific row key `device_12345` and fetch the relevant columns (with dates around `2023-12-20`)."
      },
      {
        "date": "2023-12-14T00:37:00.000Z",
        "voteCount": 1,
        "content": "C - the answer should  the right answer.\nKey is \"all the data for a given device for a given day\"\nas in, Device first, and all the data + data points after. \nThis has nothing to do with Date-based search."
      },
      {
        "date": "2023-11-07T22:38:00.000Z",
        "voteCount": 1,
        "content": "A - Key should be less granular item first to more granular item, there are more devices than date key (every 15 min)"
      },
      {
        "date": "2023-10-07T20:27:00.000Z",
        "voteCount": 1,
        "content": "the closest match to this in the provided options is:\n\nC. Rowkey: device_id Column data: date, data_point\n\nThus, option C would be the best choice from the given option"
      },
      {
        "date": "2023-05-25T11:05:00.000Z",
        "voteCount": 3,
        "content": "It all comes down to the most common query"
      },
      {
        "date": "2023-08-26T02:37:00.000Z",
        "voteCount": 1,
        "content": "Exactly\n\"all the data for a given device for a given day\" \nThat's why the answer is C. You start by selecting the device and then the date. This solution is not prone to hot-spotting, yours is."
      },
      {
        "date": "2023-01-26T04:51:00.000Z",
        "voteCount": 2,
        "content": "A. Rowkey: date#device_id Column data: data_point This schema would allow querying all data for a given device for a given day by looking up the row key, which would be the date followed by the device_id. This would be the most efficient way to access the data as it would be stored in sorted order by date and device_id."
      },
      {
        "date": "2023-01-17T22:54:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      },
      {
        "date": "2022-12-29T07:15:00.000Z",
        "voteCount": 2,
        "content": "I vote on A, none is the ideal answer as often here \nneeds to distribute data  within cluster by date and device. \nthis key will answer most used query - so OK for me"
      },
      {
        "date": "2022-12-15T18:45:00.000Z",
        "voteCount": 2,
        "content": "Answer : A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 42,
    "url": "https://www.examtopics.com/discussions/google/view/16660-exam-professional-data-engineer-topic-1-question-42/",
    "body": "Your company has recently grown rapidly and now ingesting data at a significantly higher rate than it was previously. You manage the daily batch MapReduce analytics jobs in Apache Hadoop. However, the recent increase in data has meant the batch jobs are falling behind. You were asked to recommend ways the development team could increase the responsiveness of the analytics without increasing costs. What should you recommend they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite the job in Pig.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite the job in Apache Spark.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the Hadoop cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the size of the Hadoop cluster but also rewrite the job in Hive."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T05:43:00.000Z",
        "voteCount": 35,
        "content": "I would say B since Apache Spark is faster than Hadoop/Pig/MapReduce"
      },
      {
        "date": "2024-03-18T08:01:00.000Z",
        "voteCount": 1,
        "content": "But it requires much more memory causing it more expensive, which is not what we're aiming for here.."
      },
      {
        "date": "2023-01-03T23:48:00.000Z",
        "voteCount": 18,
        "content": "Wow, a question that does not recommend to use Google product"
      },
      {
        "date": "2023-11-21T10:19:00.000Z",
        "voteCount": 1,
        "content": "Just a regular Spark. B"
      },
      {
        "date": "2023-11-17T01:30:00.000Z",
        "voteCount": 1,
        "content": "C. I think it should be C because intent of asking question is to realize the problem of on-prem auto-scaling not the optimization that we achieve using spark in-memory features. Its GCP exam they want to highlight if hadoop cluster commodity hard doesn't increase when data increases then it can create problem unlike GCP. Hence migrate to GCP."
      },
      {
        "date": "2023-07-10T06:12:00.000Z",
        "voteCount": 11,
        "content": "None. Being a GCP exam, it must be either Dataflow or BigQuery :D"
      },
      {
        "date": "2023-04-25T09:33:00.000Z",
        "voteCount": 5,
        "content": "I would like to take a moment to thank you all guys\nYou guys are awesome!!!"
      },
      {
        "date": "2022-12-22T11:27:00.000Z",
        "voteCount": 8,
        "content": "looks like he's trying to spark the company up."
      },
      {
        "date": "2023-07-10T06:10:00.000Z",
        "voteCount": 2,
        "content": "It seems he's not well paid."
      },
      {
        "date": "2022-12-19T16:11:00.000Z",
        "voteCount": 4,
        "content": "Both Pig &amp; Spark requires rewriting the code so its an additional overhead, but as an architect I would think about a long lasting solution. Resizing Hadoop cluster can resolve the problem statement for the workloads at that point in time but not on longer run. So Spark is the right choice, although its a cost to start with, it will certainly be a long lasting solution"
      },
      {
        "date": "2022-06-26T11:08:00.000Z",
        "voteCount": 2,
        "content": "Ans is B . Apache spark."
      },
      {
        "date": "2022-04-20T07:17:00.000Z",
        "voteCount": 4,
        "content": "SPARK &gt; hadoop, pig, hive"
      },
      {
        "date": "2022-02-09T16:31:00.000Z",
        "voteCount": 1,
        "content": "B - Apache Spark"
      },
      {
        "date": "2023-04-06T03:42:00.000Z",
        "voteCount": 2,
        "content": "https://www.ibm.com/cloud/blog/hadoop-vs-spark"
      },
      {
        "date": "2022-01-28T16:44:00.000Z",
        "voteCount": 1,
        "content": "B Spark for optimization and processing."
      },
      {
        "date": "2022-01-13T18:13:00.000Z",
        "voteCount": 1,
        "content": "B: Spark is suitable for the given operation is much more powerful"
      },
      {
        "date": "2022-01-04T05:28:00.000Z",
        "voteCount": 1,
        "content": "as explained by pr2web"
      },
      {
        "date": "2021-12-10T09:13:00.000Z",
        "voteCount": 1,
        "content": "Ans B: \nSpark is a 100 times faster and utilizes memory, instead of Hadoop Mapreduce's two-stage paradigm."
      },
      {
        "date": "2021-11-15T10:53:00.000Z",
        "voteCount": 1,
        "content": "B as Spark can improve the performance as it performs lazy in-memory execution.\nSpark is important because it does part of its pipeline processing in memory rather than copying from disk. For some applications, this makes Spark extremely fast."
      },
      {
        "date": "2021-11-15T10:54:00.000Z",
        "voteCount": 1,
        "content": "With a Spark pipeline, you have two different kinds of operations, transforms and actions. Spark builds its pipeline used an abstraction called a directed graph. Each transform builds additional nodes into the graph but spark doesn't execute the pipeline until it sees an action.\nSpark waits until it has the whole story, all the information. This allows Spark to choose the best way to distribute the work and run the pipeline. The process of waiting on transforms and executing on actions is called, lazy execution. For a transformation, the input is an RDD and the output is an RDD. When Spark sees a transformation, it registers it in the directed graph and then it waits. An action triggers Spark to process the pipeline, the output is usually a result format, such as a text file, rather than an RDD."
      },
      {
        "date": "2021-11-15T10:54:00.000Z",
        "voteCount": 3,
        "content": "Option A is wrong as Pig is wrapper and would initiate Map Reduce jobs\nOption C is wrong as it would increase the cost.\nOption D is wrong Hive is wrapper and would initiate Map Reduce jobs. Also, reducing the size would reduce performance."
      },
      {
        "date": "2022-10-19T02:58:00.000Z",
        "voteCount": 1,
        "content": "Wont Option B increase the cost ? Cost of re-writing the job in Spark + Cost of additional memory ?"
      },
      {
        "date": "2021-10-15T11:41:00.000Z",
        "voteCount": 2,
        "content": "Ans: B\nSpark performs better than MapReduce due to in memory processing."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 43,
    "url": "https://www.examtopics.com/discussions/google/view/16819-exam-professional-data-engineer-topic-1-question-43/",
    "body": "You work for a large fast food restaurant chain with over 400,000 employees. You store employee information in Google BigQuery in a Users table consisting of a FirstName field and a LastName field. A member of IT is building an application and asks you to modify the schema and data in BigQuery so the application can query a FullName field consisting of the value of the FirstName field concatenated with a space, followed by the value of the LastName field for each employee. How can you make that data available while minimizing cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view in BigQuery that concatenates the FirstName and LastName field values to produce the FullName.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a new column called FullName to the Users table. Run an UPDATE statement that updates the FullName column for each user with the concatenation of the FirstName and LastName values.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Cloud Dataflow job that queries BigQuery for the entire Users table, concatenates the FirstName value and LastName value for each user, and loads the proper values for FirstName, LastName, and FullName into a new table in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to export the data for the table to a CSV file. Create a Google Cloud Dataproc job to process the CSV file and output a new CSV file containing the proper values for FirstName, LastName and FullName. Run a BigQuery load job to load the new CSV file into BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T09:45:00.000Z",
        "voteCount": 66,
        "content": "Answer will be A because when you create View it does not store extra space and its a logical representation, for rest of the option you need to write large code and extra processing for dataflow/dataproc"
      },
      {
        "date": "2020-03-23T20:00:00.000Z",
        "voteCount": 11,
        "content": "Because views are not materialized, the query that defines the view is run each time the view is queried. Queries are billed according to the total amount of data in all table fields referenced directly or indirectly by the top-level query"
      },
      {
        "date": "2020-08-07T03:25:00.000Z",
        "voteCount": 3,
        "content": "Wouldn\u00b4t \"total amount of data in all table fields referenced directly or indirectly by the top-level query\" be FirstName and LastName?"
      },
      {
        "date": "2021-05-22T08:55:00.000Z",
        "voteCount": 4,
        "content": "You're right, BigQuery bills on number of bytes processed, regardless of them being materialized. If you don't create a new column and use a view instead, you will probably have a small performance hit but query costs would be the same and storage cost wouldn't increase (unlike storing a new column)"
      },
      {
        "date": "2021-09-12T11:12:00.000Z",
        "voteCount": 11,
        "content": "You are asked to modify the schema and data. By using a view, the underlined table remains intact."
      },
      {
        "date": "2022-09-08T08:48:00.000Z",
        "voteCount": 1,
        "content": "good catch, yoshik."
      },
      {
        "date": "2022-05-10T06:34:00.000Z",
        "voteCount": 3,
        "content": "Views are cached the same as regular tables are, so I don't get the point of billing. It will cost the same as query to a regular table."
      },
      {
        "date": "2022-11-28T10:13:00.000Z",
        "voteCount": 3,
        "content": "the point of billing is extra storage costs for a new concatenated column"
      },
      {
        "date": "2020-03-23T20:01:00.000Z",
        "voteCount": 5,
        "content": "Can't be A"
      },
      {
        "date": "2022-11-05T11:58:00.000Z",
        "voteCount": 2,
        "content": "I agree that A is correct. Also, I think B is wrong as the UPDATE statement is used to update values in existing columns, not to create a new column."
      },
      {
        "date": "2022-11-18T18:56:00.000Z",
        "voteCount": 2,
        "content": "Of course, you use UPDATE after creating the new column, that is what the option said"
      },
      {
        "date": "2023-02-21T12:36:00.000Z",
        "voteCount": 1,
        "content": "What happen if there are new employees joining the company, update every single time?"
      },
      {
        "date": "2021-02-19T10:48:00.000Z",
        "voteCount": 17,
        "content": "cannot be 'A'as it clearly says that you need to change the schema and data."
      },
      {
        "date": "2022-01-22T19:13:00.000Z",
        "voteCount": 15,
        "content": "Your primary task is to \"make data available\".\nChanging the schema is just the request from the member \"A member of IT is building an application and ***asks you to modify the schema and data*** in BigQuery\". You don't have to follow it if it does not make sense."
      },
      {
        "date": "2022-09-01T07:47:00.000Z",
        "voteCount": 4,
        "content": "A yes, That make a lot of sense and also if you update the table only once with UPDATE if there is a new employee it will not be up to date with the new column, if the app use a view it will be up to date every time it query.\nBut in any case the cost will not be minimized."
      },
      {
        "date": "2022-01-22T19:19:00.000Z",
        "voteCount": 6,
        "content": "There is always different application requirement to use different format. That way you will just creating more and more redundant columns in different formats. That is tedious."
      },
      {
        "date": "2021-03-12T16:37:00.000Z",
        "voteCount": 47,
        "content": "Correct: B\nBigQuery has no quota on the DML statements. (Search Google - does bigquery have quota for update).\nWhy not C: This is a one time activity and SQL is the easiest way to program it. DataFlow is way overkill for this. You will need to find an engineer who can develop DataFlow pipelines. Whereas, SQL is so much more widely known and easier. One of the great features about BigQuery is its SQL interface. Even for BigQueryML services."
      },
      {
        "date": "2021-05-22T08:56:00.000Z",
        "voteCount": 5,
        "content": "DML statements don't increase costs, but storing a new column does. I see A is correct (also see my comment above)"
      },
      {
        "date": "2022-01-22T19:15:00.000Z",
        "voteCount": 2,
        "content": "Exactly. Cost is the reason to reject B.\nHow come so many people vote for this wrong option?"
      },
      {
        "date": "2023-01-03T23:50:00.000Z",
        "voteCount": 7,
        "content": "Storage is cheap compared to computation"
      },
      {
        "date": "2022-12-13T15:04:00.000Z",
        "voteCount": 2,
        "content": "But you need to maintain table means regularly you have to execute the update query whenever new data comes."
      },
      {
        "date": "2021-05-22T09:01:00.000Z",
        "voteCount": 8,
        "content": "I will also add that B would imply changing upstream workloads to write the new field every time a records gets added"
      },
      {
        "date": "2024-07-31T13:31:00.000Z",
        "voteCount": 1,
        "content": "E. Say to the IT specialist to take care of it on the app side...\nB would work for historical data if we had an underlying change made to automate the concatenation for new records. It is not clear, so I would say A is a quick solution."
      },
      {
        "date": "2024-05-01T00:16:00.000Z",
        "voteCount": 1,
        "content": "Requirement is to be able to filter on full name. So, you would be querying all data unless you have materialized full name column."
      },
      {
        "date": "2024-01-28T06:25:00.000Z",
        "voteCount": 1,
        "content": "Definitely A"
      },
      {
        "date": "2023-11-21T10:54:00.000Z",
        "voteCount": 2,
        "content": "The question might be outdated, but I would like to offer my perspective:\n\n1. Ideally, I would opt for a materialized view to avoid updating pipelines\n2. In 2023, I see no concerns regarding the costs involved in storing denormalized data for analytical needs\n3. Regarding this question I would choose option A, although the concern about extra costs due to recalculations is valid for me"
      },
      {
        "date": "2023-12-19T02:26:00.000Z",
        "voteCount": 2,
        "content": "Did u pass the exam ?"
      },
      {
        "date": "2023-11-09T01:11:00.000Z",
        "voteCount": 1,
        "content": "Answer should be A 'cos the First request is: make that data available."
      },
      {
        "date": "2023-08-05T10:18:00.000Z",
        "voteCount": 1,
        "content": "Its A ..... \"asked to change schema\" is a trick to test your skills. Better to make use of MV's if anyhow the application is gonna query repeatedly. MV's will rebuild itself, if query invalidates from cache results"
      },
      {
        "date": "2023-07-30T18:05:00.000Z",
        "voteCount": 2,
        "content": "In the case of B, the data pipeline that adds new employee information must also be modified, which is not the correct answer in terms of cost minimization."
      },
      {
        "date": "2023-07-24T03:19:00.000Z",
        "voteCount": 1,
        "content": "It's A. If you add a column to the table, you will be billed every time you query that new column. The same way you would be billed with the view created by A. \n\nB,C and D create a new column. A does not create a new column. It just provides the interface for the application to access the data. B,C and D will have to be rerun to compute the column value of new customers. \n\nA is done only once, costs 0 for storage, and is charged about the same as all the others when it comes to compute because even if you choose B C and D you would have to query the data in the end anyway."
      },
      {
        "date": "2023-07-22T04:02:00.000Z",
        "voteCount": 1,
        "content": "modify the schema"
      },
      {
        "date": "2023-07-17T12:12:00.000Z",
        "voteCount": 1,
        "content": "Can you code a script for a BQ Column? I don't think it's \"B\", it is pretty tricky"
      },
      {
        "date": "2023-06-27T05:07:00.000Z",
        "voteCount": 1,
        "content": "Everything but A) new view is wrong.\n\nB) sounds okay, but introduces a new column which means more storage, thus increasing cost.\nC) Dataflow is obvious overkill for a simple task such as concatenating two strings.\nD) Starting up a Dataproc cluster just for string concatenation is super overkill."
      },
      {
        "date": "2023-05-21T07:42:00.000Z",
        "voteCount": 1,
        "content": "if a new field is only necessary for one project, and it is only the concatenation of two existing fields, it is ok to create a view that gets used for a specific task."
      },
      {
        "date": "2023-04-25T12:39:00.000Z",
        "voteCount": 2,
        "content": "I'd go for A.\nThe main issue with answers B,C,D is that they are just temporary solution. Whenever a new employee comes in (there are 400.000 of them at the moment, so we can expect every day a few new guys) we need to update the fullname table/field again. Additionally each of these answers need twice as much capacity (BigQuery stores data in a columnar format, so optimizing is not possible). Although the price for the needed capacity will be far below 0.01$/month.\nThe main argument against A is that compute power costs more than capacity. Please look how BQ is priced: https://cloud.google.com/bigquery/pricing#query_pricing\nIn the default On-demand compute pricing it is charged for \"the number of bytes processed by each query\" so there will be no any difference in computing costs for any option.\nYeah, there is also this argument about modyfing schema in the requirements. Lets be professional - it is not a requirement for OUR schema. If you can resolve the issue with 0 change to YOUR schema then you are more than ok. And anyway, from requestor point of view, the schema HE uses in his app will be modifed as he needed."
      },
      {
        "date": "2023-04-22T23:43:00.000Z",
        "voteCount": 1,
        "content": "Should be A."
      },
      {
        "date": "2023-04-11T13:31:00.000Z",
        "voteCount": 2,
        "content": "I vote B.\n\nA is expensive, the requirements say we need to minimize cost. \n\nB works and meets the requirements. We create an empty column and then UPDATE it to set it to a desired values.\n\nhttps://cloud.google.com/bigquery/docs/managing-table-schemas#console\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#update_statement"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 44,
    "url": "https://www.examtopics.com/discussions/google/view/79762-exam-professional-data-engineer-topic-1-question-44/",
    "body": "You are deploying a new storage system for your mobile application, which is a media streaming service. You decide the best fit is Google Cloud Datastore. You have entities with multiple properties, some of which can take on multiple values. For example, in the entity 'Movie' the property 'actors' and the property<br>'tags' have multiple values but the property 'date released' does not. A typical query would ask for all movies with actor=&lt;actorname&gt; ordered by date_released or all movies with tag=Comedy ordered by date_released. How should you avoid a combinatorial explosion in the number of indexes?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually configure the index in your index config as follows: <img src=\"/assets/media/exam-media/04341/0003700001.png\" class=\"in-exam-image\">\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually configure the index in your index config as follows: <img src=\"/assets/media/exam-media/04341/0003700002.png\" class=\"in-exam-image\">",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the following in your entity options: exclude_from_indexes = 'actors, tags'",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the following in your entity options: exclude_from_indexes = 'date_published'"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-13T15:34:00.000Z",
        "voteCount": 7,
        "content": "Correct answer is A \nRead in reference : https://cloud.google.com/datastore/docs/concepts/indexes#index_limits\nn this case, you can circumvent the exploding index by manually configuring an index in your index configuration file:\nindexes:\n- kind: Task\n  properties:\n  - name: tags\n  - name: created\n- kind: Task\n  properties:\n  - name: collaborators\n  - name: created\nThis reduces the number of entries needed to only (|tags| * |created| + |collaborators| * |created|), or 6 entries instead of 9"
      },
      {
        "date": "2022-12-20T04:27:00.000Z",
        "voteCount": 1,
        "content": "you can circumvent the exploding index by manually configuring an index in your index configuration file:\n\nhttps://cloud.google.com/datastore/docs/concepts/indexes#index_limits"
      },
      {
        "date": "2022-12-19T16:22:00.000Z",
        "voteCount": 3,
        "content": "Tempted to go with D as the syntax in Option A seems incorrect. D is still a possible answer because one of the ways to get rid of index errors is to remove the entities that are causing the index to explode. In this case its date_released and hence D appears right to me"
      },
      {
        "date": "2022-12-13T15:12:00.000Z",
        "voteCount": 3,
        "content": "Option B &amp; D reject because mention date_publised in question date_released is column\nOption C also not correct, I would go with option A."
      },
      {
        "date": "2022-09-28T09:30:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer D:\n\nThis is the way the DB is typically queried:\n- movies with actor=&lt;actorname&gt; ordered by date_released \n- movies with tag=Comedy ordered by date_released\n\nso it seems that we need indices in actor,tag and date_released for sorting. \n\n\u274c A: this would be the correct answer, however, the format is incorrect, the correct format would be '- name: date_released' correctly indented.\n\n\u274c B: This seems to be unnecessary, since typically actor and tag are not queried together. also, there is a clear indentation issue\n\n\u274c C: We don't want to ignore actor and tag, we need those indices.\n\n\u2705 D: If we leave datastore to automatically create the indices and if we specify that the 'date_released' property needs to be excluded from indices, then we would have less indices (but maybe slower queries when ordering them, but hey, how many 'comedies' there could be in the world)"
      },
      {
        "date": "2022-09-28T09:34:00.000Z",
        "voteCount": 3,
        "content": "And here is the correct way to configure indices:\nhttps://cloud.google.com/datastore/docs/tools/indexconfig\n\nso this would be the best answer:\nindexes:\n- kind: Movie\n  properties:\n  - name: actors\n  - name: date_released\n    direction: asc. &lt;This could be left out, it defaults to direction: asc if excluded&gt;\n\n- kind: Movie\n  properties:\n  - name: tag\n  - name: date_released\n    direction: asc. &lt;This could be left out, it defaults to direction: asc if excluded&gt;"
      },
      {
        "date": "2022-09-28T09:30:00.000Z",
        "voteCount": 1,
        "content": "*Findings for this answer*:\nIndices, if not defined, will be automatically created:\n\"By default, a Datastore mode database automatically predefines an index for each property of each entity kind. These single property indexes are suitable for simple types of queries.\"\nsource: https://cloud.google.com/datastore/docs/concepts/indexes\n\nIn the index limits section we see this:\n\"a Datastore mode database creates an entry in a predefined index for every property of every entity except those you have explicitly declared as excluded from your indexes.\"\nsource: https://cloud.google.com/datastore/docs/concepts/indexes#index_limits"
      },
      {
        "date": "2022-09-18T08:43:00.000Z",
        "voteCount": 1,
        "content": "What do people think about C? The question is asking how to avoid a  combinatorial explosion in the number of indexes. It says \"You have entities with multiple properties, some of which can take on multiple values\". Put this with the below text from the documentation for Datastore indexes, it seems they're looking for \"exclude the properties that will cause combinatorial explosion\" which would be C.\n\n\"The situation becomes worse in the case of entities with multiple properties, each of which can take on multiple values. To accommodate such an entity, the index must include an entry for every possible combination of property values. Custom indexes that refer to multiple properties, each with multiple values, can \"explode\" combinatorially, requiring large numbers of entries for an entity with only a relatively small number of possible property values.\"[1]\n[1] https://cloud.google.com/datastore/docs/concepts/indexes#index_limits"
      },
      {
        "date": "2022-09-06T04:59:00.000Z",
        "voteCount": 1,
        "content": "B. is correct\nTo avoid combinatoric explosion of indexes.\n\"Two queries of the same form but with different filter values use the same index.\"\nhttps://cloud.google.com/datastore/docs/concepts/indexes"
      },
      {
        "date": "2022-09-13T15:36:00.000Z",
        "voteCount": 1,
        "content": "Correct answer is A \nIn the same reference you provided \nIn this case, you can circumvent the exploding index by manually configuring an index in your index configuration file:\nindexes:\n- kind: Task\nproperties:\n- name: tags\n- name: created\n- kind: Task\nproperties:\n- name: collaborators\n- name: created\nThis reduces the number of entries needed to only (|tags| * |created| + |collaborators| * |created|), or 6 entries instead of 9"
      },
      {
        "date": "2022-09-03T05:26:00.000Z",
        "voteCount": 1,
        "content": "A. Manually configure the index in your index config as follows:"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 45,
    "url": "https://www.examtopics.com/discussions/google/view/17080-exam-professional-data-engineer-topic-1-question-45/",
    "body": "You work for a manufacturing plant that batches application log files together into a single log file once a day at 2:00 AM. You have written a Google Cloud<br>Dataflow job to process that log file. You need to make sure the log file in processed once per day as inexpensively as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the processing job to use Google Cloud Dataproc instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tManually start the Cloud Dataflow job each morning when you get into the office.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron job with Google App Engine Cron Service to run the Cloud Dataflow job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure the Cloud Dataflow job as a streaming job so that it processes the log data immediately."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-20T23:34:00.000Z",
        "voteCount": 22,
        "content": "Answer: C"
      },
      {
        "date": "2020-03-27T07:22:00.000Z",
        "voteCount": 13,
        "content": "Answer: C\nDescription: Scheduler for adhoc jobs \u2013 3 jobs free and $0.10 per job"
      },
      {
        "date": "2023-11-22T02:32:00.000Z",
        "voteCount": 1,
        "content": "Service was renamed, but the answer is still - C"
      },
      {
        "date": "2023-10-07T20:41:00.000Z",
        "voteCount": 1,
        "content": "C. Using the Google App Engine Cron Service to run the Cloud Dataflow job allows you to automate the execution of the job. By creating a cron job, you can ensure that the Dataflow job is triggered exactly once per day at a specified time. This approach is automated, reliable, and fits the requirement of processing the log file once per day."
      },
      {
        "date": "2023-07-10T06:40:00.000Z",
        "voteCount": 5,
        "content": "C. For a modern solution, Cloud Scheduler"
      },
      {
        "date": "2023-05-29T17:11:00.000Z",
        "voteCount": 2,
        "content": "Currently, Cloud Scheduler takes over the scheduling functions."
      },
      {
        "date": "2023-02-22T05:27:00.000Z",
        "voteCount": 2,
        "content": "I don't understand why that dataflow is used for processing? even though it should be processed once per a day?? is it more suitable for processing by using Dataproc instead?"
      },
      {
        "date": "2024-05-17T03:15:00.000Z",
        "voteCount": 2,
        "content": "Actually, google recommends Dataflow over Dataproc for both batch and streaming. Dataproc is only recommended if you are coming from hadoop, spark, ...."
      },
      {
        "date": "2023-01-09T05:52:00.000Z",
        "voteCount": 5,
        "content": "C was correct but nowadays you'd schedule a Dataflow job with Cloud Scheduler: https://cloud.google.com/community/tutorials/schedule-dataflow-jobs-with-cloud-scheduler"
      },
      {
        "date": "2022-09-28T09:55:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: C.\n\n\u274c A: Dataproc is a managed Apache Spark and Apache Hadoop service, makes no sense to use it\n\n \u274c B: This might sound as the cheapest, but is highly error prone, besides, anyone in charge of this has a salary and I doubt is a low one.\n\n\u2705 C: This is the easiest/fastest/cheapest way to trigger job runs, you can even set retry attempts.\nsource: https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml.\n\n\u274c D: Setting this would be much more expensive than the cron-job"
      },
      {
        "date": "2022-06-20T06:34:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2021-10-15T11:53:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-10-12T19:46:00.000Z",
        "voteCount": 3,
        "content": "I know probably this question is testing on if you know corn.yaml and its function in App Engine. But why B will be more expensive? Human capital cost? Let's say if hiring a person click the button will be cheaper than launch an app engine, should we reconsider B?"
      },
      {
        "date": "2022-06-06T07:44:00.000Z",
        "voteCount": 3,
        "content": "Would you rather pay someone $100,000 a year to click 'run' on jobs all day, or have them automate it and do more cutting edge work? This would be opportunity cost."
      },
      {
        "date": "2021-09-29T10:21:00.000Z",
        "voteCount": 3,
        "content": "Scheduling Jobs with cron.yaml\n\nFree applications can have up to 20 scheduled tasks. Paid applications can have up to 250 scheduled tasks."
      },
      {
        "date": "2021-06-27T07:07:00.000Z",
        "voteCount": 2,
        "content": "Vote for 'C'"
      },
      {
        "date": "2021-02-07T10:28:00.000Z",
        "voteCount": 3,
        "content": "Correct C"
      },
      {
        "date": "2020-11-19T08:47:00.000Z",
        "voteCount": 5,
        "content": "Answer is C. https://cloud.google.com/appengine/docs/flexible/nodejs/scheduling-jobs-with-cron-yaml"
      },
      {
        "date": "2020-08-18T17:20:00.000Z",
        "voteCount": 4,
        "content": "C Correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 46,
    "url": "https://www.examtopics.com/discussions/google/view/16253-exam-professional-data-engineer-topic-1-question-46/",
    "body": "You work for an economic consulting firm that helps companies identify economic trends as they happen. As part of your analysis, you use Google BigQuery to correlate customer data with the average prices of the 100 most common goods sold, including bread, gasoline, milk, and others. The average prices of these goods are updated every 30 minutes. You want to make sure this data stays up to date so you can combine it with other data in BigQuery as cheaply as possible.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data every 30 minutes into a new partitioned table in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore and update the data in a regional Google Cloud Storage bucket and create a federated data source in BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in Google Cloud Datastore. Use Google Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Cloud Datastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-11T06:31:00.000Z",
        "voteCount": 42,
        "content": "this is one of the sample exam questions that google has on their website. The correct answer is B"
      },
      {
        "date": "2024-08-12T01:58:00.000Z",
        "voteCount": 1,
        "content": "B - since it seems that not all data is in BigQuery but the analysis is done using BigQuery so federated query is the optimal approach"
      },
      {
        "date": "2020-03-27T07:27:00.000Z",
        "voteCount": 12,
        "content": "Answer: B\nDescription: B is correct because regional storage is cheaper than BigQuery storage."
      },
      {
        "date": "2021-02-19T11:04:00.000Z",
        "voteCount": 9,
        "content": "it's not only cheaper but the requirement is that the data keep updating every 30 min and you need to combine the data in bigquery, use external tables to do that is the recommended practice"
      },
      {
        "date": "2024-04-21T18:55:00.000Z",
        "voteCount": 1,
        "content": "D. Store the data in a file in a regional Google Cloud Storage bucket. Use Cloud Dataflow to query BigQuery and combine the data programmatically with the data stored in Google Cloud Storage.\n\nHere's why this approach is ideal:\n\nCost-Effective Storage: Cloud Storage offers regional storage classes that are cost-effective for frequently accessed data. Storing the price data in a regional Cloud Storage bucket keeps it readily available.\n\nCloud Dataflow for Updates: Cloud Dataflow is a managed service for building data pipelines. You can create a Dataflow job that runs every 30 minutes to:\n\nDownload the latest economic data file from Cloud Storage.\nProcess and potentially transform the data as needed.\nLoad the updated data into BigQuery.\nBigQuery Integration: BigQuery seamlessly integrates with Cloud Dataflow. The Dataflow job can directly load the processed data into a BigQuery table for further analysis with your customer data."
      },
      {
        "date": "2023-12-14T05:30:00.000Z",
        "voteCount": 2,
        "content": "BigQuery supports partitioned tables, which allow for efficient querying and management of large datasets that are updated frequently. By loading the updated data into a new partition every 30 minutes, you can ensure that only relevant partitions are queried, reducing the amount of data processed and thereby minimizing costs.\nWhat's wrong with B ?  While creating a federated data source in BigQuery pointing to a Google Cloud Storage bucket is feasible, it might not be the most efficient for data that is updated every 30 minutes. Querying federated data sources can sometimes be more expensive and less performant than querying data stored directly in BigQuery."
      },
      {
        "date": "2023-04-26T16:52:00.000Z",
        "voteCount": 1,
        "content": "Federated queries let you send a query statement to Cloud Spanner or Cloud SQL databases not to cloud storage"
      },
      {
        "date": "2023-06-19T16:26:00.000Z",
        "voteCount": 3,
        "content": "Is you are right about \"federated queries\", but the option B says about \"federated data source\". These are different concepts"
      },
      {
        "date": "2023-04-19T05:59:00.000Z",
        "voteCount": 1,
        "content": "ChatGPT says partitioned tables is the best approach, The answers here are quite contrasting with that answer, Even i thought it has to be option A, I am so confused now? Any proper straight forward answer ?"
      },
      {
        "date": "2023-02-23T13:49:00.000Z",
        "voteCount": 1,
        "content": "Answer B:\nUploading data into staging tables/ external tables or federated source in BQ is the best approach. \nOption A is also good approach, anyone can explain about his part what is wrong about this?"
      },
      {
        "date": "2023-03-05T02:29:00.000Z",
        "voteCount": 7,
        "content": "we can't implement A, it's because biquery partition table can only be done minimun in range 1 hour, the requirement said it must be update every 30 minutes, so A is imposible option as the minimum partition is in hour level"
      },
      {
        "date": "2023-01-04T06:10:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-12-19T16:29:00.000Z",
        "voteCount": 2,
        "content": "Discounting A due to limitations on partitions\nDiscounting C because datastore does not fit into the nature of data we are talking about and federation between BQ and datastore it an overkill\nBetween B and D, updating the price file on GCS and joining BQ tables and external tables sourcing data from GCS is most cost optimal way for this use case"
      },
      {
        "date": "2023-01-12T18:22:00.000Z",
        "voteCount": 1,
        "content": "D is also overkill for this use case, so I'd pick B"
      },
      {
        "date": "2022-12-17T07:08:00.000Z",
        "voteCount": 1,
        "content": "Consideration: As cheaply as possible. Make sure data stays up to date.\n\nInitially chose A. But in actuality there is no need to maintain or store past data so storage of past data and partitioning doesn't seem like a key requirement.\n\nInstead we can connect just to a single Cloud Storage file, either by:\ni. replace previous prices with latest prices\nii. store previous prices in GCS if required to be retained"
      },
      {
        "date": "2022-12-13T15:25:00.000Z",
        "voteCount": 1,
        "content": "B is most inexpensive approach."
      },
      {
        "date": "2022-12-11T02:48:00.000Z",
        "voteCount": 2,
        "content": "The technical requirement is having frequently access info to join with other BQ data, as cheap as possible. B fits perfectly.\nCorner cases for external data sources:\n\t\u2022 Avoiding duplicate data in BigQuery storage\n\t\u2022 Queries that do not have strong performance requirements\n\t\u2022 Small amount of frequently changing data to join with other tables in BigQuery\nhttps://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer"
      },
      {
        "date": "2022-11-21T06:56:00.000Z",
        "voteCount": 1,
        "content": "I would say D, regional Google Cloud Storage bucket - cheap.\nA - not cheap\nB - NoSQL database for your web and mobile applications\nC - Federated queries let you send a query statement to Cloud Spanner or Cloud SQL databases\nAnd we need to combine data in DQ with data from bucket"
      },
      {
        "date": "2022-10-24T12:38:00.000Z",
        "voteCount": 2,
        "content": "according to this : \nhttps://cloud.google.com/bigquery/docs/external-data-sources\nFederated queries don't work with Cloud Storage.\nhow can it be B ?"
      },
      {
        "date": "2022-11-05T14:22:00.000Z",
        "voteCount": 1,
        "content": "Correct, it cannot be B because BQ federated queries only work with Cloud SQL or Spanner"
      },
      {
        "date": "2022-11-16T12:29:00.000Z",
        "voteCount": 1,
        "content": "It seems to me that they do: https://cloud.google.com/bigquery/docs/external-data-cloud-storage"
      },
      {
        "date": "2022-08-28T16:07:00.000Z",
        "voteCount": 2,
        "content": "I voted for B"
      },
      {
        "date": "2022-06-15T08:58:00.000Z",
        "voteCount": 1,
        "content": "The average prices of these goods are updated every 30 minutes\n--&gt; No Cloud Storage"
      },
      {
        "date": "2022-06-09T09:44:00.000Z",
        "voteCount": 2,
        "content": "need to udpate the data"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 47,
    "url": "https://www.examtopics.com/discussions/google/view/16820-exam-professional-data-engineer-topic-1-question-47/",
    "body": "You are designing the database schema for a machine learning-based food ordering service that will predict what users want to eat. Here is some of the information you need to store:<br>\u2711 The user profile: What the user likes and doesn't like to eat<br>\u2711 The user account information: Name, address, preferred meal times<br>\u2711 The order information: When orders are made, from where, to whom<br>The database will be used to store all the transactional data of the product. You want to optimize the data schema. Which Google Cloud Platform product should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 32,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T12:12:00.000Z",
        "voteCount": 57,
        "content": "You want to optimize the data schema + Machine Learning --&gt; Bigquery. So A"
      },
      {
        "date": "2021-09-14T02:39:00.000Z",
        "voteCount": 25,
        "content": "BigQuery is a datawarehouse, not a transactional db. You need to store transactional data as a requirement."
      },
      {
        "date": "2022-07-16T07:59:00.000Z",
        "voteCount": 4,
        "content": "Biquery Supports transactions:\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/transactions\n, but indeed is not a good DB for OLTP.\n\nBut I would said or CloudSQL or BigQuery"
      },
      {
        "date": "2021-10-07T02:41:00.000Z",
        "voteCount": 7,
        "content": "In my opinion transactional data doesnt mean transactions they could be grouped so there is no need to write register by register."
      },
      {
        "date": "2021-10-16T13:37:00.000Z",
        "voteCount": 5,
        "content": "In other questions they talk about 'transactional log data' when referring to past transactions, but you could be right, agree. In that case ok A BigQuery. Nevertheless, the question is formulated ambiguously."
      },
      {
        "date": "2020-03-21T00:36:00.000Z",
        "voteCount": 26,
        "content": "Answer: Should be D - Datastore"
      },
      {
        "date": "2020-11-14T04:35:00.000Z",
        "voteCount": 20,
        "content": "There is SQLML with BigQuery, you know that?\nYou cannot optimize a schema in datastore, it is a NoSQL document database built for automatic scaling, high performance, and ease of application development. It does not work based on schemas!"
      },
      {
        "date": "2021-12-04T04:36:00.000Z",
        "voteCount": 7,
        "content": "BQML is there. But, In question do they want to do ML on BQ?? Its saying just ML Based Company."
      },
      {
        "date": "2023-06-07T04:53:00.000Z",
        "voteCount": 5,
        "content": "It was also a difficult one for Chat GPT, it did give different answers each time I inquiry more about the question. After a few iterations, we also agreed on \"D\" :) - because;\n\nIn the context of a food ordering service, storing data about what a user likes and doesn't like to eat can potentially involve a varied and dynamic set of data. Some users might have a long list of food preferences, while others might have only a few. Some users might update their likes and dislikes frequently, while others rarely or never. This kind of data is a good match for a NoSQL database like Datastore, which can easily accommodate such variations."
      },
      {
        "date": "2024-09-22T02:22:00.000Z",
        "voteCount": 1,
        "content": "The details of the information definitely look suited to noSql to me, so that means C or D. Datastore is designed for this sort of thing - transactional nosql for an App. I took the question to mean \"the machine learning app already exists\" so the fact bigquery allows ML isn't relevant. It would be a leap to assume that the ML is done in Bigquery (I have a current Google ML pro cert, and this wouldn't say bigquery to me from that cert)"
      },
      {
        "date": "2024-08-27T04:56:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL is a fully-managed relational database service that supports MySQL, PostgreSQL, and SQL Server. It is well-suited for transactional workloads, allowing you to store structured data with relationships between different entities, such as users, orders, and profiles."
      },
      {
        "date": "2024-05-19T11:18:00.000Z",
        "voteCount": 5,
        "content": "he best answer for this scenario is B. Cloud SQL. Here's why:\n\nRelational Data: The information you need to store (user profile, account information, order information) is highly structured and relational. Cloud SQL, being a relational database service, is designed to handle this type of data efficiently.\nTransactional Workloads: Food ordering involves transactional operations (placing orders, updating user preferences, etc.). Cloud SQL is optimized for transactional workloads, ensuring data consistency and integrity.\nEase of Use: Cloud SQL is a managed service, meaning Google handles maintenance, updates, and backups, making it easier to manage than some other options.\nIntegration with Machine Learning: Cloud SQL can easily integrate with other Google Cloud Platform products like BigQuery and Vertex AI, which are crucial for machine learning tasks."
      },
      {
        "date": "2024-03-13T23:04:00.000Z",
        "voteCount": 1,
        "content": "BigQuery is a fully managed, serverless data warehouse that enables scalable analysis of large datasets. It is designed to handle large volumes of data and support complex queries, making it suitable for storing transactional data and performing analytics. With BigQuery, you can optimize your data schema and easily scale as your data grows. Additionally, BigQuery integrates well with other Google Cloud Platform services, including machine learning services, enabling you to build advanced analytics and predictive models on your transactional data."
      },
      {
        "date": "2024-01-28T08:25:00.000Z",
        "voteCount": 1,
        "content": "C\nIt says that the database will be used to store the transactions data. BigQuery is not usually characterized as a data storage system. Also a databased is used for storing transactional Data not a Data Wharehouse."
      },
      {
        "date": "2024-01-28T08:21:00.000Z",
        "voteCount": 1,
        "content": "My choice is C\nIt says \"DataBase Schema\" not DataWharehouse Schema. It didn't mention if the ML is to be done in the DataBase or not, it just states that a database is to be created."
      },
      {
        "date": "2023-12-15T08:46:00.000Z",
        "voteCount": 3,
        "content": "I asked ChatGPT this question. \nIt first answered Datastore.\nI said the question asks us to optimize the data schema and Datastore has no schema.\nThen it answered CloudSQL.\nI said the question asks about Machine learning aspect.\nThen it answered Bigquery.\nI said its a food ordering service and must need low latency\nThen it answered Bigtable.\nGPT is clearly not a good tool to use for the prep please avoid it. Its flawed currently that is DEC2023!"
      },
      {
        "date": "2023-12-19T03:04:00.000Z",
        "voteCount": 1,
        "content": "Hello Camaro, when u going to take the exam ? am appearing on 23rd Dec and keen to know if this set is still valid ?"
      },
      {
        "date": "2023-11-08T16:33:00.000Z",
        "voteCount": 1,
        "content": "A. BigQuery -&gt; Most Probably the answer because for analytic it should automatically be big query. But my question is why not others. I've used BigQuery and I know that it allows schema optimization as well as one of it's feature is built in machine learning.\nMore here:https://cloud.google.com/bigquery/docs/introduction\n\nB. Cloud SQL -&gt; I am not being about to find the doc for cloud sql where it talks about being able to optimize the schema and if it can be used for machine learning applications.\n\nC. Cloud Bigtable -&gt; Possible answer because there's schema and can be used for machine learning application.\nDoc: https://cloud.google.com/bigtable/docs/overview\n\nD. Cloud Datastore -&gt; Wrong because Datastore doesn\u2019t have a schema.\nDoc: https://cloud.google.com/datastore/docs/concepts/overview"
      },
      {
        "date": "2023-09-27T07:52:00.000Z",
        "voteCount": 1,
        "content": "The right answer is D because Datastore is transactional, scalable, and can deliver an output to ML."
      },
      {
        "date": "2023-08-22T07:24:00.000Z",
        "voteCount": 1,
        "content": "Although you could argue with the formulation of the question, I did also read that this is about a transactional database which BigQuery is not. Thus I would go with Cloud SQL."
      },
      {
        "date": "2023-08-05T10:41:00.000Z",
        "voteCount": 1,
        "content": "Initially I also thought it would be D, but then I re-read &amp; it mentions the whole question is about Data Analytics, hence it has to be A... BigQuery"
      },
      {
        "date": "2023-07-31T09:49:00.000Z",
        "voteCount": 1,
        "content": "A  sahi hai"
      },
      {
        "date": "2023-07-30T18:24:00.000Z",
        "voteCount": 1,
        "content": "to store all the transactional data of the product"
      },
      {
        "date": "2023-07-21T07:11:00.000Z",
        "voteCount": 2,
        "content": "BigQuery is right.\n\nCloud SQL: Not convenient to build ML models on top of it. Also it is a relational database. So what does optimize schema mean here? Imo it means make it more performant. BigQuery de-normalized tables can be more performant than Cloud SQL imposing relational constraints and doing expensive joins.\nCloud BigTable: Don't see why we should use BigTable to store such data and spin up a cluster if we don't want to do serving for an app that requires really high throughput and low latency. Nothing about latency is mentioned in the description so nothing that points to BigTable.\nCloud Datastore: I don't know enough about Datastore, but I have never seen it been mentioned together with ML in my experience with GCP and preparing for the exam."
      },
      {
        "date": "2023-07-19T00:15:00.000Z",
        "voteCount": 3,
        "content": "A: Just because it says 'machine learning based service' and BigQuery has BQML doesn't mean we want to carry out machine learning in the database itself.\nB: The only aspect that speaks for Cloud SQL is 'transactional data'. However, they don't state explicitly that the database here should be used for the transactions.\nC: No real reason here to use Bigtable, which is very expensive.\n\nD: Datastore would make sense. Information such as user food preferences and profile information are likely semi-structured, thus fitting for a document-like data model. Also, if we want transactions, Datastore also supports that."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 48,
    "url": "https://www.examtopics.com/discussions/google/view/17082-exam-professional-data-engineer-topic-1-question-48/",
    "body": "Your company is loading comma-separated values (CSV) files into Google BigQuery. The data is fully imported successfully; however, the imported data is not matching byte-to-byte to the source file. What is the most likely cause of this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CSV data loaded in BigQuery is not flagged as CSV.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CSV data has invalid rows that were skipped on import.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CSV data loaded in BigQuery is not using BigQuery's default encoding.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe CSV data has not gone through an ETL phase before loading into BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T07:35:00.000Z",
        "voteCount": 26,
        "content": "Answer: C\nDescription: Bigquery understands UTF-8 encoding anything other than that will result in data issues with schema"
      },
      {
        "date": "2021-08-20T01:03:00.000Z",
        "voteCount": 17,
        "content": "Answer : C :\n\" If you don't specify an encoding, or if you specify UTF-8 encoding when the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to UTF-8. Generally, your data will be loaded successfully, but it may not match byte-for-byte what you expect.\"\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#details_of_loading_csv_data"
      },
      {
        "date": "2023-01-17T12:31:00.000Z",
        "voteCount": 2,
        "content": "SITUATION:\n- Your company is loading comma-separated values (CSV) files into Google BigQuery. \n- Data is fully imported successfully. \nPROBLEM:\n- Imported data is not matching byte-to-byte to the source file. Reason?"
      },
      {
        "date": "2023-01-17T12:32:00.000Z",
        "voteCount": 2,
        "content": "A. The CSV data loaded in BigQuery is not flagged as CSV.\nSince BigQuery support multiple formats it could be that maybe avro or json was selected. \nBut the file import was successful hence csv was selected. Either manually or it was left as is since the default file type is csv. Lastly, this is WRONG.\nB. The CSV data has invalid rows that were skipped on import.\n-&gt; Since the data was successfully imported there were no invalid rows. Hence, This is wrong answer too."
      },
      {
        "date": "2023-01-17T12:32:00.000Z",
        "voteCount": 2,
        "content": "C. The CSV data loaded in BigQuery is not using BigQuery's default encoding.\n-&gt; \"BigQuery supports UTF-8 encoding for both nested or repeated and flat data. BigQuery supports ISO-8859-1 encoding for flat data only for CSV files.\"\nSource: https://cloud.google.com/bigquery/docs/loading-data\nDefault BQ Encoding: UTF-8\nThis is probably the correct answer because if the csv file encoding was not UTF-8 and instead it was ISO-8859-1 then we would have to tell bigquery that orelse it will assume it is UTF-8. Hence, Imported data is not matching byte-to-byte to the source file. CORRECT ANSWER!"
      },
      {
        "date": "2023-01-17T12:32:00.000Z",
        "voteCount": 2,
        "content": "D. The CSV data has not gone through an ETL phase before loading into BigQuery.\n-&gt; ETL means Extract, Transform and Load and this is actually very important content for Cloud Data Engineers. Look into it if interested! But getting back to the topic: ETL is usually required when the source format and target format are different. You need to extract source file and the transform it before loading the data to fit the target. This is also not a viable option. Also Data is imported successfully and the question doesn't mention anything regarding ETL."
      },
      {
        "date": "2022-01-04T07:25:00.000Z",
        "voteCount": 6,
        "content": "A is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nC is correct because this is the only situation that would cause successful import.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table."
      },
      {
        "date": "2021-11-28T02:28:00.000Z",
        "voteCount": 2,
        "content": "C is correct because this is the only situation that would cause successful import.\nA is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table.\nhttps://cloud.google.com/bigquery/docs/loading-data#loading_encoded_data"
      },
      {
        "date": "2022-12-12T00:15:00.000Z",
        "voteCount": 1,
        "content": "Exactly\u2b06\nThe updated link (Dec. 2022) and the quote:\n\ud83d\udd17 https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#encoding\n\"If you don't specify an encoding, or if you specify UTF-8 encoding when the CSV file is not UTF-8 encoded, BigQuery attempts to convert the data to UTF-8. Generally, your data will be loaded successfully, but it may not match byte-for-byte what you expect.\""
      },
      {
        "date": "2021-10-15T12:57:00.000Z",
        "voteCount": 3,
        "content": "Ans: C"
      },
      {
        "date": "2021-06-27T09:34:00.000Z",
        "voteCount": 3,
        "content": "Vote for 'C'"
      },
      {
        "date": "2021-07-08T08:28:00.000Z",
        "voteCount": 2,
        "content": "A is not correct because if another data format other than CSV was selected then the data would not import successfully.\nB is not correct because the data was fully imported meaning no rows were skipped.\nC is correct because this is the only situation that would cause successful import.\nD is not correct because whether the data has been previously transformed will not affect whether the source file will match the BigQuery table."
      },
      {
        "date": "2021-02-07T10:44:00.000Z",
        "voteCount": 2,
        "content": "Correct C"
      },
      {
        "date": "2020-08-18T17:26:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2020-08-17T14:10:00.000Z",
        "voteCount": 6,
        "content": "C is correct answer, Refer below link for more informaiton.\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-csv#details_of_loading_csv_data"
      },
      {
        "date": "2020-03-21T00:36:00.000Z",
        "voteCount": 10,
        "content": "Answer: C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 49,
    "url": "https://www.examtopics.com/discussions/google/view/17083-exam-professional-data-engineer-topic-1-question-49/",
    "body": "Your company produces 20,000 files every hour. Each data file is formatted as a comma separated values (CSV) file that is less than 4 KB. All files must be ingested on Google Cloud Platform before they can be processed. Your company site has a 200 ms latency to Google Cloud, and your Internet connection bandwidth is limited as 50 Mbps. You currently deploy a secure FTP (SFTP) server on a virtual machine in Google Compute Engine as the data ingestion point. A local SFTP client runs on a dedicated machine to transmit the CSV files as is. The goal is to make reports with data from the previous day available to the executives by 10:00 a.m. each day. This design is barely able to keep up with the current volume, even though the bandwidth utilization is rather low.<br>You are told that due to seasonality, your company expects the number of files to double for the next three months. Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce data compression for each file to increase the rate file of file transfer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tContact your internet service provider (ISP) to increase your maximum bandwidth to at least 100 Mbps.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedesign the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssemble 1,000 files into a tape archive (TAR) file. Transmit the TAR files instead, and disassemble the CSV files in the cloud upon receiving them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an S3-compatible storage endpoint in your network, and use Google Cloud Storage Transfer Service to transfer on-premises data to the designated storage bucket."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-12-16T17:57:00.000Z",
        "voteCount": 54,
        "content": "E cannot be: Transfer Service is recommended for 300mbps or faster\nhttps://cloud.google.com/storage-transfer/docs/on-prem-overview\n\nBandwidth is not an issue, so B is not an answer\n\nCloud Storage loading gets better throughput the larger the files are. Therefore making them smaller with compression does not seem a solution. -m option to do parallel work is recommended. Therefore A is not and C is an answer.\nhttps://medium.com/@duhroach/optimizing-google-cloud-storage-small-file-upload-performance-ad26530201dc\n\nThat leaves D as the other option. It is true you cannot user tar directly with gsutil, but you can load the tar file to Cloud Storage, move the file to a Compute Engine instance with Linux, use tar to split files and copy them back to Cloud Storage. Batching  many files in a larger tar will improve Cloud Storage throughput.\n\nSo, given the alternatives, I think answer is CD"
      },
      {
        "date": "2021-07-04T11:39:00.000Z",
        "voteCount": 2,
        "content": "This should be the correct answer."
      },
      {
        "date": "2023-02-23T15:21:00.000Z",
        "voteCount": 1,
        "content": "50mbps is so slow, why you think bandwidth is ok! For parallel upload you need good internet ?"
      },
      {
        "date": "2023-02-28T16:46:00.000Z",
        "voteCount": 2,
        "content": "normally the solutions are Google Cloud Services based, as it's a vendor exam"
      },
      {
        "date": "2023-04-25T14:37:00.000Z",
        "voteCount": 2,
        "content": "They have 20.000 files 4kb each per hour, so bandwith needed for it is far below 1mbps. 50mbps is enough to upload all day generated data in about 5 minutes."
      },
      {
        "date": "2021-10-08T08:09:00.000Z",
        "voteCount": 4,
        "content": "D is incorrect. gsutil with -m option uses multiprocessing/multithreading. It means it will copy the file in parallel. The benefit of multiprocessing/multithreading is significantly high when working with large number of files, instead of file size. The important point of multiprocessing/multithreading is sending multiple files in parallel. Hence file size doesn't give impact to gsutil with -m option. Gsutil with -m option doesn't split a big file into multiple chunks and transfer it in parallel. So in my opinion the answer is A and C."
      },
      {
        "date": "2021-10-08T08:13:00.000Z",
        "voteCount": 3,
        "content": "Here is the docs which support my opinion: https://cloud.google.com/storage/docs/gsutil/addlhelp/TopLevelCommandLineOptions"
      },
      {
        "date": "2023-07-24T03:34:00.000Z",
        "voteCount": 1,
        "content": "We have small files of 4KB and no issues with bandwidth. It's not an issue that -m does not split files. Our problem is with total volume."
      },
      {
        "date": "2023-07-21T07:20:00.000Z",
        "voteCount": 2,
        "content": "As far as I understand compression is not something we want here because bandwidth is not an issue and compressed files will need to be decompressed on the cloud. On top of that if we want to load those files later in BigQuery to create the report we know that we cannot load compressed csv files in parallel.\n\ngsutil makes the most sense because it will be used to load all new files in parallel.\n\nI answered D as well because I thought that none of the others made sense and D is the only one that mentions creating the bucket on GCS and perhaps migrating data that is missed during the update in the architecture.\n\nSo D to create the bucket, C to update the process and move the data to the bucket, then D to move any lost data during the update."
      },
      {
        "date": "2023-07-24T03:36:00.000Z",
        "voteCount": 1,
        "content": "Typo, I meant E in my post. C and E, not C and D."
      },
      {
        "date": "2020-03-21T00:48:00.000Z",
        "voteCount": 33,
        "content": "Should be AC"
      },
      {
        "date": "2020-11-14T15:38:00.000Z",
        "voteCount": 3,
        "content": "support this with a link...."
      },
      {
        "date": "2021-02-25T14:22:00.000Z",
        "voteCount": 1,
        "content": "Here you go: https://cloud.google.com/storage-transfer/docs/overview#gsutil"
      },
      {
        "date": "2022-05-04T07:20:00.000Z",
        "voteCount": 1,
        "content": "This link does support for C, but what about A? any supported links?"
      },
      {
        "date": "2021-03-12T18:01:00.000Z",
        "voteCount": 8,
        "content": "Thank you! From this doc:\nFollow these rules of thumb when deciding whether to use gsutil or Storage Transfer Service:\n\nTransfer scenario\tRecommendation\nTransferring from another cloud storage provider\tUse Storage Transfer Service.\nTransferring less than 1 TB from on-premises\tUse gsutil.\nTransferring more than 1 TB from on-premises\tUse Transfer service for on-premises data.\nTransferring less than 1 TB from another Cloud Storage region\tUse gsutil.\nTransferring more than 1 TB from another Cloud Storage region\tUse Storage Transfer Service."
      },
      {
        "date": "2024-07-31T14:59:00.000Z",
        "voteCount": 1,
        "content": "C - because gsutil is recommended for transferring less than 1 TB from on-premises \nC excludes E; \nbandwidth is not a problem due to a simple math, so we exclude B;\n4 KB file is compressed enough, so we exclude A;\nD - works fine because even with -m flag we can send tars in parallel."
      },
      {
        "date": "2024-03-05T09:42:00.000Z",
        "voteCount": 1,
        "content": "AC is the answer"
      },
      {
        "date": "2023-11-06T04:16:00.000Z",
        "voteCount": 1,
        "content": "How can C and E be the answer? They are solving the same problem with different approaches. If you pick C then E can not be an answer. If you pick E then C can not be an answer. This question also seems a bit dated because of gcloud storage cli which is much more performant than gsutil. I would pick C&amp;D as the combination makes the most sense given the choices."
      },
      {
        "date": "2023-06-06T16:17:00.000Z",
        "voteCount": 1,
        "content": "@hendrixlives arguments are correct. The approach between the resources in use and how to optimize the ingestion must be balanced."
      },
      {
        "date": "2023-05-13T01:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct without an doubt\nI was in doubt between D and E \nA and B does not seems correct because it states that the bandwidth is not fully utilized .\nNow D and E \nIf the bandwidth was higher the E would be good\nD even if it seems that will not make difference because tar files does not have compression transmit one file instead of 1000 is significantly faster so I would choose \nC and D"
      },
      {
        "date": "2023-05-05T02:50:00.000Z",
        "voteCount": 1,
        "content": "CD , i guess. Liked explanation in discussion. \nchanging from BC to CD"
      },
      {
        "date": "2023-04-26T14:48:00.000Z",
        "voteCount": 2,
        "content": "Guys. need a help. anyone appeared for the exam very recently (Apr 2023)? preparing all the questions from here, would be enough?"
      },
      {
        "date": "2023-04-25T15:10:00.000Z",
        "voteCount": 3,
        "content": "It seems that Google would like AC. A is not neccessary - it doesnt make a significant change - small files do not compress well and bandwith is so big that file size is not an issue - the isue is 0,2s latency. The biggest benefit is that we can simply enable compression from gsutils parameters, it will not add any implementation complexity. \nFor me C solo is ok and D solo might be even better, but more complex. C and D cannot be mixed - they exclude each other. C is more simple and uses Google service so it seems to be desired answer. And it makes sense if they want us to select 2 actions we have to make - If we go for C we can also get some benefit from A, if we go for D there is no other answer we can select and it is much more complex in implementation than AC(which is by far good enough)."
      },
      {
        "date": "2023-10-15T23:43:00.000Z",
        "voteCount": 1,
        "content": "Yes the 2 are excluding each other. So it's AC"
      },
      {
        "date": "2023-02-14T03:15:00.000Z",
        "voteCount": 3,
        "content": "Answer: B&amp;C \nA: files are 4kb, no need for compression\nB: more files to be transmitted per unit time with 100mbps or get 5g network (~200 mbps)\nC: gsutil parallel ingestion will reduce time \nD: TAR is not a good compression and slower in transfer even slower than csv. speed is 50mbps so don't go with it. \nE: Storage Transfer service needs good internet and used for large size of data and for on premises storage, this one is regular ingestion."
      },
      {
        "date": "2023-02-08T01:26:00.000Z",
        "voteCount": 1,
        "content": "-- From ChatGPT --\n\nB. Redesign the data ingestion process to use gsutil tool to send the CSV les to a storage bucket in parallel.\nA. Introduce data compression for each file to increase the rate of file transfer.\n\nReasoning:\nB. Redesigning the data ingestion process to use gsutil tool to send the CSV files to a storage bucket in parallel will help improve the rate at which the data is transferred to the cloud. This is because gsutil allows for parallel transfers, thereby utilizing the available bandwidth more efficiently and reducing the time required to transfer the data.\n\nA. Introducing data compression for each file will also help improve the rate of file transfer. This is because compressed data takes up less space and can be transferred faster, thereby reducing the time required to transfer the data."
      },
      {
        "date": "2023-02-08T01:27:00.000Z",
        "voteCount": 1,
        "content": "Why not option D?\nOption D, which involves assembling 1000 files into a TAR file and then transmitting it, may not be an effective solution for the current situation. While TAR archives can help reduce the number of files that need to be transmitted, disassembling the TAR archive in the cloud after receiving it could increase the time required to process the data. This could make it difficult to meet the goal of making reports with data from the previous day available by 10:00 a.m. each day.\n\nFurthermore, compressing the TAR archive could increase the time required to create the archive, and may not provide a significant improvement in terms of transfer time, as the individual CSV files are already small in size. This makes it less effective compared to the other options of parallel transfers and data compression."
      },
      {
        "date": "2023-04-25T14:55:00.000Z",
        "voteCount": 1,
        "content": "II wouldnt agree, the main issue here is latency 0,2s and 20000 files per hour - it is even beyond possible transfer without paralelisation or file merging. Compression and sending 1000 files at once resolves the issue. Just as option C. But they don't make any sense together. I think they exclude D because of additional complexity - compression and then decompression is much more difficult than using gsutil. Thus we go for C. If we need one more then only A makes some sense, but I wouldn't go for it. We have enough bandwith for this size of file. We just need get rid of latency, by paralelization."
      },
      {
        "date": "2023-04-25T15:00:00.000Z",
        "voteCount": 1,
        "content": "OK. AC seems to be rright as we can simply enable the compression by gsutil options."
      },
      {
        "date": "2022-11-25T16:02:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/parallel-composite-uploads"
      },
      {
        "date": "2022-04-22T23:27:00.000Z",
        "voteCount": 1,
        "content": "A is incorrect as rate of file transfer is not an issue, system is not able to handle current load itself, compression will make it even faster"
      },
      {
        "date": "2022-04-20T13:32:00.000Z",
        "voteCount": 1,
        "content": "Multiple small files transfer is a bad practice.  You should always use some aggregation strategy like tar or zip multiple files. A is discarded because talks about compressing a single file. B is discarded because the bandwidth is not the problem.\n\nOption C could be , but multi threading has a limit. Then the best option is D or use some google on prem mirroring service like E."
      },
      {
        "date": "2022-06-14T23:14:00.000Z",
        "voteCount": 2,
        "content": "E is wrong, as Bandwidth already low, so storage Transfer service will not help here"
      },
      {
        "date": "2022-03-22T11:10:00.000Z",
        "voteCount": 1,
        "content": "E is wrong Google Cloud Storage Transfer Service (online) != Transfer Appliance(on-premise)"
      },
      {
        "date": "2022-03-15T21:32:00.000Z",
        "voteCount": 1,
        "content": "CD is correct option"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 50,
    "url": "https://www.examtopics.com/discussions/google/view/16661-exam-professional-data-engineer-topic-1-question-50/",
    "body": "You are choosing a NoSQL database to handle telemetry data submitted from millions of Internet-of-Things (IoT) devices. The volume of data is growing at 100<br>TB per year, and each data entry has about 100 attributes. The data processing pipeline does not require atomicity, consistency, isolation, and durability (ACID).<br>However, high availability and low latency are required.<br>You need to analyze the data by querying against individual fields. Which three databases meet your requirements? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedis",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHBase\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMySQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMongoDB\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCassandra\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHDFS with Hive"
    ],
    "answer": "BDE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BDE",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "BEF",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T05:59:00.000Z",
        "voteCount": 37,
        "content": "BDE. Hive is not for NoSQL"
      },
      {
        "date": "2021-09-28T08:52:00.000Z",
        "voteCount": 1,
        "content": "Redis is also NoSQL"
      },
      {
        "date": "2021-10-08T08:20:00.000Z",
        "voteCount": 3,
        "content": "Redis is limited to 1 TB capacity quota per region. So it doesn't satisfy the requirement.\nhttps://cloud.google.com/memorystore/docs/redis/quotas"
      },
      {
        "date": "2023-09-21T04:25:00.000Z",
        "voteCount": 1,
        "content": "Memorystore, Google's managed Redis service is. But OS Redis is not. Though it is hard to find a 100GB RAM machine"
      },
      {
        "date": "2021-07-04T11:59:00.000Z",
        "voteCount": 32,
        "content": "Answer is BDE - \nA. Redis - Redis is an in-memory non-relational key-value store. Redis is a great choice for implementing a highly available in-memory cache to decrease data access latency, increase throughput, and ease the load off your relational or NoSQL database and application. Since the question does not ask cache, A is discarded.\nB. HBase - Meets reqs\nC. MySQL - they do not need ACID, so not needed.\nD. MongoDB - Meets reqs\nE. Cassandra - Apache Cassandra is an open source NoSQL distributed database trusted by thousands of companies for scalability and high availability without compromising performance. Linear scalability and proven fault-tolerance on commodity hardware or cloud infrastructure make it the perfect platform for mission-critical data.\nF. HDFS with Hive - Hive allows users to read, write, and manage petabytes of data using SQL. Hive is built on top of Apache Hadoop, which is an open-source framework used to efficiently store and process large datasets. As a result, Hive is closely integrated with Hadoop, and is designed to work quickly on petabytes of data. HIVE IS NOT A DATABSE."
      },
      {
        "date": "2023-02-20T08:34:00.000Z",
        "voteCount": 1,
        "content": "HDFS is. Hadoop Distributed File System. HDFS is storage and HIVE is for processing."
      },
      {
        "date": "2023-02-14T03:31:00.000Z",
        "voteCount": 1,
        "content": "BDE\nFaster Database are NoSql db than SQL, Cassandra is the fastest one in market now than Hbase and then others, in given list MongoBD"
      },
      {
        "date": "2022-08-26T12:09:00.000Z",
        "voteCount": 1,
        "content": "\"Which three databases meet your requirements? \"\nHive is not a database server.\nHBase, Mongo and Cassandra are and meet the criteria.\nBDE is the right answer"
      },
      {
        "date": "2022-01-14T12:35:00.000Z",
        "voteCount": 1,
        "content": "@hendrixlives"
      },
      {
        "date": "2022-01-04T07:32:00.000Z",
        "voteCount": 1,
        "content": "as explained by  hendrixlives"
      },
      {
        "date": "2021-12-17T01:36:00.000Z",
        "voteCount": 13,
        "content": "BDE:\nA. Redis is a key-value store (and in many cases used as in-memory and non persistent cache). It is not designed for \"100TB per year\" of highly available storage.\nB. HBase is similar to Google Bigtable, fits the requirements perfectly: highly available, scalable and with very low latency.\nC. MySQL is a relational DB, designed precisely for ACID transactions and not for the stated requirements. Also, growth may be an issue.\nD. MongoDB is a document-db used for high volume data and maintains currently used data in RAM, so performance is usually really good. Should also fit the requirements well.\nE. Cassandra is designed precisely for highly available massive datasets, and a fine tuned cluster may offer low latency in reads. Fits the requirements.\nF. HDFS with Hive is great for OLAP and data-warehouse scenarios, allowing to solve map-reduce problems using an SQL subset, but the latency is usually really high (we may talk about seconds, not milliseconds, when obtaining results), so this does not complies with the requirements."
      },
      {
        "date": "2021-11-28T03:09:00.000Z",
        "voteCount": 1,
        "content": "Very strange question, seems outdated and irrelevant to me as it doesn't contain any GCP products :)\n\nAnyway, I would choose BEF.\nRedis is in-memory key value, not good\nHBase yes, excelent case for linear growth and a column-oriented database\nmysql not good, too big and no need for transactionality\nMongodb, document db with flexible schema ??\nYes Cassandra, good use case\nApache Hive is a data warehouse software project built on top of Apache Hadoop for providing data query and analysis. \nhttps://www.wikiwand.com/en/Apache_Hive"
      },
      {
        "date": "2021-12-17T01:01:00.000Z",
        "voteCount": 2,
        "content": "Latency in Hive is usually quite high, and one of the requirements is \"low latency\""
      },
      {
        "date": "2022-01-25T10:27:00.000Z",
        "voteCount": 2,
        "content": "agreed on BDE"
      },
      {
        "date": "2022-01-22T09:41:00.000Z",
        "voteCount": 1,
        "content": "good point!"
      },
      {
        "date": "2021-10-15T13:16:00.000Z",
        "voteCount": 2,
        "content": "Ans: B, D and E"
      },
      {
        "date": "2021-06-27T09:59:00.000Z",
        "voteCount": 2,
        "content": "vote for BDE"
      },
      {
        "date": "2021-03-12T18:08:00.000Z",
        "voteCount": 2,
        "content": "BEF\nB: HBASE is based upon BigTable\nE: Cassandra is low latency columnar distributed database like BigTable\nF: HDFS is low latency distributed file system and Hive will help with running the queries"
      },
      {
        "date": "2021-04-15T14:39:00.000Z",
        "voteCount": 5,
        "content": "Hive is not for low latency queries. It is for analytics."
      },
      {
        "date": "2021-03-10T11:42:00.000Z",
        "voteCount": 2,
        "content": "BDE:\nThese are NoSQL DB, Hive is not for NoSQL."
      },
      {
        "date": "2021-02-23T00:31:00.000Z",
        "voteCount": 1,
        "content": "The answer is ADE, the statement says they require a NoSQL with high availability and low latency, they do not require consistency. \nC. it is not NoSQL.\nF. it is not NoSQL.\nB. it is NoSQL but focused on strong consistency and based on HDFS, you need HDFS for Hbase.\nTherefore the answer is ADE"
      },
      {
        "date": "2021-02-14T18:52:00.000Z",
        "voteCount": 1,
        "content": "BDF:\nRedis and Cassandra have  only Rowkey and couldn't be indexed, and MySQL isn't NoSQL, Then B D and E is correct answer."
      },
      {
        "date": "2021-02-07T10:55:00.000Z",
        "voteCount": 3,
        "content": "Correct BDE"
      },
      {
        "date": "2020-12-30T00:37:00.000Z",
        "voteCount": 3,
        "content": "it should be BDE because Hive is a sql based datawarehouse , it is not a nosql DB"
      },
      {
        "date": "2020-11-14T23:20:00.000Z",
        "voteCount": 2,
        "content": "I agree with HaroldBenites and I like your answer. I did some research and yes, you cannot query HBase by individual fields. See https://opensource.com/article/19/8/apache-hive-vs-apache-hbase  you cannot query by row 00001 or 00002 etc but not by the field!!! wow"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 51,
    "url": "https://www.examtopics.com/discussions/google/view/16468-exam-professional-data-engineer-topic-1-question-51/",
    "body": "You are training a spam classifier. You notice that you are overfitting the training data. Which three actions can you take to resolve this problem? (Choose three.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGet more training examples\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of training examples",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a smaller set of features\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a larger set of features",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the regularization parameters\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"F\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tF.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDecrease the regularization parameters"
    ],
    "answer": "ACE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "ACE",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "ADE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-13T02:28:00.000Z",
        "voteCount": 67,
        "content": "it should be ACE"
      },
      {
        "date": "2020-03-21T01:04:00.000Z",
        "voteCount": 19,
        "content": "Should be ACE"
      },
      {
        "date": "2020-03-21T01:06:00.000Z",
        "voteCount": 14,
        "content": "prevent overfitting: less variables, regularisation, early ending on the training"
      },
      {
        "date": "2023-12-14T05:50:00.000Z",
        "voteCount": 4,
        "content": "To address the problem of overfitting in training a spam classifier, you should consider the following three actions:\n\nA. Get more training examples:\n\nWhy: More training examples can help the model generalize better to unseen data. A larger dataset typically reduces the chance of overfitting, as the model has more varied examples to learn from.\nC. Use a smaller set of features:\n\nWhy: Reducing the number of features can help prevent the model from learning noise in the data. Overfitting often occurs when the model is too complex for the amount of data available, and having too many features can contribute to this complexity.\nE. Increase the regularization parameters:\n\nWhy: Regularization techniques (like L1 or L2 regularization) add a penalty to the model for complexity. Increasing the regularization parameter will strengthen this penalty, encouraging the model to be simpler and thus reducing overfitting."
      },
      {
        "date": "2023-07-21T07:25:00.000Z",
        "voteCount": 2,
        "content": "100% ACE\n\nWe need more data because less data induces overfitting. We need less features to make the problem simpler to learn and not promote learning a very complex function for thousands of features that might not apply to the test data. We also need to use regularization to keep the weights constrained."
      },
      {
        "date": "2023-07-17T12:25:00.000Z",
        "voteCount": 1,
        "content": "Definitely ACE. \nMore training data and less variables can prevent the model from being too picky or specific."
      },
      {
        "date": "2023-02-23T00:08:00.000Z",
        "voteCount": 1,
        "content": "? why A is answer? even though 'more training example' not 'more dataset example'. I understand that there is dataset same and there is only change the size of training examples size. in this case there are valid and test example should be reduced. isn't it?"
      },
      {
        "date": "2023-01-13T10:35:00.000Z",
        "voteCount": 1,
        "content": "Collect more training data: This will help the model generalize better and reduce overfitting.\n\nUse regularization techniques: Techniques such as L1 and L2 regularization can be applied to the model's weights to prevent them from becoming too large and causing overfitting.\n\nUse early stopping: This involves monitoring the performance of the model on a validation set during training, and stopping the training when the performance on the validation set starts to degrade. This helps to prevent the model from becoming too complex and overfitting the training data."
      },
      {
        "date": "2023-01-13T10:39:00.000Z",
        "voteCount": 1,
        "content": "Regularization is a technique that penalizes the coefficient. In an overfit model, the coefficients are generally inflated. Thus, Regularization adds penalties to the parameters and avoids them weigh heavily.\n\nA &amp; C are correct... the third one --- not sure on"
      },
      {
        "date": "2023-01-09T10:45:00.000Z",
        "voteCount": 1,
        "content": "A -The training data is causing the overfiting for the testing data, so addition of training data will solve this.\nC - Larger sets will cause overfitting, so we have to use smaller sets or reduce features\nE - Increase the regularization is a method for solving the Overfitting model"
      },
      {
        "date": "2023-01-04T14:02:00.000Z",
        "voteCount": 3,
        "content": "Answers are;\nA. Get more training examples\nC. Use a smaller set of features\nE. Increase the regularization parameters\n\nPrevent overfitting: less variables, regularisation, early ending on the training\n\nReference:\nhttps://cloud.google.com/bigquery-ml/docs/preventing-overfitting"
      },
      {
        "date": "2022-12-13T19:19:00.000Z",
        "voteCount": 1,
        "content": "Answer ADE"
      },
      {
        "date": "2022-10-25T08:56:00.000Z",
        "voteCount": 1,
        "content": "100% sure ACE\n\nhttps://elitedatascience.com/overfitting-in-machine-learning"
      },
      {
        "date": "2022-08-26T12:14:00.000Z",
        "voteCount": 1,
        "content": "Answer is : ACE\nhttps://www.ibm.com/cloud/learn/overfitting#:~:text=Overfitting%20is%20a%20concept%20in,unseen%20data%2C%20defeating%20its%20purpose."
      },
      {
        "date": "2022-08-14T09:41:00.000Z",
        "voteCount": 1,
        "content": "im vote for ACE"
      },
      {
        "date": "2022-08-04T10:12:00.000Z",
        "voteCount": 1,
        "content": "It should be ACE"
      },
      {
        "date": "2022-01-14T12:49:00.000Z",
        "voteCount": 1,
        "content": "@medeis_jar"
      },
      {
        "date": "2022-01-04T07:36:00.000Z",
        "voteCount": 4,
        "content": "As MaxNRG wrote:\nThe tools to prevent overfitting: less variables, regularization, early ending on the training.\n\n- Adding more training data will increase the complexity of the training set and help with the variance problem.\n- Reducing the feature set will ameliorate the overfitting and help with the variance problem.\n- Increasing the regularization parameter will reduce overfitting and help with the variance problem."
      },
      {
        "date": "2021-12-18T11:51:00.000Z",
        "voteCount": 4,
        "content": "ACE\n\nThe tools to prevent overfitting: less variables, regularization, early ending on the training\u2026\nOverfitting means that the classifier knows too well the data and fails to generalize. We should use a smaller number of features to help the classifier generalize, and more examples so that it can have more variety.\nThe gap in errors between training and test suggests a high variance problem in which the algorithm has overfit the training set.\n- Adding more training data will increase the complexity of the training set and help with the variance problem.\n- Reducing the feature set will ameliorate the overfitting and help with the variance problem.\n- Increasing the regularization parameter will reduce overfitting and help with the variance problem.\nhttps://github.com/mGalarnyk/datasciencecoursera/blob/master/Stanford_Machine_Learning/Week6/AdviceQuiz.md"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 52,
    "url": "https://www.examtopics.com/discussions/google/view/16822-exam-professional-data-engineer-topic-1-question-52/",
    "body": "You are implementing security best practices on your data pipeline. Currently, you are manually executing jobs as the Project Owner. You want to automate these jobs by taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud<br>Dataproc cluster, and depositing the results into Google BigQuery.<br>How should you securely run this workload?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRestrict the Google Cloud Storage bucket so only you can see the files",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant the Project Owner role to a service account, and run the job with it",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a service account with the ability to read the batch files and to write to BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a user account with the Project Viewer role on the Cloud Dataproc cluster to read the batch files and write to BigQuery"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-24T19:15:00.000Z",
        "voteCount": 34,
        "content": "A is wrong, if only I can see the bucket no automation is possible, besides, also needs launch the dataproc job\nB is too much, does not follow the security best practices\nC has one point missing\u2026you need to submit dataproc jobs.\nIn D viewer role will not be able to submit dataproc jobs, the rest is ok\n\nThus\u2026.the only one that would work is B! BUT this service account has too many permissions. Should have dataproc editor, write big query and read from bucket"
      },
      {
        "date": "2020-06-30T08:54:00.000Z",
        "voteCount": 15,
        "content": "Hence - Contextually, Option [C] looks to be the right fit"
      },
      {
        "date": "2021-09-28T11:47:00.000Z",
        "voteCount": 5,
        "content": "C doesn't need permission to submit dataproc jobs, it's workload SA. Job can be submitted by any other identity"
      },
      {
        "date": "2020-03-16T21:01:00.000Z",
        "voteCount": 31,
        "content": "Should be C"
      },
      {
        "date": "2023-07-24T03:56:00.000Z",
        "voteCount": 1,
        "content": "We need permissions for submitting dataproc jobs and writing to BigQuery. Project Owner will fix all of that even though it's not a good solution. The rest won't work at all."
      },
      {
        "date": "2023-04-12T20:55:00.000Z",
        "voteCount": 3,
        "content": "C\nProject Owner is too much, violates the principle of least privilege"
      },
      {
        "date": "2023-01-26T06:07:00.000Z",
        "voteCount": 3,
        "content": "C. Use a service account with the ability to read the batch files and to write to BigQuery\n\nIt is best practice to use service accounts with the least privilege necessary to perform a specific task when automating jobs. In this case, the job needs to read the batch files from Cloud Storage and write the results to BigQuery. Therefore, you should create a service account with the ability to read from the Cloud Storage bucket and write to BigQuery, and use that service account to run the job."
      },
      {
        "date": "2023-01-05T01:23:00.000Z",
        "voteCount": 1,
        "content": "B works for the given requirement"
      },
      {
        "date": "2022-12-20T03:33:00.000Z",
        "voteCount": 2,
        "content": "Least privilege principle. Option C. job can be submitted or triggered using a Cron or a composer which uses another SA with different set of privileges"
      },
      {
        "date": "2022-12-13T19:35:00.000Z",
        "voteCount": 2,
        "content": "B because we need to run job .. option C mentioned permission about read and write nothing mention to run the job . In case project owner to service account it\u2019s similar just running job and doing rest of tasks read and writing as well."
      },
      {
        "date": "2022-04-22T19:43:00.000Z",
        "voteCount": 2,
        "content": "The answer is C because Service Account is the best way to access the BigQuery API if your application can run jobs associated with service credentials rather than an end-user's credentials, such as a batch processing pipeline.\nhttps://cloud.google.com/bigquery/docs/authentication"
      },
      {
        "date": "2022-01-12T02:15:00.000Z",
        "voteCount": 4,
        "content": "Data owners cant create jobs or queries. -&gt; B out \nWe need service Account -&gt; D out\nAccess only granting me does not solve the problem -&gt; A out \nThe answer is C. ( Minimum rights to perform the job)"
      },
      {
        "date": "2022-01-04T07:41:00.000Z",
        "voteCount": 1,
        "content": "\"taking nightly batch files containing non-public information from Google Cloud Storage, processing them with a Spark Scala job on a Google Cloud\nDataproc cluster, and depositing the results into Google BigQuery\""
      },
      {
        "date": "2021-12-28T11:27:00.000Z",
        "voteCount": 1,
        "content": "C should be okay,since he is already a project owner, I guess compute service account created will have access to run the jobs"
      },
      {
        "date": "2021-12-18T11:57:00.000Z",
        "voteCount": 1,
        "content": "C,\nProject Owner role to a service account - is too much"
      },
      {
        "date": "2021-11-26T03:51:00.000Z",
        "voteCount": 6,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C"
      },
      {
        "date": "2021-10-15T13:30:00.000Z",
        "voteCount": 3,
        "content": "Ans: C\nSee this: https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/service-accounts#dataproc_service_accounts_2"
      },
      {
        "date": "2021-09-09T11:41:00.000Z",
        "voteCount": 4,
        "content": "C as service account invoked to read the data into GCS and write to BQ once transformed via Data Proc. Assumes DataProc can inherit SA authorisation to perform transform and propagate.  \nB seems to violate key IAM principle enforcing least privilege;\nhttps://cloud.google.com/iam/docs/recommender-overview"
      },
      {
        "date": "2021-06-27T15:05:00.000Z",
        "voteCount": 4,
        "content": "Vote for 'C\""
      },
      {
        "date": "2021-07-08T12:06:00.000Z",
        "voteCount": 3,
        "content": "Vote for B, (though it's too much access) - But C has one accessing missing (i.e Dataproc job execution) Thus B is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 53,
    "url": "https://www.examtopics.com/discussions/google/view/17085-exam-professional-data-engineer-topic-1-question-53/",
    "body": "You are using Google BigQuery as your data warehouse. Your users report that the following simple query is running very slowly, no matter when they run the query:<br>SELECT country, state, city FROM [myproject:mydataset.mytable] GROUP BY country<br>You check the query plan for the query and see the following output in the Read section of Stage:1:<br><img src=\"/assets/media/exam-media/04341/0004200001.jpg\" class=\"in-exam-image\"><br>What is the most likely cause of the delay for this query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUsers are running too many concurrent queries in the system",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe [myproject:mydataset.mytable] table has too many partitions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEither the state or the city columns in the [myproject:mydataset.mytable] table have too many NULL values",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMost rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T01:32:00.000Z",
        "voteCount": 26,
        "content": "Should be D"
      },
      {
        "date": "2020-04-18T11:13:00.000Z",
        "voteCount": 25,
        "content": "D; Purple is reading, Blue is writing. so majority is reading."
      },
      {
        "date": "2021-10-29T09:54:00.000Z",
        "voteCount": 1,
        "content": "I have been looking for the color code descriptions for a while. Thank you!"
      },
      {
        "date": "2024-07-31T15:16:00.000Z",
        "voteCount": 1,
        "content": "D - stands for Data Skew\nThe Read section of the query plan shows a heavy concentration of processing in one area (as indicated by the pink bar being much longer than the purple bar). This typically indicates data skew, where the majority of the data is processed by a small subset of nodes."
      },
      {
        "date": "2023-12-16T05:41:00.000Z",
        "voteCount": 2,
        "content": "The most likely cause of the delay for this query is option D. Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew.\n\nGroup by queries in BigQuery can run slowly when there is significant data skew on the grouped columns. Since the query is grouping by country, if most rows have the same country value, all that data will need to be shuffled to a single reducer to perform the aggregation. This can cause a data skew slowdown.\n\nOptions A and B might cause general slowness but are unlikely to affect this specific grouping query. Option C could also cause some slowness but not to the degree that heavy data skew on the grouped column could. So D is the most likely root cause. Optimizing the data distribution to reduce skew on the grouped column would likely speed up this query."
      },
      {
        "date": "2023-12-12T15:09:00.000Z",
        "voteCount": 1,
        "content": "Data skew is when one or some partitions have significantly more data compared to other partitions. Data-skew is usually the result of operations that require re-partitioning the data, mostly join and grouping ( GroupBy ) operations. So D."
      },
      {
        "date": "2023-01-26T06:19:00.000Z",
        "voteCount": 4,
        "content": "D. Most rows in the [myproject:mydataset.mytable] table have the same value in the country column, causing data skew\n\nData skew occurs when one or more values in a column have a disproportionately large number of rows compared to other values in that column. This can cause performance issues when running queries that group by that column, like the one in the question. In this case, if most of the rows in the [myproject:mydataset.mytable] table have the same value in the country column, then the query will need to process a large number of rows with that value, which can cause significant delay."
      },
      {
        "date": "2023-01-04T06:29:00.000Z",
        "voteCount": 3,
        "content": "D is right"
      },
      {
        "date": "2022-12-20T03:51:00.000Z",
        "voteCount": 2,
        "content": "data skewing causing imbalance in data distribution across slots. It also causes errors if the group by column has NULLS. Since option C does not call out the Group by column, D is a closer answer contextually"
      },
      {
        "date": "2022-11-15T05:35:00.000Z",
        "voteCount": 3,
        "content": "A is the best option becouse the color bar show the high number of reads and i think its not a skew becouse biguery was build to compute the data fast"
      },
      {
        "date": "2022-09-05T02:05:00.000Z",
        "voteCount": 11,
        "content": "The query would throw the error because you're using a group by clause on country but not aggregating city or state."
      },
      {
        "date": "2022-08-26T12:21:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/best-practices-performance-patterns"
      },
      {
        "date": "2022-05-01T08:46:00.000Z",
        "voteCount": 18,
        "content": "BTW, how is the query even syntactically valid? It has non aggregated columns in the SELECT part of the query. That query will not run in the first place, unless I'm missing something."
      },
      {
        "date": "2022-03-04T08:56:00.000Z",
        "voteCount": 1,
        "content": "D\nImage says that average(dark) and maximum(light) have difference in few times, this it is a skew\n\nhttps://cloud.google.com/bigquery/query-plan-explanation\nThe color indicators show the relative timings for all steps across all stages. For example, the COMPUTE step of Stage 00 shows a bar whose shaded fraction is 21/30 since 30ms is the maximum time spent in a single step of any stage. The parallel input information shows that each stage required only a single worker, so there's no variance between average and slowest timings."
      },
      {
        "date": "2022-01-14T12:55:00.000Z",
        "voteCount": 3,
        "content": "https://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d"
      },
      {
        "date": "2022-01-14T12:54:00.000Z",
        "voteCount": 1,
        "content": "https://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d"
      },
      {
        "date": "2022-01-04T07:45:00.000Z",
        "voteCount": 5,
        "content": "D\nColors: Purple is reading, Blue is writing. so the majority is reading.\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-patterns"
      },
      {
        "date": "2021-11-29T18:05:00.000Z",
        "voteCount": 1,
        "content": "If you read this https://medium.com/slalom-build/using-bigquery-execution-plans-to-improve-query-performance-af141b0cc33d C can't be right because the skewness happen when the column you use for grouping contains lots of null values, here C mentions columns that aren't part of the grouping clause. \n\nD, that's not how data get skewed, it gets skewed due to null values.\n\nA is the only answer here."
      },
      {
        "date": "2021-12-04T05:47:00.000Z",
        "voteCount": 1,
        "content": "A Cant be answer. Since users whenever running queries facing the problems."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 54,
    "url": "https://www.examtopics.com/discussions/google/view/16670-exam-professional-data-engineer-topic-1-question-54/",
    "body": "Your globally distributed auction application allows users to bid on items. Occasionally, users place identical bids at nearly identical times, and different application servers process those bids. Each bid event contains the item, amount, user, and timestamp. You want to collate those bid events into a single location in real time to determine which user bid first. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a file on a shared file and have the application servers write all bid events to that file. Process the file with Apache Hadoop to identify which user bid first.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave each application server write the bid events to Cloud Pub/Sub as they occur. Push the events from Cloud Pub/Sub to a custom endpoint that writes the bid event information into Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a MySQL database for each application server to write bid events into. Periodically query each of those distributed MySQL databases and update a master MySQL database with bid event information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud Dataflow. Give the bid for each item to the user in the bid event that is processed first.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 31,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T08:17:00.000Z",
        "voteCount": 64,
        "content": "I'd go with B: real-time is requested, and the only scenario for real time (in the 4 presented) is the use of pub/sub with push."
      },
      {
        "date": "2022-02-14T01:17:00.000Z",
        "voteCount": 5,
        "content": "B. \n- for realtime pub/sub push is critical. pull creates latency. (eliminates D)\n- process by event-time, not by process -time (eliminates D)"
      },
      {
        "date": "2022-03-25T08:47:00.000Z",
        "voteCount": 1,
        "content": "no push avail: https://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#streaming-pull-migration"
      },
      {
        "date": "2023-02-23T04:14:00.000Z",
        "voteCount": 1,
        "content": "The dataflow is designed for realtime processing. and this case should be needed to use dataflow because there is no option to order the data if not using dataflow. So D is answer I think"
      },
      {
        "date": "2023-01-04T06:35:00.000Z",
        "voteCount": 1,
        "content": "Agree with B"
      },
      {
        "date": "2021-04-17T14:21:00.000Z",
        "voteCount": 28,
        "content": "i would go with option B, Cause option D states \"Give the bid for each item to the user in the bid event that is processed first\"  . The requirement is to get the first bid based on event time not processed first in dataflow."
      },
      {
        "date": "2023-02-04T03:15:00.000Z",
        "voteCount": 3,
        "content": "This approach is not ideal because it requires a custom endpoint to write the bid event information into Cloud SQL. This adds additional complexity and potential points of failure to the architecture, as well as adding latency to the processing of bid events, since the data must be written to both Pub/Sub and Cloud SQL. Additionally, it can be more challenging to ensure that bid events are processed in the order they were received, since the data is being written to multiple databases. Finally, using a single database to store bid events could limit scalability and availability, and can also result in slow query performance."
      },
      {
        "date": "2020-04-10T18:19:00.000Z",
        "voteCount": 34,
        "content": "D\nThe need is to collate the messages in real-time. We need to de-dupe the messages based on timestamp of when the event occurred. This can be done by publishing ot Pub-Sub and consuming via Dataflow."
      },
      {
        "date": "2022-02-14T01:23:00.000Z",
        "voteCount": 2,
        "content": "Yeap, that's why B is the right one. It has pub/sub push, more real time than pub/sub pull. You need to aware at some point , something has to be pulled which adds a latency."
      },
      {
        "date": "2023-03-21T19:31:00.000Z",
        "voteCount": 3,
        "content": "D isnt correct, Pub/sub can send messages out of order, it is no guaranty that the event with lowest timestamp will be processed first\nB is correct"
      },
      {
        "date": "2024-09-22T07:40:00.000Z",
        "voteCount": 1,
        "content": "It feels like it depends what's actually in the dataflow pipeline. D I believe is the answer they intend, even if messages are pulled out of order."
      },
      {
        "date": "2024-07-17T13:02:00.000Z",
        "voteCount": 1,
        "content": "While using Cloud Pub/Sub for real-time event streaming is a good choice, pushing events to a custom endpoint that writes to Cloud SQL introduces additional complexity.\nCustom endpoints need to be maintained, and the process of writing to Cloud SQL might not be as efficient as using a purpose-built data processing service."
      },
      {
        "date": "2024-07-03T04:53:00.000Z",
        "voteCount": 1,
        "content": "In D the user gets it where the data is ingested first. That can be wrong for a global auction solution"
      },
      {
        "date": "2024-05-06T02:58:00.000Z",
        "voteCount": 2,
        "content": "This is the most suitable solution for the requirements. Google Cloud Pub/Sub can handle high throughput and low-latency data ingestion. Coupled with Google Cloud Dataflow, which can process data streams in real time, this setup allows for immediate processing of bid events. Dataflow can also handle ordering and timestamp extraction, crucial for determining which bid came first. This architecture supports scalability and real-time analytics, which are essential for a global auction system."
      },
      {
        "date": "2024-05-05T20:44:00.000Z",
        "voteCount": 2,
        "content": "the Answer should be D for the following \nReal-time Processing\nCentralized Processing\nWinner Determination\nalso, B is unsuitable as While Pub/Sub can ingest data, Cloud SQL is a relational database not designed for real-time processing at this scale. Maintaining a custom endpoint adds complexity."
      },
      {
        "date": "2024-03-14T01:37:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud Pub/Sub is a scalable and reliable messaging service that can handle high volumes of data and deliver messages in real-time. By having each application server publish bid events to Cloud Pub/Sub, you ensure that all bid events are collected centrally.\n\nUsing Cloud Dataflow with a pull subscription allows you to process the bid events in real-time. Cloud Dataflow provides a managed service for stream and batch processing, and it can handle the real-time processing requirements efficiently.\n\nBy processing the bid events with Cloud Dataflow, you can determine which user bid first by applying the appropriate logic within your Dataflow pipeline. This approach ensures scalability, reliability, and real-time processing capabilities, making it suitable for handling bid events from multiple application servers."
      },
      {
        "date": "2024-01-30T09:48:00.000Z",
        "voteCount": 1,
        "content": "B should be the answer, because it writes the bid into Cloud SQL to a distributed system. This way the customer know if they get the bid or not, immediately.\nAlso, push requests are faster than pull requests, hence they are better for realtime experience."
      },
      {
        "date": "2023-12-24T04:13:00.000Z",
        "voteCount": 1,
        "content": "pub/sub for entry time stamp + event time \ndataflow for processing and dataflow is better for real time"
      },
      {
        "date": "2023-12-09T03:53:00.000Z",
        "voteCount": 1,
        "content": "To accurately determine who bid first in a globally distributed auction application, utilizing a push mechanism instead of a pull mechanism is generally considered the more reliable approach. \nB should be correct answer."
      },
      {
        "date": "2023-11-21T06:21:00.000Z",
        "voteCount": 2,
        "content": "key words is \"single location in real time\""
      },
      {
        "date": "2023-11-13T13:41:00.000Z",
        "voteCount": 2,
        "content": "Answer : D\nWe need to de-dupe the messages based on timestamp of when the event occurred. This can be done by publishing ot Pub-Sub and consuming via Dataflow.\nD sounds like a complete answer. B does not."
      },
      {
        "date": "2023-10-09T21:00:00.000Z",
        "voteCount": 1,
        "content": "D. Have each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud.\nThis approach leverages Google Cloud Pub/Sub for real-time data ingestion and Google Cloud Dataflow for real-time data processing, ensuring that bids are processed as they occur, which aligns with real-time requirements.\n\nIt's not B because there is a step involving a custom endpoint that writes data into Cloud SQL. This additional step could introduce some latency, and it's important to ensure that the custom endpoint and Cloud SQL database can handle the real-time load effectively."
      },
      {
        "date": "2023-10-15T23:51:00.000Z",
        "voteCount": 1,
        "content": "But D treats the bids according to the processed time. We need to consider event time that's why B is the right answer."
      },
      {
        "date": "2023-10-07T23:15:00.000Z",
        "voteCount": 1,
        "content": "D. Have each application server write the bid events to Google Cloud Pub/Sub as they occur. Use a pull subscription to pull the bid events using Google Cloud Dataflow. Give the bid for each item to the user in the bid event that is processed first."
      },
      {
        "date": "2023-10-02T23:45:00.000Z",
        "voteCount": 2,
        "content": "B. Have each application server write the bid events to Cloud Pub/Sub as they occur. Push the events from Cloud Pub/Sub to a custom endpoint that writes the bid event information into Cloud SQL.  is correct"
      },
      {
        "date": "2023-09-22T13:12:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer is B. option D is based on processing first and not based on event first. so option D cannot be right answer"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 55,
    "url": "https://www.examtopics.com/discussions/google/view/16669-exam-professional-data-engineer-topic-1-question-55/",
    "body": "Your organization has been collecting and analyzing data in Google BigQuery for 6 months. The majority of the data analyzed is placed in a time-partitioned table named events_partitioned. To reduce the cost of queries, your organization created a view called events, which queries only the last 14 days of data. The view is described in legacy SQL. Next month, existing applications will be connecting to BigQuery to read the events data via an ODBC connection. You need to ensure the applications can connect. Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new view over events using standard SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new partitioned table using a standard SQL query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new view over events_partitioned using standard SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account for the ODBC connection to use for authentication\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared \u05d2\u20acevents\u05d2\u20ac"
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "CE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T08:14:00.000Z",
        "voteCount": 52,
        "content": "C = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role."
      },
      {
        "date": "2020-03-21T02:38:00.000Z",
        "voteCount": 10,
        "content": "Answer: CD"
      },
      {
        "date": "2024-01-07T00:36:00.000Z",
        "voteCount": 1,
        "content": "I think question should be rewrite slightly like which 3 actions should you take rather than 2 ..\nThen answer would be A,D and E..No ambiguity then"
      },
      {
        "date": "2024-01-04T22:29:00.000Z",
        "voteCount": 1,
        "content": "ODBC connections require standard SQL, not legacy SQL.\nService account for the ODBC connection"
      },
      {
        "date": "2023-10-01T22:50:00.000Z",
        "voteCount": 2,
        "content": "This dump is full of wrong answers - not sure which one to go for."
      },
      {
        "date": "2023-08-05T11:19:00.000Z",
        "voteCount": 1,
        "content": "CD..... C because, ODBC drivers dont support switch b/w legacy SQL &amp; google SQL, hence better to create a new view from recent partitioned table &amp; D as Google best practice for role binding"
      },
      {
        "date": "2023-06-17T08:41:00.000Z",
        "voteCount": 1,
        "content": "the answer is C &amp; D"
      },
      {
        "date": "2023-02-14T04:30:00.000Z",
        "voteCount": 2,
        "content": "answer: A &amp; D\nConfusion here: Legacy SQL vs Standard, BQ supports legacy SQL but ODBC or Most RDBMS connection doesn't support Legacy SQL, so in this case we need to create a new view on existing view or replace the existing one by changing syntax. \nFor ODBC, you just need a service account to authenticate as its external service connection. Option E is not necessary."
      },
      {
        "date": "2023-02-23T15:35:00.000Z",
        "voteCount": 1,
        "content": "Go for B, create a new view from the table, If you modify the syntex in option A, its also mean you created a new view on table :P"
      },
      {
        "date": "2023-01-26T06:43:00.000Z",
        "voteCount": 4,
        "content": "D. Create a service account for the ODBC connection to use for authentication. This service account will be used to authenticate the ODBC connection, and will be granted specific permissions to access the BigQuery resources.\nE. Create a Cloud IAM role for the ODBC connection and shared events. This role will be used to grant permissions to the service account created in step D, and will allow the applications to access the events view in BigQuery.\nCreating a new view over events using standard SQL may also be beneficial to improve performance and compatibility with the applications, but is not required for the ODBC connection to work."
      },
      {
        "date": "2023-01-19T08:55:00.000Z",
        "voteCount": 3,
        "content": "INFO:\n- The majority of the data analyzed is placed in a time-partitioned table named events_partitioned. \n- To reduce the cost of queries, your organization created a view called events, which queries only the last 14 days of data. \n- The view is described in legacy SQL. \nQUESTION:\nNext month, existing applications will be connecting to BigQuery to read the events data via an ODBC connection. You need to ensure the applications can connect. Which two actions should you take? (Choose two.)\n\n-&gt; First and foremost we need to understand the information. So our actual data is stored in events_partitioned table. The organization is currently using view called events to reduce the cost. \n-&gt; Since the view called events only has last 14 days of data we cannot use that view.\n-&gt; We also cannot use that view because standard SQL is not used to describe the view. In order to connectt ODBC we need a view described by standard SQL."
      },
      {
        "date": "2023-01-19T19:01:00.000Z",
        "voteCount": 1,
        "content": "A. Create a new view over events using standard SQL\n-&gt; Wrong, events view contains only last 14 days of data and also it uses Legacy SQL.\n\nB. Create a new partitioned table using a standard SQL query\n-&gt; Partitioned Table is not helpful in this situation.Hence, I am ruling it out.\n\nC. Create a new view over events_partitioned using standard SQL\n-&gt; Correct this is exactly what we need. \n1.We need to create a new view over events_partitioned.\n2. We need to use Standard SQL.\nThis is a valid option.\n\nD. Create a service account for the ODBC connection to use for authentication.\n- Correct answer because we are required to authenticate before ODBC connection.\n\nE. Create a Google Cloud Identity and Access Management (Cloud IAM) role for the ODBC connection and shared \u05d2\u20acevents\u05d2\u20ac\n- This option is of no use in this scenario"
      },
      {
        "date": "2023-01-19T00:38:00.000Z",
        "voteCount": 1,
        "content": "CE is the correct answer"
      },
      {
        "date": "2022-08-26T12:28:00.000Z",
        "voteCount": 2,
        "content": "needed a service account for ODBC drivers\nstandard SQL vs legacy SQL."
      },
      {
        "date": "2022-07-19T09:58:00.000Z",
        "voteCount": 4,
        "content": "1. Create Services account from IAM &amp; Admin\n2. Add Services account permission Roles as \"BigQuery Admin\" or any custom Role.\nOther options are not related ' to ensure the applications can connect'"
      },
      {
        "date": "2022-07-19T09:59:00.000Z",
        "voteCount": 4,
        "content": "typo - D; E"
      },
      {
        "date": "2022-03-04T09:23:00.000Z",
        "voteCount": 1,
        "content": "As stated by jvg637\n\nC = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role."
      },
      {
        "date": "2022-01-04T07:58:00.000Z",
        "voteCount": 3,
        "content": "As stated by jvg637\n\nC = A standard SQL query cannot reference a view defined using legacy SQL syntax.\nD = For the ODBC drivers is needed a service account which will get a standard Bigquery role."
      },
      {
        "date": "2021-12-18T11:46:00.000Z",
        "voteCount": 6,
        "content": "A standard SQL query cannot reference a view defined using legacy SQL syntax. In order to connect through ODBC connection, we need to use standard SQL. So, we need to create a new view over events_partitioned table using standard SQL which is C. Need service account to connect through ODBC which is option D. Check the links below.\nI am not sure about A whether we can create a view over another view which was built using legacy SQL\nhttps://cloud.google.com/bigquery/docs/views\nhttps://cloud.google.com/community/tutorials/bigquery-from-excel\nhttps://www.simba.com/products/BigQuery/doc/ODBC_InstallGuide/mac/content/odbc/bq/configuring/authenticating/serviceaccount.htm"
      },
      {
        "date": "2021-12-18T11:46:00.000Z",
        "voteCount": 2,
        "content": "It has to be standard because of this:\nGoogle has collaborated with Magnitude Simba to provide ODBC and JDBC drivers that leverage the power of BigQuery\u2019s \u2013&gt;standard SQL.\nOn what should we build the view, on the events_partitioned, just like the view you had before but in standard SQL.\nno sense in creating a new partitioned table as B says.\nTo let it access the data you should access with a service account.\nYou can configure the driver to authenticate the connection with a Google service account. When you authenticate your connection this way, the driver handles authentication on behalf of the service account, so that an individual user account is not directly involved and no user input is required.\nSo I think is C and D."
      },
      {
        "date": "2021-11-26T04:09:00.000Z",
        "voteCount": 8,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C,D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 56,
    "url": "https://www.examtopics.com/discussions/google/view/82857-exam-professional-data-engineer-topic-1-question-56/",
    "body": "You have enabled the free integration between Firebase Analytics and Google BigQuery. Firebase now automatically creates a new table daily in BigQuery in the format app_events_YYYYMMDD. You want to query all of the tables for the past 30 days in legacy SQL. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the TABLE_DATE_RANGE function\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the WHERE_PARTITIONTIME pseudo column",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse WHERE date BETWEEN YYYY-MM-DD AND YYYY-MM-DD",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SELECT IF.(date &gt;= YYYY-MM-DD AND date &lt;= YYYY-MM-DD"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-19T12:29:00.000Z",
        "voteCount": 9,
        "content": "A. is correct according to this link:\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql"
      },
      {
        "date": "2024-09-22T07:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/reference/legacy-sql#table-date-range"
      },
      {
        "date": "2024-02-22T16:08:00.000Z",
        "voteCount": 1,
        "content": "We don\u2019t have TABLE DATE RANGE function in legacy SQL. Answer should be B"
      },
      {
        "date": "2024-05-17T05:56:00.000Z",
        "voteCount": 1,
        "content": "We actually have, look at the documentation, \n\nhttps://cloud.google.com/bigquery/docs/reference/legacy-sql"
      },
      {
        "date": "2023-10-14T13:02:00.000Z",
        "voteCount": 1,
        "content": "The recommended action is to use the TABLE_DATE_RANGE function (option A). This function allows you to specify a range of dates to query across multiple tables."
      },
      {
        "date": "2023-10-02T23:52:00.000Z",
        "voteCount": 2,
        "content": "The TABLE_DATE_RANGE function in BigQuery is a table wildcard function that can be used to query a range of daily tables. It takes two arguments: a table prefix and a date range. The table prefix is the beginning of the table names, and the date range is the start and end dates of the tables to be queried.\n\nThe TABLE_DATE_RANGE function will expand to cover all tables in the dataset that match the table prefix and are within the date range. For example, if you have a dataset that contains daily tables named my_table_20230804, my_table_20230805, and my_table_20230806, you could use the TABLE_DATE_RANGE function to query all of the tables in the dataset between August 4, 2023 and August 6, 2023 as follows:\nSELECT *\nFROM TABLE_DATE_RANGE('my_table_', '2023-08-04', '2023-08-06');"
      },
      {
        "date": "2023-01-19T19:04:00.000Z",
        "voteCount": 2,
        "content": "A Is correct.\n\nTABLE_DATE_RANGE()\t : Queries multiple daily tables that span a date range."
      },
      {
        "date": "2023-01-22T18:12:00.000Z",
        "voteCount": 2,
        "content": "Example:\nSELECT *\nFROM TABLE_DATE_RANGE(app_events_, TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 30 DAY), CURRENT_TIMESTAMP())"
      },
      {
        "date": "2022-12-16T05:14:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/reference/legacy-sql\nTABLE_DATE_RANGE()\tQueries multiple daily tables that span a date range."
      },
      {
        "date": "2022-11-10T12:26:00.000Z",
        "voteCount": 1,
        "content": "A. is Correct...\nfrom...https://cloud.google.com/blog/products/management-tools/using-bigquery-and-firebase-analytics-to-understand-your-mobile-app\nSELECT\n  user_dim.app_info.app_platform as appPlatform,\n  user_dim.device_info.device_category as deviceType,\n  COUNT(user_dim.device_info.device_category) AS device_type_count FROM\nTABLE_DATE_RANGE([firebase-analytics-sample-data:android_dataset.app_events_], DATE_ADD('2016-06-07', -7, 'DAY'), CURRENT_TIMESTAMP()),\nTABLE_DATE_RANGE([firebase-analytics-sample-data:ios_dataset.app_events_], DATE_ADD('2016-06-07', -7, 'DAY'), CURRENT_TIMESTAMP())\nGROUP BY\n  1,2\nORDER BY\n  device_type_count DESC"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 57,
    "url": "https://www.examtopics.com/discussions/google/view/16673-exam-professional-data-engineer-topic-1-question-57/",
    "body": "Your company is currently setting up data pipelines for their campaign. For all the Google Cloud Pub/Sub streaming data, one of the important business requirements is to be able to periodically identify the inputs and their timings during their campaign. Engineers have decided to use windowing and transformation in Google Cloud Dataflow for this purpose. However, when testing this feature, they find that the Cloud Dataflow job fails for the all streaming insert. What is the most likely cause of this problem?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey have not assigned the timestamp, which causes the job to fail",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey have not set the triggers to accommodate the data coming in late, which causes the job to fail",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey have not applied a global windowing function, which causes the job to fail when the pipeline is created",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThey have not applied a non-global windowing function, which causes the job to fail when the pipeline is created\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T19:42:00.000Z",
        "voteCount": 66,
        "content": "Answer: D\nDescription: Caution: Beam\u2019s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n\u2014-&gt;&gt;&gt;&gt;&gt;&gt;Set a non-global windowing function. See Setting your PCollection\u2019s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur.\n\u2014-&gt;&gt;&gt;&gt;If you don\u2019t set a non-global windowing function or a non-default trigger for your unbounded PCollection and subsequently use a grouping transform such as GroupByKey or Combine, your pipeline will generate an error upon construction and your job will fail.\nSo it looks like D"
      },
      {
        "date": "2023-01-22T19:39:00.000Z",
        "voteCount": 1,
        "content": "Why not C?\nBecause I think that the most likely cause of the problem is C. They have not applied a global windowing function, which causes the job to fail when the pipeline is created.\n\nIn Dataflow, windowing is used to divide the input data into smaller time intervals, called windows. Without a windowing function, all the data may be treated as part of the same window and the pipeline may not be able to process the data correctly. In this specific scenario, the engineers are trying to use windowing and transformation in Google Cloud Dataflow to periodically identify the inputs and their timings during the campaign, so it's likely that they need to use a windowing function to divide the data into smaller time intervals in order to process it correctly. Not applying a windowing function, or applying the wrong one can cause the job to fail.\n\nSomeone Clarify? Am I missing an important point?"
      },
      {
        "date": "2023-07-22T07:01:00.000Z",
        "voteCount": 3,
        "content": "You are missing that the global window is the default window that we typically use for batch processing. The global window by default waits until all data is available before processing it so if you want to use it with streaming you need to set some custom trigger so that we don't wait indefinitely to wait until we aggregate. All in all it doesn't sound right. \n\nhttps://www.youtube.com/watch?v=oJ-LueBvOcM\nhttps://www.youtube.com/watch?v=MuFA6CSti6M"
      },
      {
        "date": "2020-03-15T09:05:00.000Z",
        "voteCount": 15,
        "content": "Global windowing is the default behavior, so I don't think C is right.\nAn error can occur if a non-global window or a non-default trigger is not set.\nI would say D.\n(https://beam.apache.org/documentation/programming-guide/#windowing)"
      },
      {
        "date": "2024-05-20T07:50:00.000Z",
        "voteCount": 2,
        "content": "The most likely cause of this problem is A. They have not assigned the timestamp, which causes the job to fail.\n\nHere's why:\n\nImportance of Timestamps in Windowing: Windowing in Dataflow relies on timestamps to group elements into windows. If timestamps are not explicitly assigned or extracted from the data, Dataflow cannot determine which elements belong to which windows, leading to failures in the job.\nLet's look at the other options:\n\nB. They have not set the triggers to accommodate the data coming in late: While triggers are important for managing late data, not setting them would not cause the job to fail for all streaming inserts. It might affect the accuracy of the results, but the job would still run.\nC &amp; D. Global vs. Non-global Windowing: The choice between global and non-global windowing depends on the specific requirements of the analysis. While incorrect windowing choices can lead to unexpected results, they would not typically cause the job to fail completely."
      },
      {
        "date": "2024-01-31T06:05:00.000Z",
        "voteCount": 1,
        "content": "D\nYou have to apply a non-global windowing function because the global windowing function is a default windowing function for every pub/sub stream or batch data."
      },
      {
        "date": "2023-09-25T11:27:00.000Z",
        "voteCount": 2,
        "content": "option B: They have not set the triggers to accommodate the data coming in late, which causes the job to fail.\n\nIn a streaming data processing pipeline, it's common to encounter data that arrives late, meaning it arrives after the event time has passed for the associated window. If you don't handle late data appropriately by setting triggers, it can cause issues in your pipeline, including job failures."
      },
      {
        "date": "2023-04-24T21:58:00.000Z",
        "voteCount": 2,
        "content": "gpt: The most likely cause of the problem is A, that they have not assigned the timestamp.\nIn streaming data processing, timestamps are essential for proper windowing and triggering of data. Without timestamps, the system cannot correctly determine which window a particular piece of data belongs to, or when it is safe to trigger processing of a window. If the engineers did not assign timestamps to the data, the Cloud Dataflow job would not be able to process the data correctly, and it would fail.\n\nOption B, not setting triggers to accommodate late data, is also an important consideration for streaming data processing. However, it is less likely to cause the job to fail outright than missing timestamps.\n\nOption C, not applying a global windowing function, and Option D, not applying a non-global windowing function, are also important considerations for windowing in Cloud Dataflow. However, neither of these would cause the job to fail when the pipeline is created. Instead, they would affect the performance and correctness of the data processing."
      },
      {
        "date": "2023-04-25T01:01:00.000Z",
        "voteCount": 4,
        "content": "without a correct timestamp, the pipeline still run fine with the default timestamp. The result maybe incorrect but the job will not fail."
      },
      {
        "date": "2023-04-27T04:28:00.000Z",
        "voteCount": 2,
        "content": "okay, so D maybe"
      },
      {
        "date": "2023-04-27T04:35:00.000Z",
        "voteCount": 1,
        "content": "gpt pt2: For example, if your use case requires you to calculate a running average of values over a fixed time interval, you would likely use a non-global windowing function with a fixed time interval. On the other hand, if you need to perform a computation on the entire stream of data at once, a global windowing function might be more appropriate.\n\nSo, the choice of windowing function should be based on the specific requirements of the data processing task at hand, and it may or may not be important to apply a non-global windowing function when the pipeline is created.\n------ \nquestion says that we need to identify streaming input, time, so non-global needed, let it be d..."
      },
      {
        "date": "2023-04-27T04:35:00.000Z",
        "voteCount": 1,
        "content": "- is it impotant to apply a non-global windowing function when the pipeline is created?\nGPT: It is important to choose the appropriate windowing function for your data processing needs, but whether it should be a global or non-global windowing function depends on the requirements of your specific use case.\nA global windowing function considers all data elements within a bounded time interval as a single window, whereas a non-global windowing function divides the data stream into smaller windows based on specified criteria (such as a fixed time interval or a number of elements)."
      },
      {
        "date": "2023-11-17T13:15:00.000Z",
        "voteCount": 1,
        "content": "Which is the moment I decided that AI was nothing to fear"
      },
      {
        "date": "2023-03-19T00:01:00.000Z",
        "voteCount": 1,
        "content": "what about A? This can cause the job to fail"
      },
      {
        "date": "2023-02-27T01:32:00.000Z",
        "voteCount": 1,
        "content": "A: note that without a correct timestamp, the pipeline still run fine with the default timestamp. The result maybe incorrect but the job will not fail.\nD: For unbound collection, this will fail if any aggregation function is done."
      },
      {
        "date": "2023-02-14T04:55:00.000Z",
        "voteCount": 1,
        "content": "Answer: A\nAll Streaming Insert failed, because there is no TimeStamp added, otherwise there is already a DEFAULT global windowing function and can execute without assigning any windowing function. \nI mean first there should be Timestamp in the data, then according to our aggregation outcome either its full time (global) or batch/chunks time aggregation(non global)  will be performed."
      },
      {
        "date": "2022-12-16T05:15:00.000Z",
        "voteCount": 1,
        "content": "https://beam.apache.org/documentation/programming-guide/#windowing\nBeam\u2019s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n\nSet a non-global windowing function. See Setting your PCollection\u2019s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur."
      },
      {
        "date": "2022-09-15T07:42:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-09-10T03:55:00.000Z",
        "voteCount": 2,
        "content": "Correct answer is D. C does not make sense because for unbounded source like Pub/Sub, the global functions are applied by default. The reason for failure would be they are using specific aggregations that require non-global window functions, e.g. tumbling or hopping windows."
      },
      {
        "date": "2022-06-08T09:04:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\nhttps://beam.apache.org/documentation/programming-guide/#windowing-bounded-collections\n\n8.2.4. The single global window\nBy default, all data in a PCollection is assigned to the single global window, and late data is discarded. If your data set is of a fixed size, you can use the global window default for your PCollection (not our case because we are streaming).\nYou can use the single global window if you are working with an unbounded data set (e.g. from a streaming data source) but use caution when applying aggregating transforms such as GroupByKey and Combine. The single global window with a default trigger generally requires the entire data set to be available before processing, which is not possible with continuously updating data. To perform aggregations on an unbounded PCollection that uses global windowing, you should specify a non-default trigger for that PCollection."
      },
      {
        "date": "2022-04-13T11:02:00.000Z",
        "voteCount": 3,
        "content": "Why A won't cause an error?"
      },
      {
        "date": "2022-03-31T01:32:00.000Z",
        "voteCount": 3,
        "content": "Answer: D\nDescription: Caution: Beam\u2019s default windowing behavior is to assign all elements of a PCollection to a single, global window and discard late data, even for unbounded PCollections. Before you use a grouping transform such as GroupByKey on an unbounded PCollection, you must do at least one of the following:\n\u2014-&gt;&gt;&gt;&gt;&gt;&gt;Set a non-global windowing function. See Setting your PCollection\u2019s windowing function.\nSet a non-default trigger. This allows the global window to emit results under other conditions, since the default windowing behavior (waiting for all data to arrive) will never occur.\n\u2014-&gt;&gt;&gt;&gt;If you don\u2019t set a non-global windowing function or a non-default trigger for your unbounded PCollection and subsequently use a grouping transform such as GroupByKey or Combine, your pipeline will generate an error upon construction and your job will fail.\nSo it looks like D"
      },
      {
        "date": "2022-01-04T08:03:00.000Z",
        "voteCount": 3,
        "content": "D.\nhttps://beam.apache.org/documentation/programming-guide/#windowing"
      },
      {
        "date": "2021-11-26T10:41:00.000Z",
        "voteCount": 4,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 58,
    "url": "https://www.examtopics.com/discussions/google/view/17090-exam-professional-data-engineer-topic-1-question-58/",
    "body": "You architect a system to analyze seismic data. Your extract, transform, and load (ETL) process runs as a series of MapReduce jobs on an Apache Hadoop cluster. The ETL process takes days to process a data set because some steps are computationally expensive. Then you discover that a sensor calibration step has been omitted. How should you change your ETL process to carry out sensor calibration systematically in the future?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify the transformMapReduce jobs to apply sensor calibration before they do anything else.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce a new MapReduce job to apply sensor calibration to raw data, and ensure all other MapReduce jobs are chained after this.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd sensor calibration data to the output of the ETL process, and document that all users need to apply sensor calibration themselves.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an algorithm through simulation to predict variance of data output from the last MapReduce job based on calibration factors, and apply the correction to all data."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-22T03:11:00.000Z",
        "voteCount": 58,
        "content": "Should go with B. Two reasons, it is a cleaner approach with single job to handle the calibration before the data is used in the pipeline. Second, doing this step in later stages can be complex and maintenance of those jobs in the future will become challenging."
      },
      {
        "date": "2021-08-03T09:21:00.000Z",
        "voteCount": 7,
        "content": "B. different MR jobs execute in series, adding 1 more job makes sense in this case."
      },
      {
        "date": "2020-03-27T19:52:00.000Z",
        "voteCount": 20,
        "content": "Answer: A\nDescription: My take on this is for sensor calibration you just need to update the transform function, rather than creating a whole new mapreduce job and storing/passing the values to next job"
      },
      {
        "date": "2021-05-27T16:38:00.000Z",
        "voteCount": 11,
        "content": "It's B. A would involving changing every single job (notice it said jobS, plural, not a single job). If that is computationally intensive, which it is, you're repeating a computationally intense process needlessly several times. SteelWarrior and YuriP are right on this one."
      },
      {
        "date": "2024-05-17T06:31:00.000Z",
        "voteCount": 1,
        "content": "Why all jobs, change only the first job for calibration, right?"
      },
      {
        "date": "2024-07-05T03:38:00.000Z",
        "voteCount": 1,
        "content": "I'll choose A. WHY? cause the process already takes DAYS and adding another step will increase the time more"
      },
      {
        "date": "2023-02-27T23:53:00.000Z",
        "voteCount": 1,
        "content": "What kinds of sensor calibrations exists? I don't understand how computation in pipeline would be expense due to calibration being omitted..?"
      },
      {
        "date": "2023-01-22T19:45:00.000Z",
        "voteCount": 1,
        "content": "B. Introduce a new MapReduce job to apply sensor calibration to raw data, and ensure all other MapReduce jobs are chained after this.\n\nThis approach would ensure that sensor calibration is systematically carried out every time the ETL process runs, as the new MapReduce job would be responsible for calibrating the sensors before the data is processed by the other steps. This would ensure that all data is calibrated before being analyzed, thus avoiding the omission of the sensor calibration step in the future.\nIt also allows you to chain all other MapReduce jobs after this one, so that the calibrated data is used in all the downstream jobs."
      },
      {
        "date": "2023-01-22T19:45:00.000Z",
        "voteCount": 1,
        "content": "Option A is not ideal, as it would be time-consuming to modify all the transformMapReduce jobs to apply sensor calibration before doing anything else, and there is a risk of introducing bugs or errors.\nOption C is not ideal, as it would rely on users to apply sensor calibration themselves, which would be inefficient and could introduce inconsistencies in the data.\nOption D is not ideal, as it would require a lot of simulation and testing to develop an algorithm that can predict the variance of data output accurately and it may not be as accurate as calibrating the sensor directly."
      },
      {
        "date": "2022-12-16T05:16:00.000Z",
        "voteCount": 1,
        "content": "It is much cleaner approach"
      },
      {
        "date": "2022-12-14T14:26:00.000Z",
        "voteCount": 1,
        "content": "Best approach is calibration will be separate job because if we need to tune the calibration later also it would be to maintain without worries about all other jobs."
      },
      {
        "date": "2022-12-07T01:49:00.000Z",
        "voteCount": 1,
        "content": "Should be B. My reason, this is like an Anti corruption layer, and that's a good practice, \nC- , if you modify your transformMapReduce will be harder to test and debug, so it's a bad practice.\nC the idea de introduce manual operation is an anti patron and has a lot of problems\nD It's overkilling, a don't have sense in this scenario."
      },
      {
        "date": "2022-01-16T13:32:00.000Z",
        "voteCount": 3,
        "content": "SteelWarrior explanation is correct :)"
      },
      {
        "date": "2022-01-15T11:01:00.000Z",
        "voteCount": 1,
        "content": "SteelWarrior explanation is correct"
      },
      {
        "date": "2022-01-04T08:06:00.000Z",
        "voteCount": 1,
        "content": "SteelWarrior explanation is correct"
      },
      {
        "date": "2021-12-17T20:55:00.000Z",
        "voteCount": 1,
        "content": "SteelWarrior's answer is correct"
      },
      {
        "date": "2021-10-15T14:21:00.000Z",
        "voteCount": 1,
        "content": "Ans: B\nAdding a new job in the beginning of chain makes more sense than updating existing chain of jobs."
      },
      {
        "date": "2021-06-28T07:10:00.000Z",
        "voteCount": 5,
        "content": "Vote for 'B' (introduce new job) over 'A',  (instead of modifying existing job)"
      },
      {
        "date": "2020-08-03T02:24:00.000Z",
        "voteCount": 5,
        "content": "Should be B. It's a Data Quality step which has to go right after Raw Ingest. Otherwise you repeat the same step unknown (see \"job_s_\" in A) number of times, possibly for no reason, therefore extending ETL time."
      },
      {
        "date": "2020-03-21T03:31:00.000Z",
        "voteCount": 4,
        "content": "It's between A or B.\nShould choose A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 59,
    "url": "https://www.examtopics.com/discussions/google/view/80950-exam-professional-data-engineer-topic-1-question-59/",
    "body": "An online retailer has built their current application on Google App Engine. A new initiative at the company mandates that they extend their application to allow their customers to transact directly via the application. They need to manage their shopping transactions and analyze combined data from multiple datasets using a business intelligence (BI) tool. They want to use only a single database for this purpose. Which Google Cloud database should they choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud BigTable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-01-26T08:20:00.000Z",
        "voteCount": 10,
        "content": "B. Cloud SQL would be the most appropriate choice for the online retailer in this scenario. Cloud SQL is a fully-managed relational database service that allows for easy management and analysis of data using SQL. It is well-suited for applications built on Google App Engine and can handle the transactional workload of an e-commerce application, as well as the analytical workload of a BI tool."
      },
      {
        "date": "2023-07-22T07:11:00.000Z",
        "voteCount": 3,
        "content": "Cloud SQL seems to fit the best. It supports transactions and can be used to run queries and do analytics.\n\nBigQuery is good for the analysis part but it's not good for managing transactions. If the question needed a database just to store the data for analysis it would be ok. But if we want to update single transactions or add them row by row, then it's not good. BigQuery is not made to support an application. It's a DW.\n\nBigTable is can not carry transactions over multiple rows and is better for large scale analytics jobs. Also we should pick it for use-cases with high throughput/low latency requirements. Seems redundant."
      },
      {
        "date": "2023-04-16T12:14:00.000Z",
        "voteCount": 4,
        "content": "Big Query because of analysis"
      },
      {
        "date": "2023-04-11T15:29:00.000Z",
        "voteCount": 1,
        "content": "Should be bigtable"
      },
      {
        "date": "2024-09-22T08:33:00.000Z",
        "voteCount": 1,
        "content": "I can't really see that. Bigtable is only ever the right choice for noSql at vast scale."
      },
      {
        "date": "2023-03-18T12:56:00.000Z",
        "voteCount": 3,
        "content": "I think BigQuery makes sense here. It works for transactions too."
      },
      {
        "date": "2023-03-31T02:35:00.000Z",
        "voteCount": 2,
        "content": "I just did a session with an official trainer from Google that said BigTable is better."
      },
      {
        "date": "2023-10-24T08:33:00.000Z",
        "voteCount": 2,
        "content": "I'm an official trainer from Google and I can say that my best two options for this scenario would be Cloud SQL and BigQuery in that order. \nAlso we can consider datastore since we're using it with a web app, but it's another topic."
      },
      {
        "date": "2023-03-07T08:06:00.000Z",
        "voteCount": 4,
        "content": "A. \"They want to use only a single database for this purpose\" is a key requirement. You can use BigQuery for transactions, though it is not efficient. You can not use CloudSQL for analytics. So it is probably BQ."
      },
      {
        "date": "2024-09-22T08:34:00.000Z",
        "voteCount": 1,
        "content": "Yeah my thinking was the same, but actually cloud SQL is fine to connect BI tools to, which is specified in this question."
      },
      {
        "date": "2023-03-03T01:07:00.000Z",
        "voteCount": 2,
        "content": "Transactional Data need to written first by application before it could be analysed so cloudsql."
      },
      {
        "date": "2023-01-22T19:51:00.000Z",
        "voteCount": 2,
        "content": "Both BigQuery and Cloud Bigtable are valid options for this use case, but BigQuery is better suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool.\n\nBigQuery is a fully-managed, cloud-native data warehouse that enables super-fast SQL queries using the processing power of Google's infrastructure. It can handle large, complex datasets and is well-suited for both transactional and analytical workloads. It can also handle data from multiple datasets and can be integrated with other Google Cloud services, such as Dataflow, Dataproc and Looker for BI analysis.\n\nWhile Cloud Bigtable is also a good option for this use case as it is a highly scalable and performant NoSQL database that is well-suited for handling large amounts of data and high-write loads. It is not as good as BigQuery for analytical workloads and it may not be as well-suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool."
      },
      {
        "date": "2023-02-28T00:03:00.000Z",
        "voteCount": 1,
        "content": "Bigquery is a OLAP. So it could be not a answer I think."
      },
      {
        "date": "2023-01-22T19:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL and Cloud Datastore are also good options for certain use cases, but they may not be as well-suited for this specific scenario where the retailer needs to manage and analyze large amounts of data from multiple datasets using a BI tool."
      },
      {
        "date": "2023-01-13T12:21:00.000Z",
        "voteCount": 2,
        "content": "The Community is choosing Answer B - Cloud SQL, as per the question.\nHowever when they explain - they're speaking about BQ[????] \n\nSo is it BigQuery or Cloud SQL?"
      },
      {
        "date": "2022-12-16T05:20:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables"
      },
      {
        "date": "2022-12-16T05:17:00.000Z",
        "voteCount": 4,
        "content": "It needs support for transaction so cloud sql is the choice of database and with Bigquery we can still analyze cloud sql data via federated queries https://cloud.google.com/bigquery/docs/reference/legacy-sql"
      },
      {
        "date": "2022-12-14T14:36:00.000Z",
        "voteCount": 1,
        "content": "Most important part of question is transaction (RDBMS) strong ACID property database. Second part analysis of data, yes possible using any BI tool its possible with RDBMS db."
      },
      {
        "date": "2022-12-07T05:23:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/bigquery/docs/cloud-sql-federated-queries\nBigQuery Cloud SQL federation enables BigQuery to query data residing in Cloud SQL in real time, without copying or moving data. Query federation supports both MySQL (2nd generation) and PostgreSQL instances in Cloud SQL."
      },
      {
        "date": "2023-04-16T05:06:00.000Z",
        "voteCount": 2,
        "content": "Agreed. Two catches here: transactional, and BI tool. Although BigQuery nowadays can handle everything, if we specifically deal with questions highlighting transactional data, I believe to differenticate services, we should choose what they primarily mean to be ."
      },
      {
        "date": "2022-12-07T01:55:00.000Z",
        "voteCount": 2,
        "content": "C and D are not able to work with BI directly, so discard.\nA: It's the best option for BI for awful for transactions\nB: it's the best option for transaction, and works for BI, so this must be the answer"
      },
      {
        "date": "2022-11-26T00:30:00.000Z",
        "voteCount": 1,
        "content": "BigQuery for Analytics and BI"
      },
      {
        "date": "2022-11-04T07:53:00.000Z",
        "voteCount": 2,
        "content": "Cloud Sql is Used to store Transactional Data and supports Sql Transactions. Where as Big Query is used for Analytics."
      },
      {
        "date": "2022-11-01T15:29:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL supports transactions as well as analysis through a BI tool. Firestore/Datastore does not support SQL syntax typically needed to do analysis done by a BI tool. BigQuery is not suitable for transactional use case. BigTable does not support SQL.\nIt's A."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 60,
    "url": "https://www.examtopics.com/discussions/google/view/17096-exam-professional-data-engineer-topic-1-question-60/",
    "body": "You launched a new gaming app almost three years ago. You have been uploading log files from the previous day to a separate Google BigQuery table with the table name format LOGS_yyyymmdd. You have been using table wildcard functions to generate daily and monthly reports for all time ranges. Recently, you discovered that some queries that cover long date ranges are exceeding the limit of 1,000 tables and failing. How can you resolve this issue?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert all daily log tables into date-partitioned tables",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert the sharded tables into a single partitioned table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable query caching so you can cache data from previous months",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate views to cover each month, and query from these views"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T05:28:00.000Z",
        "voteCount": 38,
        "content": "should be B\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables"
      },
      {
        "date": "2020-07-05T05:05:00.000Z",
        "voteCount": 5,
        "content": "The above link does mention about shard ing benefits but only about partition tables.\nA is correct."
      },
      {
        "date": "2021-01-12T08:18:00.000Z",
        "voteCount": 6,
        "content": "keyword is single"
      },
      {
        "date": "2021-09-29T11:04:00.000Z",
        "voteCount": 12,
        "content": "you are right.\n\nPartitioning versus sharding\nTable sharding is the practice of storing data in multiple tables, using a naming prefix such as [PREFIX]_YYYYMMDD.\n\nPartitioning is recommended over table sharding, because partitioned tables perform better. With sharded tables, BigQuery must maintain a copy of the schema and metadata for each table. BigQuery might also need to verify permissions for each queried table. This practice also adds to query overhead and affects query performance.\n\nIf you previously created date-sharded tables, you can convert them into an ingestion-time partitioned table."
      },
      {
        "date": "2021-10-08T09:12:00.000Z",
        "voteCount": 5,
        "content": "The question mentions tables are sharded. So B is more appropriate answer I think.\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#convert-date-sharded-tables"
      },
      {
        "date": "2022-02-16T02:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables provides that info you are looking for. Shortly, partitioning performs better than sharding (PREFIX_yymmdd). and it is easy and supported that you can convert sharded tables into ingestion-time partitioned table.\nSo, B is only option and better one."
      },
      {
        "date": "2023-02-28T00:42:00.000Z",
        "voteCount": 1,
        "content": "is it already partitioned? there is a table [table]_yyyymmdd it seems to partitioned by date from log files. but I confuse why D. is not a answer? if there is only reason to fail from query that exceeding 1,000 tables then I think creating views could be solution because querying views containing under 1,000 tables by a view could be queried."
      },
      {
        "date": "2020-03-27T20:03:00.000Z",
        "voteCount": 26,
        "content": "Answer: B\nDescription: Google says that when you have multiple wildcard tables, best option is to shard it into single partitioned table. Time and cost efficient"
      },
      {
        "date": "2020-09-02T01:27:00.000Z",
        "voteCount": 2,
        "content": "Can you please share the reference?"
      },
      {
        "date": "2021-09-01T01:09:00.000Z",
        "voteCount": 7,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard"
      },
      {
        "date": "2023-04-24T22:26:00.000Z",
        "voteCount": 1,
        "content": "gpt: Thank you for your feedback and additional information. You are correct that partitioned tables have a limit of 4,000 partitions, so partitioning tables by date could potentially run into this limit in the future. In this case, option B, converting sharded tables into a single partitioned table, could be a reasonable solution to avoid exceeding the maximum number of tables in BigQuery.\n\nAs you mentioned, sharded tables require additional metadata and permissions verification, which can impact query performance. Converting sharded tables into a single partitioned table can improve performance and reduce query overhead.\n\nTherefore, based on the information provided, option B seems to be the most appropriate solution for avoiding the limit of 1,000 tables in BigQuery and optimizing query performance."
      },
      {
        "date": "2023-03-23T17:19:00.000Z",
        "voteCount": 2,
        "content": "The question seems pretty badly written. One important thing to remember is that partitioned tables also have a limit of 4000 partitions (https://cloud.google.com/bigquery/docs/partitioned-tables#ingestion_time), so moving everything to one table would just delay the problem. However, option A is not clear on how it will be done. One table per year with daily partitions? Best solution as no limit will be reached. One table per day? Then we have the same 1000 tables problem.\nAll things considered I'll stick to B, simply because the problem will definitely be solved for the next few years, so I'd say it's a reasonable solution."
      },
      {
        "date": "2023-01-26T08:31:00.000Z",
        "voteCount": 2,
        "content": "Answer is B.\nTable sharding is the practice of storing data in multiple tables, using a naming prefix such as [PREFIX]_YYYYMMDD.\nPartitioning is recommended over table sharding, because partitioned tables perform better. With sharded tables, BigQuery must maintain a copy of the schema and metadata for each table. BigQuery might also need to verify permissions for each queried table. This practice also adds to query overhead and affects query performance.\nIn answer A. we still are creating tableS (even though partioned). So we still facing the issue of max 1000 tables. In B. we have only ONE table (partioned)"
      },
      {
        "date": "2023-01-22T19:56:00.000Z",
        "voteCount": 3,
        "content": "Why not A?\nBy converting all daily log tables into date-partitioned tables, you can take advantage of partition pruning to limit the number of tables that need to be scanned during a query. Partition pruning allows BigQuery to skip scanning partitions that are not within the date range specified in the query, thus reducing the number of tables that need to be scanned and can help to avoid reaching the 1,000 table limit.\nA Seems like the correct answer but I can be wrong..."
      },
      {
        "date": "2023-01-11T11:32:00.000Z",
        "voteCount": 1,
        "content": "B. Convert the sharded tables into a single partitioned table\nIt was a sharded Table (format is the HINT here); converting to partition table is the option.\nAlso as per GCP its recommended to use Partition over Sharding"
      },
      {
        "date": "2023-01-08T03:13:00.000Z",
        "voteCount": 1,
        "content": "I chose option A. From all the comments I have seen, there are various things that are misunderstood.\n1. Option A is a single table with multiple shards! Google does recommend to use partition rather than shard as it has a better performance (https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard)\n2. Option B is a single table with single partition! Single partition is a no for large table"
      },
      {
        "date": "2022-12-16T05:21:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables"
      },
      {
        "date": "2022-12-14T14:54:00.000Z",
        "voteCount": 1,
        "content": "Option A - already doing same loading data in separate table daily and reached 1000 table limit. \nOption B - Use wild card to query the data\nOption C &amp; D - make no sense"
      },
      {
        "date": "2022-12-07T02:04:00.000Z",
        "voteCount": 1,
        "content": "its B.\nA - Even if you have 100+ partitioned tables, you still have the limit of less than 1000 tables. So this doesn't work for this problem.\nC It's a no sense. Cache its 24h for every table that has been query in the last 24 and has no changes. Also, cache is not support with wildcard multiple tables.\nD Will not work because it's a recursive issue. You still will have 100+ tables, beam query\nB will work, you materialize in only one table, so will be working perfectly."
      },
      {
        "date": "2022-10-11T09:54:00.000Z",
        "voteCount": 2,
        "content": "Convert MANY sharded tables into a single ONE  (partitioned) table"
      },
      {
        "date": "2022-08-29T09:11:00.000Z",
        "voteCount": 1,
        "content": "selecting for daily/monthly data from one single partition will be very expensive. I think A is the best answer"
      },
      {
        "date": "2022-06-15T10:59:00.000Z",
        "voteCount": 2,
        "content": "C'mon, how much time are you going to take to partition every single table you have? second point and the most important, you have a table for every SINGLE DAY \"LOGS_YYYYMMDD\" partitioning every table will end on scanning all the records of each table when you query them by date ranges using the wildcards, there will be no difference on time-partitioning each table versus consuming them as described."
      },
      {
        "date": "2022-06-09T06:47:00.000Z",
        "voteCount": 1,
        "content": "If you follow option A, you will end up with the same amount of tables, e.g 1500 tables, though they will all be partitioned, which is not helpful.\nOption B takes all the sharded tables and makes one large partitioned table."
      },
      {
        "date": "2022-08-29T09:15:00.000Z",
        "voteCount": 1,
        "content": "Partitions are not tables. The issue is not performance. It is the limit imposed by bq regarding how many tables you can query."
      },
      {
        "date": "2022-05-04T06:09:00.000Z",
        "voteCount": 1,
        "content": "It's B\nhttps://cloud.google.com/bigquery/docs/creating-partitioned-tables#converting_date-sharded_tables_into_ingestion-time_partitioned_tables"
      },
      {
        "date": "2022-01-04T08:14:00.000Z",
        "voteCount": 2,
        "content": "Partitioning &gt; table sharding:\nhttps://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 61,
    "url": "https://www.examtopics.com/discussions/google/view/16747-exam-professional-data-engineer-topic-1-question-61/",
    "body": "Your analytics team wants to build a simple statistical model to determine which customers are most likely to work with your company again, based on a few different metrics. They want to run the model on Apache Spark, using data housed in Google Cloud Storage, and you have recommended using Google Cloud<br>Dataproc to execute this job. Testing has shown that this workload can run in approximately 30 minutes on a 15-node cluster, outputting the results into Google<br>BigQuery. The plan is to run this workload weekly. How should you optimize the cluster for cost?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workload to Google Cloud Dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse pre-emptible virtual machines (VMs) for the cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a higher-memory node so that the job runs faster",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SSDs on the worker nodes so that the job can run faster"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-16T07:32:00.000Z",
        "voteCount": 46,
        "content": "B. (Hadoop/Spark jobs are run on Dataproc, and the pre-emptible machines cost 80% less)"
      },
      {
        "date": "2020-03-16T23:24:00.000Z",
        "voteCount": 17,
        "content": "I think the answer should be B:\n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms"
      },
      {
        "date": "2023-07-17T12:37:00.000Z",
        "voteCount": 2,
        "content": "I believe it might be \"B\", but what if the job is mission critical? \nPre-emptible VMs would be of no use."
      },
      {
        "date": "2024-07-27T06:41:00.000Z",
        "voteCount": 1,
        "content": "Mission critical workloads can't be needed \"weekly\""
      },
      {
        "date": "2023-04-23T19:16:00.000Z",
        "voteCount": 7,
        "content": "I believe Exam Topics ought to provide brief explanation or supporting link to picked correct answers such as this one. Option A may be correct from the view point that Dataflow is a Serverless service that is fast, cost-effective and the fact that Preemptible VMs though can give large price discount may not always be available. It will be great to know the reason(s) behind Exam Topic selected option."
      },
      {
        "date": "2023-01-22T20:02:00.000Z",
        "voteCount": 3,
        "content": "B. Use pre-emptible virtual machines (VMs) for the cluster\n\nUsing pre-emptible VMs allows you to take advantage of lower-cost virtual machine instances that may be terminated by Google Cloud after a short period of time, typically after 24 hours. These instances can be a cost-effective way to handle workloads that can be interrupted, such as batch processing jobs like the one described in the question.\n\nOption A is not ideal, as it would require you to migrate the workload to Google Cloud Dataflow, which may cause additional complexity and would not address the issue of cost optimization.\nOption C is not ideal, as it would require you to use a higher-memory node which would increase the cost.\nOption D is not ideal, as it would require you to use SSDs on the worker nodes which would increase the cost.\n\nUsing pre-emptible VMs is a better option as it allows you to take advantage of lower-cost virtual machine instances and handle workloads that can be interrupted, which can help to optimize the cost of the cluster."
      },
      {
        "date": "2023-01-05T12:02:00.000Z",
        "voteCount": 2,
        "content": "What is happening with this test \"correct answer\" a lot of times it doesn't make any sense. As this one... Clear it's B"
      },
      {
        "date": "2022-12-16T05:26:00.000Z",
        "voteCount": 2,
        "content": "Using preemtible machines are cost effective , and because is suitable for a job  mentioned here as it is fault tolerant ."
      },
      {
        "date": "2022-12-14T15:16:00.000Z",
        "voteCount": 1,
        "content": "User Pre-emptible VM machine and save process cost, and question want simple solution."
      },
      {
        "date": "2022-12-07T02:52:00.000Z",
        "voteCount": 1,
        "content": "A- Data flow it's not cost-effective in comparison with dataproc\nB- Preemptible VM instances are available at much lower price\u2014a 60-91% discount\u2014compared to the price of standar, so this is the answer \nC and D are more expensive."
      },
      {
        "date": "2022-09-07T09:40:00.000Z",
        "voteCount": 1,
        "content": "B is right way to go"
      },
      {
        "date": "2022-05-30T12:26:00.000Z",
        "voteCount": 1,
        "content": "Preemptible workers are the default secondary worker type. They are reclaimed and removed from the cluster if they are required by Google Cloud for other tasks. Although the potential removal of preemptible workers can affect job stability, you may decide to use preemptible instances to lower per-hour compute costs for non-critical data processing or to create very large clusters at a lower total cost \n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms"
      },
      {
        "date": "2022-03-23T09:14:00.000Z",
        "voteCount": 4,
        "content": "B is teh right answer. examtopics update your answers or make your site free again."
      },
      {
        "date": "2022-03-16T00:46:00.000Z",
        "voteCount": 4,
        "content": "B is right answer.\nmy experience is not good with Examtopics, so many wrong answers."
      },
      {
        "date": "2022-02-03T16:31:00.000Z",
        "voteCount": 2,
        "content": "B should be the right answer. \nI am amazed that almost 60% of the marked answers on the site are wrong."
      },
      {
        "date": "2022-01-24T06:44:00.000Z",
        "voteCount": 1,
        "content": "Ans : B, \nhere we are checking on reducing cost, so pre-emptiable machines are best choice"
      },
      {
        "date": "2022-01-04T08:18:00.000Z",
        "voteCount": 4,
        "content": "\"this workload can run in approximately 30 minutes on a 15-node cluster,\"\nso you need performance for only 30 mins -&gt; preemptible VMs\n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/preemptible-vms"
      },
      {
        "date": "2021-12-20T11:31:00.000Z",
        "voteCount": 1,
        "content": "A is not valid, for apache spark jobs dataproc y the best choice.\nC and D are not correct, that might speed up the job or not.\nFor sure if we use pre-emptible machines this will be cheaper and since we don\u2019t have severe time restriction\u2026thats the one. B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 62,
    "url": "https://www.examtopics.com/discussions/google/view/17104-exam-professional-data-engineer-topic-1-question-62/",
    "body": "Your company receives both batch- and stream-based event data. You want to process the data using Google Cloud Dataflow over a predictable time period.<br>However, you realize that in some instances data can arrive late or out of order. How should you design your Cloud Dataflow pipeline to handle data that is late or out of order?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a single global window to capture all the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet sliding windows to capture all the lagged data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse watermarks and timestamps to capture the lagged data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T20:11:00.000Z",
        "voteCount": 44,
        "content": "Answer: C\nDescription: A watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data."
      },
      {
        "date": "2020-03-21T07:16:00.000Z",
        "voteCount": 18,
        "content": "Answer: C"
      },
      {
        "date": "2023-09-25T11:32:00.000Z",
        "voteCount": 1,
        "content": "option C: Use watermarks and timestamps to capture the lagged data."
      },
      {
        "date": "2023-09-25T11:32:00.000Z",
        "voteCount": 1,
        "content": "option C: Use watermarks and timestamps to capture the lagged data."
      },
      {
        "date": "2023-01-22T20:06:00.000Z",
        "voteCount": 9,
        "content": "C: Use watermarks and timestamps to capture the lagged data.\n\nWatermarks are a way to indicate that some data may still be in transit and not yet processed. By setting a watermark, you can define a time period during which Dataflow will continue to accept late or out-of-order data and incorporate it into your processing. This allows you to maintain a predictable time period for processing while still allowing for some flexibility in the arrival of data.\n\nTimestamps, on the other hand, are used to order events correctly, even if they arrive out of order. By assigning timestamps to each event, you can ensure that they are processed in the correct order, even if they don't arrive in that order."
      },
      {
        "date": "2023-01-22T20:06:00.000Z",
        "voteCount": 4,
        "content": "Option A: Set a single global window to capture all the data is not a good idea because it may not allow for late or out-of-order data to be processed.\n\nOption B: Set sliding windows to capture all the lagged data is not suitable for the case where you want to process the data over a predictable time period. Sliding windows are used when you want to process data over a period of time that is continuously moving forward, not a fixed period.\n\nOption D: Ensure every datasource type (stream or batch) has a timestamp, and use the timestamps to define the logic for lagged data is a good practice but not a complete solution, because it only ensures that data is ordered correctly, but it does not account for data that may be late."
      },
      {
        "date": "2023-01-16T16:35:00.000Z",
        "voteCount": 1,
        "content": "Answer is C:\n\nThere is no such thing as a sliding windows using by dataflow."
      },
      {
        "date": "2023-01-21T01:30:00.000Z",
        "voteCount": 1,
        "content": "I highly doubt, DataFlow windowing is divided into three(3) types:\n\n1. Fixed\n2. Sliding \n3. Session"
      },
      {
        "date": "2023-07-22T07:20:00.000Z",
        "voteCount": 1,
        "content": "The naming in Apache Beam is: Fixed, Sliding, Session\nIn Dataflow it's: Tumbling, Hopping, Session.\nI was very confused at first too when I saw \"hopping\" in a question."
      },
      {
        "date": "2023-01-04T18:58:00.000Z",
        "voteCount": 1,
        "content": "Answer is Use watermarks and timestamps to capture the lagged data.\n\nA watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data."
      },
      {
        "date": "2022-12-14T15:36:00.000Z",
        "voteCount": 2,
        "content": "Watermark is use for late date,"
      },
      {
        "date": "2022-11-10T08:12:00.000Z",
        "voteCount": 3,
        "content": "Watermark doesn't solve the out-of-order data problem. It only solves the problem of late data. However, with D, you can use the timestamps to solve both problems (for instance, if you're storing incoming data in a table, you can easily insert any late data to its correct place a time-partionned table using the timestamp of the element)"
      },
      {
        "date": "2022-11-19T21:24:00.000Z",
        "voteCount": 1,
        "content": "with watermarks, when the late data arrives, it goes into its rightful window and gets in order"
      },
      {
        "date": "2022-11-19T21:25:00.000Z",
        "voteCount": 1,
        "content": "C even says watermarks AND timestamps."
      },
      {
        "date": "2022-05-30T12:23:00.000Z",
        "voteCount": 1,
        "content": "Preemptible workers are the default secondary worker type. They are reclaimed and removed from the cluster if they are required by Google Cloud for other tasks. Although the potential removal of preemptible workers can affect job stability, you may decide to use preemptible instances to lower per-hour compute costs for non-critical data processing or to create very large clusters at a lower total cost \n\nhttps://cloud.google.com/dataproc/docs/concepts/compute/secondary-vms"
      },
      {
        "date": "2022-05-30T12:26:00.000Z",
        "voteCount": 8,
        "content": "delete this answer. The answer belongs to another question"
      },
      {
        "date": "2022-02-16T09:12:00.000Z",
        "voteCount": 1,
        "content": "That's why we have watermarks in apache beam."
      },
      {
        "date": "2022-02-12T05:47:00.000Z",
        "voteCount": 1,
        "content": "Answer is  C Use watermarks and timestamps to capture the lagged data.\n\nA watermark is a threshold that indicates when Dataflow expects all of the data in a window to have arrived. If new data arrives with a timestamp that's in the window but older than the watermark, the data is considered late data."
      },
      {
        "date": "2022-01-04T08:22:00.000Z",
        "voteCount": 3,
        "content": "\"Watermark in implementation is a monotonically increasing timestamp. When Beam/Dataflow see a record with an event timestamp that is earlier than the watermark, the record is treated as late data.\""
      },
      {
        "date": "2021-12-20T11:18:00.000Z",
        "voteCount": 4,
        "content": "A is a direct No, if data don\u2019t have timestamp, we\u2019ll only have the procesing time and not the \u201cevent time\u201d.\nB is not either, sliding windows are not for this. Hopping|sliding windowing is useful for taking running averages of data, but not to process late data.\nD looks correct but has one concept missing, the watermark to know if the process time is ok with the event time or not. I\u2019m not 100% sure is incorrect. If, since we have a \u201cpredictable time period\u201d, might be this will do. I mean, if our dashboard is shown after the last input data has arrived (single global window), this should be ok. We\u2019d have a \u201cperfect watermark\u201d. Anyway we\u2019d need triggering ."
      },
      {
        "date": "2021-12-20T11:19:00.000Z",
        "voteCount": 3,
        "content": "C is, I think, the correct answer: Watermark is different from late data. Watermark in implementation is a monotonically increasing timestamp. When Beam/Dataflow see a record with an event timestamp that is earlier than the watermark, the record is treated as late data.\nI\u2019ll try to explain: Late data is inherent to Beam\u2019s model for out-of-order processing. What does it mean for data to be late? The definition and its properties are intertwined with watermarks that track the progress of each computation across the event time domain. The simple intuition behind handling lateness is this: only late input should result in late data anywhere in the pipeline.\nSo, is not easy to decide between C and D. If you ask me I\u2019d say C since for D we ought to make some suppositions."
      },
      {
        "date": "2021-12-20T11:19:00.000Z",
        "voteCount": 2,
        "content": "https://docs.google.com/document/d/12r7frmxNickxB5tbpuEh_n35_IJeVZn1peOrBrhhP6Y/edit#heading=h.7a03n7d5mf6g\nhttps://www.oreilly.com/ideas/the-world-beyond-batch-streaming-102\nhttps://www.oreilly.com/library/view/streaming-systems/9781491983867/\nhttps://docs.google.com/presentation/d/1ln5KndBTiskEOGa1QmYSCq16YWO9Dtmj7ZwzjU7SsW4/edit#slide=id.g19b6635698_3_4"
      },
      {
        "date": "2021-12-17T13:24:00.000Z",
        "voteCount": 4,
        "content": "\"Expert Verified\" but &gt;50% questions have random answer. \"Sliding window\" really? Please, this can be fixed easyly with our most voted answer. Of course, the correct answer is C."
      },
      {
        "date": "2021-11-26T11:04:00.000Z",
        "voteCount": 4,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: C"
      },
      {
        "date": "2021-10-17T10:51:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 63,
    "url": "https://www.examtopics.com/discussions/google/view/16745-exam-professional-data-engineer-topic-1-question-63/",
    "body": "You have some data, which is shown in the graphic below. The two dimensions are X and Y, and the shade of each dot represents what class it is. You want to classify this data accurately using a linear algorithm. To do this you need to add a synthetic feature. What should the value of that feature be?<br><img src=\"/assets/media/exam-media/04341/0005000001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tX2+Y2\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tX2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tY2",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcos(X)"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-16T06:39:00.000Z",
        "voteCount": 41,
        "content": "For fitting a linear classifier when the data is in a circle use A."
      },
      {
        "date": "2020-03-27T20:14:00.000Z",
        "voteCount": 15,
        "content": "Answer: A"
      },
      {
        "date": "2024-09-22T23:28:00.000Z",
        "voteCount": 1,
        "content": "Just a note, they are using X2 and Y2 to mean Xsquared, and Ysquared. This is a circle in the form X2+Y2 = k, so for a given k will split that dataset nicely."
      },
      {
        "date": "2023-07-22T07:33:00.000Z",
        "voteCount": 1,
        "content": "It's not obvious to me it is A.\n\nAs others said, cos(X) does ignore the Y value. But answer A does not seem good either. The differences seem minimal.\n\nIf you do A then you have the following issues. If you take elements in the bottom right or the top left of the circle, they will all have the same value, ZERO. Not only that, they will actually have the same value with the elements in the middle of the circle which are completely black. Moreover, elements on the extreme right and extreme right will have different values (-x_max and +x_max). \n\nHowever, if you use a cos(x) then the elements in the beginning"
      },
      {
        "date": "2023-07-22T08:36:00.000Z",
        "voteCount": 3,
        "content": "Nevermind I did not understand that X2 and Y2 meant X^2 and Y2. Answer is A because that gives the distance from the circle. Circle radius = sqrt(X^2 + Y^2). So even though it's not a perfect answer, it makes sense."
      },
      {
        "date": "2023-01-22T20:08:00.000Z",
        "voteCount": 3,
        "content": "A. X2+Y2\n\nThe synthetic feature that should be added in this case is the squared value of the distance from the origin (0,0). This is equivalent to X2+Y2. By adding this feature, the classifier will be able to make more accurate predictions by taking into account the distance of each data point from the origin.\n\nX2 and Y2 alone will not give enough information to classify the data because they do not take into account the relationship between X and Y.\n\nD. cos(X) is not a suitable option because it does not take into account the Y coordinate."
      },
      {
        "date": "2023-01-19T22:58:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer as graph of circle is x^2 + y^2"
      },
      {
        "date": "2023-01-16T16:39:00.000Z",
        "voteCount": 1,
        "content": "Answer is A:\nThe answer reflects 'x' to the 2nd power + 'y' the 2nd power.  \nI guess they can't use carots in the exam answers!"
      },
      {
        "date": "2023-01-04T14:05:00.000Z",
        "voteCount": 1,
        "content": "A is right\nReference:\nhttps://medium.com/@sachinkun21/using-a-linear-model-to-deal-with-nonlinear-dataset-c6ed0f7f3f51"
      },
      {
        "date": "2022-12-16T05:35:00.000Z",
        "voteCount": 1,
        "content": "https://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture"
      },
      {
        "date": "2022-12-14T16:18:00.000Z",
        "voteCount": 2,
        "content": "linear circle X2+Y2   https://www.stat.cmu.edu/~cshalizi/dm/20/lectures/08/lecture-08.html"
      },
      {
        "date": "2022-06-21T08:48:00.000Z",
        "voteCount": 2,
        "content": "If the shape was a circle, it would be (x^2 + y ^2). But I think that a quadric curve will do a better job of separating the two classes, so it would be (x^2)"
      },
      {
        "date": "2022-05-07T08:50:00.000Z",
        "voteCount": 1,
        "content": "Answer: A.\nX^2+Y^2 is the equation of a circle."
      },
      {
        "date": "2022-04-30T19:35:00.000Z",
        "voteCount": 2,
        "content": "C'est A"
      },
      {
        "date": "2022-02-16T09:12:00.000Z",
        "voteCount": 1,
        "content": "only A is draw a circle"
      },
      {
        "date": "2022-01-15T11:19:00.000Z",
        "voteCount": 1,
        "content": "Equation of circle as represented in the question"
      },
      {
        "date": "2022-01-07T01:10:00.000Z",
        "voteCount": 1,
        "content": "F(x) as A B C will have always a positive values as result, for A will need a third dimenssion Z to represent data, only D:cos(x) can be presented as the shown classification. this is a math question"
      },
      {
        "date": "2022-04-27T14:15:00.000Z",
        "voteCount": 1,
        "content": "A B C will only have positive values\nimaginary numbers (i + j) : am I a joke to you?"
      },
      {
        "date": "2022-01-04T08:27:00.000Z",
        "voteCount": 5,
        "content": "The 2 variables that make a circle in http://playground.tensorflow.org are x1^2 and x2^2.\nSin(x) or cos(x) would just make horizontal stripes.\n\nTo do this you\u2019d use those 2 variables, learning rate 0,3 for example, classification type, no regularization needed and any activation function will work fine."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 64,
    "url": "https://www.examtopics.com/discussions/google/view/17105-exam-professional-data-engineer-topic-1-question-64/",
    "body": "You are integrating one of your internal IT applications and Google BigQuery, so users can query BigQuery from the application's interface. You do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset. You need to securely access BigQuery from your IT application. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate groups for your users and give those groups access to the dataset",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntegrate with a single sign-on (SSO) platform, and pass each user's credentials along with the query request",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a service account and grant dataset access to that account. Use the service account's private key to access the dataset\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dummy user and grant dataset access to that user. Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T07:26:00.000Z",
        "voteCount": 27,
        "content": "Correct: C"
      },
      {
        "date": "2020-03-27T20:16:00.000Z",
        "voteCount": 16,
        "content": "Answer: C\nDescription: Service Account are best option when granting access from tools/appllications"
      },
      {
        "date": "2023-01-22T20:09:00.000Z",
        "voteCount": 4,
        "content": "C. Create a service account and grant dataset access to that account. Use the service account's private key to access the dataset.\n\nCreating a service account and granting dataset access to that account is the most secure way to access BigQuery from an IT application. Service accounts are designed for use in automated systems and do not require user interaction, eliminating the need for individual users to authenticate to BigQuery. Additionally, by using the private key of the service account to access the dataset, you can ensure that the authentication process is secure and that only authorized users have access to the data."
      },
      {
        "date": "2023-01-22T20:10:00.000Z",
        "voteCount": 4,
        "content": "Option A: Create groups for your users and give those groups access to the dataset, is not the best option because it still requires users to authenticate to BigQuery\n\nOption B: Integrate with a single sign-on (SSO) platform, and pass each user's credentials along with the query request is not the best option because it still requires users to authenticate to BigQuery.\n\nOption D: Create a dummy user and grant dataset access to that user. Store the username and password for that user in a file on the files system, and use those credentials to access the BigQuery dataset is not a secure option because it involves storing sensitive information in a file on the file system, which can be easily accessed by unauthorized users."
      },
      {
        "date": "2022-12-14T16:37:00.000Z",
        "voteCount": 1,
        "content": "Service account approach is secure in GCP to communicate with between service or application"
      },
      {
        "date": "2022-11-06T06:38:00.000Z",
        "voteCount": 1,
        "content": "[C]\nThe reason in https://cloud.google.com/bigquery/docs/data-governance#identity\n\"Users of BigQuery might be humans, but they might also be nonhuman applications that communicate using a BigQuery client library or the REST API. These applications should identify themselves using a service account, the special type of Google identity intended to represent a nonhuman user.\""
      },
      {
        "date": "2022-02-16T09:29:00.000Z",
        "voteCount": 2,
        "content": "such kind of questions are always service account oriented. and sa can be used as a user ..not just for machine2machine\n\nusers may or may not enter their credentials app login window. that's not main point by the way"
      },
      {
        "date": "2022-02-02T10:35:00.000Z",
        "voteCount": 2,
        "content": "Correct: C"
      },
      {
        "date": "2021-10-17T11:06:00.000Z",
        "voteCount": 2,
        "content": "Ans: C"
      },
      {
        "date": "2021-03-10T16:19:00.000Z",
        "voteCount": 3,
        "content": "C:\nIt says \"do not want individual users to authenticate to BigQuery and you do not want to give them access to the dataset\", then C is the best choice."
      },
      {
        "date": "2020-11-09T02:41:00.000Z",
        "voteCount": 4,
        "content": "Granting access to the app through a service account would mean all of the users that access the app have access to the BQ. Question was to filter it out, so I believe each user would have to be added to a group that does or doesn't have access to the dataset."
      },
      {
        "date": "2021-10-18T17:15:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. \nWhen access data through application, Google recommendation is using service account."
      },
      {
        "date": "2020-11-17T00:22:00.000Z",
        "voteCount": 4,
        "content": "Yes A seems to be right"
      },
      {
        "date": "2020-11-17T00:26:00.000Z",
        "voteCount": 3,
        "content": "It says individually don't want to authorise service account could be right too"
      },
      {
        "date": "2020-09-29T20:46:00.000Z",
        "voteCount": 3,
        "content": "Ans is C, Service account is best for secure data"
      },
      {
        "date": "2020-08-19T07:08:00.000Z",
        "voteCount": 3,
        "content": "C is correct"
      },
      {
        "date": "2020-07-05T06:00:00.000Z",
        "voteCount": 4,
        "content": "Correct C."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 65,
    "url": "https://www.examtopics.com/discussions/google/view/16476-exam-professional-data-engineer-topic-1-question-65/",
    "body": "You are building a data pipeline on Google Cloud. You need to prepare data using a casual method for a machine-learning process. You want to support a logistic regression model. You also need to monitor and adjust for null values, which must remain real-valued and cannot be removed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataprep to find null values in sample source data. Convert all nulls to 'none' using a Cloud Dataproc job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow to find null values in sample source data. Convert all nulls to 'none' using a Cloud Dataprep job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow to find null values in sample source data. Convert all nulls to 0 using a custom script."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-16T07:31:00.000Z",
        "voteCount": 39,
        "content": "real-valued can not be null N/A or empty, have to be \u201c0\u201d, so it has to be B."
      },
      {
        "date": "2020-03-21T07:51:00.000Z",
        "voteCount": 16,
        "content": "Should be B"
      },
      {
        "date": "2024-04-04T08:35:00.000Z",
        "voteCount": 1,
        "content": "B. Dataprep has the feature to convert it into 0."
      },
      {
        "date": "2024-02-29T22:31:00.000Z",
        "voteCount": 1,
        "content": "0 is still a value, which can add bias in the model and the model will take that into account while making predictions so 'none'"
      },
      {
        "date": "2023-12-10T06:32:00.000Z",
        "voteCount": 1,
        "content": "Why not D? keyword is Monitor, B would replace all empty fields and also cause unintended bias."
      },
      {
        "date": "2023-12-10T06:38:00.000Z",
        "voteCount": 1,
        "content": "However, Sergiomujica is right. If we need to prepare data using a casual method then its B \"Dataprep\"."
      },
      {
        "date": "2023-09-04T19:30:00.000Z",
        "voteCount": 1,
        "content": "The questions says \"You need to prepare data using a casual method \", thats dataprep and values should be 0 so the right answer is B"
      },
      {
        "date": "2023-07-22T08:39:00.000Z",
        "voteCount": 1,
        "content": "No brainer. We need  a real value and Dataprep is made for this. Dataflow is mainly for pre-processing before BigQuery ingests the data."
      },
      {
        "date": "2023-07-17T13:09:00.000Z",
        "voteCount": 1,
        "content": "Dataprep is made for this kind of stuff, no reason to use a streaming service such as Dataflow."
      },
      {
        "date": "2023-04-24T23:19:00.000Z",
        "voteCount": 1,
        "content": "gpt:Cloud Dataprep is a data preparation service that can be used to transform, clean and shape data in a visually interactive way. It provides an easy-to-use interface to find and replace null values.\n\nCloud Dataflow is a fully-managed service for executing data processing pipelines, which allows for parallel execution of data processing tasks. However, it requires more expertise to set up and operate than Cloud Dataprep, and is usually used for more complex data processing needs.\n\nTherefore, option B is the most suitable approach for the given requirements."
      },
      {
        "date": "2023-01-22T20:24:00.000Z",
        "voteCount": 2,
        "content": "Seems to me like Answers are both B and D.\nB is faster to implement while D takes time.\nDoesnt mean that it's wrong though. I m not sure why everyone has picked just B. Why not D? D works and does the same job. And also having custom script provides more flexibility and control over the data processing tasks and it allows you to handle missing values in a more flexible and efficient way."
      },
      {
        "date": "2023-05-16T01:43:00.000Z",
        "voteCount": 1,
        "content": "The \"casual way\" or easy way to convert to to 0 is using Dataprep job rather than using the custom script."
      },
      {
        "date": "2023-05-24T07:36:00.000Z",
        "voteCount": 1,
        "content": "A simple rule. Whenever any service is available by GCP for a task, always recommend to use GCP service over any other."
      },
      {
        "date": "2023-01-19T23:00:00.000Z",
        "voteCount": 1,
        "content": "B is the correct answer."
      },
      {
        "date": "2023-01-04T14:06:00.000Z",
        "voteCount": 3,
        "content": "Answer is Use Cloud Dataprep to find null values in sample source data. Convert all nulls to 0 using a Cloud Dataprep job.\n\nKey phrases are \"casual method\", \"need to replace null with real values\", \"logistic regression\". Logistic regression works on numbers. Null need to be replaced with a number. And Cloud dataprep is best casual tool out of given options."
      },
      {
        "date": "2022-12-14T16:41:00.000Z",
        "voteCount": 1,
        "content": "real value 0"
      },
      {
        "date": "2022-01-24T06:52:00.000Z",
        "voteCount": 2,
        "content": "It is B"
      },
      {
        "date": "2022-01-05T08:06:00.000Z",
        "voteCount": 1,
        "content": "Dataprep + real value (0)"
      },
      {
        "date": "2021-12-20T11:24:00.000Z",
        "voteCount": 3,
        "content": "Dataprep is the tool. A or B.\nSince they need to have a real-valued cannot be null N/A or empty, have to be \u201c0\u201d, so it has to be B."
      },
      {
        "date": "2021-10-17T11:12:00.000Z",
        "voteCount": 2,
        "content": "Ans: B\nDataprep suites this, so none of dataflow options qualify as answer. Then 0 can be real-value than a \"~none'."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 66,
    "url": "https://www.examtopics.com/discussions/google/view/17109-exam-professional-data-engineer-topic-1-question-66/",
    "body": "You set up a streaming data insert into a Redis cluster via a Kafka cluster. Both clusters are running on Compute Engine instances. You need to encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-10-29T14:19:00.000Z",
        "voteCount": 57,
        "content": "Dear Admin, almost every answer is incorrect . Please check the comments and update your website."
      },
      {
        "date": "2020-03-21T08:16:00.000Z",
        "voteCount": 22,
        "content": "correct: B"
      },
      {
        "date": "2020-03-21T08:18:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/security/encryption-at-rest/"
      },
      {
        "date": "2020-07-13T08:18:00.000Z",
        "voteCount": 4,
        "content": "Based on the info at the link you referred, it seems C is the right answer"
      },
      {
        "date": "2021-12-15T13:49:00.000Z",
        "voteCount": 2,
        "content": "If you create it locally, you can't rotate keys. Answer should be B"
      },
      {
        "date": "2024-04-16T01:18:00.000Z",
        "voteCount": 1,
        "content": "But KMS doesnt create keys. It only stores them right?"
      },
      {
        "date": "2023-12-14T08:00:00.000Z",
        "voteCount": 1,
        "content": "Google Cloud Key Management Service (KMS) provides a centralized cloud service for managing cryptographic keys. By creating encryption keys in Cloud KMS, you can easily manage the lifecycle of these keys, including creation, rotation, and destruction.\n WYY NOT Create Keys Locally and Upload to Cloud KMS? \n While it\u2019s possible to create keys locally and then upload them to Cloud KMS, it\u2019s generally simpler and more secure to create the keys directly in Cloud KMS. This reduces the risk associated with transferring keys and leverages the security and compliance features of Cloud KMS."
      },
      {
        "date": "2023-11-01T12:07:00.000Z",
        "voteCount": 2,
        "content": "Help!\nI chose \"C\" because of the statement, \"encrypt data at rest with encryption keys that you can create, rotate, and destroy as needed\" and read that as needing to generate the keys locally. Can you please explain where I went wrong?"
      },
      {
        "date": "2023-07-23T10:11:00.000Z",
        "voteCount": 1,
        "content": "the answer is C Read the full statement.\n\n\" You need to encrypt data at rest with encryption keys that you can create \""
      },
      {
        "date": "2023-07-17T13:22:00.000Z",
        "voteCount": 1,
        "content": "B!\nC is useless overhead and you cannot rotate that easily!"
      },
      {
        "date": "2023-05-15T14:32:00.000Z",
        "voteCount": 1,
        "content": "Well for what I remember from cloud arch and what I found in https://cloud.google.com/compute/docs/disks/customer-managed-encryption\n\nThere is two options or the customer manage entirely or he will use the service to generate the keys so based on that is the B"
      },
      {
        "date": "2023-01-22T20:27:00.000Z",
        "voteCount": 2,
        "content": "B. Create encryption keys in Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances.\n\nCloud Key Management Service (KMS) is a fully managed service that allows you to create, rotate, and destroy encryption keys as needed. By creating encryption keys in Cloud KMS, you can use them to encrypt your data at rest in the Compute Engine cluster instances, which is running your Redis and Kafka clusters. This ensures that your data is protected even when it is stored on disk."
      },
      {
        "date": "2023-01-22T20:27:00.000Z",
        "voteCount": 3,
        "content": "Option A: Create a dedicated service account, and use encryption at rest to reference your data stored in your Compute Engine cluster instances as part of your API service calls is not the best option as it does not provide encryption at rest.\n\nOption C: Create encryption keys locally. Upload your encryption keys to Cloud Key Management Service. Use those keys to encrypt your data in all of the Compute Engine cluster instances, is not the best option as it does not provide a way to manage the encryption keys centrally.\n\nOption D: Create encryption keys in Cloud Key Management Service. Reference those keys in your API service calls when accessing the data in your Compute Engine cluster instances, is not the best option as it does not provide encryption at rest, it only secure the data in transit."
      },
      {
        "date": "2022-12-14T16:58:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer generate key using KMS, why locally again it is overhead to  upload and use everywhere."
      },
      {
        "date": "2022-11-25T01:29:00.000Z",
        "voteCount": 1,
        "content": "B\nIf you use Google Cloud, Cloud Key Management Service lets you create your own encryption keys that you can use to add envelope encryption to your data. Using Cloud KMS, you can create, rotate, track, and delete keys. \nhttps://cloud.google.com/docs/security/encryption/default-encryption#:~:text=If%20you%20use%20Google%20Cloud%2C%20Cloud%20Key%20Management%20Service%20lets%20you%20create%20your%20own%20encryption%20keys%20that%20you%20can%20use%20to%20add%20envelope%20encryption%20to%20your%20data.%20Using%20Cloud%20KMS%2C%20you%20can%20create%2C%20rotate%2C%20track%2C%20and%20delete%20keys."
      },
      {
        "date": "2022-01-05T08:08:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/security/encryption-at-rest/"
      },
      {
        "date": "2021-12-21T09:44:00.000Z",
        "voteCount": 8,
        "content": "A makes no sense, you need to use your own keys.\nYou don\u2019t create keys locally and upload them, you should import it to make it work..using the kms public key\u2026not just \u201cuploading\u201d it. C is also out.\nIT\u2019s between B and D\nCloud KMS is a cloud-hosted key management service that lets you manage cryptographic keys for your cloud services the same way you do on-premises, You can generate, use, rotate, and destroy cryptographic keys from there.\nSince you want to encrypt data at rest, is B, you don\u2019t use them for any API calls.\nhttps://cloud.google.com/compute/docs/disks/customer-managed-encryption"
      },
      {
        "date": "2021-10-25T11:38:00.000Z",
        "voteCount": 2,
        "content": "I believe you cannot upload custom keys to KMS for Compute Engine. Only via API Calls. See: https://cloud.google.com/security/encryption/customer-supplied-encryption-keys\nWith that said, option B"
      },
      {
        "date": "2021-09-28T05:26:00.000Z",
        "voteCount": 2,
        "content": "Answer is B : both clusters are on GCP, so we can use KMS to manage the keys."
      },
      {
        "date": "2021-08-18T11:26:00.000Z",
        "voteCount": 3,
        "content": "Well there are two things that a user a user can do via KMS-\n1. You may be using existing cryptographic keys that were created on your premises or in an external key management system. If you migrate an application to Google Cloud or if you add cryptographic support to an existing Google Cloud application, you can import the relevant keys into Cloud KMS. In the given situation, user is not having any key. So let's check 2nd option.\n\n2.Cloud Key Management Service allows you to create, import, and manage cryptographic keys-- a general KMS definition.\n\nNow if you don't have a key, why would you generate it locally then import it in KMS if you have a option to create a key yourself in KMS.\n\nhttps://cloud.google.com/kms/docs\nAns - B"
      },
      {
        "date": "2021-07-08T12:24:00.000Z",
        "voteCount": 2,
        "content": "it's B.\nWith KMS (Key Management Service), customer can create and destroy the keys as shown here: https://cloud.google.com/kms/docs/quickstart\nCustomer can also rotate key: https://cloud.google.com/kms/docs/rotating-keys"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 67,
    "url": "https://www.examtopics.com/discussions/google/view/17110-exam-professional-data-engineer-topic-1-question-67/",
    "body": "You are developing an application that uses a recommendation engine on Google Cloud. Your solution should display new videos to customers based on past views. Your solution needs to generate labels for the entities in videos that the customer has viewed. Your design must be able to provide very fast filtering suggestions based on data from other customer preferences on several TB of data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild and train a complex classification model with Spark MLlib to generate labels and filter the results. Deploy the models using Cloud Dataproc. Call the model from your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild and train a classification model with Spark MLlib to generate labels. Build and train a second classification model with Spark MLlib to filter results to match customer preferences. Deploy the models using Cloud Dataproc. Call the models from your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud Bigtable, and filter the predicted labels to match the user's viewing history to generate preferences.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud SQL, and join and filter the predicted labels to match the user's viewing history to generate preferences."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T08:25:00.000Z",
        "voteCount": 36,
        "content": "Answer: C\nA &amp; B - Need to build your own model, so discarded as options C D can do the job here using Cloud Video Intelligence API. BigTable is better option. So C is correct"
      },
      {
        "date": "2023-02-28T04:46:00.000Z",
        "voteCount": 1,
        "content": "Is there any notice that has to reject own model in question..?"
      },
      {
        "date": "2023-02-28T04:47:00.000Z",
        "voteCount": 1,
        "content": "I don't understand why Vision API should be a answer for labeling? there is no information about input data. isn't it?"
      },
      {
        "date": "2020-03-27T20:29:00.000Z",
        "voteCount": 14,
        "content": "Answer: C\nDescription: Why to build own model, Video API with Bigtable is best solution"
      },
      {
        "date": "2023-07-22T08:52:00.000Z",
        "voteCount": 2,
        "content": "I don't even know if MLLib has out-of-the-box Computer Vision models. Developing this in Dataproc would be a nightmare.\n\nUsing the computer vision API on the other hand makes perfect sense.\n\nThe fact that the filtering must happen very fast and that this is a customer facing application points to BigTable so that there is very little latency and high availability. BigTable is eventually consistent but that doesn't really matter for this application.\n\nCloudSQL will ensure strong consistency which we don't really need but it is slower and supports max 64 TB. The description mentions multiple TBs. Not really sure what several means here, but Cloud SQL doesn't have a high cap."
      },
      {
        "date": "2023-07-05T01:15:00.000Z",
        "voteCount": 1,
        "content": "We need a model that extracts labels from videos, so Vision API could be used.\nThen we need a DB very fast and that can handle several TB of data, so BigTable is the best choice.\nAnswer is C."
      },
      {
        "date": "2023-01-21T18:35:00.000Z",
        "voteCount": 2,
        "content": "Option C is the correct choice because it utilizes the Cloud Video Intelligence API to generate labels for the entities in the videos, which would save time and resources compared to building and training a model from scratch. Additionally, by storing the data in Cloud Bigtable, it allows for fast and efficient filtering of the predicted labels based on the user's viewing history and preferences. This is a more efficient and cost-effective approach than storing the data in Cloud SQL and performing joins and filters."
      },
      {
        "date": "2023-01-04T14:08:00.000Z",
        "voteCount": 1,
        "content": "Answer is C\n Build an application that calls the Cloud Video Intelligence API to generate labels. Store data in Cloud Bigtable, and filter the predicted labels to match the user's viewing history to generate preferences.\n\n1. Rather than building a new model - it is better to use Google provide APIs, here - Google Video Intelligence. So option A and B rules out\n2. Between SQL and Bigtable - Bigtable is the better option as Bigtable support row-key filtering. Joining the filters is not required.\n\nReference:\nhttps://cloud.google.com/video-intelligence/docs/feature-label-detection"
      },
      {
        "date": "2021-12-21T09:52:00.000Z",
        "voteCount": 2,
        "content": "C.\nThe cloud video intelillence api does the label generation without the need of building any model, A and B are excluded. Now, the bbdd most suitable for this is bigtable and not SQL (this big joins would be anything but fast).\nhttps://cloud.google.com/video-intelligence/docs/feature-label-detection"
      },
      {
        "date": "2021-06-28T16:52:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2021-03-23T14:30:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nReference https://cloud.google.com/video-intelligence/docs/feature-label-detection"
      },
      {
        "date": "2021-03-11T04:23:00.000Z",
        "voteCount": 7,
        "content": "answer C:\nIf we presume that use label of video as a rowkey, Bigtable will be the best option. because it can store several TB, but Cloud SQL is limited to 30TB."
      },
      {
        "date": "2020-12-16T19:58:00.000Z",
        "voteCount": 3,
        "content": "Answer: C"
      },
      {
        "date": "2020-11-14T02:42:00.000Z",
        "voteCount": 7,
        "content": "Option C is the correct answer.\n1. Rather than building a new model - it is better to use Google provide APIs, here - Google Video Intelligence.  \nSo option A and B rules out\n2) Between SQL and Bigtable - Bigtable is the better option as Bigtable support row-key filtering.  Joining the filters is not required."
      },
      {
        "date": "2020-09-23T07:59:00.000Z",
        "voteCount": 2,
        "content": "Answer is D : BigTable doesnt support JOIN and not built for transactions - https://cloud.google.com/bigtable/docs/overview"
      },
      {
        "date": "2020-10-27T09:09:00.000Z",
        "voteCount": 4,
        "content": "There are no joins but filtering based on condition."
      },
      {
        "date": "2021-02-13T19:50:00.000Z",
        "voteCount": 2,
        "content": "but the requirement involves join as well, it is stated in the problem."
      },
      {
        "date": "2021-07-08T14:20:00.000Z",
        "voteCount": 1,
        "content": "Where? Though it's mention - \" very fast filtering suggestions\" - which means something like dictionary in python OR Key: Value (which is Bigtable)"
      },
      {
        "date": "2022-01-15T12:07:00.000Z",
        "voteCount": 1,
        "content": "I think \"based on other customer preferences\" from the questions requires a join before a filter is applied for collaborative filtering"
      },
      {
        "date": "2022-04-02T00:30:00.000Z",
        "voteCount": 1,
        "content": "Recommendation based on other customer\u201ds views cannot be achieved through simple joins. A class pf machine learning algorithms called collaborative filtering is required for that. You need big table to run these algorithms."
      },
      {
        "date": "2020-08-19T16:32:00.000Z",
        "voteCount": 2,
        "content": "Correct C"
      },
      {
        "date": "2020-07-06T10:14:00.000Z",
        "voteCount": 2,
        "content": "I doubt if C can be an answer. Will Bigtable allow filtering on labels?"
      },
      {
        "date": "2020-07-13T08:21:00.000Z",
        "voteCount": 3,
        "content": "Yes, if its part of the rowkey"
      },
      {
        "date": "2020-07-05T06:27:00.000Z",
        "voteCount": 4,
        "content": "Answer is C."
      },
      {
        "date": "2020-04-11T00:41:00.000Z",
        "voteCount": 7,
        "content": "C.\nThe recommendation requires filtering based on several TB of data, therefore BigTable is the recommended option vs Cloud SQL which is limited to 10TB."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 68,
    "url": "https://www.examtopics.com/discussions/google/view/16478-exam-professional-data-engineer-topic-1-question-68/",
    "body": "You are selecting services to write and transform JSON messages from Cloud Pub/Sub to BigQuery for a data pipeline on Google Cloud. You want to minimize service costs. You also want to monitor and accommodate input data volume that will vary in size with minimal manual intervention. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataproc to run your transformations. Monitor CPU utilization for the cluster. Resize the number of worker nodes in your cluster via the command line.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataproc to run your transformations. Use the diagnose command to generate an operational output archive. Locate the bottleneck and adjust cluster resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow to run your transformations. Monitor the job system lag with Stackdriver. Use the default autoscaling setting for worker instances.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow to run your transformations. Monitor the total execution time for a sampling of jobs. Configure the job to use non-default Compute Engine machine types when needed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-13T06:01:00.000Z",
        "voteCount": 36,
        "content": "Answer should be C"
      },
      {
        "date": "2020-03-21T08:30:00.000Z",
        "voteCount": 26,
        "content": "Answer: C - best suitable for the purpose with autoscaling and google recommended transform engine between pubsub n bq"
      },
      {
        "date": "2024-05-09T03:51:00.000Z",
        "voteCount": 2,
        "content": "using Cloud Dataflow for transformations with monitoring via Stackdriver and leveraging its default autoscaling settings, is the best choice. Cloud Dataflow is purpose-built for this type of workload, providing seamless scalability and efficient processing capabilities for streaming data. Its autoscaling feature minimizes manual intervention and helps manage costs by dynamically adjusting resources based on the actual processing needs, which is crucial for handling fluctuating data volumes efficiently and cost-effectively."
      },
      {
        "date": "2023-09-30T09:39:00.000Z",
        "voteCount": 1,
        "content": "Option C suggests using Cloud Dataflow to run the transformations and monitoring the job system lag with Stackdriver while using the default autoscaling setting for worker instances.\n\nWhile using Cloud Dataflow is a suitable choice for processing data from Cloud Pub/Sub to BigQuery, and monitoring with Stackdriver provides valuable insights, the specific emphasis on configuring non-default Compute Engine machine types (as mentioned in option D) gives you more control over cost optimization and performance tuning.\n\nBy configuring non-default machine types, you can precisely tailor the computational resources to match the specific requirements of your workload. This fine-grained control allows you to optimize costs further by avoiding over-provisioning of resources, especially if your workload is memory-intensive, CPU-bound, or requires specific configurations that are not met by the default settings."
      },
      {
        "date": "2023-09-30T09:39:00.000Z",
        "voteCount": 1,
        "content": "Additionally, having the flexibility to adjust machine types based on workload characteristics ensures that you can achieve the desired performance levels without overspending on unnecessary resources. This level of customization is not provided by simply relying on the default autoscaling settings, making option D a more comprehensive and cost-effective solution for managing varying data volumes."
      },
      {
        "date": "2023-07-22T09:01:00.000Z",
        "voteCount": 1,
        "content": "At first I answered C. However, Dataproc is indeed cheaper than Dataflow. And both of them can scale automatically horizontically.\n\nDataflow horizontal scaling applies to both primary and secondary nodes. Scaling secondary nodes scales up CPU/compute and scaling primary nodes scales up both memory and CPU/compute.\n\nI don't quite understand the second part of answer B where it says I should allocate resources accordingly. I guess I could do that, but auto-scaling should be enough."
      },
      {
        "date": "2023-02-23T22:07:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C"
      },
      {
        "date": "2023-01-23T12:10:00.000Z",
        "voteCount": 3,
        "content": "C. Use Cloud Dataflow to run your transformations. Monitor the job system lag with Stackdriver. Use the default autoscaling setting for worker instances.\n\nCloud Dataflow is a managed service that allows you to write and execute data transformations in a highly scalable and fault-tolerant way. By default, it will automatically scale the number of worker instances based on the input data volume and job performance, which can help minimize costs. Monitoring the job system lag with Stackdriver can help you identify any issues that may be impacting performance and take action as needed. Additionally, using the default autoscaling setting for worker instances can help you minimize manual intervention and ensure that resources are used efficiently."
      },
      {
        "date": "2022-12-07T15:59:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-07T03:21:00.000Z",
        "voteCount": 11,
        "content": "@admin why all the answers are wrong. I paid 30 euros for this web and its garbage.\nDataproc has no sense in this scenario, because you want to have minimal intervention/operation.  D is not a good practice, the answer is C."
      },
      {
        "date": "2022-12-07T16:00:00.000Z",
        "voteCount": 9,
        "content": "you need to look at community vote distribution and comments, and not the suggested answer."
      },
      {
        "date": "2022-01-05T08:13:00.000Z",
        "voteCount": 4,
        "content": "C only as referred by  MaxNRG"
      },
      {
        "date": "2021-12-21T09:52:00.000Z",
        "voteCount": 9,
        "content": "C.\nDataproc does not seem to be a good solution in this case as it always require a manual intervention to adjust resources.\nAutoscaling with dataflow will automatically handle changing data volumes with no manual intervention, and monitoring through Stackdriver can be used to spot bottleneck. Total execution time is not good there as it does not provide a precise view on potential bottleneck."
      },
      {
        "date": "2021-11-29T03:59:00.000Z",
        "voteCount": 3,
        "content": "Dataflow, Stackdriver and autoscaling"
      },
      {
        "date": "2021-11-12T00:53:00.000Z",
        "voteCount": 4,
        "content": "Admin, please take a look on the comments. Almost all answers are wrong"
      },
      {
        "date": "2021-09-14T23:30:00.000Z",
        "voteCount": 4,
        "content": "Answer should be C as  dataflow is unpredictable size ( input that will vary in size), dataproc is with known size"
      },
      {
        "date": "2022-02-16T10:58:00.000Z",
        "voteCount": 1,
        "content": "dataflow over dataproc is always the preferred way in gcp. use dataproc only there is specific client requirements such as existing hadoop workloads, etc.."
      },
      {
        "date": "2021-08-08T09:53:00.000Z",
        "voteCount": 3,
        "content": "Option C is the answer"
      },
      {
        "date": "2021-06-28T16:56:00.000Z",
        "voteCount": 1,
        "content": "Vote for C"
      },
      {
        "date": "2020-12-30T02:18:00.000Z",
        "voteCount": 2,
        "content": "B , it is correct , as it says minimum service cost, dataflow is more expansive than dataproc."
      },
      {
        "date": "2021-03-11T04:40:00.000Z",
        "voteCount": 1,
        "content": "but it said \"with minimal manual intervention\" and for Dataproc you need to manage cluster manually, then C is the best option."
      },
      {
        "date": "2021-04-10T07:19:00.000Z",
        "voteCount": 1,
        "content": "You have to transform the JSON messages. Hence, you need to use dataflow."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 69,
    "url": "https://www.examtopics.com/discussions/google/view/16072-exam-professional-data-engineer-topic-1-question-69/",
    "body": "Your infrastructure includes a set of YouTube channels. You have been tasked with creating a process for sending the YouTube channel data to Google Cloud for analysis. You want to design a solution that allows your world-wide marketing teams to perform ANSI SQL and other types of analysis on up-to-date YouTube channels log data. How should you set up the log data transfer into Google Cloud?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Storage Transfer Service to transfer the offsite backup files to a Cloud Storage Regional bucket as a final destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Regional storage bucket as a final destination."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-07-26T02:19:00.000Z",
        "voteCount": 74,
        "content": "Correct Answer: A\n\nDestination is GCS and having multi-regional so A is the best option available.\n\nEven since BigQuery Data Transfer Service initially supports Google application sources like Google Ads, Campaign Manager, Google Ad Manager and YouTube but it does not support destination anything other than bq data set"
      },
      {
        "date": "2021-01-01T17:59:00.000Z",
        "voteCount": 7,
        "content": "What about ANSI SQL?"
      },
      {
        "date": "2021-05-27T17:33:00.000Z",
        "voteCount": 8,
        "content": "I guess they are assuming that you will just query the data in Cloud Storage from BQ. The question specifically is, \"How should you set up the log data transfer into Google Cloud?\", not \"How should you set up the querying.\" ANSI SQL is a distraction!"
      },
      {
        "date": "2022-06-01T07:49:00.000Z",
        "voteCount": 2,
        "content": "use external table for it"
      },
      {
        "date": "2021-09-04T01:41:00.000Z",
        "voteCount": 9,
        "content": "Currently, you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery.\n\nhttps://cloud.google.com/bigquery-transfer/docs/introduction"
      },
      {
        "date": "2023-06-05T07:10:00.000Z",
        "voteCount": 4,
        "content": "You can use BQ Data transfer Service for Youtube channels mow"
      },
      {
        "date": "2023-05-24T07:55:00.000Z",
        "voteCount": 3,
        "content": "but I think now you can use BigQuery Data Transfer Service for youtube channels and many other"
      },
      {
        "date": "2021-08-24T22:24:00.000Z",
        "voteCount": 15,
        "content": "Kindly re-read the question,the question says Google Cloud not Cloud storage...once you master that you will understand the whole question and be able to pick the right answer which is C"
      },
      {
        "date": "2021-09-18T03:31:00.000Z",
        "voteCount": 4,
        "content": "logs like stuff goes better on buckets"
      },
      {
        "date": "2021-08-26T13:20:00.000Z",
        "voteCount": 57,
        "content": "all the option clearly says \"storage bucket\", once you master that, you'll realize the correct option is A"
      },
      {
        "date": "2023-04-18T16:50:00.000Z",
        "voteCount": 2,
        "content": "Gottem!"
      },
      {
        "date": "2022-09-08T10:16:00.000Z",
        "voteCount": 3,
        "content": "good one :)"
      },
      {
        "date": "2020-04-19T19:01:00.000Z",
        "voteCount": 20,
        "content": "None of the answers make any sense. \nBigQuery Transfer Service is for moving data from various sources (S3, Youtube etc) into BigQuery, not Google Cloud Storage.\nFurther, how are we supposed to use SQL to query data if it is stored in Cloud Storage?"
      },
      {
        "date": "2020-06-30T07:29:00.000Z",
        "voteCount": 1,
        "content": "Agreed! - All Options look wrong"
      },
      {
        "date": "2020-06-30T07:33:00.000Z",
        "voteCount": 5,
        "content": "Option [A] is the least worse option... for world wide teams to perform ANSI SQL Queries, it would be easier to create a ext. table or load from Multi AZ bucket... BQ Data Transfer service is used to push data in BQ, hence ruling out Option C &amp; D"
      },
      {
        "date": "2021-09-17T01:40:00.000Z",
        "voteCount": 3,
        "content": "The best option would be to use \"BigQuery Transfer Service\" to upload data to BQ. But BQ is not present as a destination, so the only working option is Multi Regional GCS"
      },
      {
        "date": "2020-10-03T02:25:00.000Z",
        "voteCount": 9,
        "content": "Kindly re-read the question,the question says Google Cloud not Cloud storage...once you master that  you will understand the whole question and be able to pick the right answer which is C"
      },
      {
        "date": "2020-10-03T02:26:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery-transfer/docs/youtube-channel-transfer\nthis link will help to cement the answer."
      },
      {
        "date": "2024-09-23T02:21:00.000Z",
        "voteCount": 1,
        "content": "This is A, because it's \"offsite backup files\". You can transfer direct from Youtube to BigQuery: https://cloud.google.com/bigquery/docs/youtube-channel-transfer, but this isn't that. It's direct from some backup files to Cloud Storage, all answers mandate that fact, and all that remains is \"should this be Storage Transfer, or BigQuery\" - clearly this is storage transfer, and all the youtube/ANSI sql stuff is just distraction."
      },
      {
        "date": "2024-08-01T11:52:00.000Z",
        "voteCount": 1,
        "content": "E. Use BigQuery Data Transfer Service to transfer the offsite backup files to BigQuery as a final destination."
      },
      {
        "date": "2024-05-06T21:36:00.000Z",
        "voteCount": 1,
        "content": "Using BQ data transfer we can only load data into the Bigquery storage, which means a table inside the bq dataset. \n\nWhat is Storage Transfer Service? \nStorage Transfer Service automates the transfer of data to, from, and between object and file storage systems, including Google Cloud Storage, Amazon S3, Azure Storage, on-premises data, and more. It can be used to transfer large amounts of data quickly and reliably, without the need to write any code.\n\nAs per the above lines from the Google's documentation on Storage transfer service, we can go with option A. \n\nFor additional info, have a look at the below link. \nhttps://cloud.google.com/storage-transfer/docs/overview?_gl=1*jaku2h*_ga*MjA5Mzc4OTM0LjE2ODQ3MzA5NzQ.*_ga_WH2QY8WWF5*MTcxNTA1OTM0OC4xODMuMS4xNzE1MDU5NTY3LjAuMC4w&amp;_ga=2.6452401.-209378934.1684730974&amp;_gac=1.162721358.1713078448.CjwKCAjw_e2wBhAEEiwAyFFFo4Da6-2MNQqNJuzAGSyJmCXdaPpXXiqaI0lkZYHlcln0IBbtWSJjLBoCp_4QAvD_BwE"
      },
      {
        "date": "2024-04-16T01:41:00.000Z",
        "voteCount": 2,
        "content": "Correct answer: A\nSolution should cater a worldwide solution which makes B and D invalid.\nYou don't use BigQuery Data transfer to move data to a bucket. So C is also invalid."
      },
      {
        "date": "2024-03-14T05:15:00.000Z",
        "voteCount": 2,
        "content": "BigQuery Data Transfer Service can only transfer to BigQuery not in Cloud Storage. So Ans A is correct."
      },
      {
        "date": "2024-03-03T03:58:00.000Z",
        "voteCount": 2,
        "content": "No sense to use BQ data transfer serice to store the data to a storage bucket ...  It is obviously A"
      },
      {
        "date": "2023-11-19T23:04:00.000Z",
        "voteCount": 2,
        "content": "To transfer YouTube channel data to Google Cloud for analysis, you can use the BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination 1. This service allows you to automatically schedule and manage recurring load jobs for YouTube Channel reports 1. The BigQuery Data Transfer Service for YouTube Channel reports supports the following reporting options: Channel Reports (automatically loaded into BigQuery) 1. When you transfer data from a YouTube Channel into BigQuery, the data is loaded into BigQuery tables that are partitioned by date 1."
      },
      {
        "date": "2023-10-15T17:35:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is C. https://cloud.google.com/bigquery/docs/dts-introduction"
      },
      {
        "date": "2023-09-21T17:18:00.000Z",
        "voteCount": 2,
        "content": "not c\nThe BigQuery Data Transfer Service automates data movement into [[[[ BigQuery ]]]] on a scheduled, managed basis. Your analytics team can lay the foundation for a BigQuery data warehouse without writing a single line of code.\n...\nAfter you configure a data transfer, the BigQuery Data Transfer Service automatically loads data into [[[[[ BigQuery ]]]]] on a regular basis."
      },
      {
        "date": "2023-09-12T01:13:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A.\nAt first, it's between A and C because of the term multi-regional. However, since you cannot use the BigQuery Data Transfer Service to transfer data out of BigQuery, A is the correct answer.\n\nSource: https://cloud.google.com/bigquery/docs/dts-introduction"
      },
      {
        "date": "2023-07-22T09:06:00.000Z",
        "voteCount": 1,
        "content": "What VishalB said is correct."
      },
      {
        "date": "2023-04-27T04:53:00.000Z",
        "voteCount": 1,
        "content": "GPT: Option A: Using Storage Transfer Service to transfer offsite backup files to a Cloud Storage Multi-Regional storage bucket is not the best option for this use case since this service is designed to transfer data from on-premises or other cloud providers to Google Cloud, not for data transfer from YouTube channels.\nOption B: Using Storage Transfer Service to transfer offsite backup files to a Cloud Storage Regional bucket as a final destination is also not the best option since this service is designed for data transfer from on-premises or other cloud providers to Google Cloud, not for data transfer from YouTube channels."
      },
      {
        "date": "2023-04-27T04:54:00.000Z",
        "voteCount": 1,
        "content": "Option C: BigQuery Data Transfer Service is specifically designed for ingesting data into BigQuery from various sources, including YouTube channels, and supports automated data transfers on a schedule. The service can also transfer data to Cloud Storage buckets, which makes it an ideal choice for this use case. The Multi-Regional storage bucket would ensure high availability and low latency for access to the data globally."
      },
      {
        "date": "2023-04-27T04:54:00.000Z",
        "voteCount": 2,
        "content": "Option D: Using BigQuery Data Transfer Service to transfer data to a Cloud Storage Regional storage bucket is not the best option for this use case since it may not be the most efficient or cost-effective solution for global teams that need to access the data. A multi-regional storage bucket would be more suitable for this use case.\nTherefore, option C is the best option for setting up the log data transfer from YouTube channels to Google Cloud for analysis."
      },
      {
        "date": "2023-04-25T00:09:00.000Z",
        "voteCount": 1,
        "content": "Option C suggests using BigQuery Data Transfer Service to transfer data to a Cloud Storage Multi-Regional storage bucket. While this is a valid approach to transfer data to a storage bucket, using a multi-regional bucket is not necessary for this use case and can incur additional cost."
      },
      {
        "date": "2023-04-25T00:16:00.000Z",
        "voteCount": 1,
        "content": "also chatgpt: Option C suggests using BigQuery Data Transfer Service to transfer the offsite backup files to a Cloud Storage Multi-Regional storage bucket as a final destination. This option is more appropriate as it allows you to easily transfer the data from YouTube channels to a Cloud Storage Multi-Regional bucket, which can then be accessed and analyzed using ANSI SQL and other types of analysis in BigQuery. ....Sh*t"
      },
      {
        "date": "2023-04-15T23:10:00.000Z",
        "voteCount": 3,
        "content": "Answer is A\n\nAs Bigquery data transfer service cannot support loading data from offsite locations.\n\nhttps://cloud.google.com/bigquery/docs/dts-introduction"
      },
      {
        "date": "2023-04-13T18:32:00.000Z",
        "voteCount": 4,
        "content": "I think it is talking about this one. \nThe BigQuery Data Transfer Service for YouTube allows you to automatically schedule and manage recurring load jobs for YouTube Channel reports.\nSince then, it should be \"C\"\nhttps://cloud.google.com/bigquery/docs/youtube-channel-transfer"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 70,
    "url": "https://www.examtopics.com/discussions/google/view/16631-exam-professional-data-engineer-topic-1-question-70/",
    "body": "You are designing storage for very large text files for a data pipeline on Google Cloud. You want to support ANSI SQL queries. You also want to support compression and parallel load from the input locations using Google recommended practices. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform text files to compressed Avro using Cloud Dataflow. Use BigQuery for storage and query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTransform text files to compressed Avro using Cloud Dataflow. Use Cloud Storage and BigQuery permanent linked tables for query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress text files to gzip using the Grid Computing Tools. Use BigQuery for storage and query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCompress text files to gzip using the Grid Computing Tools. Use Cloud Storage, and then import into Cloud Bigtable for query."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-11T06:08:00.000Z",
        "voteCount": 59,
        "content": "B.\nThe question is focused on designing storage for very large files, with support for compression, ANSI SQL queries, and parallel loading from the input locations. This can be met using GCS for storage and Bigquery permanent tables with external data source in GCS."
      },
      {
        "date": "2020-07-28T13:35:00.000Z",
        "voteCount": 10,
        "content": "why GCS as external since Bigquery can be used as storage as well?"
      },
      {
        "date": "2020-07-28T13:35:00.000Z",
        "voteCount": 11,
        "content": "A seems correct for me"
      },
      {
        "date": "2020-08-20T21:17:00.000Z",
        "voteCount": 4,
        "content": "Since its best practice, i go by with B not A"
      },
      {
        "date": "2021-03-12T07:23:00.000Z",
        "voteCount": 2,
        "content": "They want to store the files if you try with bq I think you will need to strike the word compression."
      },
      {
        "date": "2022-12-17T07:24:00.000Z",
        "voteCount": 5,
        "content": "The question focuses on \"designing storage\", rather than designing a data warehouse."
      },
      {
        "date": "2020-03-21T08:38:00.000Z",
        "voteCount": 15,
        "content": "Should be A"
      },
      {
        "date": "2022-06-15T00:04:00.000Z",
        "voteCount": 7,
        "content": "Not A : Importing data into BigQuery may take more time compared to creating external tables on data. Additional storage costs by BigQuery is another issue which can be more expensive than Google Storage."
      },
      {
        "date": "2024-08-28T03:08:00.000Z",
        "voteCount": 1,
        "content": "copy to gcs and use external tble in bq"
      },
      {
        "date": "2024-07-25T14:13:00.000Z",
        "voteCount": 1,
        "content": "Should be A.\n\nCheck this link for the advantage of load Avro data to BigQuery https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro#advantages_of_avro\n\n\"\"\"The Avro binary format:\n* Is faster to load. The data can be read in parallel, even if the data blocks are compressed.\n* Doesn't require typing or serialization.\n* Is easier to parse because there are no encoding issues found in other formats such as ASCII.\nWhen you load Avro files into BigQuery, the table schema is automatically retrieved from the self-describing source data.\"\"\""
      },
      {
        "date": "2024-07-25T14:15:00.000Z",
        "voteCount": 1,
        "content": "While option B can work, it introduces additional complexity by linking Cloud Storage with BigQuery. Directly storing data in BigQuery is more efficient for querying purposes.\n\nThere are no requirements about cost, So simple is better"
      },
      {
        "date": "2024-03-28T23:42:00.000Z",
        "voteCount": 2,
        "content": "B makes sense"
      },
      {
        "date": "2023-12-17T00:42:00.000Z",
        "voteCount": 3,
        "content": "1. Store Avro files in GCS\n2. Query them in BigQuery (federated tables)"
      },
      {
        "date": "2023-05-31T05:17:00.000Z",
        "voteCount": 6,
        "content": "Answer is B.\nThe requirements are:\n- storage for compressed text files\n- parallel loads to SQL tool\n\nAVRO is a compressed format for text files, which makes it possible to load chunks of a very large file in parallel to BigQuery.\n\ngzip files are seamless in GCS though, but cannot load in parallel to BQ."
      },
      {
        "date": "2023-01-22T15:35:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer:\nA. Transform text files to compressed Avro using Cloud Dataflow. Use BigQuery for storage and query.\n\nThis option offers several advantages:\n\n- Transforming the text files to compressed Avro using Cloud Dataflow allows for parallel processing of the input data, improving the efficiency of the pipeline.\n\n- Compressing the data in Avro format further reduces the storage space required and improves data transfer performance.\n\n- Storing the data in BigQuery supports ANSI SQL queries and allows for easy querying of the data.\n\n- BigQuery is a fully-managed data warehousing solution, it's scalable and can handle large datasets and concurrent queries, so it's suitable for large text files."
      },
      {
        "date": "2023-01-22T15:35:00.000Z",
        "voteCount": 1,
        "content": "Option B is similar to option A but it's using a permanent linked table between Cloud Storage and BigQuery, this approach is not recommended as it's not efficient and could lead to data duplication, and it doesn't take advantage of the parallel processing capabilities of Cloud Dataflow.\n\nOption C and D are incorrect because they don't take advantage of the parallel processing capabilities of Cloud Dataflow, and they don't use Avro format for compression which is more efficient and recommended by Google. Storing the data in Cloud Bigtable also doesn't support ANSI SQL queries which is a requirement for this use case."
      },
      {
        "date": "2022-12-17T07:32:00.000Z",
        "voteCount": 3,
        "content": "Designing storage solution, not data warehousing -&gt; So Cloud Storage.\n\nSupport compression -&gt; just use Avro\nParallel load -&gt; refers to upload from input locations, NOT download.\n\nLoad in parallel using -m flag for gsutil cp\n\nhttps://cloud.google.com/storage/docs/uploads-downloads#console"
      },
      {
        "date": "2022-12-11T04:47:00.000Z",
        "voteCount": 3,
        "content": "C and D are discarted.\nA and B are possible.\nA is the best for query, but \u2026 the sentence says: ou also want to support compression and parallel load from the input locations using Google recommended practices.\nBigQuery only support parallel load from storage, storage support parallel load from CLI. So the only option is B."
      },
      {
        "date": "2022-12-06T06:44:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-03T22:56:00.000Z",
        "voteCount": 1,
        "content": "\"Very large files\" and \"long term storage\" are two key phrases- both of which indicate to pick cloud storage as option. Hence B is correct."
      },
      {
        "date": "2022-12-02T04:04:00.000Z",
        "voteCount": 3,
        "content": "All the comments argue about [A] and [B] as a storage destination. But there is a limitation on loading compressed Avro files into BigQuery that cuts the Gordian knot:\n\u2757 \"... Compressed Avro files are not supported, but compressed data blocks are ...\"\nFrom: https://cloud.google.com/bigquery/docs/batch-loading-data#loading_compressed_and_uncompressed_data"
      },
      {
        "date": "2023-04-13T18:35:00.000Z",
        "voteCount": 1,
        "content": "No, it is not\nhttps://github.com/GoogleCloudPlatform/bigquery-ingest-avro-dataflow-sample"
      },
      {
        "date": "2023-10-24T09:12:00.000Z",
        "voteCount": 1,
        "content": "Compressed AVRO files are supported by BQ"
      },
      {
        "date": "2022-11-06T10:43:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro\n\nAdvantages of Avro:\nAvro is the preferred format for loading data into BigQuery. Loading Avro files has the following advantages over CSV and JSON (newline delimited):\n\n    The Avro binary format:\n        Is faster to load. The data can be read in parallel, even if the data blocks are compressed.\n        Doesn't require typing or serialization.\n        Is easier to parse because there are no encoding issues found in other formats such as ASCII.\n    When you load Avro files into BigQuery, the table schema is automatically retrieved from the self-describing source data."
      },
      {
        "date": "2022-05-08T14:25:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/bigquery/docs/loading-data-cloud-storage-avro\nThe Avro binary format:\nIs faster to load. The data can be read in parallel, even if the data blocks are compressed"
      },
      {
        "date": "2022-11-06T10:49:00.000Z",
        "voteCount": 2,
        "content": "Your comment supports A more than B"
      },
      {
        "date": "2022-04-15T05:36:00.000Z",
        "voteCount": 1,
        "content": "B\nBecause they are talking about the parallel loading from input locations."
      },
      {
        "date": "2022-04-05T13:32:00.000Z",
        "voteCount": 2,
        "content": "B. The objetive is to follow the best practices."
      },
      {
        "date": "2022-04-05T13:33:00.000Z",
        "voteCount": 1,
        "content": "Sory I mean A not B :-)"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 71,
    "url": "https://www.examtopics.com/discussions/google/view/17111-exam-professional-data-engineer-topic-1-question-71/",
    "body": "You are developing an application on Google Cloud that will automatically generate subject labels for users' blog posts. You are under competitive pressure to add this feature quickly, and you have no additional developer resources. No one on your team has experience with machine learning. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the Cloud Natural Language API from your application. Process the generated Entity Analysis as labels.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall the Cloud Natural Language API from your application. Process the generated Sentiment Analysis as labels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild and train a text classification model using TensorFlow. Deploy the model using Cloud Machine Learning Engine. Call the model from your application and process the results as labels.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild and train a text classification model using TensorFlow. Deploy the model using a Kubernetes Engine cluster. Call the model from your application and process the results as labels."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-07-18T09:37:00.000Z",
        "voteCount": 36,
        "content": "Correct Answer : A\n\nEntity analysis  -&gt; Identify entities within documents receipts, invoices, and contracts and label them by types such as date, person, contact information, organization, location, events, products, and media.\n\nSentiment analysis -&gt; Understand the overall opinion, feeling, or attitude sentiment expressed in a block of text.\n--  Avoid Custom models"
      },
      {
        "date": "2023-01-04T14:12:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/natural-language/docs/analyzing-entities\nhttps://cloud.google.com/natural-language/docs/analyzing-sentiment"
      },
      {
        "date": "2020-03-21T08:45:00.000Z",
        "voteCount": 12,
        "content": "should be A"
      },
      {
        "date": "2023-09-30T12:03:00.000Z",
        "voteCount": 3,
        "content": "For the first time, the answer in exam topics matches community vote :-)."
      },
      {
        "date": "2023-05-24T23:50:00.000Z",
        "voteCount": 2,
        "content": "Of course the answer is A. Since the problem already states that you don't have time, resources or expertise. So the best solution in the case is to utilize the available API. Also since we need to extract the labels and not the sentiment of the text, we'll go for option A and not B"
      },
      {
        "date": "2023-01-23T13:24:00.000Z",
        "voteCount": 1,
        "content": "A. Call the Cloud Natural Language API from your application. Process the generated Entities Analysis as labels.\n\nThe Cloud Natural Language API is a pre-trained machine learning model that can be used for natural language processing tasks such as entity recognition, sentiment analysis, and syntax analysis. The API can be called from your application using a simple API call, and it can generate entities analysis that can be used as labels for the user's blog posts. This would be the quickest and easiest option for your team since it would not require any machine learning expertise or additional developer resources to build and train a model. Additionally, it will give you accurate and up-to-date results as the API is constantly updated by Google."
      },
      {
        "date": "2023-01-04T14:09:00.000Z",
        "voteCount": 1,
        "content": "Answer is A \nCall the Cloud Natural Language API from your application. Process the generated Entity Analysis as labels.\n\nEntity analysis -&gt; Identify entities within documents receipts, invoices, and contracts and label them by types such as date, person, contact information, organization, location, events, products, and media.\n\nSentiment analysis -&gt; Understand the overall opinion, feeling, or attitude sentiment expressed in a block of text."
      },
      {
        "date": "2022-12-06T06:43:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/natural-language/docs/analyzing-entities\nEntity Analysis inspects the given text for known entities (proper nouns such as public figures, landmarks, etc.), and returns information about those entities."
      },
      {
        "date": "2022-11-08T01:00:00.000Z",
        "voteCount": 2,
        "content": "Apparently, there is unanimity on answer [A]\nWhat if there was another available answer in an actual exam?\n\nE. Call the Cloud Natural Language API from your application. Process the generated Content Classification as labels\n\nWhat would you choose, A or E?\n\nMy opinion is that Content Classification is more suitable for detecting subject."
      },
      {
        "date": "2022-07-16T09:33:00.000Z",
        "voteCount": 1,
        "content": "A is the right one . Doc says:\nEntity analysis inspects the given text for known entities (Proper nouns such as public figures, landmarks, and so on. Common nouns such as restaurant, stadium, and so on.) and returns information about those entities. Entity analysis is performed with the analyzeEntities method."
      },
      {
        "date": "2022-05-10T04:09:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      },
      {
        "date": "2022-02-10T07:26:00.000Z",
        "voteCount": 1,
        "content": "a is correct"
      },
      {
        "date": "2021-11-05T16:26:00.000Z",
        "voteCount": 1,
        "content": "A. \nCD don't work as it requires Machine Learning experience. \nB - Sentiment Analysis is to analyze attitude, opinion, etc. So A."
      },
      {
        "date": "2021-06-29T04:51:00.000Z",
        "voteCount": 3,
        "content": "Vote for A"
      },
      {
        "date": "2020-08-19T17:01:00.000Z",
        "voteCount": 4,
        "content": "A is correct"
      },
      {
        "date": "2020-03-27T20:44:00.000Z",
        "voteCount": 5,
        "content": "Answer: A\nDescription: As time is less, use cloud NLP and entity is used to label general subjects, sentiment label for sentiment analysis"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 72,
    "url": "https://www.examtopics.com/discussions/google/view/16832-exam-professional-data-engineer-topic-1-question-72/",
    "body": "You are designing storage for 20 TB of text files as part of deploying a data pipeline on Google Cloud. Your input data is in CSV format. You want to minimize the cost of querying aggregate values for multiple users who will query the data in Cloud Storage with multiple engines. Which storage service and schema design should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Bigtable for storage. Install the HBase shell on a Compute Engine instance to query the Cloud Bigtable data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Bigtable for storage. Link as permanent tables in BigQuery for query.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage for storage. Link as permanent tables in BigQuery for query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage for storage. Link as temporary tables in BigQuery for query."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2021-03-11T07:02:00.000Z",
        "voteCount": 52,
        "content": "answer C:\nBigQuery can access data in external sources, known as federated sources. Instead of first\nloading data into BigQuery, you can create a reference to an external source. External\nsources can be Cloud Bigtable, Cloud Storage, and Google Drive.\nWhen accessing external data, you can create either permanent or temporary external\ntables. Permanent tables are those that are created in a dataset and linked to an external\nsource. Dataset-level access controls can be applied to these tables. When you are using a\ntemporary table, a table is created in a special dataset and will be available for approxi-\nmately 24 hours. Temporary tables are useful for one-time operations, such as loading data\ninto a data warehouse.\n\"Dan Sullivan\" Book"
      },
      {
        "date": "2020-03-21T08:48:00.000Z",
        "voteCount": 30,
        "content": "Should be C"
      },
      {
        "date": "2023-11-01T12:57:00.000Z",
        "voteCount": 1,
        "content": "On so many of these questions, how do you actually know if you're correct. I said C but the correct answer was A. Honestly, it's driving me crazy."
      },
      {
        "date": "2023-07-23T02:11:00.000Z",
        "voteCount": 1,
        "content": "For the ones saying BigTable is cheaper, BigTable in eu-north1 costs $0.748/hour per node. So if you were to run the node 24/7 you would pay more than 500$ per month. Querying 1TB of data in BigQuery is 7.5$. With smart querying and good database design you can minimize the bytes processed by BQ. So even though BigTable does not directly charge for querying, it charges for running the cluster and the overall price does not make sense. And as far as I know, it's not possible to spin up and shut down BigTable automatically. \n\nAlso, since the table is an external table to BigQuery, we incur no cost for storing that data in BigQuery and paying 300$ per month for storage."
      },
      {
        "date": "2023-01-23T14:05:00.000Z",
        "voteCount": 2,
        "content": "C. Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.\n\nCloud Storage is a highly durable and cost-effective object storage service that can be used to store large amounts of text files. By storing the input data in CSV format in Cloud Storage, you can minimize costs while still being able to query the data using BigQuery.\n\nBigQuery is a fully-managed, highly-scalable data warehouse that allows you to perform fast SQL-like queries on large datasets. By linking the Cloud Storage data as permanent tables in BigQuery, you can enable multiple users to query the data using multiple engines without the need for additional compute resources. This approach would be the most cost-effective for querying aggregate values for multiple users, as BigQuery charges based on the amount of data scanned per query, so the more data you store in BigQuery the less you pay per query."
      },
      {
        "date": "2023-01-23T14:05:00.000Z",
        "voteCount": 1,
        "content": "Option D, using Cloud Storage for storage and linking as temporary tables in BigQuery for query, would not be the best choice because temporary tables only exist for the duration of a user session or query and you would need to create and delete them each time a user queries the data, which would add additional cost and complexity to the process.\n\nOption A, Using Cloud Bigtable for storage, and installing the HBase shell on a Compute Engine instance to query the data, is not a cost-effective solution as Cloud Bigtable is a managed NoSQL database service which is more expensive than storing in Cloud Storage and querying in BigQuery.\n\nOption B, Using Cloud Bigtable for storage, and linking as permanent tables in BigQuery for query, is not a cost-effective solution as Cloud Bigtable is a managed NoSQL database service which is more expensive than storing in Cloud Storage and querying in BigQuery."
      },
      {
        "date": "2023-01-11T17:27:00.000Z",
        "voteCount": 3,
        "content": "CSV files - Cloud Storage\nBigQuery - Aggregate, multiple users\nPermanent table - multiple users\nExternal Tables is Easy to implement, cost effective"
      },
      {
        "date": "2022-12-11T15:12:00.000Z",
        "voteCount": 7,
        "content": "The 'correct' answers on this platform are ridiculous"
      },
      {
        "date": "2022-12-06T06:40:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigquery/docs/external-data-cloud-storage#create_a_permanent_external_table"
      },
      {
        "date": "2022-02-12T05:51:00.000Z",
        "voteCount": 1,
        "content": "Answer is C Use Cloud Storage for storage. Link as permanent tables in BigQuery for query.\n\nBigQuery can access data in external sources, known as federated sources. Instead of first loading data into BigQuery, you can create a reference to an external source. External sources can be Cloud Bigtable, Cloud Storage, and Google Drive.\n\nWhen accessing external data, you can create either permanent or temporary external tables. Permanent tables are those that are created in a dataset and linked to an external source. Dataset-level access controls can be applied to these tables. When you are using a temporary table, a table is created in a special dataset and will be available for approxi- mately 24 hours. Temporary tables are useful for one-time operations, such as loading data into a data warehouse"
      },
      {
        "date": "2022-01-05T08:31:00.000Z",
        "voteCount": 3,
        "content": "Bigtable is expensive. So Cloud Storage for storing and BigQuery with permanent table for linking and querying."
      },
      {
        "date": "2021-12-22T10:31:00.000Z",
        "voteCount": 5,
        "content": "Not A or B\nBig table is expensive, que initial data is in csv format, besides, if others are going to query data with multiple engines\u2026 GCS is the storage. Between c and D is all about permanent or temorary.\nPermanent table is a table that is created in a dataset and is linked to your external data source. Because the table is permanent, you can use dataset-level access controls to share the table with others who also have access to the underlying external data source, and you can query the table at any time.\nWhen you use a temporary table, you do not create a table in one of your BigQuery datasets. Because the table is not permanently stored in a dataset, it cannot be shared with others. Querying an external data source using a temporary table is useful for one-time, ad-hoc queries over external data, or for extract, transform, and load (ETL) processes.\nI think is C."
      },
      {
        "date": "2021-12-22T10:31:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/blog/products/gcp/accessing-external-federated-data-sources-with-bigquerys-data-access-layer.\nPermanent table\u2014You create a table in a BigQuery dataset that is linked to your external data source. This allows you to use BigQuery dataset-level IAM roles to share the table with others who may have access to the underlying external data source. Use permanent tables when you need to share the table with others.\nTemporary table\u2014You submit a command that includes a query and creates a non-permanent table linked to the external data source. With this approach you do not create a table in one of your BigQuery datasets, so make sure to give consideration towards sharing the query or table. Consider using a temporary table for one-time, ad-hoc queries, or for one time extract, transform, or load (ETL) workflows"
      },
      {
        "date": "2021-12-01T20:38:00.000Z",
        "voteCount": 2,
        "content": "Answer is A. While C seems the most reasonable answer there are 2 points to notics: a) load jobs are limited to 15 TB across all input files in BigQuery (https://cloud.google.com/bigquery/quotas);  b) It is requested to minimize the cost of querying and queries in BigTable are free, while queries in BigQuery are charged per byte (https://cloud.google.com/bigquery/pricing)"
      },
      {
        "date": "2021-11-24T01:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/external-data-bigtable#:~:text=shared%20with%20others.-,Querying%20an%20external%20data%20source%20using%20a%20temporary%20table%20is%20useful%20for%20one%2Dtime%2C%20ad%2Dhoc%20queries%20over%20external%20data%2C%20or%20for%20extract%2C%20transform%2C%20and%20load%20(ETL)%20processes.,-Querying%20Cloud%20Bigtable"
      },
      {
        "date": "2021-10-19T19:46:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2021-09-28T05:53:00.000Z",
        "voteCount": 3,
        "content": "A is correct since the question asks \"You want to minimize the cost of querying aggregate values\" =&gt; Big Table is free when querying data."
      },
      {
        "date": "2021-09-15T15:49:00.000Z",
        "voteCount": 1,
        "content": "Vote for C"
      },
      {
        "date": "2021-07-05T20:04:00.000Z",
        "voteCount": 4,
        "content": "Interesting options. For me, A &amp; B ruled out because BigTable doesn\u2019t fit this use case, leaves us with C &amp; D. C will incur additional cost of storing data in GCS &amp; BigQuery because it mentions linking. \n\nSo I would go with D ie store the data in GCS and create external tables in BigQuery."
      },
      {
        "date": "2021-08-03T10:11:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/writing-results#temporary_and_permanent_tables"
      },
      {
        "date": "2021-08-26T13:38:00.000Z",
        "voteCount": 1,
        "content": "Storage cost of data for BQ is the same as standard cloud storage, actually less for long term storage as it automatically moves to nearline storage.\n\nhttps://cloud.google.com/bigquery/pricing#storage\nhttps://cloud.google.com/storage#section-10"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 73,
    "url": "https://www.examtopics.com/discussions/google/view/17112-exam-professional-data-engineer-topic-1-question-73/",
    "body": "You are designing storage for two relational tables that are part of a 10-TB database on Google Cloud. You want to support transactions that scale horizontally.<br>You also want to optimize data for range queries on non-key columns. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for storage. Add secondary indexes to support query patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL for storage. Use Cloud Dataflow to transform data to support query patterns.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner for storage. Add secondary indexes to support query patterns.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner for storage. Use Cloud Dataflow to transform data to support query patterns."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T20:51:00.000Z",
        "voteCount": 31,
        "content": "Answer: C\nDescription: Spanner allows transaction tables to scale horizontally and secondary indexes for range queries"
      },
      {
        "date": "2020-03-21T08:52:00.000Z",
        "voteCount": 9,
        "content": "Correct: C"
      },
      {
        "date": "2023-04-14T06:28:00.000Z",
        "voteCount": 1,
        "content": "Correct: C"
      },
      {
        "date": "2023-01-26T13:30:00.000Z",
        "voteCount": 2,
        "content": "Cloud Spanner is a fully-managed, horizontally scalable relational database service that supports transactions and allows you to optimize data for range queries on non-key columns. By using Cloud Spanner for storage, you can ensure that your database can scale horizontally to meet the needs of your application.\nTo optimize data for range queries on non-key columns, you can add secondary indexes, this will allow you to perform range scans on non-key columns, which can improve the performance of queries that filter on non-key columns."
      },
      {
        "date": "2023-01-23T14:09:00.000Z",
        "voteCount": 3,
        "content": "C. Use Cloud Spanner for storage. Add secondary indexes to support query patterns.\n\nCloud Spanner is a fully-managed, horizontally scalable relational database service that supports transactions and allows you to optimize data for range queries on non-key columns. By using Cloud Spanner for storage, you can ensure that your database can scale horizontally to meet the needs of your application.\nTo optimize data for range queries on non-key columns, you can add secondary indexes, this will allow you to perform range scans on non-key columns, which can improve the performance of queries that filter on non-key columns."
      },
      {
        "date": "2023-01-23T14:09:00.000Z",
        "voteCount": 1,
        "content": "- Option A, Using Cloud SQL for storage and adding secondary indexes to support query patterns, may not be the best option as Cloud SQL is a relational database service that does not support horizontal scaling and may not be able to handle the large amount of data and the number of queries required by your application."
      },
      {
        "date": "2023-01-23T14:09:00.000Z",
        "voteCount": 1,
        "content": "- Option B, Using Cloud SQL for storage and using Cloud Dataflow to transform data to support query patterns, may not be the best option as Cloud SQL is a relational database service that does not support horizontal scaling and may not be able to handle the large amount of data and the number of queries required by your application. Additionally, Cloud Dataflow is a data processing service and not a storage service, so it may not be the best fit for this use case.\n\n- Option D, Using Cloud Spanner for storage and using Cloud Dataflow to transform data to support query patterns, is not necessary as Cloud Spanner provides the ability to optimize data for range queries on non-key columns by adding secondary indexes. Cloud Spanner also supports transactional consistency, which is a feature that allows you to perform multiple operations that must be performed together in a single transaction. Additionally, Cloud Dataflow is a data processing service and not a storage service, so it may not be the best fit for this use case."
      },
      {
        "date": "2023-07-23T02:16:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL does support replicas to increase availability. Why is that not considered horizontal scaling?"
      },
      {
        "date": "2022-12-06T06:36:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/autoscaling-cloud-spanner\nWhen you create a Cloud Spanner instance, you choose the number of compute capacity nodes or processing units to serve your data. However, if the workload of an instance changes, Cloud Spanner doesn't automatically adjust the size of the instance. This document introduces the Autoscaler tool for Cloud Spanner (Autoscaler), an open source tool that you can use as a companion tool to Cloud Spanner. This tool lets you automatically increase or reduce the number of nodes or processing units in one or more Spanner instances based on how their capacity is being used.\n\nhttps://cloud.google.com/spanner/docs/secondary-indexes\nYou can also create secondary indexes for other columns. Adding a secondary index on a column makes it more efficient to look up data in that column."
      },
      {
        "date": "2022-09-13T16:15:00.000Z",
        "voteCount": 1,
        "content": "As sumanshu said"
      },
      {
        "date": "2021-10-19T19:47:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2021-06-29T05:59:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2021-07-08T15:33:00.000Z",
        "voteCount": 8,
        "content": "A is not correct because Cloud SQL does not natively scale horizontally.\nB is not correct because Cloud SQL does not natively scale horizontally.\nC is correct because Cloud Spanner scales horizontally, and you can create secondary indexes for the range queries that are required.\nD is not correct because Dataflow is a data pipelining tool to move and transform data, but the use case is centered around querying."
      },
      {
        "date": "2021-03-23T15:11:00.000Z",
        "voteCount": 2,
        "content": "Answer: C\nhttps://cloud.google.com/spanner/docs/secondary-indexes"
      },
      {
        "date": "2020-12-21T11:42:00.000Z",
        "voteCount": 3,
        "content": "Correct: C"
      },
      {
        "date": "2020-11-15T08:26:00.000Z",
        "voteCount": 2,
        "content": "Correct answers is C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 74,
    "url": "https://www.examtopics.com/discussions/google/view/79320-exam-professional-data-engineer-topic-1-question-74/",
    "body": "Your financial services company is moving to cloud technology and wants to store 50 TB of financial time-series data in the cloud. This data is updated frequently and new data will be streaming in all the time. Your company also wants to move their existing Apache Hadoop jobs to the cloud to get insights into this data.<br>Which product should they use to store the data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGoogle Cloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-06T06:29:00.000Z",
        "voteCount": 12,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable\nBigtable is Google's NoSQL Big Data database service. It's the same database that powers many core Google services, including Search, Analytics, Maps, and Gmail. Bigtable is designed to handle massive workloads at consistent low latency and high throughput, so it's a great choice for both operational and analytical applications, including IoT, user analytics, and financial data analysis.\n\nBigtable is an excellent option for any Apache Spark or Hadoop uses that require Apache HBase. Bigtable supports the Apache HBase 1.0+ APIs and offers a Bigtable HBase client in Maven, so it is easy to use Bigtable with Dataproc."
      },
      {
        "date": "2024-08-04T11:51:00.000Z",
        "voteCount": 1,
        "content": "Gmail is migrated to Spanner now!"
      },
      {
        "date": "2022-12-17T11:58:00.000Z",
        "voteCount": 2,
        "content": "Hbase concept here us beautiful"
      },
      {
        "date": "2024-02-06T21:58:00.000Z",
        "voteCount": 3,
        "content": "Every time you hear financial, time series, fast reads and write data,  Any of that combinations, think Big Table first.\nSo A."
      },
      {
        "date": "2023-07-23T02:26:00.000Z",
        "voteCount": 2,
        "content": "At first I thought that GCS was the answer but the question does mention that the data is updated frequently. Thereby, it has to be BigTable since we talk about a large amount of data, a streaming application and many individual updates. Storing the data in BigQuery and having to make individual updates doesn't make sense, and neither does running Apache jobs.\n\nIf the requirement for updates was not there I would not see any issue with GCS. GCS could serve as a replacement to HDFS and run Hadoop jobs from Dataproc."
      },
      {
        "date": "2023-06-18T23:28:00.000Z",
        "voteCount": 1,
        "content": "This scenario screams for BigTable.\n\nIt's not B) BigQuery or C) Cloud Storage because both aren't supposed to contain data that is updated frequently. Then, we have to decide between A) BigTable and D) Datastore.\n\nIt is A) BigTable because\n- it is the most suited for real-time / high-frequency updates\n- it is similar to HBase, which is commonly used in Hadoop ecosystem stacks to store streaming / time-series data."
      },
      {
        "date": "2023-05-25T01:28:00.000Z",
        "voteCount": 1,
        "content": "Many here also selected Cloud Storage. But the way I see it BigTable is specifically for low latency, high throughput, mission critical streaming data (financial data is one of them). Also the mentioning of Hadoop that points to HBase functionality if BigTable clarifies the choice more."
      },
      {
        "date": "2023-05-01T01:26:00.000Z",
        "voteCount": 2,
        "content": "BigTable - a No-SQL database but does not support SQL Querying\nApache HBase - Based on Google's BigTable on top of HDFS and you can migrate Hadoop Apps to Cloud BigTable with the HBase API"
      },
      {
        "date": "2023-04-13T18:44:00.000Z",
        "voteCount": 1,
        "content": "A. time series data"
      },
      {
        "date": "2023-02-27T20:44:00.000Z",
        "voteCount": 1,
        "content": "Please note that there is Connector for Bigtable for Hadoop\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable"
      },
      {
        "date": "2023-01-23T18:22:00.000Z",
        "voteCount": 1,
        "content": "Why not Biquery?\n\nGoogle BigQuery would be the best option for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time. It is a fully managed, cloud-native data warehouse that allows you to analyze large datasets using SQL-like queries, and it can handle streaming data as well as batch data. Additionally, it can easily integrate with Apache Hadoop to allow your company to run their existing Hadoop jobs in the cloud and gain insights into the data."
      },
      {
        "date": "2023-01-23T18:22:00.000Z",
        "voteCount": 2,
        "content": "A. Google Bigtable is a fully managed, NoSQL, wide-column database that is designed for large scale, low-latency workloads. It is well suited for use cases such as real-time analytics, IoT, and gaming, but it may not be the best fit for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time. It lacks built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It is more focused on handling high-performance low-latency workloads, while BigQuery is focused on providing an easy and cost-effective way to analyze large amounts of data using SQL-like queries. Additionally, Bigtable doesn't provide built-in support for running Apache Hadoop jobs, and it would require additional work to integrate it with Hadoop and set it up for data warehousing and Business Intelligence use cases."
      },
      {
        "date": "2023-01-23T18:23:00.000Z",
        "voteCount": 2,
        "content": "C. Google Cloud Storage is an object storage service that allows you to store and retrieve large amounts of unstructured data, such as video, audio, images and other files. It is not a data warehouse and does not provide built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It would not be suitable for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time.\n\nD. Google Cloud Datastore is a fully-managed, NoSQL document database that allows you to store, retrieve, and query data. It is not a data warehouse and does not provide built-in support for SQL-like queries, which is a standard way of analyzing data in Data Warehousing and Business Intelligence. It would not be suitable for storing and analyzing large amounts of financial time-series data that is frequently updated and streamed in real-time."
      },
      {
        "date": "2023-01-23T18:23:00.000Z",
        "voteCount": 1,
        "content": "Can someone clarify why Bigtable and Not Bigquery? Super Confused."
      },
      {
        "date": "2023-04-25T01:46:00.000Z",
        "voteCount": 1,
        "content": "Yes, it is possible to analyze data in Bigtable. Bigtable is a distributed NoSQL database that is designed to handle large volumes of structured data with high read and write throughput. While Bigtable itself does not provide analysis tools, it is often used in combination with other tools and technologies to perform analysis on the stored data."
      },
      {
        "date": "2023-01-17T11:45:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigtable/docs/schema-design-time-series"
      },
      {
        "date": "2022-11-21T04:54:00.000Z",
        "voteCount": 3,
        "content": "Time series data = Bigtable... So it's A"
      },
      {
        "date": "2022-11-20T12:47:00.000Z",
        "voteCount": 1,
        "content": "Option A seems right"
      },
      {
        "date": "2022-11-20T10:50:00.000Z",
        "voteCount": 1,
        "content": "Big Table has a HBase compliant API and is transactional unlike GCS."
      },
      {
        "date": "2022-11-17T05:31:00.000Z",
        "voteCount": 1,
        "content": "BigTable can take in data from dataproc, spark and hadoop\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable#using_with"
      },
      {
        "date": "2022-11-06T11:08:00.000Z",
        "voteCount": 3,
        "content": "It must be C because of the existing Hadoop jobs"
      },
      {
        "date": "2022-11-08T14:56:00.000Z",
        "voteCount": 6,
        "content": "On 2nd thought, it\u2019s Bigtable: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-bigtable"
      },
      {
        "date": "2022-10-04T15:43:00.000Z",
        "voteCount": 2,
        "content": "I think it is C"
      },
      {
        "date": "2022-09-28T09:02:00.000Z",
        "voteCount": 2,
        "content": "Use Datarproc with Cloud Storage in combo with HDFS\nhttps://cloud.google.com/dataproc/docs/concepts/dataproc-hdfs"
      },
      {
        "date": "2023-07-07T06:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A: Hadoop doesn't mean Dataproc + HDFS. This scenario is about time series that is a use-case for BigTable. Coincidentally BigTable is the best solution for migration of HBase..."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 75,
    "url": "https://www.examtopics.com/discussions/google/view/79767-exam-professional-data-engineer-topic-1-question-75/",
    "body": "An organization maintains a Google BigQuery dataset that contains tables with user-level data. They want to expose aggregates of this data to other Google<br>Cloud projects, while still controlling access to the user-level data. Additionally, they need to minimize their overall storage cost and ensure the analysis cost for other projects is assigned to those projects. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and share an authorized view that provides the aggregate results.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and share a new dataset and view that provides the aggregate results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and share a new dataset and table that contains the aggregate results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 25,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-01T20:01:00.000Z",
        "voteCount": 16,
        "content": "A is the answer. Don't be confused by the documentation saying \"Authorized views should be created in a different dataset\". It is a best practice but not a technical requirement. And we don't create a new dataset for each authorized view. If you are not clear on this, try in the system, don't just read the documentation without understanding.\nB is wrong when saying we must SHARE Dataset. Although creating a dataset and view in it will not incur extra cost, but sharing dataset is something we always try not to do.\nAt for the project that run the query it the project to be billed, that is standard behaviour. View only give access to data, whoever run the view will need pay for the query cost"
      },
      {
        "date": "2023-07-13T13:49:00.000Z",
        "voteCount": 3,
        "content": "Have to consider where the billing goes to:\nhttps://stackoverflow.com/questions/52201034/bigquery-authorized-view-cost-billing-account\nhence anwer is B"
      },
      {
        "date": "2023-07-23T02:36:00.000Z",
        "voteCount": 4,
        "content": "Did you even read the answer in the SO link you shared?\nPart of the answer is below:\n\"\"\"After a deeper investigation and some test scenarios, I have confirmed that the billing charges related to the query jobs are applied to the Billing account associated to the project that executes the query; however, the view owner keeps getting the charges related to the storage of the source data.\"\"\"\n\nSoo, if you create an authorized view, the users from the other project that has access to the view will get billed for the querying.\n\nThe only reason to pick up B over A is that it's the recommended approach to store views in a different dataset than the base data."
      },
      {
        "date": "2023-03-04T12:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/authorized-views#:~:text=An%20authorized%20view%20and%20authorized,users%20are%20able%20to%20query."
      },
      {
        "date": "2022-10-25T11:25:00.000Z",
        "voteCount": 13,
        "content": "The link on authorized views (https://cloud.google.com/bigquery/docs/share-access-views) explicitly states \"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\" therefore B is the correct answer because we are to create a new dataset and view within that dataset."
      },
      {
        "date": "2023-11-27T22:45:00.000Z",
        "voteCount": 2,
        "content": "A. Create and share an authorized view that provides the aggregate results.\n\nAn authorized view is a BigQuery feature that allows you to share only a specific subset of data from a table, while still keeping the original data private. This way, the organization can expose only the aggregate data to other projects, while still controlling access to the user-level data. By using an authorized view, the organization can minimize their overall storage cost as the aggregate data takes up less storage space than the original data. Additionally, by using authorized view, the analysis cost for other projects is assigned to those projects."
      },
      {
        "date": "2023-07-23T10:22:00.000Z",
        "voteCount": 1,
        "content": "I thing that is B because for security yo need to create a new data set when share a view, apart when yo grant access the top level is a data set if you share a view in same dataset that you have your tables, that access can see all tables inside dataset."
      },
      {
        "date": "2023-06-18T07:09:00.000Z",
        "voteCount": 2,
        "content": "\"Authorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data.\"\nhttps://cloud.google.com/bigquery/docs/share-access-views?hl=en#console_5"
      },
      {
        "date": "2023-04-14T08:16:00.000Z",
        "voteCount": 1,
        "content": "Selected Answer: A\nB is wrong by itself. why do you need to create a view, if you have already created an aggregated dataset?"
      },
      {
        "date": "2023-03-29T06:01:00.000Z",
        "voteCount": 3,
        "content": "\"they need to minimize their overall storage cost\". Also, you are sharing the aggregate's results, not the underlying table"
      },
      {
        "date": "2023-03-11T00:21:00.000Z",
        "voteCount": 1,
        "content": "minimize cost so view"
      },
      {
        "date": "2023-02-28T09:03:00.000Z",
        "voteCount": 3,
        "content": "A should be the answer, as we need to separate costs according to projects. As in the following SO question (and the attached google resources), the 'project that runs the queries is the project that gets billed.' \nSo we can generate a view and give it's access to the other project to run the analysis\nhttps://stackoverflow.com/questions/52201034/bigquery-authorized-view-cost-billing-account"
      },
      {
        "date": "2023-02-14T08:42:00.000Z",
        "voteCount": 2,
        "content": "I will go with A, as i wanna save cost, dont need to create separate dataset for permanent storage."
      },
      {
        "date": "2023-01-23T18:30:00.000Z",
        "voteCount": 2,
        "content": "A. Create and share an authorized view that provides the aggregate results.\n\nAn authorized view is a BigQuery feature that allows you to share only a specific subset of data from a table, while still keeping the original data private. This way, the organization can expose only the aggregate data to other projects, while still controlling access to the user-level data. By using an authorized view, the organization can minimize their overall storage cost as the aggregate data takes up less storage space than the original data. Additionally, by using authorized view, the analysis cost for other projects is assigned to those projects."
      },
      {
        "date": "2023-01-23T18:31:00.000Z",
        "voteCount": 2,
        "content": "B. Creating and sharing a new dataset and view that provides the aggregate results is also a correct option but not as optimal as authorized view as it creates a copy of the data and increases the storage costs.\nC. Creating and sharing a new dataset and table that contains the aggregate results is also a correct option but not as optimal as authorized view as it creates a copy of the data and increases the storage costs.\nD. Creating dataViewer Identity and Access Management (IAM) roles on the dataset to enable sharing is not the best option as it would give access to the user-level data, not just the aggregate data."
      },
      {
        "date": "2023-01-23T11:13:00.000Z",
        "voteCount": 2,
        "content": "Ensure the analysis cost for other projects is assigned to those projects indicates B is the correct answer."
      },
      {
        "date": "2023-01-20T03:22:00.000Z",
        "voteCount": 2,
        "content": "B is the correct answer"
      },
      {
        "date": "2022-12-27T07:01:00.000Z",
        "voteCount": 1,
        "content": "I would say 1 too"
      },
      {
        "date": "2022-12-20T00:46:00.000Z",
        "voteCount": 2,
        "content": "B is the correct approach."
      },
      {
        "date": "2022-12-14T09:34:00.000Z",
        "voteCount": 1,
        "content": "I think its B, as projects need to be charged separately"
      },
      {
        "date": "2022-12-06T06:26:00.000Z",
        "voteCount": 5,
        "content": "B is the answer.\n\nhttps://cloud.google.com/bigquery/docs/share-access-views#create_a_dataset_where_you_can_store_your_view\nAfter creating your source dataset, you create a new, separate dataset to store the authorized view that you share with your data analysts. In a later step, you grant the authorized view access to the data in the source dataset. Your data analysts then have access to the authorized view, but not direct access to the source data.\n\nAuthorized views should be created in a different dataset from the source data. That way, data owners can give users access to the authorized view without simultaneously granting access to the underlying data. The source data dataset and authorized view dataset must be in the same regional location."
      },
      {
        "date": "2022-12-29T01:28:00.000Z",
        "voteCount": 1,
        "content": "But the wording of option B says create and share a new dataset, do you also need to share dataset apart from authorized view access? In option A, isn't is implicit that authorized view is created on a new dataset and hence option A. B also doesn't mention about Authorized keyword so you may interpret it as normal view which doesn't make sense?"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 76,
    "url": "https://www.examtopics.com/discussions/google/view/17114-exam-professional-data-engineer-topic-1-question-76/",
    "body": "Government regulations in your industry mandate that you have to maintain an auditable record of access to certain types of data. Assuming that all expiring logs will be archived correctly, where should you store data that is subject to that mandate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypted on Cloud Storage with user-supplied encryption keys. A separate decryption key will be given to each authorized user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn Cloud SQL, with separate database user names to each user. The Cloud SQL Admin activity logs will be used to provide the auditability.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIn a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-20T11:27:00.000Z",
        "voteCount": 50,
        "content": "Keywords here are\n1. \"Archived\": Immutable and hence, BQ and Cloud SQL are ruled out\n2. \"Auditable\": Means track any changes done. \nOnly D can provide the audibility piece!\nI will go with D"
      },
      {
        "date": "2023-05-03T10:41:00.000Z",
        "voteCount": 10,
        "content": "I have no idea why so many upvotes on this answer:\n1) archived doesn't mean immutable and cloud storage is not immutable too.\n2) auditable means viewable for authorized personel - and in this case not changes need to be monitored but any access.\n3) with option D it is easy to go around logging - you can add another access to the bucket read the data remove the access and no one will ever know that you accessed the data.\n4) option D is much more difficult - you need to application on AppEngine to log the data and provide access for users.\n5) option D doesn;t explain where and how it stores the audit data - it could be accessed and modified from some side app/service."
      },
      {
        "date": "2020-03-27T20:57:00.000Z",
        "voteCount": 23,
        "content": "Answer: B\nDescription: Bigquery is used to analyse access logs, data access logs capture the details of the user that accessed the data"
      },
      {
        "date": "2021-07-05T06:54:00.000Z",
        "voteCount": 12,
        "content": "The question has no mention of ANALYZE.. BQ is not correct. I would go with D."
      },
      {
        "date": "2022-01-15T17:42:00.000Z",
        "voteCount": 1,
        "content": "There is no option for archiving with BQ"
      },
      {
        "date": "2022-04-16T08:23:00.000Z",
        "voteCount": 5,
        "content": "You dont need to archive the expiring logs, you have to archive the un-archived data here! See the question, it says \"Assuming that all expiring logs will be archived correctly\", which means they are already stored somewhere like in GCS!!! Hence, better to store the remaining un-archived data in BQ."
      },
      {
        "date": "2022-07-15T14:15:00.000Z",
        "voteCount": 3,
        "content": "The question is about where to store the _data_ for which the logs will be generated. \n\nThe bit you quoted is about the _logs_ that will be generated when accesssing data. The \u201carchived correctly\u201d implies that proper retention policies will be set up if you choose GCS."
      },
      {
        "date": "2024-02-06T22:15:00.000Z",
        "voteCount": 1,
        "content": "In recent GCP, we have cloud audit."
      },
      {
        "date": "2023-12-02T06:21:00.000Z",
        "voteCount": 1,
        "content": "Option B is valid only when analytics to be performed over logs, which is not mentioned anywhere"
      },
      {
        "date": "2023-11-30T16:38:00.000Z",
        "voteCount": 3,
        "content": "For maintaining an auditable record of access to certain types of data, especially when government regulations are in place, the most suitable option would be:\n\nB. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nStoring the data in a BigQuery dataset with restricted access ensures control over who can view the data, and utilizing Data Access logs provides a comprehensive audit trail for compliance purposes. This option aligns well with the need for maintaining an auditable record as mandated by government regulations."
      },
      {
        "date": "2023-05-03T10:53:00.000Z",
        "voteCount": 8,
        "content": "If you are going for option D, why do you eliminate option B? The only REAL difference is that for opption D you need to develop an app for storing log data and providing bucket link and in option B you have it all done BETTER by GCP. You might also pay a bit more for BQ storage, but the question never mentions about cost optimization.\nBTW in the D option the bucket is accessible only by AppEngine service, so what will the user do with the provided link? he has no access anyway... And if he even has the access to this link what stops him form using the same link many times? How the AppEngine get and store the information what specific data he accessed and how?"
      },
      {
        "date": "2023-06-05T09:34:00.000Z",
        "voteCount": 2,
        "content": "I was about to say the same thing. Why go through that stress?"
      },
      {
        "date": "2023-05-16T11:27:00.000Z",
        "voteCount": 3,
        "content": "That was my thought, either B or D could work but D it\u2019s a little bit odd create an app to do something that could be achieved natively gcp"
      },
      {
        "date": "2023-04-20T08:02:00.000Z",
        "voteCount": 2,
        "content": "D amongus"
      },
      {
        "date": "2023-03-23T12:06:00.000Z",
        "voteCount": 3,
        "content": "They want to know where you can store **data** in a way that every access is logged in an auditable way.\n\nBoth BQ and GCS have audit logs, except that in alternative D you're circumventing it by creating your own logs. I doubt Google would recommend that.\n\nBy types of data you can understand \"financial type\", \"marketing type\", etc."
      },
      {
        "date": "2023-03-01T20:27:00.000Z",
        "voteCount": 4,
        "content": "I was thinking it should be A. However, 'data' in this question is too vague. It does not say anywhere that the data could fit in BigQuery tables. It could be unstructure data such as videos or images\nOption D seems to involve more setup but it is the only viable option for this scenario. Note that GCS do have Cloud Audit logs. That should be the best option. Maybe this question was asked when Cloud Audit log is not yet available for GCS."
      },
      {
        "date": "2023-01-29T04:24:00.000Z",
        "voteCount": 1,
        "content": "It is so clear that is B lol"
      },
      {
        "date": "2023-01-28T20:31:00.000Z",
        "voteCount": 1,
        "content": "B bigquery for a record set store"
      },
      {
        "date": "2023-01-27T02:12:00.000Z",
        "voteCount": 2,
        "content": "B. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nBigQuery provides built-in logging of all data access, including the user's identity, the specific query run and the time of the query. This log can be used to provide an auditable record of access to the data. Additionally, BigQuery allows you to control access to the dataset using Identity and Access Management (IAM) roles, so you can ensure that only authorized personnel can view the dataset."
      },
      {
        "date": "2023-01-23T19:52:00.000Z",
        "voteCount": 3,
        "content": "B. In a BigQuery dataset that is viewable only by authorized personnel, with the Data Access log used to provide the auditability.\n\nBigQuery provides built-in logging of all data access, including the user's identity, the specific query run and the time of the query. This log can be used to provide an auditable record of access to the data. Additionally, BigQuery allows you to control access to the dataset using Identity and Access Management (IAM) roles, so you can ensure that only authorized personnel can view the dataset."
      },
      {
        "date": "2023-01-23T19:52:00.000Z",
        "voteCount": 2,
        "content": "A. Encrypted on Cloud Storage with user-supplied encryption keys. A separate decryption key will be given to each authorized user. is a good option for data security but it does not provide an auditable record of access to the data.\n\nC. In Cloud SQL, with separate database user names to each user. The Cloud SQL Admin activity logs will be used to provide the auditability. is also a good option for data security but it does not provide an auditable record of access to the data.\n\nD. In a bucket on Cloud Storage that is accessible only by an AppEngine service that collects user information and logs the access before providing a link to the bucket. is also a good option but it requires additional setup and maintenance of the AppEngine service, and it may not provide an auditable record of access to the data."
      },
      {
        "date": "2023-04-25T02:08:00.000Z",
        "voteCount": 2,
        "content": "gpt: You are correct that option A does not provide an auditable record of access to the data, as it only addresses data security through encryption. Option C provides auditability through Cloud SQL Admin activity logs, but it may not be the best option as it requires additional setup and management.\n\nOption D is a feasible solution, but as you mentioned, it requires additional setup and maintenance of the AppEngine service. It also may not provide a comprehensive audit log of all data access.\n\nOption B, storing the data in a BigQuery dataset that is viewable only by authorized personnel and using the Data Access log to provide auditability, is the most appropriate option as it provides built-in logging of all data access and allows you to control access to the dataset using IAM roles. Therefore, it provides both data security and auditable access to the data.   /// ok let it be B"
      },
      {
        "date": "2023-04-27T05:08:00.000Z",
        "voteCount": 1,
        "content": "OR MAYBE D...."
      },
      {
        "date": "2023-05-10T11:05:00.000Z",
        "voteCount": 1,
        "content": "!!! confused. Give 69% confidence to B, as user Jarek7 explained"
      },
      {
        "date": "2023-01-20T03:32:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      },
      {
        "date": "2023-01-11T17:46:00.000Z",
        "voteCount": 1,
        "content": "Keys\nTYPES of data --&gt; Cloud Storage not BQ \nArchival --&gt; Cloud Storage\nAccess --&gt; No decryption keys to all users"
      },
      {
        "date": "2022-12-27T22:12:00.000Z",
        "voteCount": 1,
        "content": "I will go with D"
      },
      {
        "date": "2022-12-14T19:44:00.000Z",
        "voteCount": 1,
        "content": "Keyword, Archiver , certain type of data, auditable, GCS is better option . Durability 11 time 9 to store log immutable for long time."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 77,
    "url": "https://www.examtopics.com/discussions/google/view/79343-exam-professional-data-engineer-topic-1-question-77/",
    "body": "Your neural network model is taking days to train. You want to increase the training speed. What can you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubsample your test dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubsample your training dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of input features to your model.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of layers in your neural network."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-05-10T07:58:00.000Z",
        "voteCount": 9,
        "content": "Answer: B. Subsample your training dataset.\n\nSubsampling your training dataset can help increase the training speed of your neural network model. By reducing the size of your training dataset, you can speed up the process of updating the weights in your neural network. This can help you quickly test and iterate your model to improve its accuracy.\n\nSubsampling your test dataset, on the other hand, can lead to inaccurate evaluation of your model's performance and may result in overfitting. It is important to evaluate your model's performance on a representative test dataset to ensure that it can generalize to new data.\n\nIncreasing the number of input features or layers in your neural network can also improve its performance, but this may not necessarily increase the training speed. In fact, adding more layers or features can increase the complexity of your model and make it take longer to train. It is important to balance the model's complexity with its performance and training time."
      },
      {
        "date": "2023-08-03T13:58:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2023-08-01T23:20:00.000Z",
        "voteCount": 1,
        "content": "B should be correct. Increasing the layers can also decrease the training time but may introduce vanishing gradient hence D may not be correct"
      },
      {
        "date": "2023-04-29T13:24:00.000Z",
        "voteCount": 1,
        "content": "answer is B"
      },
      {
        "date": "2023-03-23T12:15:00.000Z",
        "voteCount": 2,
        "content": "Reduce training time and probably accuracy too."
      },
      {
        "date": "2023-02-22T16:22:00.000Z",
        "voteCount": 1,
        "content": "all other are wrong"
      },
      {
        "date": "2023-01-27T02:26:00.000Z",
        "voteCount": 1,
        "content": "of course !"
      },
      {
        "date": "2023-01-23T19:57:00.000Z",
        "voteCount": 2,
        "content": "B. Subsampling your training dataset can decrease the amount of data the model needs to process and can speed up training time. However, it can lead to decrease in the model's accuracy.\n\nAlthough it shouldn't matter since we are not even in testing phase yet and we aren't looking for accuracy."
      },
      {
        "date": "2023-01-20T03:34:00.000Z",
        "voteCount": 2,
        "content": "B is the answer as we are bothered about speed not the accuracy."
      },
      {
        "date": "2023-01-08T22:10:00.000Z",
        "voteCount": 1,
        "content": "The answer is B. Building a more complex model by increasing the number of layer will not reduce the training time."
      },
      {
        "date": "2022-12-20T01:09:00.000Z",
        "voteCount": 4,
        "content": "By SubSampling the training data, you will reduce the training time. \n\nIn case of D, if you increase the number of layers, then the model's accuracy will be increased. But it will not reduce the time required to train the model."
      },
      {
        "date": "2022-12-14T19:50:00.000Z",
        "voteCount": 1,
        "content": "Increase speed of the help to train quicker.. option B is sub sample that also help but it drop accurately of model . So I think Option D is good to go."
      },
      {
        "date": "2023-02-28T18:55:00.000Z",
        "voteCount": 2,
        "content": "That makes speed of training model lower absolutely. because not only throughput of inference but back-propagation calculation would be increase so, D should be not a answer. there is only answer in those options is B. while it makes dropping performance"
      },
      {
        "date": "2022-12-06T06:19:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-10-06T18:56:00.000Z",
        "voteCount": 4,
        "content": "It is B. D would improve the accuracy, not speed."
      },
      {
        "date": "2022-09-30T17:49:00.000Z",
        "voteCount": 1,
        "content": "It's B. D Would be for increase performance"
      },
      {
        "date": "2022-09-11T03:57:00.000Z",
        "voteCount": 1,
        "content": "if you Increase the number of layers, you increase the training time, right?"
      },
      {
        "date": "2022-09-08T10:35:00.000Z",
        "voteCount": 1,
        "content": "Both B and D seems correct."
      },
      {
        "date": "2022-12-11T01:51:00.000Z",
        "voteCount": 3,
        "content": "Increasing D will increase training time"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 78,
    "url": "https://www.examtopics.com/discussions/google/view/17115-exam-professional-data-engineer-topic-1-question-78/",
    "body": "You are responsible for writing your company's ETL pipelines to run on an Apache Hadoop cluster. The pipeline will require some checkpointing and splitting pipelines. Which method should you use to write the pipelines?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPigLatin using Pig\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHiveQL using Hive",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJava using MapReduce",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPython using MapReduce"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T21:01:00.000Z",
        "voteCount": 23,
        "content": "Answer: A\nDescription: Pig is scripting language which can be used for checkpointing and splitting pipelines"
      },
      {
        "date": "2020-03-21T10:11:00.000Z",
        "voteCount": 15,
        "content": "Should be A"
      },
      {
        "date": "2023-08-21T01:38:00.000Z",
        "voteCount": 1,
        "content": "A as others have said"
      },
      {
        "date": "2023-04-25T02:16:00.000Z",
        "voteCount": 1,
        "content": "Comment content is too short"
      },
      {
        "date": "2023-03-23T13:20:00.000Z",
        "voteCount": 2,
        "content": "PigLatin is the correct answer, however... the last release was 6 years ago and has lots of bugs."
      },
      {
        "date": "2023-02-15T03:02:00.000Z",
        "voteCount": 6,
        "content": "This answer depends which language you are comfortable with. \nHadoop is your framework, where mapReduce is your Native programming model in JAVA, which is designed to scale, parallel processing, restart pipeline from any checkpoint etc. , So if you are comfortable with JAVA, you can customize your checkpoint at lowlevel in better way. otherwise, choose PIG which is another programming concept run over JAVA but then you need to learn this also, if not choose python as it can be deployed with hadoop because hadoop has been making updates for python clients regularly. \nOption C: is the best one."
      },
      {
        "date": "2023-01-26T09:53:00.000Z",
        "voteCount": 3,
        "content": "C. Java using MapReduce or D. Python using MapReduce\n\nApache Hadoop is a distributed computing framework that allows you to process large datasets using the MapReduce programming model. There are several options for writing ETL pipelines to run on a Hadoop cluster, but the most common are using Java or Python with the MapReduce programming model."
      },
      {
        "date": "2023-01-26T09:53:00.000Z",
        "voteCount": 3,
        "content": "A. PigLatin using Pig is a high-level data flow language that is used to create ETL pipelines. Pig is built on top of Hadoop, and it allows you to write scripts in PigLatin, a SQL-like language that is used to process data in Hadoop. Pig is a simpler option than MapReduce but it lacks some capabilities like the control over low-level data manipulation operations.\n\nB. HiveQL using Hive is a SQL-like language for querying and managing large datasets stored in Hadoop's distributed file system. Hive is built on top of Hadoop and it provides an SQL-like interface for querying data stored in Hadoop. Hive is more suitable for querying and managing large datasets stored in Hadoop than for ETL pipelines.\n\nBoth Java and Python using MapReduce provide low-level control over data manipulation operations, and they allow you to write custom mapper and reducer functions that can be used to process data in a Hadoop cluster. The choice between Java and Python will depend on the development team's expertise and preference."
      },
      {
        "date": "2023-05-24T05:00:00.000Z",
        "voteCount": 1,
        "content": "It has to be C \nbecause while Pig can be used to simplify the writing of complex data transformation tasks and can store intermediate results, it doesn't provide the detailed control over checkpointing and pipeline splitting in the way that is typically implied by those terms.\n\nalso, while one can write MapReduce jobs in languages other than Java (like Python) using Hadoop Streaming or other similar APIs, it may not be as efficient or as seamless as using Java due to the JVM-native nature of Hadoop."
      },
      {
        "date": "2022-09-08T21:24:00.000Z",
        "voteCount": 1,
        "content": "Description: Pig is scripting language which can be used for checkpointing and splitting pipelines"
      },
      {
        "date": "2022-02-16T02:54:00.000Z",
        "voteCount": 1,
        "content": "Why not D?"
      },
      {
        "date": "2022-01-23T13:13:00.000Z",
        "voteCount": 1,
        "content": "PigLatin supports checkpoints"
      },
      {
        "date": "2022-01-23T12:49:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2021-10-15T11:55:00.000Z",
        "voteCount": 3,
        "content": "Pig is just a scripting language, how pig can be used in creation of pipelines, should be answer from c &amp; D"
      },
      {
        "date": "2021-06-29T17:57:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      },
      {
        "date": "2021-02-17T10:21:00.000Z",
        "voteCount": 2,
        "content": "Found this slideset that puts in favor answer A (pig) :\nhttps://poloclub.github.io/cx4242-2019fall-campus/slides/17-CSE6242-612-ScalingUp-hive.pdf"
      },
      {
        "date": "2020-09-16T07:34:00.000Z",
        "voteCount": 10,
        "content": "Is this really a question that could appear in Google Cloud Professional Data Engineer Exam? What does it have to do with Google Cloud? I would use DataProc no?"
      },
      {
        "date": "2020-10-24T11:07:00.000Z",
        "voteCount": 1,
        "content": "Did you take the exam? I am ready to do it this month"
      },
      {
        "date": "2021-12-22T23:46:00.000Z",
        "voteCount": 2,
        "content": "seems like a very old question :)\nnot sure it's actual"
      },
      {
        "date": "2020-08-19T17:47:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2020-08-18T02:25:00.000Z",
        "voteCount": 2,
        "content": "A should be correct answer here."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 79,
    "url": "https://www.examtopics.com/discussions/google/view/17118-exam-professional-data-engineer-topic-1-question-79/",
    "body": "Your company maintains a hybrid deployment with GCP, where analytics are performed on your anonymized customer data. The data are imported to Cloud<br>Storage from your data center through parallel uploads to a data transfer server running on GCP. Management informs you that the daily transfers take too long and have asked you to fix the problem. You want to maximize transfer speeds. Which action should you take?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the CPU size on your server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the Google Persistent Disk on your server.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease your network bandwidth from your datacenter to GCP.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease your network bandwidth from Compute Engine to Cloud Storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T10:17:00.000Z",
        "voteCount": 16,
        "content": "correct: C"
      },
      {
        "date": "2020-03-27T21:04:00.000Z",
        "voteCount": 15,
        "content": "Answer: C\nDescription : Speed of data transfer depends on Bandwidth \nFew things in computing highlight the hardware limitations of networks as transferring large amounts of data. Typically you can transfer 1 GB in eight seconds over a 1 Gbps network. If you scale that up to a huge dataset (for example, 100 TB), the transfer time is 12 days. Transferring huge datasets can test the limits of your infrastructure and potentially cause problems for your business."
      },
      {
        "date": "2024-05-21T02:47:00.000Z",
        "voteCount": 1,
        "content": "There is a test in google's own Professional Data Engineer training https://docs.google.com/forms/d/e/1FAIpQLSeEeHoJ-u8MuvEBw8xd8TKIZMo5wx4JqQv_bVo-5BKn_3mZIw/viewform \nit has this question, and the answer is A according to them."
      },
      {
        "date": "2023-07-23T03:28:00.000Z",
        "voteCount": 1,
        "content": "We are talking about transfer speed. Network transfer speed does not increase with CPU, but with bandwidth. Since there is no other extra information about what the issue, we have to assume that they imply network transfer speed."
      },
      {
        "date": "2023-05-16T11:36:00.000Z",
        "voteCount": 3,
        "content": "To be honest this question is incomplete, I would go increasing the bandwidth, but first I would analyze why it\u2019s taking long time maybe I\u2019m uploading many files so I could compress and agregate then and upload just one,  maybe the target cpu is overloaded at the time of the upload, maybe the target disk reaching the max iops,"
      },
      {
        "date": "2023-05-03T11:13:00.000Z",
        "voteCount": 1,
        "content": "Even if transfer server  is deployed on the slowest machine available in GCP there is no way it is bottleneck for simple data transfer without any data processing."
      },
      {
        "date": "2023-04-27T05:15:00.000Z",
        "voteCount": 1,
        "content": "GPT:  Option A, increasing the CPU size on the data transfer server, could potentially increase the transfer speeds if the bottleneck in the data transfer process is the processing power of the server. By increasing the CPU size, the server may be able to process data more quickly, leading to faster transfers. \nOption C, increasing the network bandwidth from the datacenter to GCP, could potentially improve the transfer speeds, but it may not be feasible or cost-effective depending on the current infrastructure and network limitations."
      },
      {
        "date": "2023-05-03T11:14:00.000Z",
        "voteCount": 4,
        "content": "Please stop using GPT as knowledge source. v3.5 is usually wrong even in simple cases. v4 is much better, but it is not designed to be knowledge source. Looking at the answer you must have used v3.5. The question says nothing about cost-effectivness. The issue is data transfer. No any data processing is done on the data while it is transferred. Simple transfer doesn't need much processing power - the real bottleneck even on slowest machines available on GCP must be data transfer - it is obvious.\nBTW for me GPT3.5 said it is C."
      },
      {
        "date": "2023-05-05T03:54:00.000Z",
        "voteCount": 1,
        "content": "yea, i know it can make mistakes. Thank you.\nThat`s why i always mark \"GPT\" at the start of my answer."
      },
      {
        "date": "2023-05-05T03:59:00.000Z",
        "voteCount": 1,
        "content": "it  should be C, for real, bcz nothing said about cost restrictions in the question. And the user \"snamburi3 \" found docs."
      },
      {
        "date": "2023-04-24T17:14:00.000Z",
        "voteCount": 1,
        "content": "it's refer to data transfer server slow here. not transfer data to cloud slow. \n100% A"
      },
      {
        "date": "2023-03-04T07:27:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-02-28T19:08:00.000Z",
        "voteCount": 1,
        "content": "This question makes people confused only. there is no refer to network or size of data or something could be referred. the answer could be A or C"
      },
      {
        "date": "2023-02-25T12:19:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2023-02-21T05:31:00.000Z",
        "voteCount": 1,
        "content": "Very confusing question. I selected A because I assume increasing the CPU size on the cloud server is easier to change, as a data engineer, than the bandwidth."
      },
      {
        "date": "2023-01-27T08:57:00.000Z",
        "voteCount": 3,
        "content": "C. Increase your network bandwidth from your datacenter to GCP.\nThis will likely have the most impact on transfer speeds as it addresses the bottleneck in the transfer between your data center and GCP. Increasing the CPU size or the size of the Google Persistent Disk on the server may help with processing the data once it has been transferred, but will not address the bottleneck in the transfer itself. Increasing the network bandwidth from Compute Engine to Cloud Storage would also help with processing the data once it has been transferred but will not address the bottleneck in the transfer itself as well."
      },
      {
        "date": "2022-12-06T05:41:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-10-12T00:31:00.000Z",
        "voteCount": 1,
        "content": "A bit unprofessional question, having performance issues  should be addressed by analyzing and looking for saturation in the system  and understanding \"wait-events\". Only than adding more resources."
      },
      {
        "date": "2022-06-29T09:25:00.000Z",
        "voteCount": 3,
        "content": "\"The data are imported to Cloud Storage from your data center through parallel uploads to a data transfer server running on GCP. \"\n\nThis makes zero sense. Is it to GCS or GCE? Question had to make up its mind. Nonsense, literally."
      },
      {
        "date": "2021-11-13T01:59:00.000Z",
        "voteCount": 1,
        "content": "Vote for C. Mostly because other options seems useless"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 80,
    "url": "https://www.examtopics.com/discussions/google/view/17119-exam-professional-data-engineer-topic-1-question-80/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>Ensure secure and efficient transport and storage of telemetry data<br>Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>MJTelco is building a custom interface to share data. They have these requirements:<br>1. They need to do aggregations over their petabyte-scale datasets.<br>2. They need to scan specific time range rows with a very fast response time (milliseconds).<br>Which combination of Google Cloud Platform products should you recommend?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore and Cloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable and Cloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery and Cloud Bigtable\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery and Cloud Storage"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-21T10:19:00.000Z",
        "voteCount": 19,
        "content": "correct: C"
      },
      {
        "date": "2020-07-30T23:17:00.000Z",
        "voteCount": 9,
        "content": "C\nBigquery and Big table =PB storage capacity \nBigtable=to read scan rows Big query select row to read"
      },
      {
        "date": "2023-06-18T11:03:00.000Z",
        "voteCount": 1,
        "content": "Response C =&gt; Bigquery and bigtable"
      },
      {
        "date": "2023-04-30T11:45:00.000Z",
        "voteCount": 4,
        "content": "Why not A? If we're already using Bigtable, what's the use of another, slower analytic solution, like BigQuery? Wouldn't Datastore be more useful to store our data than BigQuery?"
      },
      {
        "date": "2023-01-21T14:55:00.000Z",
        "voteCount": 1,
        "content": "bigquery and bigtable"
      },
      {
        "date": "2021-08-08T12:01:00.000Z",
        "voteCount": 1,
        "content": "C is correct, no doubt"
      },
      {
        "date": "2021-06-29T18:12:00.000Z",
        "voteCount": 2,
        "content": "Vote for C"
      },
      {
        "date": "2021-03-11T10:05:00.000Z",
        "voteCount": 6,
        "content": "C:\nThey need to do aggregations over their petabyte-scale datasets: Bigquery\nThey need to scan specific time range rows with a very fast response time (milliseconds): Bigtable"
      },
      {
        "date": "2020-10-30T13:26:00.000Z",
        "voteCount": 1,
        "content": "Why not D? Biqquery and GCS.\nAlso Big Table is no sql, where as BQ is SQL"
      },
      {
        "date": "2020-12-21T08:39:00.000Z",
        "voteCount": 5,
        "content": "With GCS you can only scan the rows from BigQuery using External federated Datasources, with that millisecond latency is not possible. Also \"scan specific time range rows with a very fast response time\" is a natural fit use case for Cloud Bigtable."
      },
      {
        "date": "2020-08-19T17:49:00.000Z",
        "voteCount": 4,
        "content": "C is correct."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 81,
    "url": "https://www.examtopics.com/discussions/google/view/17264-exam-professional-data-engineer-topic-1-question-81/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br><img src=\"/assets/media/exam-media/04341/0006100002.png\" class=\"in-exam-image\"><br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>Ensure secure and efficient transport and storage of telemetry data<br>Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>You need to compose visualization for operations teams with the following requirements:<br>\u2711 Telemetry must include data from all 50,000 installations for the most recent 6 weeks (sampling once every minute)<br>\u2711 The report must not be more than 3 hours delayed from live data.<br>\u2711 The actionable report should only show suboptimal links.<br>\u2711 Most suboptimal links should be sorted to the top.<br>Suboptimal links can be grouped and filtered by regional geography.<br><img src=\"/assets/media/exam-media/04341/0006200009.png\" class=\"in-exam-image\"><br>\u2711 User response time to load the report must be &lt;5 seconds.<br>You create a data source to store the last 6 weeks of data, and create visualizations that allow viewers to see multiple date ranges, distinct geographic regions, and unique installation types. You always show the latest data without any changes to your visualizations. You want to avoid creating and updating new visualizations each month. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLook through the current data and compose a series of charts and tables, one for each possible combination of criteria.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLook through the current data and compose a small set of generalized charts and tables bound to criteria filters that allow value selection.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data to a spreadsheet, compose a series of charts and tables, one for each possible combination of criteria, and spread them across multiple tabs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into relational database tables, write a Google App Engine application that queries all rows, summarizes the data across each criteria, and then renders results using the Google Charts and visualization API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T10:19:00.000Z",
        "voteCount": 30,
        "content": "Should be B"
      },
      {
        "date": "2023-05-03T11:46:00.000Z",
        "voteCount": 7,
        "content": "First I thought B, as D seems too complex with writing app for AppEngine. But B is too simple - just look through the data doesnt seem right.\nIt must be very old question. Today you would load the data to BQ, optionally you can use Dataprep for simple data cleaning or a Dataflow job for more complex data processing, and finally use Looker to create tables and charts."
      },
      {
        "date": "2023-05-05T04:12:00.000Z",
        "voteCount": 3,
        "content": "As per old question - must be. I heard, that the exam will mostly have questions rather from 100 to 205 than form 1 to 100. And smb told me, that the other w/s gave questions, that happened more often in exam, in comparison to questions given here"
      },
      {
        "date": "2023-10-05T02:19:00.000Z",
        "voteCount": 1,
        "content": "bound to criteria filters that allow value selection.   - Simple and Smart."
      },
      {
        "date": "2023-01-27T03:13:00.000Z",
        "voteCount": 1,
        "content": "D.\neverything is fixed except data that is updated regularly in order to keep the last 6 weeks. Then, the pipeline does not change ==&gt; obtaining (same) charts and viz on regularly updated data"
      },
      {
        "date": "2022-12-01T06:35:00.000Z",
        "voteCount": 3,
        "content": "B \nBut can someone explain the question and selection clearly?"
      },
      {
        "date": "2022-11-06T11:45:00.000Z",
        "voteCount": 4,
        "content": "It's B. All the other choices are unreasonable."
      },
      {
        "date": "2022-11-05T18:18:00.000Z",
        "voteCount": 1,
        "content": "ACD-Design for each possible combination of criteria, so if your team has new requirements, you must design new charts.\nSo, answer shoud be B."
      },
      {
        "date": "2022-08-25T01:00:00.000Z",
        "voteCount": 2,
        "content": "the key is \" You want to avoid creating and updating new visualizations each month.\"\nonly D work for that phrase"
      },
      {
        "date": "2022-12-24T00:20:00.000Z",
        "voteCount": 1,
        "content": "D you might need to load data from source to table for each month. It stated the source will keep last 6 weeks data, but not in D"
      },
      {
        "date": "2022-07-01T04:55:00.000Z",
        "voteCount": 1,
        "content": "must be D"
      },
      {
        "date": "2022-06-29T18:49:00.000Z",
        "voteCount": 2,
        "content": "The answer is B"
      },
      {
        "date": "2022-06-29T09:27:00.000Z",
        "voteCount": 3,
        "content": "This Q feels very disconnected from GCP products....."
      },
      {
        "date": "2022-05-12T00:22:00.000Z",
        "voteCount": 4,
        "content": "Vote D.\nSince B just uses \"current data\", which means if new data enters, you need to re-run those charts again."
      },
      {
        "date": "2022-12-24T00:18:00.000Z",
        "voteCount": 1,
        "content": "But q says the data sources only have latest 6 weeks data, so current data means latest?"
      },
      {
        "date": "2022-02-18T22:07:00.000Z",
        "voteCount": 1,
        "content": "B is optimal to avoid creating and updating new visualizations each month"
      },
      {
        "date": "2021-10-08T21:24:00.000Z",
        "voteCount": 4,
        "content": "Answer D: Data in SQL so querying becomes easier on any pattern. create mutiple charts, graphs to fulfill your requirements."
      },
      {
        "date": "2021-08-08T12:06:00.000Z",
        "voteCount": 2,
        "content": "Yes it's B"
      },
      {
        "date": "2021-06-30T15:14:00.000Z",
        "voteCount": 1,
        "content": "vote for B"
      },
      {
        "date": "2020-10-25T11:57:00.000Z",
        "voteCount": 4,
        "content": "Yes agreed. B seems to be the only reasonable choice here."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 82,
    "url": "https://www.examtopics.com/discussions/google/view/17263-exam-professional-data-engineer-topic-1-question-82/",
    "body": "MJTelco Case Study -<br><br>Company Overview -<br>MJTelco is a startup that plans to build networks in rapidly growing, underserved markets around the world. The company has patents for innovative optical communications hardware. Based on these patents, they can create many reliable, high-speed backbone links with inexpensive hardware.<br><br>Company Background -<br>Founded by experienced telecom executives, MJTelco uses technologies originally developed to overcome communications challenges in space. Fundamental to their operation, they need to create a distributed data infrastructure that drives real-time analysis and incorporates machine learning to continuously optimize their topologies. Because their hardware is inexpensive, they plan to overdeploy the network allowing them to account for the impact of dynamic regional politics on location availability and cost.<br>Their management and operations teams are situated all around the globe creating many-to-many relationship between data consumers and provides in their system. After careful consideration, they decided public cloud is the perfect environment to support their needs.<br><br>Solution Concept -<br>MJTelco is running a successful proof-of-concept (PoC) project in its labs. They have two primary needs:<br>\u2711 Scale and harden their PoC to support significantly more data flows generated when they ramp to more than 50,000 installations.<br>\u2711 Refine their machine-learning cycles to verify and improve the dynamic models they use to control topology definition.<br>MJTelco will also use three separate operating environments `\" development/test, staging, and production `\" to meet the needs of running experiments, deploying new features, and serving production customers.<br><br>Business Requirements -<br>\u2711 Scale up their production environment with minimal cost, instantiating resources when and where needed in an unpredictable, distributed telecom user community.<br>\u2711 Ensure security of their proprietary data to protect their leading-edge machine learning and analysis.<br>\u2711 Provide reliable and timely access to data for analysis from distributed research workers<br>\u2711 Maintain isolated environments that support rapid iteration of their machine-learning models without affecting their customers.<br><br>Technical Requirements -<br>Ensure secure and efficient transport and storage of telemetry data<br>Rapidly scale instances to support between 10,000 and 100,000 data providers with multiple flows each.<br>Allow analysis and presentation against data tables tracking up to 2 years of data storing approximately 100m records/day<br>Support rapid iteration of monitoring infrastructure focused on awareness of data pipeline problems both in telemetry flows and in production learning cycles.<br><br>CEO Statement -<br>Our business model relies on our patents, analytics and dynamic machine learning. Our inexpensive hardware is organized to be highly reliable, which gives us cost advantages. We need to quickly stabilize our large distributed data pipelines to meet our reliability and capacity commitments.<br><br>CTO Statement -<br>Our public cloud services must operate as advertised. We need resources that scale and keep our data secure. We also need environments in which our data scientists can carefully study and quickly adapt our models. Because we rely on automation to process our data, we also need our development and test environments to work as we iterate.<br><br>CFO Statement -<br>The project is too large for us to maintain the hardware and software required for the data and analysis. Also, we cannot afford to staff an operations team to monitor so many data feeds, so we will rely on automation and infrastructure. Google Cloud's machine learning will allow our quantitative researchers to work on our high-value problems instead of problems with our data pipelines.<br>Given the record streams MJTelco is interested in ingesting per day, they are concerned about the cost of Google BigQuery increasing. MJTelco asks you to provide a design solution. They require a single large data table called tracking_table. Additionally, they want to minimize the cost of daily queries while performing fine-grained analysis of each day's events. They also want to use streaming ingestion. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table called tracking_table and include a DATE column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a partitioned table called tracking_table and include a TIMESTAMP column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate sharded tables for each day following the pattern tracking_table_YYYYMMDD.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table called tracking_table with a TIMESTAMP column to represent the day."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T10:17:00.000Z",
        "voteCount": 19,
        "content": "Correct - B"
      },
      {
        "date": "2023-07-10T19:49:00.000Z",
        "voteCount": 1,
        "content": "B, Partition tables in BQ have different cost. If a partition is not modified (DML) for 90 days then cost will be less by 50%, while querying will be efficient since its single large table."
      },
      {
        "date": "2022-11-23T06:43:00.000Z",
        "voteCount": 1,
        "content": "always partion large tables"
      },
      {
        "date": "2021-11-13T02:18:00.000Z",
        "voteCount": 3,
        "content": "B for sure"
      },
      {
        "date": "2021-09-15T18:57:00.000Z",
        "voteCount": 3,
        "content": "Correct is B"
      },
      {
        "date": "2021-08-08T12:08:00.000Z",
        "voteCount": 2,
        "content": "Option B for sure"
      },
      {
        "date": "2021-07-05T09:39:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard - Supports B"
      },
      {
        "date": "2021-06-30T15:33:00.000Z",
        "voteCount": 1,
        "content": "Vote for 'B' Partitioned Table for Faster Query and Low cost (because it will process less data)"
      },
      {
        "date": "2021-03-25T22:09:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2021-01-02T09:39:00.000Z",
        "voteCount": 2,
        "content": "Correct - B"
      },
      {
        "date": "2020-12-10T01:58:00.000Z",
        "voteCount": 1,
        "content": "should be C"
      },
      {
        "date": "2021-01-18T18:16:00.000Z",
        "voteCount": 3,
        "content": "They're using BigQuery so partitioning is the better choice here. B"
      },
      {
        "date": "2020-08-20T10:46:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 83,
    "url": "https://www.examtopics.com/discussions/google/view/17262-exam-professional-data-engineer-topic-1-question-83/",
    "body": "Flowlogistic Case Study -<br><br>Company Overview -<br>Flowlogistic is a leading logistics and supply chain provider. They help businesses throughout the world manage their resources and transport them to their final destination. The company has grown rapidly, expanding their offerings to include rail, truck, aircraft, and oceanic shipping.<br><br>Company Background -<br>The company started as a regional trucking company, and then expanded into other logistics market. Because they have not updated their infrastructure, managing and tracking orders and shipments has become a bottleneck. To improve operations, Flowlogistic developed proprietary technology for tracking shipments in real time at the parcel level. However, they are unable to deploy it because their technology stack, based on Apache Kafka, cannot support the processing volume. In addition, Flowlogistic wants to further analyze their orders and shipments to determine how best to deploy their resources.<br><br>Solution Concept -<br>Flowlogistic wants to implement two concepts using the cloud:<br>\u2711 Use their proprietary technology in a real-time inventory-tracking system that indicates the location of their loads<br>\u2711 Perform analytics on all their orders and shipment logs, which contain both structured and unstructured data, to determine how best to deploy resources, which markets to expand info. They also want to use predictive analytics to learn earlier when a shipment will be delayed.<br><br>Existing Technical Environment -<br>Flowlogistic architecture resides in a single data center:<br>\u2711 Databases<br>- 8 physical servers in 2 clusters<br>- SQL Server `\" user data, inventory, static data<br>- 3 physical servers<br>- Cassandra `\" metadata, tracking messages<br>10 Kafka servers `\" tracking message aggregation and batch insert<br>\u2711 Application servers `\" customer front end, middleware for order/customs<br>- 60 virtual machines across 20 physical servers<br>- Tomcat `\" Java services<br>- Nginx `\" static content<br>- Batch servers<br>\u2711 Storage appliances<br>- iSCSI for virtual machine (VM) hosts<br>- Fibre Channel storage area network (FC SAN) `\" SQL server storage<br>Network-attached storage (NAS) image storage, logs, backups<br>\u2711 10 Apache Hadoop /Spark servers<br>- Core Data Lake<br>- Data analysis workloads<br>\u2711 20 miscellaneous servers<br>- Jenkins, monitoring, bastion hosts,<br><br>Business Requirements -<br>\u2711 Build a reliable and reproducible environment with scaled panty of production.<br>\u2711 Aggregate data in a centralized Data Lake for analysis<br>\u2711 Use historical data to perform predictive analytics on future shipments<br>\u2711 Accurately track every shipment worldwide using proprietary technology<br>\u2711 Improve business agility and speed of innovation through rapid provisioning of new resources<br>\u2711 Analyze and optimize architecture for performance in the cloud<br>\u2711 Migrate fully to the cloud if all other requirements are met<br><br>Technical Requirements -<br>\u2711 Handle both streaming and batch data<br>\u2711 Migrate existing Hadoop workloads<br>\u2711 Ensure architecture is scalable and elastic to meet the changing demands of the company.<br>\u2711 Use managed services whenever possible<br>\u2711 Encrypt data flight and at rest<br>Connect a VPN between the production data center and cloud environment<br><br>SEO Statement -<br>We have grown so quickly that our inability to upgrade our infrastructure is really hampering further growth and efficiency. We are efficient at moving shipments around the world, but we are inefficient at moving data around.<br>We need to organize our information so we can more easily understand where our customers are and what they are shipping.<br><br>CTO Statement -<br>IT has never been a priority for us, so as our data has grown, we have not invested enough in our technology. I have a good staff to manage IT, but they are so busy managing our infrastructure that I cannot get them to do the things that really matter, such as organizing our data, building the analytics, and figuring out how to implement the CFO' s tracking technology.<br><br>CFO Statement -<br>Part of our competitive advantage is that we penalize ourselves for late shipments and deliveries. Knowing where out shipments are at all times has a direct correlation to our bottom line and profitability. Additionally, I don't want to commit capital to building out a server environment.<br>Flowlogistic's management has determined that the current Apache Kafka servers cannot handle the data volume for their real-time inventory tracking system.<br>You need to build a new system on Google Cloud Platform (GCP) that will feed the proprietary tracking software. The system must be able to ingest data from a variety of global sources, process and query in real-time, and store the data reliably. Which combination of GCP products should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, and Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataflow, and Local SSD",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud SQL, and Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Load Balancing, Cloud Dataflow, and Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataflow, Cloud SQL, and Cloud Storage"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-24T03:34:00.000Z",
        "voteCount": 25,
        "content": "Seems like A..Data should ingest from multiple sources which might be real time or batch ."
      },
      {
        "date": "2021-07-06T23:39:00.000Z",
        "voteCount": 3,
        "content": "How is it possible to query in real time with option A. It needs Dataflow"
      },
      {
        "date": "2021-07-06T23:40:00.000Z",
        "voteCount": 2,
        "content": "To use Dataflow SQL it needs BigQuery"
      },
      {
        "date": "2020-09-14T14:14:00.000Z",
        "voteCount": 11,
        "content": "Repeated Question see ques 35"
      },
      {
        "date": "2021-07-05T09:47:00.000Z",
        "voteCount": 1,
        "content": "These exams make people over analyse. People who vote A earlier in 35 seem to be confused here.. haha"
      },
      {
        "date": "2021-01-12T06:08:00.000Z",
        "voteCount": 2,
        "content": "Well Done mikey007, Many people have already answered as A."
      },
      {
        "date": "2022-12-27T07:17:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-11-06T11:47:00.000Z",
        "voteCount": 1,
        "content": "It's A"
      },
      {
        "date": "2022-08-29T16:19:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2022-02-18T22:13:00.000Z",
        "voteCount": 2,
        "content": "ingest data from a variety of global sources - cloud pub/sub\nprocess and query in real-time - cloud Dataflow\nstore the data reliably -  Cloud Storage"
      },
      {
        "date": "2022-01-06T11:27:00.000Z",
        "voteCount": 1,
        "content": "PubSub (for global ingestion from multiple sources) + Dataflow (for process and query) + reliable (gcs)."
      },
      {
        "date": "2021-11-21T22:02:00.000Z",
        "voteCount": 1,
        "content": "using Dataflow you can apply the propriety analytics and you can push the data in to Cloud storage"
      },
      {
        "date": "2021-10-22T11:59:00.000Z",
        "voteCount": 1,
        "content": "Also read the technical requirements section. Not just the last 3 lines of the question. \n\nWhen you do that, you'll know the answer is PubSub (for global ingestion) + Dataflow (for process and query) + reliable (gcs).\n\nAnswer is: A"
      },
      {
        "date": "2021-10-08T21:33:00.000Z",
        "voteCount": 1,
        "content": "Answer C:\nLook the 3 requirement in the question \"ingest data from a variety of global sources, process and query in real-time, and store the data reliably\"\nIngest data from global sources: Pub-Sub\nProcess  and Query in realtime: Cloud SQL\nStore reliably: Cloud storage\nI can understand Databflow is required in case you need to analyze and transform data but question does not refer it."
      },
      {
        "date": "2022-06-21T20:31:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL, is not suitable and efficient for storing real time data ingested from PUB/SUB, so A is the answer"
      },
      {
        "date": "2021-09-15T19:04:00.000Z",
        "voteCount": 1,
        "content": "Correct is A.  \nKafka --&gt; replace  by PubSub, Streaming then Dataflow, store data reliably and not mention any other condition then Cloud Storage"
      },
      {
        "date": "2021-06-30T15:46:00.000Z",
        "voteCount": 2,
        "content": "Vote for 'A'\n\nSQL - will not handle the volume"
      },
      {
        "date": "2021-03-11T10:34:00.000Z",
        "voteCount": 1,
        "content": "Dataflow SQL cannot output to cloud storage:\nhttps://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations\nbut the main problem is that Cloud SQL can't do process, then response is A or C."
      },
      {
        "date": "2020-09-29T01:57:00.000Z",
        "voteCount": 4,
        "content": "A\nI don't expect this question to come up, but if I had to write the answer, it would be A.\nThe problem statement \"Flowlogistic's management has determined that the current Apache Kafka servers cannot handle the\ndata volume for their real-time inventory tracking system.\nAs it says, \"we cannot determine the data volume\", but it doesn't say that we can't calculate it either.\n\nRequirement definition: The system must be able to\ningest data from a variety of global sources\nprocess and query in real-time\nStore the data reliably. \n\nIt says above, if you look at the Google page.\n\nLogging to multiple systems. for example, a Google Compute Engine instance can write logs to a monitoring system, to a database for later querying, and so on.\nhttps://cloud.google.com/pubsub/docs/overview#scenarios\n\nstream processing with Dataflow\nhttps://cloud.google.com/pubsub/docs/pubsub-dataflow?hl=en-419\n\nThe answer is A, since it is stated above."
      },
      {
        "date": "2020-09-20T09:00:00.000Z",
        "voteCount": 3,
        "content": "A.  SQL queries can be written in Dataflow too.\nhttps://cloud.google.com/dataflow/docs/guides/sql/dataflow-sql-intro#running-queries"
      },
      {
        "date": "2020-10-09T19:50:00.000Z",
        "voteCount": 2,
        "content": "Dataflow SQL cannot output to cloud storage only BigQuery...so I am confused on this one."
      },
      {
        "date": "2021-03-01T07:55:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/pubsub/docs/pubsub-dataflow... It is possible to load the data to Cloud Storage. Can refer to above docs."
      },
      {
        "date": "2021-03-11T10:29:00.000Z",
        "voteCount": 1,
        "content": "he said correct, DataflowDataflow SQL cannot output to cloud storage:\nhttps://cloud.google.com/dataflow/docs/guides/sql/data-sources-destinations"
      },
      {
        "date": "2021-09-06T10:07:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C then?"
      },
      {
        "date": "2020-09-10T09:54:00.000Z",
        "voteCount": 1,
        "content": "should be E"
      },
      {
        "date": "2020-09-07T20:38:00.000Z",
        "voteCount": 2,
        "content": "Should be A ...data need to feed to the propriority system and for that dataflow is required."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 84,
    "url": "https://www.examtopics.com/discussions/google/view/16835-exam-professional-data-engineer-topic-1-question-84/",
    "body": "After migrating ETL jobs to run on BigQuery, you need to verify that the output of the migrated jobs is the same as the output of the original. You've loaded a table containing the output of the original job and want to compare the contents with output from the migrated job to show that they are identical. The tables do not contain a primary key column that would enable you to join them together for comparison.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect random samples from the tables using the RAND() function and compare the samples.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSelect random samples from the tables using the HASH() function and compare the samples.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate stratified random samples using the OVER() function and compare equivalent samples from each table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T00:31:00.000Z",
        "voteCount": 33,
        "content": "C is the only way which all records will be compared."
      },
      {
        "date": "2022-12-07T05:10:00.000Z",
        "voteCount": 2,
        "content": "Agree with your argument"
      },
      {
        "date": "2020-03-27T21:10:00.000Z",
        "voteCount": 16,
        "content": "Answer: C\nDescription: Full comparison with this option, rest are comparison on sample which doesnot ensure all the data will be ok"
      },
      {
        "date": "2023-03-01T23:45:00.000Z",
        "voteCount": 3,
        "content": "In practice, I will do B. That means it may have error due to randomness. But that is how we normally do validation/QA in general, i.e. we test random samples\n\nIn this question, I will do C."
      },
      {
        "date": "2023-02-15T04:02:00.000Z",
        "voteCount": 1,
        "content": "key words here- Hash or collect value on \"EACH table\", after sorting the table. \nOption C"
      },
      {
        "date": "2023-01-29T17:04:00.000Z",
        "voteCount": 1,
        "content": "C. Use a Dataproc cluster and the BigQuery Hadoop connector to read the data from each table and calculate a hash from non-timestamp columns of the table after sorting. Compare the hashes of each table. This approach will ensure that the data is read in a consistent order, and the hash function will provide a quick and efficient way to compare the contents of the tables and ensure that they are identical."
      },
      {
        "date": "2023-01-29T17:23:00.000Z",
        "voteCount": 2,
        "content": "A. Selecting random samples from the tables using the RAND() function may not provide an accurate representation of the data and there is a risk that the comparison will not identify any differences between the tables.\n\nB. Selecting random samples from the tables using the HASH() function may not be an effective method for comparison, as the HASH() function may return different results for equivalent data.\n\nD. Creating stratified random samples using the OVER() function may not provide a comprehensive comparison between the tables as there is a risk that important differences could be missed in the sample data."
      },
      {
        "date": "2022-12-06T05:39:00.000Z",
        "voteCount": 2,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-07T05:11:00.000Z",
        "voteCount": 1,
        "content": "All records need to be checked to be sure, so C is the answer"
      },
      {
        "date": "2022-11-26T11:46:00.000Z",
        "voteCount": 1,
        "content": "All records"
      },
      {
        "date": "2022-10-10T03:22:00.000Z",
        "voteCount": 1,
        "content": "B is the only way which all records will be compared."
      },
      {
        "date": "2022-11-06T11:50:00.000Z",
        "voteCount": 2,
        "content": "You must have meant to say C"
      },
      {
        "date": "2022-01-06T11:29:00.000Z",
        "voteCount": 1,
        "content": "HASH() to compare data skipping dates and timestamps"
      },
      {
        "date": "2022-04-29T07:28:00.000Z",
        "voteCount": 1,
        "content": "The hash in answer C is used to select a sample of the table, not to compare them"
      },
      {
        "date": "2022-04-29T07:31:00.000Z",
        "voteCount": 1,
        "content": "Ignore my comment, it was about answer B.\nI suggest you to go with answer C which is the only solution comparing all the rows/tables"
      },
      {
        "date": "2021-12-23T00:02:00.000Z",
        "voteCount": 2,
        "content": "options A B and D only will determine that it \u201cmight\u201d be identical since is only a sample. HASH() can be helpful when doing bulk comparisons, but you still have to compare field by field to get the final answer.\nThe only one left is C which looks good to me"
      },
      {
        "date": "2021-11-08T12:23:00.000Z",
        "voteCount": 1,
        "content": "C. \nThe rest use RAND() at some point, which makes it hard to compare for consistency, unless there's a 'seed' option, which wasn't mentioned. So C."
      },
      {
        "date": "2021-10-07T07:20:00.000Z",
        "voteCount": 3,
        "content": "Since there is no PK and it is possible that set of values is commons in some records which result in same hashkey for those records. But still Anwer is C"
      },
      {
        "date": "2021-06-30T15:52:00.000Z",
        "voteCount": 1,
        "content": "Vote for 'C\""
      },
      {
        "date": "2021-02-16T16:47:00.000Z",
        "voteCount": 3,
        "content": "B:\nBecause said migrated to BigQuery, then we don't need Dataproc, and samples don't mean you don't compare all of data."
      },
      {
        "date": "2021-09-18T08:24:00.000Z",
        "voteCount": 1,
        "content": "a sample is a subset of data. then you should assure that the union of the samples contain the data set. Excessively complicated.\nYou migrate to BigQuery but need to check BigQuery output, that is why you should use another tool, Dataproc in this case.\nAgree that then you should control Dataproc output but suppositions are becoming too many."
      },
      {
        "date": "2020-08-21T20:12:00.000Z",
        "voteCount": 3,
        "content": "C\nUsing Cloud Storage with big data\n\nCloud Storage is a key part of storing and working with Big Data on Google Cloud. Examples include:\n\n    Loading data into BigQuery.\n\n    Using Dataproc, which automatically installs the HDFS-compatible Cloud Storage connector, enabling the use of Cloud Storage buckets in parallel with HDFS.\n\n    Using a bucket to hold staging files and temporary data for Dataflow pipelines.\n\nFor Dataflow, a Cloud Storage bucket is required. For BigQuery and Dataproc, using a Cloud Storage bucket is optional but recommended.\n\ngsutil is a command-line tool that enables you to work with Cloud Storage buckets and objects easily and robustly, in particular in big data scenarios. For example, with gsutil you can copy many files in parallel with a single command, copy large files efficiently, calculate checksums on your data, and measure performance from your local computer to Cloud Storage."
      },
      {
        "date": "2020-08-20T10:51:00.000Z",
        "voteCount": 4,
        "content": "C is correct"
      },
      {
        "date": "2020-08-20T10:53:00.000Z",
        "voteCount": 3,
        "content": "It Says: \"...that they are identical.\" , You must not use sample."
      },
      {
        "date": "2020-07-05T07:20:00.000Z",
        "voteCount": 4,
        "content": "C is correct."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 85,
    "url": "https://www.examtopics.com/discussions/google/view/83115-exam-professional-data-engineer-topic-1-question-85/",
    "body": "You are a head of BI at a large enterprise company with multiple business units that each have different priorities and budgets. You use on-demand pricing for<br>BigQuery with a quota of 2K concurrent on-demand slots per project. Users at your organization sometimes don't get slots to execute their query and you need to correct this. You'd like to avoid introducing new projects to your account.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert your batch BQ queries into interactive BQ queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an additional project to overcome the 2K on-demand per-project quota.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to flat-rate pricing and establish a hierarchical priority model for your projects.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the amount of concurrent slots per project at the Quotas page at the Cloud Console."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-21T10:17:00.000Z",
        "voteCount": 8,
        "content": "I got this question on sept 2022. Answer is C"
      },
      {
        "date": "2023-04-29T13:37:00.000Z",
        "voteCount": 1,
        "content": "answer us C"
      },
      {
        "date": "2023-03-22T05:44:00.000Z",
        "voteCount": 1,
        "content": "This question is interesting.\nMy friend works as the TAM in Google and he said we could request for Quota increase if the customer is premium customer instead of changing to Flat-rate\nOtherwise, need to choose C"
      },
      {
        "date": "2023-03-01T00:20:00.000Z",
        "voteCount": 1,
        "content": "why A is not a answer? when using interactive bigquery without batch bigquery it lead to run query immediately isn't it? so it seems to solve the problems isn't it?"
      },
      {
        "date": "2023-01-31T12:42:00.000Z",
        "voteCount": 3,
        "content": "C. Switch to flat-rate pricing and establish a hierarchical priority model for your projects."
      },
      {
        "date": "2023-01-31T12:42:00.000Z",
        "voteCount": 4,
        "content": "Switching to flat-rate pricing would allow you to ensure a consistent level of service and avoid running into the on-demand slot quota per project. Additionally, by establishing a hierarchical priority model for your projects, you could allocate resources based on the specific needs and priorities of each business unit, ensuring that the most critical queries are executed first. This approach would allow you to balance the needs of each business unit while maximizing the use of your BigQuery resources."
      },
      {
        "date": "2022-12-06T05:38:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigquery/docs/reservations-intro\nThe benefits of using BigQuery Reservations include:\n- Workload management. After you purchase slots, you can allocate them to workloads. That way, a workload has a dedicated pool of BigQuery computational resources available for use. At the same time, if a workload doesn't use all of its allocated slots, any unused slots are shared automatically across your other workloads.\n- Centralized purchasing: You can purchase and allocate slots for your entire organization. You don't need to purchase slots for each project that uses BigQuery."
      },
      {
        "date": "2022-12-01T03:00:00.000Z",
        "voteCount": 1,
        "content": "C. https://cloud.google.com/bigquery/quotas - 2000 is the max no. of slots"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 86,
    "url": "https://www.examtopics.com/discussions/google/view/17173-exam-professional-data-engineer-topic-1-question-86/",
    "body": "You have an Apache Kafka cluster on-prem with topics containing web application logs. You need to replicate the data to Google Cloud for analysis in BigQuery and Cloud Storage. The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Kafka cluster on GCE VM Instances. Configure your on-prem cluster to mirror your topics to the cluster running in GCE. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Kafka cluster on GCE VM Instances with the Pub/Sub Kafka connector configured as a Sink connector. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Source connector. Use a Dataflow job to read from Pub/Sub and write to GCS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Sink connector. Use a Dataflow job to read from Pub/Sub and write to GCS."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-04-11T22:08:00.000Z",
        "voteCount": 33,
        "content": "A.\nhttps://cwiki.apache.org/confluence/pages/viewpage.action?pageId=27846330\nThe solution specifically mentions mirroring and minimizing the use of Kafka Connect plugin.\nD would be the more Google Cloud-native way of implementing the same, but the requirement is better met by A."
      },
      {
        "date": "2020-03-27T21:15:00.000Z",
        "voteCount": 10,
        "content": "Answer: A\nDescription: Question says mirroring and avoid kafka connect plugins"
      },
      {
        "date": "2023-06-07T02:02:00.000Z",
        "voteCount": 3,
        "content": "Pub/Sub Kafka connector requires Kafka Connect, as described here https://cloud.google.com/pubsub/docs/connect_kafka\nDeployment of Kafka Connect is explicitly excluded by the requirements. So the only option available is A"
      },
      {
        "date": "2023-02-04T11:59:00.000Z",
        "voteCount": 2,
        "content": "Option A: Deploy a Kafka cluster on GCE VM Instances. Configure your on-prem cluster to mirror your topics to the cluster running in GCE. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.\n\nThis option involves setting up a separate Kafka cluster in Google Cloud, and then configuring the on-prem cluster to mirror the topics to this cluster. The data from the Google Cloud Kafka cluster can then be read using either a Dataproc cluster or a Dataflow job and written to Cloud Storage for analysis in BigQuery."
      },
      {
        "date": "2023-02-04T12:00:00.000Z",
        "voteCount": 1,
        "content": "Option B: Deploy a Kafka cluster on GCE VM Instances with the Pub/Sub Kafka connector configured as a Sink connector. Use a Dataproc cluster or Dataflow job to read from Kafka and write to GCS.\n\nThis option is similar to Option A, but involves using the Pub/Sub Kafka connector as a sink connector instead of mirroring the topics from the on-prem cluster. This option would result in the same duplication of data and additional resources required as Option A, making it less desirable."
      },
      {
        "date": "2023-02-05T10:04:00.000Z",
        "voteCount": 1,
        "content": "Sorry. I messed up. The answer is probably A. My badd...."
      },
      {
        "date": "2023-02-04T12:01:00.000Z",
        "voteCount": 1,
        "content": "Option D: Deploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Sink connector. Use a Dataflow job to read from Pub/Sub and write to GCS.\n\nThis option involves deploying the Pub/Sub Kafka connector on the on-prem cluster, but configuring it as a sink connector. In this case, the data from the on-prem Kafka cluster would be sent directly to Pub/Sub, which would act as the final destination for the data. A Dataflow job would then be used to read the data from Pub/Sub and write it to Cloud Storage for analysis in BigQuery. This option would result in the data being stored in both the on-prem cluster and Pub/Sub, making it less desirable compared to option C, where the data is only stored in Pub/Sub as an intermediary between the on-prem cluster and Google Cloud."
      },
      {
        "date": "2023-02-24T06:22:00.000Z",
        "voteCount": 1,
        "content": "you use chatgpt replies, if you instruct chat gpt that you don't need to use plugins as per question say, it will answer A"
      },
      {
        "date": "2023-02-04T12:00:00.000Z",
        "voteCount": 2,
        "content": "Option C: Deploy the Pub/Sub Kafka connector to your on-prem Kafka cluster and configure Pub/Sub as a Source connector. Use a Dataflow job to read from Pub/Sub and write to GCS.\n\nThis option involves deploying the Pub/Sub Kafka connector directly on the on-prem cluster, and configuring it as a source connector. The data from the on-prem Kafka cluster is then sent directly to Pub/Sub, which acts as an intermediary between the on-prem cluster and the data stored in Google Cloud. A Dataflow job is then used to read the data from Pub/Sub and write it to Cloud Storage for analysis in BigQuery. This option avoids the duplication of data and additional resources required by the other options, making it the preferred option."
      },
      {
        "date": "2022-12-06T05:34:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-10-21T01:29:00.000Z",
        "voteCount": 1,
        "content": "\"The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins.\""
      },
      {
        "date": "2022-10-12T02:39:00.000Z",
        "voteCount": 3,
        "content": "D is the right answer"
      },
      {
        "date": "2022-09-22T17:21:00.000Z",
        "voteCount": 2,
        "content": "D is the right answer"
      },
      {
        "date": "2021-12-18T00:14:00.000Z",
        "voteCount": 6,
        "content": "\"A\" is the answer which complies with the requirements (specifically, \"The preferred replication method is mirroring to avoid deployment of Kafka Connect plugins\"). Indeed, one of the uses of what is called \"Geo-Replication\" (or Cross-Cluster Data Mirroring) in Kafka is precisely cloud migrations: https://kafka.apache.org/documentation/#georeplication\n\nHowever I agree with Ganshank, and the optimal \"Google way\" way would be \"D\", installing the Pub/Sub Kafka connector to move the data from on-prem to GCP."
      },
      {
        "date": "2021-10-22T12:06:00.000Z",
        "voteCount": 3,
        "content": "Going with \"D\"\n\nRefer: https://stackoverflow.com/questions/55277188/kafka-to-google-pub-sub-using-sink-connector"
      },
      {
        "date": "2021-12-18T10:26:00.000Z",
        "voteCount": 1,
        "content": "\"avoid deployment of Kafka Connect plugins\""
      },
      {
        "date": "2021-06-30T16:58:00.000Z",
        "voteCount": 1,
        "content": "Vote for A"
      },
      {
        "date": "2021-03-11T12:08:00.000Z",
        "voteCount": 3,
        "content": "Answer: A\nDescription: Question says mirroring to avoid kafka connect plugins"
      },
      {
        "date": "2021-02-09T13:52:00.000Z",
        "voteCount": 1,
        "content": "Correct is D"
      },
      {
        "date": "2021-07-08T16:29:00.000Z",
        "voteCount": 1,
        "content": "As per question - \"avoid deployment of Kafka Connect plugins.\""
      },
      {
        "date": "2020-09-20T09:40:00.000Z",
        "voteCount": 3,
        "content": "A. \nthe best solution would be D but given the restriction here to use mirroring and avoid connectors, A would be the natural choice"
      },
      {
        "date": "2020-09-08T00:04:00.000Z",
        "voteCount": 4,
        "content": "D should be the correct answer. Configure pub/sub as sink"
      },
      {
        "date": "2020-08-20T16:20:00.000Z",
        "voteCount": 2,
        "content": "C is correct.\nhttps://docs.confluent.io/current/connect/kafka-connect-gcp-pubsub/index.html"
      },
      {
        "date": "2020-08-20T16:39:00.000Z",
        "voteCount": 1,
        "content": "Correct Answer: D\nWhy is this correct?\nYou can connect Kafka to GCP by using a connector. The 'downstream' service (Pub/Sub) will use a sink connector."
      },
      {
        "date": "2021-06-30T16:55:00.000Z",
        "voteCount": 2,
        "content": "Question says : avoid deployment of Kafka Connect plugins."
      },
      {
        "date": "2020-08-05T20:10:00.000Z",
        "voteCount": 3,
        "content": "its D, why would google prefer Kafka in their own cert questions! :)"
      },
      {
        "date": "2021-09-06T10:24:00.000Z",
        "voteCount": 3,
        "content": "Because the questions mentions to avoid deployment of Kafka connect plugins"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 87,
    "url": "https://www.examtopics.com/discussions/google/view/16572-exam-professional-data-engineer-topic-1-question-87/",
    "body": "You've migrated a Hadoop job from an on-prem cluster to dataproc and GCS. Your Spark job is a complicated analytical workload that consists of many shuffling operations and initial data are parquet files (on average 200-400 MB size each). You see some degradation in performance after the migration to Dataproc, so you'd like to optimize for it. You need to keep in mind that your organization is very cost-sensitive, so you'd like to continue using Dataproc on preemptibles (with 2 non-preemptible workers only) for this workload.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of your parquet files to ensure them to be 1 GB minimum.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to TFRecords formats (appr. 200MB per file) instead of parquet files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T01:14:00.000Z",
        "voteCount": 68,
        "content": "Should be A:\n\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files\nhttps://www.dremio.com/tuning-parquet/\n\nC &amp; D will improve performance but need to pay more $$"
      },
      {
        "date": "2021-10-06T10:16:00.000Z",
        "voteCount": 5,
        "content": "It is A  . please read the links above"
      },
      {
        "date": "2022-12-07T05:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance"
      },
      {
        "date": "2021-07-28T06:55:00.000Z",
        "voteCount": 8,
        "content": "Point for discussion - Another reason why it can't be C or D.\nSSD's are not available on pre-emptible Worker nodes (answers didn't say whether they wanted to switch from HDD to SDD for Master nodes)\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs"
      },
      {
        "date": "2022-06-29T09:46:00.000Z",
        "voteCount": 1,
        "content": "You can have local SSDs for the dataproc normal or preemptible VMs https://cloud.google.com/dataproc/docs/concepts/compute/dataproc-pd-ssd"
      },
      {
        "date": "2021-07-28T07:01:00.000Z",
        "voteCount": 1,
        "content": "Also for Shuffling Operations, one need to override the preemptible VMs configuration to increase boot disk size.\n(Second half of answer D is correct but first half is wrong)"
      },
      {
        "date": "2022-12-07T16:46:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/dataproc/docs/support/spark-job-tuning#limit_the_number_of_files\n\nStore data in larger file sizes, for example, file sizes in the 256MB\u2013512MB range."
      },
      {
        "date": "2020-03-14T06:44:00.000Z",
        "voteCount": 12,
        "content": "Answer should be D"
      },
      {
        "date": "2020-03-20T04:18:00.000Z",
        "voteCount": 15,
        "content": "D: # By default, preemptible node disk sizes are limited to 100GB or the size of the non-preemptible node disk sizes, whichever is smaller. However you can override the default preemptible disk size to any requested size. Since the majority of our cluster is using preemptible nodes, the size of the disk used for caching operations will see a noticeable performance improvement using a larger disk. Also, SSD's will perform better than HDD. This will increase costs slightly, but is the best option available while maintaining costs."
      },
      {
        "date": "2020-06-21T04:13:00.000Z",
        "voteCount": 2,
        "content": "C is correct. D is wrong. they are using 'dataproc and GCS', not related to boot disk at all ."
      },
      {
        "date": "2020-07-20T08:52:00.000Z",
        "voteCount": 1,
        "content": "C  is recommended only  -\nIf you have many small files, consider copying files for processing to the local HDFS and then copying the results back"
      },
      {
        "date": "2020-08-16T02:53:00.000Z",
        "voteCount": 3,
        "content": "File sizes are already within the expected range for GCS (128MB-1GB) so not C.\nD seems most feasible"
      },
      {
        "date": "2024-09-23T08:23:00.000Z",
        "voteCount": 1,
        "content": "There's no mention of a drive type used, only GCS. That means A is the only sensible option."
      },
      {
        "date": "2024-07-25T10:49:00.000Z",
        "voteCount": 2,
        "content": "Question doesn't actually say they are using HDD in the scenario, for that reason I choose A"
      },
      {
        "date": "2024-02-09T04:49:00.000Z",
        "voteCount": 2,
        "content": "A\nWe don't know if HDD was used, so we can know what to do about that, but we know that the parquet files are small and much, and we can act on that by increasing the sizes to have lesser number of it."
      },
      {
        "date": "2023-12-02T03:43:00.000Z",
        "voteCount": 1,
        "content": "Should be A:\nhttps://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files"
      },
      {
        "date": "2023-12-04T05:32:00.000Z",
        "voteCount": 1,
        "content": "Given the scenario and the cost-sensitive nature of your organization, the best option would be:\n\nC. Switch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job, and copy results back to GCS.\n\nOption C allows you to leverage the benefits of SSDs and HDFS while minimizing costs by continuing to use Dataproc on preemptible VMs. This approach optimizes both performance and cost-effectiveness for your analytical workload on Google Cloud."
      },
      {
        "date": "2023-07-23T03:51:00.000Z",
        "voteCount": 1,
        "content": "https://stackoverflow.com/questions/42918663/is-it-better-to-have-one-large-parquet-file-or-lots-of-smaller-parquet-files\n\nCost effective is the key in the question."
      },
      {
        "date": "2023-03-22T05:35:00.000Z",
        "voteCount": 1,
        "content": "Preemptible VMs can't be used for HDFS storage.\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads."
      },
      {
        "date": "2023-03-02T00:15:00.000Z",
        "voteCount": 1,
        "content": "Should NOT be A as:\n1. The file size is already at the optimal size\n2. If the current file size works well in the current Hadoop, it is expected to have similar performance in Dataproc\n\nThe only difference between the current and Dataproc is that Dataproc is using preemptible nodes. So yes, it may incur a bit more cost by using SSD but assuming using the preemptible already save most of it, so we want to save less to improve the performance"
      },
      {
        "date": "2023-07-24T10:43:00.000Z",
        "voteCount": 1,
        "content": "Optimal size is 1GB"
      },
      {
        "date": "2023-02-23T18:22:00.000Z",
        "voteCount": 1,
        "content": "Cost sensitive is the keyword."
      },
      {
        "date": "2023-02-15T04:32:00.000Z",
        "voteCount": 2,
        "content": "this question asked by Google, So option C is not correct otherwise, good approach to use initial data in hdfs and swtich from HDD to SDDs for 2 non-preemptible node. \nOption D is right but they are not mentioning that they will stop using 2 non-preemptible node. but i assume it :P"
      },
      {
        "date": "2023-01-27T04:08:00.000Z",
        "voteCount": 3,
        "content": "C.\nref : https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n- size recommended is 128MB-1GB ==&gt; so it is not size issue ==&gt; not A\n- there is no issue mentioned with file format ==&gt; not B\n- D. could be a good solution, but requires overriding preemptible VMs. however, the questions asks to continue using preemtibles ==&gt; not D\n- C. is a good solution."
      },
      {
        "date": "2023-01-28T23:01:00.000Z",
        "voteCount": 1,
        "content": "agreed C over D as\nswitching from HDDs to SSDs and overriding the preemptible VMs configuration to increase the boot disk size, may not be the best solution for improving performance in this scenario because it doesn't address the main issue which is the large number of shuffling operations that are causing performance degradation. While SSDs may have faster read and write speeds than HDDs, they may not provide significant performance improvements for a workload that is primarily CPU-bound and heavily reliant on shuffling operations. Additionally, increasing the boot disk size of the preemptible VMs may not be necessary or cost-effective for this particular workload."
      },
      {
        "date": "2022-12-20T06:15:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance\n\nManage Cloud Storage file sizes\nTo get optimal performance, split your data in Cloud Storage into files with sizes from 128 MB to 1 GB. Using lots of small files can create a bottleneck. If you have many small files, consider copying files for processing to the local HDFS and then copying the results back.\n\nSwitch to SSD disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance."
      },
      {
        "date": "2022-12-07T05:26:00.000Z",
        "voteCount": 3,
        "content": "Its D 100%. It's the recommended best practice for this scenario.\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance"
      },
      {
        "date": "2022-12-06T05:32:00.000Z",
        "voteCount": 1,
        "content": "D is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#switch_to_ssd_disks\nIf you perform many shuffling operations or partitioned writes, switch to SSDs to boost performance.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms\nAs a default, preemptible VMs are created with a smaller boot disk size, and you might want to override this configuration if you are running shuffle-heavy workloads. For details, see the page on preemptible VMs in the Dataproc documentation."
      },
      {
        "date": "2022-11-24T06:53:00.000Z",
        "voteCount": 2,
        "content": "Answer id D\nnot C because cannot use HDFS with preemptible VMs\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#use_preemptible_vms"
      },
      {
        "date": "2022-11-22T21:45:00.000Z",
        "voteCount": 4,
        "content": "Option D is correct\n\nElimination Strategy:-\nA.\tIncrease the size of your parquet files to ensure them to be 1 GB minimum (doesn\u2019t make sense as the file size are fit for migration to proceed with given scenario, recommended size is between 128 MB to 1 GB.)\nB.\tSwitch to TFRecords formats (appr. 200MB per file) instead of parquet files(doesn\u2019t make sense to make changes to file format )\nC.\tSwitch from HDDs to SSDs, copy initial data from GCS to HDFS, run the Spark job and copy results back to GCS(doesn\u2019t make sense to copy the file from GCS to HDFS as the workload that consists of many shuffling operations)\nD.\tSwitch from HDDs to SSDs, override the preemptible VMs configuration to increase the boot disk size(perfect fit as the workload that consists of many shuffling operations which requires attention to increase the performance reference doc:- https://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#optimize_performance )"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 88,
    "url": "https://www.examtopics.com/discussions/google/view/79771-exam-professional-data-engineer-topic-1-question-88/",
    "body": "Your team is responsible for developing and maintaining ETLs in your company. One of your Dataflow jobs is failing because of some errors in the input data, and you need to improve reliability of the pipeline (incl. being able to reprocess all failing data).<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a filtering step to skip these types of errors in the future, extract erroneous rows from logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a try\u05d2\u20ac\u00a6 catch block to your DoFn that transforms the data, extract erroneous rows from logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a try\u05d2\u20ac\u00a6 catch block to your DoFn that transforms the data, write erroneous rows to Pub/Sub PubSub directly from the DoFn.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a try\u05d2\u20ac\u00a6 catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub/Sub later.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-02T00:28:00.000Z",
        "voteCount": 12,
        "content": "C is a big NO. Writing to PubSub in DoFn will cause bottleneck in the pipeline. For IO, we should always use those IO lib (e.g PubsubIO)\nUsing sideOutput is the correct answer here. There is a Qwiklab about this. It is recommended to do that lab to understand more."
      },
      {
        "date": "2023-02-15T12:40:00.000Z",
        "voteCount": 7,
        "content": "Based on the given scenario, option D would be the best approach to improve the reliability of the pipeline.\n\nAdding a try-catch block to the DoFn that transforms the data would allow you to catch and handle errors within the pipeline. However, storing erroneous rows in Pub/Sub directly from the DoFn (Option C) could potentially create a bottleneck in the pipeline, as it adds additional I/O operations to the data processing.\n\nOption A of filtering the erroneous data would not allow the pipeline to reprocess the failing data, which could result in data loss.\n\nOption D of using a sideOutput to create a PCollection of erroneous data would allow for reprocessing of the failed data and would not create a bottleneck in the pipeline. Storing the erroneous data in a separate PCollection would also make it easier to debug and analyze the failed data.\n\nTherefore, adding a try-catch block to the DoFn that transforms the data and using a sideOutput to create a PCollection of erroneous data that can be stored to Pub/Sub later would be the best approach to improve the reliability of the pipeline."
      },
      {
        "date": "2024-04-08T04:05:00.000Z",
        "voteCount": 2,
        "content": "I think it's D because here you can write data from Dataflow PCollection to pub/sub. https://cloud.google.com/dataflow/docs/guides/write-to-pubsub"
      },
      {
        "date": "2023-07-24T10:56:00.000Z",
        "voteCount": 1,
        "content": "Answer is C. Here is the github repo and an example from the Qwiklab where they tag the output as 'parsed_rows' and 'unparsed_rows'  before they send the data to GCS. I don't see how GCS or PubSub would make a difference at this point. It seems like a more maintanable solution to just parse the data in the DoFn.\n\n1) If the function does more than that then it serves multiple purposes and it's not good software engineering. Unless there is a good reason, writing to PubSub should be separated from the DoFn.\n\nii) It's faster to write in mini-batches or one batch than stream the errors. What's the need for streaming out errors 1 by 1? Literally no real advantage.\n\nhttps://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/quests/dataflow_python/7_Advanced_Streaming_Analytics/solution/streaming_minute_traffic_pipeline.py"
      },
      {
        "date": "2023-03-23T01:11:00.000Z",
        "voteCount": 2,
        "content": "Output errors to new PCollection \u2013 Send to collector for later analysis (Pub/Sub is a good target)"
      },
      {
        "date": "2023-02-15T04:44:00.000Z",
        "voteCount": 3,
        "content": "Option D is right approach to use to get errors as sideOutput. Apache beam has a special scripting docs not dynamic as python itself. So lets follow standard sideOutput(withoutputs in the code)\nsyntax be like in pipeline:\n'ProcessData' &gt;&gt; beam.ParDo(DoFn).with_outputs"
      },
      {
        "date": "2023-02-24T06:47:00.000Z",
        "voteCount": 1,
        "content": "After using you try: Catch: you can also send the erroneous records to dead letter sink into BQ\n``` outputTuple.get(deadLetterTag).apply(BigQuery.write(...)) ```"
      },
      {
        "date": "2023-02-08T01:47:00.000Z",
        "voteCount": 3,
        "content": "blahblahblahblahblahblahblahblah"
      },
      {
        "date": "2023-01-29T04:38:00.000Z",
        "voteCount": 2,
        "content": "It`s D.\nUse a try catch block to direct erroneous rows into a side output. The PCollection of the side output can be sent efficiently to the PubSub topic via Apache Beam PubSubIO.\n\nIt's not C because C means to sent every single invalid row in a separate request to PubSub which is very inefficient when working with Dataflow as now batching is involved."
      },
      {
        "date": "2022-12-06T05:25:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-12-01T06:50:00.000Z",
        "voteCount": 1,
        "content": "C\nD: dataflow to pub/sub is weird"
      },
      {
        "date": "2022-11-25T03:15:00.000Z",
        "voteCount": 3,
        "content": "D\nSide output is a great manner to branch the processing. Let's take the example of an input data source that contains both valid and invalid values. Valid values must be written in place #1 and the invalid ones in place#2. A naive solution suggests to use a filter and write 2 distinct processing pipelines. However this approach has one main drawback - the input dataset is read twice. If for the mentioned problem we use side outputs, we can still have 1 ParDo transform that internally dispatches valid and invalid values to appropriate places (#1 or #2, depending on value's validity).\n\n\nhttps://www.waitingforcode.com/apache-beam/side-output-apache-beam/read#:~:text=simple%20test%20cases.-,Side%20output%20defined,-%C2%B6"
      },
      {
        "date": "2022-11-24T06:55:00.000Z",
        "voteCount": 1,
        "content": "Answer is D"
      },
      {
        "date": "2022-11-06T12:01:00.000Z",
        "voteCount": 2,
        "content": "It's C.\nIn D, \"storing to PubSub later\" doesn't really make sense."
      },
      {
        "date": "2022-10-14T18:23:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. You need to reprocess all the failling data, and yes, you can use PubSub as a sink, according to the documentation: https://beam.apache.org/documentation/io/connectors/"
      },
      {
        "date": "2022-09-28T05:29:00.000Z",
        "voteCount": 4,
        "content": "Answer C"
      },
      {
        "date": "2022-09-28T05:27:00.000Z",
        "voteCount": 6,
        "content": "The error records are directly written to PubSub from the DoFn (it\u2019s equivalent in python).\nYou cannot directly write a PCollection to PubSub. You have to extract each record and write one at a time. Why do the additional work and why not write it using PubSubIO in the DoFn itself?\nYou can write the whole PCollection to Bigquery though, as explained in\n\nReference:\nhttps://medium.com/google-cloud/dead-letter-queues-simple-implementation-strategy-for-cloud-pub-sub-80adf4a4a800"
      },
      {
        "date": "2022-09-03T05:58:00.000Z",
        "voteCount": 3,
        "content": "D. Add a try-catch block to your DoFn that transforms the data, use a sideOutput to create a PCollection that can be stored to Pub/Sub later."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 89,
    "url": "https://www.examtopics.com/discussions/google/view/79337-exam-professional-data-engineer-topic-1-question-89/",
    "body": "You're training a model to predict housing prices based on an available dataset with real estate properties. Your plan is to train a fully connected neural net, and you've discovered that the dataset contains latitude and longitude of the property. Real estate professionals have told you that the location of the property is highly influential on price, so you'd like to engineer a feature that incorporates this physical dependency.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide latitude and longitude as input vectors to your neural net.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a numeric column from a feature cross of latitude and longitude.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-22T03:18:00.000Z",
        "voteCount": 8,
        "content": "Option C is correct\n\nUse L1 regularization when you need to assign greater importance to more influential features. It\nshrinks less important feature to 0.\nL2 regularization performs better when all input features influence the output &amp; all with the\nweights are of equal size."
      },
      {
        "date": "2022-09-26T14:06:00.000Z",
        "voteCount": 8,
        "content": "Ans C, use L1 regularization becuase we know the feature is a strong feature.  L2 will evenly distribute weights"
      },
      {
        "date": "2024-10-14T13:47:00.000Z",
        "voteCount": 1,
        "content": "This does not seems to be useful, minute level bucketizing will create 3,600 possible buckets per degree squared, not logical, and sparse feature space, Option A seems to be a better choice."
      },
      {
        "date": "2024-07-20T13:17:00.000Z",
        "voteCount": 1,
        "content": "Bucketing into minutes is inaccurate, up to 1.8 km are grouped. Way too much for real estste.\nTherefore B"
      },
      {
        "date": "2023-09-24T00:09:00.000Z",
        "voteCount": 1,
        "content": "Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization.\n\n    Like option C, we bucketize at the minute level, but this time we apply L2 regularization. L2 regularization, or Ridge Regression, discourages large values of weights in the model without forcing them to become sparse. It can help prevent overfitting, especially when we have a large number of features (as a result of bucketizing and crossing).\n\nGiven the options, D. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization seems to be the most appropriate. Bucketizing at the minute level captures localized patterns, and L2 regularization can help control the complexity of the model without enforcing sparsity."
      },
      {
        "date": "2023-09-21T05:09:00.000Z",
        "voteCount": 3,
        "content": "What does bucketizing at the minute level mean in the context of this question?"
      },
      {
        "date": "2023-11-08T08:27:00.000Z",
        "voteCount": 4,
        "content": "Coordinates are written with Degrees, minutes and seconds (one minute being equal to about 1.8 km). So you group your coordinates in buckets with a miute precision"
      },
      {
        "date": "2023-08-27T03:43:00.000Z",
        "voteCount": 1,
        "content": "I strongly believe it's  B."
      },
      {
        "date": "2023-07-23T03:56:00.000Z",
        "voteCount": 1,
        "content": "The right answer is B. What the hell does bucketize the feature cross of latitude and longtitude even mean? They are not a time feature. C and D don't even make sense. The L1 regularization is something that doesn't answer anything in the question. The only valid feature engineered here is option B. A is not an engineered feature.\n\n Create a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization."
      },
      {
        "date": "2024-09-23T08:36:00.000Z",
        "voteCount": 1,
        "content": "Bucketising means that we're saying \"anyone in this square 1.8km (minute) region is considered a single area\" - it's actually recommended as a default way to deal with lat/lon, as they don't really work as seperate columns (or at least we'd be hoping the FCNN buckets them intelligently itself, which it won't mostly)"
      },
      {
        "date": "2023-07-11T10:01:00.000Z",
        "voteCount": 1,
        "content": "D\n\nYou have to use L2, since you have create a new variable with two already existing the risk of multicollinearity is high, L1 is good for selecting feature to avoid curse of dimensionality not for multicollinearity"
      },
      {
        "date": "2023-05-01T07:44:00.000Z",
        "voteCount": 1,
        "content": "Why not L2? L2 (Ridge) uses a squared value coefficient as a penalty term to the loss function, while L1 (Lasso) uses an absolute value coefficient. Isn't a squared penalty stronger than an absolute one? \nhttps://towardsdatascience.com/l1-and-l2-regularization-methods-ce25e7fc831c"
      },
      {
        "date": "2023-09-21T05:08:00.000Z",
        "voteCount": 2,
        "content": "L1 regression forces unimportant coefficients to zero. Since the location is extremely important, L1 will force less important coefficients to zero, thereby further increasing the importance of the location coefficient."
      },
      {
        "date": "2023-04-25T05:12:00.000Z",
        "voteCount": 2,
        "content": "gpt: Option C and D suggest bucketizing the feature cross of latitude and longitude at the minute level and using L1 or L2 regularization during optimization. While regularization can help prevent overfitting, bucketizing at such a granular level may not be necessary and could lead to overfitting. It's also not clear how bucketizing at the minute level would capture the spatial relationship between the latitude and longitude features."
      },
      {
        "date": "2023-01-27T04:39:00.000Z",
        "voteCount": 1,
        "content": "D. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L2 regularization during optimization. This will create a new feature that captures the physical dependency of the location of the property on the price, and bucketing it at the minute level will reduce the number of unique values and prevent overfitting. L2 regularization will also help to prevent overfitting by penalizing large weights in the model."
      },
      {
        "date": "2023-05-25T02:11:00.000Z",
        "voteCount": 1,
        "content": "chat-gpt also says D\nexplanation: \nThis approach effectively creates a grid of the geographical area in your data, allowing the model to learn weights for each grid cell (bucket). This helps capture the spatial relationship between latitude and longitude, which can be crucial for real estate prices. Additionally, using L2 regularization helps prevent overfitting by discouraging complex models, which can be particularly important when working with high-dimensional crossed features."
      },
      {
        "date": "2022-12-06T05:24:00.000Z",
        "voteCount": 3,
        "content": "C is the answer.\n\nhttps://developers.google.com/machine-learning/crash-course/feature-crosses/video-lecture\nA feature cross is a synthetic feature formed by multiplying (crossing) two or more features. Crossing combinations of features can provide predictive abilities beyond what those features can provide individually.\n\nhttps://developers.google.com/machine-learning/crash-course/regularization-for-sparsity/l1-regularization"
      },
      {
        "date": "2022-09-11T08:49:00.000Z",
        "voteCount": 2,
        "content": "https://medium.com/riga-data-science-club/geographic-coordinate-encoding-with-tensorflow-feature-columns-e750ae338b7c#:~:text=to%20the%20rescue!-,Feature%20Crosses,-Combining%20features%20into"
      },
      {
        "date": "2022-09-11T08:50:00.000Z",
        "voteCount": 1,
        "content": "Feature cross seems to be the right feature option"
      },
      {
        "date": "2022-09-11T08:51:00.000Z",
        "voteCount": 4,
        "content": "So it's B option"
      },
      {
        "date": "2022-09-05T19:44:00.000Z",
        "voteCount": 1,
        "content": "Regularization + location into one"
      },
      {
        "date": "2022-09-02T21:43:00.000Z",
        "voteCount": 5,
        "content": "C. Create a feature cross of latitude and longitude, bucketize it at the minute level and use L1 regularization during optimization."
      },
      {
        "date": "2022-09-02T02:26:00.000Z",
        "voteCount": 1,
        "content": "C or D?\nhttps://medium.com/riga-data-science-club/geographic-coordinate-encoding-with-tensorflow-feature-columns-e750ae338b7c"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 90,
    "url": "https://www.examtopics.com/discussions/google/view/17260-exam-professional-data-engineer-topic-1-question-90/",
    "body": "You are deploying MariaDB SQL databases on GCE VM Instances and need to configure monitoring and alerting. You want to collect metrics including network connections, disk IO and replication status from MariaDB with minimal development effort and use StackDriver for dashboards and alerts.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the OpenCensus Agent and create a custom metric collection application with a StackDriver exporter.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPlace the MariaDB instances in an Instance Group with a Health Check.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the StackDriver Logging Agent and configure fluentd in_tail plugin to read MariaDB logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall the StackDriver Agent and configure the MySQL plugin.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-18T09:58:00.000Z",
        "voteCount": 26,
        "content": "Answer : A\nMariaDB needs costume metrics , and stackdriver built-in monitoring tools will not provide these metrics . Opencensus Agent will do this for you \nFor more info , refer to :\nhttps://cloud.google.com/monitoring/custom-metrics/open-census"
      },
      {
        "date": "2021-09-14T09:49:00.000Z",
        "voteCount": 13,
        "content": "It is definitely A.\nB: can't be because Health Checks just checks that machine is online\nC: StackDriver Logging is for Logging. Here we talk of Monitoring / Alerting\nD: StackDriver Agent monitors default metrics of VMs and some Database stuff with the MySQL Plugin. Here you want to monitor some more custom stuff like Replication of MariaDB (I didn't find anything of this sort in the plugin page), and you may want to use Custom Metrics rather than default metrics. \"Cloud Monitoring automatically collects more than 1,500 built-in metrics from more than 100 monitored resources. But those metrics cannot capture application-specific data or client-side system data. Those metrics can give you information on backend latency or disk usage, but they can't tell you how many background routines your application spawned.\" https://cloud.google.com/monitoring/custom-metrics/open-census#monitoring_opencensus_metrics_quickstart-python"
      },
      {
        "date": "2020-03-27T21:44:00.000Z",
        "voteCount": 13,
        "content": "Answer: C\nDescription: The GitHub repository named google-fluentd-catch-all-config which includes the configuration files for the Logging agent for ingesting the logs from various third-party software packages."
      },
      {
        "date": "2021-08-20T08:33:00.000Z",
        "voteCount": 2,
        "content": "I think its D, because its Selfmanaged DB and for this we use Stackdriver Agents. and in this question its asking about metrics not logs."
      },
      {
        "date": "2023-12-04T05:52:00.000Z",
        "voteCount": 1,
        "content": "Here's the rationale:\nStackDriver Agent: The StackDriver Agent is designed to collect system and application metrics from virtual machine instances and send them to StackDriver Monitoring. It simplifies the process of collecting and forwarding metrics.\nMySQL Plugin: The StackDriver Agent has a MySQL plugin that allows you to collect MySQL-specific metrics without the need for additional custom development. This includes metrics related to network connections, disk IO, and replication status \u2013 which are the specific metrics you mentioned.\n\nOption D is the most straightforward and least development-intensive approach to achieve the monitoring and alerting requirements for MariaDB on GCE VM Instances using StackDriver."
      },
      {
        "date": "2023-11-08T14:33:00.000Z",
        "voteCount": 1,
        "content": "replication status seems to be not included in sql agent metrics. but I do not like A in terms of efforts"
      },
      {
        "date": "2023-03-06T23:51:00.000Z",
        "voteCount": 1,
        "content": "it can't be A as it saying minimal development but for opencensus the development is needed."
      },
      {
        "date": "2022-12-20T06:41:00.000Z",
        "voteCount": 1,
        "content": "To use metrics collected by OpenCensus in your Google Cloud project, you must make the OpenCensus metrics libraries and the Stackdriver exporter available to your application. The Stackdriver exporter exports the metrics that OpenCensus collects to your Google Cloud project. You can then use Cloud Monitoring to chart or monitor those metrics."
      },
      {
        "date": "2022-12-06T05:19:00.000Z",
        "voteCount": 9,
        "content": "D is the answer.\n\nhttps://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party/mariadb"
      },
      {
        "date": "2022-12-25T07:25:00.000Z",
        "voteCount": 3,
        "content": "For supplement, \u2018Stackdriver agent' now called as Ops agent, 'Operations Suite'"
      },
      {
        "date": "2022-11-22T02:45:00.000Z",
        "voteCount": 4,
        "content": "Option D is Correct\nMariaDB is a community-developed, commercially supported fork of the MySQL relational database management system (RDBMS). To collect logs and metrics for MariaDB, use the mysql receivers.\n\nThe mysql receiver connects by default to a local MariaDB server using a Unix socket and Unix authentication as the root user.\n\nreference:-https://cloud.google.com/stackdriver/docs/solutions/agents/ops-agent/third-party/mariadb"
      },
      {
        "date": "2022-10-29T06:22:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb"
      },
      {
        "date": "2022-09-22T17:10:00.000Z",
        "voteCount": 2,
        "content": "C is the answer, fluentd plug in is needed as the DB is on GCE"
      },
      {
        "date": "2022-08-29T16:28:00.000Z",
        "voteCount": 2,
        "content": "go for D"
      },
      {
        "date": "2022-08-21T04:34:00.000Z",
        "voteCount": 2,
        "content": "A\nStackDriver Agent monitors default metrics of VMs and some Database stuff with the MySQL Plugin. Here you want to monitor some more custom stuff like Replication of MariaDB (I didn\u2019t find anything of this sort in the plugin page), and you may want to use Custom Metrics rather than default metrics. \u201cCloud Monitoring automatically collects more than 1,500 built-in metrics from more than 100 monitored resources. But those metrics cannot capture application-specific data or client-side system data. Those metrics can give you information on backend latency or disk usage, but they can\u2019t tell you how many background routines your application spawned.\u201d https://cloud.google.com/monitoring/custom-metrics/open-census#monitoring_opencensus_metrics_quickstart-python"
      },
      {
        "date": "2022-06-22T10:52:00.000Z",
        "voteCount": 1,
        "content": "I'm not 100% sure as I have no experience with that issue, but I would say it's D - both A and D should work, but the keyword is \"with minimal development effort\" (and using pre-built plugin &gt; creating custom metric in terms of simplicity, that's obvious) and all of the relevant data (as per question) should be there: https://cloud.google.com/monitoring/api/metrics_agent#agent-mysql\n\nI'm not sure if C would work, but it also seems more advanced in implementation than D. B is 100% wrong and insufficient for that use case.\n\nFeel free to prove me wrong :)"
      },
      {
        "date": "2022-04-23T04:04:00.000Z",
        "voteCount": 1,
        "content": "A and D both seem like viable options here, unsure which is Google's preferred method as that would be deemed the correct answer in the exam. Any opinions?"
      },
      {
        "date": "2022-04-15T07:27:00.000Z",
        "voteCount": 4,
        "content": "D\nmariaDB is an extension of mysql and mysql plugin must work fine to extract the metrics of mariaDB."
      },
      {
        "date": "2022-05-25T00:55:00.000Z",
        "voteCount": 2,
        "content": "\"MariaDB is a community-developed, commercially supported fork of the MySQL relational database management system (RDBMS). To collect logs and metrics for MariaDB, use the mysql receivers.\"\n\nhttps://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb"
      },
      {
        "date": "2022-01-23T13:59:00.000Z",
        "voteCount": 2,
        "content": "Opencensus Agent is right one"
      },
      {
        "date": "2021-09-15T20:24:00.000Z",
        "voteCount": 4,
        "content": "D is correct. Answer : D\nmariaDB is an extension of mysql and mysql plugin must work fine to extract the metrics of mariaDB."
      },
      {
        "date": "2022-01-11T02:35:00.000Z",
        "voteCount": 2,
        "content": "agree; https://cloud.google.com/monitoring/agent/ops-agent/third-party/mariadb#configure-instance"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 91,
    "url": "https://www.examtopics.com/discussions/google/view/17256-exam-professional-data-engineer-topic-1-question-91/",
    "body": "You work for a bank. You have a labelled dataset that contains information on already granted loan application and whether these applications have been defaulted. You have been asked to train a model to predict default rates for credit applicants.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the dataset by collecting additional data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a linear regression to predict a credit default risk score.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the bias from the data and collect applications that have been declined loans.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMatch loan applicants with their social profiles to enable feature engineering."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-09-02T00:55:00.000Z",
        "voteCount": 38,
        "content": "A is incorrect as you need to work with the data you have available \nC is an optimisation not a solution \nD is ethically incorrect and invasion to privacy, there could be several legal implications with this\nB although oversimplified but is a workable solution"
      },
      {
        "date": "2021-10-04T03:38:00.000Z",
        "voteCount": 1,
        "content": "Information in social profiles are public"
      },
      {
        "date": "2021-10-04T03:40:00.000Z",
        "voteCount": 1,
        "content": "according to the privacy settings and shareable informations"
      },
      {
        "date": "2021-04-06T12:58:00.000Z",
        "voteCount": 21,
        "content": "We have labelled data that contains whether a loan application is accepted or defaulted - So Classification Problem Data.\n\nWe need to predict (Default Rates for applicants) - I think whether application will be granted or defaulted. - So Binary Classification.\n\nNo option matches the answer. - if we mark 'B' - It should be Logistic Regression, Instead of Linear Regression"
      },
      {
        "date": "2021-12-18T02:24:00.000Z",
        "voteCount": 22,
        "content": "Question says: \"to predict default RATES for credit applicants\".\nIt is not binary classification, so Linear Regression would work here. \nI think B is correct answer."
      },
      {
        "date": "2023-05-17T18:59:00.000Z",
        "voteCount": 1,
        "content": "Correct approach is to use logistic regression to predict default/not default, and then take the confidence/probability of the outcome as the \"default rate\". Linear regression doesn't make sense since we are not given a default rate label in our data, we are just given the labels default vs no default."
      },
      {
        "date": "2023-03-07T11:52:00.000Z",
        "voteCount": 2,
        "content": "You cannot predict rate. You predict a realization, which is either default or not. This question is terribly written."
      },
      {
        "date": "2024-10-14T02:01:00.000Z",
        "voteCount": 1,
        "content": "\"social\" does not mean Social Media. This could be linked to demograhic data, si it could improve the score."
      },
      {
        "date": "2023-12-21T05:06:00.000Z",
        "voteCount": 3,
        "content": "To predict default rates for credit applicants using the labeled dataset of granted loan applications, the most appropriate course of action would be:\n\nB. Train a linear regression to predict a credit default risk score.\n\nHere's the rationale for this approach:\n\nAppropriate Model for Prediction: Linear regression is a common statistical method used for predictive modeling, particularly when the outcome variable (in this case, the likelihood of default) is continuous. In the context of credit scoring, linear regression can be used to predict a risk score that represents the probability of default.\n\nUtilization of Labeled Data: Since you already have a labeled dataset containing information on loans that have been granted and whether they have defaulted, you can use this data to train the regression model. This historical data provides the model with examples of borrower characteristics and their corresponding default outcomes."
      },
      {
        "date": "2023-12-04T05:56:00.000Z",
        "voteCount": 1,
        "content": "B. Train a linear regression to predict a credit default risk score."
      },
      {
        "date": "2023-08-17T21:10:00.000Z",
        "voteCount": 1,
        "content": "What would be the target variable if B is correct i.e. training a linear regression model? Default/No-Default is a categorical variable one cannot train a linear regression model with this target variable"
      },
      {
        "date": "2023-07-28T05:42:00.000Z",
        "voteCount": 1,
        "content": "C - it is a typical approach in credit loans. \nKeeping only the accepted loans leads to a bias in the application"
      },
      {
        "date": "2023-07-23T04:04:00.000Z",
        "voteCount": 1,
        "content": "Linear regression is not the good way to solve such a problem, but you can totally apply linear regression to solve a classification problem. Just set the labels to numeric values 0 and 1 and linear regression will try to predict a value inbetween and round to the closest label (0 or 1). \n\nTotally not the way to go about it, but actually it's possible."
      },
      {
        "date": "2023-03-24T12:11:00.000Z",
        "voteCount": 1,
        "content": "Cannot be B. This is logistic regression, not linear regression.\n\nD is the only acceptable option.\nSocial profile can include things like high or low income, for example.\nWhen you apply for a credit you usually have to give this information, so totally legal."
      },
      {
        "date": "2024-09-25T02:11:00.000Z",
        "voteCount": 1,
        "content": "Credit scores are numbers, so this is a regression. Whether or not a client defaults could be a classification, but the option specifies the use of scores, which is fine."
      },
      {
        "date": "2023-04-25T08:19:00.000Z",
        "voteCount": 1,
        "content": "D. Matching loan applicants with their social profiles to enable feature engineering is not recommended as it raises privacy concerns and may not be legal in some jurisdictions. Additionally, social profiles may not be a good indicator of creditworthiness, and relying on them may introduce bias or discrimination."
      },
      {
        "date": "2023-03-01T19:59:00.000Z",
        "voteCount": 1,
        "content": "Because there is no option to know what dataset schema is even though B is needed for this question's purpose Nobody can't select B. So there is none to answer"
      },
      {
        "date": "2023-02-15T05:48:00.000Z",
        "voteCount": 1,
        "content": "all options are wrong: Still in favour of B\nA: ofc its good to have more data but its not clear how much data we have\nB: Linear can be a workable approach but current situation is not for linear approach, decision tree, random forest etc can be good for it. \nC: DAta should be unbiased, removing bias is negative for tranining"
      },
      {
        "date": "2023-01-22T07:07:00.000Z",
        "voteCount": 1,
        "content": "default rates can be predicted with linear regression."
      },
      {
        "date": "2023-12-04T07:09:00.000Z",
        "voteCount": 1,
        "content": "default rates is classification probability"
      },
      {
        "date": "2022-12-25T19:43:00.000Z",
        "voteCount": 2,
        "content": "Answer should actually be a logistic Regression model"
      },
      {
        "date": "2022-12-06T05:11:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-01T05:18:00.000Z",
        "voteCount": 2,
        "content": "The question asks about default RATES, as in you are predicting a continuous variable, not a discrete one (classification). This is a regression problem, so choice B."
      },
      {
        "date": "2022-11-08T07:39:00.000Z",
        "voteCount": 8,
        "content": "I used to be a Credit Risk modeler and I think this question is stupid."
      },
      {
        "date": "2022-10-20T06:12:00.000Z",
        "voteCount": 2,
        "content": "Data that you have is binary - Defaulted or not. You want default rates - Linear Regression. HOWEVER. The data that you have is for \"ALREADY GRANTED\" loan applications and whether they have defaulted or not.\nBut you want to \"train a model to predict default rates for credit applicants\", which would include applicants who would not be granted the loan. \nIf you just work on that dataset your model will not be as accurate as it wont have considered profiles of applicants that normally would not be granted those loans in the first place. Am I missing something here?"
      },
      {
        "date": "2022-10-20T06:12:00.000Z",
        "voteCount": 2,
        "content": "So answer should be C i think."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 92,
    "url": "https://www.examtopics.com/discussions/google/view/17257-exam-professional-data-engineer-topic-1-question-92/",
    "body": "You need to migrate a 2TB relational database to Google Cloud Platform. You do not have the resources to significantly refactor the application that uses this database and cost to operate is of primary concern.<br>Which service do you select for storing and serving your data?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Firestore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T08:39:00.000Z",
        "voteCount": 24,
        "content": "Answer - D"
      },
      {
        "date": "2020-03-27T21:51:00.000Z",
        "voteCount": 13,
        "content": "Answer: D\nDescription: Cloud SQl cheap and relational DB."
      },
      {
        "date": "2023-03-02T00:53:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL: max storage for shared core = 3TB and for dedicated core = up to 64TB\n\nOnly use Spanner if we need autoscale (Note that Cloud SQL could scale too but not automatic yet) or the size is too big (as above) or 4/5 9s HA (Cloud SQL is only 99.95)"
      },
      {
        "date": "2022-12-06T05:09:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-10-12T07:29:00.000Z",
        "voteCount": 1,
        "content": "Cloud  SQL is  relational DB (pg mssql, mysql)"
      },
      {
        "date": "2022-06-11T03:55:00.000Z",
        "voteCount": 1,
        "content": "Answer - D"
      },
      {
        "date": "2022-05-11T12:35:00.000Z",
        "voteCount": 1,
        "content": "answer D"
      },
      {
        "date": "2021-07-01T06:49:00.000Z",
        "voteCount": 3,
        "content": "Vote for D"
      },
      {
        "date": "2021-03-11T14:31:00.000Z",
        "voteCount": 1,
        "content": "D:\nhttps://cloud.google.com/sql/docs/features"
      },
      {
        "date": "2020-12-16T21:32:00.000Z",
        "voteCount": 3,
        "content": "D, cloud SQL is a relational database; if &gt; 10tb, then choose spanner"
      },
      {
        "date": "2020-08-22T00:20:00.000Z",
        "voteCount": 3,
        "content": "D\nCloud SQL supports MySQL 5.6 or 5.7, and provides up to 624 GB of RAM and 30 TB of data storage, with the option to automatically increase the storage size as needed."
      },
      {
        "date": "2021-11-23T21:01:00.000Z",
        "voteCount": 5,
        "content": "64TB AS OF TODAY"
      },
      {
        "date": "2020-08-20T17:57:00.000Z",
        "voteCount": 3,
        "content": "D is correct. Obviously"
      },
      {
        "date": "2020-04-18T11:28:00.000Z",
        "voteCount": 3,
        "content": "But cloud SQL storage is limited to several hundreds of GB's  for all instances \nand we need 2TB.\nSo, Cloud spanner is much closer to this, with the exception of the cost"
      },
      {
        "date": "2020-05-22T01:11:00.000Z",
        "voteCount": 5,
        "content": "Sorry ,  I think it's  D \nhttps://cloud.google.com/sql/docs/features\n(Cloud SQL supports MySQL 5.6 or 5.7, and provides up to 416 GB of RAM and 30 TB of data storage, with the option to automatically increase the storage size as needed.)"
      },
      {
        "date": "2020-12-11T07:47:00.000Z",
        "voteCount": 5,
        "content": "Another consideration is that Cloud SQL uses standard databases like MySQL, PostgreSQL and now MS SQL. Cloud Spanner is a proprietary product of Google and does some things differently than typical databases (no stored procedures and triggers). So migrating to Cloud Spanner makes application refactoring necessary. So Cloud SQL is the answer."
      },
      {
        "date": "2023-12-17T09:59:00.000Z",
        "voteCount": 1,
        "content": "Well explained I can confirmed."
      },
      {
        "date": "2020-04-27T21:34:00.000Z",
        "voteCount": 13,
        "content": "At this moment, Cloud SQL is providing up to 30,720GB(about 30TB)\nSo I think it's D."
      },
      {
        "date": "2024-02-24T10:55:00.000Z",
        "voteCount": 1,
        "content": "Nope. Now, the Dedicated core is Up to 64 TB\nhttps://cloud.google.com/sql/docs/quotas"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 93,
    "url": "https://www.examtopics.com/discussions/google/view/79773-exam-professional-data-engineer-topic-1-question-93/",
    "body": "You're using Bigtable for a real-time application, and you have a heavy load that is a mix of read and writes. You've recently identified an additional use case and need to perform hourly an analytical job to calculate certain statistics across the whole database. You need to ensure both the reliability of your production application as well as the analytical workload.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Bigtable dump to GCS and run your analytical job on top of the exported files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a second cluster to an existing instance with a multi-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a second cluster to an existing instance with a single-cluster routing, use live-traffic app profile for your regular workload and batch-analytics profile for the analytics workload.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of your existing cluster twice and execute your analytics workload on your new resized cluster."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T11:22:00.000Z",
        "voteCount": 22,
        "content": "Answer is C\n\nWhen you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\n\nhttps://cloud.google.com/bigtable/docs/replication-overview#use-cases"
      },
      {
        "date": "2024-08-21T07:44:00.000Z",
        "voteCount": 1,
        "content": "full example here: https://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve"
      },
      {
        "date": "2022-09-08T11:16:00.000Z",
        "voteCount": 2,
        "content": "Agreed :)"
      },
      {
        "date": "2022-11-10T20:58:00.000Z",
        "voteCount": 1,
        "content": "\"When you use a single cluster\", here we are creating a 2nd cluster, so we'll be using 2 different clusters. We want to redirect analysis jobs to the 2nd cluster, and the other job to the 1st cluster. Thus, I think that D is more adequate"
      },
      {
        "date": "2022-12-06T22:44:00.000Z",
        "voteCount": 2,
        "content": "Option D didnt say to create a new cluster, rather it said to increase the size of the cluster. There is a difference. Hence c is the correct answer to run the batch processing in a single cluster mode"
      },
      {
        "date": "2024-09-25T02:42:00.000Z",
        "voteCount": 1,
        "content": "To answer some confusion - \"single cluster routing\" is routing to one cluster per profile, rather than having failover options per profile. So we have two clusters, but it's not multicluster, because we have two profiles, so it's one cluster per profile, so \"single cluster routing\". We COULD use multicluster, but none of the answers give the steps required to do so, so the assumption as to be that we're using single."
      },
      {
        "date": "2024-09-25T02:43:00.000Z",
        "voteCount": 1,
        "content": "(an example of multicluster in this case would be 4 clusters, 2 for the transactional load and 2 for the analytical load)"
      },
      {
        "date": "2024-06-30T03:22:00.000Z",
        "voteCount": 1,
        "content": "B better than C. Multi-cluster routing to  handle failovers automatically. Reference: https://cloud.google.com/bigtable/docs/replication-settings#regional-failover"
      },
      {
        "date": "2024-03-27T04:02:00.000Z",
        "voteCount": 1,
        "content": "B is correct. \nTwo different job profiles to redirect trafiic to two different cluster. C is incorrect because there is no tpoint in creating app profile for two different workloads in the same cluster. One cluster handles writes and another handle reads."
      },
      {
        "date": "2023-12-09T00:53:00.000Z",
        "voteCount": 2,
        "content": "IIt is C:\n\"Workload isolation:\nUsing separate app profiles lets you use different routing policies for different purposes. For example, consider a situation when you want to prevent a batch read job (workload A) from increasing CPU usage on clusters that handle an application's steady reads and writes (workload B). You can create an app profile for workload B that routes to a cluster group that excludes one cluster. Then you create an app profile for workload A that specifies single-cluster routing to the cluster that workload B doesn't send requests to.\n\nYou can change the settings for one application or function without affecting other applications that connect to the same data.\"\nhttps://cloud.google.com/bigtable/docs/app-profiles"
      },
      {
        "date": "2023-07-14T05:46:00.000Z",
        "voteCount": 4,
        "content": "It was actually illustrated here\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve"
      },
      {
        "date": "2023-04-14T10:20:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve"
      },
      {
        "date": "2023-05-04T02:07:00.000Z",
        "voteCount": 1,
        "content": "I see what you say on C but the question states high availability how do you handle that with option C when you have a single region cluster hence answer needs to be with multi-region cluster - To configure your instance for a high availability (HA) use case, create a new app profile that uses multi-cluster routing, or update the default app profile to use multi-cluster routing."
      },
      {
        "date": "2024-04-24T03:18:00.000Z",
        "voteCount": 1,
        "content": "It actually addresses the issue of High availability in that same link if you scroll down a bit more. \nhttps://cloud.google.com/bigtable/docs/replication-settings#high-availability"
      },
      {
        "date": "2023-05-04T02:08:00.000Z",
        "voteCount": 1,
        "content": "i meant single-cluster routing"
      },
      {
        "date": "2023-03-24T12:31:00.000Z",
        "voteCount": 3,
        "content": "C. This is exactly the example in the documentation.\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve"
      },
      {
        "date": "2023-04-14T10:18:00.000Z",
        "voteCount": 1,
        "content": "Correct \n2 jobs &gt;&gt; 2 cluster\n3 jobs &gt;&gt; 3 cluster\napp profiles with single-cluster routing used to route to specific cluster\nJob1 &gt;&gt; Cluster 1\nJob2 &gt;&gt; Cluster 2 ....."
      },
      {
        "date": "2023-02-15T07:45:00.000Z",
        "voteCount": 1,
        "content": "Answer B: \nreason 1: If you don' t have any cost constraint use multi-cluster routing, \nreason 2: Single cluster is less scalable as we need high scalability i would go with B"
      },
      {
        "date": "2023-02-05T11:36:00.000Z",
        "voteCount": 1,
        "content": "I am going for C?"
      },
      {
        "date": "2022-12-20T23:46:00.000Z",
        "voteCount": 2,
        "content": "When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\n\nSingle cluster routing - You can use single-cluster routing for this use case if you don't want your Bigtable cluster to automatically fail over if a zone or region becomes unavailable. \n\nMulti-cluster routing - If you want Bigtable to automatically fail over to one region if your application cannot reach the other region, use multi-cluster routing."
      },
      {
        "date": "2022-12-06T05:08:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve\nWhen you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users."
      },
      {
        "date": "2022-12-03T09:44:00.000Z",
        "voteCount": 2,
        "content": "Answer is C\n\n\"When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\"\n\nhttps://cloud.google.com/bigtable/docs/replication-overview#batch-vs-serve"
      },
      {
        "date": "2022-11-24T06:06:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-11-21T23:44:00.000Z",
        "voteCount": 3,
        "content": "Option B is correct\n\nAn app profile specifies the routing policy that Bigtable should use for each request.\n\nSingle-cluster routing routes all requests to 1 cluster in your instance. If that cluster becomes unavailable, you must manually fail over to another cluster.\n\nMulti-cluster routing automatically routes requests to the nearest cluster in an instance. If the cluster becomes unavailable, traffic automatically fails over to the nearest cluster that is available. Bigtable considers clusters in a single region to be equidistant, even though they are in different zones. You can configure an app profile to route to any cluster in an instance, or you can specify a cluster group that tells the app profile to route to only some of the clusters in the instance.\n\nCluster group routing sends requests to the nearest available cluster within a cluster group that you specify in the app profile settings.\n\nReference:-https://cloud.google.com/bigtable/docs/app-profiles#routing"
      },
      {
        "date": "2022-11-21T05:49:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigtable/docs/replication-settings#batch-vs-serve\n\n\n\"When you use a single cluster to run a batch analytics job that performs numerous large reads alongside an application that performs a mix of reads and writes, the large batch job can slow things down for the application's users. With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\"\n\nIt is C."
      },
      {
        "date": "2022-11-18T02:56:00.000Z",
        "voteCount": 1,
        "content": "C - \"With replication, you can use app profiles with single-cluster routing to route batch analytics jobs and application traffic to different clusters, so that batch jobs don't affect your applications' users.\" - https://cloud.google.com/bigtable/docs/replication-overview#batch-vs-serve"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 94,
    "url": "https://www.examtopics.com/discussions/google/view/16841-exam-professional-data-engineer-topic-1-question-94/",
    "body": "You are designing an Apache Beam pipeline to enrich data from Cloud Pub/Sub with static reference data from BigQuery. The reference data is small enough to fit in memory on a single worker. The pipeline should write enriched results to BigQuery for analysis. Which job type and transforms should this pipeline use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch job, PubSubIO, side-inputs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming job, PubSubIO, JdbcIO, side-outputs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming job, PubSubIO, BigQueryIO, side-inputs\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStreaming job, PubSubIO, BigQueryIO, side-outputs"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T01:34:00.000Z",
        "voteCount": 31,
        "content": "Why not C? Without BigQueryIO how can data be written back to BigQuery?"
      },
      {
        "date": "2020-03-20T05:02:00.000Z",
        "voteCount": 8,
        "content": "C should be right"
      },
      {
        "date": "2020-03-27T22:02:00.000Z",
        "voteCount": 15,
        "content": "Answer: C\nDescription: Sideinput for Bigquery data"
      },
      {
        "date": "2023-12-26T15:15:00.000Z",
        "voteCount": 1,
        "content": "Side inputs\nIn addition to the main input PCollection, you can provide additional inputs to a ParDo transform in the form of side inputs. A side input is an additional input that your DoFn can access each time it processes an element in the input PCollection. When you specify a side input, you create a view of some other data that can be read from within the ParDo transform\u2019s DoFn while processing each element.\n\nSide inputs are useful if your ParDo needs to inject additional data when processing each element in the input PCollection, but the additional data needs to be determined at runtime (and not hard-coded). Such values might be determined by the input data, or depend on a different branch of your pipeline."
      },
      {
        "date": "2023-12-26T15:16:00.000Z",
        "voteCount": 1,
        "content": "https://beam.apache.org/documentation/programming-guide/#side-inputs"
      },
      {
        "date": "2023-08-15T12:17:00.000Z",
        "voteCount": 1,
        "content": "Why not side-output?"
      },
      {
        "date": "2023-08-03T22:56:00.000Z",
        "voteCount": 1,
        "content": "B. Use multi-cluster routing to add a second cluster to the existing instance, utilizing a live traffic app profile for the regular workload and a batch analytics profile for the analytical workload."
      },
      {
        "date": "2023-07-23T04:12:00.000Z",
        "voteCount": 2,
        "content": "The answer is C. It's a trap so that you answer A because of batch vs streaming but you need BigQueryIO. On the other hand, streaming is absolutely redundant here and will incur extra costs. C is right but would be better with batch."
      },
      {
        "date": "2023-01-02T14:49:00.000Z",
        "voteCount": 1,
        "content": "A is the Answer.\n A. Batch job, PubSubIO, side-inputs"
      },
      {
        "date": "2022-12-06T05:00:00.000Z",
        "voteCount": 4,
        "content": "C is the answer.\n\nhttps://cloud.google.com/dataflow/docs/tutorials/ecommerce-java#side-input-pattern\nIn streaming analytics applications, data is often enriched with additional information that might be useful for further analysis. For example, if you have the store ID for a transaction, you might want to add information about the store location. This additional information is often added by taking an element and bringing in information from a lookup table."
      },
      {
        "date": "2022-09-21T10:20:00.000Z",
        "voteCount": 2,
        "content": "I got this question on sept 2022. Answer is C"
      },
      {
        "date": "2022-10-25T14:30:00.000Z",
        "voteCount": 1,
        "content": "dear can you please help, i have some questions about how to prepare the cerification exam using this questionnaire. this is my email cmayola@yahoo.fr, ping me to have some conversation"
      },
      {
        "date": "2022-02-03T06:04:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2022-01-06T11:57:00.000Z",
        "voteCount": 5,
        "content": "I vote for C, because data will come from Pub/Sub, so it should be streaming, we'll need PubSubIO to be able to read from PubSub and BigQueryIO to be able to write to BigQuery, finally the side-inputs pattern let us enrich data"
      },
      {
        "date": "2021-12-28T10:52:00.000Z",
        "voteCount": 4,
        "content": "Static reference data from BigQuery will go as side-inputs and data from pub-sub will go as streaming data using PubSubIO and finally BigQueryIO is required to push the final data to BigQuery"
      },
      {
        "date": "2021-11-26T19:34:00.000Z",
        "voteCount": 1,
        "content": "Ans: C"
      },
      {
        "date": "2021-08-27T03:00:00.000Z",
        "voteCount": 6,
        "content": "Answer is C,\nYou need pubsubIO and BigQueryIO for streaming data and writing enriched data back to BigQuery. side-inputs are a way to enrich the data\nhttps://cloud.google.com/architecture/e-commerce/patterns/slow-updating-side-inputs"
      },
      {
        "date": "2021-08-18T17:26:00.000Z",
        "voteCount": 3,
        "content": "I choose C, because data will come from Pub/Sub, so it should be streaming, we'll need PubSubIO to be able to read from PubSub y BigQueryIO to be able to write to BigQuery, finally the side-inputs pattern let us enrich data\nhttps://beam.apache.org/releases/javadoc/2.4.0/org/apache/beam/sdk/io/gcp/pubsub/PubsubIO.html\nhttps://cloud.google.com/architecture/e-commerce/patterns/slow-updating-side-inputs\nhttps://beam.apache.org/releases/javadoc/2.3.0/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIO.html"
      },
      {
        "date": "2021-03-11T14:46:00.000Z",
        "voteCount": 2,
        "content": "C:\nwe have to use Streaming job because of Pub/Sub, and side-input thanks to static reference data. and we have to leverage BigQueryIO since finally we want to write data to BigQuery. then C is the correct answer."
      },
      {
        "date": "2021-02-19T04:41:00.000Z",
        "voteCount": 1,
        "content": "Correct A. batch is cost-effective and no need to go for streaming"
      },
      {
        "date": "2021-02-19T14:20:00.000Z",
        "voteCount": 1,
        "content": "How you are going to write back to BQ?"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 95,
    "url": "https://www.examtopics.com/discussions/google/view/16842-exam-professional-data-engineer-topic-1-question-95/",
    "body": "You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys. You want to monitor your pipeline to determine when to increase the size of your Cloud Bigtable cluster. Which two actions can you take to accomplish this? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Read pressure index is above 100.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReview Key Visualizer metrics. Increase the size of the Cloud Bigtable cluster when the Write pressure index is above 100.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor the latency of write operations. Increase the size of the Cloud Bigtable cluster when there is a sustained increase in write latency.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor storage utilization. Increase the size of the Cloud Bigtable cluster when utilization increases above 70% of max capacity.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor latency of read operations. Increase the size of the Cloud Bigtable cluster of read operations take longer than 100 ms."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-20T03:06:00.000Z",
        "voteCount": 53,
        "content": "Answer is C &amp; D.\nC \u2013&gt; Adding more nodes to a cluster (not replication) can improve the write performance https://cloud.google.com/bigtable/docs/performance\nD \u2013&gt; since Google recommends adding nodes when storage utilization is &gt; 70% https://cloud.google.com/bigtable/docs/modifying-instance#nodes"
      },
      {
        "date": "2021-10-04T23:11:00.000Z",
        "voteCount": 1,
        "content": "Adding nodes to the cluster In Bigtable scales linearly the performances both read and write\nhttps://cloud.google.com/bigtable/docs/performance#typical-workloads"
      },
      {
        "date": "2020-10-06T08:07:00.000Z",
        "voteCount": 4,
        "content": "Storage utilization (% max)\t\nThe percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.\n\nIn general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit.\n\nImportant: If any cluster in an instance exceeds the hard limit on the amount of storage per node, writes to all clusters in that instance will fail until you add nodes to each cluster that is over the limit. Also, if you try to remove nodes from a cluster, and the change would cause the cluster to exceed the hard limit on storage, Cloud Bigtable will deny the request.\nIf you are using more than the recommended percentage of the storage limit, add nodes to the cluster. You can also delete existing data, but deleted data takes up more space, not less, until a compaction occurs."
      },
      {
        "date": "2020-10-06T08:07:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigtable/docs/monitoring-instance"
      },
      {
        "date": "2020-04-18T13:20:00.000Z",
        "voteCount": 10,
        "content": "Key visualizer is bigtable metric , So  A and B incorrect\nstorage utilization also bigtable metric , So D incorrect\nThe question want you to monitor pipeline metrics (which is dataflow metrics) , in our case we can only monitor latency .\nThe answer will be :  C  &amp;   E"
      },
      {
        "date": "2020-06-21T05:55:00.000Z",
        "voteCount": 14,
        "content": "No. it is C, D. \"You have a data pipeline that writes data to Cloud Bigtable using well-designed row keys.\"\nwhy are you monitoring read anyway? you are just writing."
      },
      {
        "date": "2024-05-20T01:53:00.000Z",
        "voteCount": 1,
        "content": "The question focus is on writing. BC is correct. when the writing pressure is above 100, it is time to increase. same logic with C"
      },
      {
        "date": "2023-02-15T08:18:00.000Z",
        "voteCount": 1,
        "content": "why not B ?"
      },
      {
        "date": "2023-02-24T07:24:00.000Z",
        "voteCount": 2,
        "content": "i am feeling to go with B and D. In option C, when latency is low, latency can be low for write operation for other reason. \nbut in option B, its showing clearly when write pressure more than 100. But why no one is talking about B"
      },
      {
        "date": "2023-01-21T20:37:00.000Z",
        "voteCount": 3,
        "content": "Key visualizer is Metrics for Performance issues. Ruled out\nStorage and Write Operations ; C and D"
      },
      {
        "date": "2022-12-06T04:56:00.000Z",
        "voteCount": 3,
        "content": "CD is the answer.\n\nhttps://cloud.google.com/bigtable/docs/monitoring-instance#disk\nStorage utilization (% max)\t\n- The percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.\nIn general, do not use more than 70% of the hard limit on total storage, so you have room to add more data."
      },
      {
        "date": "2022-09-15T02:16:00.000Z",
        "voteCount": 2,
        "content": "Well-designed row key : A B are not nessary\nWrite : CD both are involved in the question the most."
      },
      {
        "date": "2022-07-04T01:41:00.000Z",
        "voteCount": 2,
        "content": "Answer: CD\nhttps://cloud.google.com/bigtable/docs/scaling"
      },
      {
        "date": "2022-01-06T12:01:00.000Z",
        "voteCount": 2,
        "content": "as explained by  MaxNRG"
      },
      {
        "date": "2021-12-28T10:55:00.000Z",
        "voteCount": 3,
        "content": "D: In general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit\nC: If this value is frequently at 100%, you might experience increased latency. Add nodes to the cluster to reduce the disk load percentage.\nThe key visualizer metrics options, suggest other things other than increase the cluster size.\nhttps://cloud.google.com/bigtable/docs/monitoring-instance"
      },
      {
        "date": "2021-12-18T00:41:00.000Z",
        "voteCount": 1,
        "content": "CD.\n\nI agree with jvg637"
      },
      {
        "date": "2021-11-29T09:18:00.000Z",
        "voteCount": 2,
        "content": "from https://cloud.google.com/bigtable/docs/monitoring-instance\nDisk load - If this value is frequently at 100%, you might experience increased latency. Add nodes to the cluster to reduce the disk load percentage.\nStorage utilization (% max)\t- In general, do not use more than 70% of the hard limit on total storage, so you have room to add more data. If you do not plan to add significant amounts of data to your instance, you can use up to 100% of the hard limit."
      },
      {
        "date": "2021-11-10T18:39:00.000Z",
        "voteCount": 2,
        "content": "I am Voting for CD"
      },
      {
        "date": "2021-10-06T07:59:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D &amp; E"
      },
      {
        "date": "2022-04-07T03:30:00.000Z",
        "voteCount": 1,
        "content": "Why are you monitoring read operations, when youre supposed to write? why E?"
      },
      {
        "date": "2021-10-04T23:03:00.000Z",
        "voteCount": 2,
        "content": "D--&gt; 70% is the the recommended percentage of the cluster's storage capacity that is being used, If you are using more than 70% storage limit, add nodes to the cluster \nhttps://cloud.google.com/bigtable/quotas#storage-per-node\nhttps://cloud.google.com/bigtable/docs/monitoring-instance#disk\nE--&gt; 100 ms is an order of magnitude lower latency than Google claimed (&lt;10ms)\nhttps://cloud.google.com/bigtable/docs/performance#typical-workloads"
      },
      {
        "date": "2021-08-20T01:11:00.000Z",
        "voteCount": 1,
        "content": "BC\nD:you can just add node, not cluster\nThe percentage of the cluster's storage capacity that is being used. The capacity is based on the number of nodes in your cluster.(https://cloud.google.com/bigtable/docs/monitoring-instance)\nAfter you create a Cloud Bigtable instance, you can update any of the following settings without any downtime:\n\n(The number of nodes in each cluster)\nhttps://cloud.google.com/bigtable/docs/modifying-instance"
      },
      {
        "date": "2021-07-01T07:57:00.000Z",
        "voteCount": 2,
        "content": "B, C , D - all three looks okay to me"
      },
      {
        "date": "2021-07-01T07:58:00.000Z",
        "voteCount": 3,
        "content": "Vote for C &amp; D, \nOption B eliminated, as Row are well defined (as per question) - so no need of key-visualizer"
      },
      {
        "date": "2021-10-17T18:12:00.000Z",
        "voteCount": 2,
        "content": "Answer is C, D. \nB is not correct, because B is Key Visualizer, it means the row key needs re-design again."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 96,
    "url": "https://www.examtopics.com/discussions/google/view/16843-exam-professional-data-engineer-topic-1-question-96/",
    "body": "You want to analyze hundreds of thousands of social media posts daily at the lowest cost and with the fewest steps.<br>You have the following requirements:<br>\u2711 You will batch-load the posts once per day and run them through the Cloud Natural Language API.<br>\u2711 You will extract topics and sentiment from the posts.<br>\u2711 You must store the raw posts for archiving and reprocessing.<br>\u2711 You will create dashboards to be shared with people both inside and outside your organization.<br>You need to store both the data extracted from the API to perform analysis as well as the raw social media posts for historical archiving. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the social media posts and the data extracted from the API in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the social media posts and the data extracted from the API in Cloud SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the raw social media posts in Cloud Storage, and write the data extracted from the API into BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFeed to social media posts into the API directly from the source, and write the extracted data from the API into BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-27T22:17:00.000Z",
        "voteCount": 46,
        "content": "Answer: C\nDescription: Social media posts can images/videos which cannot be stored in bigquery"
      },
      {
        "date": "2021-10-18T17:25:00.000Z",
        "voteCount": 1,
        "content": "Yes, the raw data needs to be archived too"
      },
      {
        "date": "2020-07-06T10:34:00.000Z",
        "voteCount": 4,
        "content": "but the posts are fed into cloud natural language api. which means we have to consider the posts to be text only"
      },
      {
        "date": "2021-08-27T02:08:00.000Z",
        "voteCount": 1,
        "content": "Also to run batch queries data needs to be in Cloud Storage, so why not just store it there?"
      },
      {
        "date": "2020-04-13T12:45:00.000Z",
        "voteCount": 17,
        "content": "Answer should be C, becose they ask you to save a copy of the raw posts for archival, which may not be possible if you directly feed the posts to the API."
      },
      {
        "date": "2023-02-28T23:40:00.000Z",
        "voteCount": 2,
        "content": "can any one help me with the rest of question from 101 to 209 as i dont have a contributor access"
      },
      {
        "date": "2022-12-06T04:51:00.000Z",
        "voteCount": 2,
        "content": "C is the answer."
      },
      {
        "date": "2022-09-21T10:22:00.000Z",
        "voteCount": 5,
        "content": "I got this question on sept 2022. Answer is C"
      },
      {
        "date": "2022-09-09T07:09:00.000Z",
        "voteCount": 1,
        "content": "C is the correct one"
      },
      {
        "date": "2022-01-06T12:02:00.000Z",
        "voteCount": 2,
        "content": "Only C make sense."
      },
      {
        "date": "2021-12-30T07:50:00.000Z",
        "voteCount": 3,
        "content": "You must store the raw posts for archiving and reprocessing, Store the raw social media posts in Cloud Storage.\nB is expensive\nD is not valid since you have to store the raw posts for archiving\nBetween A and C I\u2019s say C, since we\u2019re going to make dashboards and Data Studio will connect well with big query.\nand besides A would probably be more expensive."
      },
      {
        "date": "2021-12-04T10:08:00.000Z",
        "voteCount": 4,
        "content": "SAY MY NAME!"
      },
      {
        "date": "2021-11-30T01:32:00.000Z",
        "voteCount": 2,
        "content": "Analysis BQ\nStorage GCS"
      },
      {
        "date": "2021-08-17T03:11:00.000Z",
        "voteCount": 1,
        "content": "I believe the API accesses data only from GCS Buckets not BigQuery (but I'm not entirely sure)"
      },
      {
        "date": "2021-07-01T09:34:00.000Z",
        "voteCount": 2,
        "content": "Vote for C"
      },
      {
        "date": "2021-01-16T18:01:00.000Z",
        "voteCount": 2,
        "content": "Answer should be C because we need to consider storage archival"
      },
      {
        "date": "2020-11-18T02:32:00.000Z",
        "voteCount": 2,
        "content": "I'll go with option C"
      },
      {
        "date": "2020-11-09T03:56:00.000Z",
        "voteCount": 3,
        "content": "I will go with Option C, because of the following reasons:-\na) Social media posts are \"raw\" - which means - it can be of any format (blob/object storage) is preferred. \nb) The output from the application (assuming the application is Cloud NLP) is to be future stored for archival purpose - and hence again Google Cloud storage is the best option - so option C\nOption A &amp;C  - Incorrect, although Option D fulfils the requirement of \"fewest step\" but storing data in big query for archival purpose is not a google recommended approach \nOption B : Cloud SQL rules out as it does not solve either for archival storage or for analytics purpose."
      },
      {
        "date": "2020-09-24T15:25:00.000Z",
        "voteCount": 1,
        "content": "cost of long term storing is almost same in GCS and BQ, so answer D makes sense from that angle.."
      },
      {
        "date": "2020-09-08T03:50:00.000Z",
        "voteCount": 2,
        "content": "The job is supposed to run in batch process once in a day , so there is no requirement of stream data. The best economical and less complex steps is answer C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 97,
    "url": "https://www.examtopics.com/discussions/google/view/16109-exam-professional-data-engineer-topic-1-question-97/",
    "body": "You store historic data in Cloud Storage. You need to perform analytics on the historic data. You want to use a solution to detect invalid data entries and perform data transformations that will not require programming or knowledge of SQL.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow with Beam to detect errors and perform transformations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataprep with recipes to detect errors and perform transformations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataproc with a Hadoop job to detect errors and perform transformations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse federated tables in BigQuery with queries to detect errors and perform transformations."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-10T07:10:00.000Z",
        "voteCount": 56,
        "content": "Use Dataprep ! It's THE tool for this"
      },
      {
        "date": "2020-03-17T01:53:00.000Z",
        "voteCount": 53,
        "content": "Yes B. \n\nHonest speaking, sometime I thought the answers being posted here were intentionally to mislead people whose do not have proper knowledge on the subject, but just memorizing answers to pass the exam."
      },
      {
        "date": "2021-01-28T18:47:00.000Z",
        "voteCount": 6,
        "content": "True.. might be legal issue?!"
      },
      {
        "date": "2023-09-06T19:13:00.000Z",
        "voteCount": 1,
        "content": "A is the right way to do it... dataprepo is clumsy"
      },
      {
        "date": "2023-10-12T17:36:00.000Z",
        "voteCount": 7,
        "content": "...Did you even read through the question? It says \"not require programming or knowledge of SQL\". YOU are the one who's clumsy, not dataprep."
      },
      {
        "date": "2023-08-03T14:06:00.000Z",
        "voteCount": 1,
        "content": "no programming -&gt; B"
      },
      {
        "date": "2023-07-30T10:26:00.000Z",
        "voteCount": 2,
        "content": "I honestly do not understand what is the deal with this website. The correct answer is obviously Dataprep. How can they say it's A?"
      },
      {
        "date": "2023-01-14T02:25:00.000Z",
        "voteCount": 2,
        "content": "It'B. DataPrep it's the right tool. \nhttps://cloud.google.com/dataprep"
      },
      {
        "date": "2022-12-06T04:50:00.000Z",
        "voteCount": 1,
        "content": "B is the answer.\n\nhttps://cloud.google.com/dataprep\nDataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning."
      },
      {
        "date": "2022-09-21T10:23:00.000Z",
        "voteCount": 4,
        "content": "I got this question on sept 2022."
      },
      {
        "date": "2022-09-14T23:28:00.000Z",
        "voteCount": 2,
        "content": "B. Actually there are two tools to fix this problem. \nDataprep rely on dataflw\nDatafusion  rely on dataproc"
      },
      {
        "date": "2024-09-25T03:00:00.000Z",
        "voteCount": 1,
        "content": "Datafusion is lo-code but not no-code. The only no-code system is dataprep."
      },
      {
        "date": "2022-05-09T04:18:00.000Z",
        "voteCount": 1,
        "content": "A is the best answer !"
      },
      {
        "date": "2023-01-17T16:12:00.000Z",
        "voteCount": 1,
        "content": "dataflow  IS Apache Beam..."
      },
      {
        "date": "2022-02-24T09:55:00.000Z",
        "voteCount": 2,
        "content": "B Dataprep"
      },
      {
        "date": "2022-01-06T12:03:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dataprep/"
      },
      {
        "date": "2021-12-30T07:42:00.000Z",
        "voteCount": 6,
        "content": "B, \u201cCloud Dataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning\u201d\nhttps://cloud.google.com/dataprep/"
      },
      {
        "date": "2022-07-20T04:06:00.000Z",
        "voteCount": 2,
        "content": "max you rock man!"
      },
      {
        "date": "2021-10-22T16:40:00.000Z",
        "voteCount": 3,
        "content": "Answer is B. Data prep. The keyword here is no programming skills required."
      },
      {
        "date": "2021-09-16T21:07:00.000Z",
        "voteCount": 2,
        "content": "B- Dataprep"
      },
      {
        "date": "2021-08-06T00:37:00.000Z",
        "voteCount": 2,
        "content": "Use Dataprep ....is the answer"
      },
      {
        "date": "2021-07-01T09:36:00.000Z",
        "voteCount": 2,
        "content": "Vote for B"
      },
      {
        "date": "2021-07-01T09:58:00.000Z",
        "voteCount": 1,
        "content": "Cloud Dataprep - almost fully automated"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 98,
    "url": "https://www.examtopics.com/discussions/google/view/17255-exam-professional-data-engineer-topic-1-question-98/",
    "body": "Your company needs to upload their historic data to Cloud Storage. The security rules don't allow access from external IPs to their on-premises resources. After an initial upload, they will add new data from existing on-premises applications every day. What should they do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExecute gsutil rsync from the on-premises servers.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow and write the data to Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a job template in Dataproc to perform the data transfer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall an FTP server on a Compute Engine VM to receive the files and move them to Cloud Storage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T08:23:00.000Z",
        "voteCount": 24,
        "content": "Should be A"
      },
      {
        "date": "2020-04-19T08:22:00.000Z",
        "voteCount": 14,
        "content": "should be A, dataflow is on cloud is external; \"don't allow access from external IPs to their on-premises resources\" so no dataflow."
      },
      {
        "date": "2024-07-26T09:56:00.000Z",
        "voteCount": 1,
        "content": "The gcloud storage command is the standard tool for small- to medium-sized transfers over a typical enterprise-scale network, from a private data center or from another cloud provider to Google Cloud."
      },
      {
        "date": "2023-01-14T02:29:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-06T04:46:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data\nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud. We recommend that you include gsutil in your default path when you use Cloud Shell. It's also available by default when you install the Google Cloud CLI. It's a reliable tool that provides all the basic features you need to manage your Cloud Storage instances, including copying your data to and from the local file system and Cloud Storage. It can also move and rename objects and perform real-time incremental syncs, like rsync, to a Cloud Storage bucket."
      },
      {
        "date": "2022-10-12T03:15:00.000Z",
        "voteCount": 2,
        "content": "Should be A"
      },
      {
        "date": "2022-01-06T12:05:00.000Z",
        "voteCount": 1,
        "content": "Without this \"The security rules don't allow access from external IPs to their on-premises resources\" B would be an answer."
      },
      {
        "date": "2021-12-30T07:44:00.000Z",
        "voteCount": 6,
        "content": "A is the better and most simple IF there is no problem in having gsutil in our servers.\nB and C no way, the comms will go GCP\u2013Home, which sais is not allowed.\nD is valid, we can send the files with http://ftp\u2026BUT ftp is not secure, and we\u2019ll need to move them to the cloud storage afterwards, which is not detailed in the answer.\nhttps://cloud.google.com/storage/docs/gsutil/commands/rsync"
      },
      {
        "date": "2021-12-24T07:42:00.000Z",
        "voteCount": 1,
        "content": "I am confused . which one is correct A or B ???"
      },
      {
        "date": "2021-12-18T16:22:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2021-10-16T11:16:00.000Z",
        "voteCount": 2,
        "content": "This is the link:https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data"
      },
      {
        "date": "2021-08-30T08:28:00.000Z",
        "voteCount": 3,
        "content": "How rsynch will handle private network?\n \"..The security rules don't allow access from external IPs to their on-premises resources..\""
      },
      {
        "date": "2021-07-01T10:17:00.000Z",
        "voteCount": 2,
        "content": "Vote for A"
      },
      {
        "date": "2021-03-11T15:22:00.000Z",
        "voteCount": 4,
        "content": "A:\nhttps://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets#options_available_from_google"
      },
      {
        "date": "2021-12-03T08:59:00.000Z",
        "voteCount": 1,
        "content": "How could gsutil connect to Cloud Storage, if there is not access from external IPs?  Should I understand that there is not access from outside to inside, but it is possible to send from inside to outside?"
      },
      {
        "date": "2021-12-18T03:20:00.000Z",
        "voteCount": 3,
        "content": "Yes. There is no access to on-prem from external IPs, but on prem can talk to external"
      },
      {
        "date": "2020-08-22T03:57:00.000Z",
        "voteCount": 4,
        "content": "gsutil rsync will be used to transfer the files ANS A"
      },
      {
        "date": "2020-08-21T04:08:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2020-07-07T05:06:00.000Z",
        "voteCount": 6,
        "content": "Ans : A\nThe gsutil rsync command makes the contents under dst_url the same as the contents under src_url, by copying any missing files/objects (or those whose data has changed), and (if the -d option is specified) deleting any extra files/objects. src_url must specify a directory, bucket, or bucket subdirectory"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 99,
    "url": "https://www.examtopics.com/discussions/google/view/16845-exam-professional-data-engineer-topic-1-question-99/",
    "body": "You have a query that filters a BigQuery table using a WHERE clause on timestamp and ID columns. By using bq query `\"-dry_run you learn that the query triggers a full scan of the table, even though the filter on timestamp and ID select a tiny fraction of the overall data. You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate table for each ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the LIMIT keyword to reduce the number of rows returned.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecreate the table with a partitioning column and clustering column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the bq query --maximum_bytes_billed flag to restrict the number of bytes billed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T02:05:00.000Z",
        "voteCount": 43,
        "content": "should be C:\n\nhttps://cloud.google.com/bigquery/docs/best-practices-costs"
      },
      {
        "date": "2020-03-22T08:28:00.000Z",
        "voteCount": 17,
        "content": "Correct - C"
      },
      {
        "date": "2022-12-05T05:49:00.000Z",
        "voteCount": 4,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nA partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nlustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs."
      },
      {
        "date": "2022-07-04T02:23:00.000Z",
        "voteCount": 3,
        "content": "C is the answer\nhttps://cloud.google.com/bigquery/docs/best-practices-costs"
      },
      {
        "date": "2022-01-06T12:07:00.000Z",
        "voteCount": 2,
        "content": "C only make sense"
      },
      {
        "date": "2021-12-30T07:45:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/bigquery/docs/best-practices-costs\nApplying a LIMIT clause to a SELECT * query does not affect the amount of data read. You are billed for reading all bytes in the entire table, and the query counts against your free tier quota.\nA and D doesnt make sense\nIts C, when you want to select by a partition you should write something like:\nCREATE TABLE `blablabla.partitioned`\nPARTITION BY\nDATE(timestamp)\nCLUSTER BY id\nAS\nSELECT * FROM `blablabla`"
      },
      {
        "date": "2021-12-14T09:03:00.000Z",
        "voteCount": 3,
        "content": "this is a trap to make people fail by giving wrong answer as B."
      },
      {
        "date": "2021-12-02T03:55:00.000Z",
        "voteCount": 1,
        "content": "It's D,  here is the link\nhttps://cloud.google.com/bigquery/docs/best-practices-costs"
      },
      {
        "date": "2021-12-03T09:02:00.000Z",
        "voteCount": 1,
        "content": "Well, you mean C, isn't it?"
      },
      {
        "date": "2021-11-09T13:21:00.000Z",
        "voteCount": 5,
        "content": "Are they having a laugh at us by giving so many bad answers?"
      },
      {
        "date": "2021-10-20T14:01:00.000Z",
        "voteCount": 1,
        "content": "Answer: B\nNote: minimal change to sql"
      },
      {
        "date": "2021-11-03T14:17:00.000Z",
        "voteCount": 3,
        "content": "Not B. LIMIT will not reduce amount of data scanned - only limit the final output, but you will still be billed for scanning whole table.\nC is correct. After applying partitioning ans clustering amount of bytes scanned will decrease"
      },
      {
        "date": "2021-09-28T07:55:00.000Z",
        "voteCount": 2,
        "content": "\"You want to reduce the amount of data scanned by BigQuery with minimal changes to existing SQL queries\" that doesn't mean that you can create or edit existing tables ! you only can edit the SQL query !!! so answer D is the correct one."
      },
      {
        "date": "2021-11-03T14:19:00.000Z",
        "voteCount": 1,
        "content": "I don't agree. Question says \"minimal changes to existing SQL queries\" - if you recreate table with partitioning and clustering you don't need to change SQLs that read from that table.\nC is correct answer here."
      },
      {
        "date": "2021-10-17T17:53:00.000Z",
        "voteCount": 1,
        "content": "D would just block your query. The answer is C."
      },
      {
        "date": "2021-09-16T21:22:00.000Z",
        "voteCount": 2,
        "content": "C - create partition table"
      },
      {
        "date": "2021-07-01T10:19:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2020-12-15T11:16:00.000Z",
        "voteCount": 4,
        "content": "LIMIT keyword is applied only at the end, i.e., only to limit the results already calculated. Therefore, a full table scan will have already happened. The where clause on the other hand would provide the desired filtering depending on the case. So, C is the correct answer."
      },
      {
        "date": "2020-11-26T09:48:00.000Z",
        "voteCount": 2,
        "content": "Not sure, why option C selected! The correct Answer is B. the question clearly says \"minimal changes to existing SQL queries\". who said that, recreate the table, with partitioning layout is minimal and is PART of SQL queries!"
      },
      {
        "date": "2021-07-27T03:52:00.000Z",
        "voteCount": 1,
        "content": "In addition to the previous reply, the LIMIT statement applies to the output (what you see in the UI), the full table scan will still happen. C is correct according to best practices."
      },
      {
        "date": "2020-11-30T08:18:00.000Z",
        "voteCount": 5,
        "content": "recreating table will not affect existing sql queries as they will still be selecting the same table name, but the scan will hugely decrease. so, option C is the correct answer."
      },
      {
        "date": "2021-10-17T17:55:00.000Z",
        "voteCount": 1,
        "content": "Recreating table is recommended by Google."
      },
      {
        "date": "2020-11-18T03:06:00.000Z",
        "voteCount": 2,
        "content": "should be C:"
      },
      {
        "date": "2020-09-16T11:34:00.000Z",
        "voteCount": 3,
        "content": "Correct - C :\n\"Limit\" keyword restricts the final dataset to \"n\" rows, but is not able to restrict full table scan"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 100,
    "url": "https://www.examtopics.com/discussions/google/view/16982-exam-professional-data-engineer-topic-1-question-100/",
    "body": "You have a requirement to insert minute-resolution data from 50,000 sensors into a BigQuery table. You expect significant growth in data volume and need the data to be available within 1 minute of ingestion for real-time analysis of aggregated trends. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse bq load to load a batch of sensor data every 60 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Dataflow pipeline to stream data into the BigQuery table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the INSERT statement to insert a batch of data every 60 seconds.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the MERGE statement to apply updates in batch every 60 seconds."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-19T09:39:00.000Z",
        "voteCount": 29,
        "content": "I think we need a pipeline, so it's B to me."
      },
      {
        "date": "2020-03-22T08:31:00.000Z",
        "voteCount": 16,
        "content": "Correct - B"
      },
      {
        "date": "2023-12-01T13:20:00.000Z",
        "voteCount": 1,
        "content": "\u201cneed the data to be available within 1 minute of ingestion for real-time analysis\u201d \u2192 low latency requirement \u2192 Dataflow streaming\n\nThe database can either be BQ or BigTable for this kind of requirement in data volume and latency. But it mentioned that the destination has to be BQ, so B."
      },
      {
        "date": "2023-08-05T05:53:00.000Z",
        "voteCount": 3,
        "content": "ANSWER b.\nFULL question ihave if you nee mail me \nneonitin6ATtherate......"
      },
      {
        "date": "2023-09-05T13:27:00.000Z",
        "voteCount": 1,
        "content": "full email id please ?"
      },
      {
        "date": "2023-04-25T10:49:00.000Z",
        "voteCount": 3,
        "content": "I think we need a pipeline, so it's B to me.))"
      },
      {
        "date": "2023-04-17T05:38:00.000Z",
        "voteCount": 2,
        "content": "I think we need a pipeline, so it's B to me."
      },
      {
        "date": "2023-03-06T01:32:00.000Z",
        "voteCount": 1,
        "content": "Why the answer from the &lt;reveal answer&gt; is C??"
      },
      {
        "date": "2022-12-05T05:44:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-11-13T14:52:00.000Z",
        "voteCount": 1,
        "content": "Need pipeline so its B"
      },
      {
        "date": "2022-09-21T10:24:00.000Z",
        "voteCount": 7,
        "content": "I got this question on sept 2022. Answer is B"
      },
      {
        "date": "2022-01-06T12:08:00.000Z",
        "voteCount": 2,
        "content": "omg. B only"
      },
      {
        "date": "2021-12-30T07:47:00.000Z",
        "voteCount": 7,
        "content": "Is B, if we expect a growth we\u2019ll need some buffer (that will be pub-sub) and the dataflow pipeline to stream data in big query.\nThe tabledata.insertAll method is not valid here."
      },
      {
        "date": "2021-12-18T16:23:00.000Z",
        "voteCount": 3,
        "content": "B, streaming with dataflow"
      },
      {
        "date": "2021-11-26T19:45:00.000Z",
        "voteCount": 1,
        "content": "Wrong answer shown again by examtopics.com\nAns: B"
      },
      {
        "date": "2021-09-28T07:58:00.000Z",
        "voteCount": 2,
        "content": "B =&gt; with dataflow you can parallelize data ingestion"
      },
      {
        "date": "2021-11-03T14:20:00.000Z",
        "voteCount": 1,
        "content": "And make it streaming"
      },
      {
        "date": "2021-08-08T13:26:00.000Z",
        "voteCount": 2,
        "content": "B is the right answer"
      },
      {
        "date": "2021-07-01T10:21:00.000Z",
        "voteCount": 4,
        "content": "Vote for B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 101,
    "url": "https://www.examtopics.com/discussions/google/view/17204-exam-professional-data-engineer-topic-1-question-101/",
    "body": "You need to copy millions of sensitive patient records from a relational database to BigQuery. The total size of the database is 10 TB. You need to design a solution that is secure and time-efficient. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the records from the database as an Avro file. Upload the file to GCS using gsutil, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the records from the database as an Avro file. Copy the file onto a Transfer Appliance and send it to Google, and then load the Avro file into BigQuery using the BigQuery web UI in the GCP Console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the records from the database into a CSV file. Create a public URL for the CSV file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the CSV file into BigQuery using the BigQuery web UI in the GCP Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the records from the database as an Avro file. Create a public URL for the Avro file, and then use Storage Transfer Service to move the file to Cloud Storage. Load the Avro file into BigQuery using the BigQuery web UI in the GCP Console."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-19T22:37:00.000Z",
        "voteCount": 60,
        "content": "You are transferring sensitive patient information, so C &amp; D are ruled out. Choice comes down to A &amp; B. Here it gets tricky. How to choose Transfer Appliance: (https://cloud.google.com/transfer-appliance/docs/2.0/overview)\nWithout knowing the bandwidth, it is not possible to determine whether the upload can be completed within 7 days, as recommended by Google. So the safest and most performant way is to use Transfer Appliance.\nTherefore my choice is B."
      },
      {
        "date": "2020-07-13T12:43:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/solutions/migration-to-google-cloud-transferring-your-large-datasets\nThe table shows for 1Gbps, it takes 30 hrs for 10 TB. Generally, corporate internet speeds are over 1Gbps. I'm inclined to pick A"
      },
      {
        "date": "2021-12-04T10:23:00.000Z",
        "voteCount": 3,
        "content": "SAY MY NAME!\nYou need to Transfer Sensitive Patient information, over public ISP you shouldn't do that."
      },
      {
        "date": "2023-05-31T07:12:00.000Z",
        "voteCount": 2,
        "content": "If you transfer 10TBs over the wire, your network will be blocked for the entire transfer time. This isn't something a company would be happy to swallow."
      },
      {
        "date": "2020-09-29T03:52:00.000Z",
        "voteCount": 18,
        "content": "Answer is B,gsutil has a limit of 1TBaccording to Google documentation,if data is morethan 1TBthen we have to use Transfer Appliance."
      },
      {
        "date": "2021-08-04T10:01:00.000Z",
        "voteCount": 9,
        "content": "The answer is clearly seen here: https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"
      },
      {
        "date": "2022-12-30T19:17:00.000Z",
        "voteCount": 4,
        "content": "B is right answer"
      },
      {
        "date": "2020-07-06T19:48:00.000Z",
        "voteCount": 8,
        "content": "Answer should be B: A is also correct but it has its own limit. It allows only 5TB data upload at a time to cloud storage. \nhttps://cloud.google.com/storage/quotas\nI will go with B"
      },
      {
        "date": "2021-01-08T03:40:00.000Z",
        "voteCount": 2,
        "content": "5Tb \"for individual objects\". Create smaller AVRO files."
      },
      {
        "date": "2021-01-08T03:45:00.000Z",
        "voteCount": 3,
        "content": "AVRO compression can reduce file size to a tenth"
      },
      {
        "date": "2024-09-24T16:35:00.000Z",
        "voteCount": 1,
        "content": "Also, \nFor your scenario with 10 TB of data in Cloud SQL, if you export to Avro without specifying compression, you can expect the resulting Avro file to be around the same size, potentially slightly smaller depending on the data characteristics. Here in this question, there is no mentioning about compression. \n\nSo let's not assume that the data being used in Avro format will get compressed. \n\nIf Google cloud storage itself, can't handle an object of size greater than 5 TB, there is no point of using gsutil"
      },
      {
        "date": "2024-09-24T16:28:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/storage-transfer/docs/known-limitations-transfer\n\nCloud Storage 5TiB object size limit\nCloud Storage supports a maximum single-object size up 5 tebibytes. If you have objects larger than 5TiB, the object transfer fails for those objects for either Cloud Storage or Storage Transfer Service."
      },
      {
        "date": "2024-06-29T07:13:00.000Z",
        "voteCount": 1,
        "content": "while Option A is feasible and could work depending on specific requirements and security measures implemented, Option D (exporting as Avro, using Storage Transfer Service, and then loading into BigQuery) generally offers a more secure, efficient, and managed approach for transferring sensitive patient records into BigQuery from a relational database.Avro files uploaded to GCS will need to be secured. While GCS itself offers security features like IAM policies and access controls, using a public URL (as suggested in Option A) introduces additional security concerns."
      },
      {
        "date": "2024-05-02T01:05:00.000Z",
        "voteCount": 1,
        "content": "to securely transfer data and looking at the size of data B is the correct option."
      },
      {
        "date": "2024-02-02T05:00:00.000Z",
        "voteCount": 1,
        "content": "IMO \"A\" is the most suitable option since the transfer appliance could take 25 days to get the appliance and then 25 days to ship it back and have the data available.\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds"
      },
      {
        "date": "2023-12-21T06:36:00.000Z",
        "voteCount": 1,
        "content": "Given the sensitivity of the patient records and the large size of the data, using Google's Transfer Appliance is a secure and efficient method. The Transfer Appliance is a hardware solution provided by Google for transferring large amounts of data. It enables you to securely transfer data without exposing it over the internet."
      },
      {
        "date": "2023-12-04T19:31:00.000Z",
        "voteCount": 1,
        "content": "Option B combines security, efficiency, and ease of use, making it a suitable choice for transferring sensitive patient records to BigQuery."
      },
      {
        "date": "2023-11-07T18:30:00.000Z",
        "voteCount": 4,
        "content": "10 TB is nothing. With a single 10 GB interconnect you could transfer the data in 3 hours or even with a 1 GB speeds without interconnect you could transfer it in one weekend. The transfer appliance will take 25 days to get the appliance and then 25 days while you wait for the data to be available that is not \"time-efficient\" at all. I go with A instead of B."
      },
      {
        "date": "2023-11-07T18:35:00.000Z",
        "voteCount": 2,
        "content": "I got the 25 days + 25 days from here: https://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds"
      },
      {
        "date": "2023-09-22T23:50:00.000Z",
        "voteCount": 3,
        "content": "transfer appliance will take time more than gsutil. and we did not mention yet if the location of the organization has google data centre"
      },
      {
        "date": "2023-08-20T19:24:00.000Z",
        "voteCount": 2,
        "content": "As per Google recommendation above 1TB of transfer from onprem or from Google cloud or other cloud storage like s3 etc we need to use storage transfer service."
      },
      {
        "date": "2023-08-19T16:16:00.000Z",
        "voteCount": 1,
        "content": "Transfer Appliance would take 20 days for epected turnaround time. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#expected%20turnaround:~:text=The%20expected%20turnaround%20time%20for%20a%20network%20appliance%20to%20be%20shipped%2C%20loaded%20with%20your%20data%2C%20shipped%20back%2C%20and%20rehydrated%20on%20Google%20Cloud%20is%2020%20days.\n\nThe best answer would be A. \nIf gsutil consume/leverage 100MB it would take 12 days and more time-efficient than B. \nThis is a reasonable assumption. \nhttps://cloud.google.com/static/architecture/images/big-data-transfer-how-to-get-started-transfer-size-and-speed.png"
      },
      {
        "date": "2023-08-17T03:20:00.000Z",
        "voteCount": 1,
        "content": "I will go with \" A\" because of the transition time to take transfer appliance to Google and that also depends in the organisation location. gsutil works anywhere internet is available."
      },
      {
        "date": "2023-07-14T06:26:00.000Z",
        "voteCount": 1,
        "content": "A will take crazy time if the organization didnt have a dedicated link"
      },
      {
        "date": "2023-07-10T05:21:00.000Z",
        "voteCount": 3,
        "content": "Transfer Appliance is not as time-efficient when you have enough bandwitdh. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer_appliance_for_larger_transfers"
      },
      {
        "date": "2023-06-23T01:33:00.000Z",
        "voteCount": 1,
        "content": "There is no \"cost effective\", if this is not a clear case for the appliance than what is?"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 102,
    "url": "https://www.examtopics.com/discussions/google/view/17205-exam-professional-data-engineer-topic-1-question-102/",
    "body": "You need to create a near real-time inventory dashboard that reads the main inventory tables in your BigQuery data warehouse. Historical inventory data is stored as inventory balances by item and location. You have several thousand updates to inventory every hour. You want to maximize performance of the dashboard and ensure that the data is accurate. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage BigQuery UPDATE statements to update the inventory balances as they are changing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the inventory balance table by item to reduce the amount of data scanned with each inventory update.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery streaming the stream changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery bulk loader to batch load inventory changes into a daily inventory movement table. Calculate balances in a view that joins it to the historical inventory balance table. Update the inventory balance table nightly."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 56,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 53,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-30T08:03:00.000Z",
        "voteCount": 33,
        "content": "A - New correct answer\nC - Old correct answer (for 2019)"
      },
      {
        "date": "2023-12-17T03:39:00.000Z",
        "voteCount": 4,
        "content": "C is better\n\nThe best approach is to use BigQuery streaming to stream the inventory changes into a daily inventory movement table. Then calculate balances in a view that joins the inventory movement table to the historical inventory balance table. Finally, update the inventory balance table nightly (option C)."
      },
      {
        "date": "2023-12-17T03:39:00.000Z",
        "voteCount": 3,
        "content": "The key reasons this is better than the other options:\n\nUsing BigQuery UPDATE statements (option A) would be very inefficient for thousands of updates per hour. It is better to batch updates.\nPartitioning the inventory balance table (option B) helps query performance, but does not solve the need to incrementally update balances.\nUsing the bulk loader (option D) would require batch loading the updates, which adds latency. Streaming inserts updates with lower latency.\nSo option C provides a scalable architecture that streams updates with low latency while batch updating the balances only once per day for efficiency. This balances performance and accuracy needs."
      },
      {
        "date": "2023-12-17T03:46:00.000Z",
        "voteCount": 1,
        "content": "Here's why the other options are less suitable:\n\nA. Leverage BigQuery UPDATE statements: While technically possible, this approach is inefficient for frequent updates as it requires individual record scans and updates, affecting performance and potentially causing data race conditions.\n\nB. Partition the inventory balance table: Partitioning helps with query performance for large datasets, but it doesn't address the need for near real-time updates.\n\nD. Use the BigQuery bulk loader: Bulk loading daily changes is helpful for historical data ingestion, but it won't provide near real-time updates necessary for the dashboard."
      },
      {
        "date": "2023-12-17T03:46:00.000Z",
        "voteCount": 1,
        "content": "Option C offers the following advantages:\n\nStreams inventory changes near real-time: BigQuery streaming ingests data immediately, keeping the inventory movement table constantly updated.\nDaily balance calculation: Joining the movement table with the historical balance table provides an accurate view of current inventory levels without affecting the actual balance table.\nNightly update for historical data: Updating the main inventory balance table nightly ensures long-term data consistency while maintaining near real-time insights through the view.\nThis approach balances near real-time updates with efficiency and data accuracy, making it the optimal solution for the given scenario."
      },
      {
        "date": "2023-07-14T02:53:00.000Z",
        "voteCount": 2,
        "content": "There are still limitations on DML statements (2023) e.g. only 2 concurrent UPDATES and up to 20 queued hence not appropriate for this scenario:\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements"
      },
      {
        "date": "2023-07-17T17:12:00.000Z",
        "voteCount": 2,
        "content": "option A:what limitation here 1500/perday okay in question we will get max 24 jobs hourly updated okay,\nnow speed 5 operation /10 sec , 1 operation 2sec , and we are getting new update in 1 hour so we have time 3600 sec and we need to update around 1000 update according to speed take 2000sec still we have 1600 sec rest to getting new update so .\nthats why I thing DML is best option for this work"
      },
      {
        "date": "2023-12-16T03:38:00.000Z",
        "voteCount": 1,
        "content": "In question it mentioned several thousands of updates every hour, several thousands could be 20-30 thousands as well. Where it is mentioned for only 1000 updates?"
      },
      {
        "date": "2020-08-21T04:24:00.000Z",
        "voteCount": 25,
        "content": "C is correct.\nIt says \u201cupdate Every hour\u201d\nAnd need \u201c accuracy\u201d"
      },
      {
        "date": "2023-07-17T17:12:00.000Z",
        "voteCount": 2,
        "content": "option A:what limitation here 1500/perday okay in question we will get max 24 jobs hourly updated okay,\nnow speed 5 operation /10 sec , 1 operation 2sec , and we are getting new update in 1 hour so we have time 3600 sec and we need to update around 1000 update according to speed take 2000sec still we have 1600 sec rest to getting new update so .\nthats why I thing DML is best option for this work"
      },
      {
        "date": "2024-07-21T00:00:00.000Z",
        "voteCount": 1,
        "content": "The answer is C because the requirement is near real-time"
      },
      {
        "date": "2023-12-17T03:40:00.000Z",
        "voteCount": 1,
        "content": "The best approach is to use BigQuery streaming to stream the inventory changes into a daily inventory movement table. Then calculate balances in a view that joins the inventory movement table to the historical inventory balance table. Finally, update the inventory balance table nightly (option C)."
      },
      {
        "date": "2023-12-17T03:40:00.000Z",
        "voteCount": 1,
        "content": "The key reasons this is better than the other options:\n\nUsing BigQuery UPDATE statements (option A) would be very inefficient for thousands of updates per hour. It is better to batch updates.\n\nPartitioning the inventory balance table (option B) helps query performance, but does not solve the need to incrementally update balances.\n\nUsing the bulk loader (option D) would require batch loading the updates, which adds latency. Streaming inserts updates with lower latency.\n\nSo option C provides a scalable architecture that streams updates with low latency while batch updating the balances only once per day for efficiency. This balances performance and accuracy needs."
      },
      {
        "date": "2023-12-05T22:50:00.000Z",
        "voteCount": 3,
        "content": "Option C.\n\nUsing the BigQuery streaming to stream changes into a daily inventory movement table and calculating balances in a view that joins it to the historical inventory balance table can help you achieve the desired performance and accuracy. You can then update the inventory balance table nightly. This approach can help you avoid the overhead of scanning large amounts of data with each inventory update, which can be time-consuming and resource-intensive.\nLeveraging BigQuery UPDATE statements to update the inventory balances as they are changing (option A) can be resource-intensive and may not be the most efficient way to achieve the desired performance."
      },
      {
        "date": "2023-11-23T21:39:00.000Z",
        "voteCount": 1,
        "content": "As per other answers C"
      },
      {
        "date": "2023-11-05T06:28:00.000Z",
        "voteCount": 1,
        "content": "Simple and will work"
      },
      {
        "date": "2023-10-20T08:07:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. Why because \u201cUpdate\u201d limits is 1500/per day, and the question say: You have several thousand updates to inventory every hour. So is impossible to use updates all the time."
      },
      {
        "date": "2023-10-06T02:34:00.000Z",
        "voteCount": 1,
        "content": "A. Leverage BigQuery UPDATE statements to update the inventory balances as they are changing - is so simple and RIGHT!"
      },
      {
        "date": "2023-09-16T08:30:00.000Z",
        "voteCount": 2,
        "content": "C is more universal and sustainable"
      },
      {
        "date": "2023-07-10T05:27:00.000Z",
        "voteCount": 3,
        "content": "UPDATE is too expensive. Joining main and delta tables is the right wat to capture data change."
      },
      {
        "date": "2023-07-07T08:10:00.000Z",
        "voteCount": 2,
        "content": "I think the answer is C. The question is about maximizing performance and accuracy, it's ok if we need expensive JOINs. BigQuery has a daily quota of 1500 UPDATEs, and the question talks about several thousand updates every hour."
      },
      {
        "date": "2023-09-26T23:26:00.000Z",
        "voteCount": 1,
        "content": "DML statements do not count toward the number of table modifications per day.\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements\n\nSo I would go with A."
      },
      {
        "date": "2023-09-26T23:28:00.000Z",
        "voteCount": 1,
        "content": "Sorry, wrong link. Here is the correct one: https://cloud.google.com/bigquery/quotas#standard_tables"
      },
      {
        "date": "2023-07-03T04:19:00.000Z",
        "voteCount": 1,
        "content": "C create a view that joins to a table seems dumb to me"
      },
      {
        "date": "2023-05-31T07:19:00.000Z",
        "voteCount": 3,
        "content": "Too frequent updates are way too expensive in an OLAP solution. This is much more likely to stream changes to the table(s) and aggregate these changes in the view.\n\nhttps://stackoverflow.com/questions/74657435/bigquery-frequent-updates-to-a-record"
      },
      {
        "date": "2023-04-08T22:56:00.000Z",
        "voteCount": 1,
        "content": "Has to be C. \nDML has hard limit of 1500 operations per table per day: https://cloud.google.com/bigquery/quotas#standard_tables"
      },
      {
        "date": "2023-03-26T00:46:00.000Z",
        "voteCount": 2,
        "content": "Update action is not efficient"
      },
      {
        "date": "2023-07-17T17:13:00.000Z",
        "voteCount": 1,
        "content": "option A:what limitation here 1500/perday okay in question we will get max 24 jobs hourly updated okay,\nnow speed 5 operation /10 sec , 1 operation 2sec , and we are getting new update in 1 hour so we have time 3600 sec and we need to update around 1000 update according to speed take 2000sec still we have 1600 sec rest to getting new update so .\nthats why I thing DML is best option for this work"
      },
      {
        "date": "2023-03-09T00:15:00.000Z",
        "voteCount": 3,
        "content": "This question has 2 parts:\n1. Query the table in real-time\n2. Update the table with thousands of records per hour ~ 10 updates per second\n\nWithout (1), C seems to be the good approach by using staging table to buffer the update using Change Data Capture method. However, that method will make the query expensive due to the JOIN. So A is a better choice here."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 103,
    "url": "https://www.examtopics.com/discussions/google/view/79775-exam-professional-data-engineer-topic-1-question-103/",
    "body": "You have a data stored in BigQuery. The data in the BigQuery dataset must be highly available. You need to define a storage, backup, and recovery strategy of this data that minimizes cost. How should you configure the BigQuery table that have a recovery point objective (RPO) of 30 days?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the BigQuery dataset to be regional. In the event of an emergency, use a point-in-time snapshot to recover the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the BigQuery dataset to be regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the BigQuery dataset to be multi-regional. In the event of an emergency, use a point-in-time snapshot to recover the data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the BigQuery dataset to be multi-regional. Create a scheduled query to make copies of the data to tables suffixed with the time of the backup. In the event of an emergency, use the backup copy of the table."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-09-15T15:31:00.000Z",
        "voteCount": 7,
        "content": "Answer is B. Timetravel only covers for 7 days and a scheduled query is needed for creating Table snapshots for 30 days. Also table snapshot must remain in the same region as base table (please refer to limitation of table snapshot from below link)  https://cloud.google.com/bigquery/docs/table-snapshots-intro"
      },
      {
        "date": "2023-01-19T13:26:00.000Z",
        "voteCount": 5,
        "content": "Answer is C: https://cloud.google.com/bigquery/docs/table-snapshots-intro\n\"Benefits of using table snapshots include the following:\n\nKeep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n\nMinimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table.\" \nBut the wording is foolish... It's table snapshot, NOT point in time snapshot!\n\nhttps://cloud.google.com/bigquery/docs/time-travel#restore-a-table\nthis is point in time using time travel window - max is 7 days..."
      },
      {
        "date": "2024-10-10T11:16:00.000Z",
        "voteCount": 1,
        "content": "D.Create monthly snapshots of a table by using a service account that runs a scheduled query. Link: https://cloud.google.com/bigquery/docs/table-snapshots-scheduled"
      },
      {
        "date": "2024-06-04T15:23:00.000Z",
        "voteCount": 3,
        "content": "HA =&gt; multi-region\n30-days RPO =&gt; manual backups as max time-travel is 7 days"
      },
      {
        "date": "2024-05-21T06:04:00.000Z",
        "voteCount": 3,
        "content": "This is in one of google's training practice questions and the answer for it is C."
      },
      {
        "date": "2024-05-25T07:18:00.000Z",
        "voteCount": 1,
        "content": "Agreed. Multi-regional datasets offer higher availability by replicating data across multiple regions"
      },
      {
        "date": "2023-12-05T22:57:00.000Z",
        "voteCount": 3,
        "content": "ou should consider option A.\n\nSetting the BigQuery dataset to be regional and using a point-in-time snapshot to recover the data in the event of an emergency can help you achieve the desired level of availability and minimize cost. This approach can help you avoid the additional cost of creating and maintaining backup copies of the data, which can be expensive.\n\nSetting the BigQuery dataset to be multi-regional (options C and D) can provide additional redundancy and availability. However, this approach can be more expensive than setting the dataset to be regional, especially if you do not require the additional level of redundancy."
      },
      {
        "date": "2023-10-06T02:43:00.000Z",
        "voteCount": 3,
        "content": "I'm going for A: \n1. Set the BigQuery dataset to be regional. This will reduce the cost of storage compared to a multi-regional dataset.\n2. building Snapshot: bq snapshot --dataset &lt;dataset_id&gt; --table &lt;table_id&gt; &lt;snapshot_id&gt;"
      },
      {
        "date": "2023-10-24T11:00:00.000Z",
        "voteCount": 1,
        "content": "typically Multi-region cost is equal or less than a region. https://cloud.google.com/bigquery/pricing#storage"
      },
      {
        "date": "2023-09-20T00:44:00.000Z",
        "voteCount": 4,
        "content": "I think the answer is A: \n\nThis option meets the 30-day RPO requirement, assuming that the snapshot is maintained for that long. It offers high availability as data is written synchronously to 2 zones within a region: https://cloud.google.com/blog/topics/developers-practitioners/backup-disaster-recovery-strategies-bigquery/. The cost would be lower than maintaining a multi-regional dataset, but you'll incur the cost of the snapshot."
      },
      {
        "date": "2023-03-16T18:52:00.000Z",
        "voteCount": 3,
        "content": "Why not B? Setting dataset regional or multi does not affect the backup and recovery strategy."
      },
      {
        "date": "2023-03-09T00:58:00.000Z",
        "voteCount": 4,
        "content": "1. HA -&gt; Multi-region\n2. DR -&gt; Snapshot"
      },
      {
        "date": "2023-02-24T19:17:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/table-snapshots-scheduled"
      },
      {
        "date": "2022-12-21T01:51:00.000Z",
        "voteCount": 2,
        "content": "One of the criteria is to minimize cost.  While C is an ideal solution but more expensive compared to D.  In my opinion the correct answer should be D."
      },
      {
        "date": "2022-12-22T04:45:00.000Z",
        "voteCount": 2,
        "content": "How is using snapshots more expensive, snapshots store the delta of the state of the table during time of taking snapshot and the current state of the table (https://cloud.google.com/bigquery/docs/table-snapshots-intro#storage_costs)\n\nWhen a snapshot is created, there is literally zero cost... until changes are made to the base table, only then will the delta storage cost be charged.. This certainly seems cheaper than creating a new table"
      },
      {
        "date": "2022-12-13T17:43:00.000Z",
        "voteCount": 3,
        "content": "Even when the dataset/table is regional, google provides high availability with data available in 2 zones. Isn't A correct answer?"
      },
      {
        "date": "2022-12-05T05:23:00.000Z",
        "voteCount": 4,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots\nA BigQuery table snapshot preserves the contents of a table (called the base table) at a particular time. You can save a snapshot of a current table, or create a snapshot of a table as it was at any time in the past seven days. A table snapshot can have an expiration; when the configured amount of time has passed since the table snapshot was created, BigQuery deletes the table snapshot. You can query a table snapshot as you would a standard table. Table snapshots are read-only, but you can create (restore) a standard table from a table snapshot, and then you can modify the restored table."
      },
      {
        "date": "2022-11-30T07:38:00.000Z",
        "voteCount": 3,
        "content": "point in time snapshots only have data from past 7 days"
      },
      {
        "date": "2022-12-16T22:13:00.000Z",
        "voteCount": 1,
        "content": "We can keep PIT snapshots for as long as we want. I think you confused snapshots with time travel... \n\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\"\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots"
      },
      {
        "date": "2023-01-19T13:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/time-travel#configure_the_time_travel_window\n'You can set the duration of the time travel window, from a minimum of two days to a maximum of seven days'\n\nNot the same as snapshot"
      },
      {
        "date": "2022-11-11T17:12:00.000Z",
        "voteCount": 2,
        "content": "C\nBenefits of using table snapshots include the following:\n\nKeep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n\nMinimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table."
      },
      {
        "date": "2022-11-06T12:48:00.000Z",
        "voteCount": 2,
        "content": "C\nMulti-region for HA\nSnapshot for recovery"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 104,
    "url": "https://www.examtopics.com/discussions/google/view/79318-exam-professional-data-engineer-topic-1-question-104/",
    "body": "You used Dataprep to create a recipe on a sample of data in a BigQuery table. You want to reuse this recipe on a daily upload of data with the same schema, after the load job with variable execution time completes. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cron schedule in Dataprep.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an App Engine cron job to schedule the execution of the Dataprep job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the recipe as a Dataprep template, and create a job in Cloud Scheduler.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the Dataprep job as a Dataflow template, and incorporate it into a Composer job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T22:14:00.000Z",
        "voteCount": 12,
        "content": "I'd pick D because it's the only option which allows variable execution (since we need to execute the dataprep job only after the prior load job). Although D suggests the export of Dataflow templates,  this discussion suggests that the export option is no longer available (https://stackoverflow.com/questions/72544839/how-to-get-the-dataflow-template-of-a-dataprep-job), there are already Airflow Operators for Dataprep which we should be using instead - https://airflow.apache.org/docs/apache-airflow-providers-google/stable/operators/cloud/dataprep.html"
      },
      {
        "date": "2023-03-09T01:20:00.000Z",
        "voteCount": 9,
        "content": "Since the load job execution time is unexpected, schedule the Dataprep based on a fixed time window may not work.\nWhen the Dataprep job run the first time, we can find the Dataflow job for that in the console. We can use that to create the Template --&gt; With the help of the Composer to determine if the load job is completed, we can then trigger the Dataflow job"
      },
      {
        "date": "2023-12-21T07:07:00.000Z",
        "voteCount": 2,
        "content": "Dataprep by Trifacta allows you to schedule the execution of recipes. You can set up a cron schedule directly within Dataprep to automatically run your recipe at specified intervals, such as daily.\nWHY NOT D ? : This option involves significant additional complexity. Exporting the Dataprep job as a Dataflow template and then incorporating it into a Composer (Apache Airflow) job is a more complicated process and is typically used for more complex orchestration needs that go beyond simple scheduling."
      },
      {
        "date": "2023-12-17T03:57:00.000Z",
        "voteCount": 1,
        "content": "We have external dependency \"after the load job with variable execution time completes\"\nwhich requires DAG -&gt; Airflow (Cloud Composer)\n\nThe reasons:\n\nA scheduler like Cloud Scheduler won't handle the dependency on the BigQuery load completion time\nUsing Composer allows creating a DAG workflow that can:\nTrigger the BigQuery load\nWait for BigQuery load to complete\nTrigger the Dataprep Dataflow job\nDataflow template allows easy reuse of the Dataprep transformation logic\nComposer coordinates everything based on the dependencies in an automated workflow"
      },
      {
        "date": "2023-12-07T19:54:00.000Z",
        "voteCount": 1,
        "content": "I'd pick D because it's the only option which allows variable execution"
      },
      {
        "date": "2023-08-30T21:52:00.000Z",
        "voteCount": 3,
        "content": "The key here is \"after the load job with variable execution time completes\" which means the execution of this job depends on the completion of another job which has a variable execution time. Hence D"
      },
      {
        "date": "2023-08-23T04:07:00.000Z",
        "voteCount": 1,
        "content": "This approach ensures the dynamic triggering of the Dataprep job based on the completion of the preceding load job, ensuring data is processed accurately and in sequen"
      },
      {
        "date": "2023-04-15T18:18:00.000Z",
        "voteCount": 2,
        "content": "A is correct. D is too complicated. \n\nA is correct, because you can schedule a job right from Dataprep UI.\n\nhttps://cloud.google.com/blog/products/gcp/scheduling-and-sampling-arrive-for-google-cloud-dataprep\nScheduling and sampling arrive for Google Cloud Dataprep\nThroughout our early releases, users\u2019 most common request has been Flow scheduling. As of Thursday\u2019s release, Flows can be scheduled with minute granularity at any frequency."
      },
      {
        "date": "2023-03-26T01:13:00.000Z",
        "voteCount": 4,
        "content": "I think C it is more straighforward"
      },
      {
        "date": "2023-02-24T07:53:00.000Z",
        "voteCount": 3,
        "content": "Answer C: Use Recipe Template feature of dataprep. Don't need to change the service."
      },
      {
        "date": "2023-01-11T04:35:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2022-12-07T04:17:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-06T07:51:00.000Z",
        "voteCount": 4,
        "content": "It's A. You can set it directly in Dataprep a job and it will use Dataflow under the hood."
      },
      {
        "date": "2022-12-06T07:50:00.000Z",
        "voteCount": 2,
        "content": "It's A. You can set it directly in Dataprep a job and it will use Dataflow under the hood. No need to export nor incorporate into a Composer job.\nDataprep by trifacta - https://docs.trifacta.com/display/DP/cron+Schedule+Syntax+Reference\nDataprep job uses dataflow - https://cloud.google.com/dataprep"
      },
      {
        "date": "2022-12-18T22:00:00.000Z",
        "voteCount": 4,
        "content": "The question mentions after a load job with variable time, i dont think setting a dataprep cron job can address the issue of variable load times"
      },
      {
        "date": "2022-11-06T12:50:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2022-09-14T09:21:00.000Z",
        "voteCount": 2,
        "content": "Dataprep and  Dataflow are same famitly"
      },
      {
        "date": "2022-09-03T06:05:00.000Z",
        "voteCount": 4,
        "content": "D. Export the Dataprep job as a Dataflow template, and incorporate it into a Composer job.\nReveal Solution"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 105,
    "url": "https://www.examtopics.com/discussions/google/view/79319-exam-professional-data-engineer-topic-1-question-105/",
    "body": "You want to automate execution of a multi-step data pipeline running on Google Cloud. The pipeline includes Dataproc and Dataflow jobs that have multiple dependencies on each other. You want to use managed services where possible, and the pipeline will run every day. Which tool should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tcron",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Scheduler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWorkflow Templates on Dataproc"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T07:10:00.000Z",
        "voteCount": 1,
        "content": "Of course, it is Cloud Composer!"
      },
      {
        "date": "2023-10-06T02:47:00.000Z",
        "voteCount": 1,
        "content": "B. Cloud Composer is the right answer !"
      },
      {
        "date": "2023-07-27T02:36:00.000Z",
        "voteCount": 1,
        "content": "Cloud Composer is a managed service that allows you to create and run Apache Airflow workflows. Airflow is a workflow management platform that can be used to automate complex data pipelines. It is a good choice for this use case because it is a managed service, which means that Google will take care of the underlying infrastructure. It also supports multiple dependencies, so you can easily schedule a multi-step pipeline"
      },
      {
        "date": "2023-04-25T05:16:00.000Z",
        "voteCount": 2,
        "content": "Airflow is the only choiche to handle dependencies and being able to call all of the services included in the question"
      },
      {
        "date": "2023-02-22T02:02:00.000Z",
        "voteCount": 2,
        "content": "Multi-step sequential pipelines -&gt; Cloud Composer"
      },
      {
        "date": "2022-12-30T12:35:00.000Z",
        "voteCount": 2,
        "content": "Cloud composer B is right"
      },
      {
        "date": "2022-12-07T10:29:00.000Z",
        "voteCount": 1,
        "content": "Cloud Composer (Airflow) is the answer to chain different steps from different apps..."
      },
      {
        "date": "2022-10-25T19:03:00.000Z",
        "voteCount": 1,
        "content": "\" multiple dependencies on each other. You want to use managed service\"\n = Cloud Composer"
      },
      {
        "date": "2022-09-14T09:29:00.000Z",
        "voteCount": 2,
        "content": "if you want your wf to schedule there are 3 ways to perform it, it of them is composer\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions"
      },
      {
        "date": "2022-09-07T13:41:00.000Z",
        "voteCount": 1,
        "content": "composer :)"
      },
      {
        "date": "2022-09-05T00:48:00.000Z",
        "voteCount": 1,
        "content": "Composer"
      },
      {
        "date": "2022-09-03T06:05:00.000Z",
        "voteCount": 1,
        "content": "B. Cloud Composer"
      },
      {
        "date": "2022-09-02T01:41:00.000Z",
        "voteCount": 1,
        "content": "Use composer to schedule tasks"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 106,
    "url": "https://www.examtopics.com/discussions/google/view/79777-exam-professional-data-engineer-topic-1-question-106/",
    "body": "You are managing a Cloud Dataproc cluster. You need to make a job run faster while minimizing costs, without losing work in progress on your clusters. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the cluster size with more non-preemptible workers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the cluster size with preemptible worker nodes, and configure them to forcefully decommission.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the cluster size with preemptible worker nodes, and use Cloud Stackdriver to trigger a script to preserve work.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-30T12:38:00.000Z",
        "voteCount": 6,
        "content": "D is right\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#using_graceful_decommissioning"
      },
      {
        "date": "2023-12-07T20:02:00.000Z",
        "voteCount": 1,
        "content": "D is right\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters#using_graceful_decommissioning"
      },
      {
        "date": "2022-12-22T02:06:00.000Z",
        "voteCount": 1,
        "content": "Should be A. You can configure the preemptible worker to gracefull decommission, its for non preemptible worker nodes."
      },
      {
        "date": "2023-01-02T10:10:00.000Z",
        "voteCount": 1,
        "content": "nope, they are not only for non-preeemtible workers"
      },
      {
        "date": "2022-12-16T02:30:00.000Z",
        "voteCount": 2,
        "content": "graceful decommissioning: to finish work in progress on a worker before it is removed from the Cloud Dataproc cluster.\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters"
      },
      {
        "date": "2022-12-07T10:27:00.000Z",
        "voteCount": 1,
        "content": "All your workers need to be the same kind. Use Graceful Decommissioning for don't lose any data and add more(increase the cluster) preemptible workers because there are more cost-effective ."
      },
      {
        "date": "2022-11-12T09:33:00.000Z",
        "voteCount": 2,
        "content": "A. \"graceful decommissioning\" is not a configuration value but a parameter passed with scale down action - to decrease the number of workers to save money (see Graceful Decommissioning as an option to use when downsizing a cluster to avoid losing work in progress)"
      },
      {
        "date": "2022-09-14T07:16:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/scaling-clusters\nWhy scale a Dataproc cluster?\nto increase the number of workers to make a job run faster\nto decrease the number of workers to save money (see Graceful Decommissioning as an option to use when downsizing a cluster to avoid losing work in progress).\nto increase the number of nodes to expand available Hadoop Distributed Filesystem (HDFS) storage"
      },
      {
        "date": "2022-10-30T02:05:00.000Z",
        "voteCount": 2,
        "content": "This weird.\nThe question mentions that increase cluster, but  Graceful Decommissioning use in downscale the cluster"
      },
      {
        "date": "2022-12-07T10:27:00.000Z",
        "voteCount": 1,
        "content": "All your workers need to be the same kind. Use Graceful Decommissioning for don't lose any data and add more preemptible workers because there are more cost-effective"
      },
      {
        "date": "2022-09-03T06:06:00.000Z",
        "voteCount": 1,
        "content": "D. Increase the cluster size with preemptible worker nodes, and configure them to use graceful decommissioning."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 107,
    "url": "https://www.examtopics.com/discussions/google/view/79778-exam-professional-data-engineer-topic-1-question-107/",
    "body": "You work for a shipping company that uses handheld scanners to read shipping labels. Your company has strict data privacy standards that require scanners to only transmit tracking numbers when events are sent to Kafka topics. A recent software update caused the scanners to accidentally transmit recipients' personally identifiable information (PII) to analytics systems, which violates user privacy rules. You want to quickly build a scalable solution using cloud-native managed services to prevent exposure of PII to the analytics systems. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an authorized view in BigQuery to restrict access to tables with sensitive data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall a third-party data validation tool on Compute Engine virtual machines to check the incoming data for sensitive information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging to analyze the data passed through the total pipeline to identify transactions that may contain sensitive information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-27T10:48:00.000Z",
        "voteCount": 5,
        "content": "The cloud function with DLP seems the best option"
      },
      {
        "date": "2023-07-24T23:28:00.000Z",
        "voteCount": 1,
        "content": "DLP is required"
      },
      {
        "date": "2022-09-08T11:45:00.000Z",
        "voteCount": 1,
        "content": "D option"
      },
      {
        "date": "2022-09-03T06:07:00.000Z",
        "voteCount": 2,
        "content": "D. Build a Cloud Function that reads the topics and makes a call to the Cloud Data Loss Prevention (Cloud DLP) API. Use the tagging and confidence levels to either pass or quarantine the data in a bucket for review."
      },
      {
        "date": "2022-12-30T12:45:00.000Z",
        "voteCount": 1,
        "content": "Agreed"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 108,
    "url": "https://www.examtopics.com/discussions/google/view/17209-exam-professional-data-engineer-topic-1-question-108/",
    "body": "You have developed three data processing jobs. One executes a Cloud Dataflow pipeline that transforms data uploaded to Cloud Storage and writes results to<br>BigQuery. The second ingests data from on-premises servers and uploads it to Cloud Storage. The third is a Cloud Dataflow pipeline that gets information from third-party data providers and uploads the information to Cloud Storage. You need to be able to schedule and monitor the execution of these three workflows and manually execute them when needed. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Direct Acyclic Graph in Cloud Composer to schedule and monitor the jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Stackdriver Monitoring and set up an alert with a Webhook notification to trigger the jobs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an App Engine application to schedule and request the status of the jobs using GCP API calls.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up cron jobs in a Compute Engine instance to schedule and monitor the pipelines using GCP API calls."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T06:46:00.000Z",
        "voteCount": 36,
        "content": "Should be A"
      },
      {
        "date": "2020-03-21T22:48:00.000Z",
        "voteCount": 22,
        "content": "Create dependency in Cloud Composer and schedule it."
      },
      {
        "date": "2022-10-31T07:02:00.000Z",
        "voteCount": 1,
        "content": "the jobs are not interdependent. just 3 individual jobs"
      },
      {
        "date": "2023-10-25T01:30:00.000Z",
        "voteCount": 1,
        "content": "yes answer A"
      },
      {
        "date": "2023-05-31T07:47:00.000Z",
        "voteCount": 1,
        "content": "Cloud Composer. No doubt"
      },
      {
        "date": "2022-12-30T12:45:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2022-12-27T10:52:00.000Z",
        "voteCount": 1,
        "content": "Cloud composer's DAG would manage the dependencies"
      },
      {
        "date": "2022-12-05T04:54:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers."
      },
      {
        "date": "2022-07-05T17:55:00.000Z",
        "voteCount": 2,
        "content": "This should be A"
      },
      {
        "date": "2022-01-06T12:21:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/composer/docs/how-to/using/writing-dags"
      },
      {
        "date": "2022-01-01T14:23:00.000Z",
        "voteCount": 5,
        "content": "Cloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers.\nhttps://cloud.google.com/composer/?hl=en"
      },
      {
        "date": "2021-12-21T09:15:00.000Z",
        "voteCount": 6,
        "content": "A\nThough the jobs are not dependent, they are data-driven. Refer to the below link:\nhttps://cloud.google.com/blog/topics/developers-practitioners/choosing-right-orchestrator-google-cloud"
      },
      {
        "date": "2022-01-01T14:43:00.000Z",
        "voteCount": 1,
        "content": "nice article thanks!"
      },
      {
        "date": "2021-11-25T08:35:00.000Z",
        "voteCount": 3,
        "content": "Cloud Composer"
      },
      {
        "date": "2021-11-25T08:34:00.000Z",
        "voteCount": 3,
        "content": "Correct: A"
      },
      {
        "date": "2021-08-09T00:03:00.000Z",
        "voteCount": 5,
        "content": "should be option A"
      },
      {
        "date": "2021-07-02T06:32:00.000Z",
        "voteCount": 4,
        "content": "Vote for A"
      },
      {
        "date": "2021-02-06T20:16:00.000Z",
        "voteCount": 3,
        "content": "COrrect A: Couldn't understand why a option with no connection with actual problem has been given as correct option (D)"
      },
      {
        "date": "2020-11-18T23:22:00.000Z",
        "voteCount": 2,
        "content": "I'll go for A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 109,
    "url": "https://www.examtopics.com/discussions/google/view/79780-exam-professional-data-engineer-topic-1-question-109/",
    "body": "You have Cloud Functions written in Node.js that pull messages from Cloud Pub/Sub and send the data to BigQuery. You observe that the message processing rate on the Pub/Sub topic is orders of magnitude higher than anticipated, but there is no error logged in Cloud Logging. What are the two most likely causes of this problem? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPublisher throughput quota is too small.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTotal outstanding messages exceed the 10-MB maximum.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tError handling in the subscriber code is not handling run-time errors properly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe subscriber code cannot keep up with the messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe subscriber code does not acknowledge the messages that it pulls.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "CE",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CE",
        "count": 4,
        "isMostVoted": true
      },
      {
        "answer": "DE",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-13T23:45:00.000Z",
        "voteCount": 12,
        "content": "Answer C E\nBy not acknowleding the pulled message, this result in it be putted back in Cloud Pub/Sub, meaning the messages accumulate instead of being consumed and removed from Pub/Sub. The same thing can happen ig the subscriber maintains the lease on the message it receives in case of an error. This reduces the overall rate of processing because messages get stuck on the first subscriber. Also, errors in Cloud Function do not show up in Stackdriver Log Viewer if they are not correctly handled."
      },
      {
        "date": "2024-09-23T15:18:00.000Z",
        "voteCount": 1,
        "content": "D. The subscriber code cannot keep up with the messages.\n\nIf the processing rate of the subscriber (Cloud Functions) is lower than the incoming message rate, it can lead to a backlog of messages. This would result in higher-than-expected message rates, as messages accumulate while waiting to be processed.\n\nE. The subscriber code does not acknowledge the messages that it pulls.\n\nIf messages are not acknowledged properly, Pub/Sub will keep retrying to deliver them, which can lead to the same messages being sent repeatedly. This could also contribute to the perception that the message processing rate is very high.\n\nBoth of these issues can lead to unanticipated behavior in your message processing pipeline without generating errors that would be logged in Cloud Logging."
      },
      {
        "date": "2024-08-11T20:14:00.000Z",
        "voteCount": 1,
        "content": "The code in the CF can't keep up with the amount of messages, thus C D is a better fit"
      },
      {
        "date": "2023-04-25T21:39:00.000Z",
        "voteCount": 1,
        "content": "Ref chatgpt\nOption C, \"Error handling in the subscriber code is not handling run-time errors properly,\" suggests that the subscriber code may not be correctly handling errors that occur during message processing. If the subscriber code encounters an error that it cannot handle, such as a syntax error or a network issue, it may stop processing messages, leading to a slowdown in message processing.\n\nHowever, the lack of error logs in Cloud Logging suggests that there are no errors being logged, which makes it less likely that this is the primary cause of the observed behavior. Additionally, while incorrect error handling could contribute to the issue, it may not be the primary reason why the message processing rate is much higher than anticipated."
      },
      {
        "date": "2023-11-11T07:08:00.000Z",
        "voteCount": 1,
        "content": "Chat says about Option C: \"it may stop processing messages, leading to a slowdown in message processing\" - but is doesn't say there's a slowdown in the question. It says it's increased.\n\nI would replace C with D.  If the Cloud Function isn't capable of processing messages as quickly as they arrive, the backlog will grow, leading to higher processing rates as the function continuously tries to catch up. This scenario might not generate errors in Cloud Logging if the function is simply falling behind."
      },
      {
        "date": "2023-03-09T21:29:00.000Z",
        "voteCount": 1,
        "content": "C - as no error shown in Cloud Logging\nBetween D &amp; E, both could lead to the problem. I have worked with lot of PubSub issues, most of them are due to the bottleneck at the code where it takes too long to process 1 message and causes backlog. E could lead to backlog too, but it is too obvious and not likely to happen in reality.\nHowever, when I ask AI the same question, it said C and E"
      },
      {
        "date": "2023-06-19T07:00:00.000Z",
        "voteCount": 1,
        "content": "C. Error handling in the subscriber (Cloud Functions) code is not handling run-time errors properly.\n\nThis would mean to have error logs in Cloud Logging as CF by default logs to it."
      },
      {
        "date": "2023-02-16T06:49:00.000Z",
        "voteCount": 3,
        "content": "Answer D&amp;E\nI am not in the favour of C, error handling is a side factor but not the primary cause. \nFirst check the configuration access. \nDoes subscriber has enough acknowledge policies (option E)\nDoes sub have ability to keep up the message( enough network, cpu and capable codes) (option D)\noption C is just a part of option D somewhere showing incapable handling"
      },
      {
        "date": "2023-01-19T14:23:00.000Z",
        "voteCount": 3,
        "content": "My question is: 'What is the actual problem?'\n- That there is no logs in Cloud Logging?\n- That Pub/Sub is having a problem?\n- Or there an actual problem?\n- Is there an actual error?\n\nSo what is Pub/Sub the message processing rate is high...Does that mean there is a problem?\n\nThoughts?"
      },
      {
        "date": "2023-08-19T17:18:00.000Z",
        "voteCount": 1,
        "content": "Like TNT87 mentioned the message processing rate is high \"meaning the messages accumulate instead of being consumed and removed from Pub/Sub.\""
      },
      {
        "date": "2022-12-30T12:52:00.000Z",
        "voteCount": 1,
        "content": "C, E seems correct"
      },
      {
        "date": "2022-09-11T19:48:00.000Z",
        "voteCount": 3,
        "content": "D might also be right? \nSubscriber might not be provisioned enough"
      },
      {
        "date": "2022-09-03T06:09:00.000Z",
        "voteCount": 3,
        "content": "C. Error handling in the subscriber code is not handling run-time errors properly.\nE. The subscriber code does not acknowledge the messages that it pulls."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 110,
    "url": "https://www.examtopics.com/discussions/google/view/17252-exam-professional-data-engineer-topic-1-question-110/",
    "body": "You are creating a new pipeline in Google Cloud to stream IoT data from Cloud Pub/Sub through Cloud Dataflow to BigQuery. While previewing the data, you notice that roughly 2% of the data appears to be corrupt. You need to modify the Cloud Dataflow pipeline to filter out this corrupt data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a SideInput that returns a Boolean if the element is corrupt.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a ParDo transform in Cloud Dataflow to discard corrupt elements.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a Partition transform in Cloud Dataflow to separate valid data from corrupt data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a GroupByKey transform in Cloud Dataflow to group all of the valid data together and discard the rest."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T06:51:00.000Z",
        "voteCount": 16,
        "content": "Correct - B"
      },
      {
        "date": "2020-03-27T23:49:00.000Z",
        "voteCount": 12,
        "content": "Answer: B\nDescription: ParDo is used to do transformation and create side output"
      },
      {
        "date": "2023-03-12T17:39:00.000Z",
        "voteCount": 3,
        "content": "A - SideInput is often used to validate data, however, we need to create the SideInput first. When using SideInput to filter data, it is actually another ParDo call.\nC, D - This is common way to filter too, but we will need the key in order to partition or GroupByKey\nB - ParDo is the most basic method, it can do anything to the PCollection"
      },
      {
        "date": "2022-12-30T12:56:00.000Z",
        "voteCount": 1,
        "content": "B. Add a ParDo transform in Cloud Dataflow to discard corrupt elements."
      },
      {
        "date": "2022-12-05T04:49:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/beam-programming-model#concepts\nParDo is the core parallel processing operation in the Apache Beam SDKs, invoking a user-specified function on each of the elements of the input PCollection. ParDo collects the zero or more output elements into an output PCollection. The ParDo transform processes elements independently and possibly in parallel."
      },
      {
        "date": "2022-07-06T02:05:00.000Z",
        "voteCount": 4,
        "content": "vote B :https://beam.apache.org/documentation/programming-guide/#pardo \n\nFiltering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it.\nFormatting or type-converting each element in a data set. If your input PCollection contains elements that are of a different type or format than you want, you can use ParDo to perform a conversion on each element and output the result to a new PCollection.\nExtracting parts of each element in a data set. If you have a PCollection of records with multiple fields, for example, you can use a ParDo to parse out just the fields you want to consider into a new PCollection.\nPerforming computations on each element in a data set. You can use ParDo to perform simple or complex computations on every element, or certain elements, of a PCollection and output the results as a new PCollection."
      },
      {
        "date": "2022-01-06T12:24:00.000Z",
        "voteCount": 2,
        "content": "Filtering with ParDo. ParDo is a Beam transform for generic parallel processing. ParDo is useful for common data processing operations/"
      },
      {
        "date": "2022-12-30T12:52:00.000Z",
        "voteCount": 1,
        "content": "I agree with B"
      },
      {
        "date": "2022-01-01T14:30:00.000Z",
        "voteCount": 6,
        "content": "B: ParDo is a Beam transform for generic parallel processing. ParDo is useful for common data processing operations, including:\na. Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection, or discard it.\nb. Formatting or type-converting each element in a data set.\nc. Extracting parts of each element in a data set.\nd. Performing computations on each element in a data set.\nA does not help\nC Partition is a Beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections. Again, does not help\nD GroupByKey is a Beam transform for processing collections of key/value pairs. GroupByKey is a good way to aggregate data that has something in common"
      },
      {
        "date": "2021-07-02T06:56:00.000Z",
        "voteCount": 4,
        "content": "vote for 'B', ParDo can discard the elements.\n\nhttps://beam.apache.org/documentation/programming-guide/"
      },
      {
        "date": "2021-01-12T00:30:00.000Z",
        "voteCount": 3,
        "content": "B - seems to be better option since we need to filter out, question does not specify that we do need to store it into different  Pcollection.\nhttps://beam.apache.org/documentation/transforms/python/overview/ \nParDo is general purpose whereas partition splits the elements into do different pcollections.\nhttps://beam.apache.org/documentation/transforms/python/elementwise/partition/"
      },
      {
        "date": "2020-11-18T23:35:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2020-09-22T22:51:00.000Z",
        "voteCount": 7,
        "content": "Should be B. The Partition transform would require the element identifying the valid/invalid records for partitioning the pcollection that means there is some logic to be executed before the Partition transformation is invoked. That logic can be implemented in a ParDO transform and which can both identify valid/invalid records and also generate two PCollections one with valid records and other with invalid records."
      },
      {
        "date": "2020-08-21T05:01:00.000Z",
        "voteCount": 3,
        "content": "B is correct"
      },
      {
        "date": "2020-07-31T09:52:00.000Z",
        "voteCount": 4,
        "content": "B, ParDo is useful for a variety of common data processing operations, including:\n\nFiltering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it."
      },
      {
        "date": "2020-07-13T13:43:00.000Z",
        "voteCount": 2,
        "content": "Looks like C it is\nhttps://beam.apache.org/documentation/programming-guide/"
      },
      {
        "date": "2020-08-22T15:19:00.000Z",
        "voteCount": 5,
        "content": "according this link its \nPardo\n* Filtering a data set. You can use ParDo to consider each element in a PCollection and either output that element to a new collection or discard it.\n* But Partition just splitting which is is a Beam transform for PCollection objects that store the same data type. Partition splits a single PCollection into a fixed number of smaller collections."
      },
      {
        "date": "2020-12-11T08:56:00.000Z",
        "voteCount": 1,
        "content": "Seems like two answers may be correct. With ParDo you can discard corrupt data. With Partition you can split the data into two PCollections: corrupt and ok. You stream ok data further to BigQuery and corrupt data to some other storage for analysis. If one is not interested in analysis, then ParDo is enough."
      },
      {
        "date": "2020-07-07T06:53:00.000Z",
        "voteCount": 5,
        "content": "Correct answer should be \"C\". A Pardo transform will allow the processing to happening in parallel using multiple workers. Partition transform will allow data to be partitions in two different Pcollections according to some logic. Using partition transform once can split the  corrupted data and finally discard it."
      },
      {
        "date": "2020-07-05T22:22:00.000Z",
        "voteCount": 4,
        "content": "Correct B."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 111,
    "url": "https://www.examtopics.com/discussions/google/view/17248-exam-professional-data-engineer-topic-1-question-111/",
    "body": "You have historical data covering the last three years in BigQuery and a data pipeline that delivers new data to BigQuery daily. You have noticed that when the<br>Data Science team runs a query filtered on a date column and limited to 30`\"90 days of data, the query scans the entire table. You also noticed that your bill is increasing more quickly than you expected. You want to resolve the issue as cost-effectively as possible while maintaining the ability to conduct SQL queries.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-create the tables using DDL. Partition the tables by a column containing a TIMESTAMP or DATE Type.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRecommend that the Data Science team export the table to a CSV file on Cloud Storage and use Cloud Datalab to explore the data by reading the files directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your pipeline to maintain the last 30\u05d2\u20ac\"90 days of data in one table and the longer history in a different table to minimize full table scans over the entire history.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an Apache Beam pipeline that creates a BigQuery table per day. Recommend that the Data Science team use wildcards on the table name suffixes to select the data they need."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T05:15:00.000Z",
        "voteCount": 35,
        "content": "should be A"
      },
      {
        "date": "2020-03-27T23:52:00.000Z",
        "voteCount": 18,
        "content": "Answer: A\nDescription: Partition is the solution for reducing cost and time"
      },
      {
        "date": "2020-05-22T08:01:00.000Z",
        "voteCount": 1,
        "content": "but how would recreating tables with 3 years of data, maintain the ability to conduct sql queries during that time?"
      },
      {
        "date": "2021-10-17T15:50:00.000Z",
        "voteCount": 2,
        "content": "Recreating the new table, the old table will still have new data coming, then append the difference to the new table."
      },
      {
        "date": "2022-12-07T11:38:00.000Z",
        "voteCount": 1,
        "content": "Answer: A, has no cost to reload the data, Also Partition is the solution for reducing cost and time"
      },
      {
        "date": "2022-12-05T04:32:00.000Z",
        "voteCount": 3,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nA partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\n\nYou can partition BigQuery tables by:\n- Time-unit column: Tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table."
      },
      {
        "date": "2022-12-30T12:55:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2022-09-20T02:58:00.000Z",
        "voteCount": 1,
        "content": "it is not B in the sense of cost-effective certainly. read below in limitation\nhttps://cloud.google.com/bigquery/docs/querying-wildcard-tables#limitations\nCurrently, cached results are not supported for queries against multiple tables using a wildcard even if the Use Cached Results option is checked. If you run the same wildcard query multiple times, you are billed for each query."
      },
      {
        "date": "2022-09-20T02:52:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables#dt_partition_shard\nPartitioning is recommended over table sharding, because partitioned tables perform better"
      },
      {
        "date": "2022-09-13T23:07:00.000Z",
        "voteCount": 1,
        "content": "A AND D , they are the most likely choiced but  the  questionn want \nissue as cost-effectively as possible while maintaining the ability to conduct SQL queries.   \n1 table may be cheaper so partition is better than wildcarf"
      },
      {
        "date": "2022-04-25T07:50:00.000Z",
        "voteCount": 2,
        "content": "answer A"
      },
      {
        "date": "2022-01-06T12:25:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables"
      },
      {
        "date": "2022-01-03T22:54:00.000Z",
        "voteCount": 1,
        "content": "A. Partiotioning\nhttps://cloud.google.com/bigquery/docs/partitioned-tables"
      },
      {
        "date": "2021-12-30T07:43:00.000Z",
        "voteCount": 2,
        "content": "Why not D? You can use SQL.\nThis is the cheapest and fastest option \nhttps://cloud.google.com/bigquery/docs/querying-wildcard-tables"
      },
      {
        "date": "2022-09-30T07:59:00.000Z",
        "voteCount": 1,
        "content": "Partitioning is recommended over table sharding, because partitioned tables perform better\nThis is a google recommendation nowaday."
      },
      {
        "date": "2021-11-30T05:40:00.000Z",
        "voteCount": 4,
        "content": "The D solution is obviously discarded. \nThe request NOT require ONLY LAST 30-90 days, so the C solution is not the right solution. \nIn addition to this, the request ask to keep the possibility to made queries, so B is wrost. \nIs not mandatory make the queries while you make the modify so the right answer is A"
      },
      {
        "date": "2021-11-09T12:16:00.000Z",
        "voteCount": 1,
        "content": "B sounds more feasible. \nThe point is 'historical' data, not new table/data. Recreating tables from the past three years is a lot of work. Might as well export the table and run analyses there. No cost for exporting in BigQuery."
      },
      {
        "date": "2021-07-02T08:34:00.000Z",
        "voteCount": 5,
        "content": "Vote for A"
      },
      {
        "date": "2020-11-18T23:40:00.000Z",
        "voteCount": 5,
        "content": "I will go with Option A"
      },
      {
        "date": "2020-11-09T21:20:00.000Z",
        "voteCount": 3,
        "content": "I will go with Option A, although at first instance I felt Option C would be correct. \nOption A : Because partitioning will help to address both the concerns mentioned in the question - i.e. faster query and reducing cost.\nOption C : Modifying the data pipeline to store last 30-90 days data would have possible, if there was a point mentioned that only the latest data (30-90 days) is kept and the older data - beyond 90 days is moved to the master table. Since that point is mot mentioned, we will land up having multiple - 30-90 days data in separate tables + the master table."
      },
      {
        "date": "2021-02-19T22:57:00.000Z",
        "voteCount": 2,
        "content": "but how will you append the data that is older than 90days in to the master table?"
      },
      {
        "date": "2020-11-09T08:06:00.000Z",
        "voteCount": 4,
        "content": "Answer is A. Recreating the DDL with new parition is easy and does not require any changes on applications that read data from it"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 112,
    "url": "https://www.examtopics.com/discussions/google/view/17249-exam-professional-data-engineer-topic-1-question-112/",
    "body": "You operate a logistics company, and you want to improve event delivery reliability for vehicle-based sensors. You operate small data centers around the world to capture these events, but leased lines that provide connectivity from your event collection infrastructure to your event processing infrastructure are unreliable, with unpredictable latency. You want to address this issue in the most cost-effective way. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy small Kafka clusters in your data centers to buffer events.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave the data acquisition devices publish data to Cloud Pub/Sub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a Cloud Interconnect between all remote data centers and Google.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Cloud Dataflow pipeline that aggregates all data in session windows."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T05:58:00.000Z",
        "voteCount": 31,
        "content": "Should be B"
      },
      {
        "date": "2020-04-13T02:20:00.000Z",
        "voteCount": 22,
        "content": "C.\nThis is a tricky one. The issue here is the unreliable connection between data collection and data processing infrastructure, and to resolve it in a cost-effective manner. However, it also mentions that the company is using leased lines. I think replacing the leased lines with Cloud InterConnect would solve the problem, and hopefully not be an added expense. \nhttps://cloud.google.com/interconnect/docs/concepts/overview"
      },
      {
        "date": "2020-06-03T11:55:00.000Z",
        "voteCount": 7,
        "content": "Yea, this would definitely solve the issue, but it's not \"the most cost-effective way\". I think PubSub is the correct answer."
      },
      {
        "date": "2020-11-19T08:22:00.000Z",
        "voteCount": 3,
        "content": "the question also talks about a cost effective way..."
      },
      {
        "date": "2020-06-19T08:01:00.000Z",
        "voteCount": 5,
        "content": "I agree, C is the only choice that addresses the problem. The problem is caused by leased line. How come pub/sub service can resolve it? Pub/sub will still use the leased line"
      },
      {
        "date": "2021-07-05T16:05:00.000Z",
        "voteCount": 7,
        "content": "DEFINITELY NOT COST EFFECT. C IS THE WORST CHOICE."
      },
      {
        "date": "2024-06-17T07:41:00.000Z",
        "voteCount": 1,
        "content": "Option B: Have the data acquisition devices publish data to Cloud Pub/Sub.\n\nRationale:\n\nManaged Service: Cloud Pub/Sub is a fully managed service, reducing the operational overhead compared to managing Kafka clusters.\nReliability and Scalability: Cloud Pub/Sub can handle high volumes of data with low latency and provides built-in mechanisms for reliable message delivery, even in the face of intermittent connectivity.\nCost-Effective: Cloud Pub/Sub offers a pay-as-you-go pricing model, which can be more cost-effective than setting up and maintaining dedicated network infrastructure like Cloud Interconnect.\nGlobal Availability: Cloud Pub/Sub is available globally and can handle data from multiple regions efficiently."
      },
      {
        "date": "2023-12-16T05:43:00.000Z",
        "voteCount": 1,
        "content": "Even with Cloud Pub/Sub, unpredictable latency or delays could still occur due to the unreliable leased lines connecting your event collection infrastructure and event processing infrastructure. While Cloud Pub/Sub offers reliable message delivery within its own network, the handoff to your processing infrastructure is still dependent on the leased lines.\nReplacing leased lines with Cloud Interconnect could potentially resolve the overall issue of unpredictable latency in event processing pipeline but it could be unnecessary expense provided data centers distributed world wide.\nCloud Pub/Sub along with other optimization techniques like Cloud VPN or edge computing might be sufficient."
      },
      {
        "date": "2023-08-13T04:04:00.000Z",
        "voteCount": 1,
        "content": "I don't know why B is the most voted. The issue here is unreliable connectivity and C is the perfect use-case for that"
      },
      {
        "date": "2023-08-05T06:44:00.000Z",
        "voteCount": 1,
        "content": "its says with unpredictable latency and here no need to worry about connection  \nSo B is the right one"
      },
      {
        "date": "2023-07-07T20:09:00.000Z",
        "voteCount": 1,
        "content": "The question is misleading. But should be C since it addresses the unpredictablility and latency directly."
      },
      {
        "date": "2023-02-16T07:48:00.000Z",
        "voteCount": 2,
        "content": "Best answer is A, By using Kafka, you can buffer the events in the data centers until a reliable connection is established with the event processing infrastructure.\nBut go with B, its google asking :P"
      },
      {
        "date": "2023-02-24T10:06:00.000Z",
        "voteCount": 1,
        "content": "I read this question again, I wanna answer C. Buying Data acquisition devices and set them up with sensor, i dont think its practical approach. Imagine, Adruino is cheapest IOT available in market for 15 dollars, but who will open the sensor box and install it .. omg,, its a big job. This question depends if IOT devices that are attached to sensor needs to be programmed. Big Headache right. Use google cloud connect to deal with current situation. Or reprogramme IOT if they have connected with sensors."
      },
      {
        "date": "2023-01-28T22:10:00.000Z",
        "voteCount": 10,
        "content": "B. Have the data acquisition devices publish data to Cloud Pub/Sub. This would provide a reliable messaging service for your event data, allowing you to ingest and process your data in a timely manner, regardless of the reliability of the leased lines. Cloud Pub/Sub also offers automatic retries and fault-tolerance, which would further improve the reliability of your event delivery. Additionally, using Cloud Pub/Sub would allow you to easily scale up or down your event processing infrastructure as needed, which would help to minimize costs."
      },
      {
        "date": "2023-01-19T14:37:00.000Z",
        "voteCount": 2,
        "content": "Are they talking about GCP in this question?\nWhere is the event processing infrastructure?\n\nAnswer A, might be correct!"
      },
      {
        "date": "2022-12-26T22:03:00.000Z",
        "voteCount": 1,
        "content": "pub/sub is region is a global service\nIt's important to note that the term \"global\" in this context refers to the geographical scope of the service"
      },
      {
        "date": "2022-12-15T07:16:00.000Z",
        "voteCount": 2,
        "content": "As usual the answer is hidden somewhere in the Google Cloud Blog:\n\"In the case of our automotive company, the data is already stored and processed in local data centers in different regions. This happens by streaming all sensor data from the cars via MQTT to local Kafka Clusters that leverage Confluent\u2019s MQTT Proxy.\"\n\"This integration from devices to a local Kafka cluster typically is its own standalone project, because you need to handle IoT-specific challenges like constrained devices and unreliable networks.\"\n\n\ud83d\udd17 https://cloud.google.com/blog/products/ai-machine-learning/enabling-connected-transformation-with-apache-kafka-and-tensorflow-on-google-cloud-platform"
      },
      {
        "date": "2023-01-19T14:35:00.000Z",
        "voteCount": 2,
        "content": "The question is asking from the on-premise infrastructure, which already has the data, to the  event processing infrastructure, which is in the GCP, is unreliable.... \n\nit not asking from the sensors to the on-premise..."
      },
      {
        "date": "2023-01-19T14:38:00.000Z",
        "voteCount": 1,
        "content": "I might have to retract my answer... Are they talking about GCP in this question?\nwhere is the event processing infrastructure?"
      },
      {
        "date": "2022-12-05T04:30:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-30T12:58:00.000Z",
        "voteCount": 1,
        "content": "yes it is  B. Have the data acquisition devices publish data to Cloud Pub/Sub."
      },
      {
        "date": "2022-11-21T07:28:00.000Z",
        "voteCount": 1,
        "content": "yeah, changing whole architecture arround the world for the use of pub/sub is so much more cost efficient than Cloud Interconnect (which is like 3k$).. \n\nIt's C."
      },
      {
        "date": "2022-12-07T11:41:00.000Z",
        "voteCount": 1,
        "content": "It's not a Cloud Interconnect, it's a lot of interconnect ones per data center, PUB/SUB addresses all the requirements. Its B"
      },
      {
        "date": "2022-12-07T11:44:00.000Z",
        "voteCount": 1,
        "content": "ALSO, the problem it's no t your connection, its the connectivity BT your event collection infrastructure to your event processing infrastructure, so PUSUB it's perfect for this"
      },
      {
        "date": "2022-12-07T17:59:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't using cloud interconnect also result in amendments to each of the data center around the world? I don't see why there would be a huge architecture change when using PubSub, the publishers would just need to push messages directly to pubsub, instead of pushing to their own cost center.\n\nAlso, if the script for pushing messages can be standardised, the data centers can share it around to"
      },
      {
        "date": "2022-09-13T23:51:00.000Z",
        "voteCount": 1,
        "content": "Cloud Pub/Sub, it supports  batch &amp; streaming , push and pull capabilities \nAnswer B"
      },
      {
        "date": "2022-08-20T19:42:00.000Z",
        "voteCount": 1,
        "content": "It has to be B."
      },
      {
        "date": "2022-08-15T10:25:00.000Z",
        "voteCount": 2,
        "content": "Feels like everyone is wrong. \n\nA. Deploy small Kafka clusters in your data centers to buffer events.\n             - Silly in a GCP cloudnative context, plus they have messaging infra anyway\nB. Have the data acquisition devices publish data to Cloud Pub/Sub.\n             - They have messaging infra, so why? Unless they want to replace, it, but that doesn't change the issue\nC. Establish a Cloud Interconnect between all remote data centers and Google.\n             - Wrong, because Interconnect is basically a leased line. There must be some telecoms issue with it, which we can assume is unresolvable e.g. long distance remote locations and sometimes water ingress, and the telco can't justify sorting it yet, or is slow to, or something. Leased lines usually don't come with awful internet connectivity, so sound physical connectivity issue. Sure, an Interconnect is better, more direct, but a leased line should be bullet proof. \nD. Write a Cloud Dataflow pipeline that aggregates all data in session windows.\n             - The only way to address dodgy/delayed data delivery"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 113,
    "url": "https://www.examtopics.com/discussions/google/view/16856-exam-professional-data-engineer-topic-1-question-113/",
    "body": "You are a retailer that wants to integrate your online sales capabilities with different in-home assistants, such as Google Home. You need to interpret customer voice commands and issue an order to the backend systems. Which solutions should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpeech-to-Text API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Natural Language API",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDialogflow Enterprise Edition\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutoML Natural Language"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T04:12:00.000Z",
        "voteCount": 26,
        "content": "should be C, since we need to recognize both voice and intent"
      },
      {
        "date": "2022-12-30T12:59:00.000Z",
        "voteCount": 1,
        "content": "C. Dialogflow Enterprise Editio"
      },
      {
        "date": "2020-11-09T23:05:00.000Z",
        "voteCount": 19,
        "content": "Option A - Cloud Speech-to-Text API. \nThe question is just asking to \" interpret customer voice commands\" .. it does not mention anything related to sentiment analysis so NLP is not required. DialogFlow is more of a chat bot services typically suited for a \"Service Desk\" kind of setup - where clients will call a centralized helpdesk and automation is achieved through Chat bot services like - google Dialog flow"
      },
      {
        "date": "2021-07-27T04:52:00.000Z",
        "voteCount": 8,
        "content": "Cloud Speech-to-Text API just converts speech to text. You will have text files as an output and then the requirement is to \"interpret customer voice commands and issue an order to the backend systems\". This is not achieved by having text files.\n\nI would go with option C, since Dialogflow can interpret the commands (intents) and integrates other applications e.g. backend systems."
      },
      {
        "date": "2023-09-22T01:36:00.000Z",
        "voteCount": 1,
        "content": "shuld be C, the key is interpret customer voice commands"
      },
      {
        "date": "2022-09-07T19:20:00.000Z",
        "voteCount": 2,
        "content": "Question also says \"in-home assistants, such as Google Home\". the idea here is to provide assistance which involves Dialog. \n\nI would go with option C"
      },
      {
        "date": "2024-09-25T13:55:00.000Z",
        "voteCount": 1,
        "content": "This is a queston from Actual exam question from Google's Professional Data Engineer so C makes sense. This is not an AWS question"
      },
      {
        "date": "2024-06-04T15:40:00.000Z",
        "voteCount": 1,
        "content": "The question clearly states \"voice commands\" which is a term for short (few words long at most) well-defined phrases to be recognized. No need for a dialog.\nEven if I were to use Dialogflow, I would use ES instead of CX (new name for Enterprise Edition), no fancy features are required for this."
      },
      {
        "date": "2023-08-05T06:49:00.000Z",
        "voteCount": 1,
        "content": "Ans C . main thing is that question is saying\" customer voice commands \" there is no need to sentimental analysis of language so thats why.\n\nC. Dialogflow Enterprise Edition\n\nDialogflow is a powerful natural language understanding platform developed by Google. It allows you to build conversational interfaces, interpret user voice commands, and integrate with various platforms and devices like Google Home. The \"Enterprise Edition\" provides additional features and support for more complex use cases, making it a good choice for a retailer looking to integrate with in-home assistants and handle customer voice commands effectively."
      },
      {
        "date": "2023-03-19T05:25:00.000Z",
        "voteCount": 2,
        "content": "Answer is C. However Google Assistant Conversational Actions will be sunsetted on June 13, 2023."
      },
      {
        "date": "2023-02-14T17:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dialogflow/es/docs/integrations/aog"
      },
      {
        "date": "2023-01-19T14:44:00.000Z",
        "voteCount": 1,
        "content": "I think the answer is A: Speech to Text.\nYou want to interpret what a user say... Diagflow is text to speech, not what the question asked for...\n\nThoughts?"
      },
      {
        "date": "2022-12-26T22:05:00.000Z",
        "voteCount": 1,
        "content": "The question is just asking to \" interpret customer voice commands\" so A is out of the box solution"
      },
      {
        "date": "2022-12-07T11:53:00.000Z",
        "voteCount": 1,
        "content": "Enable voice control\n\nImplement voice commands such as \u201cturn the volume up,\u201d and voice search such as saying \u201cwhat is the temperature in Paris?\u201d Combine this with the Text-to-Speech API to deliver voice-enabled experiences in IoT (Internet of Things) applications.\nhttps://cloud.google.com/speech-to-text#section-9"
      },
      {
        "date": "2022-12-19T10:54:00.000Z",
        "voteCount": 5,
        "content": "I change my mind, it's C.\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps"
      },
      {
        "date": "2022-12-05T04:28:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/dialogflow/es/docs\nDialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product.\n\nDialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech."
      },
      {
        "date": "2022-09-13T23:55:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps \n\nDialogflow is the answer"
      },
      {
        "date": "2022-07-19T00:39:00.000Z",
        "voteCount": 2,
        "content": "Dialogflow provides a seamless integration with Google Assistant. This integration has the following advantages: You can use the same Dialogflow agent to power Google Assistant and other integrations. Dialogflow agents provide Google Cloud enterprise-grade security, privacy, support, and SLAs"
      },
      {
        "date": "2022-04-22T06:43:00.000Z",
        "voteCount": 1,
        "content": "dialog"
      },
      {
        "date": "2022-04-22T06:06:00.000Z",
        "voteCount": 1,
        "content": "speech"
      },
      {
        "date": "2022-03-25T04:59:00.000Z",
        "voteCount": 1,
        "content": "It should be D. INTERPRET customer voice commands and issue an order to the backend systems. Option C is usually applied for conversation. But in this case, it is not a conversation."
      },
      {
        "date": "2022-07-26T18:00:00.000Z",
        "voteCount": 1,
        "content": "it can totally be a conversation. if the system is attempting to interpret voice commands, it may need clarifying questions, which would in turn be a conversation. it's not D because autoML NL analyzes TEXT. it would have to work in conjunction with speech to text API. therefore the answer is dialogflow."
      },
      {
        "date": "2022-01-06T12:30:00.000Z",
        "voteCount": 3,
        "content": "recognize voice and intent\nhttps://cloud.google.com/blog/products/gcp/introducing-dialogflow-enterprise-edition-a-new-way-to-build-voice-and-text-conversational-apps"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 114,
    "url": "https://www.examtopics.com/discussions/google/view/16628-exam-professional-data-engineer-topic-1-question-114/",
    "body": "Your company has a hybrid cloud initiative. You have a complex data pipeline that moves data between cloud provider services and leverages services from each of the cloud providers. Which cloud-native service should you use to orchestrate the entire pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataprep",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataproc"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-14T20:16:00.000Z",
        "voteCount": 30,
        "content": "Answer should be B"
      },
      {
        "date": "2020-03-22T06:29:00.000Z",
        "voteCount": 12,
        "content": "Answer - B"
      },
      {
        "date": "2023-05-31T10:31:00.000Z",
        "voteCount": 2,
        "content": "No other option is aimed for this purpose"
      },
      {
        "date": "2023-03-19T05:27:00.000Z",
        "voteCount": 1,
        "content": "Airflow"
      },
      {
        "date": "2022-12-26T22:06:00.000Z",
        "voteCount": 2,
        "content": "Cloud Composer is Airflow"
      },
      {
        "date": "2022-12-07T11:57:00.000Z",
        "voteCount": 3,
        "content": "Cloud Composer is Airflow, It's made for this job."
      },
      {
        "date": "2022-12-05T04:24:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers."
      },
      {
        "date": "2022-12-30T13:01:00.000Z",
        "voteCount": 2,
        "content": "Cloud composer is right"
      },
      {
        "date": "2022-01-06T12:31:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/composer/"
      },
      {
        "date": "2022-01-03T23:05:00.000Z",
        "voteCount": 6,
        "content": "B:\nCloud Composer is a fully managed workflow orchestration service that empowers you to author, schedule, and monitor pipelines that span across clouds and on-premises data centers.\nhttps://cloud.google.com/composer/\nCloud Composer can help create workflows that connect data, processing, and services across clouds, giving you a unified data environment.\nBuilt on the popular Apache Airflow open source project and operated using the Python programming language, Cloud Composer is free from lock-in and easy to use.\nCloud Composer gives you the ability to connect your pipeline through a single orchestration tool whether your workflow Eves on-premises, in multiple clouds, or fully within GCP. The ability to author, schedule, and monitor your workflows in a unified manner means you can break down the silos in your environment and focus less on infrastructure."
      },
      {
        "date": "2022-01-03T23:06:00.000Z",
        "voteCount": 1,
        "content": "Option A is wrong as Cloud Scheduler is a fully managed enterprise-grade cron job scheduler. It is not a multi-cloud orchestration tool.\nOption B is wrong as Google Cloud Dataflow is a fully managed service for strongly consistent, parallel data-processing pipelines. It does not support multi-cloud handling.\nOption D is wrong as Google Cloud Dataproc is a fast, easy to use, managed Spark and Hadoop service for distributed data processing."
      },
      {
        "date": "2021-11-25T22:34:00.000Z",
        "voteCount": 3,
        "content": "Answer: B"
      },
      {
        "date": "2021-08-09T00:35:00.000Z",
        "voteCount": 3,
        "content": "Cloud composer"
      },
      {
        "date": "2021-07-02T08:55:00.000Z",
        "voteCount": 5,
        "content": "Vote for B"
      },
      {
        "date": "2021-03-12T06:49:00.000Z",
        "voteCount": 4,
        "content": "B:\nHybrid and multi-cloud\nEase your transition to the cloud or maintain a hybrid data environment by orchestrating workflows that cross between on-premises and the public cloud. Create workflows that connect data, processing, and services across clouds to give you a unified data environment.\nhttps://cloud.google.com/composer#section-2"
      },
      {
        "date": "2021-02-06T22:53:00.000Z",
        "voteCount": 3,
        "content": "Correct B: without any doubt."
      },
      {
        "date": "2020-11-19T00:13:00.000Z",
        "voteCount": 4,
        "content": "B-Cloud Composer works on a multicloud environment"
      },
      {
        "date": "2020-11-09T23:07:00.000Z",
        "voteCount": 5,
        "content": "there can not be any simple question like this to choose the right answer as \"Cloud Composer\". I really feel someone must have deliberately selecting the wrong answers in Exam topics to confuse people...."
      },
      {
        "date": "2020-11-09T08:15:00.000Z",
        "voteCount": 4,
        "content": "Composer is the obvious answer. so B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 115,
    "url": "https://www.examtopics.com/discussions/google/view/79459-exam-professional-data-engineer-topic-1-question-115/",
    "body": "You use a dataset in BigQuery for analysis. You want to provide third-party companies with access to the same dataset. You need to keep the costs of data sharing low and ensure that the data is current. Which solution should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Analytics Hub to control data access, and provide third party companies with access to the dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-02T11:15:00.000Z",
        "voteCount": 23,
        "content": "I feel the answer really should be Create an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view."
      },
      {
        "date": "2022-12-05T03:35:00.000Z",
        "voteCount": 10,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/analytics-hub-introduction\nnalytics Hub is a data exchange platform that enables you to share data and insights at scale across organizational boundaries with a robust security and privacy framework.\n\nAs an Analytics Hub publisher, you can monetize data by sharing it with your partner network or within your own organization in real time. Listings let you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences. You can also manage subscriptions to your listings."
      },
      {
        "date": "2023-07-27T02:01:00.000Z",
        "voteCount": 2,
        "content": "Option A: This option is correct because Analytics Hub is a managed service that provides a centralized repository for data assets. You can use Analytics Hub to share data with other Google Cloud Platform services, as well as with third-party companies"
      },
      {
        "date": "2023-02-16T08:07:00.000Z",
        "voteCount": 1,
        "content": "You are preparing for exam: \nCreating a view and share with 3rd party is best and cheapest. \nThen create a separate dataset to share it cost less than using paid service for data access i.e analytics hub where you create data access policies\nyou choose, its just making me craazy"
      },
      {
        "date": "2023-02-24T10:12:00.000Z",
        "voteCount": 1,
        "content": "One main reason you should use analytics hub, when you want control over 3 party activites and you want to monetize ( to make money ) by sharing BQ dataset."
      },
      {
        "date": "2023-01-22T12:59:00.000Z",
        "voteCount": 2,
        "content": "Shared datasets are collections of tables and views in BigQuery defined by a data publisher and make up the unit of cross-project / cross-organizational sharing. Data subscribers get an opaque, read-only, linked dataset inside their project and VPC perimeter that they can combine with their own datasets and connect to solutions from Google Cloud or our partners. For example, a retailer might create a single exchange to share demand forecasts to the 1,000\u2019s of vendors in their supply chain\u2013having joined historical sales data with weather, web clickstream, and Google Trends data in their own BigQuery project, then sharing real-time outputs via Analytics Hub. The publisher can add metadata, track subscribers, and see aggregated usage metrics."
      },
      {
        "date": "2022-12-30T13:01:00.000Z",
        "voteCount": 1,
        "content": "A. Use Analytics Hub to control data access, and provide third party companies with access to the dataset."
      },
      {
        "date": "2022-12-07T12:00:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/analytics-hub"
      },
      {
        "date": "2022-11-25T12:40:00.000Z",
        "voteCount": 1,
        "content": "A\nMultiple choose listed wrongly \nCorrect one \nA. \nCreate an authorized view on the BigQuery table to control data access, and provide third-party companies with access to that view.\nB.\nUse Cloud Scheduler to export the data on a regular basis to Cloud Storage, and provide third-party companies with access to the bucket.\nc.\nCreate a separate dataset in BigQuery that contains the relevant data to share, and provide third-party companies with access to the new dataset.\nD.\nCreate a Cloud Dataflow job that reads the data in frequent time intervals, and writes it to the relevant BigQuery dataset or Cloud Storage bucket for third-party companies to use."
      },
      {
        "date": "2022-10-28T08:02:00.000Z",
        "voteCount": 2,
        "content": "no option is correct\nthis is correct answer -Create an authorised view on the BigQuery table to control data access, and provide third-party companies with access to that view."
      },
      {
        "date": "2022-09-03T06:11:00.000Z",
        "voteCount": 1,
        "content": "A. Use Analytics Hub to control data access, and provide third party companies with access to the dataset."
      },
      {
        "date": "2022-09-02T09:00:00.000Z",
        "voteCount": 1,
        "content": "Answer A.\nAs an Analytics Hub user, you can perform the following tasks:\n\n    As an Analytics Hub publisher, you can monetize data by sharing it with your partner network or within your own organization in real time. Listings let you share data without replicating the shared data. You can build a catalog of analytics-ready data sources with granular permissions that let you deliver data to the right audiences.\n\n    As an Analytics Hub subscriber, you can discover the data that you are looking for, combine shared data with your existing data, and leverage the built-in features of BigQuery. When you subscribe to a listing, a linked dataset is created in your project.\n\n    As an Analytics Hub viewer, you can browse through the datasets that you have access to in Analytics Hub and request the publisher to access the shared data.\n\n    As an Analytics Hub administrator, you can create data exchanges that enable data sharing, and then give permissions to data publishers and subscribers to access these data exchanges.\nhttps://cloud.google.com/bigquery/docs/analytics-hub-introduction"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 116,
    "url": "https://www.examtopics.com/discussions/google/view/79672-exam-professional-data-engineer-topic-1-question-116/",
    "body": "Your company is in the process of migrating its on-premises data warehousing solutions to BigQuery. The existing data warehouse uses trigger-based change data capture (CDC) to apply updates from multiple transactional database sources on a daily basis. With BigQuery, your company hopes to improve its handling of<br>CDC so that changes to the source systems are available to query in BigQuery in near-real time using log-based CDC streams, while also optimizing for the performance of applying changes to the data warehouse. Which two steps should they take to ensure that changes are available in the BigQuery reporting table with minimal latency while reducing compute overhead? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform a DML INSERT, UPDATE, or DELETE to replicate each individual CDC record in real time directly on the reporting table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert each new CDC record and corresponding operation type to a staging table in real time.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPeriodically DELETE outdated records from the reporting table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPeriodically use a DML MERGE to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert each new CDC record and corresponding operation type in real time to the reporting table, and use a materialized view to expose only the newest version of each unique record."
    ],
    "answer": "BD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BD",
        "count": 27,
        "isMostVoted": true
      },
      {
        "answer": "CE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T01:29:00.000Z",
        "voteCount": 15,
        "content": "To aim for minimal latency while reducing compute overhead:\n\nB. Insert each new CDC record and corresponding operation type to a staging table in real time.\n\nD. Periodically use a DML MERGE to perform several DML INSERT, UPDATE, and DELETE operations at the same time on the reporting table. (all statements comes from the staging table)"
      },
      {
        "date": "2023-02-24T10:16:00.000Z",
        "voteCount": 6,
        "content": "B&amp;D\nTricks here: Always choose google recommended approach, Use data first in Staging table then merge with original tables."
      },
      {
        "date": "2023-10-06T03:25:00.000Z",
        "voteCount": 2,
        "content": "I'm going for E &amp;C - this in the only solution with low TCO. \nE - is the best way to work with CDC when real nearline data is needed BQ snapshots can be online! . &amp; C - is good practice to delete old records."
      },
      {
        "date": "2024-08-22T02:37:00.000Z",
        "voteCount": 1,
        "content": "A isn't correct, as the requirement is \"reducing compute overhead\"\nB isn't correct, as there is no mention of a \"staging table\" in the scenario\nD isn't correct as it's done periodically, and the requirement is \"near real-time\""
      },
      {
        "date": "2022-12-28T09:18:00.000Z",
        "voteCount": 2,
        "content": "with both the delta table and the main table  changes could be queried in near realtime, by using a view that unions both tables and queries the laters record for the given key, eventually the delta table should be merged into the main table and truncated. Google recently introduced datastream that would take away all these headaches."
      },
      {
        "date": "2022-12-07T12:08:00.000Z",
        "voteCount": 2,
        "content": "The solution is B and D. I perform a similar task in my work, and this is the best way to do it at scale with BigQuery."
      },
      {
        "date": "2022-12-05T03:32:00.000Z",
        "voteCount": 4,
        "content": "BD is the answer.\n\nhttps://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#overview_of_cdc_data_replication\nDelta tables contain all change events for a particular table since the initial load. Having all change events available can be valuable for identifying trends, the state of the entities that a table represents at a particular moment, or change frequency.\n\nThe best way to merge data frequently and consistently is to use a MERGE statement, which lets you combine multiple INSERT, UPDATE, and DELETE statements into a single atomic operation."
      },
      {
        "date": "2022-11-21T01:44:00.000Z",
        "voteCount": 2,
        "content": "I really can\u2019t find a correct combination of answers. I'm between the following alternatives, but with no one fitting:\n1\ufe0f\u20e3 [B] and [D]: That's a proposed solution, but as a cost-optimized approach (along with an extra step to \"Periodically DELETE outdated records from the STAGING table\" - more details on my subsequent reply). Also, I can't imagine how an answer with the word \"Periodically\" may be compatible with the \"minimal latency\" requirement.\n2\ufe0f\u20e3 [E] and [C]: It could be a valid approach, but near-real time requirement would demand also for a materialized view refresh. And it seems to contradict the \"reducing compute overhead\" req.\n3\ufe0f\u20e3 [A] standalone: Provides immediate results but is far from compute-optimized."
      },
      {
        "date": "2022-11-21T01:49:00.000Z",
        "voteCount": 5,
        "content": "The previous guidelines were here: \n\ud83d\udd17 https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#immediate_consistency_approach\nThere were two approaches:\n1\ufe0f\u20e3 Immediate consistency approach\n2\ufe0f\u20e3 Cost-optimized approach\nFor approach 1\ufe0f\u20e3, which is the objective of this question, it proposes:\n\ta. Insert CDC data into a delta table in BigQuery =&gt; that's answer [B]\n\tb. Create a BigQuery view that joins the main and delta tables and finds the most recent row =&gt; there' no answer that fits\nFor approach 2\ufe0f\u20e3 it proposes:\n\ta. Insert CDC data into a delta table in BigQuery =&gt; that's answer [B]\n\tb. Merge delta table changes into the main table and periodically purge merged rows from the delta table - Run Merge statement on a regular interval =&gt; that's answer [D]"
      },
      {
        "date": "2022-11-21T01:48:00.000Z",
        "voteCount": 4,
        "content": "Nowadays (Nov. 2022) I don't expect to confront this question in a real exam with this set of answers since the more recent documentation proposes the use of Datastream.\n\ud83d\udd17 https://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery"
      },
      {
        "date": "2022-10-31T15:30:00.000Z",
        "voteCount": 2,
        "content": "B and E. Typically in a Data Warehouse you don't delete date. Data Warehouse should store full history to see how the data changed over time. All the solutions with 'DELETE' should not be used as this goes against being able to access the history of the data."
      },
      {
        "date": "2022-10-10T14:36:00.000Z",
        "voteCount": 1,
        "content": "https://www.striim.com/blog/oracle-to-google-bigquery/"
      },
      {
        "date": "2022-09-14T00:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#data_latency"
      },
      {
        "date": "2022-09-14T00:27:00.000Z",
        "voteCount": 1,
        "content": "https://docs.streamsets.com/platform-datacollector/latest/datacollector/UserGuide/Destinations/GBigQuery.html"
      },
      {
        "date": "2022-09-13T14:27:00.000Z",
        "voteCount": 4,
        "content": "B and D \nhttps://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture"
      },
      {
        "date": "2022-09-13T04:25:00.000Z",
        "voteCount": 2,
        "content": "B D you have to do the both to get it done.\nTo merge process, you have to perform between the report table and stage a table"
      },
      {
        "date": "2022-09-07T22:31:00.000Z",
        "voteCount": 1,
        "content": "Answers are tricky, official documentation suggests Dataflow or Datafusion path as well as inclusion of DataStreams\nhttps://cloud.google.com/blog/products/data-analytics/real-time-cdc-replication-bigquery"
      },
      {
        "date": "2022-09-06T15:31:00.000Z",
        "voteCount": 2,
        "content": "It costs more to update/delete."
      },
      {
        "date": "2022-09-02T22:25:00.000Z",
        "voteCount": 1,
        "content": "BE is correct\nBig Query only need to capture change, no need DELETE, UPDATE"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 117,
    "url": "https://www.examtopics.com/discussions/google/view/16629-exam-professional-data-engineer-topic-1-question-117/",
    "body": "You are designing a data processing pipeline. The pipeline must be able to scale automatically as load increases. Messages must be processed at least once and must be ordered within windows of 1 hour. How should you design the solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Kafka for message ingestion and use Cloud Dataproc for streaming analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Kafka for message ingestion and use Cloud Dataflow for streaming analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Pub/Sub for message ingestion and Cloud Dataproc for streaming analysis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Pub/Sub for message ingestion and Cloud Dataflow for streaming analysis.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-14T20:23:00.000Z",
        "voteCount": 27,
        "content": "Answer should be D"
      },
      {
        "date": "2020-03-22T04:25:00.000Z",
        "voteCount": 14,
        "content": "Answer - D"
      },
      {
        "date": "2023-08-05T07:02:00.000Z",
        "voteCount": 1,
        "content": "Data proc is serverbased \nDataflow is serverless which is used to run pipelines which uses apache framework in the background. Just\nneed to mention the number of workers needed.\n\nso question saying we need scale automatically . so dataproc eliminate ho gaya \nnow Dataflow is correct , pub/sub is recommended for this scenario.  D"
      },
      {
        "date": "2022-12-28T09:20:00.000Z",
        "voteCount": 2,
        "content": "google's preferred choice"
      },
      {
        "date": "2022-12-05T03:24:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-07-06T02:28:00.000Z",
        "voteCount": 1,
        "content": "Answer should be D"
      },
      {
        "date": "2022-04-26T08:30:00.000Z",
        "voteCount": 1,
        "content": "It cannot be C because Dataproc is more suitable for Hadoop jobs."
      },
      {
        "date": "2022-01-06T12:34:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub + Dataflow"
      },
      {
        "date": "2022-01-05T10:45:00.000Z",
        "voteCount": 4,
        "content": "D: Pub/Sub + Dataflow\nhttps://cloud.google.com/solutions/stream-analytics/\nhttps://cloud.google.com/blog/products/data-analytics/streaming-analytics-now-simpler-more-cost-effective-cloud-dataflow"
      },
      {
        "date": "2021-12-18T18:38:00.000Z",
        "voteCount": 3,
        "content": "D: \"at least once and must be ordered within windows\" means Pub/Sub (at least once) with Dataflow (windows)."
      },
      {
        "date": "2021-11-25T22:49:00.000Z",
        "voteCount": 3,
        "content": "Correct: D"
      },
      {
        "date": "2021-09-30T11:04:00.000Z",
        "voteCount": 8,
        "content": "rule of thumb: If you see Kafka and Pub/Sub, always go with Pub/Sub in Google exam"
      },
      {
        "date": "2021-12-19T22:18:00.000Z",
        "voteCount": 6,
        "content": "Careful doing that: I got a question where you had to choose between Kafka and Pub/Sub... and the solution required to be able to replay all messages without time limit. So no Pub/Sub there.\nThis being a Google cert does not mean that they always force Google solutions."
      },
      {
        "date": "2021-08-09T00:43:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2021-07-05T16:31:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/architecture/migrating-from-kafka-to-pubsub#comparing_features"
      },
      {
        "date": "2021-07-02T13:51:00.000Z",
        "voteCount": 3,
        "content": "Vote for D\n\nScaling - Dataflow.\nDelivery of confimed atleast 1 message - Pub/Sub"
      },
      {
        "date": "2021-02-11T11:30:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2020-11-10T02:03:00.000Z",
        "voteCount": 5,
        "content": "Indeed the correct answer is Option D. \nAgain, not sure why Exam topic answer is deliberately chosen for a wrong answer, for such simple question."
      },
      {
        "date": "2021-11-03T15:04:00.000Z",
        "voteCount": 1,
        "content": "To make us think of each question while studying, not just trying to memorize answers :) it looks that \"correct\" answers are chosen randomly :)"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 118,
    "url": "https://www.examtopics.com/discussions/google/view/79462-exam-professional-data-engineer-topic-1-question-118/",
    "body": "You need to set access to BigQuery for different departments within your company. Your solution should comply with the following requirements:<br>\u2711 Each department should have access only to their data.<br>\u2711 Each department will have one or more leads who need to be able to create and update tables and provide them to their team.<br>\u2711 Each department has data analysts who need to be able to query but not modify data.<br>How should you set access to the data in BigQuery?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset for each department. Assign the department leads the role of OWNER, and assign the data analysts the role of WRITER on their dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a dataset for each department. Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table for each department. Assign the department leads the role of Owner, and assign the data analysts the role of Editor on the project the table is in.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table for each department. Assign the department leads the role of Editor, and assign the data analysts the role of Viewer on the project the table is in."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-19T06:06:00.000Z",
        "voteCount": 11,
        "content": "Old question. It's done using IAM nowadays: bigquery.dataEditor and bigquery.dataViewer"
      },
      {
        "date": "2022-09-03T06:13:00.000Z",
        "voteCount": 8,
        "content": "B. Create a dataset for each department. Assign the department leads the role of WRITER, and assign the data analysts the role of READER on their dataset."
      },
      {
        "date": "2024-03-05T09:13:00.000Z",
        "voteCount": 1,
        "content": "WRITER role is not there in roles of BigQuery table/dataset"
      },
      {
        "date": "2023-12-21T01:15:00.000Z",
        "voteCount": 1,
        "content": "Answer : D. There is no role called WRITER or READER as preliminary role."
      },
      {
        "date": "2023-05-31T12:02:00.000Z",
        "voteCount": 1,
        "content": "both C &amp; D violate the principle of least privilege.\nA talks about OWNER and WRITER roles, and the analyst doesn't need a writer role.\nSo we're left with B."
      },
      {
        "date": "2023-04-19T07:36:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/access-control#bigquery"
      },
      {
        "date": "2023-03-12T18:29:00.000Z",
        "voteCount": 1,
        "content": "B - Lead needs to have the role to create tables and also Analyst only need to read"
      },
      {
        "date": "2023-02-24T10:21:00.000Z",
        "voteCount": 5,
        "content": "Answer B:\nWhy not D, mentioned in question: Data lead will create tables in dataset. Imagine, other department leads are creating unnecessory tables in shared dataset and you are struggling to find your tables as everyday there are some new tables. Headache right ? better to give them seperate dataset and do whatever you want in that dataset."
      },
      {
        "date": "2023-02-09T11:15:00.000Z",
        "voteCount": 3,
        "content": "Vote B, both BD can fullfill the job requirement but B is on dataset level and D on project level. \"By default, granting access to a project also grants access to datasets within it.\" D may issue unnecessary  accesses to other content in the project."
      },
      {
        "date": "2023-01-19T19:16:00.000Z",
        "voteCount": 1,
        "content": "Interestingly enough - I know believe the answer is A...\nDeleting is not the same as modify..."
      },
      {
        "date": "2023-01-19T19:03:00.000Z",
        "voteCount": 1,
        "content": "Answer is B: https://cloud.google.com/bigquery/docs/access-control\nThe question ask for the lead to be able to: \nCREATE, UPDATE, and SHARE with the team...\n\nBigQuery Data Owner  can do that\n(roles/bigquery.dataOwner)\nWhen applied to a table or view, this role provides permissions to:\n\nRead and update data and metadata for the table or view.\nShare the table or view.\nDelete the table or view.\n\nEditor cannot do that.\n\nThoughts?"
      },
      {
        "date": "2023-01-19T19:08:00.000Z",
        "voteCount": 1,
        "content": "I apologize - I thought B said Owner...\nThis questions makes no sense now..."
      },
      {
        "date": "2022-12-07T12:19:00.000Z",
        "voteCount": 2,
        "content": "It's D, because this is an outdated question, before IAM you cannot set Editor to a dataset; but the best practice is:  Create a dataset for each department. Assign the department leads the role of EDITOR(NOT OWNER), and assign the data analysts the role of READER on their dataset."
      },
      {
        "date": "2022-12-16T05:02:00.000Z",
        "voteCount": 7,
        "content": "Dude, I know there are updates to IAM, but the key point of the question is to have the leads have table creation and update roles... So they already need roles at the dataset level and hence C and D is out. We wouldn't be able to memorise all the roles, but clearly we cannot provide access on a table level..."
      },
      {
        "date": "2022-12-28T00:06:00.000Z",
        "voteCount": 1,
        "content": "and to supplement why does it need viewer role on the project the table is in?"
      },
      {
        "date": "2022-11-11T23:38:00.000Z",
        "voteCount": 4,
        "content": "Wow B is an answer \nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles#dataset-basic-roles"
      },
      {
        "date": "2022-10-31T07:31:00.000Z",
        "voteCount": 2,
        "content": "It CANNOT BE B BECAUSE OF : \n\n\n\nCaution: BigQuery's dataset-level basic roles existed prior to the introduction of IAM. We recommend that you minimize the use of basic roles. In production environments, don't grant basic roles unless there is no alternative. Instead, use predefined IAM roles.\n\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles"
      },
      {
        "date": "2023-01-19T19:00:00.000Z",
        "voteCount": 1,
        "content": "Ummm owner is a predefined role\nhttps://cloud.google.com/bigquery/docs/access-control\nBigQuery Data Owner \n(roles/bigquery.dataOwner)"
      },
      {
        "date": "2022-10-13T02:13:00.000Z",
        "voteCount": 3,
        "content": "I vote B because C and D says that the role is on the project that the table is in, this mean that the role is at the project level that implies that: \nIf you create a dataset in a project that contains any editors, BigQuery grants those users the bigquery.dataEditor predefined role for the new dataset. (from https://cloud.google.com/bigquery/docs/access-control-basic-roles#project-basic-roles)\n\nA can't not be because the analysts, in this case, can access the data.\n\nB grant to the leads update their datasets, that's mean create tables, and the analysts only read their datasets."
      },
      {
        "date": "2022-10-11T01:17:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/access-control-basic-roles\n\nCaution: BigQuery's dataset-level basic roles existed prior to the introduction of IAM. We recommend that you minimize the use of basic roles. In production environments, don't grant basic roles unless there is no alternative. Instead, use predefined IAM roles."
      },
      {
        "date": "2022-09-05T01:27:00.000Z",
        "voteCount": 5,
        "content": "Vote D, there is only Viewer, Editor and Owner roles for BQ\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles"
      },
      {
        "date": "2022-09-07T22:35:00.000Z",
        "voteCount": 6,
        "content": "Sorry but you are wrong. There is WRITER and READER role for dataset see them in this documentation. I was also confused at the beginning:\nhttps://cloud.google.com/bigquery/docs/access-control-basic-roles"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 119,
    "url": "https://www.examtopics.com/discussions/google/view/17244-exam-professional-data-engineer-topic-1-question-119/",
    "body": "You operate a database that stores stock trades and an application that retrieves average stock price for a given company over an adjustable window of time. The data is stored in Cloud Bigtable where the datetime of the stock trade is the beginning of the row key. Your application has thousands of concurrent users, and you notice that performance is starting to degrade as more stocks are added. What should you do to improve the performance of your application?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the row key syntax in your Cloud Bigtable table to begin with the stock symbol.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the row key syntax in your Cloud Bigtable table to begin with a random number per second.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the data pipeline to use BigQuery for storing stock trades, and update your application.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Dataflow to write a summary of each day's stock trades to an Avro file on Cloud Storage. Update your application to read from Cloud Storage and Cloud Bigtable to compute the responses."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T00:12:00.000Z",
        "voteCount": 41,
        "content": "Answer: A\nDescription: Timestamp at starting of rowkey causes bottleneck issues"
      },
      {
        "date": "2020-04-11T23:13:00.000Z",
        "voteCount": 13,
        "content": "Stock symbol will be similar for most of the records, so it's better to start with random number.. Answer should be B"
      },
      {
        "date": "2021-11-11T05:24:00.000Z",
        "voteCount": 13,
        "content": "You never use something called random number in bigtable rowkey because it gives you no use in querying possibilities, since we can't run sql querys in bigtable we should not randomise rowkeys in bigtable. \nDon't confuse the above point with the hotspot logic, both are different if you think so.\n\nAnd another thing is, what you said can be good choice if we are using cloud spanner and trying to comeup with primary key situation, since there we can always run sql query.\n\nI think you got the point now."
      },
      {
        "date": "2020-04-29T00:01:00.000Z",
        "voteCount": 3,
        "content": "I agree with u"
      },
      {
        "date": "2021-02-20T00:20:00.000Z",
        "voteCount": 6,
        "content": "it can start with stock symbol concated with timestamp can be a good row key design"
      },
      {
        "date": "2021-12-26T21:01:00.000Z",
        "voteCount": 3,
        "content": "for a given company, the data poits starts with the same stock symbol. The dataset is not distrubuted. It is not a good option."
      },
      {
        "date": "2024-08-23T04:13:00.000Z",
        "voteCount": 1,
        "content": "B is correct, By introducing a random number or a hash at the beginning of the row key, you distribute the writes and reads more evenly across the Bigtable cluster, thereby improving performance under heavy load.\n\nWHY NOT A?\nThis might still cause hotspots if certain stocks are more popular than others. It could lead to uneven load distribution, which wouldn't solve the performance degradation problem."
      },
      {
        "date": "2024-01-07T10:15:00.000Z",
        "voteCount": 2,
        "content": "Answer is A."
      },
      {
        "date": "2023-02-24T10:27:00.000Z",
        "voteCount": 12,
        "content": "Answer A:\nTrick to remember: Row-key adjustment always be like in decending order. \n#&lt;&lt;Least value&gt;&gt;#&lt;&lt;Lesser value&gt;&gt;\nFor example: \n1. #&lt;&lt;Earth&gt;&gt;#&lt;&lt;continents&gt;&gt;#&lt;&lt;countries&gt;&gt;#&lt;&lt;cities&gt;&gt; and so on.. \n2. #&lt;&lt;Stock&gt;&gt;#&lt;&lt;users&gt;&gt;#timestamp.. \nin 99% cases timestamp will be in the end, as its smallest division..."
      },
      {
        "date": "2023-08-16T10:11:00.000Z",
        "voteCount": 1,
        "content": "Awesome!"
      },
      {
        "date": "2022-12-05T03:17:00.000Z",
        "voteCount": 6,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys\nIt's important to create a row key that makes it possible to retrieve a well-defined range of rows. Otherwise, your query requires a table scan, which is much slower than retrieving specific rows.\n\nhttps://cloud.google.com/bigtable/docs/schema-design#row-keys-avoid\nSome types of row keys can make it difficult to query your data, and some result in poor performance. This section describes some types of row keys that you should avoid using in Bigtable.\n- Row keys that start with a timestamp. This pattern causes sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, precede it with a high-cardinality value like a user ID to avoid hotspots."
      },
      {
        "date": "2022-12-30T19:47:00.000Z",
        "voteCount": 1,
        "content": "I agree with you . A is right"
      },
      {
        "date": "2022-01-05T10:50:00.000Z",
        "voteCount": 4,
        "content": "A: https://cloud.google.com/bigtable/docs/schema-design-time-series#prefer_rows_to_column_versions"
      },
      {
        "date": "2021-11-25T22:59:00.000Z",
        "voteCount": 1,
        "content": "Correct: A"
      },
      {
        "date": "2021-11-09T18:24:00.000Z",
        "voteCount": 1,
        "content": "A and B would both work, since both would distribute the work. This question is not framed properly."
      },
      {
        "date": "2021-07-02T14:34:00.000Z",
        "voteCount": 3,
        "content": "Vote for A"
      },
      {
        "date": "2021-03-03T08:44:00.000Z",
        "voteCount": 5,
        "content": "Option A.\nBelow document explains \nHaving EXCHANGE and SYMBOL in the leading positions in the row key will naturally distribute activity.\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series"
      },
      {
        "date": "2020-11-19T08:17:00.000Z",
        "voteCount": 2,
        "content": "I think A"
      },
      {
        "date": "2020-11-18T03:45:00.000Z",
        "voteCount": 1,
        "content": "Catch here is current Rowley starts with timestamp which should not be in the starting or end position so symbolmshould be prefixed before timestamp"
      },
      {
        "date": "2020-11-09T08:26:00.000Z",
        "voteCount": 6,
        "content": "A is correct..A Good ROW KEY has to be an ID followed by timestamp. Stock symbol in this case works as an ID"
      },
      {
        "date": "2020-09-29T06:57:00.000Z",
        "voteCount": 2,
        "content": "A.\nYou can find an example in Google's introductory guide.\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series?hl=ja#financial_market_data"
      },
      {
        "date": "2020-09-17T23:02:00.000Z",
        "voteCount": 3,
        "content": "I think A would be best practice. Adding random numbers as start of rowkey doesn't help with troubleshooting"
      },
      {
        "date": "2020-09-13T22:12:00.000Z",
        "voteCount": 1,
        "content": "B should be the answer as adding random numbers in the beginning of the rowkey will distributes data across multiple nodes"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 120,
    "url": "https://www.examtopics.com/discussions/google/view/17245-exam-professional-data-engineer-topic-1-question-120/",
    "body": "You are operating a Cloud Dataflow streaming pipeline. The pipeline aggregates events from a Cloud Pub/Sub subscription source, within a window, and sinks the resulting aggregation to a Cloud Storage bucket. The source has consistent throughput. You want to monitor an alert on behavior of the pipeline with Cloud<br>Stackdriver to ensure that it is processing data. Which Stackdriver alerts should you create?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn alert based on a decrease of subscription/num_undelivered_messages for the source and a rate of change increase of instance/storage/ used_bytes for the destination",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn alert based on a decrease of instance/storage/used_bytes for the source and a rate of change increase of subscription/ num_undelivered_messages for the destination",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn alert based on an increase of instance/storage/used_bytes for the source and a rate of change decrease of subscription/ num_undelivered_messages for the destination"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-26T06:58:00.000Z",
        "voteCount": 28,
        "content": "You would want to get alerted only if Pipeline fails &amp; not if it is running fine. I think Option [B] is correct, because in event of Pipeline failure :\n1) subscription/ num_undelivered_messages would pile up at a constant rate as the source has consistent throughput\n2) instance/storage/ used_bytes will get closer to zero. Hence need to monitor it's rate of change"
      },
      {
        "date": "2020-07-02T00:46:00.000Z",
        "voteCount": 5,
        "content": "Yes, you are right, it should be B:\nThank you"
      },
      {
        "date": "2021-12-13T01:35:00.000Z",
        "voteCount": 1,
        "content": "Why would the instance/storage/used_bytes get closer to zero? If there's an error at a certain point, wouldn't we just see that the used_bytes remain constant while the num_undelivered_messages increases? I don't get why the destination's used bytes should decrease."
      },
      {
        "date": "2021-12-19T08:58:00.000Z",
        "voteCount": 1,
        "content": "\"If there's an error at a certain point, wouldn't we just see that the used_bytes remain constant while the num_undelivered_messages increases?\"\nIt's the rate of change, not the absolute value"
      },
      {
        "date": "2021-12-18T05:06:00.000Z",
        "voteCount": 6,
        "content": "\"rate of change decrease of instance/storage/ used_bytes\" - if rate of instance/storage/ used_bytes decreases that means less data is written - so something is wrong with the pipeline.\nIt's not used bytes that decreases - it's rate of change decreases. \nExample: if everything works fine your pipeline writes 5MB/s to the sink. If it decreases to 0.1MB/s it means something is wrong"
      },
      {
        "date": "2020-03-22T04:46:00.000Z",
        "voteCount": 21,
        "content": "Correct - B"
      },
      {
        "date": "2023-03-12T19:04:00.000Z",
        "voteCount": 15,
        "content": "For those who may get confuse at the start by the term 'subscription/num_undelivered_messages', it is not a division. It is the full path of the metric. So we should just read it as 'num_undelivered_messages'. The same for 'used_bytes'.\n\nSo if we see the source have more backlog (more num_undelivered_messages), or the destination ultilization going down, that is the indicator of something going wrong"
      },
      {
        "date": "2023-06-09T07:14:00.000Z",
        "voteCount": 2,
        "content": "great explanation thanks !"
      },
      {
        "date": "2023-02-24T10:33:00.000Z",
        "voteCount": 3,
        "content": "Answer B:\nTrick: In stackdriver always put Alert for Subscriber  + CPU\nSubscriber - num of undelivered message INCREASE alert\nCPU  - Instance or storage DECREASE alert. \nMake sense right !"
      },
      {
        "date": "2023-01-09T03:28:00.000Z",
        "voteCount": 1,
        "content": "Nobody seems to pay attention to instance/storage/used_bytes. I only find this metric for Spanner. \nhttps://cloud.google.com/monitoring/api/metrics_gcp#gcp-spanner\n\nWhile Dataflow processes and stores everything in Cloud Storage, Spanner could only be the source.\nhttps://cloud.google.com/spanner/docs/change-streams\n\nAlso, if it is either A or B, the instance/storage/used_bytes metric does not make sense for the destination, which is Cloud Storage.\n\nCan anyone help me understand?"
      },
      {
        "date": "2023-01-19T19:23:00.000Z",
        "voteCount": 2,
        "content": "look here: https://cloud.google.com/monitoring/api/metrics_gcp\n\ninstance/storage/used_bytes GA\nStorage used."
      },
      {
        "date": "2023-01-02T14:24:00.000Z",
        "voteCount": 1,
        "content": "B. An alert based on an increase of subscription/num_undelivered_messages for the source and a rate of change decrease of instance/storage/ used_bytes for the destination"
      },
      {
        "date": "2022-12-30T19:50:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2022-12-29T02:00:00.000Z",
        "voteCount": 1,
        "content": "An alert based on a decrease of subscription/num_undelivered_messages for the source and a rate of change increase of instance/storage/ used_bytes for the destination\n\n10 subscriptions / 1 undelivered messages = 10\n10 subscriptions / 5 undelivered messages = 2\nYou clearly want to be alerted when the number of undelivered messages increases. The ratio then decreases. In my example from 10 to 2."
      },
      {
        "date": "2023-03-19T13:47:00.000Z",
        "voteCount": 1,
        "content": "subscription/num_undelivered_messages  is a path, not a division."
      },
      {
        "date": "2022-12-05T03:08:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/pubsub/docs/monitoring#monitoring_the_backlog\nMonitor message backlog\nTo ensure that your subscribers are keeping up with the flow of messages, create a dashboard. The dashboard can show the following backlog metrics, aggregated by resource, for all your subscriptions:\n- Unacknowledged messages (subscription/num_undelivered_messages) to see the number of unacknowledged messages."
      },
      {
        "date": "2022-08-19T11:45:00.000Z",
        "voteCount": 1,
        "content": "Increase subscription/num delivered message\ndecrease instance/storage/used bytes"
      },
      {
        "date": "2022-07-06T02:27:00.000Z",
        "voteCount": 1,
        "content": "Correct - B"
      },
      {
        "date": "2021-11-25T23:01:00.000Z",
        "voteCount": 2,
        "content": "Correct: B"
      },
      {
        "date": "2021-11-11T05:28:00.000Z",
        "voteCount": 2,
        "content": "isn't B and C are same."
      },
      {
        "date": "2021-11-09T18:27:00.000Z",
        "voteCount": 3,
        "content": "B.\nIt's useful to monitor the source that keeps sending data while the destination that doesn't take anything in."
      },
      {
        "date": "2021-10-07T06:29:00.000Z",
        "voteCount": 2,
        "content": "The answer is B. \nsubscription/num_undelivered_messages: the number of messages that subscribers haven't processed https://cloud.google.com/pubsub/docs/monitoring#monitoring_forwarded_undeliverable_messages"
      },
      {
        "date": "2021-09-12T08:07:00.000Z",
        "voteCount": 2,
        "content": "Silly question: what is subscription/ num_undelivered_messages, it is divided by? or per subscription per num_undelivered_messages?"
      },
      {
        "date": "2022-04-02T10:17:00.000Z",
        "voteCount": 3,
        "content": "yes is misleading:\nthe metric \"subscription/num_undelivered_messages\" is just the path of the API URL\n\nactions.googleapis.com/...subscription/num_undelivered_messages\n\nref: https://cloud.google.com/monitoring/api/metrics_gcp#pubsub/subscription/num_undelivered_messages"
      },
      {
        "date": "2021-07-02T14:38:00.000Z",
        "voteCount": 3,
        "content": "Looks B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 121,
    "url": "https://www.examtopics.com/discussions/google/view/17240-exam-professional-data-engineer-topic-1-question-121/",
    "body": "You currently have a single on-premises Kafka cluster in a data center in the us-east region that is responsible for ingesting messages from IoT devices globally.<br>Because large parts of globe have poor internet connectivity, messages sometimes batch at the edge, come in all at once, and cause a spike in load on your<br>Kafka cluster. This is becoming difficult to manage and prohibitively expensive. What is the Google-recommended cloud native architecture for this scenario?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEdge TPUs as sensor devices for storing and transmitting the messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataflow connected to the Kafka cluster to scale the processing of incoming messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAn IoT gateway connected to Cloud Pub/Sub, with Cloud Dataflow to read and process the messages from Cloud Pub/Sub.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA Kafka cluster virtualized on Compute Engine in us-east with Cloud Load Balancing to connect to the devices around the world."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T03:22:00.000Z",
        "voteCount": 21,
        "content": "Should be C"
      },
      {
        "date": "2020-03-28T03:16:00.000Z",
        "voteCount": 19,
        "content": "Answer: C\nDescription: Pubsub is global and dataflow can scale workers"
      },
      {
        "date": "2023-05-29T06:56:00.000Z",
        "voteCount": 1,
        "content": "Can anyone pls explain what's wrong with D, the load balancing solution?"
      },
      {
        "date": "2023-02-24T10:48:00.000Z",
        "voteCount": 2,
        "content": "Answer C:\nWhat is wrong with D, nothing, Cloud load balancing can shift traffic for high volume and low internet in one region. It cost avg. 0.01-0.25 $ per GB, or if volume is too high. 0.05 $ per Hour http request. This might be the answer if your exam for network engineer."
      },
      {
        "date": "2023-02-17T02:45:00.000Z",
        "voteCount": 1,
        "content": "Answer C, but it will not solve bad internet connection, make sure 100mbps speed of internet is at sensor side."
      },
      {
        "date": "2022-12-05T03:05:00.000Z",
        "voteCount": 10,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/iot-overview#cloud-pubsub\nPub/Sub can act like a shock absorber and rate leveller for both incoming data streams and application architecture changes. Many devices have limited ability to store and retry sending telemetry data. Pub/Sub scales to handle data spikes that can occur when swarms of devices respond to events in the physical world, and buffers these spikes to help isolate them from applications monitoring the data."
      },
      {
        "date": "2022-12-30T19:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with your explanation"
      },
      {
        "date": "2022-10-31T11:29:00.000Z",
        "voteCount": 1,
        "content": "\"single on-premises Kafka cluster in a data center in the us-east region\"\nis it on-prem or in a datacenter in us-east ?"
      },
      {
        "date": "2022-07-19T11:16:00.000Z",
        "voteCount": 1,
        "content": "Answer is C"
      },
      {
        "date": "2022-02-21T09:43:00.000Z",
        "voteCount": 1,
        "content": "Answer is option C"
      },
      {
        "date": "2021-07-14T23:40:00.000Z",
        "voteCount": 4,
        "content": "Answer c\nkafka cluster in on-premise for streaming msgs\npub/sub for streaming msgs in cloud"
      },
      {
        "date": "2021-07-03T02:59:00.000Z",
        "voteCount": 4,
        "content": "Vote for C"
      },
      {
        "date": "2021-02-24T17:39:00.000Z",
        "voteCount": 4,
        "content": "Should be C"
      },
      {
        "date": "2021-02-18T06:57:00.000Z",
        "voteCount": 5,
        "content": "C is correct:\nthe main trick come from A, and response is that TPU only use when we have a deployed machine learning model that we don't have now."
      },
      {
        "date": "2021-02-15T16:32:00.000Z",
        "voteCount": 4,
        "content": "Answer - C"
      },
      {
        "date": "2020-11-10T04:35:00.000Z",
        "voteCount": 5,
        "content": "Easy Question : ANswer is Option C. \nAlterative to Kafka in google cloud native service is Pub/Sub and Dataflow punched with Pub/Sub is the google recommended option"
      },
      {
        "date": "2020-08-22T18:10:00.000Z",
        "voteCount": 4,
        "content": "C\nthe issue is with a single Kafka cluster is the need to scale automatically with Dataflow"
      },
      {
        "date": "2020-08-21T12:11:00.000Z",
        "voteCount": 4,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 122,
    "url": "https://www.examtopics.com/discussions/google/view/16858-exam-professional-data-engineer-topic-1-question-122/",
    "body": "You decided to use Cloud Datastore to ingest vehicle telemetry data in real time. You want to build a storage system that will account for the long-term data growth, while keeping the costs low. You also want to create snapshots of the data periodically, so that you can make a point-in-time (PIT) recovery, or clone a copy of the data for Cloud Datastore in a different environment. You want to archive these snapshots for a long time. Which two methods can accomplish this?<br>(Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse managed export, and then import the data into a BigQuery table created just for that export, and delete temporary export files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an application that uses Cloud Datastore client libraries to read all the entities. Treat each entity as a BigQuery table row via BigQuery streaming insert. Assign an export timestamp for each export, and attach it as an extra column for each row. Make sure that the BigQuery table is partitioned using the export timestamp column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an application that uses Cloud Datastore client libraries to read all the entities. Format the exported data into a JSON file. Apply compression before storing the data in Cloud Source Repositories."
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-13T13:50:00.000Z",
        "voteCount": 38,
        "content": "A,B\nhttps://cloud.google.com/datastore/docs/export-import-entities"
      },
      {
        "date": "2021-04-29T22:53:00.000Z",
        "voteCount": 6,
        "content": "\"while keeping the costs\"\n\nshould be A,D"
      },
      {
        "date": "2021-08-07T09:00:00.000Z",
        "voteCount": 9,
        "content": "Big query streaming inserts ARE NOT cheap"
      },
      {
        "date": "2021-09-30T20:05:00.000Z",
        "voteCount": 4,
        "content": "If you use B , not D , how can we do \"point in time\" recovery? is it possible?\nPoint in time recovery needs export along with timestamp, so that we can recover for a particular timestamp."
      },
      {
        "date": "2020-08-26T16:41:00.000Z",
        "voteCount": 23,
        "content": "AC\nhttps://cloud.google.com/datastore/docs/export-import-entities\nC: To import only a subset of entities or to import data into BigQuery, you must specify an entity filter in your export.\nB: Not correct since you want to store in a different environment than Datastore. Tho this statment is true:  Data exported from one Datastore mode database can be imported into another Datastore mode database, even one in another project.\nA is correct\nBilling and pricing for managed exports and imports in Datastore\nOutput files stored in Cloud Storage count towards your Cloud Storage data storage costs.\nSteps to Export all the entities\n1. Go to the Datastore Entities Export page in the Google Cloud Console.\n2. Go to the Datastore Export page\n2. Set the Namespace field to All Namespaces, and set the Kind field to All Kinds.\n3. Below Destination, enter the name of your \"Cloud Storage bucket\".\n4. Click Export."
      },
      {
        "date": "2021-08-04T11:30:00.000Z",
        "voteCount": 1,
        "content": "C is valid because of table snapshots. Else standard time travel is valid only for 7 days\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots\nhttps://cloud.google.com/bigquery/docs/time-travel#limitation"
      },
      {
        "date": "2021-09-30T11:30:00.000Z",
        "voteCount": 1,
        "content": "you wanna say invalid?"
      },
      {
        "date": "2022-03-27T01:47:00.000Z",
        "voteCount": 2,
        "content": "As you've mentioned in B, does the environment meant to be a project or a resource? As, we can clone a copy of the data in a datastore even in another project!? Then, it's B. \n\nAlso, in point C they didn't mention any entity filter hence we eliminate C how can you support your own statement with a different answer?"
      },
      {
        "date": "2022-12-30T19:55:00.000Z",
        "voteCount": 1,
        "content": "A, B is perfect"
      },
      {
        "date": "2021-11-09T04:03:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\nData exported without specifying an entity filter cannot be loaded into BigQuery. This is not mentioned explicitly. Safe to assume there is no filter on the exports. So options are AB"
      },
      {
        "date": "2024-04-04T14:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/datastore/docs/export-import-entities"
      },
      {
        "date": "2023-09-02T09:29:00.000Z",
        "voteCount": 5,
        "content": "AB chatgpt\nA. Use managed export, and store the data in a Cloud Storage bucket using Nearline or Coldline class:\n\nManaged export is a feature provided by Cloud Datastore to export your data.\nStoring the data in a Cloud Storage bucket, especially using Nearline or Coldline storage classes, helps keep storage costs low while allowing you to retain the snapshots for a long time.\nB. Use managed export, and then import to Cloud Datastore in a separate project under a unique namespace reserved for that export:\n\nThis method allows you to create snapshots by exporting data from Cloud Datastore (using managed export) and then importing it into a separate project under a unique namespace.\nBy importing into a separate project, you can keep a copy of the data in a different environment, which is useful for point-in-time recovery or creating clones of the data."
      },
      {
        "date": "2022-12-03T04:49:00.000Z",
        "voteCount": 3,
        "content": "AB is the answer."
      },
      {
        "date": "2022-12-01T16:09:00.000Z",
        "voteCount": 11,
        "content": "A rather complicated question, of a kind I wish I won't face in the exam. My opinion:\n\u2705 [A] A valid and cost-effective solution satisfying the requirement for PIT recovery\n\u2705 [B] A valid solution but far from ideal for archiving. It satisfies the requirement part \"you can \u2026 clone a copy of the data for Cloud Datastore in a different environment\" (an objection to the word \"namespace\", I think it should be just \"name\")"
      },
      {
        "date": "2022-12-01T16:09:00.000Z",
        "voteCount": 11,
        "content": "\u274c[C] There is the limitation \"Data exported without specifying an entity filter cannot be loaded into BigQuery\". The entity filter for this case should contain all the kinds of entities but there is another limitation of \"100 entity filter combinations\". We have no knowledge of the kinds or the namespaces of the entities.\nSources:\n\ud83d\udd17 https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\n\ud83d\udd17 https://cloud.google.com/datastore/docs/export-import-entities#exporting_specific_kinds_or_namespaces\n\u274c [D] seems a detailed candidate solution but it violates the limitation \"You cannot append Datastore export data to an existing table.\"\n\ud83d\udd17 https://cloud.google.com/bigquery/docs/loading-data-cloud-datastore#appending_to_or_overwriting_a_table_with_cloud_datastore_data\n\u274c [E] Cloud Source Repositories are for source code and not a suitable storage for this case."
      },
      {
        "date": "2022-09-30T08:50:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/datastore/docs/export-import-entities"
      },
      {
        "date": "2022-09-13T07:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/datastore/docs/export-import-entities"
      },
      {
        "date": "2022-09-13T02:10:00.000Z",
        "voteCount": 1,
        "content": "The answer is nothing to do with bigquery , so you can skip what  mention to bigquery.\n\nA B is the final answer"
      },
      {
        "date": "2022-07-20T06:27:00.000Z",
        "voteCount": 2,
        "content": "A,B\n\nFor those who say using BQ as archival, How can we achieve that while datastore are NO-SQL whereas BQ are SQL , will that work? also BQ are not created for achieving purposes."
      },
      {
        "date": "2022-06-18T11:32:00.000Z",
        "voteCount": 1,
        "content": "Option B is 36 times more expensive than C"
      },
      {
        "date": "2022-01-16T04:21:00.000Z",
        "voteCount": 2,
        "content": "AB. for sure streaming to BQ its quite expensive!"
      },
      {
        "date": "2022-01-09T02:56:00.000Z",
        "voteCount": 3,
        "content": "A - Cloud Storage (long-term data + costs low)\nD - BigQuery (timestamp for point-in-time (PIT) recovery)"
      },
      {
        "date": "2022-04-22T02:53:00.000Z",
        "voteCount": 2,
        "content": "D is wrong, BQ Streaming inserts costs are high!"
      },
      {
        "date": "2023-12-17T07:55:00.000Z",
        "voteCount": 1,
        "content": "Agreed, AB\nhttps://cloud.google.com/datastore/docs/export-import-entities"
      },
      {
        "date": "2022-01-08T04:50:00.000Z",
        "voteCount": 2,
        "content": "Option A; Cheap storage and it is a supported method https://cloud.google.com/datastore/docs/export-import-entities\nOption B; Rationale - \"Data exported from one Datastore mode database can be imported into another Datastore mode database, even one in another project.\" &lt;https://cloud.google.com/datastore/docs/export-import-entities&gt;"
      },
      {
        "date": "2021-10-17T09:01:00.000Z",
        "voteCount": 1,
        "content": "Answer is A, B. \nhttps://cloud.google.com/datastore/docs/export-import-entities#exporting_specific_kinds_or_namespaces"
      },
      {
        "date": "2021-10-07T23:27:00.000Z",
        "voteCount": 4,
        "content": "A, D\nA: Option for storage system that will account for the long-term data growth\nD: Option for snapshots, PIT recovery, copy of the data for Cloud Datastore in a different environment and, above all, archive snapshots for a long time \nB: not a good solution for archiving snapshots for a long time\nC: to import data into BigQuery, you must specify an entity filter \nE: Cloud Source Repositories is for code\nOne note: E --&gt; would be my second choice if there was Cloud Storage instead of Source Repositories (typo?)"
      },
      {
        "date": "2021-09-17T10:56:00.000Z",
        "voteCount": 1,
        "content": "Vote A B .  What\u2019s the purpose load into bigquery?"
      },
      {
        "date": "2021-09-30T11:26:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/datastore/docs/export-import-entities#import-into-bigquery\nImporting into BigQuery\nTo import data from a managed export into BigQuery, see Loading Datastore export service data.\n\nData exported without specifying an entity filter cannot be loaded into BigQuery. If you want to import data into BigQuery, your export request must include one or more kind names in the entity filter.\n\nYou have to specify an entity fliter before you can load from datastore to BQ. It didn't mention that at all. So C is incorrect"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 123,
    "url": "https://www.examtopics.com/discussions/google/view/17023-exam-professional-data-engineer-topic-1-question-123/",
    "body": "You need to create a data pipeline that copies time-series transaction data so that it can be queried from within BigQuery by your data science team for analysis.<br>Every hour, thousands of transactions are updated with a new status. The size of the initial dataset is 1.5 PB, and it will grow by 3 TB per day. The data is heavily structured, and your data science team will build machine learning models based on this data. You want to maximize performance and usability for your data science team. Which two strategies should you adopt? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDenormalize the data as must as possible.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreserve the structure of the data as much as possible.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery UPDATE to further reduce the size of the dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop a data pipeline where status updates are appended to BigQuery instead of updated.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy a daily snapshot of transaction data to Cloud Storage and store it as an Avro file. Use BigQuery's support for external data sources to query."
    ],
    "answer": "AD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AD",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "BD",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "AC",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-19T20:21:00.000Z",
        "voteCount": 39,
        "content": "I think AD is the answer. E will not improve performance."
      },
      {
        "date": "2020-03-28T03:35:00.000Z",
        "voteCount": 20,
        "content": "Answer: A, D\nDescription: Denormalization will help in performance by reducing query time, update are not good with bigquery"
      },
      {
        "date": "2021-07-08T18:41:00.000Z",
        "voteCount": 3,
        "content": "My guess is append has better performance than update."
      },
      {
        "date": "2023-03-22T21:26:00.000Z",
        "voteCount": 3,
        "content": "If we denormalize the data, the Data Science team will shout at us. Preserving it is the way to go"
      },
      {
        "date": "2023-06-07T08:21:00.000Z",
        "voteCount": 1,
        "content": "Denormalization is just a best practice when using BQ."
      },
      {
        "date": "2023-06-06T01:01:00.000Z",
        "voteCount": 5,
        "content": "Shouting data-science teams are not part of question, this is more about what is exam correct, not what it the best for your own situation"
      },
      {
        "date": "2022-12-08T02:20:00.000Z",
        "voteCount": 6,
        "content": "A and D:\nA-  Improve performance\nD- Is better for DS have all the history and not the last update..."
      },
      {
        "date": "2022-12-03T04:46:00.000Z",
        "voteCount": 4,
        "content": "AD is the answer.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-nested\nBest practice: Use nested and repeated fields to denormalize data storage and increase query performance.\n\nDenormalization is a common strategy for increasing read performance for relational datasets that were previously normalized. The recommended way to denormalize data in BigQuery is to use nested and repeated fields. It's best to use this strategy when the relationships are hierarchical and frequently queried together, such as in parent-child relationships."
      },
      {
        "date": "2022-12-30T19:58:00.000Z",
        "voteCount": 1,
        "content": "A, C is correct I agree"
      },
      {
        "date": "2022-11-04T00:57:00.000Z",
        "voteCount": 2,
        "content": "The criteria for selecting a strategy are the performance and usability for the data science team. This team performs the analysis by querying stored data. So we don't care for performance related with data ingestion. According to this point of view:\nA: YES - undisputedly favours query performance\nB: YES - Keeping the structure unchanged promotes usability (the team won't need to update queries or ML models)\nC: Questionable - Updating the status of a row instead of appending newer versions is keeping the size smaller. But does this affect significantly the analysis performance? Even if it does, creating materialized views to keep the most recent status per row eliminates it\nD: NO - has nothing to do with DS team's tasks, affects ingestion performance\nE: NO - demotes usability"
      },
      {
        "date": "2022-12-16T04:57:00.000Z",
        "voteCount": 1,
        "content": "For B there is no mention that the current data structure is being used (...data science team WILL build machine learning models based on this data.) ... We're developing a new data model to be used by them in the future"
      },
      {
        "date": "2022-11-04T01:01:00.000Z",
        "voteCount": 1,
        "content": "(mistakenly voted AC instead of AB)"
      },
      {
        "date": "2022-09-21T00:16:00.000Z",
        "voteCount": 1,
        "content": "The DML quota limit is removed since 2020, I think C is better than D now."
      },
      {
        "date": "2022-10-16T09:28:00.000Z",
        "voteCount": 5,
        "content": "Is not about the quota. You should avoid using UPDATE because it makes a big scan of the table, and is not efficient or high performant. Usually prefer appends and merges instead, and using the optimized schema approach of Big Query that denormalizes the table to avoid joins and leverages nested and repeated fields."
      },
      {
        "date": "2022-01-09T03:00:00.000Z",
        "voteCount": 4,
        "content": "A: Denormalization increases query speed for tables with billions of rows because BigQuery's performance degrades when doing JOINs on large tables, but with a denormalized data structure, you don't have to use JOINs, since all of the data has been combined into one table. \nDenormalization also makes queries simpler because you do not have to use JOIN clauses. \nhttps://cloud.google.com/solutions/bigquery-data-warehouse#denormalizing_data\nD: BigQuery append"
      },
      {
        "date": "2022-01-08T04:59:00.000Z",
        "voteCount": 3,
        "content": "requirements are -&gt;  performance and usability.\n\nDenormalization will help in performance by reducing query time, update is not good with big query.\n\nAnd append has better performance than Update."
      },
      {
        "date": "2021-11-13T21:26:00.000Z",
        "voteCount": 1,
        "content": "I think AD. E is not valid because it use external table which is not good for performance"
      },
      {
        "date": "2021-07-03T03:56:00.000Z",
        "voteCount": 5,
        "content": "A - correct (denormlization will help)\nB - data already heavily structured (no use and no impact)\nC - more than 1500 Updates not possible \nD - Not sure..(because appending will increase size and cost)\nE - Does not look good (increase cost..also we are storing for all days....again for query we need to issue mutiple query for all days....)\n\nSo, A &amp; D (left out of 5)"
      },
      {
        "date": "2021-06-22T16:44:00.000Z",
        "voteCount": 3,
        "content": "Correct Answer: AE\nA \u2013 Denormalisation helps improve performance.\nB, C  - Not helping to address the problem. \nD \u2013 Append will increase the db size and cost involved for storage and also for large number of records to scan for queries by data science team which is costlier.\nE - Addresses the problem of maximising the usability of the data science team and the data.  They can anayse the data exported to cloud storage instead of reading from bigquery which is expensive and impact performance considerably."
      },
      {
        "date": "2021-10-16T19:38:00.000Z",
        "voteCount": 1,
        "content": "It didn't mention cost is a concern"
      },
      {
        "date": "2021-09-29T08:34:00.000Z",
        "voteCount": 2,
        "content": "E is wrong, you've been asked to use bigquery and reading files from storage in bq is significantly more time consuming"
      },
      {
        "date": "2021-03-12T08:29:00.000Z",
        "voteCount": 4,
        "content": "A, D:\nUsing BigQuery as an OLTP store is considered an anti-pattern. Because OLTP stores have a high volume of updates and deletes, they are a mismatch for the data warehouse use case. To decide which storage option best fits your use case, review the Cloud storage products table.\nBigQuery is built for scale and can scale out as the size of the warehouse grows, so there is no need to delete older data. By keeping the entire history, you can deliver more insight on your business. If the storage cost is a concern, you can take advantage of BigQuery's long term storage pricing by archiving older data and using it for special analysis when the need arises. If you still have good reasons for dropping older data, you can use BigQuery's native support for date-partitioned tables and partition expiration. In other words, BigQuery can automatically delete older data.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#handling_change"
      },
      {
        "date": "2021-03-06T23:00:00.000Z",
        "voteCount": 2,
        "content": "should be AC.. \"Every hour, thousands of transactions are updated with a new status\" if we append how we will handle the new status change.."
      },
      {
        "date": "2021-07-03T03:51:00.000Z",
        "voteCount": 1,
        "content": "C not possible, maximum 1500 updates possible in a day"
      },
      {
        "date": "2021-07-24T09:34:00.000Z",
        "voteCount": 1,
        "content": "DML without limits now in BQ (below blog says March 2020, Not sure whether these questions were prepared before or after March 2020) \n\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery"
      },
      {
        "date": "2021-07-27T06:11:00.000Z",
        "voteCount": 4,
        "content": "There is no more hard limit, but UPDATES are queued:\n\"BigQuery runs up to 2 of them concurrently, after which up to 20 are queued as PENDING. When a previously running job finishes, the next pending job is dequeued and run. Currently, queued mutating DML statements share a per-table queue with maximum length 20. Additional statements past the maximum queue length for each table fail.\"\n\nWith thousands of updates per hour, this doesn't seem feasible. I would assume the question is marked as outdated anyway or the answers are update in the actual exam."
      },
      {
        "date": "2021-02-18T10:50:00.000Z",
        "voteCount": 1,
        "content": "AC:\nthe problem is exactly about Updating and preserving size of database as much as possible, then denormalization and using UPDATE function from DML will address the issue. they don't want to update faster. then A &amp; C is correct.\nhttps://cloud.google.com/solutions/bigquery-data-warehouse"
      },
      {
        "date": "2021-02-20T01:15:00.000Z",
        "voteCount": 3,
        "content": "you can update bigquery 1500 times in a day"
      },
      {
        "date": "2021-03-12T08:37:00.000Z",
        "voteCount": 3,
        "content": "A, D:\nit was my mistake, we should decrease update as Bigquery is not design for update.\n\nhttps://cloud.google.com/solutions/bigquery-data-warehouse#handling_change"
      },
      {
        "date": "2020-11-25T04:08:00.000Z",
        "voteCount": 5,
        "content": "A,D  Since the requirements are both performance and usability."
      },
      {
        "date": "2020-11-19T12:03:00.000Z",
        "voteCount": 3,
        "content": "i tink may be ita AC becuase appending its worst to increase dataset size. THe question seems to put like a problem the size of dataset and performance to datascience so inserting more rows decrease performace for them."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 124,
    "url": "https://www.examtopics.com/discussions/google/view/81264-exam-professional-data-engineer-topic-1-question-124/",
    "body": "You are designing a cloud-native historical data processing system to meet the following conditions:<br>\u2711 The data being analyzed is in CSV, Avro, and PDF formats and will be accessed by multiple analysis tools including Dataproc, BigQuery, and Compute<br>Engine.<br>\u2711 A batch pipeline moves daily data.<br>\u2711 Performance is not a factor in the solution.<br>\u2711 The solution design should maximize availability.<br>How should you design data storage for this solution?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataproc cluster with high availability. Store the data in HDFS, and perform analysis as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in BigQuery. Access the data using the BigQuery Connector on Dataproc and Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a regional Cloud Storage bucket. Access the bucket directly using Dataproc, BigQuery, and Compute Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the data in a multi-regional Cloud Storage bucket. Access the data directly using Dataproc, BigQuery, and Compute Engine.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-10-17T05:57:00.000Z",
        "voteCount": 7,
        "content": "Problem: How to store data?\nConsiderations: High availability, performance not an issue\n\nA \u2192 avoid HDFS\nC \u2192 multi-regional &gt; regional in terms of availability\n\nB could be the answer but we\u2019re dealing with PDF documents, we need blob storage (cloud storage). If we only have csv or Avro, this may be the answer"
      },
      {
        "date": "2024-09-21T10:13:00.000Z",
        "voteCount": 1,
        "content": "D is the answer"
      },
      {
        "date": "2022-12-30T20:00:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-12-29T09:36:00.000Z",
        "voteCount": 2,
        "content": "vote for D"
      },
      {
        "date": "2022-12-03T04:36:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      },
      {
        "date": "2022-10-16T09:24:00.000Z",
        "voteCount": 2,
        "content": "D of course"
      },
      {
        "date": "2022-09-08T12:07:00.000Z",
        "voteCount": 1,
        "content": "D is the correct answer"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 125,
    "url": "https://www.examtopics.com/discussions/google/view/17243-exam-professional-data-engineer-topic-1-question-125/",
    "body": "You have a petabyte of analytics data and need to design a storage and processing platform for it. You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore and process the entire dataset in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore and process the entire dataset in Bigtable.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the full dataset in BigQuery, and store a compressed copy of the data in a Cloud Storage bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore the warm data as files in Cloud Storage, and store the active data in BigQuery. Keep this ratio as 80% warm and 20% active."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-30T23:24:00.000Z",
        "voteCount": 34,
        "content": "Answer C."
      },
      {
        "date": "2020-06-21T23:27:00.000Z",
        "voteCount": 23,
        "content": "A and B can be eliminated right away as they do not talk about providing for other cloud providers. between C and D. The question says nothing about warm or cold data-rather that data should be made available for other providers--C--can fulfill this condition. Answer C."
      },
      {
        "date": "2022-12-30T20:03:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2023-09-17T10:22:00.000Z",
        "voteCount": 1,
        "content": "For me A. I can use export from BQ to Cloud Storage. There is no need to store two copies of data."
      },
      {
        "date": "2023-11-07T21:05:00.000Z",
        "voteCount": 4,
        "content": "If you export data from BQ to GCS then you will have two copies and you will be in the same architecture as answer C."
      },
      {
        "date": "2023-07-26T23:48:00.000Z",
        "voteCount": 2,
        "content": "It can be C or D , but I will go with C as storing the full dataset in BigQuery and a compressed copy of the data in Cloud Storage is a good way to balance performance and cost."
      },
      {
        "date": "2023-05-31T23:51:00.000Z",
        "voteCount": 2,
        "content": "Best answer is C, although BQ can query gzipped files stored on GCS directly.\nMaybe this double storage makes it a bit more highly available."
      },
      {
        "date": "2023-05-06T15:52:00.000Z",
        "voteCount": 1,
        "content": "D is much more accurate."
      },
      {
        "date": "2022-12-17T00:57:00.000Z",
        "voteCount": 1,
        "content": "D \u2192 does not guarantee 100% queryable or accessible/available"
      },
      {
        "date": "2022-12-03T04:35:00.000Z",
        "voteCount": 1,
        "content": "C is the answer."
      },
      {
        "date": "2022-07-12T00:25:00.000Z",
        "voteCount": 1,
        "content": "You can read streaming data from Pub/Sub, and you can write streaming data to Pub/Sub or BigQuery. \nThus Cloud Storage is not a proper sink for streaming pipeline.\nI vote for B, since it is possible to convert unstructured data and store in BQ"
      },
      {
        "date": "2022-07-12T00:30:00.000Z",
        "voteCount": 10,
        "content": "ignore this comment, please"
      },
      {
        "date": "2022-03-01T05:26:00.000Z",
        "voteCount": 1,
        "content": "BQ can reach files at google storage as external table. so my answer is D. (If data was smaller than this, I would choose C)"
      },
      {
        "date": "2022-01-17T09:38:00.000Z",
        "voteCount": 2,
        "content": "both requirements are full filled."
      },
      {
        "date": "2022-01-09T03:08:00.000Z",
        "voteCount": 1,
        "content": "D: BigQuery + Cloud Storage"
      },
      {
        "date": "2022-12-17T00:57:00.000Z",
        "voteCount": 2,
        "content": "D \u2192 does not guarantee 100% queryable or accessible/available"
      },
      {
        "date": "2022-01-08T05:08:00.000Z",
        "voteCount": 7,
        "content": "\"You must be able to perform data warehouse-style analytics on the data in Google Cloud and expose the dataset as files for batch analysis tools in other cloud providers?\"\nAnalytics -&gt; BQ\nExposing -&gt; GCS"
      },
      {
        "date": "2021-11-25T23:15:00.000Z",
        "voteCount": 2,
        "content": "Correct: C"
      },
      {
        "date": "2021-08-05T06:34:00.000Z",
        "voteCount": 3,
        "content": "vote for C"
      },
      {
        "date": "2021-07-03T04:19:00.000Z",
        "voteCount": 8,
        "content": "Vote for 'C'\n\nA - Only Half requirement fulfil,  expose as a file not getting fulfiled\nB -  Not a warehouse\nC. Both requirements fulfiled...Bigquery and GCS\nD. Both requirement fulfiled...but what if other cloud provider wants to analysis on rest 80% of the data. - \n\nSo out of 4 options, C looks okay"
      },
      {
        "date": "2021-03-01T12:19:00.000Z",
        "voteCount": 3,
        "content": "C\n\nBigQuery for analytics processing and Cloud Storage for exposing the data as files"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 126,
    "url": "https://www.examtopics.com/discussions/google/view/17235-exam-professional-data-engineer-topic-1-question-126/",
    "body": "You work for a manufacturing company that sources up to 750 different components, each from a different supplier. You've collected a labeled dataset that has on average 1000 examples for each unique component. Your team wants to implement an app to help warehouse workers recognize incoming components based on a photo of the component. You want to implement the first working version of this app (as Proof-Of-Concept) within a few working days. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Vision AutoML with the existing dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Vision AutoML, but reduce your dataset twice.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Vision API by providing custom labels as recognition hints.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain your own image recognition model leveraging transfer learning techniques."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-20T04:01:00.000Z",
        "voteCount": 54,
        "content": "B - You only need a PoC and it has be done quickly"
      },
      {
        "date": "2020-03-22T02:52:00.000Z",
        "voteCount": 20,
        "content": "Correct - A"
      },
      {
        "date": "2024-05-25T14:43:00.000Z",
        "voteCount": 2,
        "content": "AutoML Vision is deprecated since march 31, 2024. The question will refer to Vertex AI AutoML. And as bet practice, the minimum dataset size for each label is 1000. So, with an updated question, the answer would be A."
      },
      {
        "date": "2024-04-04T15:16:00.000Z",
        "voteCount": 1,
        "content": "A. Use Cloud Vision AutoML with the existing dataset.\n\nHere's why this is the most suitable option:\n\nSpeed and Ease: AutoML simplifies model building. You simply upload your labeled images, and AutoML takes care of model selection, training, and evaluation.\nExisting Dataset Sufficiency: Your dataset (750 components x 1000 images each) is a decent starting point for AutoML, allowing you to quickly test its effectiveness.\nMinimal Custom Development: AutoML's out-of-the-box deployment options let you integrate the model into your app without extensive coding."
      },
      {
        "date": "2023-09-09T04:18:00.000Z",
        "voteCount": 1,
        "content": "Option B is the fastest way to train a model that can be used to recognize the 750 different components."
      },
      {
        "date": "2023-02-17T05:57:00.000Z",
        "voteCount": 2,
        "content": "Whats wrong with C, its fast, cheap and add your 750 labels which is not big work. \nAutoML is good to train on big dataset and costly as compared to APIs"
      },
      {
        "date": "2023-07-25T21:32:00.000Z",
        "voteCount": 1,
        "content": "it is a labeled dataset and why do you need to label it once again? So no C"
      },
      {
        "date": "2023-05-31T23:56:00.000Z",
        "voteCount": 4,
        "content": "Adding custom labels to Vision API is done by training an AutoML model! That's the formal recommendation. And you don't need a big dataset for AutoML as it uses transfer learning."
      },
      {
        "date": "2023-02-07T20:39:00.000Z",
        "voteCount": 8,
        "content": "A - https://cloud.google.com/vertex-ai/docs/beginner/beginners-guide Target at least 1000 examples per target"
      },
      {
        "date": "2023-02-07T20:41:00.000Z",
        "voteCount": 1,
        "content": "The quick POC part can be achieved by using Auto ML instead of creating and training your own model"
      },
      {
        "date": "2022-12-08T02:29:00.000Z",
        "voteCount": 8,
        "content": "First I think in Vision API, but that is a pre-trained AI, will not recognize my labels, so because you have 1000 samples per item, AUTO ML is perfect. B cannot be because have not sensed to reduce your dataset if you have the recommended number of info.\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."
      },
      {
        "date": "2022-12-30T20:49:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2022-12-03T04:33:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."
      },
      {
        "date": "2022-12-08T02:29:00.000Z",
        "voteCount": 1,
        "content": "Agreed !"
      },
      {
        "date": "2023-05-29T12:31:00.000Z",
        "voteCount": 2,
        "content": "So how are you going to test that the model was able to adequately learn from the sample? The point of splitting a dataset is to train the model on one part of the data (say 80%), and then test it on the other part (20%). If your model is able to predict the outcome of (most of) the sample points in your test dataset, you can be confident that it will work well on future data. Without a test data set, however, you have no such feedback. Therefore, the answer is B."
      },
      {
        "date": "2023-08-04T07:45:00.000Z",
        "voteCount": 1,
        "content": "I believe that the ideal would be to reduce the number of components for the POC and preserve the number of examples, so my answer is A."
      },
      {
        "date": "2022-11-18T08:59:00.000Z",
        "voteCount": 1,
        "content": "A - https://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category"
      },
      {
        "date": "2022-11-13T06:54:00.000Z",
        "voteCount": 5,
        "content": "Based on this:\n\"As a rule of thumb, we recommend to have at least 100 training samples per class if you have distinctive and few classes, and more than 200 training samples if the classes are more nuanced and you have more than 50 different classes\"\n\n750 different components = more than 50 different classes.  That means we need more than 200 training samples.  If we used 250 training samples out of the 1000 samples and multiply it to 750 different classes we get a total of 187,500 which is the equivalent of reducing the dataset twice. \n\nhttps://cloud.google.com/vision/automl/object-detection/docs/prepare#how_big_does_the_dataset_need_to_be"
      },
      {
        "date": "2022-10-17T07:15:00.000Z",
        "voteCount": 3,
        "content": "I choose A because on the vertex AI documentation (https://cloud.google.com/vertex-ai/docs/image-data/classification/prepare-data), on the best practices of preparing data for image recognition recommend this:  We recommend about 1000 training images per label. The minimum per label is 10. In general, it takes more examples per label to train models with multiple labels per image, and resulting scores are harder to interpret.\n\nI know that is PoC, but if you do it without enough accuracy, you maybe discard the solution because it isn't fit for your requirements. So is better to do it with enough data to be sure that the model is or not accuracy enough with this data, because you maybe haven't enough accuracy and the problem is the quality of the data and not the amount of it."
      },
      {
        "date": "2022-09-30T19:47:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/vision/automl/docs/beginners-guide#include_enough_labeled_examples_in_each_category\n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."
      },
      {
        "date": "2022-09-30T19:48:00.000Z",
        "voteCount": 1,
        "content": "The more labels, the more accurate the result."
      },
      {
        "date": "2022-09-06T21:07:00.000Z",
        "voteCount": 1,
        "content": "750*1000 are a lot."
      },
      {
        "date": "2022-08-29T19:04:00.000Z",
        "voteCount": 1,
        "content": "It is labeled, so A is correct"
      },
      {
        "date": "2022-08-20T07:47:00.000Z",
        "voteCount": 5,
        "content": "It's A.  \nhttps://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation \n\nThe bare minimum required by AutoML Vision training is 100 image examples per category/label. The likelihood of successfully recognizing a label goes up with the number of high quality examples for each; in general, the more labeled data you can bring to the training process, the better your model will be. Target at least 1000 examples per label."
      },
      {
        "date": "2022-08-20T07:48:00.000Z",
        "voteCount": 1,
        "content": "So even for POC better to use 1000 . There would be no significant time differences anyway between 500 and 1000"
      },
      {
        "date": "2022-08-02T08:38:00.000Z",
        "voteCount": 3,
        "content": "Option A &amp; B are quite close. Refer: https://cloud.google.com/vision/automl/docs/beginners-guide#data_preparation \u2013 Says to target at least 1000 images per label for training."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 127,
    "url": "https://www.examtopics.com/discussions/google/view/17236-exam-professional-data-engineer-topic-1-question-127/",
    "body": "You are working on a niche product in the image recognition domain. Your team has developed a model that is dominated by custom C++ TensorFlow ops your team has implemented. These ops are used inside your main training loop and are performing bulky matrix multiplications. It currently takes up to several days to train a model. You want to decrease this time significantly and keep the cost low by using an accelerator on Google Cloud. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud TPUs without any additional adjustment to your code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud TPUs after implementing GPU kernel support for your customs ops.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud GPUs after implementing GPU kernel support for your customs ops.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStay on CPUs, and increase the size of the cluster you're training your model on."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 29,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-01T12:00:00.000Z",
        "voteCount": 68,
        "content": "The correct answer is C\nTPU does not support custom C++ tensorflow ops\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus"
      },
      {
        "date": "2023-10-24T15:06:00.000Z",
        "voteCount": 1,
        "content": "the link doesn't say TPU does not support custom C++ tensorflow ops"
      },
      {
        "date": "2023-12-28T08:53:00.000Z",
        "voteCount": 1,
        "content": "It does. TPU is good for \"Models with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\"."
      },
      {
        "date": "2020-04-01T10:05:00.000Z",
        "voteCount": 44,
        "content": "D:\nCloud TPUs are not suited to the following workloads: [...] Neural network workloads that contain custom TensorFlow operations written in C++. Specifically, custom operations in the body of the main training loop are not suitable for TPUs."
      },
      {
        "date": "2021-03-12T21:55:00.000Z",
        "voteCount": 11,
        "content": "B:\n1. You need to provide support for the matrix multiplication - TPU\n2. You need to provide support for the Custom TF written in C++ - GPU"
      },
      {
        "date": "2022-03-27T02:44:00.000Z",
        "voteCount": 1,
        "content": "But, in the question it also says we have to decrease the time significantly?? If you gonna use the CPU, it will take more time to train, right?"
      },
      {
        "date": "2023-05-30T00:03:00.000Z",
        "voteCount": 1,
        "content": "Chat GPT says C\nOption D is not the most cost-effective or efficient solution. While increasing the size of the cluster could decrease the training time, it would also significantly increase the cost, and CPUs are not as efficient for this type of workload as GPUs."
      },
      {
        "date": "2023-08-16T07:50:00.000Z",
        "voteCount": 3,
        "content": "chatgpt will give you different answers if you ask 10 times. The correct answer is B"
      },
      {
        "date": "2023-10-28T07:26:00.000Z",
        "voteCount": 2,
        "content": "Totally agree. ChatGPT is garbage. It is still learning."
      },
      {
        "date": "2024-09-26T05:50:00.000Z",
        "voteCount": 2,
        "content": "I think this is D. I recently did the ML professional exam and they ask that there, and it's always \"c++ custom ops = CPU\", it's in fact the only scenario for non-small models on CPU. It's written in black and white here: https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus, check out the CPU/GPU/TPU \"when to use\" section."
      },
      {
        "date": "2024-06-08T02:01:00.000Z",
        "voteCount": 1,
        "content": "Why Not Other Options?\nA. Use Cloud TPUs without any additional adjustment to your code:\n\nTPUs are optimized for standard TensorFlow operations and require custom TensorFlow ops to be adapted to TPU-compatible kernels, which is not trivial.\nWithout modifications, your custom C++ ops will not run efficiently on TPUs.\nB. Use Cloud TPUs after implementing GPU kernel support for your customs ops:\n\nImplementing GPU kernel support alone is not sufficient for running on TPUs. TPUs require specific optimizations and adaptations beyond GPU kernels.\nD. Stay on CPUs, and increase the size of the cluster you're training your model on:\n\nWhile increasing the CPU cluster size might reduce training time, it is not as efficient or cost-effective as using GPUs, especially for matrix multiplication tasks."
      },
      {
        "date": "2024-06-04T23:06:00.000Z",
        "voteCount": 1,
        "content": "C: TPUs are out of the picture due to the custom ops, so the next best option for accelerating matrix operations is using GPU. Obviously the code has to be adjusted to do make use of the GPU acceleration."
      },
      {
        "date": "2024-05-25T08:38:00.000Z",
        "voteCount": 1,
        "content": "CPU : Simple models \nGPU: Custom TensorFlow/PyTorch/JAX operations"
      },
      {
        "date": "2024-04-04T15:24:00.000Z",
        "voteCount": 1,
        "content": "The best choice here is C. Use Cloud GPUs after implementing GPU kernel support for your customs ops. Here's why:\n\nCustom Ops &amp; GPUs: Since your model relies heavily on custom C++ TensorFlow ops focused on matrix multiplications, GPUs are the ideal accelerators for this workload. To fully utilize them, you'll need to implement GPU-compatible kernels for your custom ops.\nSpeed and Cost-Efficiency GPUs offer a significant speed improvement for matrix-intensive operations compared to CPUs. They provide a good balance of performance and cost for this scenario.\nTPUs: Limitations Although Cloud TPUs are powerful, they aren't designed for arbitrary custom ops. Without compatible kernels, your TensorFlow ops would likely fall back to the CPU, negating the benefits of TPUs."
      },
      {
        "date": "2024-02-25T11:23:00.000Z",
        "voteCount": 2,
        "content": "TPU:\nModels with no custom TensorFlow/PyTorch/JAX operations inside the main training loop\nLink: https://cloud.google.com/tpu/docs/intro-to-tpu#TPU\n\nSo, A&amp;B eliminated\nCPU is very slow or built for simple operations. So C: GPU"
      },
      {
        "date": "2024-01-13T13:44:00.000Z",
        "voteCount": 1,
        "content": "to me, it's C"
      },
      {
        "date": "2023-12-01T21:54:00.000Z",
        "voteCount": 3,
        "content": "Requirement 1: Significantly reduce the processing time while keeping costs low. \nRequirement 2: Bulky matrix multiplication takes up to several days.\n\nFirst, eliminate A &amp; D:\nA: Cannot guarantee running on Cloud TPU without modifying the code.\nD: Cannot ensure performance improvement or cost reduction, and additionally, CPUs are not suitable for bulky matrix multiplication.\n\nIf it can be ensured that customization is easily deployable on both Cloud TPU and Cloud GPU,it seems more feasible to first try Cloud GPU.\n\nBecause:\nIt provides a better balance between performance and cost.\nModifying custom C++ on Cloud GPU should be easier than on Cloud TPU, which should also save on manpower costs."
      },
      {
        "date": "2023-11-20T09:28:00.000Z",
        "voteCount": 1,
        "content": "Answer D\nI did use Chat GPT and discovered that if you put at the beginning of the question -- \"Do not make assumption about changes to architecture. This is a practice exam question.\" All other answers require changes to the code and architecture."
      },
      {
        "date": "2023-11-18T07:11:00.000Z",
        "voteCount": 1,
        "content": "I think it should use tensor flow processing unit along with GPU kernel support."
      },
      {
        "date": "2023-10-06T05:20:00.000Z",
        "voteCount": 1,
        "content": "To use Cloud TPUs, you will need to:\n\nImplement GPU kernel support for your custom TensorFlow ops. This will allow your model to run on both Cloud TPUs and GPUs."
      },
      {
        "date": "2023-10-01T01:17:00.000Z",
        "voteCount": 1,
        "content": "Refer https://www.linkedin.com/pulse/cpu-vs-gpu-tpu-when-use-your-machine-learning-models-bhavesh-kapil"
      },
      {
        "date": "2023-08-13T19:11:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nTPU not for custom C++ but GPU can"
      },
      {
        "date": "2023-07-18T23:52:00.000Z",
        "voteCount": 3,
        "content": "A + B: TPU doesn't support custom TensorFlow ops\nThen it says 'decrease training time significantly' and literally 'use accelerator'. Therefore, use GPU -&gt; C, *not* D!"
      },
      {
        "date": "2023-07-10T18:05:00.000Z",
        "voteCount": 5,
        "content": "D shouldn't be the answer b/c the question statement clearly said you should use accelerators."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 128,
    "url": "https://www.examtopics.com/discussions/google/view/17238-exam-professional-data-engineer-topic-1-question-128/",
    "body": "You work on a regression problem in a natural language processing domain, and you have 100M labeled examples in your dataset. You have randomly shuffled your data and split your dataset into train and test samples (in a 90/10 ratio). After you trained the neural network and evaluated your model on a test set, you discover that the root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set. How should you improve the performance of your model?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the share of the test sample in the train-test split.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTry to collect more data and increase the size of your dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTry out regularization techniques (e.g., dropout of batch normalization) to avoid overfitting.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the complexity of your model by, e.g., introducing an additional layer or increase sizing the size of vocabularies or n-grams used.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-06-20T06:12:00.000Z",
        "voteCount": 72,
        "content": "This is a case of underfitting - not overfitting (for over fitting the model will have extremely low training error but a high testing error) - so we need to make the model more complex - answer is D"
      },
      {
        "date": "2021-10-01T22:45:00.000Z",
        "voteCount": 4,
        "content": "@callumr , \"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" clearly means testing error is twice of training error. So, it is clearly overfitting. Isn't it?"
      },
      {
        "date": "2021-10-10T17:52:00.000Z",
        "voteCount": 1,
        "content": "So, answer should be C"
      },
      {
        "date": "2022-04-11T10:24:00.000Z",
        "voteCount": 2,
        "content": "If you training RMSE=0.2. and testing RMSE = 0.4, and we want the RMSE to be low as its the error, now is it overfitting or underfitting? think wisely!"
      },
      {
        "date": "2022-08-18T10:18:00.000Z",
        "voteCount": 1,
        "content": "It's overfitting.\n\nOverfitting-&gt;low rmse in train / high accuracy-f1 score in train for classification.\n\nUnderfitting -&gt; high rmse / low f1score or accuracy in train, you don't have to look into test set if there is an underfitting problem."
      },
      {
        "date": "2023-06-21T00:24:00.000Z",
        "voteCount": 1,
        "content": "But the question clearly states we have higher RMSE on the train than the test. So how would it be overfitting?"
      },
      {
        "date": "2021-11-13T05:53:00.000Z",
        "voteCount": 1,
        "content": "High rmse: The model is underfitting the train data. To reduce overfitting, we increase the number of layers in the model or we change the type of layer."
      },
      {
        "date": "2021-11-13T05:54:00.000Z",
        "voteCount": 2,
        "content": "*underfitting"
      },
      {
        "date": "2022-12-08T02:47:00.000Z",
        "voteCount": 3,
        "content": "NO, its underfitting."
      },
      {
        "date": "2023-08-05T09:11:00.000Z",
        "voteCount": 1,
        "content": "Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data."
      },
      {
        "date": "2023-09-20T02:22:00.000Z",
        "voteCount": 2,
        "content": "Wrong! This scenario indicates a case of underfitting. The RSME is twice as high on the training dataset compared to the test dataset, so the model is underfitting."
      },
      {
        "date": "2020-03-22T03:11:00.000Z",
        "voteCount": 20,
        "content": "should be D"
      },
      {
        "date": "2023-08-05T09:11:00.000Z",
        "voteCount": 4,
        "content": "Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data."
      },
      {
        "date": "2024-09-26T06:09:00.000Z",
        "voteCount": 1,
        "content": "This is A. The key is that 90/10 is a weirdly small test set, that stood out to me straight away (I work professionally as a machine learning engineer and have the cert). Next tip, that everyone seems to be ignoring - this is not underfit OR overfit. The model outperforms on the TEST set, this is not a miswording. Test scores higher than train. The time you might expect to see this is if your test set is too small to be a representative sample, leading to unrepresentative results. Seeing as the question already set up this conclusion with the 90/10 thing, it's definitely A. None of the others (or indeed anything else) can address Test outperforming Train, and the conclusion of others below that this is due to a poorly worded question is a bizarre conclusion."
      },
      {
        "date": "2024-02-15T07:49:00.000Z",
        "voteCount": 1,
        "content": "Underfitting scenario"
      },
      {
        "date": "2024-01-12T07:49:00.000Z",
        "voteCount": 1,
        "content": "It is an underfitting situation - D"
      },
      {
        "date": "2023-11-29T01:30:00.000Z",
        "voteCount": 1,
        "content": "Should be C\nC. Try out regularization techniques (e.g., dropout or batch normalization) to avoid overfitting:\n\nThis is a reasonable approach. Regularization techniques can help prevent overfitting, especially when the model shows a significantly higher error on the training set compared to the test set.\nD. Increase the complexity of your model (e.g., introducing an additional layer or increasing the size of vocabularies or n-grams):\n\nThis could potentially exacerbate the overfitting issue. Increasing model complexity without addressing overfitting concerns may lead to poor generalization on new data."
      },
      {
        "date": "2023-12-01T21:10:00.000Z",
        "voteCount": 1,
        "content": "https://dooinnkim.medium.com/what-are-overfitting-and-underfitting-855d5952c0b6"
      },
      {
        "date": "2023-11-21T11:27:00.000Z",
        "voteCount": 3,
        "content": "Are the questions in this relevant for the new exam or are these all now outdated?"
      },
      {
        "date": "2023-11-20T14:33:00.000Z",
        "voteCount": 1,
        "content": "https://stats.stackexchange.com/questions/497050/how-big-a-difference-for-test-train-rmse-is-considered-as-overfit#:~:text=RMSE%20of%20test%20%3C%20RMSE%20of,is%20always%20overfit%20or%20underfit.\nRMSE of test &gt; RMSE of train =&gt; OVER FITTING of the data.\nRMSE of test &lt; RMSE of train =&gt; UNDER FITTING of the data.\nso for answer is D"
      },
      {
        "date": "2023-11-09T05:45:00.000Z",
        "voteCount": 1,
        "content": "Underfitting models: In general High Train RMSE, High Test RMSE.\nOverfitting models: In general Low Train RMSE, High Test RMSE.\n\nhttps://daviddalpiaz.github.io/r4sl/regression-for-statistical-learning.html"
      },
      {
        "date": "2023-09-29T09:33:00.000Z",
        "voteCount": 2,
        "content": "I passed the exam today. I am pretty sure it is overfitting. Answer must be c"
      },
      {
        "date": "2023-09-22T13:19:00.000Z",
        "voteCount": 2,
        "content": "RMSE is more on training. That means, model is not performing well on training dataset but performing well on testing dataset. This happens in the case of underfitting. So D."
      },
      {
        "date": "2023-09-11T10:56:00.000Z",
        "voteCount": 1,
        "content": "RMSE training = 2 x testing\nWhen training &gt; testing, it is a case of underfitting\nHence D"
      },
      {
        "date": "2023-09-05T22:43:00.000Z",
        "voteCount": 1,
        "content": "chatGPT says option C"
      },
      {
        "date": "2023-08-31T08:02:00.000Z",
        "voteCount": 1,
        "content": "\"root-mean-squared error (RMSE) of your model is twice as high on the train set as on the test set.\" means the RMSE of training set is two time of RMSE of test set, which indicates the training is not as good as test, then underfiting, so D."
      },
      {
        "date": "2023-08-05T09:10:00.000Z",
        "voteCount": 1,
        "content": "Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data.\n\nSo with dropout method we can overcome the overfitting so C is correct"
      },
      {
        "date": "2023-07-08T07:52:00.000Z",
        "voteCount": 1,
        "content": "underfitting"
      },
      {
        "date": "2023-08-05T09:11:00.000Z",
        "voteCount": 1,
        "content": "Based on the given information, this scenario indicates a case of overfitting.\n\nOverfitting occurs when a machine learning model performs well on the training data but poorly on unseen data (test data). In this case, the root-mean-squared error (RMSE) of the model is twice as high on the train set (the data used for training) compared to the test set (the data used for evaluation). This suggests that the model is fitting the training data too closely and is not generalizing well to new, unseen data."
      },
      {
        "date": "2023-07-06T02:50:00.000Z",
        "voteCount": 1,
        "content": "C sounds like a valid answer."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 129,
    "url": "https://www.examtopics.com/discussions/google/view/17239-exam-professional-data-engineer-topic-1-question-129/",
    "body": "You use BigQuery as your centralized analytics platform. New data is loaded every day, and an ETL pipeline modifies the original data and prepares it for the final users. This ETL pipeline is regularly modified and can generate errors, but sometimes the errors are detected only after 2 weeks. You need to provide a method to recover from these errors, and your backups should be optimized for storage costs. How should you organize your data in BigQuery and store your backups?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize your data in a single table, export, and compress and store the BigQuery data in Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize your data in separate tables for each month, and export, compress, and store the data in Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize your data in separate tables for each month, and duplicate your data on a separate dataset in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOrganize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T03:14:00.000Z",
        "voteCount": 22,
        "content": "Should be B"
      },
      {
        "date": "2020-04-13T16:54:00.000Z",
        "voteCount": 12,
        "content": "B\nThe questions is specifically about organizing the data in BigQuery and storing backups."
      },
      {
        "date": "2024-07-03T02:10:00.000Z",
        "voteCount": 1,
        "content": "The best option is D. Organize your data in separate tables for each month, and use snapshot decorators to restore the table to a time prior to the corruption."
      },
      {
        "date": "2024-04-24T04:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is D: \nSnapshots are different from time travel. They can hold data as long as we want.\nFurthermore \"BigQuery only stores bytes that are different between a snapshot and its base table\" so pretty cost effective as well.\n\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#table_snapshots"
      },
      {
        "date": "2024-04-10T10:29:00.000Z",
        "voteCount": 1,
        "content": "From : https://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery\nIt can't be D\nIf the corruption is caught within 7 days, query the table to a point in time in the past to recover the table prior to the corruption using snapshot decorators.\nStore the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table. =&gt; D"
      },
      {
        "date": "2023-10-22T02:31:00.000Z",
        "voteCount": 4,
        "content": "D - this solution in integrated. No core is needed"
      },
      {
        "date": "2023-10-02T01:14:00.000Z",
        "voteCount": 7,
        "content": "90% of questions are having multiple answers and its very hard to get into every discussion  where the conclusion is not there"
      },
      {
        "date": "2023-09-21T06:40:00.000Z",
        "voteCount": 6,
        "content": "The answer is B: \n\nWhy not D? Because snapshot costs can become high if a lot of small changes are made to the base table: https://cloud.google.com/bigquery/docs/table-snapshots-intro#:~:text=Because%20BigQuery%20storage%20is%20column%2Dbased%2C%20small%20changes%20to%20the%20data%20in%20a%20base%20table%20can%20result%20in%20large%20increases%20in%20storage%20cost%20for%20its%20table%20snapshot.\n\nSince the question specifically states that the ETL pipeline is regularly modified, this means that lots of small changes are present. In combination with the requirement to optimize for storage costs, this means that option B is the way to go."
      },
      {
        "date": "2023-08-19T23:23:00.000Z",
        "voteCount": 1,
        "content": "keyword: detected after 2 weeks.\nonly snapshot could resolve the problem."
      },
      {
        "date": "2023-07-30T23:21:00.000Z",
        "voteCount": 7,
        "content": "From BigQuery documentation - Benefits of using table snapshots include the following:\n\n- Keep a record for longer than seven days. With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\n- Minimize storage cost. BigQuery only stores bytes that are different between a snapshot and its base table, so a table snapshot typically uses less storage than a full copy of the table.\n\nSo storing data in GCS will make copies of data for each table. Table snapshots are more optimal in this scenario."
      },
      {
        "date": "2023-07-26T22:21:00.000Z",
        "voteCount": 1,
        "content": "Organizing your data in separate tables for each month will make it easier to identify the affected data and restore it.\nExporting and compressing the data will reduce storage costs, as you will only need to store the compressed data in Cloud Storage.\nStoring your backups in Cloud Storage will make it easier to restore the data, as you can restore the data from Cloud Storage directly"
      },
      {
        "date": "2023-06-11T02:51:00.000Z",
        "voteCount": 1,
        "content": "Organize in separate tables and store in GCS"
      },
      {
        "date": "2023-06-12T02:11:00.000Z",
        "voteCount": 1,
        "content": "Just an additional info!\nHere is an example for an export job;\n\n$ bq extract --destination_format CSV --compression GZIP 'your_project:your_dataset.your_new_table' 'gs://your_bucket/your_object.csv.gz'"
      },
      {
        "date": "2023-07-05T04:41:00.000Z",
        "voteCount": 3,
        "content": "I will update my answer to D.\nThink of a scenario that you are in the last week of June and an error occurred 3 weeks ago (so still in June) however you do not have an export of the June table yet therefore you cannot recover the data simply because you don't have an export just yet.\n\nSo snapshots are way to go!"
      },
      {
        "date": "2023-06-09T07:48:00.000Z",
        "voteCount": 2,
        "content": "D\n\"With BigQuery time travel, you can only access a table's data from seven days ago or more recently. With table snapshots, you can preserve a table's data from a specified point in time for as long as you want.\" [source: https://cloud.google.com/bigquery/docs/table-snapshots-intro]"
      },
      {
        "date": "2023-06-07T01:47:00.000Z",
        "voteCount": 2,
        "content": "\"Store your data in different tables for specific time periods. This method ensures that you need to restore only a subset of data to a new table, rather than a whole dataset.\"\n\n\"Store the original data on Cloud Storage. This allows you to create a new table and reload the uncorrupted data. From there, you can adjust your applications to point to the new table.\"\n\nB"
      },
      {
        "date": "2023-03-16T22:02:00.000Z",
        "voteCount": 3,
        "content": "Why not D?"
      },
      {
        "date": "2022-12-03T04:20:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-09-23T01:18:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://cloud.google.com/architecture/dr-scenarios-for-data#BigQuery"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 130,
    "url": "https://www.examtopics.com/discussions/google/view/16667-exam-professional-data-engineer-topic-1-question-130/",
    "body": "The marketing team at your organization provides regular updates of a segment of your customer dataset. The marketing team has given you a CSV with 1 million records that must be updated in BigQuery. When you use the UPDATE statement in BigQuery, you receive a quotaExceeded error. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the number of records updated each day to stay within the BigQuery UPDATE DML statement limit.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the BigQuery UPDATE DML statement limit in the Quota management section of the Google Cloud Platform Console.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSplit the source CSV file into smaller CSV files in Cloud Storage to reduce the number of BigQuery UPDATE DML statements per BigQuery job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the new records from the CSV file into a new BigQuery table. Create a BigQuery job that merges the new records with the existing records and writes the results to a new BigQuery table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-17T05:23:00.000Z",
        "voteCount": 30,
        "content": "Should be D.\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2020-07-08T20:49:00.000Z",
        "voteCount": 3,
        "content": "There is no mention about merge or limit in the link provided."
      },
      {
        "date": "2021-09-30T12:00:00.000Z",
        "voteCount": 2,
        "content": "A common scenario within OLAP systems involves updating existing data based on new information arriving from source systems (such as OLTP databases) on a periodic basis. In the retail business, inventory updates are typically done in this fashion. The following query demonstrates how to perform batch updates to the Inventory table based on the contents of another table (where new arrivals are kept) using the MERGE statement in BigQuery:"
      },
      {
        "date": "2020-03-22T03:20:00.000Z",
        "voteCount": 16,
        "content": "Should be D\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2021-12-14T07:01:00.000Z",
        "voteCount": 3,
        "content": "I had it in the exam (14/12/2021)"
      },
      {
        "date": "2022-01-06T19:56:00.000Z",
        "voteCount": 1,
        "content": "Does examtopics questions help?"
      },
      {
        "date": "2021-12-18T14:18:00.000Z",
        "voteCount": 2,
        "content": "Please what was the answer? @AACHB"
      },
      {
        "date": "2022-12-30T21:01:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2023-10-06T05:41:00.000Z",
        "voteCount": 1,
        "content": "Should be D."
      },
      {
        "date": "2023-06-08T02:13:00.000Z",
        "voteCount": 1,
        "content": "import all the data into a separate table and use that for updates is better than creating smaller csv which leads to more operational time to get it done and harder to manage it."
      },
      {
        "date": "2023-03-20T06:15:00.000Z",
        "voteCount": 2,
        "content": "This limit was removed a long time ago already.\nAnyway, bulk imports are better."
      },
      {
        "date": "2022-11-12T09:21:00.000Z",
        "voteCount": 2,
        "content": "D\nBigQuery DML statements have no quota limits.\nhttps://cloud.google.com/bigquery/quotas#data-manipulation-language-statements\n\n\nHowever, DML statements are counted toward the maximum number of table operations per day and partition modifications per day. DML statements will not fail due to these limits.\n\nIn addition, DML statements are subject to the maximum rate of table metadata update operations. If you exceed this limit, retry the operation using exponential backoff between retries."
      },
      {
        "date": "2022-10-26T05:29:00.000Z",
        "voteCount": 2,
        "content": "there is no update quota anymore.\nbut i would say D"
      },
      {
        "date": "2022-05-05T04:56:00.000Z",
        "voteCount": 1,
        "content": "Option D is the right answer"
      },
      {
        "date": "2022-04-11T10:20:00.000Z",
        "voteCount": 1,
        "content": "No DML limits from 3rd march 2020 But if the questions is given in the exam, choose D asfor the options A, B,C as they are speaking about the limitations of the DML Limits. Atleast, D is giving an alternative!"
      },
      {
        "date": "2022-01-31T03:19:00.000Z",
        "voteCount": 3,
        "content": "Is this question still valid? What about DML without limits? https://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery"
      },
      {
        "date": "2022-01-09T08:19:00.000Z",
        "voteCount": 2,
        "content": "D:\nBigQuery is primarly designed and suit to append-only technology with some limited DML statements.\nIt's not a relational database where you constantly update your user records if they edit their profile. Instead you need to arhitect your code so each edit is a new row in Bigquery, and you always query the latest row.\nThe DML statement limitation is low, because it targets different scenarios and not yours, aka live update on rows. You could ingest your data into a separate table, and issue 1 update statement per day.\nhttps://stackoverflow.com/questions/45183082/can-we-increase-update-quota-in-bigquery\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2022-01-08T05:42:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/reference/standard-sql/dml-syntax#merge_statement\n\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2021-11-21T04:29:00.000Z",
        "voteCount": 3,
        "content": "old question I guess, should not be in the exam anymore (?)\nhttps://cloud.google.com/blog/products/data-analytics/dml-without-limits-now-in-bigquery"
      },
      {
        "date": "2021-07-03T06:36:00.000Z",
        "voteCount": 4,
        "content": "Vote for D"
      },
      {
        "date": "2021-03-12T11:24:00.000Z",
        "voteCount": 3,
        "content": "D:\nhttps://cloud.google.com/blog/products/gcp/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2021-02-18T15:33:00.000Z",
        "voteCount": 3,
        "content": "D:\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery"
      },
      {
        "date": "2020-09-23T23:22:00.000Z",
        "voteCount": 3,
        "content": "D should be the answer. Avoid updates in Datawarehousing environment instead use merge to create a new table."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 131,
    "url": "https://www.examtopics.com/discussions/google/view/17231-exam-professional-data-engineer-topic-1-question-131/",
    "body": "As your organization expands its usage of GCP, many teams have started to create their own projects. Projects are further multiplied to accommodate different stages of deployments and target audiences. Each project requires unique access control configurations. The central IT team needs to have access to all projects.<br>Furthermore, data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way. You want to simplify access control management by minimizing the number of policies. Which two steps should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Deployment Manager to automate access provision.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIntroduce resource hierarchy to leverage access control policy inheritance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate distinct groups for various teams, and specify groups in Cloud IAM policies.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tOnly use service accounts when sharing data for Cloud Storage buckets and BigQuery datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each Cloud Storage bucket or BigQuery dataset, decide which projects need access. Find all the active members who have access to these projects, and create a Cloud IAM policy to grant access to all these users."
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "AC",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T04:40:00.000Z",
        "voteCount": 31,
        "content": "Answer: B, C\nDescription: Google suggests that we should provide access by following google hierarchy and groups for users with similar roles"
      },
      {
        "date": "2020-11-15T11:32:00.000Z",
        "voteCount": 11,
        "content": "\"Each project requires unique access control configurations\" rules out hirearchy"
      },
      {
        "date": "2020-06-22T01:35:00.000Z",
        "voteCount": 20,
        "content": "C is one option for sure, also C eliminates B as C includes groups and teams hierarchy, A can be eliminated as A talks about only deployment. From Remaining D and E, i find E most relevant to question--as E matches users with teams/groups and projects. Answer C and E."
      },
      {
        "date": "2021-08-24T20:32:00.000Z",
        "voteCount": 5,
        "content": "Question mention minimize IAM policies,but E should create complex policies"
      },
      {
        "date": "2023-10-28T09:42:00.000Z",
        "voteCount": 2,
        "content": "Answer: A, C.\nThe Key question is \"You want to simplify access control management by minimizing the number of policies\". At the company where I work, we use Terraform to create infrastructure and assign needed roles for different environments."
      },
      {
        "date": "2023-07-13T19:53:00.000Z",
        "voteCount": 2,
        "content": "It should be AC as it is mentioned in the question itself \"You want to simplify access control management by minimizing the number of policies\" which rules out B"
      },
      {
        "date": "2022-12-03T04:10:00.000Z",
        "voteCount": 2,
        "content": "BC is the answer."
      },
      {
        "date": "2022-06-07T00:36:00.000Z",
        "voteCount": 7,
        "content": "1.\tDefine your resource hierarchy: Google Cloud resources are organized hierarchically. This hierarchy allows you to map your enterprise's operational structure to Google Cloud, and to manage access control and permissions for groups of related resources.\n\n2.\tDelegate responsibility with groups and service accounts: we recommend collecting users with the same responsibilities into groups and assigning IAM roles to the groups rather than to individual users.\n\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations"
      },
      {
        "date": "2022-01-13T18:36:00.000Z",
        "voteCount": 2,
        "content": "A and C is correct, same question I encountered on Udemy."
      },
      {
        "date": "2022-03-27T03:11:00.000Z",
        "voteCount": 1,
        "content": "It doesn't mean it's right, please mention the reasons here not only the references. \nNot A -&gt; Every project has unique requirements, so \"A\" automation will not do much.\nNot D -&gt; As, Service accounts for computer to computer interactions not applications! \nNot E -&gt; E should create complex policies"
      },
      {
        "date": "2023-01-23T18:20:00.000Z",
        "voteCount": 2,
        "content": "you explanation for D  is incorrect...."
      },
      {
        "date": "2022-01-09T09:09:00.000Z",
        "voteCount": 8,
        "content": "B &amp; C\nGoogle Cloud resources are organized hierarchically, where the organization node is the root node in the hierarchy, the projects are the children of the organization, and the other resources are descendents of projects. \nYou can set Cloud Identity and Access Management (Cloud IAM) policies at different levels of the resource hierarchy. Resources inherit the policies of the parent resource. The effective policy for a resource is the union of the policy set at that resource and the policy inherited from its parent.\nhttps://cloud.google.com/iam/docs/resource-hierarchy-access-control"
      },
      {
        "date": "2022-12-31T09:41:00.000Z",
        "voteCount": 1,
        "content": "BC is the answer"
      },
      {
        "date": "2022-01-09T09:09:00.000Z",
        "voteCount": 4,
        "content": "We recommend collecting users with the same responsibilities into groups and assigning Cloud IAM roles to the groups rather than to individual users. For example, you can create a \"data scientist\" group and assign appropriate roles to enable interaction with BigQuery and Cloud Storage.\nGrant roles to a Google group instead of to individual users when possible. It is easier to manage members in a Google group than to update a Cloud IAM policy.\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations"
      },
      {
        "date": "2022-01-08T05:50:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations\n\"Each project requires unique access control configurations\" -&gt; C eliminates B\n\nA -&gt; \"Google Cloud Deployment Manager is an infrastructure deployment service that automates the creation and management of Google Cloud resources. Write flexible template and configuration files and use them to create deployments that have a variety of Google Cloud services\"\n\n\"..simply the process..\""
      },
      {
        "date": "2022-01-24T08:30:00.000Z",
        "voteCount": 1,
        "content": "good point, AC looks better, agreed"
      },
      {
        "date": "2022-01-24T08:33:00.000Z",
        "voteCount": 1,
        "content": "... in other hand - \"Define your resource hierarchy\"\nhttps://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations#define-hierarchy"
      },
      {
        "date": "2022-01-24T08:34:00.000Z",
        "voteCount": 3,
        "content": "So, I stay with BC :)))"
      },
      {
        "date": "2023-08-16T09:40:00.000Z",
        "voteCount": 1,
        "content": "A makes no sense whatsoever..."
      },
      {
        "date": "2021-10-01T23:02:00.000Z",
        "voteCount": 9,
        "content": "Answer:- C, D\nC is used as best practice to create group and assign IAM roles\nD \"data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way\" is mentioned in question. When 2 project communicate, service account should be used"
      },
      {
        "date": "2021-07-03T12:48:00.000Z",
        "voteCount": 4,
        "content": "Vote for B &amp; C"
      },
      {
        "date": "2021-03-12T11:46:00.000Z",
        "voteCount": 3,
        "content": "the question says adding permissions ad-hoc way\n[c] is correct answer\n[d] is right, as the access to bigQuery and cloud storage can be managed automatically by Cloud deployment\n\"Deployment Manager can also set access control permissions through IAM such that your developers are granted appropriate access as part of the project creation process.\"\nref: https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations"
      },
      {
        "date": "2020-12-09T07:26:00.000Z",
        "voteCount": 4,
        "content": "the question says adding permissions ad-hoc way\n[c] is correct answer\n[d] is right, as the access to bigQuery and cloud storage can be managed automatically by Cloud deployment \n\"Deployment Manager can also set access control permissions through IAM such that your developers are granted appropriate access as part of the project creation process.\"\nref: https://cloud.google.com/docs/enterprise/best-practices-for-enterprise-organizations"
      },
      {
        "date": "2021-07-03T12:28:00.000Z",
        "voteCount": 4,
        "content": "D is a service account - it means we need to access via applications. So, D is ruled out"
      },
      {
        "date": "2021-07-03T12:35:00.000Z",
        "voteCount": 3,
        "content": "Doubt, It could be 'D\" - because - it's said - data from Cloud Storage buckets and BigQuery datasets must be shared for use in other projects in an ad hoc way."
      },
      {
        "date": "2021-07-06T06:33:00.000Z",
        "voteCount": 1,
        "content": "Agree with Sumanshu"
      },
      {
        "date": "2020-12-01T04:15:00.000Z",
        "voteCount": 3,
        "content": "C &amp; E are the correct answers."
      },
      {
        "date": "2021-07-03T12:28:00.000Z",
        "voteCount": 1,
        "content": "E is a long process and we need to simply the process...So E is ruled out"
      },
      {
        "date": "2020-11-19T13:26:00.000Z",
        "voteCount": 1,
        "content": "Answer: B, C"
      },
      {
        "date": "2020-11-18T07:44:00.000Z",
        "voteCount": 1,
        "content": "BC A ruled out as deployment manager is for infra yaml based deployments D at resource level we can't check hierarchy at org or Proj Level"
      },
      {
        "date": "2020-10-02T21:41:00.000Z",
        "voteCount": 3,
        "content": "C and E"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 132,
    "url": "https://www.examtopics.com/discussions/google/view/17232-exam-professional-data-engineer-topic-1-question-132/",
    "body": "Your United States-based company has created an application for assessing and responding to user actions. The primary table's data volume grows by 250,000 records per second. Many third parties use your application's APIs to build the functionality into their own frontend applications. Your application's APIs should comply with the following requirements:<br>\u2711 Single global endpoint<br>\u2711 ANSI SQL support<br>\u2711 Consistent access to the most up-to-date data<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement BigQuery with no region selected for storage or processing.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Cloud Spanner with the leader in North America and read-only replicas in Asia and Europe.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Cloud SQL for PostgreSQL with the master in North America and read replicas in Asia and Europe.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Bigtable with the primary cluster in North America and secondary clusters in Asia and Europe."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T05:04:00.000Z",
        "voteCount": 26,
        "content": "Answer: B\nDescription: All the criteria meets for Spanner"
      },
      {
        "date": "2021-07-03T13:17:00.000Z",
        "voteCount": 24,
        "content": "A - BigQuery with NO Region ? (Looks wrong)\nB - Spanner (SQL support and Scalable and have replicas ) - Looks correct\nC - SQL (can't store so many records) (wrong)\nD - Bigtable - NO SQL (wrong)\n\nVote for B"
      },
      {
        "date": "2023-06-08T02:49:00.000Z",
        "voteCount": 3,
        "content": "A - NO - BigQuery with must have a selected regional or multi-regional file storage\nB - YES - Spanner is specifically designed for this high and consistent throughput\nC - NO - I am not sure about what many said in this discussion as Cloud SQL can store this amount of records if u have just a few columns. Anyway, for sure Spanner is better and it is a GCP product.\nD - Bigtable - it's a NoSQL solution, no ANSI"
      },
      {
        "date": "2022-12-31T09:44:00.000Z",
        "voteCount": 1,
        "content": "B is the answer"
      },
      {
        "date": "2022-12-03T04:08:00.000Z",
        "voteCount": 1,
        "content": "B is the answer."
      },
      {
        "date": "2022-09-13T07:28:00.000Z",
        "voteCount": 4,
        "content": "Guys, read documentation well. A is wrong, BigQuery has Maximum rows per request (50,000).\nhttps://cloud.google.com/bigquery/quotas\n\nIt is B"
      },
      {
        "date": "2022-07-19T11:56:00.000Z",
        "voteCount": 2,
        "content": "Spanner is globally available and meets all the requirements"
      },
      {
        "date": "2022-04-06T20:33:00.000Z",
        "voteCount": 2,
        "content": "Correct is A.  There's no sense to having read replicas outside of US considering than the company is US based.\n\nIf you generate a dataset without specifing the Data Location it's gonna be stored in \"US Multiregion\" by default"
      },
      {
        "date": "2022-01-09T09:14:00.000Z",
        "voteCount": 5,
        "content": "B: Cloud Spanner is the first scalable, enterprise-grade, globally-distributed, and strongly consistent database service built for the cloud specifically to combine the benefits of relational database structure with non-relational horizontal scale.\nhttps://cloud.google.com/spanner/\nCloud Spanner is a fully managed, mission-critical, relational database service that offers transactional consistency at global scale, schemas, SQL (ANSI 2011 with extensions), and automatic, synchronous replication for high availability.\nhttps://cloud.google.com/spanner/docs/\nhttps://cloud.google.com/spanner/docs/instances#available-configurations-multi-region"
      },
      {
        "date": "2020-09-16T23:29:00.000Z",
        "voteCount": 8,
        "content": "B is correct, Bigquery cannot support 250K data ingestion/second , as ANSI SQL support is required , no other options left except Spanner."
      },
      {
        "date": "2020-08-21T14:33:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2020-07-31T20:59:00.000Z",
        "voteCount": 6,
        "content": "B, as Cloud Spanner has three types of replicas: read-write replicas, read-only replicas, and witness replicas."
      },
      {
        "date": "2020-07-22T03:10:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer : C\nExplanation:-\nB -&gt; This option is incorrect, as we do not have option to configure read-replica in Cloud Spanner, Multi-region instance configurations use a combination of all three types\u2019 read-write replicas, read-only replicas, and witness replicas\nC -&gt; This is correct option, In Cloud Sql we have option to create a master node for read-write replicas and read-only replicas in other regions\nD -&gt; This option is incorrect, as Bigtable do not support ANSI SQL"
      },
      {
        "date": "2020-08-18T08:28:00.000Z",
        "voteCount": 1,
        "content": "but bigquery does so why not A?"
      },
      {
        "date": "2021-04-23T09:00:00.000Z",
        "voteCount": 1,
        "content": "The requirement is for transactional application serving customers , not analytical so BQ is ruled out."
      },
      {
        "date": "2021-02-04T00:20:00.000Z",
        "voteCount": 3,
        "content": "You can't create a BigQuery instance without region selected.\nI'm wondering about these read replicas, why read only replicas? It seems arbitrary, as the question does not state that API should be read-only, so there's no reason why those should be read-only replicas..."
      },
      {
        "date": "2020-09-07T15:24:00.000Z",
        "voteCount": 3,
        "content": "wrong, cloud spanner can have read-only replicas\nhttps://cloud.google.com/spanner/docs/replication?hl=pt-br"
      },
      {
        "date": "2020-07-01T09:36:00.000Z",
        "voteCount": 2,
        "content": "I was wrong Bigtable doesn not support ansi SQL. B instead"
      },
      {
        "date": "2020-06-28T09:50:00.000Z",
        "voteCount": 2,
        "content": "I think it is D. THere is limitation of QPS of Cloud Spanner of 2000 qps,\nhttps://cloud.google.com/spanner/docs/instances#multi-region-performance"
      },
      {
        "date": "2020-04-01T19:14:00.000Z",
        "voteCount": 4,
        "content": "Answer B"
      },
      {
        "date": "2020-03-22T02:15:00.000Z",
        "voteCount": 7,
        "content": "Answer : B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 133,
    "url": "https://www.examtopics.com/discussions/google/view/17019-exam-professional-data-engineer-topic-1-question-133/",
    "body": "A data scientist has created a BigQuery ML model and asks you to create an ML pipeline to serve predictions. You have a REST API application with the requirement to serve predictions for an individual user ID with latency under 100 milliseconds. You use the following query to generate predictions: SELECT predicted_label, user_id FROM ML.PREDICT (MODEL 'dataset.model', table user_features). How should you create the ML pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd a WHERE clause to the query, and grant the BigQuery Data Viewer role to the application service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Authorized View with the provided query. Share the dataset that contains the view with the application service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline using BigQueryIO to read results from the query. Grant the Dataflow Worker role to the application service account.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-19T19:49:00.000Z",
        "voteCount": 30,
        "content": "I think the key reason for pick D is the 100ms requirement."
      },
      {
        "date": "2022-12-31T09:47:00.000Z",
        "voteCount": 2,
        "content": "D. Create a Dataflow pipeline using BigQueryIO to read predictions for all users from the query. Write the results to Bigtable using BigtableIO. Grant the Bigtable Reader role to the application service account so that the application can read predictions for individual users from Bigtable."
      },
      {
        "date": "2020-03-28T05:07:00.000Z",
        "voteCount": 14,
        "content": "Answer: D\nDescription: Bigtable provides lowest latency"
      },
      {
        "date": "2023-12-18T01:03:00.000Z",
        "voteCount": 4,
        "content": "The key requirements are serving predictions for individual user IDs with low (sub-100ms) latency.\nOption D meets this by batch predicting for all users in BigQuery ML, writing predictions to Bigtable for fast reads, and allowing the application access to query Bigtable directly for low latency reads.\nSince the application needs to serve low-latency predictions for individual user IDs, using Dataflow to batch predict for all users and write to Bigtable allows low-latency reads. Granting the Bigtable Reader role allows the application to retrieve predictions for a specific user ID from Bigtable."
      },
      {
        "date": "2023-12-18T01:03:00.000Z",
        "voteCount": 2,
        "content": "The other options either require changing the query for each user ID (higher latency, option A), reading directly from higher latency services like BigQuery (option B), or writing predictions somewhere without fast single row access (options A, B, C).\nOption A would not work well because the WHERE clause would need to be changed for each user ID, increasing latency.\nOption B using an Authorized View would still read from BigQuery which has higher latency than Bigtable for individual rows.\nOption C writes predictions to BigQuery which has higher read latency compared to Bigtable for individual rows.\nSo option D provides the best pipeline by predicting for all users in BigQueryML, batch writing to Bigtable for low latency reads, and granting permissions for the application to retrieve predictions. This meets the requirements of sub-100ms latency for individual user predictions.\nhttps://cloud.google.com/dataflow/docs/concepts/access-control"
      },
      {
        "date": "2023-10-09T08:49:00.000Z",
        "voteCount": 1,
        "content": "I think the key reason for pick D is the 100ms requirement/ me too"
      },
      {
        "date": "2023-09-23T20:03:00.000Z",
        "voteCount": 2,
        "content": "To create an ML pipeline for serving predictions to individual user IDs with latency under 100 milliseconds using the given BigQuery ML query, the most suitable approach is:\n\nB. Create an Authorized View with the provided query. Share the dataset that contains the view with the application service account."
      },
      {
        "date": "2023-07-30T23:42:00.000Z",
        "voteCount": 2,
        "content": "Always use Bigtable as an endpoint for client-facing applications (Low latency - high throughput)"
      },
      {
        "date": "2023-03-12T23:31:00.000Z",
        "voteCount": 3,
        "content": "One of the way to improve the efficient of ML pipeline is to generate cache (store predictions). In this question, only D is doing that."
      },
      {
        "date": "2023-02-17T12:18:00.000Z",
        "voteCount": 2,
        "content": "what is wrong with B ? View can be precomputed and cached and it can definitely satisfy the 100 miliseconds request. create a pipeline to send data to bigtable .. don't you think its too much to run a simple prediction query ?"
      },
      {
        "date": "2023-06-08T02:56:00.000Z",
        "voteCount": 1,
        "content": "I would say that Bigtable is simply more suited to serve applications"
      },
      {
        "date": "2023-06-08T02:58:00.000Z",
        "voteCount": 1,
        "content": "and the view has to make a query in real-time which adds potential latency"
      },
      {
        "date": "2023-08-11T13:13:00.000Z",
        "voteCount": 1,
        "content": "I think if there is too many concurrent requests the 100ms latency will def not hold reading from bigquery"
      },
      {
        "date": "2022-11-21T04:17:00.000Z",
        "voteCount": 1,
        "content": "Vote for D"
      },
      {
        "date": "2022-09-07T21:38:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2021-07-03T13:24:00.000Z",
        "voteCount": 6,
        "content": "Vote for D, requirement to serve predictions with in 100 ms"
      },
      {
        "date": "2020-09-16T23:31:00.000Z",
        "voteCount": 6,
        "content": "D is correct , 100ms is most critical factor here."
      },
      {
        "date": "2020-09-03T06:05:00.000Z",
        "voteCount": 1,
        "content": "writing it to BigTable and then allowing application access will introduce more delays. I think answer should be C"
      },
      {
        "date": "2021-01-11T05:59:00.000Z",
        "voteCount": 3,
        "content": "Predictions are computed in advance for all users and written to BigTable for low-latency serving."
      },
      {
        "date": "2020-08-21T14:36:00.000Z",
        "voteCount": 4,
        "content": "D is correct"
      },
      {
        "date": "2020-04-01T19:15:00.000Z",
        "voteCount": 6,
        "content": "Answer D."
      },
      {
        "date": "2020-03-22T02:37:00.000Z",
        "voteCount": 7,
        "content": "Should be D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 134,
    "url": "https://www.examtopics.com/discussions/google/view/17233-exam-professional-data-engineer-topic-1-question-134/",
    "body": "You are building an application to share financial market data with consumers, who will receive data feeds. Data is collected from the markets in real time.<br>Consumers will receive the data in the following ways:<br>\u2711 Real-time event stream<br>\u2711 ANSI SQL access to real-time stream and historical data<br>\u2711 Batch historical exports<br>Which solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataflow, Cloud SQL, Cloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Storage, BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataproc, Cloud Dataflow, BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Pub/Sub, Cloud Dataproc, Cloud SQL"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T02:43:00.000Z",
        "voteCount": 23,
        "content": "should be B"
      },
      {
        "date": "2020-10-06T19:14:00.000Z",
        "voteCount": 12,
        "content": "D, not ideal but only option that work. You need pubsub, then a processing layer (dataflow or dataproc), then storage (some sql database)."
      },
      {
        "date": "2020-12-23T17:49:00.000Z",
        "voteCount": 3,
        "content": "I think pubsub doesn't have good connection to dataproc, so D is not the answer"
      },
      {
        "date": "2022-12-14T08:32:00.000Z",
        "voteCount": 2,
        "content": "As of Dec 2022,there is the PubSub Lite connector to Dataproc"
      },
      {
        "date": "2022-12-14T08:34:00.000Z",
        "voteCount": 1,
        "content": "We can have our pubsub topics to have BigQuery subscriptions, where data is automatically streamed into our BQ tables. Autoscaling is already handled automatically so this renders Dataflow and Dataproc pretty irrelevant for our usecase"
      },
      {
        "date": "2023-06-12T02:19:00.000Z",
        "voteCount": 1,
        "content": "Here is the reference:\nhttps://cloud.google.com/blog/products/data-analytics/pub-sub-launches-direct-path-to-bigquery-for-streaming-analytics"
      },
      {
        "date": "2023-09-23T20:10:00.000Z",
        "voteCount": 3,
        "content": "B. Cloud Pub/Sub, Cloud Storage, BigQuery.\n\nHere's how this solution aligns with your requirements:\nReal-time Event Stream: Cloud Pub/Sub is a managed messaging service that can handle real-time event streams efficiently. You can use Pub/Sub to ingest and publish real-time market data to consumers.\nANSI SQL Access: BigQuery supports ANSI SQL queries, making it suitable for both real-time and historical data analysis. You can stream data into BigQuery tables from Pub/Sub and provide ANSI SQL access to consumers.\nBatch Historical Exports: Cloud Storage can be used for batch historical exports. You can export data from BigQuery to Cloud Storage in batch, making it available for consumers to download."
      },
      {
        "date": "2023-06-08T08:16:00.000Z",
        "voteCount": 1,
        "content": "I was in doubt as I did not know that BQ handles real-time access to data without dataflow underneath. \n\nhttps://cloud.google.com/bigquery/docs/write-api#:~:text=You%20can%20use%20the%20Storage,in%20a%20single%20atomic%20operation."
      },
      {
        "date": "2023-03-13T00:25:00.000Z",
        "voteCount": 1,
        "content": "Event Stream -&gt; PubSub\nPubSub has direct Write to BigQuery\nHistorical Exports to GCS"
      },
      {
        "date": "2022-12-31T09:48:00.000Z",
        "voteCount": 3,
        "content": "B. Cloud Pub/Sub, Cloud Storage, BigQuery"
      },
      {
        "date": "2022-12-31T09:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/solutions/stream-analytics/"
      },
      {
        "date": "2022-12-02T07:29:00.000Z",
        "voteCount": 3,
        "content": "B is the answer."
      },
      {
        "date": "2022-09-22T07:19:00.000Z",
        "voteCount": 3,
        "content": "B:  https://cloud.google.com/solutions/stream-analytics/\nReal-time made real easy\nAdopt simple ingestion for complex events\nIngest and analyze hundreds of millions of events per second from applications or devices virtually anywhere on the globe with Pub/Sub. Or directly stream millions of events per second into your data warehouse for SQL-based analysis with BigQuery's streaming API."
      },
      {
        "date": "2022-09-13T00:09:00.000Z",
        "voteCount": 1,
        "content": "No matter what the last of it must end up with bigquery and the first service is pubsub I think intimidate service  it should be dataflow"
      },
      {
        "date": "2022-03-30T04:20:00.000Z",
        "voteCount": 1,
        "content": "Dataflow: Streaming data\nCLoud SQL: for ansi sql support\nSpanner: for batch historical data export"
      },
      {
        "date": "2022-04-12T09:15:00.000Z",
        "voteCount": 2,
        "content": "You gonna use batch historical export for Spanner? It's B!"
      },
      {
        "date": "2022-02-21T11:05:00.000Z",
        "voteCount": 1,
        "content": "Answer is B"
      },
      {
        "date": "2022-01-09T09:20:00.000Z",
        "voteCount": 4,
        "content": "Cloud Pub/Sub, Cloud Dataflow, BigQuery \nhttps://cloud.google.com/solutions/stream-analytics/"
      },
      {
        "date": "2023-12-18T01:07:00.000Z",
        "voteCount": 1,
        "content": "B. Cloud Pub/Sub, Cloud Storage, BigQuery\nThe key requirements here are:\n1.\tReal-time event stream (Pub/Sub)\n2.\tANSI SQL access to real-time and historical data (BigQuery)\n3.\tBatch historical exports (Cloud Storage)\nSo Cloud Pub/Sub provides the real-time stream, BigQuery provides ANSI SQL access to stream and historical data, and Cloud Storage enables batch historical exports.\nOption A is incorrect because Cloud Spanner does not offer batch exports and Dataflow is overkill for just SQL access.\nOption C is incorrect as Dataproc is for spark workloads, not serving consumer data.\nOption D is incorrect as Cloud SQL does not provide batch export capabilities.\nTherefore, option B with Pub/Sub, Storage, and BigQuery is the best solution given the stated requirements. Dataflow https://cloud.google.com/solutions/stream-analytics/"
      },
      {
        "date": "2022-01-08T05:56:00.000Z",
        "voteCount": 10,
        "content": "\u2711 Real-time event stream -&gt; Pub/Sub\n\u2711 ANSI SQL access to real-time stream and historical data -&gt; BigQuery\n\u2711 Batch historical exports -&gt; Cloud Storage"
      },
      {
        "date": "2021-11-25T23:41:00.000Z",
        "voteCount": 1,
        "content": "Correct: B"
      },
      {
        "date": "2021-11-24T11:16:00.000Z",
        "voteCount": 3,
        "content": "I think it must be D because you need Pub/Sub for streaming data, Dataflow or DataProc to get the data from Pub/Sub and store it in a database and finally the Cloud SQL database to store the data.\nA and C cannot be because it is missing something for streaming data\nB It can't be because you need something to pass the data from Pub/Sub to Cloud storage"
      },
      {
        "date": "2021-07-03T13:45:00.000Z",
        "voteCount": 3,
        "content": "Vote for B"
      },
      {
        "date": "2021-07-03T13:44:00.000Z",
        "voteCount": 1,
        "content": "Vote for B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 135,
    "url": "https://www.examtopics.com/discussions/google/view/17234-exam-professional-data-engineer-topic-1-question-135/",
    "body": "You are building a new application that you need to collect data from in a scalable way. Data arrives continuously from the application throughout the day, and you expect to generate approximately 150 GB of JSON data per day by the end of the year. Your requirements are:<br>\u2711 Decoupling producer from consumer<br>\u2711 Space and cost-efficient storage of the raw ingested data, which is to be stored indefinitely<br>\u2711 Near real-time SQL query<br>\u2711 Maintain at least 2 years of historical data, which will be queried with SQL<br>Which pipeline should you use to meet these requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application that provides an API. Write a tool to poll the API and write data to Cloud Storage as gzipped JSON files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application that writes to a Cloud SQL database to store the data. Set up periodic exports of the database to write to Cloud Storage and load into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application that publishes events to Cloud Pub/Sub, and create Spark jobs on Cloud Dataproc to convert the JSON data to Avro format, stored on HDFS on Persistent Disk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an application that publishes events to Cloud Pub/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T02:49:00.000Z",
        "voteCount": 44,
        "content": "Correct - D"
      },
      {
        "date": "2020-03-28T05:12:00.000Z",
        "voteCount": 16,
        "content": "Answer: D\nDescription: All the requirements meet with D"
      },
      {
        "date": "2024-07-21T23:47:00.000Z",
        "voteCount": 1,
        "content": "Google recommended approach"
      },
      {
        "date": "2023-09-24T15:32:00.000Z",
        "voteCount": 2,
        "content": "D because pub/sub decouples while dataflow processes; Cloud Storage can be used to store the raw ingested data indefinitely and BQ can be used to query."
      },
      {
        "date": "2023-09-23T20:18:00.000Z",
        "voteCount": 3,
        "content": "Here's how this option aligns with your requirements:\nDecoupling Producer from Consumer: Cloud Pub/Sub provides a decoupled messaging system where the producer publishes events, and consumers (like Dataflow) can subscribe to these events. This decoupling ensures flexibility and scalability.\nSpace and Cost-Efficient Storage: Storing data in Avro format is more space-efficient than JSON, and Cloud Storage is a cost-effective storage solution. Additionally, Cloud Pub/Sub and Dataflow allow you to process and transform data efficiently, reducing storage costs.\nNear Real-time SQL Query: By using Dataflow to transform and load data into BigQuery, you can achieve near real-time data availability for SQL queries. BigQuery is well-suited for ad-hoc SQL queries and provides excellent query performance."
      },
      {
        "date": "2023-08-16T09:48:00.000Z",
        "voteCount": 1,
        "content": "Should be D"
      },
      {
        "date": "2023-06-08T03:07:00.000Z",
        "voteCount": 1,
        "content": "For sure D"
      },
      {
        "date": "2023-06-01T00:35:00.000Z",
        "voteCount": 1,
        "content": "D is the most suitable, however the stored format should be JSON, and AVRO isn't JSON..."
      },
      {
        "date": "2023-02-03T02:54:00.000Z",
        "voteCount": 1,
        "content": "Correct - D"
      },
      {
        "date": "2023-01-23T18:29:00.000Z",
        "voteCount": 1,
        "content": "I believe this was also on the GCP PCA exam as well! ;)"
      },
      {
        "date": "2022-12-31T09:52:00.000Z",
        "voteCount": 1,
        "content": "D. Create an application that publishes events to Cloud Pub/Sub, and create a Cloud Dataflow pipeline that transforms the JSON event payloads to Avro, writing the data to Cloud Storage and BigQuery."
      },
      {
        "date": "2022-12-02T07:27:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-11-13T03:00:00.000Z",
        "voteCount": 1,
        "content": "For sure D"
      },
      {
        "date": "2022-09-22T13:23:00.000Z",
        "voteCount": 1,
        "content": "D it is!"
      },
      {
        "date": "2022-02-21T11:08:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      },
      {
        "date": "2022-01-09T09:25:00.000Z",
        "voteCount": 4,
        "content": "D:\nCloud Pub/Sub, Cloud Dataflow, Cloud Storage, BigQuery https://cloud.google.com/solutions/stream-analytics/"
      },
      {
        "date": "2022-01-08T06:03:00.000Z",
        "voteCount": 1,
        "content": "OMG  only D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 136,
    "url": "https://www.examtopics.com/discussions/google/view/16932-exam-professional-data-engineer-topic-1-question-136/",
    "body": "You are running a pipeline in Dataflow that receives messages from a Pub/Sub topic and writes the results to a BigQuery dataset in the EU. Currently, your pipeline is located in europe-west4 and has a maximum of 3 workers, instance type n1-standard-1. You notice that during peak periods, your pipeline is struggling to process records in a timely fashion, when all 3 workers are at maximum CPU utilization. Which two actions can you take to increase performance of your pipeline? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of max workers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a larger instance type for your Dataflow workers\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the zone of your Dataflow pipeline to run in us-central1",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary table in Bigtable that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Bigtable to BigQuery",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a temporary table in Cloud Spanner that will act as a buffer for new data. Create a new step in your pipeline to write to this table first, and then create a new pipeline to write from Cloud Spanner to BigQuery"
    ],
    "answer": "AB",
    "answerDescription": "",
    "votes": [
      {
        "answer": "AB",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-18T08:39:00.000Z",
        "voteCount": 50,
        "content": "A &amp;  B\ninstance n1-standard-1 is low configuration and hence need to be larger configuration, definitely B should be one of the option.\nIncrease max workers will increase parallelism and hence will be able to process faster given larger CPU size and multi core processor instance type is chosen. Option A can be a better step."
      },
      {
        "date": "2022-12-31T09:58:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2021-07-04T02:39:00.000Z",
        "voteCount": 14,
        "content": "A &amp; B.\n\nWith autoscaling enabled, the Dataflow service does not allow user control of the exact number of worker instances allocated to your job. You might still cap the number of workers by specifying the --max_num_workers option when you run your pipeline. Here as per question CAP is 3, So we can change that CAP.\n\nFor batch jobs, the default machine type is n1-standard-1. For streaming jobs, the default machine type for Streaming Engine-enabled jobs is n1-standard-2 and the default machine type for non-Streaming Engine jobs is n1-standard-4. When using the default machine types, the Dataflow service can therefore allocate up to 4000 cores per job. If you need more cores for your job, you can select a larger machine type."
      },
      {
        "date": "2024-02-20T22:48:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B is correct"
      },
      {
        "date": "2023-10-01T20:03:00.000Z",
        "voteCount": 1,
        "content": "A &amp; B is correct"
      },
      {
        "date": "2023-09-24T15:55:00.000Z",
        "voteCount": 1,
        "content": "A because more workers improves performance through parallel work\nB because the current instance size is too small"
      },
      {
        "date": "2023-09-23T20:27:00.000Z",
        "voteCount": 3,
        "content": "A. Increase the number of max workers:\nBy increasing the number of maximum workers, you allow Dataflow to allocate more computing resources to handle the peak load of incoming data. This can help improve processing speed and reduce CPU utilization per worker.\n\nB. Use a larger instance type for your Dataflow workers:\nUsing a larger instance type with more CPU and memory resources can help your Dataflow workers handle a higher volume of data and processing tasks more efficiently. It can address CPU bottlenecks during peak periods."
      },
      {
        "date": "2022-12-02T07:26:00.000Z",
        "voteCount": 1,
        "content": "AB is the answer."
      },
      {
        "date": "2022-11-13T03:05:00.000Z",
        "voteCount": 1,
        "content": "Scale in and Scale Out"
      },
      {
        "date": "2022-06-07T06:18:00.000Z",
        "voteCount": 2,
        "content": "maximum of 3 workers:  Increase the number of max workers (A)\ninstance type n1-standard-1:  Use a larger instance type for your Cloud Dataflow workers (B)"
      },
      {
        "date": "2022-01-09T10:19:00.000Z",
        "voteCount": 4,
        "content": "A &amp; B, other options don't make sense"
      },
      {
        "date": "2022-01-08T06:05:00.000Z",
        "voteCount": 2,
        "content": "Only A &amp; B make sense for improving pipeline performance."
      },
      {
        "date": "2021-12-27T10:23:00.000Z",
        "voteCount": 2,
        "content": "Should be A &amp; B"
      },
      {
        "date": "2021-02-18T22:05:00.000Z",
        "voteCount": 2,
        "content": "B, E:\nB: Dataflow manage number of worker automatically, then we only can define machine type worker.\nhttps://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline\nE: and adding a horizontally scale-able database like cloud spanner will reduce pressure on dataflow as it don't have to move data to specific zone and can be remain in same zone of EU, then E is correct."
      },
      {
        "date": "2021-05-28T04:07:00.000Z",
        "voteCount": 3,
        "content": "A &amp; B is the right answer: You can set disable auto-scaling by setting the option --numWorkers (default is 3) and select the machine type by setting --workerMachineType at the time of creation of the pipeline (this applies to both auto and manual scaling)"
      },
      {
        "date": "2020-11-18T08:25:00.000Z",
        "voteCount": 3,
        "content": "Dataset is in EU so data can't be moved outside EU due to privacy law so zone option is ruled out. AB is Ok but intermediate table will boost perf apanee ruled out not sure of bigtable"
      },
      {
        "date": "2020-11-11T03:12:00.000Z",
        "voteCount": 3,
        "content": "Option A and B for sure, \nOption C : Changing Zone has nothing to do in improving performance \nOption D and E : Adding BQ and BT is waste of many and does not solve the purpose of the question."
      },
      {
        "date": "2020-09-23T21:44:00.000Z",
        "voteCount": 2,
        "content": "B &amp; D\nDF will automatically take care of increasing workers. Developers won't need to access the settings . https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline#autoscaling"
      },
      {
        "date": "2020-09-23T21:49:00.000Z",
        "voteCount": 2,
        "content": "On second thought, A B is looking right"
      },
      {
        "date": "2021-07-04T02:36:00.000Z",
        "voteCount": 1,
        "content": "automatically taking care of workers up to 3 (as the maximum worker is 3 set as per questions)"
      },
      {
        "date": "2020-08-27T08:51:00.000Z",
        "voteCount": 2,
        "content": "AB \nis correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 137,
    "url": "https://www.examtopics.com/discussions/google/view/79674-exam-professional-data-engineer-topic-1-question-137/",
    "body": "You have a data pipeline with a Dataflow job that aggregates and writes time series metrics to Bigtable. You notice that data is slow to update in Bigtable. This data feeds a dashboard used by thousands of users across the organization. You need to support additional concurrent users and reduce the amount of time required to write the data. Which two actions should you take? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure your Dataflow pipeline to use local execution",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the number of nodes in the Bigtable cluster\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your Dataflow pipeline to use the Flatten transform before writing to Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify your Dataflow pipeline to use the CoGroupByKey transform before writing to Bigtable"
    ],
    "answer": "BC",
    "answerDescription": "",
    "votes": [
      {
        "answer": "BC",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "CD",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "BE",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-06T04:10:00.000Z",
        "voteCount": 9,
        "content": "It should be B and C"
      },
      {
        "date": "2022-09-02T22:36:00.000Z",
        "voteCount": 7,
        "content": "BC is correct\n\nWhy the comments is deleted?"
      },
      {
        "date": "2024-09-23T15:44:00.000Z",
        "voteCount": 1,
        "content": "the goal is to reduce the write latency not to improve data flow code"
      },
      {
        "date": "2023-11-13T13:03:00.000Z",
        "voteCount": 2,
        "content": "The \"Correct Answers\" are just put in with a random generator :-)  B and C"
      },
      {
        "date": "2023-10-26T05:32:00.000Z",
        "voteCount": 4,
        "content": "B - opportunity to parallelise the process\nC - increase throughput"
      },
      {
        "date": "2023-10-02T01:21:00.000Z",
        "voteCount": 1,
        "content": "Exactly opposite answers in the discussions"
      },
      {
        "date": "2023-09-23T20:34:00.000Z",
        "voteCount": 4,
        "content": "B. Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions:\nIncreasing the number of Dataflow workers can help parallelize the processing of your data, which can result in faster data updates to Bigtable and improved concurrency. You can set maxNumWorkers to a higher value to achieve this.\n\nC. Increase the number of nodes in the Bigtable cluster:\nIncreasing the number of nodes in your Bigtable cluster can improve the overall throughput and reduce latency when writing data. It allows Bigtable to handle a higher rate of data ingestion and queries, which is essential for supporting additional concurrent users."
      },
      {
        "date": "2023-09-20T04:17:00.000Z",
        "voteCount": 1,
        "content": "C definetely is correct, as it improves the read and write performance of Bigtable. \n\nHowever, I do think that the second option is actually D instead of B, because the question specifically states that the pipeline aggregates data. Flatten merges multiple PCollection objects into a single logical PCollection, allowing for faster aggregation of time series data."
      },
      {
        "date": "2023-08-04T09:22:00.000Z",
        "voteCount": 1,
        "content": "B - I believe it is consensus.\nD - The question mentions \"a Dataflow job that \"aggregates\" and writes time series metrics to Bigtable\". So CoGroupByKey performs a shuffle (grouping) operation to distribute data across workers.\n\nhttps://cloud.google.com/dataflow/docs/guides/develop-and-test-pipelines"
      },
      {
        "date": "2023-06-07T12:01:00.000Z",
        "voteCount": 1,
        "content": "I read this question as: BigTable Write operations are all over the place (key-wise), and BigTable doesn't like that. When creating groups (batch writes), of similar keys (close to each other), BigTable is happy again, which I loosely translate into DE."
      },
      {
        "date": "2023-05-04T08:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct. But I don't see how you increase the write throughput of Bigtable increasing its cluster size. It should be dataflow instance resources that have to be increased"
      },
      {
        "date": "2023-03-21T09:10:00.000Z",
        "voteCount": 1,
        "content": "BC make sense"
      },
      {
        "date": "2023-01-29T03:51:00.000Z",
        "voteCount": 1,
        "content": "BC only makes sense here  , no mention of data, no mention of keeping cost low"
      },
      {
        "date": "2022-12-31T09:59:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the maximum number of Dataflow workers by setting maxNumWorkers in PipelineOptions Most Voted\nC. Increase the number of nodes in the Bigtable cluster"
      },
      {
        "date": "2022-11-22T22:29:00.000Z",
        "voteCount": 2,
        "content": "Increase max num of workers increases pipeline performance in Dataflow\nIncrease number of nodes in Bigtable increases write throughput"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 138,
    "url": "https://www.examtopics.com/discussions/google/view/79675-exam-professional-data-engineer-topic-1-question-138/",
    "body": "You have several Spark jobs that run on a Cloud Dataproc cluster on a schedule. Some of the jobs run in sequence, and some of the jobs run concurrently. You need to automate this process. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Dataproc Workflow Template",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an initialization action to execute the jobs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Directed Acyclic Graph in Cloud Composer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Bash script that uses the Cloud SDK to create a cluster, execute jobs, and then tear down the cluster"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-18T01:21:00.000Z",
        "voteCount": 3,
        "content": "The best option for automating your scheduled Spark jobs on Cloud Dataproc, considering sequential and concurrent execution, is:\nC. Create a Directed Acyclic Graph (DAG) in Cloud Composer."
      },
      {
        "date": "2023-12-18T01:22:00.000Z",
        "voteCount": 2,
        "content": "Here's why:\nDAG workflows: Cloud Composer excels at orchestrating complex workflows with dependencies, making it ideal for managing sequential and concurrent execution of your Spark jobs. You can define dependencies between tasks to ensure certain jobs only run after others finish.\nAutomation: Cloud Composer lets you schedule workflows to run automatically based on triggers like time intervals or data availability, eliminating the need for manual intervention.\nIntegration: Cloud Composer integrates seamlessly with Cloud Dataproc, allowing you to easily launch and manage your Spark clusters within the workflow.\nScalability: Cloud Composer scales well to handle a large number of jobs and workflows, making it suitable for managing complex data pipelines."
      },
      {
        "date": "2023-12-18T01:22:00.000Z",
        "voteCount": 2,
        "content": "While the other options have some merit, they fall short in certain aspects:\nA. Cloud Dataproc Workflow Templates: While workflow templates can automate job submission on a cluster, they lack the ability to define dependencies and coordinate concurrent execution effectively.\nB. Initialization action: An initialization action can only run a single script before a Dataproc cluster starts, not suitable for orchestrating multiple scheduled jobs with dependencies.\nD. Bash script: A Bash script might work for simple cases, but it can be cumbersome to manage and lacks the advanced scheduling and error handling capabilities of Cloud Composer.\nTherefore, utilizing a Cloud Composer DAG offers the most comprehensive and flexible solution for automating your scheduled Spark jobs with sequential and concurrent execution on Cloud Dataproc."
      },
      {
        "date": "2023-11-20T12:34:00.000Z",
        "voteCount": 1,
        "content": "I thought it might be A but the templates can only run sequentially, not concurrently."
      },
      {
        "date": "2023-09-23T20:37:00.000Z",
        "voteCount": 1,
        "content": "Directed Acyclic Graph (DAG): Cloud Composer (formerly known as Cloud Composer) is a managed Apache Airflow service that allows you to create and manage workflows as DAGs. You can define a DAG that includes tasks for running Spark jobs in sequence or concurrently.\n\nScheduling: Cloud Composer provides built-in scheduling capabilities, allowing you to specify when and how often your DAGs should run. You can schedule the execution of your Spark jobs at specific times or intervals.\n\nDependency Management: In a DAG, you can define dependencies between tasks. This means you can set up tasks to run sequentially or concurrently based on your requirements. For example, you can specify that Job B runs after Job A has completed, or you can schedule jobs to run concurrently when there are no dependencies."
      },
      {
        "date": "2023-03-13T00:37:00.000Z",
        "voteCount": 2,
        "content": "I would choose A if there was one more step to schedule the Template. It is like creating DAG without running it in Airflow.\nSo only option C is correct here."
      },
      {
        "date": "2022-12-31T10:00:00.000Z",
        "voteCount": 3,
        "content": "C. Create a Directed Acyclic Graph in Cloud Composer"
      },
      {
        "date": "2022-12-19T04:50:00.000Z",
        "voteCount": 2,
        "content": "Why go for an expensive Composer when you only have to schedule and create a DAG for Dataproc, A is sufficient."
      },
      {
        "date": "2023-01-04T04:54:00.000Z",
        "voteCount": 5,
        "content": "I've would've gone for Workflow Templates as well. But those are lacking the scheduling capability. Hence you would need to use Cloud Composer (or Cloud Functions or Cloud Scheduler) anyway. Hence C seems to be the better solution.\n\nPls see here:\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions"
      },
      {
        "date": "2022-12-02T07:20:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions#cloud_composer\nCloud Composer is a managed Apache Airflow service you can use to create, schedule, monitor, and manage workflows. Advantages:\n- Supports time- and event-based scheduling\n- Simplified calls to Dataproc using Operators\n- Dynamically generate workflows and workflow parameters\n- Build data flows that span multiple Google Cloud products"
      },
      {
        "date": "2022-10-16T14:34:00.000Z",
        "voteCount": 4,
        "content": "C.\nComposer fits better to schedule Dataproc Workflows, check the documentation:\nhttps://cloud.google.com/dataproc/docs/concepts/workflows/workflow-schedule-solutions\n\nAlso A is not enough. Dataproc Workflow Template itself don't has a native schedule option."
      },
      {
        "date": "2022-10-16T00:13:00.000Z",
        "voteCount": 1,
        "content": "So that I thing the answer should be C (Composer)."
      },
      {
        "date": "2022-10-16T00:12:00.000Z",
        "voteCount": 2,
        "content": "To me, the point is \"automate\" the process, so that Composer DAG is needed and can be used with Dataproc Workflow Template."
      },
      {
        "date": "2022-10-09T10:59:00.000Z",
        "voteCount": 2,
        "content": "Ans A makes more sense, since a question is regarding Dataproc jobs only"
      },
      {
        "date": "2022-10-02T14:25:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A.  https://cloud.google.com/dataproc/docs/concepts/workflows/using-workflows"
      },
      {
        "date": "2022-09-07T22:01:00.000Z",
        "voteCount": 1,
        "content": "Option c"
      },
      {
        "date": "2022-09-02T22:39:00.000Z",
        "voteCount": 1,
        "content": "You have streaming and batch job, so Composer is the choice for me"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 139,
    "url": "https://www.examtopics.com/discussions/google/view/79676-exam-professional-data-engineer-topic-1-question-139/",
    "body": "You are building a new data pipeline to share data between two different types of applications: jobs generators and job runners. Your solution must scale to accommodate increases in usage and must accommodate the addition of new applications without negatively affecting the performance of existing ones. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an API using App Engine to receive and send messages to the applications",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table on Cloud SQL, and insert and delete rows with the job information",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table on Cloud Spanner, and insert and delete rows with the job information"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-07T08:52:00.000Z",
        "voteCount": 9,
        "content": "Job generators (they would be the publishers).\nJob runners = subscribers\n\nQuestion mentions that it must scale (of which push subscription has automatic scaling) and can accommodate additional new applications (this can be solved by having multiple subscriptions, with each relating to a unique application) to a central topic"
      },
      {
        "date": "2022-12-31T10:02:00.000Z",
        "voteCount": 4,
        "content": "Yes it is \nB. Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them"
      },
      {
        "date": "2024-02-10T05:47:00.000Z",
        "voteCount": 1,
        "content": "A. App Engine API: While scalable, it introduces a central point of failure and might not be as performant as Pub/Sub for high-volume data.\nC. Cloud SQL: Not designed for real-time data sharing and continuous updates, leading to potential bottlenecks and performance issues.\nD. Cloud Spanner: Offers strong consistency and global distribution, but its pricing model might be less suitable for high-volume, cost-sensitive workloads compared to Pub/Sub."
      },
      {
        "date": "2023-09-24T16:14:00.000Z",
        "voteCount": 1,
        "content": "B to decouple jobs being generated and run. Pub/Sub also scales seamlessly"
      },
      {
        "date": "2023-09-23T20:39:00.000Z",
        "voteCount": 2,
        "content": "B. Use a Cloud Pub/Sub topic to publish jobs, and use subscriptions to execute them.\n\nScalability: Cloud Pub/Sub is a highly scalable messaging service that can handle a significant volume of messages and subscribers. It can easily accommodate increases in usage as your data pipeline scales.\n\nDecoupling: Using Pub/Sub decouples the job generators from the job runners, which is a good architectural choice for flexibility and scalability. Job generators publish messages to a topic, and job runners subscribe to that topic to execute jobs when they are available.\n\nAdding New Applications: With Cloud Pub/Sub, adding new applications (new publishers or subscribers) is straightforward. You can simply create new publishers to send jobs or new subscribers to consume jobs without impacting existing components."
      },
      {
        "date": "2023-02-17T13:43:00.000Z",
        "voteCount": 2,
        "content": "key words here: job generators (pushlish message on pub/sub) and job runners(subscribe message for further analysis). You may add as much as pushlishing job and subscribing job to same topic. So Answer B.\nUsing API , app engine is also good approach but its more complex than pub/sub."
      },
      {
        "date": "2022-12-01T06:19:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2022-11-12T10:32:00.000Z",
        "voteCount": 1,
        "content": "A\nSince it's application i will go with"
      },
      {
        "date": "2022-09-06T04:14:00.000Z",
        "voteCount": 3,
        "content": "use pubsub"
      },
      {
        "date": "2022-09-05T03:56:00.000Z",
        "voteCount": 2,
        "content": "I would tend to think B , one of the use of pub/sub is decoupling app"
      },
      {
        "date": "2022-09-02T22:40:00.000Z",
        "voteCount": 2,
        "content": "I choose B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 140,
    "url": "https://www.examtopics.com/discussions/google/view/80270-exam-professional-data-engineer-topic-1-question-140/",
    "body": "You need to create a new transaction table in Cloud Spanner that stores product sales data. You are deciding what to use as a primary key. From a performance perspective, which strategy should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe current epoch time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA concatenation of the product name and the current epoch time",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tA random universally unique identifier number (version 4 UUID)\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe original order identification number from the sales system, which is a monotonically increasing integer"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-13T07:46:00.000Z",
        "voteCount": 9,
        "content": "According to the documentation:\nUse a Universally Unique Identifier (UUID)\nYou can use a Universally Unique Identifier (UUID) as defined by RFC 4122 as the primary key. Version 4 UUID is recommended, because it uses random values in the bit sequence. Version 1 UUID stores the timestamp in the high order bits and is not recommended.\n\nhttps://cloud.google.com/spanner/docs/schema-design"
      },
      {
        "date": "2022-12-31T10:03:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2023-09-23T20:43:00.000Z",
        "voteCount": 4,
        "content": "For a transaction table in Cloud Spanner that stores product sales data, from a performance perspective, it is generally recommended to choose a primary key that allows for even distribution of data across nodes and minimizes hotspots. Therefore, option C, which suggests using a random universally unique identifier number (version 4 UUID), is the preferred choice."
      },
      {
        "date": "2023-08-20T00:43:00.000Z",
        "voteCount": 1,
        "content": "For a RDB I would choice D.\n\nBut for Google Spanner, Google says: \nhttps://cloud.google.com/spanner/docs/schema-and-data-model#:~:text=monotonically%20increasing%20integer"
      },
      {
        "date": "2023-05-09T08:08:00.000Z",
        "voteCount": 2,
        "content": "B might work if you say timestamp instead than epoch. PK of sales should contain the exact purchase date or timestamp, not the time when the transaction was processed. I personally associate the term epoch in this context to the process timestamp instead than the purchase timestamp."
      },
      {
        "date": "2023-03-13T00:42:00.000Z",
        "voteCount": 2,
        "content": "B may cause error if same product ID came at the same time (same id + same epoch)\nSo C is the correct answer here"
      },
      {
        "date": "2024-05-23T17:54:00.000Z",
        "voteCount": 1,
        "content": "Agreed. Additionally, using the product name can lead to unbalanced distribution if some products are sold more frequently than others."
      },
      {
        "date": "2022-12-20T05:09:00.000Z",
        "voteCount": 2,
        "content": "A and D are invalid because they monotonically increases.\nB would work, but in terms of pure performance UUID 4 is the fastest because it virtually will not cause hotspots"
      },
      {
        "date": "2022-12-08T05:33:00.000Z",
        "voteCount": 2,
        "content": "A and D are not valid, because they monotonically increase.\nC avoid hotspots for sure, but It's nor relate with querys. So for writing performance it's perfect that the reason for chose this: &nbsp;\u201cYou need to create a new transaction table in Cloud Spanner that stores product sales data\u201d. They only ask you to store product data, its a writing ops.\nIf the question had spoken about query the info or hard performance read, the best option would be B, because it has the balance of writing/reading best practices.\nThere are a few disadvantages to using a UUID:\n\n    They are slightly large, using 16 bytes or more. Other options for primary keys don't use this much storage.\n    They carry no information about the record. For example, a primary key of SingerId and AlbumId has an inherent meaning, while a UUID does not.\n    You lose locality between records that are related, which is why using a UUID eliminates hotspots.\n\n\nhttps://cloud.google.com/spanner/docs/schema-design#uuid_primary_key"
      },
      {
        "date": "2022-09-05T04:09:00.000Z",
        "voteCount": 1,
        "content": "C. A random universally unique identifier number (version 4 UUID)\n\nFrom https://cloud.google.com/spanner/docs/schema-and-data-model\n\n\nThere are techniques that can spread the load across multiple servers and avoid hotspots:\n\nHash the key and store it in a column. Use the hash column (or the hash column and the unique key columns together) as the primary key.\nSwap the order of the columns in the primary key.\nUse a Universally Unique Identifier (UUID). Version 4 UUID is recommended, because it uses random values in the high-order bits. Don't use a UUID algorithm (such as version 1 UUID) that stores the timestamp in the high order bits.\nBit-reverse sequential values."
      },
      {
        "date": "2022-09-05T02:23:00.000Z",
        "voteCount": 2,
        "content": "Answer should be B as in all the other options hotspotting is possible. According to proper schema design guideline.. \nSchema design best practice #1: Do not choose a column whose value monotonically increases or decreases as the first key part for a high write rate table.\n\nSupporting link: \nhttps://cloud.google.com/spanner/docs/schema-design#primary-key-prevent-hotspots"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 141,
    "url": "https://www.examtopics.com/discussions/google/view/17225-exam-professional-data-engineer-topic-1-question-141/",
    "body": "Data Analysts in your company have the Cloud IAM Owner role assigned to them in their projects to allow them to work with multiple GCP products in their projects. Your organization requires that all BigQuery data access logs be retained for 6 months. You need to ensure that only audit personnel in your company can access the data access logs for all projects. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable data access logs in each Data Analyst's project. Restrict access to Stackdriver Logging via Cloud IAM roles.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data access logs via a project-level export sink to a Cloud Storage bucket in the Data Analysts' projects. Restrict access to the Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data access logs via a project-level export sink to a Cloud Storage bucket in a newly created projects for audit logs. Restrict access to the project with the exported logs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data access logs via an aggregated export sink to a Cloud Storage bucket in a newly created project for audit logs. Restrict access to the project that contains the exported logs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-09-24T04:40:00.000Z",
        "voteCount": 29,
        "content": "Answer D is correct. Aggregated log sink will create a single sink for all projects, the destination can be a google cloud storage, pub/sub topic, bigquery table or a cloud logging bucket. without aggregated sink this will be required to be done for each project individually which will be cumbersome.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks"
      },
      {
        "date": "2022-12-31T10:05:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2020-03-25T07:04:00.000Z",
        "voteCount": 12,
        "content": "Correct: D\nhttps://cloud.google.com/iam/docs/roles-audit-logging#scenario_external_auditors"
      },
      {
        "date": "2020-07-12T22:57:00.000Z",
        "voteCount": 3,
        "content": "The above link shows BigQuery as a sink for aggregated exports and not Cloud Storage."
      },
      {
        "date": "2021-03-12T14:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/iam/docs/job-functions/auditing#scenario_operational_monitoring"
      },
      {
        "date": "2024-03-26T00:03:00.000Z",
        "voteCount": 1,
        "content": "D is the correct ans"
      },
      {
        "date": "2023-09-23T20:47:00.000Z",
        "voteCount": 1,
        "content": "D. \nHere's why this option is recommended:\nAggregated Export Sink: By using an aggregated export sink, you can consolidate data access logs from multiple projects into a single location. This simplifies log management and retention policies.\nNewly Created Project for Audit Logs: Creating a dedicated project for audit logs allows you to centralize access control and manage logs separately from individual Data Analyst projects.\nAccess Restriction: By restricting access to the project containing the exported logs, you ensure that only authorized audit personnel have access to the logs while preventing Data Analysts from accessing them."
      },
      {
        "date": "2023-03-13T00:52:00.000Z",
        "voteCount": 1,
        "content": "To create the Log Router, at step 3 to define the logs (Source), we can include logs from many projects (aggregated)"
      },
      {
        "date": "2022-12-01T06:14:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nAggregated sinks combine and route log entries from the Google Cloud resources contained by an organization or folder. For instance, you might aggregate and route audit log entries from all the folders contained by an organization to a Cloud Storage bucket."
      },
      {
        "date": "2022-04-27T21:29:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-01-09T11:09:00.000Z",
        "voteCount": 4,
        "content": "D: https://cloud.google.com/logging/docs/export/aggregated_exports\nYou can create an aggregated export sink that can export log entries from all the projects, folders, and billing accounts of an organization. As an example, you might use this feature to export audit log entries from an organization's projects to a central location."
      },
      {
        "date": "2021-09-30T17:35:00.000Z",
        "voteCount": 4,
        "content": "The auditor needs to audit data analyst's behaviors (how they access multiple projects in BQ ). So, the key is, multiple projects. According to Google doc project-level sinks:\nhttps://cloud.google.com/logging/docs/export/configure_export_v2\nHowever, the Cloud Console can only create or view sinks in Cloud projects. To create sinks in organizations, folders, or billing accounts using the gcloud command-line tool or Cloud Logging API, see Aggregated sinks.\n\nObviously, the auditor needs to check all projects accessed by data analyst which is not project-level, a higher level like folder or organization level, this can only be done via the aggregate sink.\n\nSo D is the answer."
      },
      {
        "date": "2021-07-04T05:40:00.000Z",
        "voteCount": 6,
        "content": "A - eliminated , because logs needs to be retained for 6 months (So, some storage require)\nB - eliminated, because if we store in same project then, Data Analyst can also access (But in question it's mention, ONLY audit personnel needs access)\nC - Wrong (No need to restrict project as well as logs separately) - wording does not look okay.\nD - Correct (If we restrict the project, then all resources get restricted)\n\nVote for D"
      },
      {
        "date": "2021-07-04T07:06:00.000Z",
        "voteCount": 2,
        "content": "Option 'C'  - I guess said - restrict access to the project with the exported logs. (i.e. restrict access of that project from where we took logs) - If I am not wrong... Thus it's INCORRECT"
      },
      {
        "date": "2022-01-26T11:44:00.000Z",
        "voteCount": 1,
        "content": "Sinks are different from Aggregate Sinks, refer https://cloud.google.com/logging/docs/export/configure_export_v2#api"
      },
      {
        "date": "2021-04-22T05:25:00.000Z",
        "voteCount": 3,
        "content": "what is the difference between C and D? I think it's same."
      },
      {
        "date": "2023-08-16T10:14:00.000Z",
        "voteCount": 1,
        "content": "I think the key difference is that D talks about aggregated sinks."
      },
      {
        "date": "2020-08-22T05:17:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      },
      {
        "date": "2020-08-18T09:14:00.000Z",
        "voteCount": 3,
        "content": "D is correct answer, refer below link for more information."
      },
      {
        "date": "2020-07-11T08:31:00.000Z",
        "voteCount": 5,
        "content": "Ans : D\n Aggregated Exports, which allows you to set up a sink at the Cloud IAM organization or folder level, and export logs from all the projects inside the organization or folder."
      },
      {
        "date": "2020-03-22T00:58:00.000Z",
        "voteCount": 4,
        "content": "Answer D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 142,
    "url": "https://www.examtopics.com/discussions/google/view/81914-exam-professional-data-engineer-topic-1-question-142/",
    "body": "Each analytics team in your organization is running BigQuery jobs in their own projects. You want to enable each team to monitor slot usage within their projects.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring dashboard based on the BigQuery metric query/scanned_bytes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a log export for each project, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an aggregated log export at the organization level, capture the BigQuery job execution logs, create a custom metric based on the totalSlotMs, and create a Cloud Monitoring dashboard based on the custom metric"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-26T00:04:00.000Z",
        "voteCount": 1,
        "content": "B is correct answer"
      },
      {
        "date": "2023-12-18T07:41:00.000Z",
        "voteCount": 1,
        "content": "Viewing project and reservation slot usage in Stackdriver Monitoring\nInformation is available from the \"Slots Allocated\" metric in Stackdriver Monitoring. This metric information includes a per-reservation and per-job breakdown of slot usage. The information can also be visualized by using the custom charts metric explorer.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring\nhttps://cloud.google.com/monitoring/api/metrics_gcp"
      },
      {
        "date": "2023-09-23T20:51:00.000Z",
        "voteCount": 3,
        "content": "The slots/allocated_for_project metric provides information about the number of slots allocated to each project. It directly reflects the slot usage, making it a relevant and accurate metric for monitoring slot allocation within each project.\n\nOptions A, C, and D involve log exports and custom metrics, but they may not be as straightforward or provide the same level of detail as the built-in metric slots/allocated_for_project:"
      },
      {
        "date": "2023-09-20T04:43:00.000Z",
        "voteCount": 2,
        "content": "The naming is quite misleading in this case, but it actually seems from the documentation that slots/allocated_for_project indicates the \"slots used by project\", in which case answer B is correct: https://cloud.google.com/monitoring/api/metrics_gcp#:~:text=slots/allocated_for_project%20GA%0ASlots%20used%20by%20project"
      },
      {
        "date": "2023-08-20T02:07:00.000Z",
        "voteCount": 1,
        "content": "B slots/allocated_for_project will give you the total number of slots allocated to each project, but it will not tell you how many slots are actually being used.\n\nThe purpose to monitor 'slot usgae' is for billing. 'slot/allocated' means nothing.\nOption D is better than B.\n\nAnd, the question mention 'Each analytics team in organization', so it should be 'organization level'."
      },
      {
        "date": "2023-03-23T21:25:00.000Z",
        "voteCount": 1,
        "content": "If 'usage' = how the slots are being used, D is the corret answer\nIf 'usage' = how the slots are being allocated, B is the correct answer\n\nI think in this question, usage = how the slots are being used"
      },
      {
        "date": "2023-02-17T14:35:00.000Z",
        "voteCount": 1,
        "content": "Answer B, \nWhy not D, aggregated log export is good but it will generate all the details which is large in size and costly too. you dont need all the information. It can break data privacy. so look for B because this much is asked only. Normally, i make such errors alot."
      },
      {
        "date": "2022-12-20T21:21:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is B. You should create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project.\n\nThis metric represents the number of BigQuery slots allocated for a project. By creating a Cloud Monitoring dashboard based on this metric, you can monitor the slot usage within each project in your organization. This will allow each team to monitor their own slot usage and ensure that they are not exceeding their allocated quota.\n\nOption A is incorrect because the query/scanned_bytes metric represents the number of bytes scanned by BigQuery queries, not the slot usage.\n\nOption C is incorrect because it involves creating a log export for each project and using a custom metric based on the totalSlotMs field. While this may be a valid way to monitor slot usage, it is more complex than simply using the slots/allocated_for_project metric.\n\nOption D is also incorrect because it involves creating an aggregated log export at the organization level, which is not necessary for monitoring slot usage within individual projects."
      },
      {
        "date": "2022-09-22T04:48:00.000Z",
        "voteCount": 2,
        "content": "vote for B"
      },
      {
        "date": "2022-09-12T20:49:00.000Z",
        "voteCount": 4,
        "content": "B ,the another is related to the question as well.\nhttps://cloud.google.com/bigquery/docs/reservations-monitoring#viewing-slot-usage"
      },
      {
        "date": "2022-09-12T20:43:00.000Z",
        "voteCount": 4,
        "content": "B    the below is related to the question.\nhttps://cloud.google.com/blog/topics/developers-practitioners/monitoring-bigquery-reservations-and-slot-utilization-information_schema"
      },
      {
        "date": "2022-12-31T10:06:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Cloud Monitoring dashboard based on the BigQuery metric slots/allocated_for_project"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 143,
    "url": "https://www.examtopics.com/discussions/google/view/79678-exam-professional-data-engineer-topic-1-question-143/",
    "body": "You are operating a streaming Cloud Dataflow pipeline. Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. You want to update the running pipeline with the new version. You want to ensure that no data is lost during the update. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing job name",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to a new unique job name",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the Cloud Dataflow pipeline with the Cancel option. Create a new Cloud Dataflow job with the updated code",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStop the Cloud Dataflow pipeline with the Drain option. Create a new Cloud Dataflow job with the updated code\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 43,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-08T05:48:00.000Z",
        "voteCount": 15,
        "content": "It's D. \u2192 Your engineers have a new version of the pipeline with a different windowing algorithm and triggering strategy. \nNew version is mayor changes. Stop and drain and then launch the new code is a lot is the safer way. \nWe recommend that you attempt only smaller changes to your pipeline's windowing, such as changing the duration of fixed- or sliding-time windows. Making major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing"
      },
      {
        "date": "2022-12-14T11:41:00.000Z",
        "voteCount": 2,
        "content": "Since updating the job as in A does a compatibility check, wouldn't you want to try that first? Then if the compatibility check fails then you proceed to drain current pipeline and then launch new pipeline (Answer D)?\n\nAs in A would be correct answer, then if compatibility check fails, you proceed to D. \n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#CCheck"
      },
      {
        "date": "2023-09-21T07:18:00.000Z",
        "voteCount": 1,
        "content": "You're right in your reasoning, but since the documentation specifically uses this example for stopping and draining, it's safe to assume that the compatibility check will always fail with these adjustments. Therefore, we can go straight to D. \n\nFurthermore, answer A doesn't state: \"Update the Cloud Dataflow pipeline inflight by passing the --update option with the --jobName set to the existing name, if the compatibility check fails, THEN proceed to stopping the pipeline with the drain option\", so in itself it is not the right answer if the check fails."
      },
      {
        "date": "2023-12-30T06:01:00.000Z",
        "voteCount": 1,
        "content": "D seems the right way to go"
      },
      {
        "date": "2023-12-23T23:33:00.000Z",
        "voteCount": 1,
        "content": "Option A is the first approach to try, as it allows for an in-flight update with minimal disruption. However, if the changes in the new version of the pipeline are not compatible with an in-flight update (due to significant changes in windowing or triggering), then option D should be used. The Drain option ensures a graceful shutdown of the existing pipeline, reducing the risk of data loss, and then a new job can be started with the updated code."
      },
      {
        "date": "2023-12-18T08:40:00.000Z",
        "voteCount": 1,
        "content": "A is not an option as \"You want to ensure that no data is lost during the update. \":  \nMaking major changes to windowing or triggers, like changing the windowing algorithm, might have unpredictable results on your pipeline output.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#change_windowing"
      },
      {
        "date": "2023-09-23T21:06:00.000Z",
        "voteCount": 1,
        "content": "Drain Option: The \"Drain\" option allows the existing Dataflow job to complete processing of any in-flight data before stopping the job. This ensures that no data is lost during the transition to the new version.\nCreate a New Job: After draining the existing job, you create a new Cloud Dataflow job with the updated code. This new job starts fresh and continues processing data from where the old job left off.\n\nOption A (updating the inflight pipeline with the --update option) may not guarantee no data loss, as the update could disrupt the existing job's operation and potentially cause data loss.\n\nOption B (updating the inflight pipeline with the --update option and a new job name) is similar to option A and may not provide data loss guarantees.\n\nOption C (stopping the pipeline with the Cancel option and creating a new job) will abruptly stop the existing job without draining, potentially leading to data loss."
      },
      {
        "date": "2023-07-26T05:28:00.000Z",
        "voteCount": 1,
        "content": "Look D after seeing some docs. please check the below link https://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline"
      },
      {
        "date": "2023-07-25T21:11:00.000Z",
        "voteCount": 1,
        "content": "I will go with option D - If you want to minimize the impact of the update, then option A is the best option. However, if you are not concerned about a temporary interruption in processing, then option D is also a valid option. Option\tPros\tCons\nA\tDoes not stop the pipeline, so no data is lost.\tRequires you to create a new version of the pipeline.\nB\tCreates a new job with the updated code, so you do not have to update the running pipeline.\tCan lead to data loss if the new job does not process all of the data that was in the running pipeline.\nC\tStops the pipeline and drains any data that is currently in flight, so no data is lost.\tCauses a temporary interruption in processing."
      },
      {
        "date": "2023-03-13T01:42:00.000Z",
        "voteCount": 3,
        "content": "A is not recommeded for major changes in pipeline."
      },
      {
        "date": "2023-02-17T14:49:00.000Z",
        "voteCount": 1,
        "content": "Answer A:\n```gcloud dataflow jobs update &lt;JOB_ID&gt; --update &lt;GCS_PATH_TO_UPDATED_PIPELINE&gt; --region &lt;REGION&gt;```\n--update flag does not miss any data and you can execute this command even yourpipeline is running. Its safe any fast, you can continuously make some change and update this command. no problem.\nStop and Drain, is required when you want to test the pipeline and stop it without losing the data."
      },
      {
        "date": "2023-02-24T12:55:00.000Z",
        "voteCount": 3,
        "content": "Answer D: as per latest documents 02/2023 google has removed update flag."
      },
      {
        "date": "2022-12-22T03:52:00.000Z",
        "voteCount": 4,
        "content": "agree with odacir"
      },
      {
        "date": "2022-12-02T22:38:00.000Z",
        "voteCount": 2,
        "content": "vote A\nD: drain doesn't mention about update dataflow job just stop and preserve data\nA: replace existing job and preserve data\n(When you update your job, the Dataflow service performs a compatibility check between your currently-running job and your potential replacement job. The compatibility check ensures that things like intermediate state information and buffered data can be transferred from your prior job to your replacement job.)\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline"
      },
      {
        "date": "2022-12-01T06:08:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching\nTo update your job, launch a new job to replace the ongoing job. When you launch your replacement job, set the following pipeline options to perform the update process in addition to the job's regular options:\n- Pass the --update option.\n- Set the --jobName option in PipelineOptions to the same name as the job you want to update."
      },
      {
        "date": "2022-12-08T05:49:00.000Z",
        "voteCount": 1,
        "content": "Are mayor changes. It's not safe to update. I vote D."
      },
      {
        "date": "2022-11-25T23:12:00.000Z",
        "voteCount": 1,
        "content": "D\nA-is not because The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. \nDescription:\nWhen you update a job on the Dataflow service, you replace the existing job with a new job that runs your updated pipeline code. The Dataflow service retains the job name, but runs the replacement job with an updated Job ID. This process can cause downtime while the existing job stops, the compatibility check runs, and the new job starts.'\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python:~:text=When%20you%20update%20a,has%20the%20following%20transforms%3A\nD is correct\nDrain -&gt;clone -&gt; update -&gt; run"
      },
      {
        "date": "2022-11-25T23:32:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind to A\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#python_2:~:text=Set%20the%20%2D%2Djob_name,%2D%2Dtransform_name_mapping%20option."
      },
      {
        "date": "2022-11-24T09:20:00.000Z",
        "voteCount": 3,
        "content": "Changing windowing algorithm may break the pipeline.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing"
      },
      {
        "date": "2022-11-22T22:53:00.000Z",
        "voteCount": 1,
        "content": "No, do not drain the current job."
      },
      {
        "date": "2022-11-19T23:05:00.000Z",
        "voteCount": 1,
        "content": "in this scenario pipline is streaming pipline with windowing algorithm and triggering strategy changes to new one without loss of data,so better to go with Drain option as it fullfile all precondition described in scenario which is :-\n1.streaming\n2.code changes with windowing algorithm and triggering strategy  to new way\n3.no loss of data during update \n\nReferances:-\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#drain\nDrain a job. This method applies only to streaming pipelines. Draining a job enables the Dataflow service to finish processing the buffered data while simultaneously ceasing the ingestion of new data. For more information, see Draining a job."
      },
      {
        "date": "2022-11-19T23:08:00.000Z",
        "voteCount": 1,
        "content": "If the pipeline was batch then ans would been A"
      },
      {
        "date": "2022-11-07T08:15:00.000Z",
        "voteCount": 3,
        "content": "D: They want to preserve data and updates might not be predictable.\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#changing_windowing"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 144,
    "url": "https://www.examtopics.com/discussions/google/view/17228-exam-professional-data-engineer-topic-1-question-144/",
    "body": "You need to move 2 PB of historical data from an on-premises storage appliance to Cloud Storage within six months, and your outbound network capacity is constrained to 20 Mb/sec. How should you migrate this data to Cloud Storage?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Transfer Appliance to copy the data to Cloud Storage\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gsutil cp \u05d2\u20ac\"J to compress the content being uploaded to Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a private URL for the historical data, and then use Storage Transfer Service to copy the data to Cloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse trickle or ionice along with gsutil cp to limit the amount of bandwidth gsutil utilizes to less than 20 Mb/sec so it does not interfere with the production traffic"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T08:53:00.000Z",
        "voteCount": 26,
        "content": "Answer: A\nDescription: Huge amount of data with log network bandwidth, Transfer applicate is best for moving data over 100TB"
      },
      {
        "date": "2020-03-22T01:36:00.000Z",
        "voteCount": 10,
        "content": "Correct - A"
      },
      {
        "date": "2023-12-30T06:01:00.000Z",
        "voteCount": 1,
        "content": "A . Easy, just by the amount of data"
      },
      {
        "date": "2023-10-10T02:19:00.000Z",
        "voteCount": 1,
        "content": "In 6 months - only 0.0290304 Petabytes will be uploaded. Right - compression might help, but we do not have any info to support the ration. Lets go for A"
      },
      {
        "date": "2023-09-23T21:08:00.000Z",
        "voteCount": 1,
        "content": "Physical Transfer: Transfer Appliance is a physical device provided by Google Cloud that you can use to physically transfer large volumes of data to the cloud. It allows you to avoid the limitations of network bandwidth and transfer data much faster.\n\nCapacity: Transfer Appliance can handle large volumes of data, including the 2 PB you need to migrate, without the constraints of slow network speeds.\n\nEfficiency: It is highly efficient for large-scale data transfers and is a practical choice for transferring multi-terabyte or petabyte-scale datasets."
      },
      {
        "date": "2023-08-20T02:18:00.000Z",
        "voteCount": 2,
        "content": "it would take 34 years.\nOption A no doubt.\n\nhttps://cloud.google.com/static/architecture/images/big-data-transfer-how-to-get-started-transfer-size-and-speed.png"
      },
      {
        "date": "2023-05-09T08:43:00.000Z",
        "voteCount": 1,
        "content": "2,000,000,000,000,000 bytes = 2 Petabytes\n20,000,000 bytes = 20 Megabytes\n\nOnce we do the math: \n 2 Petabytes / 20 Megabytes = 100,000,000 seconds forecasted to migrate the data.\n\n100,000,000 seconds =\n1,666,666.7 minutes = \n27,777.8 hours = \n1,157.4 days \n\n6 months = 180 days \n\n1,157.4 days &gt; 180 days\n\nStill, with such amount Transfer Appliance is recommended."
      },
      {
        "date": "2023-02-17T15:00:00.000Z",
        "voteCount": 1,
        "content": "Transfer alliance is a physical device of size like cabin luggage or slightly larger. \nIt has Seagate/WD harddisk (these are name of companies) size varries from 100 to 480 TB. \nIn our case 2PB (2000 TB) accordingly. Google send you this and transfer data into it by wire connection and then upload data from this to GCS and empty the appliance."
      },
      {
        "date": "2022-12-29T19:38:00.000Z",
        "voteCount": 2,
        "content": "This is no brainer question, A is right"
      },
      {
        "date": "2022-12-01T06:01:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview\nTransfer Appliance is a high-capacity storage device that enables you to transfer and securely ship your data to a Google upload facility, where we upload your data to Cloud Storage."
      },
      {
        "date": "2022-10-27T07:29:00.000Z",
        "voteCount": 3,
        "content": "Problem: Transferring 2 peta data to Cloud Storage\nConsiderations: Bad network speed\n\n\nBad network = cannot initiate from client\u2019s end through network. So, B, C is out\nD will still be super slow. At this speed it will take 27,777 hours to transfer the data"
      },
      {
        "date": "2021-07-04T06:42:00.000Z",
        "voteCount": 9,
        "content": "Vote for A\n\nA - Correct , Transfer Appliance for moving offline data, large data sets, or data from a source with limited bandwidth\nhttps://cloud.google.com/storage-transfer/docs/overview\nB - Eliminated (Not recommended for large storage). recommended for &lt; 1TB\nC  - Its ONLINE, but we have bandwidth issue - So eliminated.\nD - Eliminated (Not recommended for large storage). recommended for &lt; 1TB"
      },
      {
        "date": "2020-09-24T04:59:00.000Z",
        "voteCount": 3,
        "content": "Correct answer is A. with little calculation we know the kind of data will require approx 19 months to transfer on 20Mbps bandwidth. Also, google recommends Transfer appliance for petabytes of data."
      },
      {
        "date": "2020-08-22T05:31:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2020-07-06T13:40:00.000Z",
        "voteCount": 3,
        "content": "Correct - A"
      },
      {
        "date": "2020-04-01T21:16:00.000Z",
        "voteCount": 3,
        "content": "Answer A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 145,
    "url": "https://www.examtopics.com/discussions/google/view/16674-exam-professional-data-engineer-topic-1-question-145/",
    "body": "You receive data files in CSV format monthly from a third party. You need to cleanse this data, but every third month the schema of the files changes. Your requirements for implementing these transformations include:<br>\u2711 Executing the transformations on a schedule<br>\u2711 Enabling non-developer analysts to modify transformations<br>\u2711 Providing a graphical tool for designing transformations<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad each month's CSV data into BigQuery, and write a SQL query to transform the data to a standard schema. Merge the transformed tables together with a SQL query",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHelp the analysts write a Dataflow pipeline in Python to perform the transformation. The Python code should be stored in a revision control system and modified as the incoming data's schema changes",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Spark on Dataproc to infer the schema of the CSV file before creating a Dataframe. Then implement the transformations in Spark SQL before writing the data out to Cloud Storage and loading into BigQuery"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T09:07:00.000Z",
        "voteCount": 35,
        "content": "A should be the answer"
      },
      {
        "date": "2020-03-28T08:55:00.000Z",
        "voteCount": 18,
        "content": "Answer: A\nDescription: Dataprep is used by non developers"
      },
      {
        "date": "2024-04-04T16:32:00.000Z",
        "voteCount": 1,
        "content": "A. Use Dataprep by Trifacta to build and maintain the transformation recipes, and execute them on a scheduled basis\n\nAddresses Requirements:\nScheduled Execution: Dataprep supports running transformations on a schedule.\nAnalyst-Friendly: Dataprep's visual interface is designed for non-developer analysts to build and modify transformations easily.\nGraphical Tool: It provides a drag-and-drop environment for designing data transformations.\nSchema Flexibility: Dataprep can handle schema changes. Analysts can adapt recipes using the visual interface"
      },
      {
        "date": "2023-09-23T21:11:00.000Z",
        "voteCount": 1,
        "content": "Scheduled Transformations: Dataprep by Trifacta allows you to design and schedule transformation recipes to process data on a regular basis. You can automate the data cleansing process by scheduling it to run monthly.\n\nUser-Friendly Interface: Dataprep provides a user-friendly graphical interface that enables non-developer analysts to design, modify, and maintain transformation recipes without writing code. This empowers analysts to work with the data effectively.\n\nTransformation Flexibility: Dataprep supports flexible data transformations, making it suitable for scenarios where the schema of the incoming data changes. Analysts can adapt the transformations to new schemas using the visual tools provided by Dataprep."
      },
      {
        "date": "2023-05-09T08:50:00.000Z",
        "voteCount": 4,
        "content": "Providing a graphical tool for designing transformations is enough for A"
      },
      {
        "date": "2023-02-26T00:19:00.000Z",
        "voteCount": 1,
        "content": "Your company receives a lot of financial data in CSV files. The files need to be processed, cleaned and transformed before they are made available for analytics. The schema of the data also changes every third month. The Data analysts should be able to perform the tasks\n    1. No prior knowledge of any language with no coding\n    2. Provided a GUI tool to build and modify the schema\nWhat solution best fits the need?"
      },
      {
        "date": "2022-12-01T05:59:00.000Z",
        "voteCount": 3,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dataprep\nDataprep by Trifacta is an intelligent data service for visually exploring, cleaning, and preparing structured and unstructured data for analysis, reporting, and machine learning. Because Dataprep is serverless and works at any scale, there is no infrastructure to deploy or manage. Your next ideal data transformation is suggested and predicted with each UI input, so you don\u2019t have to write code."
      },
      {
        "date": "2022-09-06T04:31:00.000Z",
        "voteCount": 2,
        "content": "non-developer analysts"
      },
      {
        "date": "2022-05-30T22:07:00.000Z",
        "voteCount": 1,
        "content": "Dataprep is for non developers"
      },
      {
        "date": "2022-05-18T23:35:00.000Z",
        "voteCount": 1,
        "content": "Option A -- Dataprep is the right answer"
      },
      {
        "date": "2022-02-25T01:52:00.000Z",
        "voteCount": 1,
        "content": "Answer is A"
      },
      {
        "date": "2022-01-09T11:33:00.000Z",
        "voteCount": 2,
        "content": "A: https://cloud.google.com/dataprep/"
      },
      {
        "date": "2022-01-08T06:55:00.000Z",
        "voteCount": 1,
        "content": "Cloud Dataprep is a tool to do the job."
      },
      {
        "date": "2021-11-26T02:42:00.000Z",
        "voteCount": 7,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: A"
      },
      {
        "date": "2022-07-27T16:19:00.000Z",
        "voteCount": 1,
        "content": "this comment is being repeated and i really appreciate this feeling :D"
      },
      {
        "date": "2021-08-09T05:29:00.000Z",
        "voteCount": 4,
        "content": "vote for option A"
      },
      {
        "date": "2021-07-04T06:58:00.000Z",
        "voteCount": 5,
        "content": "Vote for 'A', because of requirement - Enabling non-developer analysts to modify transformations"
      },
      {
        "date": "2020-08-22T05:38:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 146,
    "url": "https://www.examtopics.com/discussions/google/view/17224-exam-professional-data-engineer-topic-1-question-146/",
    "body": "You want to migrate an on-premises Hadoop system to Cloud Dataproc. Hive is the primary tool in use, and the data format is Optimized Row Columnar (ORC).<br>All ORC files have been successfully copied to a Cloud Storage bucket. You need to replicate some data to the cluster's local Hadoop Distributed File System<br>(HDFS) to maximize performance. What are two ways to start using Hive in Cloud Dataproc? (Choose two.)<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Mount the Hive tables locally.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeverage Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Replicate external Hive tables to the native ones.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the ORC files into BigQuery. Leverage BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables. Replicate external Hive tables to the native ones."
    ],
    "answer": "CD",
    "answerDescription": "",
    "votes": [
      {
        "answer": "CD",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "AD",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "DE",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "BC",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-12-14T04:09:00.000Z",
        "voteCount": 24,
        "content": "Answer is C and D 100%.\nI know it says to transfer all the files but with the options provided c is the best choice.\nExplaination\nA and B cannot be true as gsutil can copy data to master node and the to hdfs from master node.\nC -&gt; works\nD-&gt;works Recommended by google\nE-&gt; Will work but as the question says maximize performance this is not a case. As bigquery hadoop connecter stores all the BQ data to GCS as temp and then processes it to HDFS. As data is already in GCS we donot need to load it to bq and use a connector then unloads it back to GCS and then processes it."
      },
      {
        "date": "2022-01-30T16:39:00.000Z",
        "voteCount": 2,
        "content": "How can master node store data ? C is wrong."
      },
      {
        "date": "2023-10-18T19:37:00.000Z",
        "voteCount": 1,
        "content": "must go to the master node first..."
      },
      {
        "date": "2022-01-06T00:57:00.000Z",
        "voteCount": 2,
        "content": "option D is mentioned in the new release 2019:  https://cloud.google.com/blog/products/data-analytics/new-release-of-cloud-storage-connector-for-hadoop-improving-performance-throughput-and-more"
      },
      {
        "date": "2023-06-07T13:05:00.000Z",
        "voteCount": 3,
        "content": "I feel indeed this question is testing if you understand, that gsutil cannot transfer to HDFS directly (eliminate A&amp;B), and need a intermediate step, (making C doable, with a good result). D is found on official google docs. E doesn't have good end result."
      },
      {
        "date": "2020-03-22T00:31:00.000Z",
        "voteCount": 17,
        "content": "Should be B C"
      },
      {
        "date": "2023-12-30T06:07:00.000Z",
        "voteCount": 2,
        "content": "I think D and E are the best and easy way to go. For sure D, but I think that E can work too, the data can be loaded in BQ as an external table, so at the end the data will be always on the GCS."
      },
      {
        "date": "2023-09-23T21:43:00.000Z",
        "voteCount": 1,
        "content": "D. Cloud Storage Connector for Hadoop: You can use the Cloud Storage connector for Hadoop to mount the ORC files stored in Cloud Storage as external Hive tables. This allows you to query the data without copying it to HDFS. You can replicate these external Hive tables to native Hive tables in Cloud Dataproc if needed.\n\nE. Load ORC Files into BigQuery: Another approach is to load the ORC files into BigQuery, Google Cloud's data warehouse. Once the data is in BigQuery, you can use the BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables in Cloud Dataproc. This leverages the power of BigQuery for analytics and allows you to replicate external Hive tables to native ones in Cloud Dataproc."
      },
      {
        "date": "2023-09-23T21:39:00.000Z",
        "voteCount": 1,
        "content": "D. Cloud Storage Connector for Hadoop: You can use the Cloud Storage connector for Hadoop to mount the ORC files stored in Cloud Storage as external Hive tables. This allows you to query the data without copying it to HDFS. You can replicate these external Hive tables to native Hive tables in Cloud Dataproc if needed.\n\nE. Load ORC Files into BigQuery: Another approach is to load the ORC files into BigQuery, Google Cloud's data warehouse. Once the data is in BigQuery, you can use the BigQuery connector for Hadoop to mount the BigQuery tables as external Hive tables in Cloud Dataproc. This leverages the power of BigQuery for analytics and allows you to replicate external Hive tables to native ones in Cloud Dataproc."
      },
      {
        "date": "2023-07-25T20:55:00.000Z",
        "voteCount": 2,
        "content": "A is the most straightforward way to start using Hive in Cloud Dataproc. You can use the gsutil utility to transfer all ORC files from the Cloud Storage bucket to HDFS. Then, you can mount the Hive tables locally.\n\nD is another option that you can use to start using Hive in Cloud Dataproc. You can leverage the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables. Then, you can replicate the external Hive tables to the native ones."
      },
      {
        "date": "2023-07-10T02:12:00.000Z",
        "voteCount": 3,
        "content": "Answers are;\nB. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to any node of the Dataproc cluster. Mount the Hive tables locally.\nC. Run the gsutil utility to transfer all ORC files from the Cloud Storage bucket to the master node of the Dataproc cluster. Then run the Hadoop utility to copy them do HDFS. Mount the Hive tables from HDFS.\n\nYou need to replicate some data to the cluster's local Hadoop Distributed File System (HDFS) to maximize performance. HDFS lies on datanode, data on masternode needs to be copied on datanode.\nB for managed hive table option, C for external hive table"
      },
      {
        "date": "2023-05-08T04:31:00.000Z",
        "voteCount": 1,
        "content": "AD is correct"
      },
      {
        "date": "2023-05-03T03:47:00.000Z",
        "voteCount": 1,
        "content": "i choose AD. \nSearched in other w/s, read discussions here, and guess better AD."
      },
      {
        "date": "2023-05-03T03:49:00.000Z",
        "voteCount": 1,
        "content": "gpt: Yes, that is correct. Option A is a valid way to transfer the ORC files to HDFS, and then mount the Hive tables locally. Option D is also valid, as it suggests using the Cloud Storage connector for Hadoop to mount the ORC files as external Hive tables and then replicating those external Hive tables to native ones.\n\nChatgpt agreed, after inserting quesion and variants, and said that AD are correct answers. And it agreed. It adds some confidence that these are good, but gpt can make mistakes"
      },
      {
        "date": "2023-04-11T00:46:00.000Z",
        "voteCount": 1,
        "content": "A will copy to HDFS and so will D"
      },
      {
        "date": "2022-12-02T22:40:00.000Z",
        "voteCount": 3,
        "content": "C: master node doesn't make sense"
      },
      {
        "date": "2022-12-02T22:41:00.000Z",
        "voteCount": 1,
        "content": "B:  from the Cloud Storage bucket to any node of the Dataproc cluster\n-&gt; still on cloud not maxize the speed"
      },
      {
        "date": "2022-12-01T05:52:00.000Z",
        "voteCount": 4,
        "content": "CD is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage\nThe Cloud Storage connector is an open source Java library that lets you run Apache Hadoop or Apache Spark jobs directly on data in Cloud Storage, and offers a number of benefits over choosing the Hadoop Distributed File System (HDFS).\n\nConnector Support. The Cloud Storage connector is supported by Google Cloud for use with Google Cloud products and use cases, and when used with Dataproc is supported at the same level as Dataproc."
      },
      {
        "date": "2022-10-30T10:12:00.000Z",
        "voteCount": 2,
        "content": "I'll go with DE"
      },
      {
        "date": "2022-09-02T22:45:00.000Z",
        "voteCount": 2,
        "content": "CD is correct"
      },
      {
        "date": "2022-02-08T02:22:00.000Z",
        "voteCount": 1,
        "content": "You need to replicate some data to the cluster's local Hadoop Distributed File System (HDFS) to maximize performance. HDFS lies on datanode, data on masternode needs to be copied on datanode.\nB for managed hive table option, C for external hive table"
      },
      {
        "date": "2022-01-08T07:01:00.000Z",
        "voteCount": 3,
        "content": "as explained by Sid19"
      },
      {
        "date": "2021-12-10T07:07:00.000Z",
        "voteCount": 2,
        "content": "Prefer A &amp; E. Since master node does not store data in HDFS (it does checkpointing, but the data is always present on the slave nodes) -&gt; B is out. \nFurthermore, when you copy using gsutil, you are defining a HDFS path instead of a node name (it is abstracted away to which node it is going to be put and replicated finally) \nso B just makes no sense. Since performance is key, we need to have a native table eventually (when data is on HDFS it can be used as external table only, but we create the \nnative variant of it..) -&gt; is fulfilled by option D. (E is out also, since nobody cares about BigQuery, the question is how to use Hive which is possible right after the data is present on HDFS.. )"
      },
      {
        "date": "2021-12-10T07:11:00.000Z",
        "voteCount": 3,
        "content": "Prefer A &amp; D. typo, moderator can you fix it?"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 147,
    "url": "https://www.examtopics.com/discussions/google/view/16675-exam-professional-data-engineer-topic-1-question-147/",
    "body": "You are implementing several batch jobs that must be executed on a schedule. These jobs have many interdependent steps that must be executed in a specific order. Portions of the jobs involve executing shell scripts, running Hadoop jobs, and running queries in BigQuery. The jobs are expected to run for many minutes up to several hours. If the steps fail, they must be retried a fixed number of times. Which service should you use to manage the execution of these jobs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Scheduler",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Dataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Functions",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Composer\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-03-22T20:55:00.000Z",
        "voteCount": 42,
        "content": "if someone are not sure that D is the answer, I suggest to don't take the exam"
      },
      {
        "date": "2020-03-15T09:11:00.000Z",
        "voteCount": 23,
        "content": "D should be the answer"
      },
      {
        "date": "2023-12-30T06:09:00.000Z",
        "voteCount": 1,
        "content": "No duobt"
      },
      {
        "date": "2023-09-23T21:44:00.000Z",
        "voteCount": 1,
        "content": "Workflow Orchestration: Cloud Composer is a fully managed workflow orchestration service based on Apache Airflow. It allows you to define, schedule, and manage complex workflows with multiple steps, including shell scripts, Hadoop jobs, and BigQuery queries.\n\nDependency Management: You can define dependencies between different steps in your workflow to ensure they are executed in a specific order.\n\nRetry Mechanism: Cloud Composer provides built-in retry mechanisms, so if any step fails, it can be retried a fixed number of times according to your configuration.\n\nScheduled Execution: Cloud Composer allows you to schedule the execution of your workflows on a regular basis, meeting the requirement for executing the jobs on a schedule."
      },
      {
        "date": "2022-12-29T19:33:00.000Z",
        "voteCount": 2,
        "content": "D is right"
      },
      {
        "date": "2022-12-01T05:42:00.000Z",
        "voteCount": 4,
        "content": "D is the answer.\n\nhttps://cloud.google.com/composer/docs/concepts/overview\nCloud Composer is a fully managed workflow orchestration service, enabling you to create, schedule, monitor, and manage workflows that span across clouds and on-premises data centers."
      },
      {
        "date": "2022-07-20T13:25:00.000Z",
        "voteCount": 1,
        "content": "Cloud Composer for sure."
      },
      {
        "date": "2022-09-30T02:10:00.000Z",
        "voteCount": 2,
        "content": "Composer is D"
      },
      {
        "date": "2022-06-05T21:53:00.000Z",
        "voteCount": 1,
        "content": "D. \nper document \"Scheduler\" is aimed to a single service and composer for an ETL , in addition it's not even specified all jobs are on cloud so only composer can handle it."
      },
      {
        "date": "2022-06-05T21:53:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/choosing-right-orchestrator-google-cloud"
      },
      {
        "date": "2022-01-08T07:04:00.000Z",
        "voteCount": 2,
        "content": "Cloud Composer"
      },
      {
        "date": "2021-11-26T03:40:00.000Z",
        "voteCount": 2,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: D"
      },
      {
        "date": "2021-02-19T10:51:00.000Z",
        "voteCount": 4,
        "content": "D:\nthe main point is that Cloud Composer should be used when there is inter-dependencies between the job, e.g. we need the output of a job to start another whenever the first finished, and use dependencies coming from first job."
      },
      {
        "date": "2020-12-23T21:12:00.000Z",
        "voteCount": 3,
        "content": "D seems to be quiet relevant , because using composure you can do all things which are being asked to perform, even retry property is there in composure."
      },
      {
        "date": "2020-11-13T08:02:00.000Z",
        "voteCount": 5,
        "content": "The correct answer is Option A : Cloud Scheduler . \nAlthough at first instance, I thought it should be Cloud Composer but then looking at the question and reading it few times - it concluded me to go for Option A. \n\nCloud Scheduler has built in retry handling so you can set a fixed number of times and doesn't have time limits for requests. The functionality is much simpler than Cloud Composer. Cloud Composer is managed Apache Airflow that \"helps you create, schedule, monitor and manage workflows. For automate scheduled jobs - the most preferred method would be Scheduler, Composer would typically be used when we want to orchestrate many managed services and automate the work flow."
      },
      {
        "date": "2020-11-18T18:35:00.000Z",
        "voteCount": 1,
        "content": "A seems to be right"
      },
      {
        "date": "2020-12-13T02:21:00.000Z",
        "voteCount": 3,
        "content": "I think D , how scheduler can handle this part \" The jobs are expected to run for many minutes up to several hours\""
      },
      {
        "date": "2021-12-21T03:48:00.000Z",
        "voteCount": 1,
        "content": "You forgot the \"These jobs have many interdependent steps\" which can be handled only though Composer"
      },
      {
        "date": "2020-11-13T05:43:00.000Z",
        "voteCount": 1,
        "content": "should be A"
      },
      {
        "date": "2020-10-15T03:21:00.000Z",
        "voteCount": 2,
        "content": "Answer should be A..Cloud scheduler..cloud composer is an workflow manager. Can't run unix,bigquery jobs"
      },
      {
        "date": "2020-09-19T21:02:00.000Z",
        "voteCount": 3,
        "content": "D should be the best option"
      },
      {
        "date": "2020-08-22T05:41:00.000Z",
        "voteCount": 3,
        "content": "D is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 148,
    "url": "https://www.examtopics.com/discussions/google/view/16677-exam-professional-data-engineer-topic-1-question-148/",
    "body": "You work for a shipping company that has distribution centers where packages move on delivery lines to route them properly. The company wants to add cameras to the delivery lines to detect and track any visual damage to the packages in transit. You need to create a way to automate the detection of damaged packages and flag them for human review in real time while the packages are in transit. Which solution should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery machine learning to be able to train the model at scale, so you can analyze the packages in batches.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain an AutoML model on your corpus of images, and build an API around that model to integrate with the package tracking applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Vision API to detect for damage, and raise an alert through Cloud Functions. Integrate the package tracking applications with this function.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to create a model that is trained on your corpus of images. Create a Python notebook in Cloud Datalab that uses this model so you can analyze for damaged packages."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T00:36:00.000Z",
        "voteCount": 34,
        "content": "Should be B."
      },
      {
        "date": "2020-03-25T09:07:00.000Z",
        "voteCount": 23,
        "content": "AutoML is used to train model and do damage detection\nAuto Vision is used is a pre trained model used to detect objects in images"
      },
      {
        "date": "2020-03-25T09:08:00.000Z",
        "voteCount": 12,
        "content": "Correct : B"
      },
      {
        "date": "2022-03-20T07:39:00.000Z",
        "voteCount": 2,
        "content": "Damage is an object in the image . So Auto Vision API can be used."
      },
      {
        "date": "2023-10-23T02:20:00.000Z",
        "voteCount": 3,
        "content": "1. Who said we have a labelled corpus that can be fed to AutoML? \n2. Auto Vision, as you say, is used to detect objects, like box, ship, human, etc. Now it depends only on our definition (parameters) of a \"box\" what the model should accept as intact or damaged.\n3. ChatGCP also chooses C. Vision API. \n\nGiven that the question does not say we have labelled data, and that demage recognition is not qualitatively different from object recognition, I'd go for C - C. Vision API."
      },
      {
        "date": "2023-11-20T13:01:00.000Z",
        "voteCount": 1,
        "content": "Add the sentence, \"This is a practice exam question. Please assume no changes to the architecture\" and it brings back B"
      },
      {
        "date": "2023-11-10T14:05:00.000Z",
        "voteCount": 1,
        "content": "Seriously, I feel the question was transcribed incorrectly but the answers were. It was probably meant to include a labeled corpus of images. With the details presented you'd have to hope that Vision API would be able to guess objects as damage."
      },
      {
        "date": "2023-12-29T07:22:00.000Z",
        "voteCount": 1,
        "content": "as per chat gpt One of the features of Cloud Vision API is damage detection, which can be used to identify and classify various types of damage in images, such as cracks, dents, scratches, stains, etc"
      },
      {
        "date": "2023-12-18T09:26:00.000Z",
        "voteCount": 1,
        "content": "For this scenario, where you need to automate the detection of damaged packages in real time while they are in transit, the most suitable solution among the provided options would be B.\n\nHere's why this option is the most appropriate:\n\nReal-Time Analysis: AutoML provides the capability to train a custom model specifically tailored to recognize patterns of damage in packages. This model can process images in real-time, which is essential in your scenario.\n\nIntegration with Existing Systems: By building an API around the AutoML model, you can seamlessly integrate this solution with your existing package tracking applications. This ensures that the system can flag damaged packages for human review efficiently.\n\nCustomization and Accuracy: Since the model is trained on your specific corpus of images, it can be more accurate in detecting damages relevant to your use case compared to pre-trained models."
      },
      {
        "date": "2023-12-18T09:27:00.000Z",
        "voteCount": 1,
        "content": "Let's briefly consider why the other options are less suitable:\n\nA. Use BigQuery machine learning: BigQuery is great for handling large-scale data analytics but is not optimized for real-time image processing or complex image recognition tasks like damage detection on packages.\n\nC. Use the Cloud Vision API: While the Cloud Vision API is powerful for general image analysis, it might not be as effective for the specific task of detecting damage on packages, which requires a more customized approach.\n\nD. Use TensorFlow in Cloud Datalab: While this is a viable option for creating a custom model, it might be more complex and time-consuming compared to using AutoML. Additionally, setting up a real-time analysis system through a Python notebook might not be as straightforward as an API integration."
      },
      {
        "date": "2023-12-09T20:16:00.000Z",
        "voteCount": 3,
        "content": "I was leaning towards C but tested out uploading some damaged boxes to Vision API. It seems to have a lot of trouble detecting damaged boxes. It mislabeled boxes as a tire or toy. Also, there is no part of the API that seems to be able to detect damage. So I'll have to go with B. You should train a model to accomplish this then integrate with your app."
      },
      {
        "date": "2023-09-23T21:48:00.000Z",
        "voteCount": 2,
        "content": "AutoML for Custom Models: AutoML (Auto Machine Learning) is designed to simplify the process of training custom machine learning models, including image classification models. It allows you to leverage Google Cloud's pre-built AutoML Vision service to train a model specifically for detecting package damage based on your corpus of images. This ensures accurate and customized results.\nReal-time API Integration: After training the AutoML model, you can create an API endpoint that integrates seamlessly with your package tracking applications. This means that as packages move on the delivery lines, you can send images in real-time to the API for immediate analysis.\nScalability: AutoML Vision is built to scale, so it can handle the analysis of images in real-time, even as packages move continuously on the delivery lines."
      },
      {
        "date": "2023-08-20T02:50:00.000Z",
        "voteCount": 5,
        "content": "Keywords: realtime, camera streaming\n\nhttps://cloud.google.com/vision#:~:text=where%20you%20are-,Vertex%20AI%20Vision,-Vertex%C2%A0AI%20Vision \n\nOption B AutoML would be too complex and not time efficient.\n\nUsing Vision AI(Vertex AI Vision) first + AutoML \nOption D is better than B (just AutoML)."
      },
      {
        "date": "2023-08-20T02:53:00.000Z",
        "voteCount": 3,
        "content": "typo: Option C is better than B."
      },
      {
        "date": "2023-08-17T06:11:00.000Z",
        "voteCount": 1,
        "content": "B\nhttps://www.cloudskillsboost.google/focuses/22020?parent=catalog"
      },
      {
        "date": "2023-07-25T20:46:00.000Z",
        "voteCount": 1,
        "content": "Option B - AutoML"
      },
      {
        "date": "2023-05-10T02:21:00.000Z",
        "voteCount": 2,
        "content": "will stay with B. Might be more reliable, accurate.\nas many says in duscission, Vision Api does not say it has defect detection. \ni remember labs with Auto ML, where models were trained. Vertex AI labs."
      },
      {
        "date": "2022-12-29T19:21:00.000Z",
        "voteCount": 2,
        "content": "B is right"
      },
      {
        "date": "2022-11-25T22:14:00.000Z",
        "voteCount": 3,
        "content": "B\nC-is not answer \nVision API currently allows you to use the following features:\nhttps://cloud.google.com/vision/docs/features-list#:~:text=Vision%20API%20currently%20allows%20you%20to%20use%20the%20following%20features%3A"
      },
      {
        "date": "2022-11-06T14:53:00.000Z",
        "voteCount": 3,
        "content": "It looks like B is the only valid option, with the assumption that you have a corpus of images (the question does not say that you do not). \nIt would not be Cloud Vision API because that does not do damage detection (https://cloud.google.com/vision/docs/features-list)."
      },
      {
        "date": "2022-09-23T09:50:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/solutions/visual-inspection-ai#all-features"
      },
      {
        "date": "2022-11-06T14:50:00.000Z",
        "voteCount": 2,
        "content": "This is how you would do it nowadays, but the question is not referring to this solution. It only refers to \"Cloud Vision API\" (not Visual Inspection API). Cloud Vision API does not do damage detection (https://cloud.google.com/vision/docs/features-list) so you would need to do AutoML. It looks like they assume that you have your own corpus of images."
      },
      {
        "date": "2022-02-08T21:17:00.000Z",
        "voteCount": 1,
        "content": "Here it is mentioned that the company is planning to implement a camera system. So it does not have the training data yet. Without having training data , the only option left is to use pre- trained models through cloud API . C Is the answer. B is wrong as you dont have data to train the model."
      },
      {
        "date": "2022-03-28T21:15:00.000Z",
        "voteCount": 3,
        "content": "I would correct myself and go for B. I did not find any mention of cloud vision api being used for object detection."
      },
      {
        "date": "2022-01-22T14:22:00.000Z",
        "voteCount": 2,
        "content": "AutoML is used to train model and do damage detection Auto Vision is used is a pre trained model used to detect objects in images"
      },
      {
        "date": "2022-01-08T07:08:00.000Z",
        "voteCount": 3,
        "content": "Cloud Vision API -&gt; pre-trained models to detect labels, faces, words\nAutoML -&gt; custom specific models trained for specific use case"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 149,
    "url": "https://www.examtopics.com/discussions/google/view/16678-exam-professional-data-engineer-topic-1-question-149/",
    "body": "You are migrating your data warehouse to BigQuery. You have migrated all of your data into tables in a dataset. Multiple users from your organization will be using the data. They should only see certain tables based on their team membership. How should you set user permissions?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the users/groups data viewer access at the table level for each table\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate SQL views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the SQL views",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate authorized views for each team in the same dataset in which the data resides, and assign the users/groups data viewer access to the authorized views",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate authorized views for each team in datasets created for each team. Assign the authorized views data viewer access to the dataset in which the data resides. Assign the users/groups data viewer access to the datasets in which the authorized views reside"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2021-02-08T21:59:00.000Z",
        "voteCount": 40,
        "content": "Correct A: A . Now it is feasible to provide table level access to user by allowing user to query single table and no other table will be visible to user in same dataset."
      },
      {
        "date": "2024-04-10T05:08:00.000Z",
        "voteCount": 1,
        "content": "the request says \"team membership\", so access depends on the team and not the user"
      },
      {
        "date": "2021-08-21T08:31:00.000Z",
        "voteCount": 2,
        "content": "A is not at all possible"
      },
      {
        "date": "2021-10-08T07:28:00.000Z",
        "voteCount": 8,
        "content": "It is possible for about a year now. https://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case"
      },
      {
        "date": "2022-07-21T07:33:00.000Z",
        "voteCount": 2,
        "content": "The problem is that option A has a lot of work for the DevOps, meanwhile option D is easier to manage. The view is like having a shortcut to the same data, but with different permissions"
      },
      {
        "date": "2023-05-30T03:34:00.000Z",
        "voteCount": 3,
        "content": "According to Chat GPT, it is also D.\nAnd it explains why it shouldn't be \"A\" as;\n\nGranularity: While you can assign access permissions at the table level, it doesn't allow for fine-grained access control. For example, if you want to restrict access to certain columns or rows within a table based on user or group, table-level permissions would not be sufficient.\n\nScalability: In organizations with many tables and users, managing permissions at the table level can quickly become unwieldy. You would need to individually set permissions for each user for each table, which can be time-consuming and error-prone.\n\nSecurity: Table-level permissions expose the entire table to a user or a group. If the data in the table changes over time, users might get access to data they shouldn't see. With authorized views, you have more control over what data is exposed.\n\nMaintenance: If the structure of your data changes (for instance, if tables are added or removed, or if the schema of a table changes), you would need to manually update the permissions for each affected table."
      },
      {
        "date": "2021-10-28T03:03:00.000Z",
        "voteCount": 11,
        "content": "Should still be D.\n\nQuestion states - \"They should only see certain tables based on their team membership\"\n\nOption A states -  Assign the users/groups data viewer access at the table level for each table\n\nWith A, everyone will see every table. Hence D."
      },
      {
        "date": "2020-03-15T09:19:00.000Z",
        "voteCount": 27,
        "content": "D should be the answer"
      },
      {
        "date": "2021-10-08T07:27:00.000Z",
        "voteCount": 3,
        "content": "There is only one dataset mentioned in the question here. \"You have migrated all of your data into tables in a dataset\""
      },
      {
        "date": "2022-08-26T17:06:00.000Z",
        "voteCount": 1,
        "content": "It is updated, now A is correct"
      },
      {
        "date": "2024-08-13T02:38:00.000Z",
        "voteCount": 1,
        "content": "Recommended approach"
      },
      {
        "date": "2024-06-04T05:49:00.000Z",
        "voteCount": 2,
        "content": "Should be D."
      },
      {
        "date": "2023-12-18T09:37:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/solutions/migration/dw2bq/dw-bq-data-governance\nWhen you create the view, it must be created in a dataset separate from the source data queried by the view. Because you can assign access controls only at the dataset level, if the view is created in the same dataset as the source data, your users would have access to both the view and the data.\nhttps://cloud.google.com/bigquery/docs/authorized-views\nThis approach aligns with the Google Cloud best practices for data governance, ensuring that users can only access the data intended for them without having direct access to the source tables. Authorized views serve as a secure interface to the underlying data, and by placing these views in separate datasets per team, you can manage permissions effectively at the dataset level."
      },
      {
        "date": "2023-11-21T13:02:00.000Z",
        "voteCount": 1,
        "content": "but the question said that all data are copied into one dataset. so it should be C"
      },
      {
        "date": "2023-11-08T05:25:00.000Z",
        "voteCount": 2,
        "content": "A is the best answer for security as stated in the documentation - https://cloud.google.com/bigquery/docs/row-level-security-intro#comparison_of_authorized_views_row-level_security_and_separate_tables"
      },
      {
        "date": "2023-09-24T05:22:00.000Z",
        "voteCount": 1,
        "content": "A is a better fit than D for this case"
      },
      {
        "date": "2023-09-23T21:52:00.000Z",
        "voteCount": 1,
        "content": "Authorized Views: Authorized views in BigQuery allow you to control access to specific rows and columns within a table. This means you can create views for each team that restrict access to only the data relevant to that team.\nSingle Dataset: Keeping all the authorized views and the underlying data in the same dataset simplifies management and access control. It avoids the need to create multiple datasets, making the permission management process more straightforward.\n\nOption A (assigning data viewer access at the table level) would not provide the granularity you need, as it would allow users to see all tables in the dataset. This does not align with the requirement to restrict access based on team membership."
      },
      {
        "date": "2023-08-20T03:09:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/share-access-views#:~:text=the%20source%20data.-,Authorized%20views,-should%20be%20created\n\nFor best practice, Option D is bettern than others."
      },
      {
        "date": "2023-03-23T23:01:00.000Z",
        "voteCount": 1,
        "content": "[A] is correct if it is for individual table\nHowever, in practice we normally do [C] as most of the time, the view is a JOIN of a few tables or a subset of the table (some columns removed)"
      },
      {
        "date": "2023-02-18T03:25:00.000Z",
        "voteCount": 1,
        "content": "Answer A, Trick here is, if question is not asking for data level Access such as some rows or columns, don't go for authorized view in that case i would go for C. If it's Table level request only in question, then A is simple answer"
      },
      {
        "date": "2022-12-01T05:33:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/control-access-to-resources-iam#grant_access_to_a_table_or_view"
      },
      {
        "date": "2022-11-19T04:00:00.000Z",
        "voteCount": 1,
        "content": "A - table level access control now exists: https://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case"
      },
      {
        "date": "2022-11-11T04:14:00.000Z",
        "voteCount": 1,
        "content": "A.\nPlease see:\nhttps://cloud.google.com/bigquery/docs/table-access-controls-intro#example_use_case"
      },
      {
        "date": "2022-09-23T08:26:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/table-access-controls-intro\nDon't think too much ,there is nothing to do with view, the question refer to table obviousely.\nAssume that User see  certain table so he can see everything in  such a table"
      },
      {
        "date": "2022-09-12T07:55:00.000Z",
        "voteCount": 4,
        "content": "It has nothing to do with authorize view because of the following \nAuthorized views make use of query results but this question emphasise on Table level \nhttps://cloud.google.com/bigquery/docs/authorized-views\nAn authorized view lets you share query results with particular users and groups without giving them access to the underlying source data."
      },
      {
        "date": "2023-09-22T05:42:00.000Z",
        "voteCount": 1,
        "content": "finally after tens of comments, i see one that explains and makes sense"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 150,
    "url": "https://www.examtopics.com/discussions/google/view/16868-exam-professional-data-engineer-topic-1-question-150/",
    "body": "You want to build a managed Hadoop system as your data lake. The data transformation process is composed of a series of Hadoop jobs executed in sequence.<br>To accomplish the design of separating storage from compute, you decided to use the Cloud Storage connector to store all input data, output data, and intermediary data. However, you noticed that one Hadoop job runs very slowly with Cloud Dataproc, when compared with the on-premises bare-metal Hadoop environment (8-core nodes with 100-GB RAM). Analysis shows that this particular Hadoop job is disk I/O intensive. You want to resolve the issue. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate sufficient memory to the Hadoop cluster, so that the intermediary data of that particular Hadoop job can be held in memory",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate more CPU cores of the virtual machine instances of the Hadoop cluster so that the networking bandwidth for each instance can scale up",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAllocate additional network interface card (NIC), and configure link aggregation in the operating system to use the combined throughput when working with Cloud Storage"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-25T09:12:00.000Z",
        "voteCount": 39,
        "content": "Correct: B\n\nLocal HDFS storage is a good option if:\n\nYour jobs require a lot of metadata operations\u2014for example, you have thousands of partitions and directories, and each file size is relatively small.\nYou modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.)\nYou heavily use the append operation on HDFS files.\nYou have workloads that involve heavy I/O. For example, you have a lot of partitioned writes, such as the following:\n\nspark.read().write.partitionBy(...).parquet(\"gs://\")\n\nYou have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation."
      },
      {
        "date": "2020-04-02T18:28:00.000Z",
        "voteCount": 15,
        "content": "Answer B\nIts google recommended approach to use LocalDisk/HDFS to store Intermediate result and use Cloud Storage for initial and final results."
      },
      {
        "date": "2021-09-30T19:43:00.000Z",
        "voteCount": 1,
        "content": "Any link to support this recommended approach?"
      },
      {
        "date": "2023-12-18T09:40:00.000Z",
        "voteCount": 1,
        "content": "Local HDFS storage is a good option if:\n-\tYou have workloads that involve heavy I/O. For example, you have a lot of partitioned writes such as the following:\nspark.read().write.partitionBy(...).parquet(\"gs://\")\n-\tYou have I/O workloads that are especially sensitive to latency. For example, you require single-digit millisecond latency per storage operation.\n-\tYour jobs require a lot of metadata operations\u2014for example, you have thousands of partitions and directories, and each file size is relatively small.\n-\tYou modify the HDFS data frequently or you rename directories. (Cloud Storage objects are immutable, so renaming a directory is an expensive operation because it consists of copying all objects to a new key and deleting them afterwards.)\n-\tYou heavily use the append operation on HDFS files."
      },
      {
        "date": "2023-12-18T09:40:00.000Z",
        "voteCount": 1,
        "content": "We recommend using Cloud Storage as the initial and final source of data in a big-data pipeline. For example, if a workflow contains five Spark jobs in series, the first job retrieves the initial data from Cloud Storage and then writes shuffle data and intermediate job output to HDFS. The final Spark job writes its results to Cloud Storage.\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#choose_storage_options"
      },
      {
        "date": "2023-10-30T16:41:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B."
      },
      {
        "date": "2023-09-23T21:56:00.000Z",
        "voteCount": 1,
        "content": "Disk I/O Performance: In a Cloud Dataproc cluster, the default setup uses local persistent disks for HDFS storage. These disks offer good disk I/O performance and are well-suited for storing intermediate data generated during Hadoop jobs.\n\nData Locality: Storing intermediate data on native HDFS allows for better data locality. This means that the data is stored on the same nodes where computation occurs, reducing the need for data transfer over the network. This can significantly improve the performance of disk I/O-intensive jobs.\n\nScalability: Cloud Dataproc clusters can be easily scaled up or down to meet the specific requirements of your jobs. You can allocate additional disk space as needed to accommodate the intermediate data generated by this particular Hadoop job."
      },
      {
        "date": "2023-09-16T09:46:00.000Z",
        "voteCount": 1,
        "content": "Correct: A\nI'd choose A as the doc states adding more SSDs are good for disk-intensive jobs especially those with many individual read and write operations\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs"
      },
      {
        "date": "2023-09-20T19:29:00.000Z",
        "voteCount": 1,
        "content": "Typo Correct Answer is B. . Allocate sufficient persistent disk space to the Hadoop cluster, and store the intermediate data of that particular Hadoop job on native HDFS"
      },
      {
        "date": "2023-08-20T03:33:00.000Z",
        "voteCount": 1,
        "content": "I would choose A.\n\nGoogle Storage is faster than HDFS in many cases.\n\nhttps://cloud.google.com/architecture/hadoop#:~:text=It%27s%20faster%20than%20HDFS%20in%20many%20cases. \n\nThe question mention '(8-core nodes with 100-GB RAM)' on-premises Hadoop.\nthe problem may caused by insufficient memory,\nand does not mention cost would be an issue,\nso A 'memory' approach would be a better option."
      },
      {
        "date": "2023-07-25T20:31:00.000Z",
        "voteCount": 1,
        "content": "Best option is B. However  allocating sufficient persistent disk space to the Hadoop cluster, and storing the intermediate data of that particular Hadoop job on native HDFS, would not improve the performance of the Hadoop job. In fact, it might even slow down the Hadoop job, as the data would have to be read and written to disk twice."
      },
      {
        "date": "2022-12-01T05:28:00.000Z",
        "voteCount": 4,
        "content": "B is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#choosing_primary_disk_options\nIf your job is disk-intensive and is executing slowly on individual nodes, you can add more primary disk space. For particularly disk-intensive jobs, especially those with many individual read and write operations, you might be able to improve operation by adding local SSDs. Add enough SSDs to contain all of the space you need for local execution. Your local execution directories are spread across however many SSDs you add."
      },
      {
        "date": "2022-09-23T08:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-jobs#choosing_primary_disk_options"
      },
      {
        "date": "2022-08-29T17:15:00.000Z",
        "voteCount": 3,
        "content": "B is not the right answer. The problem says that for intermediate data cloud storage is to be used, while B option says:\n\nB ... the intermediate data of that particular Hadoop job on native HDFS\n\nA is the right answer. If you have enough memory then the shuffle wont spill on the disk."
      },
      {
        "date": "2022-08-29T17:16:00.000Z",
        "voteCount": 2,
        "content": "Further the question states that original on prem machines has 100gb ram.\n8-core nodes with 100-GB RAM"
      },
      {
        "date": "2022-01-10T00:57:00.000Z",
        "voteCount": 1,
        "content": "B should be the right answer: https://cloud.google.com/compute/docs/disks/performance#optimize_disk_performance"
      },
      {
        "date": "2022-01-08T07:15:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/solutions/migration/hadoop/hadoop-gcp-migration-jobs"
      },
      {
        "date": "2021-11-26T03:44:00.000Z",
        "voteCount": 3,
        "content": "Why there are so many wrong answers? Examtopics.com are you enjoying paid subscription by giving random answers from people?\nAns: B"
      },
      {
        "date": "2021-03-10T03:32:00.000Z",
        "voteCount": 3,
        "content": "If your job is disk-intensive and is executing slowly on individual nodes, you can add more primary disk space. For particularly disk-intensive jobs, especially those with many individual read and write operations, you might be able to improve operation by adding local SSDs. Add enough SSDs to contain all of the space you need for local execution. Your local execution directories are spread across however many SSDs you add.\nIts B \nhttps://cloud.google.com/solutions/migration/hadoop/hadoop-gcp-migration-jobs"
      },
      {
        "date": "2020-12-23T21:29:00.000Z",
        "voteCount": 2,
        "content": "yes B is correct"
      },
      {
        "date": "2020-11-13T08:37:00.000Z",
        "voteCount": 5,
        "content": "Correct Answer is Option B - Adding persistent disk space, reasons:-\n- The question mentions that the particular job is \"disk I/O intensive - so the word \"disk\" is explicitly mentioned. \n- Option B also mentions about local HDFS storage, which is ideally a good option of general I/O intensive work. \n-"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 151,
    "url": "https://www.examtopics.com/discussions/google/view/79680-exam-professional-data-engineer-topic-1-question-151/",
    "body": "You work for an advertising company, and you've developed a Spark ML model to predict click-through rates at advertisement blocks. You've been developing everything at your on-premises data center, and now your company is migrating to Google Cloud. Your data center will be closing soon, so a rapid lift-and-shift migration is necessary. However, the data you've been using will be migrated to migrated to BigQuery. You periodically retrain your Spark ML models, so you need to migrate existing training pipelines to Google Cloud. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Vertex AI for training existing Spark ML models",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite your models on TensorFlow, and start using Vertex AI",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataproc for training existing Spark ML models, but start reading data directly from BigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpin up a Spark cluster on Compute Engine, and train Spark ML models on the data exported from BigQuery"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 34,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-07-25T20:23:00.000Z",
        "voteCount": 6,
        "content": "Option C : It is the most rapid way to migrate your existing training pipelines to Google Cloud.\nIt allows you to continue using your existing Spark ML models.\nIt allows you to take advantage of the scalability and performance of Dataproc.\nIt allows you to read data directly from BigQuery, which is a more efficient way to process large datasets"
      },
      {
        "date": "2023-05-10T03:53:00.000Z",
        "voteCount": 6,
        "content": "the question is: is it faster to move a SparkML job to a Vertex AI or to Dataproc? I am personally not sure, I would go for Dataproc as notebooks are not mentioned, but reading the Google article:\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/\n\n\"Dataproc Serverless components for Vertex AI Pipelines that further simplify MLOps for Spark, Spark SQL, PySpark and Spark jobs.\""
      },
      {
        "date": "2023-11-20T13:10:00.000Z",
        "voteCount": 3,
        "content": "But you would need to re-write your models which can be a block"
      },
      {
        "date": "2024-07-06T20:51:00.000Z",
        "voteCount": 1,
        "content": "Vertex AI is better suited for TensorFlow or scikit-learn models. Direct Spark ML support isn't native to Vertex AI, making this a less straightforward migration path."
      },
      {
        "date": "2024-03-01T20:55:00.000Z",
        "voteCount": 1,
        "content": "C\nQuestion is about rapid lift and shift. So code changes should be minimul"
      },
      {
        "date": "2024-01-17T06:32:00.000Z",
        "voteCount": 1,
        "content": "C looks more suitable as data is alerady on BigQuery. \nRef - https://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml"
      },
      {
        "date": "2024-01-13T12:59:00.000Z",
        "voteCount": 1,
        "content": "Option C, agreed with other comments"
      },
      {
        "date": "2023-12-19T00:13:00.000Z",
        "voteCount": 4,
        "content": "Use Cloud Dataproc, BigQuery, and Apache Spark ML for Machine Learning\nhttps://cloud.google.com/dataproc/docs/tutorials/bigquery-sparkml\nUsing Apache Spark with TensorFlow on Google Cloud Platform\nhttps://cloud.google.com/blog/products/gcp/using-apache-spark-with-tensorflow-on-google-cloud-platform"
      },
      {
        "date": "2023-12-16T23:33:00.000Z",
        "voteCount": 1,
        "content": "Why not option D? To spin up the spark cluster on compute engine, considering rapid migration it potentially could be best approach as team wont have to re-work on model (may be only few configurational changes) and again to get data from Bigquery which is required periodically not all the time, could be easy.\nWith Dataproc it would have more code changes eventually can take more time.\nWith Vertex AI it doesn't support spark ML natively and also training would be black box.\n\nFor me Answer should be D."
      },
      {
        "date": "2023-09-23T22:01:00.000Z",
        "voteCount": 2,
        "content": "Dataproc for Spark: Google Cloud Dataproc is a managed Spark and Hadoop service that allows you to run Spark jobs seamlessly on Google Cloud. It provides the flexibility to run Spark jobs using Spark MLlib and other Spark libraries.\n\nBigQuery Integration: You mentioned that your data is being migrated to BigQuery. Dataproc has native integration with BigQuery, allowing you to read data directly from BigQuery tables. This eliminates the need to export data from BigQuery to another storage system before processing it with Spark.\n\nRapid Migration: This approach allows you to quickly migrate your existing Spark ML models and training pipelines without the need for a complete rewrite or extensive changes to your existing workflows. You can continue using your Spark ML models while adapting them to read data from BigQuery."
      },
      {
        "date": "2023-09-21T05:40:00.000Z",
        "voteCount": 1,
        "content": "they are talking about rapid lift and shift, in which case Dataproc cluster will be right one for Spark ML models for lift and shift. so I think the answer is C."
      },
      {
        "date": "2023-09-20T05:27:00.000Z",
        "voteCount": 4,
        "content": "The updated answer seems A based on the following article: \n\nhttps://cloud.google.com/blog/topics/developers-practitioners/announcing-serverless-spark-components-vertex-ai-pipelines/"
      },
      {
        "date": "2023-08-27T06:15:00.000Z",
        "voteCount": 1,
        "content": "The answer is C. Spin up a Cloud Dataproc Cluster, migrate spark jobs to there, and link the Cluster to Bgquery with the connector. It's a straightforward solution."
      },
      {
        "date": "2023-07-26T19:51:00.000Z",
        "voteCount": 3,
        "content": "If you wanted to use Vertex AI for training Spark ML models, you would typically need to convert your Spark ML code to another supported machine learning framework like TensorFlow or scikit-learn. Then you could use Vertex AI's pre-built training and prediction services for those frameworks."
      },
      {
        "date": "2023-07-16T04:54:00.000Z",
        "voteCount": 1,
        "content": "Through Vertex AI Workbench, Vertex AI is natively integrated with BigQuery, Dataproc, and Spark. You can use BigQuery ML to create and execute machine learning models in BigQuery using standard SQL queries on existing business intelligence tools and spreadsheets, or you can export datasets from BigQuery directly into Vertex AI Workbench and run your models from there. \nhttps://cloud.google.com/vertex-ai#all-features:~:text=Data%20and%20AI%20integration"
      },
      {
        "date": "2023-06-22T23:09:00.000Z",
        "voteCount": 1,
        "content": "Dataproc is a managed Spark and Hadoop service on Google Cloud, which makes it an ideal choice for migrating your existing Spark ML training pipelines. By using Dataproc, you can continue to leverage Spark and its ML capabilities without the need for significant code changes or rewriting your models.\nBy combining Dataproc and BigQuery, you can create Spark jobs or workflows in Dataproc that read data from BigQuery and train your existing Spark ML models. This approach allows you to quickly migrate your training pipelines to Google Cloud and take advantage of the scalability and performance benefits of both Dataproc and BigQuery."
      },
      {
        "date": "2023-06-22T03:01:00.000Z",
        "voteCount": 4,
        "content": "It is obviously C) Dataproc, since we don't want to rewrite the training from scratch, highly prefer Dataproc for anything Hadoop/Spark ecosystem, and Vertex AI doesn't support *training* with SparkML (but deploying existing models)."
      },
      {
        "date": "2023-06-16T00:41:00.000Z",
        "voteCount": 1,
        "content": "Use Dataproc for training existing Spark ML models, but start reading data directly from BigQuery"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 152,
    "url": "https://www.examtopics.com/discussions/google/view/17216-exam-professional-data-engineer-topic-1-question-152/",
    "body": "You work for a global shipping company. You want to train a model on 40 TB of data to predict which ships in each geographic region are likely to cause delivery delays on any given day. The model will be based on multiple attributes collected from multiple sources. Telemetry data, including location in GeoJSON format, will be pulled from each ship and loaded every hour. You want to have a dashboard that shows how many and which ships are likely to cause delays within a region. You want to use a storage solution that has native functionality for prediction and geospatial processing. Which storage solution should you use?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL for PostgreSQL"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T09:11:00.000Z",
        "voteCount": 22,
        "content": "Answer: A\nDescription: Geospatial and ML functionality is with bigquery"
      },
      {
        "date": "2020-03-22T00:05:00.000Z",
        "voteCount": 15,
        "content": "Answer : A"
      },
      {
        "date": "2024-03-04T02:49:00.000Z",
        "voteCount": 1,
        "content": "[Removed] Highly Voted  3 years, 11 months ago\nAnswer: A"
      },
      {
        "date": "2024-03-01T20:59:00.000Z",
        "voteCount": 1,
        "content": "Answer : A\nStatement \"You want to have a dashboard that shows how many and which ships are likely to cause delays within a region\" means we run analytical queries using ML. So BigQuery is Correct answer and it can able to store large volume of data"
      },
      {
        "date": "2023-09-23T22:04:00.000Z",
        "voteCount": 2,
        "content": "Here's why BigQuery is a good choice:\n\nScalable Data Storage: BigQuery is a fully managed, highly scalable data warehouse that can handle large volumes of data, including your 40 TB dataset. It allows you to store and manage your data efficiently.\n\nSQL for Predictive Analytics: BigQuery supports standard SQL and has built-in machine learning capabilities through BigQuery ML. You can easily build predictive models using SQL queries, which aligns with your goal of predicting ship delays.\n\nGeospatial Processing: BigQuery has robust support for geospatial data processing. It provides functions for working with GeoJSON and geospatial data types, making it suitable for your ship telemetry data and geospatial analysis.\n\nIntegration with Dashboards: BigQuery can be easily integrated with visualization tools like Google Data Studio or other BI tools. You can create interactive dashboards to monitor ship delays based on your model's predictions."
      },
      {
        "date": "2023-02-18T04:43:00.000Z",
        "voteCount": 1,
        "content": "Answer B: BigTable, \nCatchup words: Telemetry (sensor- semi structured data) as data is bigger than 500GB, datastore is not a good option. \nGEOJSON , bigquery has geospatical capabilites but still not quick enough for semi structure geojson data. \nPrediction for delay of ships &lt;&lt;likely to&gt;&gt; For me its time crucial and almost real time requirement. BigQuery is not suitable for it. \nBest solution for this case is: Use BigTable for storage, create a datflow pipeline / google cloud AI platform for time senstive prediction."
      },
      {
        "date": "2023-02-24T13:59:00.000Z",
        "voteCount": 4,
        "content": "answer A: You are just looking for a storage solution not a workflow"
      },
      {
        "date": "2022-12-01T05:19:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/geospatial-intro\nIn a data warehouse like BigQuery, location information is very common. Many critical business decisions revolve around location data. For example, you may record the latitude and longitude of your delivery vehicles or packages over time. You may also record customer transactions and join the data to another table with store location data.\n\nYou can use this type of location data to determine when a package is likely to arrive or to determine which customers should receive a mailer for a particular store location. Geospatial analytics let you analyze and visualize geospatial data in BigQuery by using geography data types and Google Standard SQL geography functions."
      },
      {
        "date": "2022-11-14T00:50:00.000Z",
        "voteCount": 1,
        "content": "A\nGeospatial analytics let you analyze and visualize geospatial data in BigQuery by using geography data types and Google Standard SQL geography functions. + BigqueryML"
      },
      {
        "date": "2021-11-24T21:01:00.000Z",
        "voteCount": 1,
        "content": "Answer: C"
      },
      {
        "date": "2021-11-18T07:01:00.000Z",
        "voteCount": 3,
        "content": "GeoJson + Native functionality for prediction -&gt; BigQuery"
      },
      {
        "date": "2021-11-12T00:44:00.000Z",
        "voteCount": 1,
        "content": "Answer : A"
      },
      {
        "date": "2021-10-07T02:58:00.000Z",
        "voteCount": 3,
        "content": "This is more of a question that an answer but: How much data can Bigquery handle?\n\n40TB seems to be a lot and bigtable can handle that, but of course Bigquery is better when it comes to ML and GIS."
      },
      {
        "date": "2020-08-22T10:28:00.000Z",
        "voteCount": 3,
        "content": "A is correct"
      },
      {
        "date": "2020-08-13T20:11:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://cloud.google.com/bigquery/docs/gis-intro"
      },
      {
        "date": "2020-04-02T18:30:00.000Z",
        "voteCount": 2,
        "content": "Answer A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 153,
    "url": "https://www.examtopics.com/discussions/google/view/17218-exam-professional-data-engineer-topic-1-question-153/",
    "body": "You operate an IoT pipeline built around Apache Kafka that normally receives around 5000 messages per second. You want to use Google Cloud Platform to create an alert as soon as the moving average over 1 hour drops below 4000 messages per second. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsume the stream of data in Dataflow using Kafka IO. Set a sliding time window of 1 hour every 5 minutes. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConsume the stream of data in Dataflow using Kafka IO. Set a fixed time window of 1 hour. Compute the average when the window closes, and send an alert if the average is less than 4000 messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to Bigtable. Use Cloud Scheduler to run a script every hour that counts the number of rows created in Bigtable in the last hour. If that number falls below 4000, send an alert.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Kafka Connect to link your Kafka message queue to Pub/Sub. Use a Dataflow template to write your messages from Pub/Sub to BigQuery. Use Cloud Scheduler to run a script every five minutes that counts the number of rows created in BigQuery in the last hour. If that number falls below 4000, send an alert."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-22T00:12:00.000Z",
        "voteCount": 27,
        "content": "Should be A"
      },
      {
        "date": "2020-03-25T09:22:00.000Z",
        "voteCount": 17,
        "content": "Correct: A\n\nDataflow can connect with Kafka and sliding window is used for taking averages"
      },
      {
        "date": "2024-03-04T09:28:00.000Z",
        "voteCount": 2,
        "content": "Option A is correct answer.\nOption B is not correct. There could be a chance middle of 1st window to middle of 2nd window less messages(i.e &gt; 4000).\nOption C &amp; D out of scope."
      },
      {
        "date": "2023-09-24T07:41:00.000Z",
        "voteCount": 2,
        "content": "Dataflow with Sliding Time Windows: Dataflow allows you to work with event-time windows, making it suitable for time-series data like incoming IoT messages. Using sliding windows every 5 minutes allows you to compute moving averages efficiently.\n\nSliding Time Window: The sliding time window of 1 hour every 5 minutes enables you to calculate the moving average over the specified time frame.\n\nComputing Averages: You can efficiently compute the average when each sliding window closes. This approach ensures that you have real-time visibility into the message rate and can detect deviations from the expected rate.\n\nAlerting: When the calculated average drops below 4000 messages per second, you can trigger an alert from within the Dataflow pipeline, sending it to your desired alerting mechanism, such as Cloud Monitoring, Pub/Sub, or another notification service.\n\nScalability: Dataflow can scale automatically based on the incoming data volume, ensuring that you can handle the expected rate of 5000 messages per second."
      },
      {
        "date": "2023-07-25T20:12:00.000Z",
        "voteCount": 2,
        "content": "Option A\n\nPros:\n\nThis option is relatively simple to implement.\nIt can be used to compute the moving average over any time window.\nCons:\n\nThis option can be computationally expensive, especially if the data stream is large.\nIt can be difficult to troubleshoot if the alert does not fire when it is supposed to."
      },
      {
        "date": "2023-05-10T04:54:00.000Z",
        "voteCount": 2,
        "content": "the correct answer is between A and B since it doesn't make sense to use Pub/Sub combined with Kafka. To have a Moving Average then we should go for A, updating the average estimation every 5 minutes using the new data that came in and eliminating the \"most far\" 5 minutes."
      },
      {
        "date": "2022-12-01T05:15:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#windows\nWindowing functions divide unbounded collections into logical components, or windows. Windowing functions group unbounded collections by the timestamps of the individual elements. Each window contains a finite number of elements.\n\nYou set the following windows with the Apache Beam SDK or Dataflow SQL streaming extensions:\n- Hopping windows (called sliding windows in Apache Beam)\n\nA hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint.\n\nFor example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the period. This example has a one-minute window and thirty-second period."
      },
      {
        "date": "2022-01-09T03:44:00.000Z",
        "voteCount": 2,
        "content": "as explained by Alasmindas"
      },
      {
        "date": "2021-12-11T05:25:00.000Z",
        "voteCount": 2,
        "content": "Correct Answer: A"
      },
      {
        "date": "2021-11-24T21:09:00.000Z",
        "voteCount": 1,
        "content": "Correct: A"
      },
      {
        "date": "2021-10-01T07:20:00.000Z",
        "voteCount": 1,
        "content": "A is enough"
      },
      {
        "date": "2021-02-19T13:33:00.000Z",
        "voteCount": 2,
        "content": "A:\nthe correct answer is between A and B, But because used \"Moving Average\" then we should go for A."
      },
      {
        "date": "2020-12-31T22:48:00.000Z",
        "voteCount": 2,
        "content": "yes , using KafkaIO , we can connect to Kafka cluster."
      },
      {
        "date": "2020-12-23T21:37:00.000Z",
        "voteCount": 3,
        "content": "yes A is correct , because sliding window can only help here."
      },
      {
        "date": "2020-11-13T20:58:00.000Z",
        "voteCount": 7,
        "content": "Option A is the correct answer. Reasons:-\na) Kafka IO and Dataflow is a valid option for interconnect (needless where Kafka is located - On Prem/Google Cloud/Other cloud) \nb) Sliding Window will help to calculate average.\n\nOption C and D are overkill and complex, considering the scenario in the question,\nhttps://cloud.google.com/solutions/processing-messages-from-kafka-hosted-outside-gcp"
      },
      {
        "date": "2020-11-13T20:41:00.000Z",
        "voteCount": 6,
        "content": "Option A is the correct answer. Reasons:-\na) Kafka IO and Dataflow is a valid option for interconnect (needless where Kafka is located - On Prem/Google Cloud/Other cloud) \nb) Sliding Window will help to calculate average.\n\nOption C and D are overkill and complex, considering the scenario in the question,"
      },
      {
        "date": "2020-08-28T09:37:00.000Z",
        "voteCount": 2,
        "content": "A\nTo take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 154,
    "url": "https://www.examtopics.com/discussions/google/view/16688-exam-professional-data-engineer-topic-1-question-154/",
    "body": "You plan to deploy Cloud SQL using MySQL. You need to ensure high availability in the event of a zone failure. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL instance in one zone, and create a failover replica in another zone within the same region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL instance in one zone, and create a read replica in another zone within the same region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL instance in one zone, and configure an external read replica in a zone in a different region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL instance in a region, and configure automatic backup to a Cloud Storage bucket in the same region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-15T11:17:00.000Z",
        "voteCount": 31,
        "content": "A should be correct answer"
      },
      {
        "date": "2021-12-18T08:52:00.000Z",
        "voteCount": 5,
        "content": "yes A is correct, whe creating ne cloud sql instance there is an option\n \"Multiple zones (Highly available)\nAutomatic failover to another zone within your selected region. Recommended for production instances. Increases cost.\""
      },
      {
        "date": "2020-03-25T09:26:00.000Z",
        "voteCount": 14,
        "content": "Correct: A\n\nhttps://cloud.google.com/sql/docs/mysql/high-availability"
      },
      {
        "date": "2024-03-01T21:05:00.000Z",
        "voteCount": 2,
        "content": "Answer : A\nQuestion is about high availability in the event of zone failure. So create Fail over replica in another zone in same region."
      },
      {
        "date": "2023-12-19T00:33:00.000Z",
        "voteCount": 5,
        "content": "A (failover replicas) as this is an old question:\n\nIn a legacy HA configuration, a Cloud SQL for MySQL instance uses a failover replica to add high availability to the instance. This functionality isn't available in Google Cloud console. \n\nThe new configuration doesn't use failover replicas. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block-level between two zones in a region.\nhttps://cloud.google.com/sql/docs/mysql/configure-legacy-ha"
      },
      {
        "date": "2023-11-21T13:16:00.000Z",
        "voteCount": 1,
        "content": "Option A is good fro leagacy soultion \nNote: Cloud SQL plans to discontinue support for legacy HA instances in the future and will soon be announcing a date to do so. Currently, legacy HA instances are still covered by the Cloud SQL SLA. We recommend you upgrade your existing legacy HA instances to regional persistent disk HA instances and create new instances using regional persistent disk HA as soon as possible\nOption C makes more sense in this regrard"
      },
      {
        "date": "2023-11-14T11:29:00.000Z",
        "voteCount": 1,
        "content": "A - Although it is legacy and will be deprecated. The correct answer is not an option--\n\"The legacy configuration for high availability used a failover replica instance. The new configuration does not use a failover replica. Instead, it uses Google's regional persistent disks, which synchronously replicate data at the block level between two zones in a region.\""
      },
      {
        "date": "2023-09-24T07:46:00.000Z",
        "voteCount": 1,
        "content": "Failover Replica: By creating a failover replica in another zone within the same region, you establish a high-availability configuration. The failover replica is kept in sync with the primary instance, and it can quickly take over in case of a failure of the primary instance.\n\nSame Region: Placing the failover replica in the same region ensures minimal latency and data consistency. In the event of a zone failure, the failover can happen within the same region, reducing potential downtime.\n\nZone Resilience: Google Cloud's regional design ensures that zones within a region are independent of each other, which adds resilience to zone failures.\n\nAutomatic Failover: In case of a primary instance failure, Cloud SQL will automatically promote the failover replica to become the new primary instance, minimizing downtime."
      },
      {
        "date": "2023-08-24T10:05:00.000Z",
        "voteCount": 1,
        "content": "Per latest Google cloud document, B is the correct answer."
      },
      {
        "date": "2023-07-16T05:23:00.000Z",
        "voteCount": 3,
        "content": "Cross-region read replicas\nCross-region replication lets you create a read replica in a different region from the primary instance. You create a cross-region read replica the same way as you create an in-region replica.\n\nCross-region replicas:\n\nImprove read performance by making replicas available closer to your application's region.\nProvide additional disaster recovery capability to guard against a regional failure.\nLet you migrate data from one region to another.\nhttps://cloud.google.com/sql/docs/mysql/replication#cross-region-read-replicas:~:text=memory%20(OOM)%20events.-,Cross%2Dregion%20read%20replicas,Let%20you%20migrate%20data%20from%20one%20region%20to%20another.,-See%20Promoting%20replicas"
      },
      {
        "date": "2023-07-08T08:57:00.000Z",
        "voteCount": 2,
        "content": "The legacy process for adding high availability to MySQL instances uses a failover replica. The legacy functionality isn't available in the Google Cloud console. See Legacy configuration: Creating a new instance configured for high availability or Legacy configuration: Configuring an existing instance for high availability."
      },
      {
        "date": "2023-07-02T20:48:00.000Z",
        "voteCount": 4,
        "content": "The correct answer is most probably B as this his scenario has an update(As of July 2023). Failover replicas are not available anymore. Same region different zone read replicas are used in case of a failover or if primary zone is not available"
      },
      {
        "date": "2023-05-20T11:59:00.000Z",
        "voteCount": 2,
        "content": "The answer is B, the failover replica is a legacy feature. \nSee here: https://cloud.google.com/sql/docs/mysql/high-availability#legacy_mysql_high_availability_option"
      },
      {
        "date": "2023-06-01T05:54:00.000Z",
        "voteCount": 1,
        "content": "Read replica isn't an alternative to the standby instance"
      },
      {
        "date": "2023-05-10T05:03:00.000Z",
        "voteCount": 2,
        "content": "read replica (B) and external read replica (C) doesn't make sense here, since we potentially need all the functionalities. Using Cloud SQL in a region combined with Cloud Storage backup may not be the best choice (D) thinking about compliance reasons starting from what has been asked, it seems also \"too much\" compared with A that fullfills the request with simpler actions. Also, compliance is required at the regional level, so then A fits."
      },
      {
        "date": "2023-03-08T06:21:00.000Z",
        "voteCount": 6,
        "content": "Failover replica's are a legacy feature. This question is outdated: https://cloud.google.com/sql/docs/mysql/configure-ha"
      },
      {
        "date": "2023-02-18T05:17:00.000Z",
        "voteCount": 2,
        "content": "Answer A, key words to remember, High Scale use extra read replica. High availablity use extra failure replica. Both should be in different zone but in same region."
      },
      {
        "date": "2023-01-26T17:39:00.000Z",
        "voteCount": 2,
        "content": "Answer is B: https://cloud.google.com/sql/docs/mysql/replication#read-replicas\n\n'As a best practice, put read replicas in a different zone than the primary instance when you use HA on your primary instance'"
      },
      {
        "date": "2023-01-26T17:40:00.000Z",
        "voteCount": 1,
        "content": "The questions asks to ensure high availability in the event of a zone failure"
      },
      {
        "date": "2022-12-01T05:12:00.000Z",
        "voteCount": 1,
        "content": "A is the answer.\n\nhttps://cloud.google.com/sql/docs/mysql/high-availability#HA-configuration\nThe HA configuration provides data redundancy. A Cloud SQL instance configured for HA is also called a regional instance and has a primary and secondary zone within the configured region. Within a regional instance, the configuration is made up of a primary instance and a standby instance. Through synchronous replication to each zone's persistent disk, all writes made to the primary instance are replicated to disks in both zones before a transaction is reported as committed. In the event of an instance or zone failure, the standby instance becomes the new primary instance. Users are then rerouted to the new primary instance. This process is called a failover."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 155,
    "url": "https://www.examtopics.com/discussions/google/view/80517-exam-professional-data-engineer-topic-1-question-155/",
    "body": "Your company is selecting a system to centralize data ingestion and delivery. You are considering messaging and data integration systems to address the requirements. The key requirements are:<br>\u2711 The ability to seek to a particular offset in a topic, possibly back to the start of all data ever captured<br>\u2711 Support for publish/subscribe semantics on hundreds of topics<br><br>Retain per-key ordering -<br><img src=\"/assets/media/exam-media/04341/0010500003.png\" class=\"in-exam-image\"><br>Which system should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tApache Kafka\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Storage",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDataflow",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFirebase Cloud Messaging"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-05T23:57:00.000Z",
        "voteCount": 10,
        "content": "A I think it's the only technology that met the requirements"
      },
      {
        "date": "2022-09-22T06:49:00.000Z",
        "voteCount": 7,
        "content": "vote for A: topics, offsets --&gt; apache kafka"
      },
      {
        "date": "2024-03-04T09:30:00.000Z",
        "voteCount": 1,
        "content": "Only Kafka can support publish/subscribe semantics on hundreds of topics"
      },
      {
        "date": "2023-09-24T07:56:00.000Z",
        "voteCount": 4,
        "content": "Ability to Seek to a Particular Offset: Kafka allows consumers to seek to a specific offset in a topic, enabling you to read data from a specific point, including back to the start of all data ever captured. This is a fundamental capability of Kafka.\n\nSupport for Publish/Subscribe Semantics: Kafka supports publish/subscribe semantics through topics. You can have hundreds of topics in Kafka, and consumers can subscribe to these topics to receive messages in a publish/subscribe fashion.\n\nRetain Per-Key Ordering: Kafka retains the order of messages within a partition. If you have a key associated with your messages, you can ensure per-key ordering by sending messages with the same key to the same partition.\n\nScalability: Kafka is designed to handle high-throughput data streaming and is capable of scaling to meet your needs.\n\nApache Kafka aligns well with the requirements you've outlined for centralized data ingestion and delivery. It's a robust choice for scenarios that involve data streaming, publish/subscribe, and retaining message ordering."
      },
      {
        "date": "2023-02-18T05:30:00.000Z",
        "voteCount": 2,
        "content": "Answer A: Apache Kafka\nKey words: Ingestion and Delivery together ( it is combination of pub/sub for ingestion, and delivery = dataflow+any database in gcp)\nOffset of a topic = Partition of a topic and reprocess specific part of topic, its not possible in pub/sub as it is designed for as come and go for 1 topic. \nPer key ordering. means message with same key can be process or assigned to a user in kafka."
      },
      {
        "date": "2022-09-26T18:58:00.000Z",
        "voteCount": 1,
        "content": "deberia ser la C, debido a que siempre es mejor escoger los servicios de google"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 156,
    "url": "https://www.examtopics.com/discussions/google/view/17211-exam-professional-data-engineer-topic-1-question-156/",
    "body": "You are planning to migrate your current on-premises Apache Hadoop deployment to the cloud. You need to ensure that the deployment is as fault-tolerant and cost-effective as possible for long-running batch jobs. You want to use a managed service. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataproc cluster. Use a standard persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy a Dataproc cluster. Use an SSD persistent disk and 50% preemptible workers. Store data in Cloud Storage, and change references in scripts from hdfs:// to gs://",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Hadoop and Spark on a 10-node Compute Engine instance group with standard instances. Install the Cloud Storage connector, and store the data in Cloud Storage. Change references in scripts from hdfs:// to gs://",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Hadoop and Spark on a 10-node Compute Engine instance group with preemptible instances. Store data in HDFS. Change references in scripts from hdfs:// to gs://"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-25T09:34:00.000Z",
        "voteCount": 33,
        "content": "Correct: A\n\nAsk for cost effective so persistent disk are HDD which are cheaper in comparison to SSD."
      },
      {
        "date": "2020-03-21T23:31:00.000Z",
        "voteCount": 16,
        "content": "Confused between A and B. For r/w intensive jobs need to use SSDs. But questions doesnt state anything about the nature of the jobs. So better to start with a default option.\nChoose A"
      },
      {
        "date": "2021-12-21T04:30:00.000Z",
        "voteCount": 3,
        "content": "\"You need to ensure that the deployment is as cost-effective as possible\"\nhence, no SSD unless stated otherwise"
      },
      {
        "date": "2024-03-04T09:33:00.000Z",
        "voteCount": 2,
        "content": "Options A is the right answer. \nOption B using SSD persistent disk which will add more cost than default HDD\nOption C &amp; D are out of scope."
      },
      {
        "date": "2023-09-24T08:01:00.000Z",
        "voteCount": 2,
        "content": "Dataproc Managed Service: Dataproc is a fully managed service for running Apache Hadoop and Spark. It provides ease of management and automation.\n\nStandard Persistent Disk: Using standard persistent disks for Dataproc workers ensures durability and is cost-effective compared to SSDs.\n\nPreemptible Workers: By using 50% preemptible workers, you can significantly reduce costs while maintaining fault tolerance. Preemptible VMs are cheaper but can be preempted by Google, so having a mix of preemptible and non-preemptible workers provides cost savings with redundancy.\n\nStoring Data in Cloud Storage: Storing data in Cloud Storage is highly durable, scalable, and cost-effective. It also makes data accessible to Dataproc clusters, and you can leverage native connectors for reading data from Cloud Storage.\n\nChanging References to gs://: Updating your scripts to reference data in Cloud Storage using gs:// ensures that your jobs work seamlessly with the cloud storage infrastructure."
      },
      {
        "date": "2023-05-10T05:29:00.000Z",
        "voteCount": 1,
        "content": "Apache Hadoop -&gt; Dataproc or Compute Engine with proper SW installation\ncost-effective -&gt; use standard persistent disk + store data in Cloud Storage\nbatch -&gt; Dataproc or Compute Engine with proper SW installation\nmanaged service -&gt; Dataproc"
      },
      {
        "date": "2022-11-30T07:01:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/overview\nDataproc is a managed Spark and Hadoop service that lets you take advantage of open source data tools for batch processing, querying, streaming, and machine learning. Dataproc automation helps you create clusters quickly, manage them easily, and save money by turning clusters off when you don't need them. With less time and money spent on administration, you can focus on your jobs and your data."
      },
      {
        "date": "2022-09-10T23:33:00.000Z",
        "voteCount": 1,
        "content": "it says cost effective , hence no SSD"
      },
      {
        "date": "2021-11-24T21:17:00.000Z",
        "voteCount": 2,
        "content": "Correct: A"
      },
      {
        "date": "2021-07-17T05:14:00.000Z",
        "voteCount": 2,
        "content": "Correct : A\nOption B is usefull if you use HDFS, and in this case as you use preemtible machines it isn't worth use SSD disks."
      },
      {
        "date": "2021-02-17T12:08:00.000Z",
        "voteCount": 1,
        "content": "Answer - B"
      },
      {
        "date": "2021-01-13T19:52:00.000Z",
        "voteCount": 5,
        "content": "Look at this link. https://cloud.google.com/bigtable/docs/choosing-ssd-hdd\nAt the First look I chose Option-B as they mentioned SSD is cost-effective on most cases. But after reading the whole page, they also mentioned that for batch workloads, HDD is suggested as long as not heavy read. So I changed my mind to Option-A (I assumed this is not ready heavy process?)."
      },
      {
        "date": "2022-08-06T20:01:00.000Z",
        "voteCount": 1,
        "content": "Caution about the link you provided as reference. It's intendedfor BigTable which is GC's low-latency solution which is totally different requirement. Mentioning only because on first read I thought SSD is the obvious choice.\nPer below link, SSD may not be required unless there is a low-latency requirement or a high I/O requirement. Since the question does not specify anything like that, A looks correct.\nhttps://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc"
      },
      {
        "date": "2020-11-13T21:07:00.000Z",
        "voteCount": 4,
        "content": "Option B - SSD disks, reasons:-\nThe question asks \"fault-tolerant and cost-effective as possible for long-running batch job\". \n3 Key words are - fault tolerant / cost effective / long running batch jobs..\n\nThe cost efficiency part mentioned in the question could be addressed by 50% preemptible disks and  storing the data in cloud storage than HDFS. \nFor long running batch jobs and as standard approach for Dataproc - we should always go with SSD disk types as per google recommendations."
      },
      {
        "date": "2020-12-08T18:10:00.000Z",
        "voteCount": 2,
        "content": "where is the proof...show me the link?"
      },
      {
        "date": "2021-03-04T03:03:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/solutions/migration/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc\nAs per this, SSD is only recommended if it is high IO intensive. In this question no where mentioned its high IO intensive, and asks for cost effective (as much as possible), so no need to use SSD. \nI will go with A."
      },
      {
        "date": "2020-08-18T21:39:00.000Z",
        "voteCount": 2,
        "content": "Ans is B, for long running SDD suitable. HDD maintenance will be additional charge for long running jobs"
      },
      {
        "date": "2020-07-06T23:45:00.000Z",
        "voteCount": 7,
        "content": "Answer is A\u2026Cloud Dataproc for Managed Cloud native application and HDD for cost-effective solution."
      },
      {
        "date": "2020-04-02T22:03:00.000Z",
        "voteCount": 5,
        "content": "Answer A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 157,
    "url": "https://www.examtopics.com/discussions/google/view/17212-exam-professional-data-engineer-topic-1-question-157/",
    "body": "Your team is working on a binary classification problem. You have trained a support vector machine (SVM) classifier with default parameters, and received an area under the Curve (AUC) of 0.87 on the validation set. You want to increase the AUC of the model. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPerform hyperparameter tuning\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain a classifier with deep neural networks, because neural networks would always beat SVMs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the model and measure the real-world AUC; it's always higher because of generalization",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScale predictions you get out of the model (tune a scaling factor as a hyperparameter) in order to get the highest AUC"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-06-09T18:17:00.000Z",
        "voteCount": 41,
        "content": "Seems to be A. Preprocessing/scaling should be done with input features, instead of predictions (output)"
      },
      {
        "date": "2020-08-13T22:54:00.000Z",
        "voteCount": 11,
        "content": "A\nDeep LEarning is not always the best solution \nD talks about fudgin the output which is wrong"
      },
      {
        "date": "2023-12-19T00:53:00.000Z",
        "voteCount": 3,
        "content": "https://www.quora.com/How-can-I-improve-Precision-Recall-AUC-under-Imbalanced-Classification"
      },
      {
        "date": "2023-05-10T05:36:00.000Z",
        "voteCount": 2,
        "content": "B,C are simply not true. D is modifing the scoring, making it not realiable anymore. A makes sense, is potentially increasing the model accuracy."
      },
      {
        "date": "2023-05-06T20:30:00.000Z",
        "voteCount": 1,
        "content": "a is the correct answer"
      },
      {
        "date": "2023-02-18T05:40:00.000Z",
        "voteCount": 1,
        "content": "Answer A, \nwhy not B, Deep Neu Net. are better for sure but AUC is 0.87 is already good. Don't go for complex and time taking model. If AUC more than 0.95, it can be a reason of overfit. \nNow just check SVM params for hypertuning if you can bring it close to 0,9-0,95"
      },
      {
        "date": "2023-01-27T05:26:00.000Z",
        "voteCount": 1,
        "content": "a is the correct answer"
      },
      {
        "date": "2022-11-06T19:45:00.000Z",
        "voteCount": 1,
        "content": "Also a good read is: https://cloud.google.com/ai-platform/training/docs/hyperparameter-tuning-overview"
      },
      {
        "date": "2022-01-09T04:01:00.000Z",
        "voteCount": 2,
        "content": "as mentioned by Spider7 \"performing tuning rather than using the model default parameters there's a way to increase the overall model performance --&gt; A.\""
      },
      {
        "date": "2021-11-24T21:19:00.000Z",
        "voteCount": 1,
        "content": "Correct: A"
      },
      {
        "date": "2021-11-12T13:03:00.000Z",
        "voteCount": 3,
        "content": "0.89 it's already not bad but by performing tuning rather then using the model default parameters there's a way to increase the overall model performance --&gt; A."
      },
      {
        "date": "2021-11-12T13:06:00.000Z",
        "voteCount": 1,
        "content": "0.87 precisely"
      },
      {
        "date": "2021-07-28T02:18:00.000Z",
        "voteCount": 3,
        "content": "Not C because real-world AUC value falls between 0.5 and 1.0 usually, this wouldn't help.\n\nA seems the most straigh forward."
      },
      {
        "date": "2021-03-21T12:41:00.000Z",
        "voteCount": 1,
        "content": "For a large enough training set DNN will most likely beat a SVM. However the opposite may or may not be true. It also depends on the complexity of the problem. Which we don\u2019t know from the question. For image, nlp, I say B can be a good answer\nHowever, if we decide to stick with SVM, D reduces overfitting and may increase AUC.\nI am torn between the two!"
      },
      {
        "date": "2021-02-17T12:21:00.000Z",
        "voteCount": 1,
        "content": "Ans - D when the model is overfitted means want to increase the AUC, we always perform hyperparameter tuning, Increase regularisations, decrease input feature parameters etc."
      },
      {
        "date": "2020-10-18T23:18:00.000Z",
        "voteCount": 2,
        "content": "AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values. So answer shall be A\nhttps://developers.google.com/machine-learning/crash-course/classification/roc-and-auc?hl=en"
      },
      {
        "date": "2020-10-15T23:07:00.000Z",
        "voteCount": 3,
        "content": "Definitely not D\nhttps://developers.google.com/machine-learning/crash-course/classification/check-your-understanding-roc-and-auc"
      },
      {
        "date": "2020-08-18T10:13:00.000Z",
        "voteCount": 4,
        "content": "A for me, read below link for more details.\n\nhttps://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 158,
    "url": "https://www.examtopics.com/discussions/google/view/16899-exam-professional-data-engineer-topic-1-question-158/",
    "body": "You need to deploy additional dependencies to all nodes of a Cloud Dataproc cluster at startup using an existing initialization action. Company security policies require that Cloud Dataproc nodes do not have access to the Internet so public initialization actions cannot fetch resources. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy the Cloud SQL Proxy on the Cloud Dataproc master",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an SSH tunnel to give the Cloud Dataproc cluster access to the Internet",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy all dependencies to a Cloud Storage bucket within your VPC security perimeter\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Resource Manager to add the service account used by the Cloud Dataproc cluster to the Network User role"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-25T09:39:00.000Z",
        "voteCount": 39,
        "content": "Correct: C\n\nIf you create a Dataproc cluster with internal IP addresses only, attempts to access the Internet in an initialization action will fail unless you have configured routes to direct the traffic through a NAT or a VPN gateway. Without access to the Internet, you can enable Private Google Access, and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs."
      },
      {
        "date": "2022-12-31T11:27:00.000Z",
        "voteCount": 1,
        "content": "Thank you for detailed explanation. C is right"
      },
      {
        "date": "2020-03-17T17:46:00.000Z",
        "voteCount": 12,
        "content": "Should be C:\n\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/init-actions"
      },
      {
        "date": "2024-03-18T01:32:00.000Z",
        "voteCount": 1,
        "content": "c looks good"
      },
      {
        "date": "2023-09-24T08:16:00.000Z",
        "voteCount": 3,
        "content": "Security Compliance: This option aligns with your company's security policies, which prohibit public Internet access from Cloud Dataproc nodes. Placing the dependencies in a Cloud Storage bucket within your VPC security perimeter ensures that the data remains within your private network.\n\nVPC Security: By placing the dependencies within your VPC security perimeter, you maintain control over network access and can restrict access to the necessary nodes only.\n\nDataproc Initialization Action: You can use a custom initialization action or script to fetch and install the dependencies from the secure Cloud Storage bucket to the Dataproc cluster nodes during startup.\n\nBy copying the dependencies to a secure Cloud Storage bucket and using an initialization action to install them on the Dataproc nodes, you can meet your security requirements while providing the necessary dependencies to your cluster."
      },
      {
        "date": "2023-08-06T02:49:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-03-13T12:49:00.000Z",
        "voteCount": 1,
        "content": "C seems good"
      },
      {
        "date": "2023-02-18T05:56:00.000Z",
        "voteCount": 2,
        "content": "Answer C,\nIt needs practical experience to understand this question. You create cluster with some package/software i.e dependencies such as python packages that you store in .zip file, then you save a jar file to run the cluster as an application such as you need java while running spark session. and some config yaml file. \nThese dependencies you can save in bucket and can use to configure cluster from external window , sdk or api. without going into UI. \nThen you need to use VPC to access these files"
      },
      {
        "date": "2022-11-30T06:54:00.000Z",
        "voteCount": 1,
        "content": "C is the answer.\n\nhttps://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#and_vpc-sc_networks\nWith VPC Service Controls, administrators can define a security perimeter around resources of Google-managed services to control communication to and between those services."
      },
      {
        "date": "2022-07-21T00:01:00.000Z",
        "voteCount": 1,
        "content": "Without access to the internet, you can enable Private Google Access and place job dependencies in Cloud Storage; cluster nodes can download the dependencies from Cloud Storage from internal IPs."
      },
      {
        "date": "2022-01-09T04:03:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only"
      },
      {
        "date": "2021-12-22T17:17:00.000Z",
        "voteCount": 3,
        "content": "When creating a Dataproc cluster, you can specify initialization actions in executables or scripts that Dataproc will run on all nodes in your Dataproc cluster immediately after the cluster is set up. Initialization actions often set up job dependencies, such as installing Python packages, so that jobs can be submitted to the cluster without having to install dependencies when the jobs are run"
      },
      {
        "date": "2021-11-24T21:23:00.000Z",
        "voteCount": 1,
        "content": "Correct: C"
      },
      {
        "date": "2020-08-07T19:00:00.000Z",
        "voteCount": 2,
        "content": "c it is!"
      },
      {
        "date": "2020-04-02T22:08:00.000Z",
        "voteCount": 2,
        "content": "Should be C"
      },
      {
        "date": "2020-03-21T23:40:00.000Z",
        "voteCount": 2,
        "content": "Should be C"
      },
      {
        "date": "2020-03-18T01:17:00.000Z",
        "voteCount": 4,
        "content": "I think the correct answer might be C instead, due to https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/network#create_a_cloud_dataproc_cluster_with_internal_ip_address_only"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 159,
    "url": "https://www.examtopics.com/discussions/google/view/17213-exam-professional-data-engineer-topic-1-question-159/",
    "body": "You need to choose a database for a new project that has the following requirements:<br>\u2711 Fully managed<br>\u2711 Able to automatically scale up<br>\u2711 Transactionally consistent<br>\u2711 Able to scale up to 6 TB<br>\u2711 Able to be queried using SQL<br>Which database do you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2020-03-25T09:45:00.000Z",
        "voteCount": 33,
        "content": "Correct: A\nIt asks for scaling up which can be done in cloud sql, horizontal scaling is not possible in cloud sql\nAutomatic storage increase\nIf you enable this setting, Cloud SQL checks your available storage every 30 seconds. If the available storage falls below a threshold size, Cloud SQL automatically adds additional storage capacity. If the available storage repeatedly falls below the threshold size, Cloud SQL continues to add storage until it reaches the maximum of 30 TB."
      },
      {
        "date": "2020-08-28T06:54:00.000Z",
        "voteCount": 8,
        "content": "C - CloudSQL does not scale automatically."
      },
      {
        "date": "2020-09-28T05:33:00.000Z",
        "voteCount": 5,
        "content": "Cloud SQL can automatically scale up storage capacity when you are near your limit"
      },
      {
        "date": "2022-10-09T11:53:00.000Z",
        "voteCount": 4,
        "content": "it does not say about type of scaling, Cloud SQL scale up automatically with storage, that should works"
      },
      {
        "date": "2020-07-06T14:04:00.000Z",
        "voteCount": 6,
        "content": "C:- Cloud SQL is not fully managed as that is one of the requirement."
      },
      {
        "date": "2021-05-11T10:24:00.000Z",
        "voteCount": 15,
        "content": "Have you really worked on GCP?"
      },
      {
        "date": "2020-09-28T05:31:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql it is fully managed"
      },
      {
        "date": "2020-12-20T09:48:00.000Z",
        "voteCount": 8,
        "content": "Google documentation says \"Cloud SQL is a fully-managed database service that helps you set up, maintain, manage, and administer your relational databases on Google Cloud Platform.\""
      },
      {
        "date": "2022-12-31T11:29:00.000Z",
        "voteCount": 1,
        "content": "A. Cloud SQL There is no need of Spanner"
      },
      {
        "date": "2020-03-21T23:42:00.000Z",
        "voteCount": 13,
        "content": "Should be C."
      },
      {
        "date": "2020-03-22T19:14:00.000Z",
        "voteCount": 8,
        "content": "May be A"
      },
      {
        "date": "2024-10-01T04:28:00.000Z",
        "voteCount": 1,
        "content": "All the arguments below can essentially be boiled down to two questions. 1) is Cloud SQL fully managed? (yes), 2) Does it autoscale? It depends, is the answer. The question is horrifyingly worded, as it comes down to an ambiguity coinflip. I'm going with C, it feels like it's a better fit."
      },
      {
        "date": "2024-08-10T06:57:00.000Z",
        "voteCount": 1,
        "content": "Did not refer to global, thus not spanner. A is correct"
      },
      {
        "date": "2024-07-14T00:47:00.000Z",
        "voteCount": 1,
        "content": "Transactional consistency: spanner provides strong consistency across rows, regions, and continents."
      },
      {
        "date": "2024-03-01T21:17:00.000Z",
        "voteCount": 1,
        "content": "Answer : A\nCloud SQL and Cloud Spanner are the options for the questions. But here as per requirement they don't need horizontal scaling, they want manager SQL instance and it should support 6 TB of storage. Cloud SQL can support up to 64 TB of storage\nhttps://cloud.google.com/sql/docs/quotas#:~:text=Cloud%20SQL%20storage%20limits,core%3A%20Up%20to%203%20TB."
      },
      {
        "date": "2024-02-15T11:37:00.000Z",
        "voteCount": 1,
        "content": "Not horizontal scaling is required, cloud SQL will work for 10 TB"
      },
      {
        "date": "2024-02-07T21:10:00.000Z",
        "voteCount": 1,
        "content": "i will go with A, since the question doesnt specifically say it should be scaling horizontally"
      },
      {
        "date": "2023-11-28T11:29:00.000Z",
        "voteCount": 1,
        "content": "\"Able to scale up to 6 TB\" -seems to be the key \nit looks like autoscaling is related to storage - possible in case of Cloud SQL"
      },
      {
        "date": "2023-12-19T23:25:00.000Z",
        "voteCount": 1,
        "content": "no way , u can automaticly scale the Cloud SQL , please read the documents of Cloud SQL, Spanner is the solution ."
      },
      {
        "date": "2023-11-20T03:09:00.000Z",
        "voteCount": 1,
        "content": "Spanner is consistent and fully-managed \nhttps://cloud.google.com/spanner/docs/transactions?hl=en"
      },
      {
        "date": "2023-11-18T08:01:00.000Z",
        "voteCount": 1,
        "content": "A seems to be correct because of the scaling factor of 6 TB because cloud sql easily supports up to 40 TB and obviously there is a limitation of GLOBALLY MULTI REGIONAL which is nothing to do with question. Hence A seems more closer."
      },
      {
        "date": "2023-10-25T04:07:00.000Z",
        "voteCount": 2,
        "content": "Guys - this is 1000% C. \".....automatically scale up\" Cloud SQL needs restart! this is not the solution. Only Spanner (and BQ) are true automatically scale up"
      },
      {
        "date": "2023-12-01T03:06:00.000Z",
        "voteCount": 1,
        "content": "What problem in restart? There are no any constraints about this, but there directly writes: \"Fully managed\""
      },
      {
        "date": "2023-10-03T06:36:00.000Z",
        "voteCount": 1,
        "content": "C: Cloud Spanner\n\nWhy not A: Cloud SQL?\nNo \"automatically\" scale up feature"
      },
      {
        "date": "2023-09-24T08:23:00.000Z",
        "voteCount": 1,
        "content": "Fully Managed: Cloud Spanner is a fully managed database service provided by Google Cloud, which means you don't have to worry about managing infrastructure, updates, or backups.\n\nAutomatic Scaling: Cloud Spanner can automatically scale both horizontally and vertically to handle increased workloads and data volume. It can handle databases ranging from a few gigabytes to multi-terabyte scale.\n\nTransactionally Consistent: Cloud Spanner offers strong transactional consistency, making it suitable for applications that require ACID compliance.\n\nSQL Querying: You can query Cloud Spanner databases using SQL, which is a familiar query language for many developers and analysts.\n\nGiven your requirements, Cloud Spanner is designed to meet the need for a fully managed, scalable, transactionally consistent, and SQL-accessible database solution."
      },
      {
        "date": "2023-08-27T07:47:00.000Z",
        "voteCount": 1,
        "content": "Able to automatically scale up = Memory and CPU\nAble to scale up to 6 TB = Disk\nCorrect is C, because Cloud SQL no scale Memory and CPU."
      },
      {
        "date": "2023-07-16T06:17:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL can also automatically scale up storage capacity when you are near your limit.\nhttps://cloud.google.com/sql/?utm_source=google&amp;utm_medium=cpc&amp;utm_campaign=japac-HK-all-en-dr-BKWS-all-super-trial-PHR-dr-1605216&amp;utm_content=text-ad-none-none-DEV_c-CRE_652279903175-ADGP_Hybrid%20%7C%20BKWS%20-%20BRO%20%7C%20Txt%20~%20Databases_Cloud%20SQL_gcp%20horizontal%20sql_main-KWID_43700076522535474-kwd-2005739330106&amp;userloc_2344-network_g&amp;utm_term=KW_google%20sql%20horizontal&amp;gclid=Cj0KCQjwqs6lBhCxARIsAG8YcDjWaS5NbG3remUHnHQ7EFK-wJNsF0I_IvePKHF0mHaiBK3_-eM7Z-UaAqQeEALw_wcB&amp;gclsrc=aw.ds#section-2:~:text=Cloud%20SQL%20can%20also%20automatically%20scale%20up%20storage%20capacity%20when%20you%20are%20near%20your%20limit."
      },
      {
        "date": "2023-06-20T05:14:00.000Z",
        "voteCount": 1,
        "content": "The requirements does not mention high-availability, and Cloud SQL is up to 64TB now \nhttps://cloud.google.com/sql/docs/quotas#storage_limits"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 160,
    "url": "https://www.examtopics.com/discussions/google/view/17214-exam-professional-data-engineer-topic-1-question-160/",
    "body": "You work for a mid-sized enterprise that needs to move its operational system transaction data from an on-premises database to GCP. The database is about 20<br>TB in size. Which database should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud SQL\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Bigtable",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Spanner",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCloud Datastore"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-03-28T13:18:00.000Z",
        "voteCount": 33,
        "content": "A. Cloud SQL (30TB)"
      },
      {
        "date": "2020-12-21T08:23:00.000Z",
        "voteCount": 2,
        "content": "Sure, however in future if the capacity grows beyond 30 TB then Cloud SQL won't work right then Spanner would be the option?"
      },
      {
        "date": "2023-01-26T18:02:00.000Z",
        "voteCount": 3,
        "content": "you can always call GCP to add quota.. .Spanner is for global reach, ideally..."
      },
      {
        "date": "2020-05-12T17:53:00.000Z",
        "voteCount": 7,
        "content": "Up to 30,720 GB, depending on the machine type. This looks like correct choice.\nhttps://cloud.google.com/sql/docs/quotas#fixed-limits"
      },
      {
        "date": "2024-08-17T16:59:00.000Z",
        "voteCount": 1,
        "content": "65 TB now in Aug 2024"
      },
      {
        "date": "2024-08-17T17:00:00.000Z",
        "voteCount": 1,
        "content": "*64 TB"
      },
      {
        "date": "2022-12-08T07:54:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/quotas#storage_limits\n64TB"
      },
      {
        "date": "2021-09-27T16:15:00.000Z",
        "voteCount": 14,
        "content": "65 TB now in Sept 2021"
      },
      {
        "date": "2022-01-06T07:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/quotas#storage_limits"
      },
      {
        "date": "2020-07-07T06:20:00.000Z",
        "voteCount": 6,
        "content": "A as limit is now 30 TB for Cloud SQL"
      },
      {
        "date": "2023-10-25T07:32:00.000Z",
        "voteCount": 2,
        "content": "two keywords: Transactional data, 20 TB"
      },
      {
        "date": "2023-09-24T08:26:00.000Z",
        "voteCount": 2,
        "content": "Scalability: Cloud Spanner is designed to handle large volumes of data, making it suitable for a 20 TB database. It can scale horizontally and vertically to accommodate growing data needs.\n\nGlobal Distribution: Cloud Spanner allows you to distribute data globally for low-latency access across regions, which can be advantageous for operational systems.\n\nStrong Consistency: It provides strong transactional consistency, which is important for operational systems that require ACID compliance.\n\nSQL Support: Cloud Spanner supports SQL, which is a familiar query language for developers.\n\nWhile Cloud SQL, Cloud Bigtable, and Cloud Datastore have their use cases, Cloud Spanner is better suited for larger databases with strong consistency requirements, making it a suitable choice for migrating a 20 TB operational system database to GCP."
      },
      {
        "date": "2023-09-14T11:07:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL, upto 64 TB now, you can always call GCP for increasing the quota though !!"
      },
      {
        "date": "2023-06-20T05:16:00.000Z",
        "voteCount": 2,
        "content": "Cloud SQL is generally better for OLTP, and Cloud SQL is up to 64 TB now.\nhttps://cloud.google.com/sql/docs/quotas#storage_limits"
      },
      {
        "date": "2023-05-12T04:43:00.000Z",
        "voteCount": 2,
        "content": "\"move its operational system transaction data from an on-premises database to GCP\". Cloud SQL may be plug-and-play"
      },
      {
        "date": "2023-02-18T06:08:00.000Z",
        "voteCount": 1,
        "content": "Not 100% in favour of A, Should i recommend my client Cloud SQL, when they are coming to me with 20TB already 30TB is limit, its transactional data, which i can't compromise. I will propose cloud spanner. There is nothing mentioned that they want to save cost."
      },
      {
        "date": "2022-12-31T11:30:00.000Z",
        "voteCount": 1,
        "content": "A. Cloud SQL"
      },
      {
        "date": "2022-11-30T06:47:00.000Z",
        "voteCount": 5,
        "content": "A is the answer.\n\nhttps://cloud.google.com/sql/docs/features#features\nUp to 64 TB of storage available, with the ability to automatically increase storage size as needed."
      },
      {
        "date": "2022-11-20T04:38:00.000Z",
        "voteCount": 2,
        "content": "With the given requirements A. Cloud SQL is more than sufficient. Don't try to overthink scenarios like what if it grows.. what if there's additional requirement in future.. what if this what if that.. just look at the question and see the stated requirement. If there are more than one answer try to see which is simple and doesn't come with extra frills."
      },
      {
        "date": "2022-11-14T00:52:00.000Z",
        "voteCount": 2,
        "content": "A\n65 TB now in Nov 2022"
      },
      {
        "date": "2022-07-27T11:49:00.000Z",
        "voteCount": 1,
        "content": "it is already 20 TB at the moment, and you probably want to change the database because the capacity of your current storage solution is not enough. Then you decide to change it to Cloud SQL(up to 30 TB) which may not increase much capacity? I am not sure about the answer but A looks weird imho."
      },
      {
        "date": "2022-07-19T05:24:00.000Z",
        "voteCount": 1,
        "content": "Cloud SQL can store 64 Tb, but in the intial set up the operation are 20tb. It will reach the limitation soon if you choose Cloud SQL"
      },
      {
        "date": "2021-10-24T12:00:00.000Z",
        "voteCount": 4,
        "content": "Depends.. I mean, C is correct if the exam is not updated. A is correct if the exam is updated. So ... kinda in catch 22 situation ..."
      },
      {
        "date": "2021-10-18T17:21:00.000Z",
        "voteCount": 2,
        "content": "Hi everyone, Can i purchase this exam? is it worthable?"
      },
      {
        "date": "2020-11-13T21:43:00.000Z",
        "voteCount": 4,
        "content": "Option A - Cloud SQL is the correct answer. Cloud SQL can store upto 30 TB.\nhttps://cloud.google.com/sql/docs/quotas#:~:text=Cloud%20SQL%20storage%20limits&amp;text=Up%20to%2030%2C720%20GB%2C%20depending,for%20PostgreSQL%20or%20SQL%20Server."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 161,
    "url": "https://www.examtopics.com/discussions/google/view/16689-exam-professional-data-engineer-topic-1-question-161/",
    "body": "You need to choose a database to store time series CPU and memory usage for millions of computers. You need to store this data in one-second interval samples. Analysts will be performing real-time, ad hoc analytics against the database. You want to avoid being charged for every query executed and ensure that the schema design will allow for future growth of the dataset. Which database and data model should you choose?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table in BigQuery, and append the new samples for CPU and memory to the table",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a wide table in BigQuery, create a column for the sample value at each second, and update the row with the interval for each second",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a wide table in Bigtable with a row key that combines the computer identifier with the sample time at each minute, and combine the values for each second as column data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-30T08:58:00.000Z",
        "voteCount": 34,
        "content": "Answer C\n\nA tall and narrow table has a small number of events per row, which could be just one event, whereas a short and wide table has a large number of events per row. As explained in a moment, tall and narrow tables are best suited for time-series data.\n\nFor time series, you should generally use tall and narrow tables. This is for two reasons: Storing one event per row makes it easier to run queries against your data. Storing many events per row makes it more likely that the total row size will exceed the recommended maximum (see Rows can be big but are not infinite).\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#patterns_for_row_key_design"
      },
      {
        "date": "2022-12-31T11:35:00.000Z",
        "voteCount": 1,
        "content": "C. Create a narrow table in Bigtable with a row key that combines the Computer Engine computer identifier with the sample time at each second"
      },
      {
        "date": "2022-06-06T00:32:00.000Z",
        "voteCount": 1,
        "content": "there is a limit of 60 columns per row according to question. in addition in D the cost will be a lower which is a requirement. so D seems more suitable."
      },
      {
        "date": "2020-03-15T11:41:00.000Z",
        "voteCount": 19,
        "content": "C correct answer"
      },
      {
        "date": "2024-03-04T09:40:00.000Z",
        "voteCount": 1,
        "content": "Option C is correct answer. Narrow table is good for time series data."
      },
      {
        "date": "2023-09-24T18:51:00.000Z",
        "voteCount": 1,
        "content": "Scalability: Bigtable can handle large-scale data efficiently, making it suitable for storing time series data for millions of computers.\n\nLow Latency: Bigtable provides low-latency access to data, which is crucial for real-time analytics.\n\nFlexible Schema: The narrow table design allows you to efficiently store and query time series data without specifying all possible columns in advance, providing flexibility for future growth.\n\nColumn Families: Bigtable supports column families, allowing you to organize data logically.\n\nRow Key Design: Combining the computer identifier with the sample time at each second in the row key allows for efficient retrieval of data for specific computers and time intervals.\n\nAnalytics: While Bigtable does not support SQL directly, it allows for efficient data retrieval and can be integrated with other tools for analytics."
      },
      {
        "date": "2023-06-07T23:23:00.000Z",
        "voteCount": 2,
        "content": "\"..and ensure that the schema design will allow for future growth of the dataset\":\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#time-buckets\n\n\"Data stored in this way is compressed more efficiently than data in tall, narrow tables.\"\n\nI read the \"future growth\" as a sign to be effective in storage, and go for the Time-Buckets."
      },
      {
        "date": "2022-11-30T06:43:00.000Z",
        "voteCount": 2,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series"
      },
      {
        "date": "2022-09-09T13:49:00.000Z",
        "voteCount": 2,
        "content": "time series = narrow table"
      },
      {
        "date": "2022-04-19T23:14:00.000Z",
        "voteCount": 3,
        "content": "What about \"avoid being charged for every query executed\"? Nothing on this topic in here https://cloud.google.com/bigtable/docs/schema-design-time-series   can anyone comment?"
      },
      {
        "date": "2022-01-09T04:15:00.000Z",
        "voteCount": 2,
        "content": "Narrow and tall table for a single event and good for time-series data\nShort and Wide table for data over a month, multiple events"
      },
      {
        "date": "2021-11-24T22:09:00.000Z",
        "voteCount": 2,
        "content": "Correct: C"
      },
      {
        "date": "2021-10-15T16:54:00.000Z",
        "voteCount": 3,
        "content": "Answer is C.\nBigtable is best suited to the following scenarios: time-series data (e.g. CPU and memory usage over time for multiple servers), financial data (e.g. transaction histories, stock prices, and currency exchange rates), and IoT (Internet of Things) use cases.\nhttps://www.xplenty.com/blog/bigtable-vs-bigquery/"
      },
      {
        "date": "2021-08-12T04:57:00.000Z",
        "voteCount": 5,
        "content": "C is the correct answer. If you consider wide table, then 60 columns for cpu usage and 60 columns for memory usage. in future, if you need to add a new kpi to the table, then the schema changes. you will have to add 60 more columns for the new feature. this is not so future proof.. so D is out of the picture."
      },
      {
        "date": "2021-08-01T02:24:00.000Z",
        "voteCount": 1,
        "content": "BQ is optimized for large-scale, ad-hoc SQL-based analysis. i Think it should be A"
      },
      {
        "date": "2021-06-08T17:44:00.000Z",
        "voteCount": 1,
        "content": "First C &amp; D won't cause hotspoting as computer_identifier is first part of row key\nI prefer D because \"ensure that the schema design will allow for future growth of the dataset.\"\nC is too tall and narrow I cannot see schema design grow in the future"
      },
      {
        "date": "2021-05-23T02:36:00.000Z",
        "voteCount": 1,
        "content": "D, Better overall, harder to implement (but that is not stated as a constraint)\nhttps://cloud.google.com/bigtable/docs/schema-design-time-series#time-buckets"
      },
      {
        "date": "2021-06-05T12:06:00.000Z",
        "voteCount": 2,
        "content": "How do you store both CPU and memory usage though? Two sets of 60 columns per row? I am wondering if that goes along with \"the schema design will allow for future growth\"...what if by future growth they mean monitoring N more metrics. That would imply N*60 columns, right?"
      },
      {
        "date": "2021-03-31T13:15:00.000Z",
        "voteCount": 7,
        "content": "Should be A\nquestion did not talk about latency\nwithout query cost -- BigQuery Cache \nflexible schema - BigQuery (nested and repeated)"
      },
      {
        "date": "2020-12-09T15:43:00.000Z",
        "voteCount": 2,
        "content": "BigQuery cannot be the answer as\n[1] This will not save the price by caching as query will not always be same\nhttps://cloud.google.com/bigquery/docs/cached-results#limitations\n[2]  And the BigTable is always the correct solution for this use case as given in \nhttps://cloud.google.com/bigtable/docs/overview#what-its-good-for\n\nBigTable has native time series support and is preferred for large analytical and operational work\nAnswer can be C or D.\nBut problem is both the options will cause hot spotting. So not sure which to select"
      },
      {
        "date": "2020-12-09T15:44:00.000Z",
        "voteCount": 2,
        "content": "I will go for C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 162,
    "url": "https://www.examtopics.com/discussions/google/view/16902-exam-professional-data-engineer-topic-1-question-162/",
    "body": "You want to archive data in Cloud Storage. Because some data is very sensitive, you want to use the `Trust No One` (TNO) approach to encrypt your data to prevent the cloud provider staff from decrypting your data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key and unique additional authenticated data (AAD). Use gsutil cp to upload each encrypted file to the Cloud Storage bucket, and keep the AAD outside of Google Cloud.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse gcloud kms keys create to create a symmetric key. Then use gcloud kms encrypt to encrypt each archival file with the key. Use gsutil cp to upload each encrypted file to the Cloud Storage bucket. Manually destroy the key previously used for encryption, and rotate the key once.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in Cloud Memorystore as permanent storage of the secret.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify customer-supplied encryption key (CSEK) in the .boto configuration file. Use gsutil cp to upload each archival file to the Cloud Storage bucket. Save the CSEK in a different project that only the security team can access."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2020-04-01T17:50:00.000Z",
        "voteCount": 42,
        "content": "The correct answer must be D\nA and B can be eliminated immediately since kms generated keys are considered potentially accessible by CSP. \nC is incorrect because memory store is essentially a cache service. \n\nAdditional authenticated data (AAD) acts as a \"salt\", it is not a cipher."
      },
      {
        "date": "2022-01-06T07:13:00.000Z",
        "voteCount": 3,
        "content": "The trust no one design philosophy requires that the keys for encryption should always be, and stay, in the hands of the user that applies them. This implies that no external party can access the encrypted data (assumed that the encryption is strong enough).\nhttps://en.wikipedia.org/wiki/Trust_no_one_(Internet_security)"
      },
      {
        "date": "2020-09-01T06:42:00.000Z",
        "voteCount": 4,
        "content": "AAD is bound to the encrypted data, because you cannot decrypt the ciphertext unless you know the AAD, but it is not stored as part of the ciphertext. AAD also does not increase the cryptographic strength of the ciphertext. Instead it is an additional check by Cloud KMS to authenticate a decryption request."
      },
      {
        "date": "2020-03-28T09:54:00.000Z",
        "voteCount": 15,
        "content": "Answer: A\nDescription: AAD is used to decrypt the data so better to keep it outside GCP for safety"
      },
      {
        "date": "2024-06-14T06:01:00.000Z",
        "voteCount": 1,
        "content": "Keep AAD Outside of Google Cloud:\n\nKeeping the AAD outside of Google Cloud ensures that Google cannot access the additional context required to decrypt the files, thus implementing the TNO approach.\n\nOption C:\nCustomer-Supplied Encryption Key (CSEK) in .boto File:\nStoring the CSEK in Cloud Memorystore or any cloud service introduces a risk where the key could be potentially accessed by cloud provider staff.\nOption D:\nCustomer-Supplied Encryption Key (CSEK) in a Different Project:\nWhile storing the CSEK in a different project adds some security, it still leaves the keys within the Google Cloud environment, which does not fully meet the TNO approach."
      },
      {
        "date": "2023-11-21T09:55:00.000Z",
        "voteCount": 1,
        "content": "I just cannot understand this question. If you can't trust the provider, in this case Google, then how can you use the KMS approach. In my mind you have to generate the key locally and upload but I'm clearly wrong and don't get why."
      },
      {
        "date": "2023-09-29T09:20:00.000Z",
        "voteCount": 2,
        "content": "IMO must be (D) : to reach TNO goal keys must be customer supplied."
      },
      {
        "date": "2023-09-24T18:56:00.000Z",
        "voteCount": 1,
        "content": "Customer-Supplied Encryption Key (CSEK): CSEK allows you to provide your encryption keys, ensuring that the cloud provider staff does not have access to the keys and cannot decrypt your data.\n\nSeparate Project for Key Management: Saving the CSEK in a different project that only the security team can access adds an additional layer of security. It isolates the encryption keys from the project where the data is stored, ensuring that even within the same cloud provider, only authorized personnel can access the keys.\n\nUse of .boto Configuration: Specifying the CSEK in the .boto configuration file ensures that it is applied consistently when interacting with Cloud Storage through tools like gsutil. This way, every archival file is encrypted using your keys.\n\nOptions A and B involve using Google Cloud Key Management Service (KMS) to manage keys, which does not align with the TNO approach because cloud provider staff could potentially access the keys stored in Google Cloud KMS."
      },
      {
        "date": "2023-09-11T12:43:00.000Z",
        "voteCount": 3,
        "content": "The answer is A\nThe question tells us that \"prevent the cloud provider staff from decrypting\", so we cannot keep anything that helps decrypt on GCP, not even in a different project. so the answer cannot be D."
      },
      {
        "date": "2023-08-04T12:09:00.000Z",
        "voteCount": 3,
        "content": "CSEKs are used when an organization needs complete control over key management."
      },
      {
        "date": "2023-07-26T10:27:00.000Z",
        "voteCount": 2,
        "content": "Option A is not the best choice for the \"Trust No One\" (TNO) approach because it involves using Google Cloud's Key Management Service (KMS) to create and manage encryption keys. This means that the cloud provider will have access to the keys, which could potentially enable their staff to decrypt the data."
      },
      {
        "date": "2023-03-13T03:14:00.000Z",
        "voteCount": 3,
        "content": "D may work, but 'Trust No One' = do not trust GCP too.  So D cannot be the answer."
      },
      {
        "date": "2023-02-18T11:29:00.000Z",
        "voteCount": 2,
        "content": "answer A: KMS + AAD is more secure than CSEK"
      },
      {
        "date": "2022-11-30T06:35:00.000Z",
        "voteCount": 7,
        "content": "A is the answer.\n\nhttps://cloud.google.com/kms/docs/additional-authenticated-data\nAdditional authenticated data (AAD) is any string that you pass to Cloud Key Management Service as part of an encrypt or decrypt request. AAD is used as an integrity check and can help protect your data from a confused deputy attack. The AAD string must be no larger than 64 KiB.\n\nCloud KMS will not decrypt ciphertext unless the same AAD value is used for both encryption and decryption.\n\nAAD is bound to the encrypted data, because you cannot decrypt the ciphertext unless you know the AAD, but it is not stored as part of the ciphertext. AAD also does not increase the cryptographic strength of the ciphertext. Instead it is an additional check by Cloud KMS to authenticate a decryption request."
      },
      {
        "date": "2022-12-31T11:41:00.000Z",
        "voteCount": 2,
        "content": "Agree with A"
      },
      {
        "date": "2022-11-20T04:58:00.000Z",
        "voteCount": 3,
        "content": "CSEK with only security team having access seems to be right approach. Not sure how A can be better."
      },
      {
        "date": "2022-11-04T10:20:00.000Z",
        "voteCount": 2,
        "content": "It\u2019s A, because you cannot decrypt the ciphertext unless you know the AAD (https://cloud.google.com/kms/docs/additional-authenticated-data)"
      },
      {
        "date": "2022-10-04T17:07:00.000Z",
        "voteCount": 1,
        "content": "Answer: A"
      },
      {
        "date": "2022-09-21T21:23:00.000Z",
        "voteCount": 1,
        "content": "D it is"
      },
      {
        "date": "2022-07-21T01:14:00.000Z",
        "voteCount": 2,
        "content": "C can not be the answer since memorystore cant be used to save CSEK key.\nhttps://cloud.google.com/memorystore/docs/redis/cmek#when_does_memorystore_interact_with_cmek_keys\n\nA is the Answer."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 163,
    "url": "https://www.examtopics.com/discussions/google/view/79472-exam-professional-data-engineer-topic-1-question-163/",
    "body": "You have data pipelines running on BigQuery, Dataflow, and Dataproc. You need to perform health checks and monitor their behavior, and then notify the team managing the pipelines if they fail. You also need to be able to work across multiple projects. Your preference is to use managed products or features of the platform. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the information to Cloud Monitoring, and set up an Alerting policy\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a Virtual Machine in Compute Engine with Airflow, and export the information to Cloud Monitoring",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the logs to BigQuery, and set up App Engine to read that information and send emails if you find a failure in the logs",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDevelop an App Engine application to consume logs using GCP API calls, and send emails if you find a failure in the logs"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-11T02:33:00.000Z",
        "voteCount": 5,
        "content": "A .    Your preference is to use managed products or features of the platform"
      },
      {
        "date": "2023-09-24T18:58:00.000Z",
        "voteCount": 2,
        "content": "Cloud Monitoring (formerly known as Stackdriver) is a fully managed monitoring service provided by GCP, which can collect metrics, logs, and other telemetry data from various GCP services, including BigQuery, Dataflow, and Dataproc.\n\nAlerting Policies: Cloud Monitoring allows you to define alerting policies based on specific conditions or thresholds, such as pipeline failures, latency spikes, or other custom metrics. When these conditions are met, Cloud Monitoring can trigger notifications (e.g., emails) to alert the team managing the pipelines.\n\nCross-Project Monitoring: Cloud Monitoring supports monitoring resources across multiple GCP projects, making it suitable for your requirement to monitor pipelines in multiple projects.\n\nManaged Solution: Cloud Monitoring is a managed service, reducing the operational overhead compared to running your own virtual machine instances or building custom solutions."
      },
      {
        "date": "2023-09-08T19:43:00.000Z",
        "voteCount": 1,
        "content": "use managed products"
      },
      {
        "date": "2023-03-03T06:03:00.000Z",
        "voteCount": 2,
        "content": "Should be A"
      },
      {
        "date": "2022-09-07T03:00:00.000Z",
        "voteCount": 1,
        "content": "Should be A"
      },
      {
        "date": "2022-09-02T10:21:00.000Z",
        "voteCount": 2,
        "content": "A. Export the information to Cloud Monitoring, and set up an Alerting policy"
      },
      {
        "date": "2022-09-02T10:03:00.000Z",
        "voteCount": 1,
        "content": "Should be A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 164,
    "url": "https://www.examtopics.com/discussions/google/view/79478-exam-professional-data-engineer-topic-1-question-164/",
    "body": "You are working on a linear regression model on BigQuery ML to predict a customer's likelihood of purchasing your company's products. Your model uses a city name variable as a key predictive component. In order to train and serve the model, your data must be organized in columns. You want to prepare your data using the least amount of coding while maintaining the predictable variables. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new view with BigQuery that does not include a column with city information.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse SQL in BigQuery to transform the state column using a one-hot encoding method, and make each city a column with binary values.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse TensorFlow to create a categorical variable with a vocabulary list. Create the vocabulary file and upload that as part of your model to BigQuery ML.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion to assign each city to a region that is labeled as 1, 2, 3, 4, or 5, and then use that number to represent the city in the model."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-05T17:34:00.000Z",
        "voteCount": 9,
        "content": "If we're rigorous, as we should because it's a professional exam, I think option B is incorrect because it's one-hot-encoding the \"state\" column, if the answer was \"city\" column, then I'd go for B. As this is not the case and I do not accept an spelling error like this in an official question, I would go for D."
      },
      {
        "date": "2023-09-08T19:46:00.000Z",
        "voteCount": 3,
        "content": "I think it should say citiy instead of state... it is a typooi in the transcription of the question"
      },
      {
        "date": "2023-07-26T21:55:00.000Z",
        "voteCount": 1,
        "content": "you are right, OHE is mentioned for state in option B, but in option B it is also mentioned to use binary conversion for the city column. an additional method can be used which is applicable for the conversion."
      },
      {
        "date": "2023-06-02T09:02:00.000Z",
        "voteCount": 1,
        "content": "But also for D, assigning each city to a numbered region could lose important information, as cities within the same region might have different characteristics affecting customer purchasing behavior (from Chat GPT)."
      },
      {
        "date": "2023-12-19T11:32:00.000Z",
        "voteCount": 5,
        "content": "One-hot encoding is a common technique used to handle categorical data in machine learning. This approach will transform the city name variable into a series of binary columns, one for each city. Each row will have a \"1\" in the column corresponding to the city it represents and \"0\" in all other city columns. This method is effective for linear regression models as it enables the model to use city data as a series of numeric, binary variables. BigQuery supports SQL operations that can easily implement one-hot encoding, thus minimizing the amount of coding required and efficiently preparing the data for the model."
      },
      {
        "date": "2023-12-19T11:32:00.000Z",
        "voteCount": 3,
        "content": "A removes the city information completely, losing a key predictive component.\n\nC requires additional coding and infrastructure with TensorFlow and vocabulary files outside of what BigQuery already provides.\n\nD transforms the distinct city values into numeric regions, losing granularity of the city data.\n\nBy using SQL within BigQuery to one-hot encode cities into multiple yes/no columns, the city data is maintained and formatted appropriately for the BigQuery ML linear regression model with minimal additional coding. This aligns with the requirements stated in the question."
      },
      {
        "date": "2023-12-19T11:54:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/auto-preprocessing#one_hot_encoding"
      },
      {
        "date": "2024-10-01T11:52:00.000Z",
        "voteCount": 1,
        "content": "This is B. It's easier to one hot in bigquery than to do it in datafusion and then import the values back into bigquery."
      },
      {
        "date": "2023-09-24T19:16:00.000Z",
        "voteCount": 4,
        "content": "One-Hot Encoding: One-hot encoding is a common technique for handling categorical variables like city names in machine learning models. It transforms categorical data into a binary matrix, where each city becomes a separate column with binary values (0 or 1) indicating the presence or absence of that city.\n\nLeast Amount of Coding: One-hot encoding in BigQuery is straightforward and can be accomplished with SQL. You can use SQL expressions to pivot the city names into separate columns and assign binary values based on the city's presence in the original data.\n\nPredictive Power: One-hot encoding retains the predictive power of city information while making it suitable for linear regression models, which require numerical input."
      },
      {
        "date": "2023-07-26T21:52:00.000Z",
        "voteCount": 3,
        "content": "One hot encoding for state and binary values for each city will allow me to choose the B option."
      },
      {
        "date": "2023-07-26T09:56:00.000Z",
        "voteCount": 2,
        "content": "I guess Option D loses the granularity of the city-level information, as multiple cities will be grouped into the same region and represented by the same number. This can result in a loss of important predictive information for your linear regression model.\n\nOn the other hand, if we use one-hot encoding to create binary columns for each city. This method preserves the city-level information, allowing the model to capture the unique effects of each city on the likelihood of purchasing your company's products. Additionally, it can be done directly in BigQuery using SQL, which requires less coding and is more efficient."
      },
      {
        "date": "2023-06-23T00:07:00.000Z",
        "voteCount": 4,
        "content": "One-hot encoding is a common technique used to represent categorical variables as binary columns. In this case, you can transform the city variable into multiple binary columns, with each column representing a specific city. This allows you to maintain the predictive city information while organizing the data in columns suitable for training and serving the linear regression model.\n\nBy using SQL in BigQuery, you can perform the necessary transformations to implement one-hot encoding."
      },
      {
        "date": "2023-06-22T05:00:00.000Z",
        "voteCount": 3,
        "content": "- A is wrong since it drops the city which is a key predictor. \n- C is wrong since we want to keep it simple, and not use Tensorflow here.\n- D is wrong since there is no specific reason to use Data Fusion, and also this encoding here is ordinal, which doesn't make sense for something non-quantitative such as cities - we want one-hot coding instead.\n\nTherefore, B must be the correct answer."
      },
      {
        "date": "2023-09-20T06:16:00.000Z",
        "voteCount": 1,
        "content": "It could be argued that a specific reason to use Data Fusion is the minimal coding requirement."
      },
      {
        "date": "2023-06-15T17:40:00.000Z",
        "voteCount": 3,
        "content": "Cloud Datafusion: least amount of coding"
      },
      {
        "date": "2023-07-26T21:59:00.000Z",
        "voteCount": 1,
        "content": "OHE is better that datafusion considering least amount coding"
      },
      {
        "date": "2023-07-26T09:58:00.000Z",
        "voteCount": 1,
        "content": "While it's true that Cloud Data Fusion can simplify data integration tasks with a visual interface, it might not be the best choice in this specific scenario as using Cloud Data Fusion to assign each city to a region might result in a loss of important predictive information due to the grouping of cities"
      },
      {
        "date": "2023-05-12T07:11:00.000Z",
        "voteCount": 1,
        "content": "A doesn't include the city column. \nC is not low code.\nD is not a one hot encoding, but an ordinal one on the city column.\n\nB applies a one hot encoding on the state column and a binary encoding on the city column, which works for me."
      },
      {
        "date": "2023-05-03T02:58:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding"
      },
      {
        "date": "2023-03-22T04:57:00.000Z",
        "voteCount": 4,
        "content": "D uses the least amount of coding... even if the model is not good.\nB encodes the \"state\", not the \"city\"."
      },
      {
        "date": "2023-01-05T08:37:00.000Z",
        "voteCount": 2,
        "content": "Manually bigquery ml does preprocessing for you however if one wants to do a manual processing one can use the ML.ONE_HOT_ENCODER function. It just acts as an analytical funciton."
      },
      {
        "date": "2022-11-30T06:23:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding\nOne-hot encoding maps each category that a feature has to its own binary feature where 0 represents the absence of the feature and 1 represents the presence (known as a dummy variable) creating N new feature columns where N is the number of unique categories for the feature across the training table."
      },
      {
        "date": "2022-11-23T10:57:00.000Z",
        "voteCount": 4,
        "content": "The Cloud Data Fusion method will add unecessary weights to categories with higher value labels, which will skew the model. The best practice for encoding nominal categorical data is to one-hot-encode them into binary values. That is conveniently done in BigQuery:\n\nhttps://cloud.google.com/bigquery-ml/docs/reference/standard-sql/bigqueryml-auto-preprocessing#one_hot_encoding"
      },
      {
        "date": "2022-11-14T11:57:00.000Z",
        "voteCount": 1,
        "content": "D\nCloud Data Fusion is a fully managed, code-free data integration service that helps users efficiently build and manage ETL/ELT data pipelines."
      },
      {
        "date": "2023-01-05T08:39:00.000Z",
        "voteCount": 1,
        "content": "Does it come with an out of the box one hot encoding template ?"
      },
      {
        "date": "2022-11-07T09:42:00.000Z",
        "voteCount": 3,
        "content": "I have the same feeling with @cloudmon so I compromised to answer [D].\nIn more detail, here is my reasoning:\n\nThe requirement \"maintaining the predictable variables\" (a.k.a. city) makes:\n[A] obiously invalid\n[B] invalid, since it broadens the prediction to be state-dependent (all cities in particular state will be treated as the same variable). Additionally, one-hot encoding is not suitable for linear regression problems, dummy encoding (drop one) is better.\n\nAnswer [C] doesn't satisfy the \"least amount of coding\" directive. Other than that (as far I understood by searcing the keyword tf.feature_column.categorical_column_with_vocabulary_list) the TensorFlow vocabulary list is another form of one-hot encoding.\n\nSo it remains [D] which offers a visual interface but uses ordinal (or label) encoding which is far from ideal for regression problems."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 165,
    "url": "https://www.examtopics.com/discussions/google/view/79627-exam-professional-data-engineer-topic-1-question-165/",
    "body": "You work for a large bank that operates in locations throughout North America. You are setting up a data storage system that will handle bank account transactions. You require ACID compliance and the ability to access data with SQL. Which solution is appropriate?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore transaction data in Cloud Spanner. Enable stale reads to reduce latency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore transaction in Cloud Spanner. Use locking read-write transactions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore transaction data in BigQuery. Disabled the query cache to ensure consistency.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore transaction data in Cloud SQL. Use a federated query BigQuery for analysis."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 38,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-04T17:31:00.000Z",
        "voteCount": 11,
        "content": "I'd say B as the documentation primarily says ACID compliance for Spanner, not Cloud SQL.\nhttps://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained\nAlso, spanner supports read-write transactions for use cases, as handling bank transactions:\nhttps://cloud.google.com/spanner/docs/transactions#read-write_transactions"
      },
      {
        "date": "2022-11-20T22:20:00.000Z",
        "voteCount": 13,
        "content": "I wonder if you understood the meaning of ACID. This is an inherent property of any relational DB. Cloud SQL is fully ACID complaint"
      },
      {
        "date": "2022-12-31T11:44:00.000Z",
        "voteCount": 1,
        "content": "B is right"
      },
      {
        "date": "2023-03-22T05:15:00.000Z",
        "voteCount": 9,
        "content": "\"locations throughout North America\" implies multi-region (northamerica-northeast1, us-central1, us-south1, us-west4, us-east5, etc.)\nCloud SQL can only do read replicas in other regions."
      },
      {
        "date": "2023-08-16T12:24:00.000Z",
        "voteCount": 3,
        "content": "Read replicas are enough to make Cloud SQL work as a multi-region service. That's not the point. The point is that the answer introduces the use of Bigquery when it's not needed for the use case. That's why B is right."
      },
      {
        "date": "2023-12-19T11:59:00.000Z",
        "voteCount": 3,
        "content": "B. Store transaction in Cloud Spanner. Use locking read-write transactions.\n\nSince the banking transaction system requires ACID compliance and SQL access to the data, Cloud Spanner is the most appropriate solution. Unlike Cloud SQL, Cloud Spanner natively provides ACID transactions and horizontal scalability.\n\nEnabling stale reads in Spanner (option A) would reduce data consistency, violating the ACID compliance requirement of banking transactions.\n\nBigQuery (option C) does not natively support ACID transactions or SQL writes which are necessary for a banking transactions system.\n\nCloud SQL (option D) provides ACID compliance but does not scale horizontally like Cloud Spanner can to handle large transaction volumes.\n\nBy using Cloud Spanner and specifically locking read-write transactions, ACID compliance is ensured while providing fast, horizontally scalable SQL processing of banking transactions."
      },
      {
        "date": "2023-12-14T02:06:00.000Z",
        "voteCount": 1,
        "content": "Spanner is an enterprise level resource which Banks require, Cloud SQL is limited to 30TB of storage. And Banking transactions should be read write locked."
      },
      {
        "date": "2023-09-24T19:19:00.000Z",
        "voteCount": 3,
        "content": "ACID Compliance: Cloud Spanner is a globally distributed, strongly consistent database service that offers ACID compliance, making it a suitable choice for handling bank account transactions where data consistency and integrity are crucial.\n\nSQL Access: Cloud Spanner supports SQL queries, which align with your requirement to access data with SQL. You can use standard SQL to interact with the data stored in Cloud Spanner.\n\nLocking Read-Write Transactions: Cloud Spanner allows you to perform locking read-write transactions, ensuring that transactions are executed in a serializable and consistent manner. This is essential for financial transactions to prevent conflicts and maintain data integrity."
      },
      {
        "date": "2023-07-29T23:27:00.000Z",
        "voteCount": 1,
        "content": "B. Store transaction data in Cloud Spanner. Use locking read-write transactions.\n\nHere's why:\n\nACID Compliance: ACID stands for Atomicity, Consistency, Isolation, and Durability. Cloud Spanner is a fully managed, globally distributed database that provides strong consistency and ACID compliance. This ensures that bank account transactions are processed reliably and accurately, avoiding issues like data corruption or incomplete transactions.\n\nAbility to access data with SQL: Cloud Spanner supports SQL, which allows you to perform standard SQL queries on the data. This means that you can use familiar SQL commands to access, retrieve, and manipulate transaction data easily."
      },
      {
        "date": "2023-04-16T15:01:00.000Z",
        "voteCount": 1,
        "content": "I initially selected B. However, it might be D.\n\nhttps://cloud.google.com/blog/topics/developers-practitioners/your-google-cloud-database-options-explained\nCloud Spanner: Cloud Spanner is an enterprise-grade, globally-distributed, and strongly-consistent database that offers up to 99.999% availability, built specifically to combine the benefits of relational database structure with non-relational horizontal scale. It is a unique database that combines ACID transactions, SQL queries, and relational structure with the scalability that you typically associate with non-relational or NoSQL databases. As a result, Spanner is best used for applications such as gaming, payment solutions, global financial ledgers, retail banking and inventory management that require ability to scale limitlessly with strong-consistency and high-availability."
      },
      {
        "date": "2023-02-24T14:38:00.000Z",
        "voteCount": 2,
        "content": "Answer B: \nlocking read-write = for data accuracy \nstate read = for speed up or latency"
      },
      {
        "date": "2023-02-18T11:43:00.000Z",
        "voteCount": 2,
        "content": "Answer B: Spanner\nIt's incomplete question, what do you assume by large bank, until we are not sure about size and scale. Region is north america, that can be managed by cloud sql. but\ni am going for spanner, as its large bank and transaction data."
      },
      {
        "date": "2023-02-05T17:54:00.000Z",
        "voteCount": 5,
        "content": "This is definitely a tricky question because both B and D are \"appropriate\" as the question suggests, of course we can make assumptions with the \"large bank\" sentence but there are other questions here where making assumptions is not accepted by the community so I wonder when can we make assumptions and when we can't.  I think the real problem here is the ambiguous question. This is one of the few questions where the community accept that both (B and D) answers are appropriate but some comments (and I agree) argue the BEST approach is B. I really think some questions can be written in a better and non-ambiguous way, it's just about thinking a little bit more and not conforming when a poor spelling."
      },
      {
        "date": "2022-12-17T01:16:00.000Z",
        "voteCount": 6,
        "content": "The question is hinting a requirement for global consistency, i.e. being available for NA region, which does not just include US but also Mexico, Argentina etc.\n\nLarge bank = priority over consistency over read-write"
      },
      {
        "date": "2023-01-26T18:23:00.000Z",
        "voteCount": 5,
        "content": "Argentina is South America..."
      },
      {
        "date": "2023-09-20T06:26:00.000Z",
        "voteCount": 1,
        "content": "Good catch, definetely Spanner in that case."
      },
      {
        "date": "2022-12-07T02:45:00.000Z",
        "voteCount": 1,
        "content": "Finally, it's [B]. \nThere is no measurable requirement that rules out [D] (Cloud SQL) and this fact made me to select it as a preferrable answer.\nBut since we are talking about a large bank (which normally implies massive reads/writes per sec.) and nobody has posed any cost limitation, in a real case I would definitely prefer the advantages of Spanner."
      },
      {
        "date": "2022-11-30T06:00:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/spanner/docs/transactions\nSpanner supports these transaction modes:\n- Locking read-write. This type of transaction is the only transaction type that supports writing data into Spanner. These transactions rely on pessimistic locking and, if necessary, two-phase commit. Locking read-write transactions may abort, requiring the application to retry."
      },
      {
        "date": "2022-11-08T23:40:00.000Z",
        "voteCount": 2,
        "content": "[A] No - Stale reads not accepted for bank account transactions. \"A stale read is read at a timestamp in the past. If your application is latency sensitive but tolerant of stale data, then stale reads can provide performance benefits.\"\n[B] Yes - Fulfills all requirements\n[C] No - BigQuery is ACID-compliant, but it is too much to use it for such a case (mainly a CRUD app)\n[D] Yes+ - Fulfills all requirements. The BigQuery part may seem redundant, but it states a true fact that doesn't violate the \"access data with SQL\" requirement.\n\nSo, when SQL Cloud and SQL Spanner fit both, there is no reason to prefer the second.\nAnd the question doesn't mention any obvious fact for which should we prefer the expensive SQL Spanner:\n- We don't know if we have to deal with a big amount of data and thousands of writes per second. \n- We don't know the database size.\n- There is no need for multi-regional writes that would exclude SQL Cloud as an alternative. Is it a coincidence that the question limits the problem to the single region of North America?"
      },
      {
        "date": "2022-12-07T02:44:00.000Z",
        "voteCount": 1,
        "content": "I changed my mind to [B] since I underestimated the given of a \"large bank\" where the cost difference for a single region Spanner wouldn't matter."
      },
      {
        "date": "2022-12-07T07:25:00.000Z",
        "voteCount": 3,
        "content": "North America has many regions, and the requirement is throughout North America, so Cloud Spanner will be more suitable to support multi-regions."
      },
      {
        "date": "2022-12-08T02:17:00.000Z",
        "voteCount": 2,
        "content": "You are correct of course. The final sentence was totally inaccurate."
      },
      {
        "date": "2023-03-06T06:44:00.000Z",
        "voteCount": 1,
        "content": "Also, correct me if I am wrong, Bigquery cannot query Cloud SQL directly, only when Cloud SQL is exported into GCS, then BQ can connect to GCS using federated queries."
      },
      {
        "date": "2022-11-06T15:13:00.000Z",
        "voteCount": 1,
        "content": "I'd go for B.\nThe only other somewhat valid option is D, but there's no requirement for analytics in the question."
      },
      {
        "date": "2022-10-22T08:30:00.000Z",
        "voteCount": 1,
        "content": "Surely its B, transactional data at a large US based bank would surely be massive in size and probably too much for CloudSQL? There is also no mention of a requirement for analytics"
      },
      {
        "date": "2022-09-09T09:41:00.000Z",
        "voteCount": 2,
        "content": "why not spanner?"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 166,
    "url": "https://www.examtopics.com/discussions/google/view/80145-exam-professional-data-engineer-topic-1-question-166/",
    "body": "A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to implement a change that would improve query performance in BigQuery. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement clustering in BigQuery on the ingest date column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement clustering in BigQuery on the package-tracking ID column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-create the table using data partitioning on the package delivery date."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T05:55:00.000Z",
        "voteCount": 9,
        "content": "B is the answer.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nClustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs.\n\nIn BigQuery, a clustered column is a user-defined table property that sorts storage blocks based on the values in the clustered columns. The storage blocks are adaptively sized based on the size of the table. A clustered table maintains the sort properties in the context of each operation that modifies it. Queries that filter or aggregate by the clustered columns only scan the relevant blocks based on the clustered columns instead of the entire table or table partition."
      },
      {
        "date": "2022-12-31T11:47:00.000Z",
        "voteCount": 1,
        "content": "Yes it is B. Implement clustering in BigQuery on the package-tracking ID column."
      },
      {
        "date": "2024-06-05T01:12:00.000Z",
        "voteCount": 1,
        "content": "Though I almost fell for D, but the delivery date information is only available on the event(s) that happen after the delivery, but not on the ones before where it will be NULL I guess. The only other option that can make some sense is B, though high cardinality is not recommended for clustering."
      },
      {
        "date": "2023-12-19T12:05:00.000Z",
        "voteCount": 1,
        "content": "B as Clustering the data on the package Id can greatly improve the performance.\nRefer GCP documentation - BigQuery Clustered Table:https://cloud.google.com/bigquery/docs/clustered-tables"
      },
      {
        "date": "2023-12-19T12:05:00.000Z",
        "voteCount": 1,
        "content": "Clustering can improve the performance of certain types of queries such as queries that use filter clauses and queries that aggregate data. When data is written to a clustered table by a query job or a load job, BigQuery sorts the data using the values in the clustering columns. These values are used to organize the data into multiple blocks in BigQuery storage. When you submit a query containing a clause that filters data based on the clustering columns, BigQuery uses the sorted blocks to eliminate scans of unnecessary data.\nCurrently, BigQuery allows clustering over a partitioned table. Use clustering over a partitioned table when:\n-\tYour data is already partitioned on a date, timestamp, or integer column.\n-\tYou commonly use filters or aggregation against particular columns in your queries.\nTable clustering is possible for tables partitioned by:\n-\tingestion time\n-\tdate/timestamp\n-\tinteger range"
      },
      {
        "date": "2023-12-19T12:06:00.000Z",
        "voteCount": 1,
        "content": "In a table partitioned by a date or timestamp column, each partition contains a single day of data. When the data is stored, BigQuery ensures that all the data in a block belongs to a single partition. A partitioned table maintains these properties across all operations that modify it: query jobs, Data Manipulation Language (DML) statements, Data Definition Language (DDL) statements, load jobs, and copy jobs. This requires BigQuery to maintain more metadata than a non-partitioned table. As the number of partitions increases, the amount of metadata overhead increases."
      },
      {
        "date": "2023-12-19T12:06:00.000Z",
        "voteCount": 1,
        "content": "Although more metadata must be maintained, by ensuring that data is partitioned globally, BigQuery can more accurately estimate the bytes processed by a query before you run it. This cost calculation provides an upper bound on the final cost of the query.\nIn a clustered table, BigQuery automatically sorts the data based on the values in the clustering columns and organizes them in optimally sized storage blocks. You can achieve more finely grained sorting by creating a table that is clustered and partitioned. A clustered table maintains the sort properties in the context of each operation that modifies it. As a result, BigQuery may not be able to accurately estimate the bytes processed by the query or the query costs. When blocks of data are eliminated during query execution, BigQuery provides a best effort reduction of the query costs."
      },
      {
        "date": "2023-12-14T02:33:00.000Z",
        "voteCount": 2,
        "content": "Package Tracking mostly contains, geospatial prefixes, Like HK0011, US0022, etc, this can help in clustering."
      },
      {
        "date": "2023-10-04T04:50:00.000Z",
        "voteCount": 4,
        "content": "D is the correct answer\n\nrequirements: analyze geospatial trends in the lifecycle of a package\n\ncuz the data of the lifecycle of the package would span across ingest-date-based partition table, it would degrade the performance.\n\nhence, re-partitoning by package delivery date, which is the package initially delivered, would improve the performance when querying such table."
      },
      {
        "date": "2023-06-13T07:37:00.000Z",
        "voteCount": 3,
        "content": "I vote D\nQueries to analyze the package lifecycle will cross partitions when using ingest date. Changing this to delivery date will allow a query to full a package's full lifecycle in a single partition."
      },
      {
        "date": "2022-11-04T11:03:00.000Z",
        "voteCount": 1,
        "content": "B. https://cloud.google.com/bigquery/docs/clustered-tables"
      },
      {
        "date": "2022-09-11T01:25:00.000Z",
        "voteCount": 3,
        "content": "D is not correct becsuse This Is problem Is The Real Time  so  ingested date is the same as delivery date."
      },
      {
        "date": "2022-09-09T06:54:00.000Z",
        "voteCount": 1,
        "content": "why not D ?"
      },
      {
        "date": "2022-12-16T04:15:00.000Z",
        "voteCount": 2,
        "content": "The table has already been partitioned"
      },
      {
        "date": "2022-10-03T04:42:00.000Z",
        "voteCount": 1,
        "content": "There are several rows that represent movement of life cycle  of 1  package-tracking ID \npackage delivery date = ingestion date   , i suppose"
      },
      {
        "date": "2022-09-07T03:14:00.000Z",
        "voteCount": 2,
        "content": "B;\nAs the table has already created with ingest-date partitioning."
      },
      {
        "date": "2022-09-04T14:50:00.000Z",
        "voteCount": 1,
        "content": "B. Implement clustering in BigQuery on the package-tracking ID column."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 167,
    "url": "https://www.examtopics.com/discussions/google/view/79492-exam-professional-data-engineer-topic-1-question-167/",
    "body": "Your company currently runs a large on-premises cluster using Spark, Hive, and HDFS in a colocation facility. The cluster is designed to accommodate peak usage on the system; however, many jobs are batch in nature, and usage of the cluster fluctuates quite dramatically. Your company is eager to move to the cloud to reduce the overhead associated with on-premises infrastructure and maintenance and to benefit from the cost savings. They are also hoping to modernize their existing infrastructure to use more serverless offerings in order to take advantage of the cloud. Because of the timing of their contract renewal with the colocation facility, they have only 2 months for their initial migration. How would you recommend they approach their upcoming migration strategy so they can maximize their cost savings in the cloud while still executing the migration in time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workloads to Dataproc plus HDFS; modernize later.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the workloads to Dataproc plus Cloud Storage; modernize later.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Spark workload to Dataproc plus HDFS, and modernize the Hive workload for BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModernize the Spark workload for Dataflow and the Hive workload for BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-30T05:51:00.000Z",
        "voteCount": 8,
        "content": "B is the answer.\n\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc#overview\nWhen you want to move your Apache Spark workloads from an on-premises environment to Google Cloud, we recommend using Dataproc to run Apache Spark/Apache Hadoop clusters. Dataproc is a fully managed, fully supported service offered by Google Cloud. It allows you to separate storage and compute, which helps you to manage your costs and be more flexible in scaling your workloads.\n\nhttps://cloud.google.com/bigquery/docs/migration/hive#data_migration\nMigrating Hive data from your on-premises or other cloud-based source cluster to BigQuery has two steps:\n1. Copying data from a source cluster to Cloud Storage\n2. Loading data from Cloud Storage into BigQuery"
      },
      {
        "date": "2022-12-31T11:49:00.000Z",
        "voteCount": 1,
        "content": "B. Migrate the workloads to Dataproc plus Cloud Storage; modernize later."
      },
      {
        "date": "2023-12-19T12:16:00.000Z",
        "voteCount": 2,
        "content": "Based on the time constraint of 2 months and the goal to maximize cost savings, I would recommend option B - Migrate the workloads to Dataproc plus Cloud Storage; modernize later.\nThe key reasons are:\n\u2022\tDataproc provides a fast, native migration path from on-prem Spark and Hive to the cloud. This allows meeting the 2 month timeline.\n\u2022\tUsing Cloud Storage instead of HDFS avoids managing clusters for variable workloads and provides cost savings.\n\u2022\tFurther optimizations and modernization to serverless (Dataflow, BigQuery) can happen incrementally later without time pressure."
      },
      {
        "date": "2023-12-19T12:16:00.000Z",
        "voteCount": 3,
        "content": "Option A still requires managing HDFS.\nOption C and D require full modernization of workloads in 2 months which is likely infeasible.\nTherefore, migrating to Dataproc with Cloud Storage fast tracks the migration within 2 months while realizing immediate cost savings, enabling the flexibility to iteratively modernize and optimize the workloads over time."
      },
      {
        "date": "2022-10-03T04:51:00.000Z",
        "voteCount": 3,
        "content": "B is most likely\n1. migrate job and infrastructure to dataproc on clound\n2. any data, move from hdfs on-premise to google cloud storage ( one of them is Hive)\nIf you want to modernize Hive to Bigquery , you are need to move it into GCS(preceding step) first and load it into bigquery\nthat is all. \n\nhttps://cloud.google.com/blog/products/data-analytics/apache-hive-to-bigquery\nhttps://cloud.google.com/architecture/hadoop/migrating-apache-spark-jobs-to-cloud-dataproc\nhttps://cloud.google.com/architecture/hadoop/hadoop-gcp-migration-data"
      },
      {
        "date": "2022-09-15T02:34:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2022-09-22T09:11:00.000Z",
        "voteCount": 1,
        "content": "you sould migrate spark to apache beam which is not the case here"
      },
      {
        "date": "2022-09-29T00:03:00.000Z",
        "voteCount": 1,
        "content": "apache beam for what???"
      },
      {
        "date": "2022-10-12T17:01:00.000Z",
        "voteCount": 1,
        "content": "dataflow uses apache beam"
      },
      {
        "date": "2023-01-11T05:40:00.000Z",
        "voteCount": 1,
        "content": "@adarifian Why use apache beam yet there is Dataflow an inhouse gcp solution to solve the problem? hence i said apache beam for what"
      },
      {
        "date": "2023-11-02T13:40:00.000Z",
        "voteCount": 1,
        "content": "Dataflow IS apache beam, Dataflow is a Beam Runner.\nIf you go for that soulution you will need to modify your pipeline to use Beam"
      },
      {
        "date": "2022-09-10T05:40:00.000Z",
        "voteCount": 3,
        "content": "D isn't feasible, within 2 months. Anyone who has worked in  a Hadoop/ Big Data data warehousing or data lake project, knows how less time 2 months is, given the amount of data and associated complexities abound. \n\nIt should be B to begin with. And then gradually move towards D."
      },
      {
        "date": "2022-09-08T23:07:00.000Z",
        "voteCount": 2,
        "content": "Ans B\n-cost saving \n-time factor\n-Spark -Data proc"
      },
      {
        "date": "2022-09-08T23:10:00.000Z",
        "voteCount": 1,
        "content": "Ans D is also relevant if you read this. Onthe other hand cloud storage isnt severless but bigquery is\nhttps://cloud.google.com/hadoop-spark-migration"
      },
      {
        "date": "2022-09-06T12:32:00.000Z",
        "voteCount": 1,
        "content": "Ans.B as per the following link\nhttps://blog.devgenius.io/migrating-spark-jobs-to-google-cloud-file-event-sensor-to-dynamically-create-spark-cluster-7eff2c75423d"
      },
      {
        "date": "2022-09-06T01:08:00.000Z",
        "voteCount": 2,
        "content": "For the time window of two month I would recommend B and then start to implement D."
      },
      {
        "date": "2022-09-03T01:34:00.000Z",
        "voteCount": 2,
        "content": "It is B or D, still confusing"
      },
      {
        "date": "2022-09-02T10:57:00.000Z",
        "voteCount": 1,
        "content": "D because the Apache Spark Runner can be used to execute Beam pipelines using Apache Spark. Also, Hive to BigQuery is not a difficult modernization/migration."
      },
      {
        "date": "2023-11-02T13:41:00.000Z",
        "voteCount": 1,
        "content": "Dataflow is a Runner of Beam it self"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 168,
    "url": "https://www.examtopics.com/discussions/google/view/79494-exam-professional-data-engineer-topic-1-question-168/",
    "body": "You work for a financial institution that lets customers register online. As new customers register, their user data is sent to Pub/Sub before being ingested into<br>BigQuery. For security reasons, you decide to redact your customers' Government issued Identification Number while allowing customer service representatives to view the original values when necessary. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery's built-in AEAD encryption to encrypt the SSN column. Save the keys to a new table that is only viewable by permissioned users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery column-level security. Set the table permissions so that only members of the Customer Service user group can see the SSN column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic hash.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBefore loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T11:01:00.000Z",
        "voteCount": 11,
        "content": "B. While C and D are intriguing, they don't specify how to enable customer service representatives to receive access to the encryption token."
      },
      {
        "date": "2023-12-19T12:25:00.000Z",
        "voteCount": 1,
        "content": "B. BigQuery column-level security:\n\nPros: Granular control over column access, ensures only authorized users see the SSN column.\nCons: Doesn't truly redact the data. The SSN values are still stored in BigQuery, even if hidden from unauthorized users. A potential security breach could expose them."
      },
      {
        "date": "2023-10-25T04:48:00.000Z",
        "voteCount": 1,
        "content": "there is no SSN in question, it can be any ID."
      },
      {
        "date": "2024-10-17T05:14:00.000Z",
        "voteCount": 1,
        "content": "Authorized users can decrypt the FPE tokens back to the original GIINs, D is the best option."
      },
      {
        "date": "2024-10-09T02:55:00.000Z",
        "voteCount": 1,
        "content": "D (FPE) does indeed allow encryption to be reversed if desired, allowing operatives to review the original key. This makes it preferable to B, as it's also more secure.\n"
      },
      {
        "date": "2024-06-08T11:22:00.000Z",
        "voteCount": 1,
        "content": "D:\nSSN is only tied to USA not in any other countries, The question did not mention SSN."
      },
      {
        "date": "2023-12-19T12:24:00.000Z",
        "voteCount": 2,
        "content": "The best option is D - Before loading the data into BigQuery, use Cloud Data Loss Prevention (DLP) to replace input values with a cryptographic format-preserving encryption token.\n\nThe key reasons are:\n\nDLP allows redacting sensitive PII like SSNs before loading into BigQuery. This provides security by default for the raw SSN values.\nUsing format-preserving encryption keeps the column format intact while still encrypting, allowing business logic relying on SSN format to continue functioning.\nThe encrypted tokens can be reversed to view original SSNs when required, meeting the access requirement for customer service reps."
      },
      {
        "date": "2023-12-19T12:24:00.000Z",
        "voteCount": 1,
        "content": "Option A does encrypt SSN but requires managing keys separately.\n\nOption B relies on complex IAM policy changes instead of encrypting by default.\n\nOption C hashes irreversibly, preventing customer service reps from viewing original SSNs when required.\n\nTherefore, using DLP format-preserving encryption before BigQuery ingestion balances both security and analytics requirements for SSN data."
      },
      {
        "date": "2023-12-19T12:26:00.000Z",
        "voteCount": 2,
        "content": "Why not B. BigQuery column-level security:\nDoesn't truly redact the data. The SSN values are still stored in BigQuery, even if hidden from unauthorized users. A potential security breach could expose them."
      },
      {
        "date": "2023-12-14T03:06:00.000Z",
        "voteCount": 2,
        "content": "Even if you provide Column level access control, The Data Owners or other hierarchies above it will also be able to view very sensitive data. Better to just use encryption and decryption. As this data can also never be used for any analytic workloads"
      },
      {
        "date": "2023-11-09T20:37:00.000Z",
        "voteCount": 2,
        "content": "Answer has to be D. Question says \"you decide to redact your customers' Government issued Identification Number while allowing customer service representatives to view the original values when necessary\"... Redact... view the original values... D is the only choice."
      },
      {
        "date": "2023-11-01T01:41:00.000Z",
        "voteCount": 1,
        "content": "It  might not be  D! \nSince - only the Frame is kept. the data will be changed. \nFormat Preserving Encryption (FPE), endorsed by NIST, is an advanced encryption technique that transforms data into an encrypted format while preserving its original structure. For instance, a 16-digit credit card number encrypted with FPE will still be a 16-digit number"
      },
      {
        "date": "2023-12-30T05:32:00.000Z",
        "voteCount": 1,
        "content": "No, the value using FPE can be decrypted with key. \n\"Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation.\"\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods"
      },
      {
        "date": "2023-10-18T02:16:00.000Z",
        "voteCount": 1,
        "content": "Customer service needs to see the original value, not possible with other options."
      },
      {
        "date": "2023-10-04T05:01:00.000Z",
        "voteCount": 1,
        "content": "of course B"
      },
      {
        "date": "2023-09-20T06:39:00.000Z",
        "voteCount": 2,
        "content": "I believe the crux to the question is that the cryptographic format-preserving encryption token is re-identifiable, whereas the cryptographic hash is not: https://cloud.google.com/dlp/docs/transformations-reference\n\nTherefore, customer service can view the original values when necessary in case of D."
      },
      {
        "date": "2023-09-21T07:55:00.000Z",
        "voteCount": 2,
        "content": "Nevermind, this can actually also be done in the case of answer B. They are both correct, just different implementations. No idea"
      },
      {
        "date": "2023-07-31T04:08:00.000Z",
        "voteCount": 4,
        "content": "I don't see why we should use DLP since we know exactly the column that should be locked or encrypted. On the other hand having a cryptographic representation of SSN helps to aggregate/analyse entries. So I will vote for D, but B is much more easy to implement. Garbage question indeed."
      },
      {
        "date": "2023-07-26T22:33:00.000Z",
        "voteCount": 2,
        "content": "the question mentions that \"user data is sent to Pub/Sub before being ingested\" instead of just saying data goes to big query through pub/sub. So some alteration is expected before being injected into the big query. So option D should work."
      },
      {
        "date": "2023-07-23T12:22:00.000Z",
        "voteCount": 1,
        "content": "D. The question says giving CSR's access to values \"when necessary\" - not default access like given in B. D is a better option using the token."
      },
      {
        "date": "2023-07-10T00:08:00.000Z",
        "voteCount": 1,
        "content": "One of the key requirement is to be able to let authorized personel see the ID. D doesn't specify that."
      },
      {
        "date": "2023-05-14T09:52:00.000Z",
        "voteCount": 1,
        "content": "The answer is between B and D as well described in many comments. \n\nI personally do not see any reason to keep the information available using a token or a mask. It is not a PAN card number, it's just a personal ID. It should not be useful for analytical purposes. \n\nI'm gonna go for D then"
      },
      {
        "date": "2023-05-14T09:52:00.000Z",
        "voteCount": 1,
        "content": "sorry B"
      },
      {
        "date": "2023-05-03T04:07:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/dlp/docs/classification-redaction"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 169,
    "url": "https://www.examtopics.com/discussions/google/view/79685-exam-professional-data-engineer-topic-1-question-169/",
    "body": "You are migrating a table to BigQuery and are deciding on the data model. Your table stores information related to purchases made across several store locations and includes information like the time of the transaction, items purchased, the store ID, and the city and state in which the store is located. You frequently query this table to see how many of each item were sold over the past 30 days and to look at purchasing trends by state, city, and individual store. How would you model this table for the best query performance?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition by transaction time; cluster by state first, then city, then store ID.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition by transaction time; cluster by store ID first, then city, then state.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTop-level cluster by state first, then city, then store ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTop-level cluster by store ID first, then city, then state."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-04T14:51:00.000Z",
        "voteCount": 9,
        "content": "A. Partition by transaction time; cluster by state first, then city, then store ID."
      },
      {
        "date": "2022-12-16T16:12:00.000Z",
        "voteCount": 7,
        "content": "A\nPartitioning is obvious\nClustering is already  mentioned in the question \n past 30 days and to look at purchasing trends by \nstate, \ncity, and \nindividual store"
      },
      {
        "date": "2023-12-19T12:44:00.000Z",
        "voteCount": 2,
        "content": "over the past 30 days -&gt; partitioning\nby state, city, and individual store -&gt; cluster order"
      },
      {
        "date": "2023-12-19T12:44:00.000Z",
        "voteCount": 1,
        "content": "For optimal query performance in BigQuery, especially for the described use cases of analyzing sales data by time and geographical hierarchies, the data should be organized to minimize the amount of data scanned during queries. Given the frequent queries over the past 30 days and analysis by location, the best approach is:\n\nOption A: Partition by transaction time; cluster by state first, then city, then store ID."
      },
      {
        "date": "2023-12-19T12:44:00.000Z",
        "voteCount": 2,
        "content": "Partitioning the table by transaction time allows for efficient querying over specific time ranges, such as the past 30 days, which reduces costs and improves performance because it limits the amount of data scanned. \n\nClustering by state, then city, and then store ID aligns with the hierarchy of geographical data and the types of queries that are run against the dataset. It organizes the data within each partition so that queries filtering by state, city, or store ID\u2014or any combination of these\u2014are optimized, as BigQuery can limit the scan to just the relevant clusters within the partitions."
      },
      {
        "date": "2023-11-20T08:14:00.000Z",
        "voteCount": 1,
        "content": "Partition by ingest time\nPartition by specified data column (Id, State and City)"
      },
      {
        "date": "2023-10-18T02:19:00.000Z",
        "voteCount": 1,
        "content": "Partition by transaction time would lead to too many partitions - if it was a date, it would have made sense."
      },
      {
        "date": "2024-08-01T05:49:00.000Z",
        "voteCount": 1,
        "content": "Even though its a timestamp, the partitioning can be configured on a daily granularity, so A is correct (https://cloud.google.com/bigquery/docs/partitioned-tables#date_timestamp_partitioned_tables)"
      },
      {
        "date": "2023-10-04T11:41:00.000Z",
        "voteCount": 1,
        "content": "It should be C. not A"
      },
      {
        "date": "2023-10-04T11:39:00.000Z",
        "voteCount": 1,
        "content": "I think it should be C.\nThe fact that we partition the table with the time of the transaction will result many transactions in each day, so it will affect negatively the query performance.\ni.e : by the end of the day I will have many partitions if I use the transaction time. A would be correct if the partition was by date and not by time.\nResponse: C."
      },
      {
        "date": "2023-05-14T10:02:00.000Z",
        "voteCount": 4,
        "content": "Partitioning for time is obvious to improve performance and costs of querying only the last 30 days of the table. \n\nSo, the answer is A or B.\n\nhttps://cloud.google.com/bigquery/docs/querying-clustered-tables\n\n\"... To get the benefits of clustering, include all of the clustered columns or a subset of the columns in left-to-right sort order, starting with the first column.\"\n\nThis means that it is a better choice to sort the table rows by region-province-city (region-state-city in the US case). \n\nSo, the answer is A."
      },
      {
        "date": "2022-12-20T05:47:00.000Z",
        "voteCount": 2,
        "content": "Should be B\nThe clustering should be according to the filtering needs"
      },
      {
        "date": "2022-11-30T05:36:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\nThis page provides an overview of partitioned tables in BigQuery. A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query.\nYou can partition BigQuery tables by:\n- Time-unit column: Tables are partitioned based on a TIMESTAMP, DATE, or DATETIME column in the table.\n\nhttps://cloud.google.com/bigquery/docs/clustered-tables\nClustered tables in BigQuery are tables that have a user-defined column sort order using clustered columns. Clustered tables can improve query performance and reduce query costs."
      },
      {
        "date": "2022-09-08T23:33:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/querying-clustered-tables"
      },
      {
        "date": "2022-09-02T23:10:00.000Z",
        "voteCount": 3,
        "content": "A\nThe question mention that the query is 30 days recently"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 170,
    "url": "https://www.examtopics.com/discussions/google/view/79515-exam-professional-data-engineer-topic-1-question-170/",
    "body": "You are updating the code for a subscriber to a Pub/Sub feed. You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. Your subscriber is not set up to retain acknowledged messages. What should you do to ensure that you can recover from errors after deployment?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up the Pub/Sub emulator on your local machine. Validate the behavior of your new subscriber logic before deploying it to production.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build for your deployment. If an error occurs after deployment, use a Seek operation to locate a timestamp logged by Cloud Build at the start of the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable dead-lettering on the Pub/Sub topic to capture messages that aren't successfully acknowledged. If an error occurs after deployment, re-deliver any messages captured by the dead-letter queue."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T11:38:00.000Z",
        "voteCount": 12,
        "content": "B. Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.\n\nAccording to the second reference in the list below, a concern with deploying new subscriber code is that the new executable may erroneously acknowledge messages, leading to message loss. Incorporating snapshots into your deployment process gives you a way to recover from bugs in new subscriber code.\n\nAnswer cannot be C because To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.\n\nReferences: \nhttps://cloud.google.com/pubsub/docs/replay-message\nhttps://cloud.google.com/pubsub/docs/replay-overview#seek_use_cases"
      },
      {
        "date": "2022-12-07T08:44:00.000Z",
        "voteCount": 1,
        "content": "Don't think we need to configure subscription to retain ack messages. It is defaulted to retain for 7 days"
      },
      {
        "date": "2023-12-19T12:53:00.000Z",
        "voteCount": 1,
        "content": "Taking a snapshot allows redelivering messages that were published while any faulty subscriber logic was running. \nThe seek timestamp would come after deployment so even erroneously acknowledged messages could be recovered.\nhttps://cloud.google.com/pubsub/docs/replay-overview#seek_use_cases\nBy creating a snapshot of the subscription before deploying new code, you can preserve the state of unacknowledged messages. If after deployment you find that the new subscriber code is erroneously acknowledging messages, you can use the Seek operation with the snapshot to reset the subscription's acknowledgment state to the time the snapshot was created. This would effectively re-deliver messages available since the snapshot, ensuring you can recover from errors. This approach does not require setting up a local emulator and directly addresses the concern of message loss due to erroneous acknowledgments."
      },
      {
        "date": "2023-09-11T12:53:00.000Z",
        "voteCount": 3,
        "content": "B.\n\nfrom the documentation: \nhttps://cloud.google.com/pubsub/docs/replay-message \nPub/Sub cannot retrieve the messages after you have acknowledged them. However, sometimes you might find it necessary to replay the acknowledged messages, for example, if you performed an erroneous acknowledgment. Then you can use the Seek feature to mark previously acknowledged messages as unacknowledged, and force Pub/Sub to redeliver those messages. You can also use seek to delete the unacknowledged messages by changing their state to acknowledged."
      },
      {
        "date": "2023-07-25T02:14:00.000Z",
        "voteCount": 2,
        "content": "pls correct me if I am wrong , option B Option B only allows you to re-deliver messages that were available before the snapshot was created. If an error occurs after the snapshot was created, you will not be able to re-deliver those messages."
      },
      {
        "date": "2023-07-05T07:09:00.000Z",
        "voteCount": 3,
        "content": "Q: You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. \n   -&gt; So the message is mistakenly acked and removed from topic/subscription. This means even if you have a snapshot of pre-deployment but you don't have a backup or copy of post-deployment messages.\n\nQ: Your subscriber is not set up to retain acknowledged messages.\n  -&gt; To seek to a time in the past and replay previously-acknowledged messages, \"you must first configure message retention on the topic\" or \"configure the subscription to retain acknowledged messages\" (ref: https://cloud.google.com/pubsub/docs/replay-overview#configuring_message_retention)\n\nSo B, C, D do not solve the problem of erroneously acked messages as long as you don't have message retention configured on topic/subscription."
      },
      {
        "date": "2023-03-26T03:22:00.000Z",
        "voteCount": 1,
        "content": "You are updating the code for a subscriber to a Pub/Sub feed. You are concerned that upon deployment the subscriber may erroneously acknowledge messages, leading to message loss. Your subscriber is not set up to retain acknowledged messages. What should you do to ensure that you can recover from errors after deployment?\nA. Set up the Pub/Sub emulator on your local machine. Validate the behavior of your new subscriber logic before deploying it to production.\nB. Create a Pub/Sub snapshot before deploying new subscriber code. Use a Seek operation to re-deliver messages that became available after the snapshot was created.\nC. Use Cloud Build for your deployment. If an error occurs after deployment, use a Seek operation to locate a timestamp logged by Cloud Build at the start of the deployment.\nD. Enable dead-lettering on the Pub/Sub topic to capture messages that aren't successfully acknowledged. If an error occurs after deployment, re-deliver any messages captured by the dead-letter queue."
      },
      {
        "date": "2023-02-18T12:06:00.000Z",
        "voteCount": 2,
        "content": "Option D:\nDead letter option allow you to recover message from errors after deployment by re-delivering any messages captured by the dead-letter queue. \nhttps://cloud.google.com/pubsub/docs/handling-failures#dead_letter_topic\nwhy not B, \nbecause snapshot is time taking process and if messages were erroneously acknowledged, it will not bring them back. It is useful when you want to secure the current data and want to make changes"
      },
      {
        "date": "2023-03-08T07:24:00.000Z",
        "voteCount": 5,
        "content": "Dead letter queue would help if the messages would not get acknowledged, however here they are talking about messages being erroneously acknowledged. Pub/Sub would interpret the message as being succesfully processed -&gt; they would not end up in the dead-letter queue -&gt; D is wrong"
      },
      {
        "date": "2022-11-30T05:31:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/pubsub/docs/replay-overview\nThe Seek feature extends subscriber functionality by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the state of one subscription to another by using seek in combination with a Snapshot."
      },
      {
        "date": "2022-11-23T09:36:00.000Z",
        "voteCount": 1,
        "content": "B\nThe Seek feature extends subscriber functionality by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the state of one subscription to another by using seek in combination with a Snapshot.\n\nhttps://cloud.google.com/pubsub/docs/replay-overview"
      },
      {
        "date": "2022-09-08T23:47:00.000Z",
        "voteCount": 1,
        "content": "Answer B"
      },
      {
        "date": "2022-09-03T06:46:00.000Z",
        "voteCount": 1,
        "content": "should be B."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 171,
    "url": "https://www.examtopics.com/discussions/google/view/79520-exam-professional-data-engineer-topic-1-question-171/",
    "body": "You work for a large real estate firm and are preparing 6 TB of home sales data to be used for machine learning. You will use SQL to transform the data and use<br>BigQuery ML to create a machine learning model. You plan to use the model for predictions against a raw dataset that has not been transformed. How should you set up your workflow in order to prevent skew at prediction time?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWhen creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. Before requesting predictions, use a saved query to transform your raw input data, and then use ML.EVALUATE.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a BigQuery view to define your preprocessing logic. When creating your model, use the view as your model training data. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPreprocess all data using Dataflow. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any further transformations on the input data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 35,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T11:44:00.000Z",
        "voteCount": 14,
        "content": "A. When creating your model, use BigQuery's TRANSFORM clause to define preprocessing steps. At prediction time, use BigQuery's ML.EVALUATE clause without specifying any transformations on the raw input data.\n\nUsing the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning.\n\nReference: https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform"
      },
      {
        "date": "2022-11-29T07:28:00.000Z",
        "voteCount": 6,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery-ml/docs/bigqueryml-transform\nUsing the TRANSFORM clause, you can specify all preprocessing during model creation. The preprocessing is automatically applied during the prediction and evaluation phases of machine learning"
      },
      {
        "date": "2024-07-04T19:43:00.000Z",
        "voteCount": 2,
        "content": "The key to preventing skew in machine learning models is to ensure that the same data preprocessing steps are applied consistently to both the training data and the prediction data. In option B, the TRANSFORM clause in BigQuery ML is used to define preprocessing steps during model creation, and a saved query is used to apply the same transformations to the raw input data before making predictions. This ensures consistency and prevents skew. The ML.EVALUATE function is then used to evaluate the model\u2019s performance on the transformed prediction data. This is the recommended workflow"
      },
      {
        "date": "2024-01-13T13:23:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2023-04-21T04:47:00.000Z",
        "voteCount": 3,
        "content": "A is correct answer if we use TRANSFORM clause in BigQuery no need to use any transform while evaluating and predicting https://cloud.google.com/bigquery/docs/bigqueryml-transform"
      },
      {
        "date": "2023-01-19T22:51:00.000Z",
        "voteCount": 2,
        "content": "A is the correct answer"
      },
      {
        "date": "2022-12-16T04:07:00.000Z",
        "voteCount": 3,
        "content": "Problem: Skew\n\nOne thing that I overlooked when answering previously is that B, C does not address skew. When we preprocess our training data, we need to save our scaled factors somewhere, and when performing predictions on our test data, we need to use the scaling factors of our training data to predict the results.\n\nML.EVALUATE already incorporates preprocessing steps for our test data using the saved scaled factors."
      },
      {
        "date": "2022-10-26T21:02:00.000Z",
        "voteCount": 1,
        "content": "Stew prediction time by remove the preprocessing!"
      },
      {
        "date": "2022-09-08T22:43:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform\nAns A"
      },
      {
        "date": "2022-09-02T18:46:00.000Z",
        "voteCount": 2,
        "content": "This query's nested SELECT statement and FROM clause are the same as those in the CREATE MODEL query. Because the TRANSFORM clause is used in training, you don't need to specify the specific columns and transformations. They are automatically restored.\n\n\nReference: https://cloud.google.com/bigquery-ml/docs/bigqueryml-transform"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 172,
    "url": "https://www.examtopics.com/discussions/google/view/79521-exam-professional-data-engineer-topic-1-question-172/",
    "body": "You are analyzing the price of a company's stock. Every 5 seconds, you need to compute a moving average of the past 30 seconds' worth of data. You are reading data from Pub/Sub and using DataFlow to conduct the analysis. How should you set up your windowed pipeline?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a fixed window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a fixed window with a duration of 30 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow().plusDelayOf (Duration.standardSeconds(5))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a sliding window with a duration of 5 seconds. Emit results by setting the following trigger: AfterProcessingTime.pastFirstElementInPane().plusDelayOf (Duration.standardSeconds(30))",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow ()\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-07-24T13:27:00.000Z",
        "voteCount": 8,
        "content": "Option D: Sliding Window: Since you need to compute a moving average of the past 30 seconds' worth of data every 5 seconds, a sliding window is appropriate. A sliding window allows overlapping intervals and is well-suited for computing rolling aggregates.\n\nWindow Duration: The window duration should be set to 30 seconds to cover the required 30 seconds' worth of data for the moving average calculation.\n\nWindow Period: The window period or sliding interval should be set to 5 seconds to move the window every 5 seconds and recalculate the moving average with the latest data.\n\nTrigger: The trigger should be set to AfterWatermark.pastEndOfWindow() to emit the computed moving average results when the watermark advances past the end of the window. This ensures that all data within the window is considered before emitting the result."
      },
      {
        "date": "2022-09-02T11:48:00.000Z",
        "voteCount": 7,
        "content": "D. Use a sliding window with a duration of 30 seconds and a period of 5 seconds. Emit results by setting the following trigger: AfterWatermark.pastEndOfWindow ()\nReveal Solution"
      },
      {
        "date": "2024-06-14T21:30:00.000Z",
        "voteCount": 1,
        "content": "Option D is the correct configuration because it uses a sliding window of 30 seconds with a period of 5 seconds, ensuring that the moving average is computed every 5 seconds based on the past 30 seconds of data. The trigger AfterWatermark.pastEndOfWindow() ensures timely and accurate results are emitted as the watermark progresses."
      },
      {
        "date": "2023-12-01T05:49:00.000Z",
        "voteCount": 2,
        "content": "AfterWatermark is an essential triggering condition in Dataflow that allows computations to be triggered based on event time rather than processing time. Then eliminate A&amp;C. Comparing B&amp;D, B will generate outcome every 30 seconds which is not what we want\n\nD. Using a sliding window with a duration of 30 seconds and a period of 5 seconds, and setting the trigger as AfterWatermark.pastEndOfWindow(), is a sliding window that generates results every 5 seconds, and each result includes data from the past 30 seconds. In other words, every 5 seconds, you get the average value of the most recent 30 seconds' data, and there is a 5-second overlap between these windows. This is what we want."
      },
      {
        "date": "2022-11-29T07:24:00.000Z",
        "voteCount": 2,
        "content": "D is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows\nYou set the following windows with the Apache Beam SDK or Dataflow SQL streaming extensions:\nHopping windows (called sliding windows in Apache Beam)\n\nA hopping window represents a consistent time interval in the data stream. Hopping windows can overlap, whereas tumbling windows are disjoint.\n\nFor example, a hopping window can start every thirty seconds and capture one minute of data. The frequency with which hopping windows begin is called the period. This example has a one-minute window and thirty-second period."
      },
      {
        "date": "2022-09-08T03:20:00.000Z",
        "voteCount": 4,
        "content": "Moving average \u2014\u2014&gt;  sliding window"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 173,
    "url": "https://www.examtopics.com/discussions/google/view/79524-exam-professional-data-engineer-topic-1-question-173/",
    "body": "You are designing a pipeline that publishes application events to a Pub/Sub topic. Although message ordering is not important, you need to be able to aggregate events across disjoint hourly intervals before loading the results to BigQuery for analysis. What technology should you use to process and load this data to<br>BigQuery while ensuring that it will scale with large volumes of events?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function to perform the necessary data processing that executes using the Pub/Sub trigger every time a new message is published to the topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a Cloud Function to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a batch Dataflow job to run hourly, pulling all available messages from the Pub/Sub topic and performing the necessary aggregations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a streaming Dataflow job that reads continually from the Pub/Sub topic and performs the necessary aggregations using tumbling windows.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-16T13:50:00.000Z",
        "voteCount": 11,
        "content": "D\n\nTUMBLE=&gt; fixed windows.\nHOP=&gt; sliding windows. \nSESSION=&gt; session windows."
      },
      {
        "date": "2023-02-24T15:06:00.000Z",
        "voteCount": 6,
        "content": "why not c ? as data is arriving hourly why we can use batch processing rather than streaming with 1 hour fixed window?"
      },
      {
        "date": "2023-04-14T04:11:00.000Z",
        "voteCount": 2,
        "content": "\"you need to be able to aggregate events across disjoint hourly intervals\" does not means data is arriving hourly. however, it's tricky! Answer D"
      },
      {
        "date": "2023-10-29T03:44:00.000Z",
        "voteCount": 1,
        "content": "I second your question. Noone who suggests Dataflow streaming (D) has given an explanation why an hourly batch job is insufficient."
      },
      {
        "date": "2023-10-29T03:42:00.000Z",
        "voteCount": 1,
        "content": "I second your question. Noone who suggests C has given an explanation why an hourly batch job is insufficient."
      },
      {
        "date": "2024-10-01T23:07:00.000Z",
        "voteCount": 1,
        "content": "Just to provide clarity to people asking \"why not C\" - the source is a pub/sub. Pub/Sub has a limit of 10 MB or 1000 messages for a single batch publish request, which means that batch dataflow will not necessarily be able to retrieve all messages. If the question had said \"there will always be less than 1000 messages and less than 10mb\", only then would batch be acceptable."
      },
      {
        "date": "2024-09-04T22:51:00.000Z",
        "voteCount": 1,
        "content": "The question asks for future scalability for large volumes of events, its better to go with streaming dataflow job."
      },
      {
        "date": "2023-11-21T12:20:00.000Z",
        "voteCount": 2,
        "content": "I just do not understand why this needs to be streamed. I understand that there might be a slight delay using batch processing but there is no indication this is critical data. Can someone please provide your thinking?"
      },
      {
        "date": "2023-07-24T13:20:00.000Z",
        "voteCount": 1,
        "content": "We can use TUMBLE(1 HOUR) to create hourly windows, where each window contains events from a specific hour."
      },
      {
        "date": "2023-07-24T13:17:00.000Z",
        "voteCount": 1,
        "content": "Option D : A streaming Dataflow job is the best way to process and load data from Pub/Sub to BigQuery in real time. This is because streaming Dataflow jobs can scale to handle large volumes of data, and they can perform aggregations using tumbling windows."
      },
      {
        "date": "2022-11-29T07:16:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#tumbling-windows"
      },
      {
        "date": "2022-10-16T18:43:00.000Z",
        "voteCount": 2,
        "content": "Answer D\nTumbling Windows = Fixed Windows"
      },
      {
        "date": "2022-09-22T03:46:00.000Z",
        "voteCount": 2,
        "content": "Answer D"
      },
      {
        "date": "2022-09-02T11:51:00.000Z",
        "voteCount": 2,
        "content": "D. Create a streaming Dataflow job that reads continually from the Pub/Sub topic and performs the necessary aggregations using tumbling windows.\n\nA tumbling window represents a consistent, disjoint time interval in the data stream.\n\nReference:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#tumbling-windows"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 174,
    "url": "https://www.examtopics.com/discussions/google/view/80144-exam-professional-data-engineer-topic-1-question-174/",
    "body": "You work for a large financial institution that is planning to use Dialogflow to create a chatbot for the company's mobile app. You have reviewed old chat logs and tagged each conversation for intent based on each customer's stated intention for contacting customer service. About 70% of customer requests are simple requests that are solved within 10 intents. The remaining 30% of inquiries require much longer, more complicated requests. Which intents should you automate first?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate the more complicated requests first because those require more of the agents' time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate a blend of the shortest and longest intents to be representative of all intents.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAutomate intents in places where common words such as 'payment' appear only once so the software isn't confused."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-20T13:07:00.000Z",
        "voteCount": 3,
        "content": "This is the best approach because it follows the Pareto principle (80/20 rule). By automating the most common 10 intents that address 70% of customer requests, you free up the live agents to focus their time and effort on the more complex 30% of requests that likely require human insight/judgement. Automating the simpler high-volume requests first allows the chatbot to handle those easily, efficiently routing only the trickier cases to agents. This makes the best use of automation for high-volume simple cases and human expertise for lower-volume complex issues."
      },
      {
        "date": "2023-07-24T13:11:00.000Z",
        "voteCount": 1,
        "content": "Option A : : By automating the intents that cover a significant majority (70%) of customer requests, you target the areas with the highest volume of interactions. This helps reduce the load on live agents, enabling them to focus on more complicated and time-consuming inquiries that require their expertise."
      },
      {
        "date": "2023-06-14T11:32:00.000Z",
        "voteCount": 1,
        "content": "A is the answer."
      },
      {
        "date": "2022-11-29T07:19:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/dialogflow/cx/docs/concept/agent-design#build-iteratively\nIf your agent will be large or complex, start by building a dialog that only addresses the top level requests. Once the basic structure is established, iterate on the conversation paths to ensure you're covering all of the possible routes an end-user may take."
      },
      {
        "date": "2022-09-18T00:55:00.000Z",
        "voteCount": 4,
        "content": "Correct answer: A\n\nAs it states in the documentation: \"If your agent will be large or complex, start by building a dialog that only addresses the top level requests. Once the basic structure is established, iterate on the conversation paths to ensure you're covering all of the possible routes an end-user may take.\" (https://cloud.google.com/dialogflow/cx/docs/concept/agent-design#build-iteratively)\n\nTherefore, you should initally automate the 70 % of the requests that are simpler before automating the more complicated ones."
      },
      {
        "date": "2022-09-04T14:50:00.000Z",
        "voteCount": 1,
        "content": "A. Automate the 10 intents that cover 70% of the requests so that live agents can handle more complicated requests."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 175,
    "url": "https://www.examtopics.com/discussions/google/view/79498-exam-professional-data-engineer-topic-1-question-175/",
    "body": "Your company is implementing a data warehouse using BigQuery, and you have been tasked with designing the data model. You move your on-premises sales data warehouse with a star data schema to BigQuery but notice performance issues when querying the data of the past 30 days. Based on Google's recommended practices, what should you do to speed up the query without increasing storage costs?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDenormalize the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tShard the data by customer ID.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMaterialize the dimensional data in views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the data by transaction date.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-02-12T14:12:00.000Z",
        "voteCount": 13,
        "content": "D is the right answer because it does not increase storage costs.\nA is not correct because denormalization typically increases the amount of storage needed."
      },
      {
        "date": "2023-12-01T05:09:00.000Z",
        "voteCount": 1,
        "content": "\"Agree with you, denormalize usually increases storage, which may lead to an increase in cost. As for speeding up the query without increasing storage costs, another method is to partition the data by transaction date.\""
      },
      {
        "date": "2023-12-14T04:18:00.000Z",
        "voteCount": 2,
        "content": "Bro, you are playing with words now. Gotta read the question fully."
      },
      {
        "date": "2023-09-25T06:13:00.000Z",
        "voteCount": 1,
        "content": "Some might say that Star schema is already denormalized, but it is considered relationnal (hence kind of normalized) from Google's perspective:\n\n\"BigQuery performs best when your data is denormalized. Rather than preserving a relational schema such as a star or snowflake schema, denormalize your data and take advantage of nested and repeated columns. Nested and repeated columns can maintain relationships without the performance impact of preserving a relational (normalized) schema.\"\n\nI would go for A\n\nhttps://cloud.google.com/bigquery/docs/nested-repeated#when_to_use_nested_and_repeated_columns"
      },
      {
        "date": "2023-10-17T06:19:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind to D because of the \"without increasing storage costs\" part."
      },
      {
        "date": "2023-07-24T13:08:00.000Z",
        "voteCount": 1,
        "content": "Option D - BigQuery supports partitioned tables, where the data is divided into smaller, manageable portions based on a chosen column (e.g., transaction date). By partitioning the data based on the transaction date, BigQuery can efficiently query only the relevant partitions that contain data for the past 30 days, reducing the amount of data that needs to be scanned.Partitioning does not increase storage costs. It organizes existing data in a more structured manner, allowing for better query performance without any additional storage expenses."
      },
      {
        "date": "2023-06-08T02:50:00.000Z",
        "voteCount": 2,
        "content": "A is not a bad idea, but this questions is written around \"please partition first on date\", which is common best practice. The \"storage\" remark is hinting on we are not going to 'explode' the data for the sake of performance."
      },
      {
        "date": "2022-12-09T07:12:00.000Z",
        "voteCount": 4,
        "content": "I think better option is [A] considering GCP Documentation: https://cloud.google.com/bigquery/docs/migration/schema-data-overview#denormalization \"BigQuery supports both star and snowflake schemas, but its native schema representation is neither of those two. It uses nested and repeated fields instead for a more natural representation of the data ..... Changing your schema to use nested and repeated fields is an excellent evolutionary choice. It reduces the number of joins required for your queries, and it aligns your schema with the BigQuery internal data representation. Internally, BigQuery organizes data using the Dremel model and stores it in a columnar storage format called Capacitor.\""
      },
      {
        "date": "2022-11-29T07:07:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/bigquery/docs/partitioned-tables\n A partitioned table is a special table that is divided into segments, called partitions, that make it easier to manage and query your data. By dividing a large table into smaller partitions, you can improve query performance, and you can control costs by reducing the number of bytes read by a query."
      },
      {
        "date": "2022-11-06T02:41:00.000Z",
        "voteCount": 4,
        "content": "A sneaky question. \n[D] Yes - Since data is queried with date criteria, partitioning by transaction date will surely speed it up without further cost.\n[A] Yes? - Star schema is a denormalized model but as user Reall01 pointed out, the option to use nested and repeated fields can be considered a further denormalization. And if the model hasn't frequently changing dimensions, this kind of denormalization will result in increased performance, according to https://cloud.google.com/bigquery/docs/loading-data#loading_denormalized_nested_and_repeated_data :\n\"In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. Avoid denormalization in these use cases:\n - You have a star schema with frequently changing dimensions\"\n\nI guess that the person who added this question, had in mind [D] as a correct answer. If the questioner had all the aforementioned under consideration, would state clearly if there are frequently changing dimensions in the schema."
      },
      {
        "date": "2022-10-18T07:32:00.000Z",
        "voteCount": 1,
        "content": "Star schema is supported by Big Query but is not the most efficient form, if you should design a schema from scratch google recommend to use nested and repeated fields.\n\nIn this case, you already have done a migration of the schema and data, so it sounds good and with less effort to do partitioning by transaction date than to redesign the schema.\n\nAnd other aspect to consider is that this is a data warehouse, so is sure that there is an ETL process and if you change the schema you must adapt the ETL process.\n\nI vote for D."
      },
      {
        "date": "2022-10-04T21:08:00.000Z",
        "voteCount": 2,
        "content": "Star schema is not denormalized itself, but this assumes you already have moved ur data to big query, because you are querying. So, as BQ is not relational, the data already have been denormalized. I go with D."
      },
      {
        "date": "2022-09-13T04:25:00.000Z",
        "voteCount": 1,
        "content": "I think Denormalizing here means ,using big queries native data representation and that is using nested and repeated columns .Thats is the best practice in GCP \nhttps://cloud.google.com/bigquery/docs/nested-repeated#example"
      },
      {
        "date": "2022-09-05T13:34:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/migration/schema-data-overview#migrating_data_and_schema_from_on-premises_to_bigquery\n\nStar schema. This is a denormalized model, where a fact table collects metrics such as order amount, discount, and quantity, along with a group of keys. These keys belong to dimension tables such as customer, supplier, region, and so on. Graphically, the model resembles a star, with the fact table in the center surrounded by dimension tables.\n\nStar schema is already denormalized so partition makes more sense going with D"
      },
      {
        "date": "2022-09-19T15:50:00.000Z",
        "voteCount": 2,
        "content": "If you drill down within that link and land at: https://cloud.google.com/architecture/bigquery-data-warehouse it mentions \u201c In some cases, you might want to use nested and repeated fields to denormalize your data.\u201d under schema design.  Feels like a poorly written question since all depends on what context you take things in as \u201cdenormalization\u201d"
      },
      {
        "date": "2022-11-06T02:18:00.000Z",
        "voteCount": 2,
        "content": "You bring up a valid point. According to denormalization best practices, there is a critical info missing in order to decide whether further denormalization with nested and repeated fields could help, if there are frequently changing dimensions. Here's a quote from https://cloud.google.com/bigquery/docs/loading-data#loading_denormalized_nested_and_repeated_data :\n\"In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. Avoid denormalization in these use cases:\n- You have a star schema with frequently changing dimensions.\""
      },
      {
        "date": "2023-06-24T12:33:00.000Z",
        "voteCount": 1,
        "content": "In some circumstances, denormalizing your data and using nested and repeated fields doesn't result in increased performance. For example, star schemas are typically optimized schemas for analytics, and as a result, performance might not be significantly different if you attempt to denormalize further.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-nested"
      },
      {
        "date": "2022-09-02T11:57:00.000Z",
        "voteCount": 3,
        "content": "D. Partition the data by transaction date.\n\nStar schema is already denormalized."
      },
      {
        "date": "2022-09-02T11:10:00.000Z",
        "voteCount": 2,
        "content": "should be D, not A"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 176,
    "url": "https://www.examtopics.com/discussions/google/view/79501-exam-professional-data-engineer-topic-1-question-176/",
    "body": "You have uploaded 5 years of log data to Cloud Storage. A user reported that some data points in the log data are outside of their expected ranges, which indicates errors. You need to address this issue and be able to run the process again in the future while keeping the original data for compliance reasons. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the data from Cloud Storage into BigQuery. Create a new BigQuery table, and skip the rows with errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Compute Engine instance and create a new copy of the data in Cloud Storage. Skip the rows with errors.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to the same dataset in Cloud Storage."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T12:25:00.000Z",
        "voteCount": 9,
        "content": "C. Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage.\n\nYou can't filter out data using BQ load commands. You must imbed the logic to filter out data (i.e. time ranges) in another decoupled way (i.e. Dataflow, Cloud Functions, etc.). Therefore, A and B add additional complexity and deviates from the Data Lake design paradigm. D is wrong as the question strictly implies that the existing data set needs to be retained for compliance."
      },
      {
        "date": "2024-02-12T03:53:00.000Z",
        "voteCount": 1,
        "content": "why not D if the versioning is activated while creating your bucket ?"
      },
      {
        "date": "2023-12-20T13:43:00.000Z",
        "voteCount": 2,
        "content": "Option C is the best approach in this situation. Here is why:\n\nOption A would remove data which may be needed for compliance reasons. Keeping the original data is preferred.\nOption B makes a copy of the data but still removes potentially useful records. Additional storage costs would be incurred as well.\nOption C uses Dataflow to clean the data by setting out of range values while keeping the original data intact. The fixed records are written to a new location for further analysis. This meets the requirements.\nOption D writes the fixed data back to the original location, overwriting the original data. This would violate the compliance needs to keep the original data untouched.\nSo option C leverages Dataflow to properly clean the data while preserving the original data for compliance, at reasonable operational costs. This best achieves the stated requirements."
      },
      {
        "date": "2023-08-16T14:04:00.000Z",
        "voteCount": 4,
        "content": "Strange answers... Since when does cloud storage have datasets? Lol\nKeeping this in mind, the answer must be C, but none is really correcg"
      },
      {
        "date": "2023-01-01T10:04:00.000Z",
        "voteCount": 1,
        "content": "C. Create a Dataflow workflow that reads the data from Cloud Storage, checks for values outside the expected range, sets the value to an appropriate default, and writes the updated records to a new dataset in Cloud Storage."
      },
      {
        "date": "2022-11-29T07:05:00.000Z",
        "voteCount": 3,
        "content": "C is the answer."
      },
      {
        "date": "2022-09-02T11:15:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 177,
    "url": "https://www.examtopics.com/discussions/google/view/79540-exam-professional-data-engineer-topic-1-question-177/",
    "body": "You want to rebuild your batch pipeline for structured data on Google Cloud. You are using PySpark to conduct data transformations at scale, but your pipelines are taking over twelve hours to run. To expedite development and pipeline run time, you want to use a serverless tool and SOL syntax. You have already moved your raw data into Cloud Storage. How should you build the pipeline on Google Cloud while meeting speed and processing requirements?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConvert your PySpark commands into SparkSQL queries to transform the data, and then run your pipeline on Dataproc to write the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest your data into Cloud SQL, convert your PySpark commands into SparkSQL queries to transform the data, and then use federated quenes from BigQuery for machine learning.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIngest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Beam Python SDK to build the transformation pipelines, and write the data into BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 30,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-05T09:12:00.000Z",
        "voteCount": 13,
        "content": "The question is C but not because the SQL Syntax, as you can perfectly use SparkSQL on Dataproc reading files from GCS. It's because the \"serverless\" requirement."
      },
      {
        "date": "2024-01-16T09:09:00.000Z",
        "voteCount": 2,
        "content": "A) Looks more suitable , serverless approach for handling and performance."
      },
      {
        "date": "2023-12-20T13:49:00.000Z",
        "voteCount": 3,
        "content": "Option C is the best approach to meet the stated requirements. Here's why:\n\nBigQuery SQL provides a fast, scalable, and serverless method for transforming structured data, easier to develop than PySpark.\nDirectly ingesting the raw Cloud Storage data into BigQuery avoids needing an intermediate processing cluster like Dataproc.\nTransforming the data via BigQuery SQL queries will be faster than PySpark, especially since the data is already loaded into BigQuery.\nWriting the transformed results to a new BigQuery table keeps the original raw data intact and provides a clean output.\nSo migrating to BigQuery SQL for transformations provides a fully managed serverless architecture that can significantly expedite development and reduce pipeline runtime versus PySpark. The ability to avoid clusters and conduct transformations completely within BigQuery is the most efficient approach here."
      },
      {
        "date": "2023-07-11T00:25:00.000Z",
        "voteCount": 3,
        "content": "Note: Dataproc by itself is not serverless\nhttps://cloud.google.com/dataproc-serverless/docs/overview"
      },
      {
        "date": "2023-04-21T05:01:00.000Z",
        "voteCount": 3,
        "content": "because of serverless nature"
      },
      {
        "date": "2023-02-18T13:02:00.000Z",
        "voteCount": 1,
        "content": "Answer C: need to setup SQL based job means transformation in not very complex. And Biqquery sql are faster than spark sql context. (google claims)\nHowever, i will make a test by myself to check it."
      },
      {
        "date": "2023-01-24T06:53:00.000Z",
        "voteCount": 2,
        "content": "In the GCP Machine Learning Engineer practice question (Q4) there's the same question with similar answers and the correct answer is A since B \"is incorrect, here transformation is done on Cloud SQL, which wouldn\u2019t scale the process\" and C \"is incorrect as this process wouldn\u2019t scale the data transformation routine. And, it is always better to transform data during ingestion\": https://medium.com/@gcpguru/google-google-cloud-professional-machine-learning-engineer-practice-questions-part-1-3ee4a2b3f0a4"
      },
      {
        "date": "2023-06-01T23:40:00.000Z",
        "voteCount": 2,
        "content": "Dataproc is not a serverless tool unless it mentions \"Dataproc Serverless\" explicitly."
      },
      {
        "date": "2022-11-23T11:11:00.000Z",
        "voteCount": 1,
        "content": "C\nD-is incorrect because you are rebuild your batch pipeline for structured data on Google Cloud."
      },
      {
        "date": "2022-11-23T11:14:00.000Z",
        "voteCount": 2,
        "content": "A could be answer if it was Dataproc serverless and no conversion of code. Dp serverless support: scala,pyspark,sparksql and SparkR"
      },
      {
        "date": "2022-09-22T03:33:00.000Z",
        "voteCount": 4,
        "content": "This same question is there on Google's Professional Machine Learning Engineer, Question 4\nAnswer is C."
      },
      {
        "date": "2022-09-12T10:54:00.000Z",
        "voteCount": 2,
        "content": "I choose C \nBigQuery SQL is more performant but more expensive. Here, it's a performance issue ( time reduction)\nSource : https://medium.com/paypal-tech/comparing-bigquery-processing-and-spark-dataproc-4c90c10e31ac"
      },
      {
        "date": "2022-09-10T03:59:00.000Z",
        "voteCount": 1,
        "content": "C is the most likely  ,  bigquery is severless   and sql  \nD is dataflow severless but it is wrong at using python sdk but using sql beam rthen it will be correct"
      },
      {
        "date": "2022-09-07T01:28:00.000Z",
        "voteCount": 2,
        "content": "Answer C"
      },
      {
        "date": "2022-09-02T18:57:00.000Z",
        "voteCount": 1,
        "content": "A\n- You have to maintain PySpark Code -&gt; Proc"
      },
      {
        "date": "2022-09-02T23:32:00.000Z",
        "voteCount": 1,
        "content": "After thinking a while, I think the question is not clear enough. To be honest"
      },
      {
        "date": "2022-09-02T23:33:00.000Z",
        "voteCount": 1,
        "content": "A or C. I go for C because they said they want to use SQL syntax..."
      },
      {
        "date": "2022-09-02T12:30:00.000Z",
        "voteCount": 2,
        "content": "C. Ingest your data into BigQuery from Cloud Storage, convert your PySpark commands into BigQuery SQL queries to transform the data, and then write the transformations to a new table.\n\nKeys: \"Serverless\" and \"SQL\""
      },
      {
        "date": "2022-09-02T13:15:00.000Z",
        "voteCount": 3,
        "content": "Changing answer to A as this is a new question referring to Dataproc Serverless. Dataproc Serverless for Spark batch workloads supports Spark SQL. Why modify ETL to ELT and convert PySpark to BigQuery SQL when it can be similar to a lift-and-shift?"
      },
      {
        "date": "2022-11-23T11:09:00.000Z",
        "voteCount": 3,
        "content": "Dataproc is diffrent than Dataproc Serveless. This question is talking about dataproc.\nBy the way dp serverless support both pyspark and sparkSql no need of conversion. \nC is best answer"
      },
      {
        "date": "2022-09-03T01:30:00.000Z",
        "voteCount": 1,
        "content": "The question said \"use SQL syntax\" \nC might still correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 178,
    "url": "https://www.examtopics.com/discussions/google/view/79547-exam-professional-data-engineer-topic-1-question-178/",
    "body": "You are testing a Dataflow pipeline to ingest and transform text files. The files are compressed gzip, errors are written to a dead-letter queue, and you are using<br>SideInputs to join data. You noticed that the pipeline is taking longer to complete than expected; what should you do to expedite the Dataflow job?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSwitch to compressed Avro files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the batch size.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRetry records that throw an error.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse CoGroupByKey instead of the SideInput.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 31,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T00:10:00.000Z",
        "voteCount": 15,
        "content": "D: it is most likely.\nThere are a lot of reference doc to tell about comparison between them\nhttps://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins\n\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-2\n\nhttps://stackoverflow.com/questions/58080383/sideinput-i-o-kills-performance"
      },
      {
        "date": "2022-11-29T06:58:00.000Z",
        "voteCount": 11,
        "content": "D is the answer.\n\nhttps://cloud.google.com/architecture/building-production-ready-data-pipelines-using-dataflow-developing-and-testing#choose_correctly_between_side_inputs_or_cogroupbykey_for_joins\nThe CoGroupByKey transform is a core Beam transform that merges (flattens) multiple PCollection objects and groups elements that have a common key. Unlike a side input, which makes the entire side input data available to each worker, CoGroupByKey performs a shuffle (grouping) operation to distribute data across workers. CoGroupByKey is therefore ideal when the PCollection objects you want to join are very large and don't fit into worker memory.\n\nUse CoGroupByKey if you need to fetch a large proportion of a PCollection object that significantly exceeds worker memory."
      },
      {
        "date": "2023-12-20T14:02:00.000Z",
        "voteCount": 1,
        "content": "To expedite the Dataflow job that involves ingesting and transforming text files, especially if the pipeline is taking longer than expected, the most effective strategy would be:\n\nD. Use CoGroupByKey instead of the SideInput."
      },
      {
        "date": "2023-12-20T14:02:00.000Z",
        "voteCount": 1,
        "content": "Here's why this approach is beneficial:\n\n1.\tEfficiency in Handling Large Datasets: SideInputs are not optimal for large datasets because they require that the entire dataset be available to each worker. This can lead to performance bottlenecks, especially if the dataset is large. CoGroupByKey, on the other hand, is more efficient for joining large datasets because it groups elements by key and allows the pipeline to process each key-group separately.\n\n2.\tScalability: CoGroupByKey is more scalable than SideInputs for large-scale data processing. It distributes the workload more evenly across the Dataflow workers, which can significantly improve the performance of your pipeline.\n\n3.\tBetter Resource Utilization: By using CoGroupByKey, the Dataflow job can make better use of its resources, as it doesn't need to replicate the entire dataset to each worker. This results in faster processing times and better overall efficiency."
      },
      {
        "date": "2023-12-20T14:03:00.000Z",
        "voteCount": 1,
        "content": "The other options may not be as effective:\n\n\u2022\tA (Switch to compressed Avro files): While Avro is a good format for certain types of data processing, simply changing the file format from gzip to Avro may not address the underlying issue causing the delay, especially if the problem is related to the way data is being joined or processed.\n\n\u2022\tB (Reduce the batch size): Reducing the batch size could potentially increase overhead and might not significantly improve the processing time, especially if the bottleneck is due to the method of data joining.\n\n\u2022\tC (Retry records that throw an error): Retrying errors could be useful in certain contexts, but it's unlikely to speed up the pipeline if the delay is due to inefficiencies in data processing methods like the use of SideInputs."
      },
      {
        "date": "2023-02-18T13:16:00.000Z",
        "voteCount": 1,
        "content": "Answer: B, \nreducing the batch size improve the speed performance also improve cpu utilisation. \nDead letter queues are generated for messages that are errorrly acknowledged and its good to use sideinputs for that to check small amount of errors in memory. \nCogroupbykey is not necessary for error messages. \nI see only batch size that can be customized to improve the performance. \nIn practical use case:\nyou check these tools Stackdriver Monitoring and Logging, Cloud Trace, and Cloud Profiler. and try to find the cause, if its file type issue in compression, or batch size."
      },
      {
        "date": "2022-12-16T13:25:00.000Z",
        "voteCount": 1,
        "content": "D\nFlatten will just merge all results into a single PCollection. To join them you can use CoGroupByKey"
      },
      {
        "date": "2022-10-06T23:41:00.000Z",
        "voteCount": 2,
        "content": "When optimizing for load speed, Avro file format is preferred. Avro is a binary row-based format which can be split and read in parallel by multiple slots including compressed files."
      },
      {
        "date": "2022-10-16T18:54:00.000Z",
        "voteCount": 1,
        "content": "that is for Big Query isn't?"
      },
      {
        "date": "2023-01-11T05:55:00.000Z",
        "voteCount": 1,
        "content": "datflow can use avro format sir. streaming or batching to bigquery in avro format it can"
      },
      {
        "date": "2022-10-06T23:40:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion"
      },
      {
        "date": "2022-10-05T09:15:00.000Z",
        "voteCount": 1,
        "content": "D probably, side inputs have to fit in memory. If the p-collection in the side input doesn't fit well in memory it's better to use CoGroupByKey."
      },
      {
        "date": "2022-09-28T23:14:00.000Z",
        "voteCount": 1,
        "content": "Answer A \nthe same question is in number 70 you transform the files to Avro using Dataflow"
      },
      {
        "date": "2023-06-22T22:03:00.000Z",
        "voteCount": 3,
        "content": "Avro requires the data to be at least semi-structured, because it wants a fixed schema. Text files are unstructured data, therefore it doesn't make sense to use Avro files for them"
      },
      {
        "date": "2022-09-21T09:57:00.000Z",
        "voteCount": 1,
        "content": "A switching to avro. No serialisation"
      },
      {
        "date": "2022-09-07T01:30:00.000Z",
        "voteCount": 2,
        "content": "Switch to Avro format \nAnswer A"
      },
      {
        "date": "2022-09-22T03:35:00.000Z",
        "voteCount": 1,
        "content": "https://docs.confluent.io/platform/current/schema-registry/serdes-develop/serdes-avro.html"
      },
      {
        "date": "2022-09-06T03:53:00.000Z",
        "voteCount": 3,
        "content": "D probably, side inputs have to fit in memory. If the p-collection in the side input doesn't fit well in memory it's better to use CoGroupByKey."
      },
      {
        "date": "2022-09-02T13:02:00.000Z",
        "voteCount": 4,
        "content": "B. Reduce the batch size."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 179,
    "url": "https://www.examtopics.com/discussions/google/view/79550-exam-professional-data-engineer-topic-1-question-179/",
    "body": "You are building a real-time prediction engine that streams files, which may contain PII (personal identifiable information) data, into Cloud Storage and eventually into BigQuery. You want to ensure that the sensitive data is masked but still maintains referential integrity, because names and emails are often used as join keys.<br>How should you use the Cloud Data Loss Prevention API (DLP API) to ensure that the PII data is not accessible by unauthorized individuals?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pseudonym by replacing the PII data with cryptogenic tokens, and store the non-tokenized data in a locked-down button.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRedact all PII data, and store a version of the unredacted data in a locked-down bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tScan every table in BigQuery, and mask the data it finds that has PII.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a pseudonym by replacing PII data with a cryptographic format-preserving token.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-29T06:53:00.000Z",
        "voteCount": 7,
        "content": "D is the answer.\n\nhttps://cloud.google.com/dlp/docs/pseudonymization#supported-methods\nFormat preserving encryption: An input value is replaced with a value that has been encrypted using the FPE-FFX encryption algorithm with a cryptographic key, and then prepended with a surrogate annotation, if specified. By design, both the character set and the length of the input value are preserved in the output value. Encrypted values can be re-identified using the original cryptographic key and the entire output value, including surrogate annotation."
      },
      {
        "date": "2024-01-18T16:23:00.000Z",
        "voteCount": 1,
        "content": "D&gt; Looks more suitable as it will handle Referential integrity. https://cloud.google.com/dlp/docs/pseudonymization"
      },
      {
        "date": "2023-11-22T13:56:00.000Z",
        "voteCount": 2,
        "content": "answer A\nhttps://cloud.google.com/dlp/docs/transformations-reference Replaces an input value with a token, or surrogate value, of the same length using AES in Synthetic Initialization Vector mode (AES-SIV). This transformation method, unlike format-preserving tokenization, has no limitation on supported string character sets, generates identical tokens for each instance of an identical input value, and uses surrogates to enable re-identification given the original encryption key."
      },
      {
        "date": "2023-08-13T04:52:00.000Z",
        "voteCount": 1,
        "content": "D is correct."
      },
      {
        "date": "2023-06-23T08:18:00.000Z",
        "voteCount": 2,
        "content": "I've also asked to GPT but I had to remind the hard condition \"names and emails are often used as join keys\".\nIt changed the answer to \"B\" after 3rd iteration.\n\nmasking all PII data may not satisfy the requirement of using names and emails as join keys, as the data is obfuscated and cannot be used for accurate join operations.\n\nIn this approach, you would redact or remove the sensitive PII data, such as names and emails, from the dataset that will be used for real-time processing and analysis. The redacted data would be stored in the primary dataset to ensure that sensitive information is not accessible.\n\nAdditionally, you would create a copy of the original dataset with the PII data still intact, but this copy would be stored in a locked-down bucket with restricted access. This ensures that authorized individuals who need access to the unredacted data for specific purposes, such as join operations, can retrieve it from the secured location."
      },
      {
        "date": "2023-07-05T07:34:00.000Z",
        "voteCount": 2,
        "content": "made a typo up there, it has to be A"
      },
      {
        "date": "2023-04-30T07:21:00.000Z",
        "voteCount": 1,
        "content": "gpt: \nThe recommended approach for using the Cloud Data Loss Prevention API (DLP API) to protect sensitive PII data while maintaining referential integrity is to create pseudonyms by replacing the PII data with cryptographic format-preserving tokens.\nThis approach ensures that sensitive data is not accessible by unauthorized individuals, while still preserving the format and length of the original data, which is essential for maintaining referential integrity.\n\nReplacing PII data with cryptogenic tokens, as mentioned in option A, is not recommended because cryptogenic tokens are not necessarily format-preserving, and this could affect the accuracy of data joins.\n\nTherefore, option D is the best approach for using the DLP API to ensure that PII data is not accessible by unauthorized individuals while still maintaining referential integrity."
      },
      {
        "date": "2023-05-09T01:49:00.000Z",
        "voteCount": 5,
        "content": "You shouldn't use ChatGPT as a source, the data used are not up to date and for such complex question a predicting text chatbot can help but, it's better to refer to the google documentation."
      },
      {
        "date": "2023-05-10T07:44:00.000Z",
        "voteCount": 1,
        "content": "that`s why i always mark \"gpt\", when copy from there... i know, thx\n\nalso, it might be A. Or D... Confusing question."
      },
      {
        "date": "2023-04-21T06:34:00.000Z",
        "voteCount": 3,
        "content": "here catch is \"cryptographic\" key"
      },
      {
        "date": "2023-02-18T13:23:00.000Z",
        "voteCount": 1,
        "content": "Answer D, \nkey word - \"referential integrity\" use format preserve option, it keeps same length of the value and last four digits of your value in column"
      },
      {
        "date": "2023-01-18T04:30:00.000Z",
        "voteCount": 1,
        "content": "The answer is D"
      },
      {
        "date": "2022-12-27T22:01:00.000Z",
        "voteCount": 1,
        "content": "I believe \"Format preserving token\" in option D makes it easier choice for me"
      },
      {
        "date": "2022-12-27T08:35:00.000Z",
        "voteCount": 1,
        "content": "D looks right"
      },
      {
        "date": "2022-12-20T07:47:00.000Z",
        "voteCount": 2,
        "content": "Question is super tricky,  B and C are not the answers since they do not maintain referential integrity.\n\nFor D, it does preserve the length of input. But since we are only concerned with referencing during joins, there is no point of maintaining the length anyway. Also, characters must be encoded as ASCII, this means that the name and email must be within the 256 character set. which is further limited to the alphabet characters, i.e. 94 characters.  (https://cloud.google.com/dlp/docs/transformations-reference#crypto)\n\nNames nowadays do not just have ASCII characters but unicode as well, so D will not necessarily work all the time."
      },
      {
        "date": "2022-12-16T13:22:00.000Z",
        "voteCount": 4,
        "content": "D is the answer\nPseudonymization is a de-identification technique that replaces sensitive data values with cryptographically generated tokens.\n\nKeywords: You want to ensure that the sensitive data is masked but still maintains referential integrity\nPart1- data is masked-Create a pseudonym by replacing PII data with a cryptographic token\nPart 2 still maintains referential integrity- with a cryptographic format-preserving token\nA Not an answer because\nthe locked-down button does not seem to google cloud word"
      },
      {
        "date": "2023-03-22T08:30:00.000Z",
        "voteCount": 1,
        "content": "\"button\" is just a typo for \"bucket\""
      },
      {
        "date": "2022-11-20T07:34:00.000Z",
        "voteCount": 1,
        "content": "Though both option A nad D maintains referential integrity,question is why you wnat to keep untokenize data in GCS,best way is option D which even support Reversible feature which is not supported by option A refer chart in reference document.\n\nreference:-\nhttps://cloud.google.com/dlp/docs/pseudonymization"
      },
      {
        "date": "2022-11-06T15:40:00.000Z",
        "voteCount": 3,
        "content": "It's D.\n\"You want to ensure that the sensitive data is masked but still maintains referential integrity.\"\nThey don't ask you to also keep the original data (which answer A relates to). \nAlso, format-preservation is important in this case."
      },
      {
        "date": "2022-11-06T15:41:00.000Z",
        "voteCount": 1,
        "content": "And, answer A does not include format preservation, which would lose referential integrity."
      },
      {
        "date": "2022-11-06T18:12:00.000Z",
        "voteCount": 1,
        "content": "I think that this isn't true.\nLook at the table https://cloud.google.com/dlp/docs/transformations-reference#transformation_methods and notice the 6th line \"Pseudonymization by replacing input value with cryptographic hash\" (which refers to the case of answer [A]). Referential integrity is preserved."
      },
      {
        "date": "2022-11-04T14:16:00.000Z",
        "voteCount": 3,
        "content": "[B] and [C] aren't correct since they don't preserve referential integrity.\n[A] describes, in other words, Cryptographic hashing, where the sensitive data is replaced with a hashed value. The hashed value can't be reversed (https://cloud.google.com/dlp/docs/transformations-reference#crypto-hashing) so the phrase \"store the non-tokenized data in a locked-down button (bucket)\" ensures that data can be restored if needed.\n[D] seems to be a valid option too. However, in https://cloud.google.com/dlp/docs/pseudonymization#fpe-ffx, there is a warning: \n\"FPE provides fewer security guarantees compared to other deterministic encryption methods such as AES- SIV ... ... For these reasons, Google strongly recommends using deterministic encryption with AES-SIV instead of FPE for all security sensitive use cases\"\nSince there is no option to select Deterministic Encryption, and the question doesn't require to preserve the format of the data (keep the same length of data), I choose [A] as a more secure approach."
      },
      {
        "date": "2022-11-04T14:18:00.000Z",
        "voteCount": 1,
        "content": "The table in https://cloud.google.com/dlp/docs/transformations-reference#transformation_methods shows which transformation preserves referential integrity"
      },
      {
        "date": "2023-08-08T08:39:00.000Z",
        "voteCount": 1,
        "content": "From here I see if A really meant Cryptographic hashing then it also satisfy referential integrity https://cloud.google.com/dlp/docs/pseudonymization#:~:text=following%20the%20table.-,Deterministic%20encryption%20using%20AES%2DSIV,-Format%20preserving%20encryption\nHowever, I cant see why A means Crpyptographic Hashing, no defition I can find online at all."
      },
      {
        "date": "2022-10-21T09:11:00.000Z",
        "voteCount": 2,
        "content": "All answers are obsolete since https://cloud.google.com/bigquery/docs/column-data-masking-intro"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 180,
    "url": "https://www.examtopics.com/discussions/google/view/79552-exam-professional-data-engineer-topic-1-question-180/",
    "body": "You are migrating an application that tracks library books and information about each book, such as author or year published, from an on-premises data warehouse to BigQuery. In your current relational database, the author information is kept in a separate table and joined to the book information on a common key. Based on Google's recommended practice for schema design, how would you structure the data to ensure optimal speed of queries about the author of each book that has been borrowed?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the schema the same, maintain the different tables for the book and each of the attributes, and query as you are doing today.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table that is wide and includes a column for each attribute, including the author's first name, last name, date of birth, etc.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table that includes information about the books and authors, but nest the author fields inside the author column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tKeep the schema the same, create a view that joins all of the tables, and always query the view."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-02-18T13:27:00.000Z",
        "voteCount": 10,
        "content": "C\nif data is time based or sequential, find  partition and cluster option\nif data is not time based, \nalways look for denomalize / nesting option."
      },
      {
        "date": "2022-11-29T06:39:00.000Z",
        "voteCount": 5,
        "content": "C is the answer.\n\nhttps://cloud.google.com/bigquery/docs/best-practices-performance-nested\nBest practice: Use nested and repeated fields to denormalize data storage and increase query performance.\n\nDenormalization is a common strategy for increasing read performance for relational datasets that were previously normalized. The recommended way to denormalize data in BigQuery is to use nested and repeated fields. It's best to use this strategy when the relationships are hierarchical and frequently queried together, such as in parent-child relationships."
      },
      {
        "date": "2023-01-01T10:15:00.000Z",
        "voteCount": 1,
        "content": "C. Create a table that includes information about the books and authors, but nest the author fields inside the author column."
      },
      {
        "date": "2022-11-23T11:44:00.000Z",
        "voteCount": 2,
        "content": "C\nBest practice: Use nested and repeated fields to denormalize data storage and increase query performance."
      },
      {
        "date": "2022-11-17T03:36:00.000Z",
        "voteCount": 2,
        "content": "Use nested and repeated fields to denormalize data storage which will increase query performance.BigQuery doesn't require a completely flat denormalization. You can use nested and repeated fields to maintain relationships"
      },
      {
        "date": "2022-09-12T10:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigquery/docs/best-practices-performance-nested"
      },
      {
        "date": "2022-09-02T19:06:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      },
      {
        "date": "2022-09-02T13:22:00.000Z",
        "voteCount": 2,
        "content": "C. Create a table that includes information about the books and authors, but nest the author fields inside the author column."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 181,
    "url": "https://www.examtopics.com/discussions/google/view/79924-exam-professional-data-engineer-topic-1-question-181/",
    "body": "You need to give new website users a globally unique identifier (GUID) using a service that takes in data points and returns a GUID. This data is sourced from both internal and external systems via HTTP calls that you will make via microservices within your pipeline. There will be tens of thousands of messages per second and that can be multi-threaded. and you worry about the backpressure on the system. How should you design your pipeline to minimize that backpressure?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCall out to the service via HTTP.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the pipeline statically in the class definition.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new object in the startBundle method of DoFn.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBatch the job into ten-second increments.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-29T23:44:00.000Z",
        "voteCount": 18,
        "content": "D:  I have insisted on this choice all aling.\n please read  find the keyword massive backpressure\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1\n\nif the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances you should consider batching these requests, instead."
      },
      {
        "date": "2022-11-08T03:09:00.000Z",
        "voteCount": 6,
        "content": "Thanks for sharing, you found exactly the same problem!\nThe document defitely proposes batching for this scenario.\n\nI'm quoting another part from the same example that would be useful for a similar question with different conditions:\n- If you're using a client in the DoFn that has heavy instantiation steps, rather than create that object in each DoFn call:\n    * If the client is thread-safe and serializable, create it statically in the class definition of the DoFn.\n    * If it's not thread-safe, create a new object in the startBundle method of DoFn. By doing so, the client will be reused across all elements of a bundle, saving initialization time."
      },
      {
        "date": "2022-11-23T19:30:00.000Z",
        "voteCount": 2,
        "content": "By the way if you see the shared Pseudocode, it's talking about start bundle and finish bundle of DoFn. The question is which one to choose to avoid back pressure? \nyou can see why you need to choose bundle instead of batching in below link\n Batching introduces some processing overhead as well as the need for a magic number to determine the key space.\nInstead, use the StartBundle and FinishBundle lifecycle elements to batch your data. With these options, no shuffling is needed.\nhttps://cloud.google.com/dataflow/docs/tutorials/ecommerce-java#micro-batch-calls"
      },
      {
        "date": "2022-11-25T01:29:00.000Z",
        "voteCount": 2,
        "content": "Valid points. but I don't change my mind, regarding the requirements of this particular question:\n- multi-threaded ability\n- no mention of heavy initialization steps or a lot of disk I/O (where shuffling might be a problem).\nAnd especially the excerpt:\n\"if the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances you should consider batching these requests, instead\"\nIt's like the guys that authored the question had this sentence in front of their eyes."
      },
      {
        "date": "2022-09-26T02:51:00.000Z",
        "voteCount": 8,
        "content": "D\nAll guys ,pls read carefully on Pattern: Calling external services for data enrichment\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1   \nA , B , C  all of them are solution for  norma case but if you  need to stand for backpressure,\nin last sector in Note : Note: When using this pattern, be sure to plan for the load that's placed on the external service and any associated backpressure. For example, imagine a pipeline that's processing tens of thousands of messages per second in steady state. If you made a callout per element, you would need the system to deal with the same number of API calls per second. Also, if the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances, you should consider batching these requests, instead.\n\nAnyone can share ideas to debate with me."
      },
      {
        "date": "2023-12-21T02:08:00.000Z",
        "voteCount": 1,
        "content": "Option D is the best approach to minimize backpressure in this scenario. By batching the jobs into 10-second increments, you can throttle the rate at which requests are made to the external GUID service. This prevents too many simultaneous requests from overloading the service.\n\nOption A would not help with backpressure since it just makes synchronous HTTP requests as messages arrive. Similarly, options B and C don't provide any inherent batching or throttling mechanism.\n\nBatching into time windows is a common strategy in stream processing to deal with high velocity data. The 10-second windows allow some buffering to happen, rather than making a call immediately for each message. This provides a natural throttling that can be tuned based on the external service's capacity."
      },
      {
        "date": "2023-12-21T02:09:00.000Z",
        "voteCount": 1,
        "content": "To design a pipeline that minimizes backpressure, especially when dealing with tens of thousands of messages per second in a multi-threaded environment, it's important to consider how each option affects system performance and scalability. Let's examine each of your options:"
      },
      {
        "date": "2023-12-21T02:09:00.000Z",
        "voteCount": 1,
        "content": "A. Call out to the service via HTTP: Making HTTP calls to an external service for each message can introduce significant latency and backpressure, especially at high throughput. This is due to the overhead of establishing a connection, waiting for the response, and handling potential network delays or failures."
      },
      {
        "date": "2023-12-21T02:09:00.000Z",
        "voteCount": 1,
        "content": "B. Create the pipeline statically in the class definition: While this approach can improve initialization time and reduce overhead during execution, it doesn't directly address the issue of backpressure caused by high message throughput."
      },
      {
        "date": "2023-12-21T02:10:00.000Z",
        "voteCount": 1,
        "content": "C. Create a new object in the startBundle method of DoFn: This approach is typically used in Apache Beam to initialize resources before processing a bundle of elements. While it can optimize resource usage and performance within each bundle, it doesn't inherently solve the backpressure issue caused by high message rates."
      },
      {
        "date": "2023-12-21T02:11:00.000Z",
        "voteCount": 1,
        "content": "D. Batch the job into ten-second increments: Batching messages can be an effective way to reduce backpressure. By grouping multiple messages into larger batches, you can reduce the frequency of external calls and distribute the processing load more evenly over time. This can lead to more efficient use of resources and potentially lower latency, as the system spends less time waiting on external services."
      },
      {
        "date": "2023-12-21T02:11:00.000Z",
        "voteCount": 1,
        "content": "Given these considerations, option D (Batch the job into ten-second increments) seems to be the most effective strategy for minimizing backpressure in your scenario. By batching messages, you can reduce the strain on your pipeline and external services, making the system more resilient and scalable under high load. However, the exact batch size and interval should be fine-tuned based on the specific characteristics of your workload and the capabilities of the external systems you are interacting with.\n\nAdditionally, it's important to consider other strategies in conjunction with batching, such as implementing efficient error handling, load balancing, and potentially using asynchronous I/O for external HTTP calls to further optimize performance and minimize backpressure."
      },
      {
        "date": "2023-05-01T04:21:00.000Z",
        "voteCount": 1,
        "content": "Option C is not correct because it does not address the issue of backpressure. Creating a new object in the startBundle method of DoFn will not help to reduce the number of calls that are made to the service, which can lead to backpressure.\n\nHere are some reasons why C is not correct:\n\nCreating a new object in the startBundle method of DoFn is not a scalable solution. As the number of messages increases, the number of objects that need to be created will also increase. This can lead to performance problems and memory usage issues.\nCreating a new object in the startBundle method of DoFn does not address the issue of backpressure. The service may still experience backpressure if the number of messages exceeds the service's capacity.\nA better solution would be to use batching to reduce the number of calls that are made to the service. This can help to improve performance and reduce backpressure."
      },
      {
        "date": "2023-04-30T07:43:00.000Z",
        "voteCount": 1,
        "content": "gpt: Option C is a better approach as it allows for object creation to occur in a more controlled manner within the DoFn, potentially reducing the pressure on the system. However, it could still create a large number of objects depending on the rate of incoming messages.\n\nOption D of batching the job into ten-second increments can also be a good solution to reduce backpressure on the system. This way, you can limit the number of messages being processed at any given time, which can help prevent bottlenecks and reduce the likelihood of backpressure.\n\nTherefore, the best approach would be to combine options C and D, creating a new object in the startBundle method of a DoFn, and batching the job into smaller time increments, such as 10 seconds. This way, you can control the rate of object creation and processing, which can help minimize backpressure on the system."
      },
      {
        "date": "2023-04-30T07:46:00.000Z",
        "voteCount": 1,
        "content": "another vague question, as we see... \nso, i`ll choose D... if i get this test\n\"However, depending on the specifics of your use case, one option may be better suited than the other. For example, if you have a high volume of incoming messages with occasional spikes, option D of batching the job into smaller time increments may be more effective in managing the load. On the other hand, if the incoming messages are more evenly distributed over time, option C of creating a new object in the startBundle method of DoFn may be a better option.\nUltimately, it may be necessary to experiment with both approaches and determine which one works best for your specific use case.\""
      },
      {
        "date": "2023-03-29T10:14:00.000Z",
        "voteCount": 1,
        "content": "D works.\nCould be C, but who said that the pipeline is in Dataflow/Beam?"
      },
      {
        "date": "2023-02-18T13:38:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nbatch increment in 10 sec, can improve load balancing, but overall back pressure (messages are generating more than consuming or publishing) in this case startBundle in DoFn or find other options in future like\n caching, \nload shedding (prioritising message flow), \nmessege queuing \nThese options handle backpressure..\nIf your cpu is performing bad then go with change in batch increment timing"
      },
      {
        "date": "2023-01-24T10:42:00.000Z",
        "voteCount": 3,
        "content": "I was hesitating between C and D, but then I realised this: https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1 \nHere is says \"If it's not thread-safe, create a new object in the startBundle method of DoFn.\" The task explicitly says \"There will be tens of thousands of messages per second and that can be multi-threaded.\" \nCorrect me if I'm wrong, but multi-threaded == thread-safe. Therefore, no need to go for the C approach."
      },
      {
        "date": "2022-11-29T06:36:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1\nFor example, imagine a pipeline that's processing tens of thousands of messages per second in steady state. If you made a callout per element, you would need the system to deal with the same number of API calls per second. Also, if the call takes on average 1 sec, that would cause massive backpressure on the pipeline. In these circumstances you should consider batching these requests, instead."
      },
      {
        "date": "2023-01-01T10:17:00.000Z",
        "voteCount": 1,
        "content": "D. Batch the job into ten-second increments."
      },
      {
        "date": "2022-11-23T19:27:00.000Z",
        "voteCount": 1,
        "content": "C\nC is an answer because\nFirst of all, no doubt that we should avoid single call of element that's why we use multi-threading else it overwhelm an external service endpoint.To avoid this issue, batch calls to external systems.\nBatch calls has also issue:GroupByKey transform or Apache Beam Timer API.\nthese approaches both require shuffling, which introduces some processing overhead as well as the need for a magic number to determine the key space.\nInstead, use the StartBundle and FinishBundle lifecycle elements to batch your data. With these options, no shuffling is needed.\nSource:\nhttps://cloud.google.com/dataflow/docs/tutorials/ecommerce-java#micro-batch-calls\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1\nSummary:\nStartBundle and FinishBundle do batch with no shuffling"
      },
      {
        "date": "2022-09-27T17:58:00.000Z",
        "voteCount": 1,
        "content": "Ans C: reference https://cloud.google.com/architecture/e-commerce/patterns/batching-external-calls"
      },
      {
        "date": "2022-09-18T03:55:00.000Z",
        "voteCount": 2,
        "content": "Based on the answers in this discussion thread, I would go for C. The most important link to support this choice is as following: https://cloud.google.com/architecture/e-commerce/patterns/batching-external-calls"
      },
      {
        "date": "2022-09-12T10:46:00.000Z",
        "voteCount": 1,
        "content": "Beam docs recommend batching \nhttps://beam.apache.org/documentation/patterns/grouping-elements-for-efficient-external-service-calls/"
      },
      {
        "date": "2022-09-10T02:39:00.000Z",
        "voteCount": 1,
        "content": "C , It is straight foward , You can take a look at https://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1  \nPattern : Calling external services for data enrichmen"
      },
      {
        "date": "2022-09-07T00:56:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nhttps://cloud.google.com/blog/products/data-analytics/guide-to-common-cloud-dataflow-use-case-patterns-part-1"
      },
      {
        "date": "2022-09-07T00:57:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/e-commerce/patterns/batching-external-calls\nTo support choice C"
      },
      {
        "date": "2022-09-06T04:16:00.000Z",
        "voteCount": 1,
        "content": "i think you are right gg"
      },
      {
        "date": "2022-09-05T02:59:00.000Z",
        "voteCount": 2,
        "content": "How about C?\nhttps://cloud.google.com/architecture/e-commerce/patterns/batching-external-calls"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 182,
    "url": "https://www.examtopics.com/discussions/google/view/79560-exam-professional-data-engineer-topic-1-question-182/",
    "body": "You are migrating your data warehouse to Google Cloud and decommissioning your on-premises data center. Because this is a priority for your company, you know that bandwidth will be made available for the initial data load to the cloud. The files being transferred are not large in number, but each file is 90 GB.<br>Additionally, you want your transactional systems to continually update the warehouse on Google Cloud in real time. What tools should you use to migrate the data and ensure that it continues to write to your warehouse?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStorage Transfer Service for the migration; Pub/Sub and Cloud Data Fusion for the real-time updates",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBigQuery Data Transfer Service for the migration; Pub/Sub and Dataproc for the real-time updates",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil for the migration; Pub/Sub and Dataflow for the real-time updates\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tgsutil for both the migration and the real-time updates"
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 25,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-29T06:32:00.000Z",
        "voteCount": 11,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data\nThe gsutil tool is the standard tool for small- to medium-sized transfers (less than 1 TB) over a typical enterprise-scale network, from a private data center to Google Cloud."
      },
      {
        "date": "2023-02-18T13:47:00.000Z",
        "voteCount": 1,
        "content": "what is wrong with A, there is no cost constraint"
      },
      {
        "date": "2023-01-01T10:20:00.000Z",
        "voteCount": 1,
        "content": "Agreed\n\nthx for sharing link"
      },
      {
        "date": "2022-09-02T13:55:00.000Z",
        "voteCount": 8,
        "content": "C. gsutil for the migration; Pub/Sub and Dataflow for the real-time updates\n\nUse Gsutil when there is enough bandwidth to meet your project deadline for less than 1 TB of data. Storage Transfer Service is for much larger volumes for migration. Moreover, Cloud Data Fusion and Dataproc are not ideal for real-time updates. BigQuery Data Transfer Service does not support all on-prem sources."
      },
      {
        "date": "2023-12-24T03:31:00.000Z",
        "voteCount": 1,
        "content": "Considering the requirement for handling large files and the need for real-time data integration, Option C (gsutil for the migration; Pub/Sub and Dataflow for the real-time updates) seems to be the most appropriate. gsutil will effectively handle the large file transfers, while Pub/Sub and Dataflow provide a robust solution for real-time data capture and processing, ensuring continuous updates to your warehouse on Google Cloud."
      },
      {
        "date": "2023-12-21T02:14:00.000Z",
        "voteCount": 1,
        "content": "Option C is the best choice given the large file sizes for the initial migration and the need for real-time updates after migration.\n\nSpecifically:\n\ngsutil can transfer large files in parallel over multiple TCP connections to maximize bandwidth. This works well for the 90GB files during initial migration.\nPub/Sub allows real-time messaging of updates that can then be streamed into Cloud Dataflow. Dataflow provides scalable stream processing to handle transforming and writing those updates into BigQuery or other sinks."
      },
      {
        "date": "2023-12-21T02:14:00.000Z",
        "voteCount": 2,
        "content": "Option A is incorrect because Storage Transfer Service is better for scheduled batch transfers, not ad hoc large migrations.\n\nOption B is incorrect because BigQuery Data Transfer Service is more focused on scheduled replication jobs, not ad hoc migrations.\n\nOption D would not work well for real-time updates after migration is complete.\n\nSo option C leverages the right Google cloud services for the one-time migration and ongoing real-time processing."
      },
      {
        "date": "2023-10-27T00:57:00.000Z",
        "voteCount": 1,
        "content": "agree with C"
      },
      {
        "date": "2022-09-07T01:02:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#gsutil_for_smaller_transfers_of_on-premises_data\nAnswer C"
      },
      {
        "date": "2022-09-06T04:20:00.000Z",
        "voteCount": 3,
        "content": "C seems legit"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 183,
    "url": "https://www.examtopics.com/discussions/google/view/79580-exam-professional-data-engineer-topic-1-question-183/",
    "body": "You are using Bigtable to persist and serve stock market data for each of the major indices. To serve the trading application, you need to access only the most recent stock prices that are streaming in. How should you design your row key and tables to ensure that you can access the data with the simplest query?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one unique table for all of the indices, and then use the index and timestamp as the row key design.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate one unique table for all of the indices, and then use a reverse timestamp as the row key design.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each index, have a separate table and use a timestamp as the row key design.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFor each index, have a separate table and use a reverse timestamp as the row key design."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T04:13:00.000Z",
        "voteCount": 17,
        "content": "This is special case , plese Take a look carefully the below link and read at last paragraph at the bottom of this comment, let everyone share idea, We will go with   B, C\nhttps://cloud.google.com/bigtable/docs/schema-design#time-based\n\nDon't use a timestamp by itself or at the beginning of a row key, because this will cause sequential writes to be pushed onto a single node, creating a hotspot.\n\nIf you usually retrieve the most recent records first, you can use a reversed timestamp in the row key by subtracting the timestamp from your programming language's maximum value for long integers (in Java, java.lang.Long.MAX_VALUE). With a reversed timestamp, the records will be ordered from most recent to least recent."
      },
      {
        "date": "2022-11-09T10:10:00.000Z",
        "voteCount": 2,
        "content": "I agree, based on the docs, B. Leading with a non-reversed timestamp will lead to hotspotting, reversed is the way to go."
      },
      {
        "date": "2022-11-29T06:28:00.000Z",
        "voteCount": 11,
        "content": "B is the answer.\n\nhttps://cloud.google.com/bigtable/docs/schema-design#time-based\nIf you usually retrieve the most recent records first, you can use a reversed timestamp in the row key by subtracting the timestamp from your programming language's maximum value for long integers (in Java, java.lang.Long.MAX_VALUE). With a reversed timestamp, the records will be ordered from most recent to least recent."
      },
      {
        "date": "2024-09-05T01:52:00.000Z",
        "voteCount": 2,
        "content": "1. Reverse Timestamp for most recent stock prices\n2. Having different table for each stock is more efficient, improves the query performance and option B doesn't specify stock in row key."
      },
      {
        "date": "2024-08-03T13:22:00.000Z",
        "voteCount": 1,
        "content": "Row keys that start with a timestamp (irrespective reversed or not) causes sequential writes to be pushed onto a single node, creating a hotspot. If you put a timestamp in a row key, precede it with a high-cardinality value (index in our case) to avoid hotspots.\n\nThe ideal option would be: \"use the index and reversed timestamp as the row key design\"."
      },
      {
        "date": "2024-01-14T09:27:00.000Z",
        "voteCount": 4,
        "content": "B is a correct answer because \"you need to access only the most recent stock prices\"\n\n\"If you usually retrieve the most recent records first, you can use a reversed timestamp in the row key by subtracting the timestamp from your programming language's maximum value for long integers (in Java, java.lang.Long.MAX_VALUE). With a reversed timestamp, the records will be ordered from most recent to least recent.\"\nhttps://cloud.google.com/bigtable/docs/schema-design#time-based"
      },
      {
        "date": "2023-12-24T03:35:00.000Z",
        "voteCount": 2,
        "content": "B. One unique table for all indices, reverse timestamp as row key:\n\nA single table for all indices keeps the structure simple.\nUsing a reverse timestamp as part of the row key ensures that the most recent data comes first in the sorted order. This design is beneficial for quickly accessing the latest data.\nFor example: you can convert the timestamp to a string and format it in reverse order, like \"yyyyMMddHHmmss\", ensuring newer dates and times are sorted lexicographically before older ones."
      },
      {
        "date": "2023-09-15T03:54:00.000Z",
        "voteCount": 1,
        "content": "Correct Is B"
      },
      {
        "date": "2023-08-21T07:20:00.000Z",
        "voteCount": 6,
        "content": "Option B using reverse timestamp only,  this is not the answer. \nthe right answer should be using the index and revers timestamp as the row key. \n\nSo, Option D is the only answer, because not A,B,C ."
      },
      {
        "date": "2023-07-31T05:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/schema-design#row-keys - If you usually retrieve the most recent records first, you can use a reversed timestamp\nB it is."
      },
      {
        "date": "2023-07-07T09:55:00.000Z",
        "voteCount": 1,
        "content": "A is the answer"
      },
      {
        "date": "2023-06-21T06:12:00.000Z",
        "voteCount": 1,
        "content": "the answer relieves on whether the application need to access the whole indexes at the same time or not. If yes then is B, if no is A.\n\nin mind the answer is yes, so B makes more sense: I retrieve all the list at the same time."
      },
      {
        "date": "2023-06-09T02:34:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/bigtable/docs/schema-design#time-based If you usually retrieve the most recent records first, you can use a reversed timestamp in the row key by subtracting the timestamp from your programming language's maximum value for long integers (in Java, java.lang.Long.MAX_VALUE). With a reversed timestamp, the records will be ordered from most recent to least recent."
      },
      {
        "date": "2023-06-08T03:46:00.000Z",
        "voteCount": 1,
        "content": "\"access the data with the simplest query\""
      },
      {
        "date": "2023-04-22T07:41:00.000Z",
        "voteCount": 3,
        "content": "yes reverse time stamp is recommended to prevent hot spot. But our query pattern is we need most recent record the is easy when you use Timestamp and Also option a stating that our row key not starting with time stamp which is index#timestamp and which is the most efficient way for this scenario."
      },
      {
        "date": "2023-03-20T02:03:00.000Z",
        "voteCount": 1,
        "content": "Reversed timestamp will definitely help here."
      },
      {
        "date": "2023-02-24T16:14:00.000Z",
        "voteCount": 1,
        "content": "Answer A:\nI checked on other website and chatgpt also suggested A, to use index per stock and timestamp."
      },
      {
        "date": "2023-02-22T20:58:00.000Z",
        "voteCount": 3,
        "content": "The key here is \"only the most recent stock prices\". Doesn't talk about accessing a specific index - so answer should be B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 184,
    "url": "https://www.examtopics.com/discussions/google/view/79593-exam-professional-data-engineer-topic-1-question-184/",
    "body": "You are building a report-only data warehouse where the data is streamed into BigQuery via the streaming API. Following Google's best practices, you have both a staging and a production table for the data. How should you design your data loading to ensure that there is only one master dataset without affecting performance on either the ingestion or reporting pieces?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave a staging table that is an append-only model, and then update the production table every three hours with the changes written to staging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave a staging table that is an append-only model, and then update the production table every ninety minutes with the changes written to staging.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave a staging table that moves the staged data over to the production table and deletes the contents of the staging table every three hours.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tHave a staging table that moves the staged data over to the production table and deletes the contents of the staging table every thirty minutes."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 29,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 4,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-06T06:40:00.000Z",
        "voteCount": 18,
        "content": "[C]\nI found the correct answer based on a real case, where Google's Solutions Architect team decided to move an internal process to use BigQuery.\nThe related doc is here: https://cloud.google.com/blog/products/data-analytics/moving-a-publishing-workflow-to-bigquery-for-new-data-insights"
      },
      {
        "date": "2022-11-06T06:40:00.000Z",
        "voteCount": 17,
        "content": "The interesting excerpts:\n\"Following common extract, transform, load (ETL) best practices, we used a staging table and a separate production table so that we could load data into the staging table without impacting users of the data. The design we created based on ETL best practices called for first deleting all the records from the staging table, loading the staging table, and then replacing the production table with the contents.\"\n\"When using the streaming API, the BigQuery streaming buffer remains active for about 30 to 60 minutes or more after use, which means that you can\u2019t delete or change data during that time. Since we used the streaming API, we scheduled the load every three hours to balance getting data into BigQuery quickly and being able to subsequently delete the data from the staging table during the load process.\""
      },
      {
        "date": "2023-10-22T17:56:00.000Z",
        "voteCount": 2,
        "content": "I second this. At my work, I run into this exact steaming buffer thing, it will not let me delete the data until after 60 minutes."
      },
      {
        "date": "2023-01-01T10:28:00.000Z",
        "voteCount": 1,
        "content": "Agreed C is right"
      },
      {
        "date": "2022-09-04T20:20:00.000Z",
        "voteCount": 11,
        "content": "Vote B - \"Some recently streamed rows might not be available for table copy typically for a few minutes. In rare cases, this can take up to 90 minutes\"\nhttps://cloud.google.com/bigquery/docs/streaming-data-into-bigquery#dataavailability"
      },
      {
        "date": "2022-12-16T03:48:00.000Z",
        "voteCount": 1,
        "content": "Aren't there other aspects of data pipelining that we should be aware of? other than merely referring to the number of 'recommended' minutes stated in docs. B doesn't address how the appended data is subsequently deleted, since the table is append-only, the size will constantly grow, and so the user may unnecessarily incur more storage costs."
      },
      {
        "date": "2022-09-06T04:29:00.000Z",
        "voteCount": 1,
        "content": "They don't seems concerned too much by data accuracy in the question"
      },
      {
        "date": "2022-10-05T10:05:00.000Z",
        "voteCount": 4,
        "content": "A and B are discarded because the UPDATE statement, is not performance efficient. Neither appending more and more values to the stagging table. It's better cleaning the stagging table, and merging with the master dataset."
      },
      {
        "date": "2023-12-21T02:50:00.000Z",
        "voteCount": 1,
        "content": "You can use BigQuery's features like MERGE to efficiently update the production table with only the new or changed data from the staging table, reducing processing time and costs."
      },
      {
        "date": "2024-05-24T02:46:00.000Z",
        "voteCount": 1,
        "content": "An append-only staging table ensures that all incoming data is captured without risk of data loss or overwrites, which is crucial for maintaining data integrity in a streaming ingestion scenario.\nThree-Hour Update Interval:\n\nUpdating the production table every three hours strikes a good balance between minimizing the latency of data availability for reporting and reducing the frequency of potentially resource-intensive update operations.\nThis interval is frequent enough to keep the production table relatively up-to-date for reporting purposes while ensuring that the performance of both ingestion and reporting processes is not significantly impacted.\nFrequent updates (like every ninety minutes or every thirty minutes) could introduce unnecessary overhead and contention, especially if the dataset is large or if there are complex transformations involved."
      },
      {
        "date": "2023-12-21T02:53:00.000Z",
        "voteCount": 1,
        "content": "Not C nor D. Moving and deleting: \nDeleting data from the staging table every 3 or 30 minutes could lead to data loss if the production table update fails, and it also requires more frequent and potentially resource-intensive operations.\n\nOptions C and D cause rebuilding of the staging table, which slows down ingestion, and may lose data if errors occur during recreation.\n\nA or B"
      },
      {
        "date": "2023-12-21T02:53:00.000Z",
        "voteCount": 1,
        "content": "When designing a report-only data warehouse in BigQuery, where data is streamed in and you have both staging and production tables, the key is to balance the frequency of updates with the performance needs of both the ingestion and reporting processes. Let's evaluate each option:"
      },
      {
        "date": "2023-12-21T02:54:00.000Z",
        "voteCount": 1,
        "content": "A. Staging table as append-only, updating production every three hours: This approach allows for a consistent flow of data into the staging table without interruptions. Updating the production table every three hours strikes a balance between having reasonably fresh data and not overloading the system with too frequent updates. However, this may not be suitable if your reporting requirements demand more up-to-date data.\n\nB. Staging table as append-only, updating production every ninety minutes: This is similar to option A but with a more frequent update cycle. This could be more appropriate if your reporting needs require more current data. However, more frequent updates can impact performance, especially during the update windows."
      },
      {
        "date": "2023-12-21T02:54:00.000Z",
        "voteCount": 1,
        "content": "C. Staging table moves data to production and clears staging every three hours: Moving data from staging to production and then clearing the staging table ensures that there is only one master dataset. However, this method might lead to more significant interruptions in data availability, both during the move and the clearing process. This might not be ideal if continuous access to the latest data is required.\n\nD. Staging table moves data to production and clears staging every thirty minutes: This option provides the most up-to-date data in the production table but could significantly impact performance. Such frequent data transfers and deletions might lead to more overhead and could interrupt both the ingestion and reporting processes."
      },
      {
        "date": "2023-12-21T02:55:00.000Z",
        "voteCount": 2,
        "content": "Considering these options, A (Staging table as append-only, updating production every three hours) seems to be the most balanced approach. It provides a good compromise between having up-to-date data in the production environment and maintaining system performance. However, the exact frequency should be fine-tuned based on the specific performance characteristics of your system and the timeliness requirements of your reports.\n\nIt's also important to implement efficient mechanisms for transferring data from staging to production to minimize the impact on system performance. Techniques like partitioning and clustering in BigQuery can be used to optimize query performance and manage large datasets more effectively."
      },
      {
        "date": "2023-12-14T08:20:00.000Z",
        "voteCount": 1,
        "content": "Neither. In the current scenario, DataStream (a new google resource) captures the CDC data and uses Dataflow to Replicate the changes to big query."
      },
      {
        "date": "2022-12-04T01:13:00.000Z",
        "voteCount": 2,
        "content": "Vote B\n C : the doc says streaming data can be used up to 90 minutes not 3 hours\n B : correct , insert staging table first with append\nand use merge from staging into production table"
      },
      {
        "date": "2022-12-04T01:16:00.000Z",
        "voteCount": 2,
        "content": "B just say \"update\", not specificlly mention DML. update can be merge"
      },
      {
        "date": "2023-12-21T02:50:00.000Z",
        "voteCount": 1,
        "content": "You can use BigQuery's features like MERGE to efficiently update the production table with only the new or changed data from the staging table, reducing processing time and costs."
      },
      {
        "date": "2022-11-29T06:20:00.000Z",
        "voteCount": 7,
        "content": "C is the answer.\n\nhttps://cloud.google.com/blog/products/data-analytics/moving-a-publishing-workflow-to-bigquery-for-new-data-insights\nFollowing common extract, transform, load (ETL) best practices, we used a staging table and a separate production table so that we could load data into the staging table without impacting users of the data. The design we created based on ETL best practices called for first deleting all the records from the staging table, loading the staging table, and then replacing the production table with the contents. \n\nWhen using the streaming API, the BigQuery streaming buffer remains active for about 30 to 60 minutes or more after use, which means that you can\u2019t delete or change data during that time. Since we used the streaming API, we scheduled the load every three hours to balance getting data into BigQuery quickly and being able to subsequently delete the data from the staging table during the load process."
      },
      {
        "date": "2022-11-23T21:31:00.000Z",
        "voteCount": 3,
        "content": "C\nFollowing common extract, transform, load (ETL) best practices, we used a staging table and a separate production table so that we could load data into the staging table without impacting users of the data. The design we created based on ETL best practices called for first deleting all the records from the staging table, loading the staging table, and then replacing the production table with the contents. \n\nWhen using the streaming API, the BigQuery streaming buffer remains active for about 30 to 60 minutes or more after use, which means that you can\u2019t delete or change data during that time. Since we used the streaming API, we scheduled the load every three hours to balance getting data into BigQuery quickly and being able to subsequently delete the data from the staging table during the load process.\nBuilding a script with BigQuery on the back end\nhttps://cloud.google.com/blog/products/data-analytics/moving-a-publishing-workflow-to-bigquery-for-new-data-insights"
      },
      {
        "date": "2022-10-04T01:43:00.000Z",
        "voteCount": 4,
        "content": "D : read more on Streaming inserts and timestamp-aware queries as the following link\nit is the same as this question exactly, but it is quite similar.\nhttps://cloud.google.com/blog/products/bigquery/performing-large-scale-mutations-in-bigquery\n\nread carefully in the content below.\nWhen using timestamps to keep track of updated and deleted records, it\u2019s a good idea to periodically delete stale entries. To illustrate, the following pair of DML statements can be used to remove older versions as well as deleted records.\n\nYou\u2019ll notice that the above DELETE statements don\u2019t attempt to remove records that are newer than 3 hours. This is because data in BigQuery\u2019s streaming buffer is not immediately available for UPDATE, DELETE, or MERGE operations, as described in DML Limitations. These queries assume that the actual values for RecordTime roughly match the actual ingestion time."
      },
      {
        "date": "2022-10-04T01:51:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#prune_merged_data\n\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/data-manipulation-language#limitations"
      },
      {
        "date": "2022-09-26T05:05:00.000Z",
        "voteCount": 1,
        "content": "Either C or D  But When will we delete stale data on staging table ? Every xxx???? \nhttps://cloud.google.com/architecture/database-replication-to-bigquery-using-change-data-capture#prune_merged_data"
      },
      {
        "date": "2023-05-10T08:01:00.000Z",
        "voteCount": 1,
        "content": "gpt: \"Overall, deleting the staging table every 30 minutes is a better choice than every 3 hours because it reduces the risk of data inconsistencies and performance issues.\""
      },
      {
        "date": "2022-09-07T01:22:00.000Z",
        "voteCount": 2,
        "content": "D. Have a staging table that moves the staged data over to the production table and deletes the contents of the staging table every thirty minutes."
      },
      {
        "date": "2022-09-07T01:21:00.000Z",
        "voteCount": 1,
        "content": "Ans D\nD. Have a staging table that moves the staged data over to the production table and deletes the contents of the staging table every thirty minutes."
      },
      {
        "date": "2022-09-02T14:36:00.000Z",
        "voteCount": 2,
        "content": "D. Have a staging table that moves the staged data over to the production table and deletes the contents of the staging table every thirty minutes."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 185,
    "url": "https://www.examtopics.com/discussions/google/view/79599-exam-professional-data-engineer-topic-1-question-185/",
    "body": "You issue a new batch job to Dataflow. The job starts successfully, processes a few elements, and then suddenly fails and shuts down. You navigate to the<br>Dataflow monitoring interface where you find errors related to a particular DoFn in your pipeline. What is the most likely cause of the errors?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tJob validation",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExceptions in worker code\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGraph or pipeline construction",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsufficient permissions"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 26,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T14:47:00.000Z",
        "voteCount": 12,
        "content": "B. Exceptions in worker code\n\nWhile your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the DoFns in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job.\n\nExceptions in user code (for example, your DoFn instances) are reported in the Dataflow monitoring interface.\n\nReference (Lists all answer choices and when to pick each one):\nhttps://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#Causes"
      },
      {
        "date": "2022-11-29T06:13:00.000Z",
        "voteCount": 6,
        "content": "B is the answer.\n\nhttps://cloud.google.com/dataflow/docs/guides/troubleshooting-your-pipeline#detect_an_exception_in_worker_code\nWhile your job is running, you might encounter errors or exceptions in your worker code. These errors generally mean that the DoFns in your pipeline code have generated unhandled exceptions, which result in failed tasks in your Dataflow job.\n\nExceptions in user code (for example, your DoFn instances) are reported in the Dataflow monitoring interface."
      },
      {
        "date": "2023-12-21T03:18:00.000Z",
        "voteCount": 2,
        "content": "The most likely cause of the errors you're experiencing in Dataflow, particularly if they are related to a particular DoFn (Dataflow's parallel processing operation), is B. Exceptions in worker code.\nWhen a Dataflow job processes a few elements successfully before failing, it suggests that the overall job setup, permissions, and pipeline graph are likely correct, as the job was able to start and initially process data. However, if it fails during execution and the errors are associated with a specific DoFn, this points towards issues in the code that executes within the workers. This could include:\n1.\tRuntime exceptions in the code logic of the DoFn.\n2.\tIssues handling specific data elements that might not be correctly managed by the DoFn code (e.g., unexpected data formats, null values, etc.).\n3.\tResource constraints or timeouts if the DoFn performs operations that are resource-intensive or long-running."
      },
      {
        "date": "2023-12-21T03:18:00.000Z",
        "voteCount": 2,
        "content": "To resolve these issues, you should:\n1.\tInspect the stack traces and error messages in the Dataflow monitoring interface for details on the exception.\n2.\tTest the DoFn with a variety of data inputs, especially edge cases, to ensure robust error handling.\n3.\tReview the resource usage and performance characteristics of the DoFn if the issue is related to resource constraints."
      },
      {
        "date": "2023-05-17T07:24:00.000Z",
        "voteCount": 3,
        "content": "A. Job validation - since it started successfully, it must have been validated.\nB. Exceptions in worker code - possible\nC. Graph or pipeline construction - same as A.\nD. Insufficient permissions - no elements to say that, and it should led to invalidation."
      },
      {
        "date": "2022-11-23T21:36:00.000Z",
        "voteCount": 1,
        "content": "C\nCode error"
      },
      {
        "date": "2022-09-12T19:09:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-09-03T19:18:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 186,
    "url": "https://www.examtopics.com/discussions/google/view/79603-exam-professional-data-engineer-topic-1-question-186/",
    "body": "Your new customer has requested daily reports that show their net consumption of Google Cloud compute resources and who used the resources. You need to quickly and efficiently generate these daily reports. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDo daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter data in Cloud Logging by project, resource, and user; then export the data in CSV format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tFilter data in Cloud Logging by project, log type, resource, and user, then import the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport Cloud Logging data to Cloud Storage in CSV format. Cleanse the data using Dataprep, filtering by project, resource, and user."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T14:58:00.000Z",
        "voteCount": 8,
        "content": "A. Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user.\n\nYou cannot import custom or filtered billing criteria into BigQuery. There are three types of Cloud Billing data tables with a fixed schema that must further drilled-down via BigQuery views.\n\nReference:\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery#setup"
      },
      {
        "date": "2023-12-21T03:26:00.000Z",
        "voteCount": 1,
        "content": "For generating daily reports that show net consumption of Google Cloud compute resources and user details, the most efficient approach would be:\n\nA. Do daily exports of Cloud Logging data to BigQuery. Create views filtering by project, log type, resource, and user."
      },
      {
        "date": "2023-12-21T03:26:00.000Z",
        "voteCount": 1,
        "content": "Here's why this option is the most effective:\n\nIntegration with BigQuery: BigQuery is a powerful tool for analyzing large datasets. By exporting Cloud Logging data directly to BigQuery, you can leverage its fast querying capabilities and advanced analysis features.\n\nAutomated Daily Exports: Setting up automated daily exports to BigQuery streamlines the reporting process, ensuring that data is consistently and efficiently transferred.\n\nCreating Views for Specific Filters: By creating views in BigQuery that filter data by project, log type, resource, and user, you can tailor the reports to the specific needs of your customer. Views also simplify repeated analysis by encapsulating complex SQL queries.\n\nEfficiency and Scalability: This method is highly efficient and scalable, handling large volumes of data without the manual intervention required for CSV exports and data cleansing."
      },
      {
        "date": "2023-12-21T03:26:00.000Z",
        "voteCount": 1,
        "content": "Option B (exporting data in CSV format) and Option D (using Cloud Storage and Dataprep) are less efficient due to the additional steps and manual handling involved. Option C is similar to A but lacks the specificity of creating views directly in BigQuery for filtering, which is a more streamlined approach."
      },
      {
        "date": "2023-12-14T08:42:00.000Z",
        "voteCount": 1,
        "content": "You can choose a sink in which you want Cloud logging to continously send Logging data. You can choose which columns you want to see (filter)."
      },
      {
        "date": "2023-12-14T08:43:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2023-05-17T07:29:00.000Z",
        "voteCount": 3,
        "content": "B, C, D do no generate a daily scalable solution."
      },
      {
        "date": "2023-05-01T06:00:00.000Z",
        "voteCount": 2,
        "content": "I see A as quite inefficient as you are exporting ALL logs (hundreds of thousands) to bq and the filtering them with views. I would go for C, assuming that it does not involve doing it manually but rather creating a SINK with the correct filters and then using BQ Dataset as sink destination. But a lot of assumptions are taking place here as I believe the questions does not provide much context."
      },
      {
        "date": "2023-03-20T18:59:00.000Z",
        "voteCount": 2,
        "content": "I almost got it wrong by choosing C. By doing C, that means we will manually filter first one by one. We should just import them all and filter using BigQuery"
      },
      {
        "date": "2023-01-25T13:33:00.000Z",
        "voteCount": 4,
        "content": "B and D do not consider the log type field. \nC looks good and I would go for it.\nHowever, A looks equally good and I've found a CloudSkillsBoost lab that is exactly describing what answer A does, i.e. exporting logs to BQ and then creating a VIEW. https://www.cloudskillsboost.google/focuses/6100?parent=catalog I think the advantage of exporting complete logs (i.e. filtering them after they reach BQ) is that in case we would want to adjust the reporting in the future, we would have the complete logs with all fields available, whereas with C we would need to take extra steps."
      },
      {
        "date": "2022-12-16T21:04:00.000Z",
        "voteCount": 2,
        "content": "A is the answer."
      },
      {
        "date": "2022-11-23T22:39:00.000Z",
        "voteCount": 1,
        "content": "A\nBad exporting data in csv or json due lack of some data\nso export is best practice\n1:10 \nhttps://www.youtube.com/watch?v=ZyMO9XabUUM"
      },
      {
        "date": "2022-12-16T12:30:00.000Z",
        "voteCount": 1,
        "content": "You need to quickly and efficiently generate these daily reports by using Materialized view /View\nA materialized view is the best solution and having filtered value with a view is good solution so A is an answer"
      },
      {
        "date": "2022-11-20T08:36:00.000Z",
        "voteCount": 1,
        "content": "A because you filter data daily by view not just once by cloud logging"
      },
      {
        "date": "2022-10-05T12:21:00.000Z",
        "voteCount": 3,
        "content": "A. The D isn't filtering by log type. B and C are discarded because you need to drill down the exported loggs in Big Query or other."
      },
      {
        "date": "2022-10-17T07:31:00.000Z",
        "voteCount": 5,
        "content": "2nd tought: Definitely A. If you go to google documentation for export billing, you see a message that \"Exporting to JSON or CSV is obsolet. Use Big Query instead\". \nAlso why A? Look\nhttps://cloud.google.com/billing/docs/how-to/export-data-bigquery\nhttps://cloud.google.com/billing/docs/how-to/bq-examples#total-costs-on-invoice\nYou can make a fast report template al Data Studio that read a Big Query view."
      },
      {
        "date": "2022-11-20T03:40:00.000Z",
        "voteCount": 1,
        "content": "A comment regarding the links you provided (and not the correctness of the selected answer).\nUsing Cloud Billing is something different than detecting compute consumption data from Cloud Logging.\nIn fact, manual exporting to CSV (and JSON) is possible through the Logs Explorer interface (I think without user data break-down):\n\ud83d\udd17 https://cloud.google.com/logging/docs/view/logs-explorer-interface#download_logs"
      },
      {
        "date": "2022-09-29T09:12:00.000Z",
        "voteCount": 1,
        "content": "The Google Cloud Storage bucket where you would like your reports to be delivered.\n\nYou can select any Cloud Storage bucket for which you are an owner, including buckets that are from different projects. This bucket must exist before you can start exporting reports and you must have owner access to the bucket. Google Cloud Storage charges for usage, so you should review the Cloud Storage pricesheet for information on how you might incur charges for the service.\n\nhttps://cloud.google.com/compute/docs/logging/usage-export"
      },
      {
        "date": "2022-09-13T00:37:00.000Z",
        "voteCount": 4,
        "content": "Ans is C\n\nhttps://cloud.google.com/logging/docs/export/aggregated_sinks\nD isn't correct because Cloud storage is used as a sink when logs are in json format not csv. https://cloud.google.com/logging/docs/export/aggregated_sinks#supported-destinations"
      },
      {
        "date": "2022-09-13T00:39:00.000Z",
        "voteCount": 1,
        "content": "On the other hand Ans A makes sense \nhttps://cloud.google.com/logging/docs/export/bigquery#overview"
      },
      {
        "date": "2022-11-14T07:02:00.000Z",
        "voteCount": 1,
        "content": "The question explicitly mentions daily generation of data so this would highlight, B and C seems that it is only suggesting a one-off filtering"
      },
      {
        "date": "2023-01-11T05:50:00.000Z",
        "voteCount": 1,
        "content": "so wjhaytsa your argument about daily generartion of data??"
      },
      {
        "date": "2022-09-12T20:41:00.000Z",
        "voteCount": 2,
        "content": "Quickly and efficiently! It's a flag to guide to DataPrep. And importing data to Bigquery does not mean a report."
      },
      {
        "date": "2022-09-12T19:27:00.000Z",
        "voteCount": 1,
        "content": "Why not C?"
      },
      {
        "date": "2022-09-12T14:20:00.000Z",
        "voteCount": 1,
        "content": "why not D ?"
      },
      {
        "date": "2022-09-11T09:42:00.000Z",
        "voteCount": 1,
        "content": "Challenging. B is right one but with B you do not automate wich makes it hard, with A you ensure automation but there is no  SQL support being mentioned which also makes me think that A is not the best choice."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 187,
    "url": "https://www.examtopics.com/discussions/google/view/79604-exam-professional-data-engineer-topic-1-question-187/",
    "body": "The Development and External teams have the project viewer Identity and Access Management (IAM) role in a folder named Visualization. You want the<br>Development Team to be able to read data from both Cloud Storage and BigQuery, but the External Team should only be able to read data from BigQuery. What should you do?<br><img src=\"/assets/media/exam-media/04341/0012100001.jpg\" class=\"in-exam-image\"><br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove Cloud Storage IAM permissions to the External Team on the acme-raw-data project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Virtual Private Cloud (VPC) firewall rules on the acme-raw-data project that deny all ingress traffic from the External Team CIDR range.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Service Controls perimeter containing both projects and BigQuery as a restricted API. Add the External Team users to the perimeter's Access Level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Service Controls perimeter containing both projects and Cloud Storage as a restricted API. Add the Development Team users to the perimeter's Access Level.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T15:02:00.000Z",
        "voteCount": 16,
        "content": "D. Create a VPC Service Controls perimeter containing both projects and Cloud Storage as a restricted API. Add the Development Team users to the perimeter's Access Level.\nReveal Solution"
      },
      {
        "date": "2022-09-22T00:14:00.000Z",
        "voteCount": 1,
        "content": "WHy do you have to put the development team at the access perimeter???"
      },
      {
        "date": "2023-05-10T08:34:00.000Z",
        "voteCount": 1,
        "content": "no, https://cloud.google.com/blog/products/serverless/cloud-run-gets-enterprise-grade-network-security-with-vpc-sc?utm_source=youtube&amp;utm_medium=unpaidsoc&amp;utm_campaign=CDR_pri_gcp_m0v4tedeiao_ThisWeekInCloud_082621&amp;utm_content=description"
      },
      {
        "date": "2023-05-10T08:36:00.000Z",
        "voteCount": 1,
        "content": "damn, i am confused anyway. Can be D."
      },
      {
        "date": "2023-05-10T08:41:00.000Z",
        "voteCount": 1,
        "content": "should be D, as i think now, because we create a \"magic bulb\" around  around Cloud storage and Dev team, and it will be protected from external influence like a human cell. Meantime Dev team will still be able to acess Bigquery. But external team will not manage to access Cloud storage."
      },
      {
        "date": "2023-01-25T14:27:00.000Z",
        "voteCount": 10,
        "content": "\"The grouping of GCP Project(s) and Service API(s) in the Service Perimeter result in restricting unauthorized access outside of the Service Perimeter to Service API endpoint(s) referencing resources inside of the Service Perimeter.\" \nhttps://scalesec.com/blog/vpc-service-controls-in-plain-english/\n\nDevelopment team: needs to access both Cloud Storage and BQ -&gt; therefore we put the Development team inside a perimeter so it can access both the Cloud Storage and the BQ\nExternal team: allowed to access only BQ -&gt; therefore we put Cloud Storage behind the restricted API and leave the external team outside of the perimeter, so it can access BQ, but is prohibited from accessing the Cloud Storage"
      },
      {
        "date": "2024-05-20T20:10:00.000Z",
        "voteCount": 1,
        "content": "It is not a network issue but a IAM permissions issue.\n https://cloud.google.com/iam/docs/deny-overview#inheritance"
      },
      {
        "date": "2023-12-14T08:58:00.000Z",
        "voteCount": 1,
        "content": "Comments are saying it correct its C"
      },
      {
        "date": "2023-09-19T03:52:00.000Z",
        "voteCount": 1,
        "content": "It's D for sure"
      },
      {
        "date": "2023-09-05T14:56:00.000Z",
        "voteCount": 5,
        "content": "A - Simple and straight forward"
      },
      {
        "date": "2023-08-11T08:05:00.000Z",
        "voteCount": 1,
        "content": "Why not B, I think CD will cause one of the team can not reach one or two of those DBs. A is not correct either"
      },
      {
        "date": "2023-08-09T03:22:00.000Z",
        "voteCount": 4,
        "content": "D. VPC Service Controls can create a service perimeter and define a restrictive API (service to protect). In this case, two projects are inside the perimeter and Cloud Storage is defined as the restrictive API. This means only services running on these two projects can access the Cloud Storage. And to allow users to access the Cloud Storage, they need have the access to the service perimeter. Hence, D is the correct answer."
      },
      {
        "date": "2023-05-01T05:15:00.000Z",
        "voteCount": 1,
        "content": "C is correct"
      },
      {
        "date": "2023-03-20T19:43:00.000Z",
        "voteCount": 1,
        "content": "D sounds more correct, but if the project is already in the Service Control, would External people can access the BigQuery dataset in that project?"
      },
      {
        "date": "2023-01-17T10:09:00.000Z",
        "voteCount": 3,
        "content": "seriously why u guys use VPC? the question never mentioned VPN or Interconnect, how can on-premise use VPC?\n\nA is the answer."
      },
      {
        "date": "2022-11-29T06:03:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2023-01-11T05:48:00.000Z",
        "voteCount": 1,
        "content": "Answer C, i dnt know if you have studied cloud security? if you have you will know"
      },
      {
        "date": "2023-05-10T08:35:00.000Z",
        "voteCount": 1,
        "content": "you might be correct. C.\nhttps://cloud.google.com/blog/products/serverless/cloud-run-gets-enterprise-grade-network-security-with-vpc-sc?utm_source=youtube&amp;utm_medium=unpaidsoc&amp;utm_campaign=CDR_pri_gcp_m0v4tedeiao_ThisWeekInCloud_082621&amp;utm_content=description\nhttps://www.youtube.com/watch?v=ABlY7FexJJI&amp;ab_channel=GoogleCloudTech"
      },
      {
        "date": "2022-11-23T23:34:00.000Z",
        "voteCount": 2,
        "content": "c\nExtend perimeters to authorized VPN or Cloud Interconnect\nYou can configure private communication to Google Cloud resources from VPC networks that span hybrid environments with Private Google Access on-premises extensions. A VPC network must be part of a service perimeter for VMs on that network to privately access managed Google Cloud resources within that service perimeter.\nhttps://cloud.google.com/vpc-service-controls/docs/overview#internet"
      },
      {
        "date": "2022-11-23T23:37:00.000Z",
        "voteCount": 2,
        "content": "I meant D Not C"
      },
      {
        "date": "2022-11-06T15:59:00.000Z",
        "voteCount": 4,
        "content": "D makes the most sense to me"
      },
      {
        "date": "2022-11-06T16:00:00.000Z",
        "voteCount": 2,
        "content": "Because \"You want the\nDevelopment Team to be able to read data from both Cloud Storage and BigQuery, but the External Team should only be able to read data from BigQuery.\""
      },
      {
        "date": "2022-11-06T16:01:00.000Z",
        "voteCount": 3,
        "content": "Therefore, Cloud Storage should be the restricted API, and you add the Development Team users to the perimeter's Access Level to allow them to access the restricted API."
      },
      {
        "date": "2022-11-05T05:30:00.000Z",
        "voteCount": 1,
        "content": "why C?\nI thought the development team would not be able to access BigQuery as I would include BigQuery in the service perimeter and add External Team to the access level"
      },
      {
        "date": "2022-11-14T06:52:00.000Z",
        "voteCount": 2,
        "content": "Exactly, why would we need to consider BigQuery as a restricted service when it can already be accessed by both Dev and External team. The restricted service we are concerned with is Cloud Storage. If we go with C, we are only adding the external team into the access level... this means that the development team still wouldn't be able to access it"
      },
      {
        "date": "2022-11-02T03:50:00.000Z",
        "voteCount": 1,
        "content": "Answer C\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate"
      },
      {
        "date": "2022-10-06T22:55:00.000Z",
        "voteCount": 4,
        "content": "Answer C\nhttps://cloud.google.com/vpc-service-controls/docs/overview#isolate"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 188,
    "url": "https://www.examtopics.com/discussions/google/view/79606-exam-professional-data-engineer-topic-1-question-188/",
    "body": "Your startup has a web application that currently serves customers out of a single region in Asia. You are targeting funding that will allow your startup to serve customers globally. Your current goal is to optimize for cost, and your post-funding goal is to optimize for global presence and performance. You must use a native<br>JDBC driver. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud SQL for PostgreSQL highly available instance first, and Bigtable with US, Europe, and Asia replication after securing funding.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud SQL for PostgreSQL zonal instance first, and Bigtable with US, Europe, and Asia after securing funding.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Cloud SQL for PostgreSQL zonal instance first, and Cloud SQL for PostgreSQL with highly available configuration after securing funding."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 33,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T15:06:00.000Z",
        "voteCount": 11,
        "content": "A. Use Cloud Spanner to configure a single region instance initially, and then configure multi-region Cloud Spanner instances after securing funding.\n\nWhen you create a Cloud Spanner instance, you must configure it as either regional (that is, all the resources are contained within a single Google Cloud region) or multi-region (that is, the resources span more than one region). \n\nYou can change the instance configuration to multi-regional (or global) at anytime."
      },
      {
        "date": "2023-05-01T05:17:00.000Z",
        "voteCount": 8,
        "content": "Although A is good, but concerning about the cost. Then D will be much more suitable"
      },
      {
        "date": "2024-09-18T11:10:00.000Z",
        "voteCount": 1,
        "content": "Although A is good, but concerning about the cost. Then D will be much more suitable"
      },
      {
        "date": "2024-04-05T16:25:00.000Z",
        "voteCount": 1,
        "content": "Although A is good, but concerning about the cost. Then D will be much more suitable"
      },
      {
        "date": "2024-01-15T02:27:00.000Z",
        "voteCount": 1,
        "content": "I think is D \n\nThe best for Web app is Cloud SQL, and Spanner is the best for data more than 30GB"
      },
      {
        "date": "2023-12-21T04:03:00.000Z",
        "voteCount": 5,
        "content": "A - This option allows for optimization for cost initially with a single region Cloud Spanner instance, and then optimization for global presence and performance after funding with multi-region instances.\nCloud Spanner supports native JDBC drivers and is horizontally scalable, providing very high performance. A single region instance minimizes costs initially. After funding, multi-region instances can provide lower latency and high availability globally.\nCloud SQL does not scale as well and has higher costs for multiple high availability regions. Bigtable does not support JDBC drivers natively. Therefore, Spanner is the best choice here for optimizing both for cost initially and then performance and availability globally post-funding."
      },
      {
        "date": "2023-03-22T21:40:00.000Z",
        "voteCount": 3,
        "content": "Spanner has some limitations with JDBC. Maybe the quetion wants to help us tp choose Cloud SQL"
      },
      {
        "date": "2023-02-24T16:51:00.000Z",
        "voteCount": 1,
        "content": "Answer D:\nCost effective transactional database Cloud SQL. Spanner is good case for data more than 30 GB"
      },
      {
        "date": "2022-12-09T02:35:00.000Z",
        "voteCount": 6,
        "content": "B and C has no sense because of the driver.\nD looks like a good option, but HA it's not to improve performance or global presence:\nThe purpose of an HA configuration is to reduce downtime when a zone or instance becomes unavailable. This might happen during a zonal outage, or when an instance runs out of memory. With HA, your data continues to be available to client applications.\nSo the best option is A."
      },
      {
        "date": "2022-09-22T00:18:00.000Z",
        "voteCount": 5,
        "content": "https://cloud.google.com/spanner/docs/jdbc-drivers\nAns A\nhttps://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations\nThe last part of the question makes it easy"
      },
      {
        "date": "2022-09-13T00:07:00.000Z",
        "voteCount": 3,
        "content": "Yes Spanner is expensive ,  but the question expresslty states that \"after securing funding you want to have a global presence\" the word globally is repeatedly stated there. \nAnswer is A."
      },
      {
        "date": "2022-09-15T01:13:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations\nAns A"
      },
      {
        "date": "2022-09-12T00:35:00.000Z",
        "voteCount": 3,
        "content": "Spanner is expensive, they haven't mentioned the size of db... optimize for cost then option is Cloud SQL which cost effective and highly available in case of multi region."
      },
      {
        "date": "2022-09-09T00:00:00.000Z",
        "voteCount": 3,
        "content": "A is the best option. It is globally scalable and it also meets the cost goal as it says that initially it will be configurated as single region wich is cheaper than multi region."
      },
      {
        "date": "2022-09-06T04:47:00.000Z",
        "voteCount": 2,
        "content": "Spanner is expensive can't be A\n\nWould choose D"
      },
      {
        "date": "2022-09-06T04:53:00.000Z",
        "voteCount": 1,
        "content": "Actually maybe C as you don't really need relational database for a webapp and BigTable is super performant and highly available"
      },
      {
        "date": "2022-10-06T22:53:00.000Z",
        "voteCount": 1,
        "content": "no its A"
      },
      {
        "date": "2022-09-15T01:16:00.000Z",
        "voteCount": 2,
        "content": "The fact that its global cloud spanner is the answer. Secondly Option D, the fact that it has to be highly avaible and multi regional its already more expensive than Cloud spanner Regional instance. https://cloud.google.com/spanner/docs/instance-configurations#tradeoffs_regional_versus_multi-region_configurations"
      },
      {
        "date": "2022-09-02T19:36:00.000Z",
        "voteCount": 3,
        "content": "Spanner still support JDBC\nhttps://cloud.google.com/spanner/docs/jdbc-drivers"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 189,
    "url": "https://www.examtopics.com/discussions/google/view/79608-exam-professional-data-engineer-topic-1-question-189/",
    "body": "You need to migrate 1 PB of data from an on-premises data center to Google Cloud. Data transfer time during the migration should take only a few hours. You want to follow Google-recommended practices to facilitate the large data transfer over a secure connection. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a Cloud Interconnect connection between the on-premises data center and Google Cloud, and then use the Storage Transfer Service.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Transfer Appliance and have engineers manually encrypt, decrypt, and verify the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a Cloud VPN connection, start gcloud compute scp jobs in parallel, and run checksums to verify the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the data into 3 TB batches, transfer the data using gsutil, and run checksums to verify the data."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-05T16:06:00.000Z",
        "voteCount": 5,
        "content": "Well it doesn't mentions anything about not enough bandwidth to meet your project deadline. I guess you can assume they have 200GBps+ of bandwith, otherwise it shouldn't take only a few hours."
      },
      {
        "date": "2024-08-03T13:50:00.000Z",
        "voteCount": 1,
        "content": "One who wanted to use Transfer Appliance to migrate data in a few hours, you should live near Google office and run really fast :D"
      },
      {
        "date": "2023-12-21T04:09:00.000Z",
        "voteCount": 2,
        "content": "Cloud Interconnect provides a dedicated private connection between on-prem and Google Cloud for high bandwidth (up to 100 Gbps) and low latency. This facilitates large, fast data transfers.\nStorage Transfer Service supports parallel data transfers over Cloud Interconnect. It can transfer petabyte-scale datasets faster by transferring objects in parallel.\nStorage Transfer Service uses HTTPS encryption in transit and at rest by default for secure data transfers.\nIt follows Google-recommended practices for large data migrations vs ad hoc methods like gsutil or scp.\nThe other options would take too long for a 1 PB transfer (VPN capped at 3 Gbps, manual transfers) or introduce extra steps like batching and checksums. Cloud Interconnect + Storage Transfer is the recommended Google solution."
      },
      {
        "date": "2023-10-28T21:52:00.000Z",
        "voteCount": 2,
        "content": "It is believed that A.\nOne reason is that for \"secure\" and \"in a few hours,\" the communication can be done securely using a direct physical line without going through an ISP. Also, depending on the case, in the case of \"Dedicated Interconnect,\" the maximum transfer can be as high as 200 Gbps, and the fastest data transfer of 1 PB can be completed in 11 hours.\nTherefore, A."
      },
      {
        "date": "2023-08-21T08:33:00.000Z",
        "voteCount": 3,
        "content": "A\nhttps://cloud.google.com/storage-transfer/docs/transfer-options#:~:text=Transferring%20more%20than%201%20TB%20from%20on%2Dpremises"
      },
      {
        "date": "2023-07-27T20:08:00.000Z",
        "voteCount": 1,
        "content": "Dedicated Interconnect provides direct physical connections between your on-premises network and Google's network. Dedicated Interconnect enables you to transfer large amounts of data between networks, which can be more cost-effective than purchasing additional bandwidth over the public internet. https://cloud.google.com/network-connectivity/docs/interconnect/concepts/dedicated-overview"
      },
      {
        "date": "2023-07-27T20:10:00.000Z",
        "voteCount": 1,
        "content": "This link has additional clarity\nhttps://cloud.google.com/network-connectivity/docs/interconnect/concepts/terminology"
      },
      {
        "date": "2023-07-03T02:24:00.000Z",
        "voteCount": 3,
        "content": "1 PB and \"few hours\". It is clearly referring to Transfer Appliance\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time"
      },
      {
        "date": "2023-07-27T20:06:00.000Z",
        "voteCount": 3,
        "content": "Transfer Appliance is a slow process. wont be able to do in few hours"
      },
      {
        "date": "2023-04-30T10:30:00.000Z",
        "voteCount": 1,
        "content": "gpt: Based on security and speed, if the data is highly sensitive and security is the top priority, then option B (using a Transfer Appliance) may be a better choice. Transfer Appliance uses hardware encryption to transfer data and is designed to securely transfer large amounts of data. However, if speed is the primary concern, then option A (using Cloud Interconnect and Storage Transfer Service) may be a better choice as it allows for faster transfer speeds over a dedicated and secure connection. It ultimately depends on the specific needs and priorities of the organization.\n\n  A vague treaky question. Bad author of it...\n  B is also good. As were said in discuss. by smb, a question asks \"safe connection\", so  - a Cloud Interconnect (A)"
      },
      {
        "date": "2023-03-20T22:37:00.000Z",
        "voteCount": 3,
        "content": "Either this question is very tricky or very poor written. It says 'Data transfer time during the migration should take only a few hours'. We should not add the 20days for overhead time for Appliance into the total time of migration.\n\nIf 'a few hours' = 30hours or more, A will be good enough.\nIf 'a few hours' = 10 or less, B is the only way (with multiple devices to copy at the same time)"
      },
      {
        "date": "2023-11-06T19:19:00.000Z",
        "voteCount": 1,
        "content": "B can't be the answer - You have to wait 25 days to receive the appliance and another 25 days to get the appliance back and data loaded to cloud storage: https://cloud.google.com/transfer-appliance/docs/4.0/overview#transfer-speeds"
      },
      {
        "date": "2023-03-19T01:31:00.000Z",
        "voteCount": 1,
        "content": "Expected time via transfer appliance is around 20 days , and achieving the same using Storage transfer service with highest bandwidth of 100GPS is 30 hrs, so hence its been asked for hrs .. its A\nAcquiring a Transfer Appliance is straightforward. In the Google Cloud console, you request a Transfer Appliance, indicate how much data you have, and then Google ships one or more appliances to your requested location. You're given a number of days to transfer your data to the appliance (\"data capture\") and ship it back to Google.\n\nThe expected turnaround time for a network appliance to be shipped, loaded with your data, shipped back, and rehydrated on Google Cloud is 20 days. If your online transfer timeframe is calculated to be substantially more than this timeframe, consider Transfer Appliance. The total cost for the 300 TB device process is less than $2,500."
      },
      {
        "date": "2023-07-03T02:22:00.000Z",
        "voteCount": 1,
        "content": "it says data transfer during the migration. It mean from when the migration is \"activated\", which means from when the Transfer Appliace device is plugged and ready to be used"
      },
      {
        "date": "2023-03-09T03:10:00.000Z",
        "voteCount": 2,
        "content": "Even with 100gbps bandwith, you will not reach a data transfer time within the range of \"hours\" for 1PB. Transfer appliance is the way to go. https://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#time"
      },
      {
        "date": "2023-02-19T06:08:00.000Z",
        "voteCount": 1,
        "content": "Answer A, \nOne time transfer is cheaper and less secure always using Transfer Appliance. \nyou need to do it in faster way, set up Interconnect speed limit is 50mbps - 10GBps\nand Transfer Appliance speed can goes up to 40GBps. \nI am choosing A for security concern only."
      },
      {
        "date": "2023-01-01T17:37:00.000Z",
        "voteCount": 1,
        "content": "A is right"
      },
      {
        "date": "2023-01-01T17:38:00.000Z",
        "voteCount": 1,
        "content": "A. Establish a Cloud Interconnect connection between the on-premises data center and Google Cloud, and then use the Storage Transfer Service. Most Voted"
      },
      {
        "date": "2022-11-28T10:02:00.000Z",
        "voteCount": 2,
        "content": "A is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nLike gsutil, Storage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage. Although gsutil can support small transfer sizes (up to 1 TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files)."
      },
      {
        "date": "2022-11-25T15:30:00.000Z",
        "voteCount": 2,
        "content": "B\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#:~:text=Few%20things%20in,not%20be%20obtained."
      },
      {
        "date": "2022-11-25T15:58:00.000Z",
        "voteCount": 4,
        "content": "B\nIt takes 30hrs with 100Gbps bandwidth- more than a day to transfer\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#:~:text=addresses%20or%20NATs.-,Online%20versus%20offline%20transfer,A%20certain%20amount%20of%20management%20overhead%20is%20built%20into%20these%20calculations.,-As%20noted%20earlier"
      },
      {
        "date": "2022-09-13T01:11:00.000Z",
        "voteCount": 1,
        "content": "A is correct."
      },
      {
        "date": "2022-09-11T18:25:00.000Z",
        "voteCount": 1,
        "content": "A is correct\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer_appliance_for_larger_transfers"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 190,
    "url": "https://www.examtopics.com/discussions/google/view/79609-exam-professional-data-engineer-topic-1-question-190/",
    "body": "You are loading CSV files from Cloud Storage to BigQuery. The files have known data quality issues, including mismatched data types, such as STRINGs and<br>INT64s in the same column, and inconsistent formatting of values such as phone numbers or addresses. You need to create the data pipeline to maintain data quality and perform the required cleansing and transformation. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Fusion to transform the data before loading it into BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Fusion to convert the CSV files to a self-describing data format, such as AVRO, before loading the data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the CSV files into a staging table with the desired schema, perform the transformations with SQL, and then write the results to the final destination table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a table with the desired schema, load the CSV files into the table, and perform the transformations in place using SQL."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 26,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-18T03:36:00.000Z",
        "voteCount": 6,
        "content": "I'm kinda inclined towards C as SQL seems a powerful option to treat this kind of use case.\n\nAlso, I didn't get how the transformations mentioned on this page will help to clean the data (https://cloud.google.com/data-fusion/docs/concepts/transformation-pushdown#supported_transformations)\n\nBut I guess using Wrangler plugin, this kind of stuff can be done on DataFusion, also the question talks about an pipeline, so A is the final choice."
      },
      {
        "date": "2023-12-21T04:16:00.000Z",
        "voteCount": 3,
        "content": "Data Fusion's advantages:\n\nVisual interface: Offers a user-friendly interface for designing data pipelines without extensive coding, making it accessible to a wider range of users.\nBuilt-in transformations: Includes a wide range of pre-built transformations to handle common data quality issues, such as:\nData type conversions\nData cleansing (e.g., removing invalid characters, correcting formatting)\nData validation (e.g., checking for missing values, enforcing constraints)\nData enrichment (e.g., adding derived fields, joining with other datasets)\nCustom transformations: Allows for custom transformations using SQL or Java code for more complex cleaning tasks.\nScalability: Can handle large datasets efficiently, making it suitable for processing CSV files with potential data quality issues.\nIntegration with BigQuery: Integrates seamlessly with BigQuery, allowing for direct loading of transformed data."
      },
      {
        "date": "2023-12-21T04:16:00.000Z",
        "voteCount": 1,
        "content": "Why other options are less suitable:\n\nB. Converting to AVRO: While AVRO is a self-describing format, it doesn't inherently address data quality issues. Transformations would still be needed, and Data Fusion provides a more comprehensive environment for this.\nC. Staging table: Requires manual SQL transformations, which can be time-consuming and error-prone for large datasets with complex data quality issues.\nD. Transformations in place: Modifying data directly in the destination table can lead to data loss or corruption if errors occur. It's generally safer to keep raw data intact and perform transformations separately.\nBy using Data Fusion, you can create a robust and efficient data pipeline that addresses data quality issues upfront, ensuring that only clean and consistent data is loaded into BigQuery for accurate analysis and insights."
      },
      {
        "date": "2023-10-23T06:20:00.000Z",
        "voteCount": 4,
        "content": "The answer is C. That is what we do at work. We have landing/staging table,  sort table and deliver table,"
      },
      {
        "date": "2023-10-23T06:25:00.000Z",
        "voteCount": 4,
        "content": "Okay, second thought, it is asking for a pipeline, so the answer should be A. At work, we use dataflow inside the composer to build a pipeline injecting data into landing/staging table, then transform/clean data in the sort table, then send the cleaned data to deliver table."
      },
      {
        "date": "2023-06-11T10:23:00.000Z",
        "voteCount": 3,
        "content": "Keyword:  Data Pipeline"
      },
      {
        "date": "2023-05-06T23:18:00.000Z",
        "voteCount": 2,
        "content": "same as @saurabhsingh4k"
      },
      {
        "date": "2023-04-16T17:23:00.000Z",
        "voteCount": 4,
        "content": "C is the right answer. Do ELT in BigQuery. Data Fusion is not the right too for this job."
      },
      {
        "date": "2023-02-19T06:12:00.000Z",
        "voteCount": 4,
        "content": "Answer C, \nDatafusion is costly and current transformation is just a cast transformation in a column. \nI guess no one wanna pay for datafusion for this little transformation and Staging table processing handles such minor cleaning."
      },
      {
        "date": "2023-01-25T15:12:00.000Z",
        "voteCount": 4,
        "content": "Data Fusion enables changing the data type directly as shown in this lab: https://www.cloudskillsboost.google/focuses/25335?parent=catalog \nWrangler is the feature to enable that, as already mentioned: https://stackoverflow.com/questions/58699872/google-cloud-data-fusion-how-to-change-datatype-from-string-to-date"
      },
      {
        "date": "2023-01-01T02:59:00.000Z",
        "voteCount": 2,
        "content": "Apparently chatGPT thinks C is the correct answer just sayin (for the same reason that @saurabhsingh4k wrote)"
      },
      {
        "date": "2022-12-16T02:00:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://cloud.google.com/data-fusion/docs/concepts/overview#:~:text=The%20Cloud%20Data%20Fusion%20web%20UI%20lets%20you%20to%20build%20scalable%20data%20integration%20solutions%20to%20clean%2C%20prepare%2C%20blend%2C%20transfer%2C%20and%20transform%20data%2C%20without%20having%20to%20manage%20the%20infrastructure."
      },
      {
        "date": "2022-11-28T09:53:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/data-fusion/docs/concepts/overview\nloud Data Fusion is a fully managed, cloud-native, enterprise data integration service for quickly building and managing data pipelines.\n\nThe Cloud Data Fusion web UI lets you to build scalable data integration solutions to clean, prepare, blend, transfer, and transform data, without having to manage the infrastructure."
      },
      {
        "date": "2023-01-01T17:40:00.000Z",
        "voteCount": 1,
        "content": "thx for sharing link"
      },
      {
        "date": "2022-11-23T15:37:00.000Z",
        "voteCount": 2,
        "content": "The Correct Ans is C"
      },
      {
        "date": "2022-12-17T09:16:00.000Z",
        "voteCount": 1,
        "content": "although this is my preferred answer. this doesn\u2019t satisfy how this becomes a pipeline."
      },
      {
        "date": "2022-11-21T06:13:00.000Z",
        "voteCount": 1,
        "content": "Data Fusion"
      },
      {
        "date": "2022-09-13T00:00:00.000Z",
        "voteCount": 1,
        "content": "Ans A\nhttps://cloud.google.com/data-fusion/docs/concepts/transformation-pushdown#supported_transformations"
      },
      {
        "date": "2022-09-02T19:43:00.000Z",
        "voteCount": 1,
        "content": "A is correct for me"
      },
      {
        "date": "2022-09-02T15:15:00.000Z",
        "voteCount": 1,
        "content": "A. Use Data Fusion to transform the data before loading it into BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 191,
    "url": "https://www.examtopics.com/discussions/google/view/79643-exam-professional-data-engineer-topic-1-question-191/",
    "body": "You are developing a new deep learning model that predicts a customer's likelihood to buy on your ecommerce site. After running an evaluation of the model against both the original training data and new test data, you find that your model is overfitting the data. You want to improve the accuracy of the model when predicting new data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the training dataset, and increase the number of input features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the size of the training dataset, and decrease the number of input features.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the size of the training dataset, and increase the number of input features.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReduce the size of the training dataset, and decrease the number of input features."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-26T23:19:00.000Z",
        "voteCount": 11,
        "content": "There 2 parts and they are relevant to each other\n1. Overfit  is fixed by decreasing the number of input features (select only essential features)\n2. Accuracy is improved by increasing the amount of training data examples."
      },
      {
        "date": "2022-09-26T23:19:00.000Z",
        "voteCount": 2,
        "content": "https://docs.aws.amazon.com/machine-learning/latest/dg/model-fit-underfitting-vs-overfitting.html"
      },
      {
        "date": "2024-01-13T13:28:00.000Z",
        "voteCount": 2,
        "content": "Option B, the model learned to listen to too much stuff/noise. We need to reduce it, by decreasing the number of input feature, and we need to give the model more data, by increasing the amount of training data"
      },
      {
        "date": "2023-07-29T22:54:00.000Z",
        "voteCount": 1,
        "content": "Increase the size of the training dataset: By adding more diverse examples of customers and their buying behavior to the training data, the model will have a broader understanding of different scenarios and be better equipped to generalize to new customers.\n\nIncrease the number of input features: Providing the model with more relevant information about customers can help it identify meaningful patterns and make better predictions. These input features could include things like the customer's age, past purchase history, browsing behavior, or any other relevant data that might impact their buying likelihood."
      },
      {
        "date": "2023-05-18T05:36:00.000Z",
        "voteCount": 1,
        "content": "A. can be a solution for a specific case, but it is not the academic answer as we do not know the quantity and proportion between them of n and k added. More records and more variables together can lead to even more overfitting due also to the curse of dimensionality. Adding a variable is much more impactful than records.\nB. just more records can lead to a more robust estimation and fewer variables certainly lead to at most the same estimation, but potentially reduce the fit on the training set.\nC. reduce n in favor of k is never a choice. it is against logic and it will lead to more overfitting.\nD. decrease both will reduce overfitting for sure but at the price of losing robustness on the model predictive power"
      },
      {
        "date": "2023-01-01T17:43:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the size of the training dataset, and decrease the number of input features."
      },
      {
        "date": "2022-09-14T03:41:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2022-09-07T22:54:00.000Z",
        "voteCount": 3,
        "content": "Answer B\nhttps://machinelearningmastery.com/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates/"
      },
      {
        "date": "2022-09-07T10:32:00.000Z",
        "voteCount": 3,
        "content": "Option B\nFeature selection is the one the ways to resolve overfitting. Which means reducing the features\nwhen the size of the training data is small, then the network tends to have greater control over the training data. so increasing the size of data would help."
      },
      {
        "date": "2022-09-06T05:09:00.000Z",
        "voteCount": 1,
        "content": "Best option is not mentioned: generalize you neural net by decreasing the complexity of it's structure.\n\nA part from that I guess you could remove some features and increase the size of the training dataset ==&gt; B"
      },
      {
        "date": "2022-09-04T14:56:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the size of the training dataset, and decrease the number of input features.\n\nSorry, B is right. Read through extensive best-practices on ML."
      },
      {
        "date": "2022-09-03T19:18:00.000Z",
        "voteCount": 1,
        "content": "D is correct"
      },
      {
        "date": "2022-09-02T21:39:00.000Z",
        "voteCount": 1,
        "content": "D. Reduce the size of the training dataset, and decrease the number of input features.\nReveal Solution"
      },
      {
        "date": "2022-09-02T19:46:00.000Z",
        "voteCount": 1,
        "content": "B. Increase the size of the training dataset, and decrease the number of input features."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 192,
    "url": "https://www.examtopics.com/discussions/google/view/79526-exam-professional-data-engineer-topic-1-question-192/",
    "body": "You are implementing a chatbot to help an online retailer streamline their customer service. The chatbot must be able to respond to both text and voice inquiries.<br>You are looking for a low-code or no-cade option, and you want to be able to easily train the chatbot to provide answers to keywords. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Speech-to-Text API to build a Python application in App Engine.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Speech-to-Text API to build a Python application in a Compute Engine instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dialogflow to implement the chatbot, defining the intents based on the most common queries collected.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T11:54:00.000Z",
        "voteCount": 12,
        "content": "D is correct:\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts#:~:text=Dialogflow%20can%20use%20Cloud%20Text,to%2Dspeech%2C%20or%20TTS."
      },
      {
        "date": "2023-12-21T10:35:00.000Z",
        "voteCount": 1,
        "content": "The best option would be to use Dialogflow to implement the chatbot, defining the intents based on the most common queries collected.\n\nDialogflow is a conversational AI platform that allows for easy implementation of chatbots without needing to code. It has built-in integration for both text and voice input via APIs like Cloud Speech-to-Text. Defining intents and entity types allows you to map common queries and keywords to responses. This would provide a low/no-code way to quickly build and iteratively improve the chatbot capabilities.\n\nOption A and B would require more heavy coding to handle speech input/output. Option C still requires coding the complex query handling. Only option D leverages the full capabilities of Dialogflow to enable no-code chatbot development and ongoing improvements as more conversational data is collected. Hence, option D is the best approach given the requirements."
      },
      {
        "date": "2023-07-31T06:34:00.000Z",
        "voteCount": 1,
        "content": "Low-code or no-cade requirement makes it easy to decide."
      },
      {
        "date": "2022-11-28T09:51:00.000Z",
        "voteCount": 4,
        "content": "D is the answer.\n\nhttps://cloud.google.com/dialogflow/docs\nDialogflow is a natural language understanding platform that makes it easy to design and integrate a conversational user interface into your mobile app, web application, device, bot, interactive voice response system, and so on. Using Dialogflow, you can provide new and engaging ways for users to interact with your product.\n\nDialogflow can analyze multiple types of input from your customers, including text or audio inputs (like from a phone or voice recording). It can also respond to your customers in a couple of ways, either through text or with synthetic speech."
      },
      {
        "date": "2023-01-01T17:52:00.000Z",
        "voteCount": 1,
        "content": "Agree with D"
      },
      {
        "date": "2022-11-24T00:08:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts#:~:text=Dialogflow%20can%20use%20Cloud%20Text,to%2Dspeech%2C%20or%20TTS."
      },
      {
        "date": "2022-10-17T08:15:00.000Z",
        "voteCount": 4,
        "content": "D definitely, as the documentation says (specially that you can call the detect Intect method for audio inputs):\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts \nAlso Speech-To-Text API does nothing more than translate."
      },
      {
        "date": "2022-09-07T23:00:00.000Z",
        "voteCount": 4,
        "content": "Answer D\n\nhttps://cloud.google.com/dialogflow/es/docs/how/detect-intent-tts"
      },
      {
        "date": "2022-09-05T03:41:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dialogflow/es/docs/how/detect-intent-stream\nVote D"
      },
      {
        "date": "2022-09-02T19:49:00.000Z",
        "voteCount": 3,
        "content": "C. Use Dialogflow for simple queries and the Cloud Speech-to-Text API for complex queries.\n\nThis seem the best answer here but not the best answer in real world.\nBut with the Question, the answer must be the combination of both Diagflow and Speech API"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 193,
    "url": "https://www.examtopics.com/discussions/google/view/79644-exam-professional-data-engineer-topic-1-question-193/",
    "body": "An aerospace company uses a proprietary data format to store its flight data. You need to connect this new data source to BigQuery and stream the data into<br>BigQuery. You want to efficiently import the data into BigQuery while consuming as few resources as possible. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shell script that triggers a Cloud Function that performs periodic ETL batch jobs on the new data source.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a standard Dataflow pipeline to store the raw data in BigQuery, and then transform the format later when the data is used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Apache Hive to write a Dataproc job that streams the data into BigQuery in CSV format.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-10-31T21:53:00.000Z",
        "voteCount": 17,
        "content": "This has to be D. How could it even be B? The source is a proprietary format. Dataflow wouldn't have a built-in template to  ead the file. You will have to create something custom."
      },
      {
        "date": "2022-10-17T08:23:00.000Z",
        "voteCount": 12,
        "content": "For me it's clearly D\nIt's between B and D, but read B, store raw data in Big Query? Use a Dataflow pipeline just to store raw data into Big Query, and transform later? You'd need to do another pipeline for that, and is not efficient."
      },
      {
        "date": "2023-12-21T10:41:00.000Z",
        "voteCount": 3,
        "content": "Option D is the best approach given the constraints - use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.\nThe key reasons:\n\u2022\tDataflow provides managed resource scaling for efficient stream processing\n\u2022\tAvro format has schema evolution capabilities and efficient serialization for flight telemetry data\n\u2022\tApache Beam connectors avoid having to write much code to integrate proprietary data sources\n\u2022\tStreaming inserts data efficiently compared to periodic batch jobs\nIn contrast, option A uses Cloud Functions which lack native streaming capabilities. Option B stores data in less efficient JSON format. Option C uses Dataproc which requires manual cluster management.\nSo leveraging Dataflow + Avro + Beam provides the most efficient way to stream proprietary flight data into BigQuery while using minimal resources."
      },
      {
        "date": "2023-12-14T09:23:00.000Z",
        "voteCount": 1,
        "content": "Its talking about streaming? none of the options talk about triggering a load to begin. We need a trigger or schedule to run first."
      },
      {
        "date": "2023-10-15T05:54:00.000Z",
        "voteCount": 2,
        "content": "Option D allows you to use a custom connector to read the proprietary data format and write the data to BigQuery in Avro format."
      },
      {
        "date": "2023-09-08T21:51:00.000Z",
        "voteCount": 1,
        "content": "the keyword is streaming"
      },
      {
        "date": "2023-07-27T20:30:00.000Z",
        "voteCount": 3,
        "content": "Between B and D. Firstly transformation is not mentioned in the question, So B is less probable. Then Efficient import is mentioned in the question, Converting to Avro will consume less space. I am going with D"
      },
      {
        "date": "2023-02-19T06:22:00.000Z",
        "voteCount": 1,
        "content": "Answer is D , \nWhy not B, changing data format before uploading to bigquery is good approach."
      },
      {
        "date": "2023-01-23T05:30:00.000Z",
        "voteCount": 1,
        "content": "I believe keyword here is \"An aerospace company uses a proprietary data format\"\nSo if we list the connectors available in Apache Beam, we are listed with these options;\nhttps://beam.apache.org/documentation/io/connectors/\n\nSo I believe, we have to create our own custom connector to read from the proprietary data format hence the answer should be B"
      },
      {
        "date": "2023-01-23T05:31:00.000Z",
        "voteCount": 1,
        "content": "sorry the answer should be D"
      },
      {
        "date": "2023-01-01T17:54:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-12-05T02:57:00.000Z",
        "voteCount": 3,
        "content": "D is the answer."
      },
      {
        "date": "2023-01-11T05:44:00.000Z",
        "voteCount": 1,
        "content": "There is dataflow connector and D isnt cost effective"
      },
      {
        "date": "2022-12-04T00:54:00.000Z",
        "voteCount": 2,
        "content": "B is the most efficient"
      },
      {
        "date": "2022-10-06T03:56:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/spanner/docs/change-streams/use-dataflow#core-concepts"
      },
      {
        "date": "2022-09-07T23:09:00.000Z",
        "voteCount": 2,
        "content": "Ans B\nhttps://cloud.google.com/architecture/streaming-avro-records-into-bigquery-using-dataflow\nIs there a reason to use apache beam connector yet there is dataflow which is a standard solution for that scenario?"
      },
      {
        "date": "2022-09-12T01:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/bigquery-explained-data-ingestion"
      },
      {
        "date": "2022-09-14T07:58:00.000Z",
        "voteCount": 1,
        "content": "Can standard dataflow be used to ingest any proprietary format of file ?\nshouldn't we use custom apache beam connector ? \nSo I think it is D ,though it isn't simple ,But in this scenario they have asked to use less resources to import data"
      },
      {
        "date": "2022-09-15T01:03:00.000Z",
        "voteCount": 1,
        "content": "Option D streams, thats not cost effective. We need something that is cost effectictive, hence B is the option"
      },
      {
        "date": "2022-09-21T23:51:00.000Z",
        "voteCount": 1,
        "content": "I mean that consumes fewer resources"
      },
      {
        "date": "2022-09-15T01:01:00.000Z",
        "voteCount": 1,
        "content": "Do you mind reading the links i provided and revisiting the question, then you will understand why D isnt the best. Why use Apache beam yet there is Dataflow"
      },
      {
        "date": "2022-09-22T06:44:00.000Z",
        "voteCount": 1,
        "content": "Can Bigquery handle a proprietary file format?"
      },
      {
        "date": "2022-09-28T22:40:00.000Z",
        "voteCount": 1,
        "content": "BigQuery uses a proprietary format because it can evolve in tandem with the query engine, which takes advantage of deep knowledge of the data layout to optimize ..."
      },
      {
        "date": "2022-09-02T21:52:00.000Z",
        "voteCount": 3,
        "content": "D. Use an Apache Beam custom connector to write a Dataflow pipeline that streams the data into BigQuery in Avro format.\nReveal Solution"
      },
      {
        "date": "2022-09-02T19:52:00.000Z",
        "voteCount": 2,
        "content": "B is the most efficient for me."
      },
      {
        "date": "2022-09-03T01:18:00.000Z",
        "voteCount": 2,
        "content": "Sorry, D is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 194,
    "url": "https://www.examtopics.com/discussions/google/view/79529-exam-professional-data-engineer-topic-1-question-194/",
    "body": "An online brokerage company requires a high volume trade processing architecture. You need to create a secure queuing system that triggers jobs. The jobs will run in Google Cloud and call the company's Python API to execute trades. You need to efficiently implement a solution. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an application hosted on a Compute Engine instance that makes a push subscription to the Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite an application that makes a queue in a NoSQL database.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to subscribe to a Pub/Sub topic and call the Python API."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-03-26T05:30:00.000Z",
        "voteCount": 6,
        "content": "A and D are both good. I go for A because we have high volume and easy to scale and optmize cost"
      },
      {
        "date": "2024-07-01T03:45:00.000Z",
        "voteCount": 2,
        "content": "D is the answer."
      },
      {
        "date": "2024-08-25T02:31:00.000Z",
        "voteCount": 1,
        "content": "There is no need for a composer to call a Python API only - it's an overkill."
      },
      {
        "date": "2023-10-23T15:37:00.000Z",
        "voteCount": 3,
        "content": "Answer is D, at work we use solution A for low volume of Pub/Sub messages and Cloud function, and using D Composer for high volume Pub/Sub messages."
      },
      {
        "date": "2023-02-19T06:27:00.000Z",
        "voteCount": 4,
        "content": "Answer A:\nassume, Company wants to buy immediately in same second if stock goes down or up. \nSomehow, it is connected to PubSub as SINK connector, then immediately there is PUSH to subcriber (cloud function) that is connected to their python API (internal application) that makes the purchase."
      },
      {
        "date": "2023-01-01T17:57:00.000Z",
        "voteCount": 1,
        "content": "A. Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API."
      },
      {
        "date": "2022-11-28T09:44:00.000Z",
        "voteCount": 3,
        "content": "A is the answer."
      },
      {
        "date": "2022-11-09T16:50:00.000Z",
        "voteCount": 4,
        "content": "Because trading platform requires securely transmission to queuing \nIf you use cloud compose then we need some other job to trigger composer \u2026 would that be cloud composer api or cloud function \u2026"
      },
      {
        "date": "2022-09-28T22:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/functions/docs/calling/pubsub"
      },
      {
        "date": "2022-09-07T23:16:00.000Z",
        "voteCount": 4,
        "content": "Ans A\nhttps://cloud.google.com/functions/docs/calling/pubsub#deployment"
      },
      {
        "date": "2022-09-06T05:18:00.000Z",
        "voteCount": 3,
        "content": "A because D is stupidly high latency"
      },
      {
        "date": "2022-09-04T23:34:00.000Z",
        "voteCount": 1,
        "content": "Vote A, can't see the need for composer"
      },
      {
        "date": "2022-09-04T02:38:00.000Z",
        "voteCount": 3,
        "content": "A might be enough. Cloud composer will be an overkill"
      },
      {
        "date": "2022-09-02T21:58:00.000Z",
        "voteCount": 4,
        "content": "A. Use a Pub/Sub push subscription to trigger a Cloud Function to pass the data to the Python API."
      },
      {
        "date": "2022-09-02T19:54:00.000Z",
        "voteCount": 1,
        "content": "D is a more recommend way by Google, IMO."
      },
      {
        "date": "2023-10-23T15:36:00.000Z",
        "voteCount": 2,
        "content": "I agree, at work use solution A for low volume of Pub/Sub messages and function, and using Composer for high volume Pub/Sub messages."
      },
      {
        "date": "2022-09-02T11:58:00.000Z",
        "voteCount": 1,
        "content": "A. more sense to me."
      },
      {
        "date": "2022-09-02T19:53:00.000Z",
        "voteCount": 2,
        "content": "Composer support exception and retry for complex pipeline.\nD might be correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 195,
    "url": "https://www.examtopics.com/discussions/google/view/79645-exam-professional-data-engineer-topic-1-question-195/",
    "body": "Your company wants to be able to retrieve large result sets of medical information from your current system, which has over 10 TBs in the database, and store the data in new tables for further query. The database must have a low-maintenance architecture and be accessible via SQL. You need to implement a cost-effective solution that can support data analytics for large result sets. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud SQL, but first organize the data into tables. Use JOIN in queries to retrieve data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery as a data warehouse. Set output destinations for caching large queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a MySQL cluster installed on a Compute Engine managed instance group for scalability.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Spanner to replicate the data across regions. Normalize the data in a series of tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T22:01:00.000Z",
        "voteCount": 8,
        "content": "B. Use BigQuery as a data warehouse. Set output destinations for caching large queries."
      },
      {
        "date": "2023-12-21T10:57:00.000Z",
        "voteCount": 3,
        "content": "Option B is the best approach - use BigQuery as a data warehouse, and set output destinations for caching large queries.\n\nThe key reasons why BigQuery fits the requirements:\n\nIt is a fully managed data warehouse built to scale to handle massive datasets and perform fast SQL analytics\nIt has a low maintenance architecture with no infrastructure to manage\nSQL capabilities allow easy querying of the medical data\nOutput destinations allow configurable caching for fast retrieval of large result sets\nIt provides a very cost-effective solution for these large scale analytics use cases\nIn contrast, Cloud Spanner and Cloud SQL would not scale as cost effectively for 10TB+ data volumes. Self-managed MySQL on Compute Engine also requires more maintenance. Hence, leveraging BigQuery as a fully managed data warehouse is the optimal solution here."
      },
      {
        "date": "2023-01-01T17:58:00.000Z",
        "voteCount": 2,
        "content": "B. Use BigQuery as a data warehouse. Set output destinations for caching large queries. Most Voted"
      },
      {
        "date": "2022-11-28T09:34:00.000Z",
        "voteCount": 3,
        "content": "B is the answer."
      },
      {
        "date": "2022-09-12T01:57:00.000Z",
        "voteCount": 4,
        "content": "Answer B.\nhttps://cloud.google.com/bigquery/docs/query-overview"
      },
      {
        "date": "2022-09-02T19:56:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 196,
    "url": "https://www.examtopics.com/discussions/google/view/79646-exam-professional-data-engineer-topic-1-question-196/",
    "body": "You have 15 TB of data in your on-premises data center that you want to transfer to Google Cloud. Your data changes weekly and is stored in a POSIX-compliant source. The network operations team has granted you 500 Mbps bandwidth to the public internet. You want to follow Google-recommended practices to reliably transfer your data to Google Cloud on a weekly basis. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to trigger the gsutil command. Use the -m parameter for optimal parallelism.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Transfer Appliance to migrate your data into a Google Kubernetes Engine cluster, and then configure a weekly transfer job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Storage Transfer Service for on-premises data in your data center, and then configure a weekly transfer job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInstall Storage Transfer Service for on-premises data on a Google Cloud virtual machine, and then configure a weekly transfer job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-11-28T09:27:00.000Z",
        "voteCount": 9,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nLike gsutil, Storage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage. Although gsutil can support small transfer sizes (up to 1 TB), Storage Transfer Service for on-premises data is designed for large-scale transfers (up to petabytes of data, billions of files)."
      },
      {
        "date": "2023-02-19T06:39:00.000Z",
        "voteCount": 7,
        "content": "answer C, \nTo avoid confustion: Install Storage Transfer Service is always on EXTERNAL OR NON GOOGLE service or data centre to connect google service."
      },
      {
        "date": "2023-04-21T09:00:00.000Z",
        "voteCount": 3,
        "content": "C is the Answer as we need weekly run Storage transfer service has the feature to schedule."
      },
      {
        "date": "2022-11-24T07:00:00.000Z",
        "voteCount": 3,
        "content": "The fact that it's about a POSIX source makes necessary the set up of Storage Transfer Service agents. \nThis detail limits [C] to be the correct answer, since it's the data center hosting the files where the agent must be installed.\n--\nSome excerpts:\n(an older version of documentation was definite)\n\"The following is a high-level overview of how Transfer service for on-premises data works:\n 1.Install Docker and run a small piece of software, called an agent, in your private data center. \"\nSource: https://web.archive.org/web/20210529161414/https://cloud.google.com/storage-transfer/docs/on-prem-overview\n--\n\"Storage Transfer Service agents are applications running inside a Docker container, that coordinate with Storage Transfer Service to read data from POSIX file system sources, and/or write data to POSIX file system sinks.\nIf your transfer does not involve a POSIX file system, you do not need to set up agents.\"\nSource: https://cloud.google.com/storage-transfer/docs/managing-on-prem-agents"
      },
      {
        "date": "2022-11-24T00:19:00.000Z",
        "voteCount": 2,
        "content": "C\nStorage Transfer Service agents are applications running inside a Docker container, that coordinate with Storage Transfer Service to read data from POSIX file system sources, and/or write data to POSIX file system sinks.\n\nhttps://cloud.google.com/storage-transfer/docs/managing-on-prem-agents#:~:text=Storage%20Transfer%20Service%20agents,agents%20on%20your%20servers."
      },
      {
        "date": "2022-09-22T06:18:00.000Z",
        "voteCount": 1,
        "content": "why can't it be D?"
      },
      {
        "date": "2022-11-28T09:31:00.000Z",
        "voteCount": 2,
        "content": "Installation is done for the source which is in your data centre, and not in GCP."
      },
      {
        "date": "2022-09-12T15:53:00.000Z",
        "voteCount": 2,
        "content": "I vote for C"
      },
      {
        "date": "2022-09-12T01:27:00.000Z",
        "voteCount": 2,
        "content": "Ans C \nhttps://cloud.google.com/storage-transfer/docs/overview"
      },
      {
        "date": "2022-09-10T08:15:00.000Z",
        "voteCount": 3,
        "content": "can you help with difference between c and d ?"
      },
      {
        "date": "2022-11-23T07:56:00.000Z",
        "voteCount": 2,
        "content": "If you install the software for on-premise data center on a Google Cloud VM, then it's not on-premise, it's on GCP, so it can't access your on-premise data."
      },
      {
        "date": "2022-09-02T22:15:00.000Z",
        "voteCount": 2,
        "content": "C. Install Storage Transfer Service for on-premises data in your data center, and then configure a weekly transfer job."
      },
      {
        "date": "2022-09-02T19:57:00.000Z",
        "voteCount": 2,
        "content": "C is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 197,
    "url": "https://www.examtopics.com/discussions/google/view/79647-exam-professional-data-engineer-topic-1-question-197/",
    "body": "You are designing a system that requires an ACID-compliant database. You must ensure that the system requires minimal human intervention in case of a failure.<br>What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud SQL for MySQL instance with point-in-time recovery enabled.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud SQL for PostgreSQL instance with high availability enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Bigtable instance with more than one cluster.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a BigQuery table with a multi-region configuration."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 53,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-11-07T06:49:00.000Z",
        "voteCount": 39,
        "content": "We exclude [C[ as non ACID and [D] for being invalid (location is configured on Dataset level, not Table).\nThen, let's focus on \"minimal human intervention in case of a failure\" requirement in order to eliminate one answer among [A] and [B]. \nBasically, we have to compare point-in-time recovery with high availability. It doesn't matter whether it's about MySQL or PostgreSQL since both databases support those features.\n- Point-in-time recovery logs are created automatically, but restoring an instance in case of failure requires manual steps (described here: https://cloud.google.com/sql/docs/mysql/backup-recovery/pitr#perform-pitr)\n- High availability, in case of failure requires no human intervention: \"If an HA-configured instance becomes unresponsive, Cloud SQL automatically switches to serving data from the standby instance.\" (from https://cloud.google.com/sql/docs/postgres/high-availability#failover-overview)\nSo answer [B] wins."
      },
      {
        "date": "2023-10-23T15:52:00.000Z",
        "voteCount": 2,
        "content": "Will you change your answer if the answer D says dataset instead of table?"
      },
      {
        "date": "2022-11-10T00:09:00.000Z",
        "voteCount": 2,
        "content": "Your explanation is perfect, thanks"
      },
      {
        "date": "2024-07-18T11:39:00.000Z",
        "voteCount": 1,
        "content": "Its B because of HA\ncant be A because point in time recovery still requires human intervention"
      },
      {
        "date": "2023-12-21T12:44:00.000Z",
        "voteCount": 2,
        "content": "The best option to meet the ACID compliance and minimal human intervention requirements is to configure a Cloud SQL for PostgreSQL instance with high availability enabled.\n\nKey reasons:\n\nCloud SQL for PostgreSQL provides full ACID compliance, unlike Bigtable which provides only atomicity and consistency guarantees.\nEnabling high availability removes the need for manual failover as Cloud SQL will automatically failover to a standby replica if the leader instance goes down.\nPoint-in-time recovery in MySQL requires manual intervention to restore data if needed.\nBigQuery does not provide transactional guarantees required for an ACID database.\nTherefore, a Cloud SQL for PostgreSQL instance with high availability meets the ACID and minimal intervention requirements best. The automatic failover will ensure availability and uptime without administrative effort."
      },
      {
        "date": "2023-08-09T03:36:00.000Z",
        "voteCount": 2,
        "content": "I vote for D - BigQuery with multi region configuration. \nAccording to https://cloud.google.com/bigquery/docs/introduction , BigQuery support ACID and automatically replicated for high availability.\n\"\"\"BigQuery stores data using a columnar storage format that is optimized for analytical queries. BigQuery presents data in tables, rows, and columns and provides full support for database transaction semantics (ACID). BigQuery storage is automatically replicated across multiple locations to provide high availability.\"\"\""
      },
      {
        "date": "2023-07-24T04:22:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2023-02-19T06:35:00.000Z",
        "voteCount": 2,
        "content": "Answer B,\nACID -compliant database are Spanner and CloudSQL\nOption A could be the answer if they setup a secondary or failure replicas and auto maintenance window that could trigger in non business hours. \nOption B, does not explain about extra replica but in postgresql Highavailablity option means the same extra replicas instances are available for emergency."
      },
      {
        "date": "2023-01-01T18:02:00.000Z",
        "voteCount": 1,
        "content": "B. Configure a Cloud SQL for PostgreSQL instance with high availability enabled."
      },
      {
        "date": "2022-11-28T09:23:00.000Z",
        "voteCount": 3,
        "content": "B is the answer.\n\nhttps://cloud.google.com/sql/docs/postgres/high-availability#HA-configuration\nThe purpose of an HA configuration is to reduce downtime when a zone or instance becomes unavailable. This might happen during a zonal outage, or when an instance runs out of memory. With HA, your data continues to be available to client applications."
      },
      {
        "date": "2022-11-23T15:05:00.000Z",
        "voteCount": 1,
        "content": "I voted for B"
      },
      {
        "date": "2022-09-27T01:18:00.000Z",
        "voteCount": 1,
        "content": "B   it is exact anwer."
      },
      {
        "date": "2022-09-12T01:28:00.000Z",
        "voteCount": 2,
        "content": "Ans B\nPostgres is highly ACID compliant as compared to Mysql"
      },
      {
        "date": "2022-09-10T06:05:00.000Z",
        "voteCount": 2,
        "content": "cloud sql with high availability enabled is enough"
      },
      {
        "date": "2022-09-02T22:20:00.000Z",
        "voteCount": 1,
        "content": "B. Configure a Cloud SQL for PostgreSQL instance with high availability enabled."
      },
      {
        "date": "2022-09-02T19:57:00.000Z",
        "voteCount": 1,
        "content": "I voted for B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 198,
    "url": "https://www.examtopics.com/discussions/google/view/79648-exam-professional-data-engineer-topic-1-question-198/",
    "body": "You are implementing workflow pipeline scheduling using open source-based tools and Google Kubernetes Engine (GKE). You want to use a Google managed service to simplify and automate the task. You also want to accommodate Shared VPC networking considerations. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow for your workflow pipelines. Use Cloud Run triggers for scheduling.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow for your workflow pipelines. Use shell scripts to schedule workflows.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the host project.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T22:25:00.000Z",
        "voteCount": 16,
        "content": "D. Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.\n\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. When Cloud Composer participates in a Shared VPC, the Cloud Composer environment is in the service project.\n\nReference:\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc"
      },
      {
        "date": "2024-05-24T03:21:00.000Z",
        "voteCount": 1,
        "content": "Placing Cloud Composer resources in the service project can lead to more complex network configurations and management overhead compared to placing them in the host project, which is designed to manage Shared VPC resources."
      },
      {
        "date": "2023-07-24T04:09:00.000Z",
        "voteCount": 1,
        "content": "Please correct if I am wrong.. I think it is Option C coz  I feel Option D is incorrect because placing the Cloud Composer resources in the service project would not allow you to access resources in the host project."
      },
      {
        "date": "2023-11-06T15:46:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/composer/docs/composer-2/configure-shared-vpc#shared-vpc-guidelines"
      },
      {
        "date": "2023-06-05T09:56:00.000Z",
        "voteCount": 1,
        "content": "\u2705 Use Cloud Composer in a Shared VPC configuration. Place the Cloud Composer resources in the service project.\n\n- Cloud Composer is a managed Apache Airflow service. It is an open-source tool that programmatically author, schedule, and monitor pipelines, which fits your needs perfectly.\n- In a Shared VPC configuration, Cloud Composer resources should be placed in the service project. This provides network isolation while still allowing the Cloud Composer environment to communicate with resources in the host project.\n- With Shared VPC, the host project's network (including its subnets and secondary IP ranges) is shared by other service projects, which promotes network peering, and it's compliant with the networking considerations of GKE."
      },
      {
        "date": "2023-09-20T08:11:00.000Z",
        "voteCount": 2,
        "content": "That's answer D though."
      },
      {
        "date": "2022-11-28T08:30:00.000Z",
        "voteCount": 3,
        "content": "D is the answer.\n\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc\nShared VPC enables organizations to establish budgeting and access control boundaries at the project level while allowing for secure and efficient communication using private IPs across those boundaries. In the Shared VPC configuration, Cloud Composer can invoke services hosted in other Google Cloud projects in the same organization without exposing services to the public internet."
      },
      {
        "date": "2023-03-23T21:11:00.000Z",
        "voteCount": 1,
        "content": "I thought it was C, but after reading docs I realized it is D. \nIn D case we can have isolation and reduce project complicity(It could be overlapped resources in Host project, and it is harder to restrict composer access for Host project resources)"
      },
      {
        "date": "2023-01-01T18:04:00.000Z",
        "voteCount": 2,
        "content": "Agreed"
      },
      {
        "date": "2023-04-30T23:12:00.000Z",
        "voteCount": 1,
        "content": "agreed to what.."
      },
      {
        "date": "2023-04-30T23:41:00.000Z",
        "voteCount": 1,
        "content": "D it is, as per doc link, provided by users. thx"
      },
      {
        "date": "2023-02-24T17:29:00.000Z",
        "voteCount": 2,
        "content": "why not option C? if composer in host project it will be easier to connect one or more service project with it."
      },
      {
        "date": "2022-11-24T00:31:00.000Z",
        "voteCount": 2,
        "content": "D\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. \nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc#:~:text=This%20page%20describes,the%20service%20project."
      },
      {
        "date": "2022-09-02T20:00:00.000Z",
        "voteCount": 2,
        "content": "D according to documentation\n\nShared VPC requires that you designate a host project to which networks and subnetworks belong and a service project, which is attached to the host project. When Cloud Composer participates in a Shared VPC, the Cloud Composer environment is in the service project.\n\nhttps://cloud.google.com/composer/docs/how-to/managing/configuring-shared-vpc#set_up_shared_vpc_and_attach_the_service_project"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 199,
    "url": "https://www.examtopics.com/discussions/google/view/79649-exam-professional-data-engineer-topic-1-question-199/",
    "body": "You are using BigQuery and Data Studio to design a customer-facing dashboard that displays large quantities of aggregated data. You expect a high volume of concurrent users. You need to optimize the dashboard to provide quick visualizations with minimal latency. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery BI Engine with materialized views.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery BI Engine with logical views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery BI Engine with streaming data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery BI Engine with authorized views."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 28,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T22:27:00.000Z",
        "voteCount": 10,
        "content": "A. Use BigQuery BI Engine with materialized views."
      },
      {
        "date": "2022-11-28T08:24:00.000Z",
        "voteCount": 7,
        "content": "A is the answer.\n\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro\nIn BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. BigQuery leverages precomputed results from materialized views and whenever possible reads only delta changes from the base tables to compute up-to-date results. Materialized views can be queried directly or can be used by the BigQuery optimizer to process queries to the base tables.\n\nQueries that use materialized views are generally faster and consume fewer resources than queries that retrieve the same data only from the base tables. Materialized views can significantly improve the performance of workloads that have the characteristic of common and repeated queries."
      },
      {
        "date": "2024-02-20T17:57:00.000Z",
        "voteCount": 1,
        "content": "Option A is the better one.\n\nBut keep in mind for real life:\nhttps://cloud.google.com/bigquery/docs/bi-engine-preferred-tables\n\nLimitations\nBI Engine preferred tables have the following limitations:\n\nYou cannot add views into the preferred tables reservation list. BI Engine preferred tables only support tables.\nQueries to materialized views are only accelerated if both the materialized views and their base tables are in the preferred tables list."
      },
      {
        "date": "2023-07-23T14:36:00.000Z",
        "voteCount": 2,
        "content": "Materialized views are precomputed query results that are stored in memory, allowing for faster retrieval of aggregated data. When you create a materialized view in BigQuery, it stores the results of a query as a table, and subsequent queries that can leverage this materialized view can be significantly faster compared to computing them on the fly."
      },
      {
        "date": "2023-08-29T00:40:00.000Z",
        "voteCount": 1,
        "content": "If we take minimal latency into consideration, I am not sure a materialized view will be the right answer since the user gets data from the cache but is not up to date."
      },
      {
        "date": "2023-06-11T13:51:00.000Z",
        "voteCount": 1,
        "content": "periodically cache the results for perfomance"
      },
      {
        "date": "2022-10-31T08:25:00.000Z",
        "voteCount": 3,
        "content": "A.\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro\nIn BigQuery, materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency"
      },
      {
        "date": "2022-09-28T10:59:00.000Z",
        "voteCount": 2,
        "content": "I vote A\nhttps://cloud.google.com/bigquery/docs/bi-engine-intro#:~:text=Materialized%20views%20%2D%20Materialized%20views%20in%20BigQuery%20perform%20precomputation%2C%20thereby%20reducing%20query%20time.%20You%20should%20create%20materialized%20views%20to%20improve%20performance%20and%20to%20reduce%20processed%20data%20by%20using%20aggregations%2C%20filters%2C%20inner%20joins%2C%20and%20unnests."
      },
      {
        "date": "2022-09-10T22:43:00.000Z",
        "voteCount": 3,
        "content": "use materialized views is better option here"
      },
      {
        "date": "2022-09-02T20:02:00.000Z",
        "voteCount": 1,
        "content": "By integrating BI Engine with BigQuery streaming, you can perform real-time data analysis over streaming data without sacrificing write speeds or data freshness.\n\nhttps://cloud.google.com/bigquery/docs/bi-engine-intro"
      },
      {
        "date": "2022-09-03T01:22:00.000Z",
        "voteCount": 2,
        "content": "Sorry, A is correct\nAs  AWSandeep mention"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 200,
    "url": "https://www.examtopics.com/discussions/google/view/79650-exam-professional-data-engineer-topic-1-question-200/",
    "body": "Government regulations in the banking industry mandate the protection of clients' personally identifiable information (PII). Your company requires PII to be access controlled, encrypted, and compliant with major data protection standards. In addition to using Cloud Data Loss Prevention (Cloud DLP), you want to follow<br>Google-recommended practices and use service accounts to control access to PII. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the required Identity and Access Management (IAM) roles to every employee, and create a single service account to access project resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse one service account to access a Cloud SQL database, and use separate service accounts for each human user.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage to comply with major data protection standards. Use one service account shared by all users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 39,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2022-12-05T01:08:00.000Z",
        "voteCount": 18,
        "content": "\u2705[A] is the only acceptable answer.\n\u274c[B] rejected (no need to elaborate)\n\u274c[C] and [D] rejected. Why should we be obliged to use Cloud Storage? Other storage options in Google Cloud aren't compliant with \"major data protection standards\"?\n=============================================\n\u2757[D] has another rejection reason, the following quotes:\n\ud83d\udd38From &lt;https://cloud.google.com/iam/docs/service-accounts&gt;: \"You can add service accounts to a Google group, then grant roles to the group. However, adding service accounts to groups is not a best practice. Service accounts are used by applications, and each application is likely to have its own access requirements\"\n\ud83d\udd38From &lt;https://cloud.google.com/iam/docs/best-practices-service-accounts#groups&gt;: \"Avoid using groups for granting service accounts access to resources\""
      },
      {
        "date": "2023-06-23T03:10:00.000Z",
        "voteCount": 4,
        "content": "Rejecting C + D solely based on Cloud Storage, which CAN be used in this scenario, is not sound reasoning."
      },
      {
        "date": "2023-12-21T13:07:00.000Z",
        "voteCount": 2,
        "content": "A single shared service account or granting every employee direct access violates security best practices, so not [A]."
      },
      {
        "date": "2023-01-23T06:08:00.000Z",
        "voteCount": 13,
        "content": "for A: please refer to this link below which suggests \"Sharing a single service account across multiple applications can complicate the management of the service account\" - meaning it's not a best practice.\nhttps://cloud.google.com/iam/docs/best-practices-service-accounts#single-purpose \nAlso, what if we have hundreds of users, does it really make sense to manage each user's IAM individually?\n\n\nfor D: it's indeed not one of the best practices but I believe it's much more managable and better than A"
      },
      {
        "date": "2023-12-21T13:09:00.000Z",
        "voteCount": 1,
        "content": "To align with Google's recommended practices for managing access to personally identifiable information (PII) in compliance with banking industry regulations, let's analyze the options:\n\nA. Assign the required IAM roles to every employee, and create a single service account to access project resources: While assigning specific IAM roles to employees is a good practice for access control, using a single service account for all access to PII is not ideal. Service accounts should be used for applications and automated processes, not as a shared account for multiple users or employees."
      },
      {
        "date": "2023-12-21T13:09:00.000Z",
        "voteCount": 1,
        "content": "B. Use one service account to access a Cloud SQL database, and use separate service accounts for each human user: Again, service accounts are intended for automated tasks or applications, not for individual human users. Assigning separate service accounts to each human user is not a recommended practice and does not align with the principle of least privilege."
      },
      {
        "date": "2023-12-21T13:09:00.000Z",
        "voteCount": 1,
        "content": "C. Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users: Using Cloud Storage can indeed help comply with data protection standards, especially when configured correctly with encryption and access controls. However, sharing a single service account among all users is not a best practice. It goes against the principle of least privilege and does not provide adequate granularity for access control."
      },
      {
        "date": "2023-12-21T13:09:00.000Z",
        "voteCount": 1,
        "content": "D. Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group: This approach is more aligned with best practices. Using Cloud Storage can ensure compliance with data protection standards. Creating multiple service accounts, each with specific access controls attached to different IAM groups, allows for more granular and controlled access to PII. This setup adheres to the principle of least privilege, ensuring that each service (or group of services) only has access to the resources necessary for its function.\n\nBased on these considerations, option D is the most appropriate choice. It ensures compliance with data protection standards, uses Cloud Storage for secure data management, and employs multiple service accounts tied to IAM groups for granular access control, aligning well with Google-recommended practices and regulatory requirements in the banking industry."
      },
      {
        "date": "2023-08-09T04:25:00.000Z",
        "voteCount": 1,
        "content": "D. Not the best, but seems most reasonable out of 4."
      },
      {
        "date": "2023-07-23T14:31:00.000Z",
        "voteCount": 2,
        "content": "Option D - Using multiple service accounts attached to IAM groups helps enforce the principle of least privilege. Each group can be assigned only the necessary permissions, reducing the risk of unauthorized access to sensitive data."
      },
      {
        "date": "2023-07-09T23:39:00.000Z",
        "voteCount": 2,
        "content": "Google Cloud Storage is designed to comply with major data protection standards. Creating multiple service accounts and attaching them to IAM groups provides granular control over who has access to the data. This approach is aligned with the principle of least privilege, a security best practice where a user is given the minimum levels of access necessary to complete their tasks."
      },
      {
        "date": "2023-06-23T03:11:00.000Z",
        "voteCount": 2,
        "content": "It\u00b4s not A because \n1. assigning IAM roles to single users instead of groups is not Google best practice, and\n2. the question explicitly states that we want to use multiple service accounts."
      },
      {
        "date": "2023-06-05T10:23:00.000Z",
        "voteCount": 2,
        "content": "D. Use Cloud Storage to comply with major data protection standards. Use multiple service accounts attached to IAM groups to grant the appropriate access to each group.\n\n- Google Cloud Storage is built for secure and compliant data storage. It supports compliance with major data protection standards, which is essential in the banking industry where data protection regulations are stringent.\n- Service accounts in Google Cloud represent non-human users (applications or services) that need to authenticate and be authorized to access specific Google Cloud resources.\n- Creating multiple service accounts attached to IAM groups allows you to manage access control in a granular manner. This follows the principle of least privilege, providing each group with only the permissions they need to perform their tasks, which is a recommended practice for managing access to sensitive data like PII."
      },
      {
        "date": "2023-06-05T10:24:00.000Z",
        "voteCount": 1,
        "content": "\u274c D. Use Cloud Storage to comply with major data protection standards. Use one service account shared by all users.\n\n- Sharing one service account among all users is not a secure practice. It goes against the principle of least privilege and does not allow for granular control over access permissions. If the shared service account were to be compromised, all resources accessible by the account would be at risk."
      },
      {
        "date": "2023-03-22T14:39:00.000Z",
        "voteCount": 6,
        "content": "Why are so many questions like this?\nNone of the answers is best practice."
      },
      {
        "date": "2023-11-29T08:05:00.000Z",
        "voteCount": 1,
        "content": "I would like to ask your question to those who decide the questions on the exams. I don't understand what they're trying to do, many of the questions cause divided responses, because they don't have a clear answer. The certification process is a waste of time."
      },
      {
        "date": "2023-03-14T06:58:00.000Z",
        "voteCount": 5,
        "content": "I could be wrong but I think the wording in D caused this confusion, so it is an English problem. -- \"Use multiple service accounts attached to IAM groups to grant the appropriate access to each group\"\nI believe what D really means is that you can create a group for a bunch of people who only need access to resource A, so attach a Service account to the group and service account only have access to A.\nThen you create another group for another bunch of people who only need access to resource B, so attach a service account to this group. this service account can only access to B.\nSo each group/service account has a very specific access target, and purpose of the group is very narrowly defined which is allowed by best practice. However, wording in option D merged all these into one sentence causing confusions.\nOption A is an administrative nightmare to manage IAM for a larger user population which is actually also against GCP best practices."
      },
      {
        "date": "2023-02-14T02:10:00.000Z",
        "voteCount": 2,
        "content": "D it is"
      },
      {
        "date": "2023-01-01T18:08:00.000Z",
        "voteCount": 1,
        "content": "D is right"
      },
      {
        "date": "2022-12-24T06:30:00.000Z",
        "voteCount": 1,
        "content": "A is the correct one"
      },
      {
        "date": "2022-12-22T04:13:00.000Z",
        "voteCount": 1,
        "content": "Agree with NicolasN, D is bad practice. For D this may result in permission creep, where a group is granted access to an increasing number of resources. Only grant service accounts specific access to resources."
      },
      {
        "date": "2022-12-21T11:38:00.000Z",
        "voteCount": 1,
        "content": "A is the answer, as NicalsN says.\nhttps://cloud.google.com/iam/docs/service-accounts#groups"
      },
      {
        "date": "2022-12-16T04:19:00.000Z",
        "voteCount": 2,
        "content": "Avoid using groups for granting service accounts access to resources -&gt; D"
      },
      {
        "date": "2022-12-16T04:20:00.000Z",
        "voteCount": 1,
        "content": "Sorry A"
      },
      {
        "date": "2022-11-28T08:11:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 201,
    "url": "https://www.examtopics.com/discussions/google/view/79651-exam-professional-data-engineer-topic-1-question-201/",
    "body": "You need to migrate a Redis database from an on-premises data center to a Memorystore for Redis instance. You want to follow Google-recommended practices and perform the migration for minimal cost, time and effort. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMake a secondary instance of the Redis database on a Compute Engine instance and then perform a live cutover.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow job to read the Redis database from the on-premises data center and write the data to a Memorystore for Redis instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a shell script to migrate the Redis data and create a new Memorystore for Redis instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-09-02T22:32:00.000Z",
        "voteCount": 13,
        "content": "A. Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance.\n\nThe import and export feature uses the native RDB snapshot feature of Redis to import data into or export data out of a Memorystore for Redis instance. The use of the native RDB format prevents lock-in and makes it very easy to move data within Google Cloud or outside of Google Cloud. Import and export uses Cloud Storage buckets to store RDB files.\n\nReference:\nhttps://cloud.google.com/memorystore/docs/redis/import-export-overview"
      },
      {
        "date": "2023-07-23T14:26:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2023-01-01T18:09:00.000Z",
        "voteCount": 1,
        "content": "A. Make an RDB backup of the Redis database, use the gsutil utility to copy the RDB file into a Cloud Storage bucket, and then import the RDB file into the Memorystore for Redis instance."
      },
      {
        "date": "2022-11-28T08:07:00.000Z",
        "voteCount": 4,
        "content": "A is the answer.\n\nhttps://cloud.google.com/memorystore/docs/redis/about-importing-exporting\nThe import and export feature uses the native RDB snapshot feature of Redis to import data into or export data out of a Memorystore for Redis instance. The use of the native RDB format prevents lock-in and makes it very easy to move data within Google Cloud or outside of Google Cloud. Import and export uses Cloud Storage buckets to store RDB files."
      },
      {
        "date": "2022-11-24T03:04:00.000Z",
        "voteCount": 1,
        "content": "A\nhttps://cloud.google.com/memorystore/docs/redis/import-data"
      },
      {
        "date": "2022-11-24T00:39:00.000Z",
        "voteCount": 2,
        "content": "A\nImport and export uses Cloud Storage buckets to store RDB files.\nhttps://cloud.google.com/memorystore/docs/redis/about-importing-exporting#:~:text=Import%20and%20export%20uses%20Cloud%20Storage%20buckets%20to%20store%20RDB%20files."
      },
      {
        "date": "2022-09-02T20:12:00.000Z",
        "voteCount": 2,
        "content": "A\nhttps://cloud.google.com/memorystore/docs/redis/general-best-practices"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 202,
    "url": "https://www.examtopics.com/discussions/google/view/79652-exam-professional-data-engineer-topic-1-question-202/",
    "body": "Your platform on your on-premises environment generates 100 GB of data daily, composed of millions of structured JSON text files. Your on-premises environment cannot be accessed from the public internet. You want to use Google Cloud products to query and explore the platform data. What should you do?<br>",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Scheduler to copy data daily from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Transfer Appliance to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Data Transfer Service dataset copy to transfer all data into BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-04-23T15:18:00.000Z",
        "voteCount": 9,
        "content": "Therefore, the correct option is C. Use Transfer Service for on-premises data to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery.\n\nOption A is incorrect because Cloud Scheduler is not designed for data transfer, but rather for scheduling the execution of Cloud Functions, Cloud Run, or App Engine applications.\n\nOption B is incorrect because Transfer Appliance is designed for large-scale data transfers from on-premises environments to Google Cloud and is not suitable for transferring data on a daily basis.\n\nOption D is also incorrect because the BigQuery Data Transfer Service dataset copy feature is designed for copying datasets between BigQuery projects and not suitable for copying data from on-premises environments to BigQuery."
      },
      {
        "date": "2024-01-15T01:23:00.000Z",
        "voteCount": 1,
        "content": "With BigQuery Data Transfer Service we can copy files not only from other BigQuery, but also a bunch of cloud services listed here: \nhttps://cloud.google.com/bigquery/docs/dts-introduction\nBut you are right. It wont work with on-premises."
      },
      {
        "date": "2023-06-27T04:32:00.000Z",
        "voteCount": 7,
        "content": "\"Your on-premises environment cannot be accessed from the public internet\" statement suggests that inbound traffic from internet is NOT allowed however, it doesn't mean that outbound internet connectivity from on-prem resources is not possible. Any on-prem system with outbound internet access can copy/transfer the CSV files.\n\nCSV files are located on a filesystem, therefore you cannot copy them with BQ Transfer Service.\n\nLeaving only possible option;\nfirst copy CSVs to cloud storage\nthen run BQ Transfer Service\n\npls refer to https://cloud.google.com/bigquery/docs/dts-introduction#supported_data_sources"
      },
      {
        "date": "2024-10-04T03:52:00.000Z",
        "voteCount": 1,
        "content": "They don't define \"cannot be accessed from the public internet\" - does this mean no incoming traffic, or no traffic or any kind regardless of the initiation point? We simply do not know, and so are left guessing. C? Probably, but could be B, just depending."
      },
      {
        "date": "2023-06-14T05:19:00.000Z",
        "voteCount": 2,
        "content": "the correct option is C"
      },
      {
        "date": "2023-03-09T04:09:00.000Z",
        "voteCount": 3,
        "content": "I would say B. It is the ONLY option that is possible without data being accessible over the public (unless we assume that a direct interconnect is already set up, which seems farfetched). Also, nowhere does it say how up-to-date the data needs to be that we are querying or how often we need to query, only that the data increases in size by 100gb per day (indicating that its going to be a lot of data)"
      },
      {
        "date": "2023-02-20T04:19:00.000Z",
        "voteCount": 2,
        "content": "Answer C,\nWhat is wrong with B ? Key words = Daily transfer .. so no to transfer appliance,"
      },
      {
        "date": "2022-11-28T08:03:00.000Z",
        "voteCount": 3,
        "content": "C is the answer.\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#storage-transfer-service-for-large-transfers-of-on-premises-data\nStorage Transfer Service for on-premises data enables transfers from network file system (NFS) storage to Cloud Storage.\n\nhttps://cloud.google.com/bigquery/docs/cloud-storage-transfer-overview\nThe BigQuery Data Transfer Service for Cloud Storage lets you schedule recurring data loads from Cloud Storage buckets to BigQuery."
      },
      {
        "date": "2023-01-01T18:11:00.000Z",
        "voteCount": 1,
        "content": "yes, It is C"
      },
      {
        "date": "2022-11-24T00:43:00.000Z",
        "voteCount": 1,
        "content": "C\nD-no answer because bq transfer service don't support from on-prem"
      },
      {
        "date": "2022-11-25T13:08:00.000Z",
        "voteCount": 1,
        "content": "B-is not answer because you want transfer appliance for one time bulk transfer but the question is You want to use Google Cloud products to query and explore the platform data. \n\n query and explore is the key"
      },
      {
        "date": "2022-10-04T09:03:00.000Z",
        "voteCount": 1,
        "content": "Transfer Service for on-premises is optimal for on-premises google  ( large files (&lt; 1 TB) and bandwidth available and scheduling)\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-storage-transfer-service-for-on-premises-data\n\nBigQuery Data Transfer Service  is good for gcs to bigquery\nhttps://cloud.google.com/bigquery/docs/cloud-storage-transfer"
      },
      {
        "date": "2022-10-04T09:05:00.000Z",
        "voteCount": 1,
        "content": "Sorry I am wrong \n ( large files  &gt; 1 TB  + bandwidth available on internal IP address communication + daily scheduling)"
      },
      {
        "date": "2022-10-04T09:03:00.000Z",
        "voteCount": 2,
        "content": "Your on-premises environment cannot be accessed from the public internet.\nIt signifies that we can apply private connection like Cloud Interconnect https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview"
      },
      {
        "date": "2022-09-13T16:17:00.000Z",
        "voteCount": 3,
        "content": "I will go with C"
      },
      {
        "date": "2022-09-10T08:10:00.000Z",
        "voteCount": 1,
        "content": "I  will g with C\n\nhttps://cloud.google.com/architecture/migration-to-google-cloud-transferring-your-large-datasets#transfer-options"
      },
      {
        "date": "2022-09-08T06:55:00.000Z",
        "voteCount": 2,
        "content": "C is correct, b is suitable for weekly .\nhttps://cloud.google.com/transfer-appliance/docs/4.0/overview"
      },
      {
        "date": "2022-10-04T08:55:00.000Z",
        "voteCount": 1,
        "content": "C\nYour on-premises environment cannot be accessed from the public internet. \nIt signifies  that   we can apply private connection  like Cloud Interconnect   https://cloud.google.com/network-connectivity/docs/interconnect/concepts/overview"
      },
      {
        "date": "2022-09-07T22:15:00.000Z",
        "voteCount": 1,
        "content": "Ans C\nhttps://cloud.google.com/storage-transfer/docs/on-prem-agent-best-practices"
      },
      {
        "date": "2022-09-07T09:13:00.000Z",
        "voteCount": 1,
        "content": "I would go with option C. \nYou need a service to transfer data from on-premises to cloud storage. so \"Transfer service\" is the best option &amp; additionally you can easily configure the network so that data flows through private network. \n\ncloud scheduler on other hand is used mostly for automation. You can schedule a service but in my view cannot be used solo to transfer data."
      },
      {
        "date": "2022-09-04T23:44:00.000Z",
        "voteCount": 2,
        "content": "Data is generated daily. Unlikely to ship Transfer Appliance every day.\n\nVote for C instead.  \"Transfer Service for on-premises data is a free Google Cloud service that's intended to streamline the process of uploading data into Google Cloud Storage buckets\"\n\nhttps://cloud.google.com/blog/products/storage-data-transfer/introducing-storage-transfer-service-for-on-premises-data"
      },
      {
        "date": "2022-09-02T20:15:00.000Z",
        "voteCount": 1,
        "content": "B. Use a Transfer Appliance to copy data from your on-premises environment to Cloud Storage. Use the BigQuery Data Transfer Service to import data into BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 203,
    "url": "https://www.examtopics.com/discussions/google/view/89246-exam-professional-data-engineer-topic-1-question-203/",
    "body": "A TensorFlow machine learning model on Compute Engine virtual machines (n2-standard-32) takes two days to complete training. The model has custom TensorFlow operations that must run partially on a CPU. You want to reduce the training time in a cost-effective manner. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the VM type to n2-highmem-32.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the VM type to e2-standard-32.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model using a VM with a GPU hardware accelerator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTrain the model using a VM with a TPU hardware accelerator."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2022-12-17T22:48:00.000Z",
        "voteCount": 5,
        "content": "Cost effective - among the choices, it is cheaper to have a temporary accelerator instead of increasing our VM cost for an indefinite amount of time\nD -&gt; TPU accelerator cannot support custom operations"
      },
      {
        "date": "2023-12-22T10:32:00.000Z",
        "voteCount": 5,
        "content": "The best way to reduce the TensorFlow training time in a cost-effective manner is to use a VM with a GPU hardware accelerator. TensorFlow can take advantage of GPUs to significantly speed up training time for many models.\n\nSpecifically, option C is the best choice.\n\nChanging the VM to another standard type like n2-highmem-32 or e2-standard-32 (options A and B) may provide some improvement, but likely not a significant speedup.\n\nUsing a TPU (option D) could speed up training, but TPUs are more costly than GPUs. For a cost-effective solution, GPU acceleration provides the best performance per dollar.\n\nSince the model must run partially on CPUs, a VM instance with GPUs added will allow TensorFlow to offload appropriate operations to the GPUs while keeping CPU-specific operations on the CPU. This can provide a significant reduction in training time for many common TensorFlow models while keeping costs reasonable"
      },
      {
        "date": "2024-05-05T04:48:00.000Z",
        "voteCount": 1,
        "content": "key pjrse is \"run partially on a CPU\" from https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus refers to GPU"
      },
      {
        "date": "2023-11-06T14:57:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/tpu/docs/intro-to-tpu#when_to_use_tpus"
      },
      {
        "date": "2023-01-01T18:12:00.000Z",
        "voteCount": 1,
        "content": "C. Train the model using a VM with a GPU hardware accelerator."
      },
      {
        "date": "2022-11-30T15:05:00.000Z",
        "voteCount": 1,
        "content": "C\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus:~:text=Models%20with%20a%20significant%20number%20of%20custom%20TensorFlow%20operations%20that%20must%20run%20at%20least%20partially%20on%20CPUs"
      },
      {
        "date": "2022-12-16T01:08:00.000Z",
        "voteCount": 3,
        "content": "The model has custom TensorFlow operations that must run partially on a CPU. is the key for GPU"
      },
      {
        "date": "2022-11-29T09:01:00.000Z",
        "voteCount": 4,
        "content": "C is the answer.\n\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus\nGPUs\n- Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs"
      },
      {
        "date": "2022-11-29T06:15:00.000Z",
        "voteCount": 1,
        "content": "I agree with C, for choosing a GPU one of the cases says:\n\"Models with a significant number of custom TensorFlow operations that must run at least partially on CPUs\"\nhttps://cloud.google.com/tpu/docs/tpus#when_to_use_tpus"
      },
      {
        "date": "2022-12-01T02:49:00.000Z",
        "voteCount": 1,
        "content": "C is not cost-effective, so I stand corrected. I do not know the answer."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 204,
    "url": "https://www.examtopics.com/discussions/google/view/89456-exam-professional-data-engineer-topic-1-question-204/",
    "body": "You want to create a machine learning model using BigQuery ML and create an endpoint for hosting the model using Vertex AI. This will enable the processing of continuous streaming data in near-real time from multiple vendors. The data may contain invalid values. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new BigQuery dataset and use streaming inserts to land the data from multiple vendors. Configure your BigQuery ML model to use the \"ingestion\" dataset as the framing data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery streaming inserts to land the data from multiple vendors where your BigQuery dataset ML model is deployed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic and send all vendor data to it. Connect a Cloud Function to the topic to process the data and store it in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub topic and send all vendor data to it. Use Dataflow to process and sanitize the Pub/Sub data and stream it to BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-09T09:15:00.000Z",
        "voteCount": 1,
        "content": "Why is the answer A? After paying $44 I am getting wrong answers."
      },
      {
        "date": "2024-07-22T16:28:00.000Z",
        "voteCount": 1,
        "content": "The discussion is where the real answer is."
      },
      {
        "date": "2024-01-13T13:36:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2023-07-23T13:08:00.000Z",
        "voteCount": 2,
        "content": "Option D -Dataflow provides a scalable and flexible way to process and clean the incoming data in real-time before loading it into BigQuery."
      },
      {
        "date": "2023-01-01T18:13:00.000Z",
        "voteCount": 1,
        "content": "D. Create a Pub/Sub topic and send all vendor data to it. Use Dataflow to process and sanitize the Pub/Sub data and stream it to BigQuery."
      },
      {
        "date": "2022-12-09T03:08:00.000Z",
        "voteCount": 2,
        "content": "D is the best option to sanitize the data to its D"
      },
      {
        "date": "2022-12-04T02:31:00.000Z",
        "voteCount": 2,
        "content": "Better to use pubsub for streaming and reading message data\n\nDataflow ParDo can perform filtering of data"
      },
      {
        "date": "2022-12-03T05:01:00.000Z",
        "voteCount": 1,
        "content": "D is the answer."
      },
      {
        "date": "2022-12-01T01:56:00.000Z",
        "voteCount": 2,
        "content": "It's D"
      },
      {
        "date": "2022-11-30T15:08:00.000Z",
        "voteCount": 2,
        "content": "Answer is D"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 205,
    "url": "https://www.examtopics.com/discussions/google/view/89458-exam-professional-data-engineer-topic-1-question-205/",
    "body": "You have a data processing application that runs on Google Kubernetes Engine (GKE). Containers need to be launched with their latest available configurations from a container registry. Your GKE nodes need to have GPUs, local SSDs, and 8 Gbps bandwidth. You want to efficiently provision the data processing infrastructure and manage the deployment process. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Compute Engine startup scripts to pull container images, and use gcloud commands to provision the infrastructure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse GKE to autoscale containers, and use gcloud commands to provision the infrastructure.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to provision the data pipeline, and use Cloud Scheduler to run the job."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-09T09:18:00.000Z",
        "voteCount": 2,
        "content": "another wrong answer?"
      },
      {
        "date": "2024-01-02T11:36:00.000Z",
        "voteCount": 2,
        "content": "- Dataflow is a fully managed service for stream and batch data processing and is well-suited for real-time data processing tasks like identifying longtail and outlier data points. \n- Using BigQuery as a sink allows to efficiently store the cleansed and processed data for further analysis and serving it to AI models."
      },
      {
        "date": "2023-12-22T10:43:00.000Z",
        "voteCount": 4,
        "content": "B is the best option to efficiently provision and manage the deployment process for this data processing application on GKE:"
      },
      {
        "date": "2023-12-22T10:43:00.000Z",
        "voteCount": 2,
        "content": "\u2022\tCloud Build allows you to automate the building, testing, and deployment of your application using Docker containers.\n\u2022\tUsing Terraform with Cloud Build provides Infrastructure as Code capabilities to provision the GKE cluster with GPUs, SSDs, and network bandwidth.\n\u2022\tTerraform can be configured to pull the latest container images from the registry when deploying.\n\u2022\tCloud Build triggers provide event-based automation to rebuild and redeploy when container images are updated.\n\u2022\tThis provides an automated CI/CD pipeline to launch the application on GKE using the desired infrastructure and latest images.\n\u2022\tDataflow and Cloud Scheduler don't directly provide infrastructure provisioning or deployment orchestration for GKE.\n\u2022\tgcloud commands can be used but don't provide the same automation benefits as Cloud Build + Terraform."
      },
      {
        "date": "2023-12-22T21:17:00.000Z",
        "voteCount": 2,
        "content": "So using Cloud Build with Terraform templates provides the most efficient way to provision and deploy this data processing application on GKE."
      },
      {
        "date": "2023-11-06T14:16:00.000Z",
        "voteCount": 1,
        "content": "I don't really like B or C... but given the choices I would go with B.\nB-Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images. {The Terraform command is Terraform Apply and not Terraform build, but also why not use gcloud container command instead of introducing 3rd party builder image?)... I don't like this choice but it is the best one.\nC. Use GKE to autoscale containers, and use gcloud commands to provision the infrastructure. {This doesn't handle the building of the infra, or the deployment of the latest images, this one is clearly wrong, not sure why it is marked as the right choice}"
      },
      {
        "date": "2023-07-23T13:01:00.000Z",
        "voteCount": 2,
        "content": "B is correct"
      },
      {
        "date": "2023-04-14T04:24:00.000Z",
        "voteCount": 1,
        "content": "B is correct"
      },
      {
        "date": "2023-02-12T10:35:00.000Z",
        "voteCount": 1,
        "content": "b is ok"
      },
      {
        "date": "2023-01-01T18:15:00.000Z",
        "voteCount": 1,
        "content": "B. Use Cloud Build to schedule a job using Terraform build to provision the infrastructure and launch with the most current container images."
      },
      {
        "date": "2022-12-05T02:58:00.000Z",
        "voteCount": 2,
        "content": "B is the answer."
      },
      {
        "date": "2022-12-04T00:28:00.000Z",
        "voteCount": 3,
        "content": "Maybe B\nref: https://cloud.google.com/architecture/managing-infrastructure-as-code"
      },
      {
        "date": "2022-11-30T15:10:00.000Z",
        "voteCount": 1,
        "content": "C is correct answer"
      },
      {
        "date": "2022-11-30T23:20:00.000Z",
        "voteCount": 3,
        "content": "Sorry I meant B"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 206,
    "url": "https://www.examtopics.com/discussions/google/view/129853-exam-professional-data-engineer-topic-1-question-206/",
    "body": "You need ads data to serve AI models and historical data for analytics. Longtail and outlier data points need to be identified. You want to cleanse the data in near-real time before running it through AI models. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Storage as a data warehouse, shell scripts for processing, and BigQuery to create views for desired datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to ingest, prepare, and then analyze the data, and then run queries to create views.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to identify longtail and outlier data points, and then output a usable dataset to BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-03-13T06:15:00.000Z",
        "voteCount": 1,
        "content": "Dataflow for Real-Time Processing: Dataflow allows you to process data in near-real time, making it well-suited for identifying longtail and outlier data points as they occur. You can use Dataflow to implement custom data cleansing and outlier detection algorithms that operate on streaming data.\n\nBigQuery as a Sink: Using BigQuery as a sink allows you to store the cleaned and processed data efficiently for further analysis or use in AI models. Dataflow can write the cleaned data to BigQuery tables, enabling seamless integration with downstream processes."
      },
      {
        "date": "2024-02-15T08:31:00.000Z",
        "voteCount": 1,
        "content": "B. Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink."
      },
      {
        "date": "2024-01-15T01:42:00.000Z",
        "voteCount": 1,
        "content": "B. Use Dataflow to identify longtail and outlier data points programmatically, with BigQuery as a sink."
      },
      {
        "date": "2024-01-13T01:22:00.000Z",
        "voteCount": 1,
        "content": "B: Dataflow, solves exactly the use case described"
      },
      {
        "date": "2024-01-07T03:06:00.000Z",
        "voteCount": 2,
        "content": "B is the best option for cleansing the ads data in near real-time before running it through AI models.\nThe key reasons are:\n\u2022\tDataflow allows for stream processing of data in near real-time. This allows you to identify and cleanse longtail and outlier data points as the data is streamed in.\n\u2022\tDataflow has built-in capabilities for detecting and handling outliers and anomalies in streaming data. This makes it well-suited for programmatically identifying longtail and outlier data points.\n\u2022\tUsing BigQuery as the output sink allows the cleansed data to be immediatley available for analysis and serving to AI models. BigQuery can act as a serving layer for the models.\n\u2022\tOptions A, C, and D either don't provide real-time processing (A and C) or don't easily integrate with BigQuery for analysis and serving (D)."
      },
      {
        "date": "2024-01-07T03:06:00.000Z",
        "voteCount": 1,
        "content": "So B is the best architecture here to meet the needs of near real-time cleansing, identification of longtail/outlier data points, and integration with BigQuery for serving AI models."
      },
      {
        "date": "2024-01-02T11:37:00.000Z",
        "voteCount": 1,
        "content": "- Dataflow is a fully managed service for stream and batch data processing and is well-suited for real-time data processing tasks like identifying longtail and outlier data points.\n- Using BigQuery as a sink allows to efficiently store the cleansed and processed data for further analysis and serving it to AI models."
      },
      {
        "date": "2023-12-30T01:24:00.000Z",
        "voteCount": 1,
        "content": "Real-time Data Processing: Dataflow excels at handling large-scale, streaming data with low latency, enabling near-real-time cleansing.\nScalability: Easily scales to handle growing data volumes and processing needs.\nProgrammatic Data Cleaning: Allows you to write custom logic in Apache Beam for identifying longtail and outlier data points accurately and efficiently.\nIntegration with BigQuery: Seamless integration with BigQuery enables you to store cleansed data for AI model training and historical analytics.\nCost-Effective: Dataflow's pay-as-you-go model optimizes costs for real-time data processing."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 207,
    "url": "https://www.examtopics.com/discussions/google/view/129854-exam-professional-data-engineer-topic-1-question-207/",
    "body": "You are collecting IoT sensor data from millions of devices across the world and storing the data in BigQuery. Your access pattern is based on recent data, filtered by location_id and device_version with the following query:<br><br><img src=\"https://img.examtopics.com/professional-data-engineer/image1.png\"><br><br>You want to optimize your queries for cost and performance. How should you structure your data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition table data by create_date, location_id, and device_version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition table data by create_date, cluster table data by location_id, and device_version.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster table data by create_date, location_id, and device_version.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster table data by create_date, partition by location_id, and device_version."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-15T08:33:00.000Z",
        "voteCount": 1,
        "content": "B. Partition table data by create_date, cluster table data by location_id, and device_version."
      },
      {
        "date": "2024-01-15T01:45:00.000Z",
        "voteCount": 1,
        "content": "B. Partition table data by create_date, cluster table data by location_id, and device_version."
      },
      {
        "date": "2024-01-13T01:25:00.000Z",
        "voteCount": 1,
        "content": "B: Partitioning makes date-related querying efficient, clustering will keep relevant data close together and optimize the performance of filters for the cluster columns"
      },
      {
        "date": "2024-01-07T03:11:00.000Z",
        "voteCount": 2,
        "content": "1.\tPartitioning the data by create_date will allow BigQuery to prune partitions that are not relevant to the query by date.\n2.\tClustering the data by location_id and device_version within each partition will keep related data close together and optimize the performance of filters on those columns. \nThis provides both the pruning benefits of partitioning and locality benefits of clustering for filters on multiple columns.\nThe query provided indicates that the access pattern is primarily based on the most recent data (within the last 7 days), filtered by location_id and device_version. Given this pattern, you would want to optimize your table structure in such a way that queries scanning through the data will process the least amount of data possible to reduce costs and improve performance."
      },
      {
        "date": "2024-01-07T01:58:00.000Z",
        "voteCount": 1,
        "content": "Only correct answer is B, you can only partition by one field, and you can only cluster on partitioned tables"
      },
      {
        "date": "2024-01-02T11:39:00.000Z",
        "voteCount": 2,
        "content": "Answer is B:\n- Partitioning the table by create_date allows us to efficiently query data based on time, which is common in access patterns that prioritize recent data. \n- Clustering the table by location_id and device_version further organizes the data within each partition, making queries filtered by these columns more efficient and cost-effective."
      },
      {
        "date": "2023-12-30T01:29:00.000Z",
        "voteCount": 2,
        "content": "The best answer is B. Partition table data by create_date, cluster table data by location_id, and device_version.\n\nHere's a breakdown of why this structure is optimal:\n\nPartitioning by create_date:\n\nAligns with query pattern: Filters for recent data based on create_date, so partitioning by this column allows BigQuery to quickly narrow down the data to scan, reducing query costs and improving performance.\nManages data growth: Partitioning effectively segments data by date, making it easier to manage large datasets and optimize storage costs.\nClustering by location_id and device_version:\n\nEnhances filtering: Frequently filtering by location_id and device_version, clustering physically co-locates related data within partitions, further reducing scan time and improving performance."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 208,
    "url": "https://www.examtopics.com/discussions/google/view/129855-exam-professional-data-engineer-topic-1-question-208/",
    "body": "A live TV show asks viewers to cast votes using their mobile phones. The event generates a large volume of data during a 3-minute period. You are in charge of the \"Voting infrastructure\" and must ensure that the platform can handle the load and that all votes are processed. You must display partial results while voting is open. After voting closes, you need to count the votes exactly once while optimizing cost. What should you do?<br><br><img src=\"https://img.examtopics.com/professional-data-engineer/image2.png\">",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Memorystore instance with a high availability (HA) configuration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud SQL for PostgreSQL database with high availability (HA) configuration and multiple read replicas.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite votes to a Pub/Sub topic and have Cloud Functions subscribe to it and write votes to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T03:17:00.000Z",
        "voteCount": 6,
        "content": "Since cost optimization and minimal latency are key requirements, option D is likely the best choice to meet all the needs:\n\nThe key reasons option D works well:\n\nUsing Pub/Sub to ingest votes provides scalable, reliable transport.\n\nLoading into Bigtable and BigQuery provides both:\n\nLow latency reads from Bigtable for real-time results.\nCost effective storage in BigQuery for longer term analysis.\nShutting down Bigtable after voting concludes reduces costs.\n\nBigQuery remains available for cost-optimized storage and analysis.\n\nSo you are correct that option D combines the best of real-time performance for queries using Bigtable, with cost-optimized storage in BigQuery.\n\nThe only additional consideration may be if 3 minutes of Bigtable usage still incurs higher charges than ingesting directly into BigQuery. But for minimizing latency while optimizing cost, option D is likely the right architectural choice given the requirements."
      },
      {
        "date": "2024-02-15T08:34:00.000Z",
        "voteCount": 1,
        "content": "D. Write votes to a Pub/Sub topic and load into both Bigtable and BigQuery via a Dataflow pipeline. Query Bigtable for real-time results and BigQuery for later analysis. Shut down the Bigtable instance when voting concludes."
      },
      {
        "date": "2024-01-13T01:26:00.000Z",
        "voteCount": 1,
        "content": "D, i do agree with everything MaxNRG said."
      },
      {
        "date": "2024-01-07T02:06:00.000Z",
        "voteCount": 1,
        "content": "Pub/Sub for sure, and Cloud Functions + BigQuery Streaming seems a good solution. Won't use BigTable as need at least 100GB of data (don't thing a voting system could arrive to that amount of data) and needs to \"heat\" to work right for &gt;10 minutes... and would be $$$ over C solution"
      },
      {
        "date": "2024-01-02T11:54:00.000Z",
        "voteCount": 2,
        "content": "Answer is D:\n- Google Cloud Pub/Sub can manage the high-volume data ingestion. \n- Google Cloud Dataflow can efficiently process and route data to both Bigtable and BigQuery.\n- Bigtable is excellent for handling high-throughput writes and reads, making it suitable for real-time vote tallying. \n- BigQuery is ideal for exact vote counting and deeper analysis once voting concludes."
      },
      {
        "date": "2023-12-30T01:30:00.000Z",
        "voteCount": 3,
        "content": "Handling High-Volume Data Ingestion:\n\nPub/Sub: Decouples vote collection from processing, ensuring scalability and resilience under high load.\nDataflow: Efficiently ingests and processes large data streams, scaling as needed.\nReal-Time Results with Exactly-Once Processing:\n\nBigtable: Optimized for low-latency, high-throughput reads and writes, ideal for real-time partial results.\nExactly-Once Semantics: Dataflow guarantees each vote is processed only once, ensuring accurate counts.\nCost Optimization:\n\nTemporary Bigtable Instance: Running Bigtable only during voting minimizes costs.\nBigQuery Storage: Cost-effective for long-term storage and analysis."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 209,
    "url": "https://www.examtopics.com/discussions/google/view/129856-exam-professional-data-engineer-topic-1-question-209/",
    "body": "A shipping company has live package-tracking data that is sent to an Apache Kafka stream in real time. This is then loaded into BigQuery. Analysts in your company want to query the tracking data in BigQuery to analyze geospatial trends in the lifecycle of a package. The table was originally created with ingest-date partitioning. Over time, the query processing time has increased. You need to copy all the data to a new clustered table. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRe-create the table using data partitioning on the package delivery date.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement clustering in BigQuery on the package-tracking ID column.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement clustering in BigQuery on the ingest date column.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTier older data onto Cloud Storage files and create a BigQuery table using Cloud Storage as an external data source."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-15T08:37:00.000Z",
        "voteCount": 1,
        "content": "B. Implement clustering in BigQuery on the package-tracking ID column."
      },
      {
        "date": "2024-01-15T01:55:00.000Z",
        "voteCount": 1,
        "content": "B. Implement clustering in BigQuery on the package-tracking ID column."
      },
      {
        "date": "2024-01-13T01:28:00.000Z",
        "voteCount": 1,
        "content": "Definitely B"
      },
      {
        "date": "2024-01-07T03:30:00.000Z",
        "voteCount": 3,
        "content": "This looks like Question #166\n\nOption B, implementing clustering in BigQuery on the package-tracking ID column, seems the most appropriate. It directly addresses the query slowdown issue by reorganizing the data in a way that aligns with the analysts' query patterns, leading to more efficient and faster query execution."
      },
      {
        "date": "2024-01-02T12:02:00.000Z",
        "voteCount": 3,
        "content": "Answer is B"
      },
      {
        "date": "2023-12-30T01:31:00.000Z",
        "voteCount": 4,
        "content": "Query Focus: Analysts are interested in geospatial trends within individual package lifecycles. Clustering by package-tracking ID physically co-locates related data, significantly improving query performance for these analyses.\n\nAddressing Slow Queries: Clustering addresses the query slowdown issue by optimizing data organization for the specific query patterns.\n\nPartitioning vs. Clustering:\n\nPartitioning: Divides data into segments based on a column's values, primarily for managing large datasets and optimizing query costs.\nClustering: Organizes data within partitions for faster querying based on specific columns."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 210,
    "url": "https://www.examtopics.com/discussions/google/view/129857-exam-professional-data-engineer-topic-1-question-210/",
    "body": "You are designing a data mesh on Google Cloud with multiple distinct data engineering teams building data products. The typical data curation design pattern consists of landing files in Cloud Storage, transforming raw data in Cloud Storage and BigQuery datasets, and storing the final curated data product in BigQuery datasets. You need to configure Dataplex to ensure that each team can access only the assets needed to build their data products. You also need to ensure that teams can easily share the curated data product. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.<br>2. Provide each data engineering team access to the virtual lake.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a single Dataplex virtual lake and create a single zone to contain landing, raw, and curated data.<br>2. Build separate assets for each data product within the zone.<br>3. Assign permissions to the data engineering teams at the zone level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Dataplex virtual lake for each data product, and create a single zone to contain landing, raw, and curated data.<br>2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Dataplex virtual lake for each data product, and create multiple zones for landing, raw, and curated data.<br>2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-15T10:22:00.000Z",
        "voteCount": 1,
        "content": "Answer D"
      },
      {
        "date": "2024-01-15T01:58:00.000Z",
        "voteCount": 1,
        "content": "D. 1. Create a Dataplex virtual lake for each data product, and create multiple zones for landing, raw, and curated data.\n2. Provide the data engineering teams with full access to the virtual lake assigned to their data product.\n\nLake: A logical construct representing a data domain or business unit. For example, to organize data based on group usage, you can set up a lake for each department (for example, Retail, Sales, Finance).\nZone: A subdomain within a lake, which is useful to categorize data by the following:\nStage: For example, landing, raw, curated data analytics, and curated data science."
      },
      {
        "date": "2024-01-15T01:58:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dataplex/docs/introduction"
      },
      {
        "date": "2024-01-13T01:32:00.000Z",
        "voteCount": 1,
        "content": "D: 1 virtual lake per Data Product (which stands for domain basically), zones to split data by \"status\". Each Data Eng team can access their own data exclusively and in a data mesh compliant way"
      },
      {
        "date": "2024-01-07T10:59:00.000Z",
        "voteCount": 3,
        "content": "The best approach is to create a Dataplex virtual lake for each data product, with multiple zones for landing, raw, and curated data. Then provide the data engineering teams with access only to the zones they need within the virtual lake assigned to their product.\n\nTo enable teams to easily share curated data products, you should use cross-lake sharing in Dataplex. This allows curated zones to be shared across virtual lakes while maintaining data isolation for other zones."
      },
      {
        "date": "2024-01-07T11:00:00.000Z",
        "voteCount": 3,
        "content": "So the steps would be:\n1.\tCreate a Dataplex virtual lake for each data product.\n2.\tWithin each lake, create separate zones for landing, raw, and curated data.\n3.\tProvide each data engineering team with access only to the zones they need within their assigned virtual lake.\n4.\tConfigure cross-lake sharing on the curated data zones to share curated data products between teams.\nThis provides isolation and access control between teams for raw data while enabling easy sharing of curated data products.\nhttps://cloud.google.com/dataplex/docs/introduction#a_domain-centric_data_mesh"
      },
      {
        "date": "2024-01-07T02:10:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is B, but there is a misspelling in the answer, should say \"create multiple zones\""
      },
      {
        "date": "2024-01-06T18:26:00.000Z",
        "voteCount": 1,
        "content": "Each lake should be created per data product since data product sounds like a domain in this question. \n\nSince we have landing, raw, curated data, we should create different zones. \n\n\"Zones are of two types: raw and curated.\n\nRaw zone: Contains data that is in its raw format and not subject to strict type-checking.\n\nCurated zone: Contains data that is cleaned, formatted, and ready for analytics. The data is columnar, Hive-partitioned, and stored in Parquet, Avro, Orc files, or BigQuery tables. Data undergoes type-checking- for example, to prohibit the use of CSV files because they don't perform as well for SQL access.\"\n\nRef: https://cloud.google.com/dataplex/docs/introduction#terminology"
      },
      {
        "date": "2024-01-05T09:58:00.000Z",
        "voteCount": 4,
        "content": "why not B?"
      },
      {
        "date": "2024-01-05T01:49:00.000Z",
        "voteCount": 3,
        "content": "Why not B?"
      },
      {
        "date": "2024-01-17T02:31:00.000Z",
        "voteCount": 2,
        "content": "Because it's the best practice is separated zones. One zone for landing, raw and curated. \n\nThe answer B - has this part that excluded it \"create a single zone to contain landing\"\n\nThe correct awser is D"
      },
      {
        "date": "2024-01-02T18:38:00.000Z",
        "voteCount": 2,
        "content": "The answer is D"
      },
      {
        "date": "2023-12-30T01:33:00.000Z",
        "voteCount": 2,
        "content": "Virtual Lake per Data Product: Each virtual lake acts as a self-contained domain for a specific data product, aligning with the data mesh principle of decentralized ownership and responsibility.\nTeam Autonomy: Teams have full control over their virtual lake, enabling independent development, management, and sharing of their data products."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 211,
    "url": "https://www.examtopics.com/discussions/google/view/129858-exam-professional-data-engineer-topic-1-question-211/",
    "body": "You are using BigQuery with a multi-region dataset that includes a table with the daily sales volumes. This table is updated multiple times per day. You need to protect your sales table in case of regional failures with a recovery point objective (RPO) of less than 24 hours, while keeping costs to a minimum. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a daily export of the table to a Cloud Storage dual or multi-region bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a daily copy of the dataset to a backup region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a daily BigQuery snapshot of the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tModify ETL job to load the data into both the current and another backup region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T11:26:00.000Z",
        "voteCount": 12,
        "content": "Why not C:\n\nA table snapshot must be in the same region, and under the same organization, as its base table.\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#limitations"
      },
      {
        "date": "2024-01-07T11:27:00.000Z",
        "voteCount": 5,
        "content": "Based on the information provided and the need to avoid data loss in the case of a hard regional failure in BigQuery, which could result in the destruction of all data in that region, the focus should be on creating backups in a geographically distinct region. Considering this scenario, the most suitable option would be Option A\n\nHere's why this option is the most appropriate:"
      },
      {
        "date": "2024-01-07T11:28:00.000Z",
        "voteCount": 3,
        "content": "\u2022\tCross-Region Backup: Exporting the data to a Google Cloud Storage bucket that is either dual or multi-regional ensures that your backups are stored in a different geographic location. This is critical for protecting against hard regional failures.\n\u2022\tData Durability: Cloud Storage provides high durability for stored data, making it a reliable option for backups in the case of regional disasters.\n\u2022\tCost-Effectiveness: While there are costs associated with storage and data transfer, this method can be more cost-effective compared to maintaining active replicas of the data in multiple regions, especially if the data is large."
      },
      {
        "date": "2024-01-07T11:28:00.000Z",
        "voteCount": 3,
        "content": "\u2022\tFlexibility and Automation: The export process can be automated and scheduled to occur daily, aligning with your RPO of less than 24 hours. This ensures that the most recent data is always backed up.\n\u2022\tRecovery Process: In the event of a hard regional failure, the data can be restored from the Cloud Storage backup to another operational BigQuery region, ensuring continuity of operations."
      },
      {
        "date": "2024-01-07T11:29:00.000Z",
        "voteCount": 4,
        "content": "The other options, while viable in certain scenarios, do not provide the same level of protection against a hard regional failure:\n\u2022\tOption B (Copy to Backup Region) and Option D (Modify ETL to Load into Backup Region) do not address the possibility of a hard regional failure adequately, as they do not necessarily imply storing data in a geographically distinct region.\n\u2022\tOption C (BigQuery Snapshot) is useful for point-in-time recovery but does not inherently protect against hard regional failures since the snapshots are within the same BigQuery service.\nFocusing on a robust disaster recovery strategy is crucial. Option A provides a balance between ensuring data availability in the event of a regional disaster and managing costs, aligning with best practices for data management in the cloud."
      },
      {
        "date": "2024-01-02T13:13:00.000Z",
        "voteCount": 7,
        "content": "Option C provides cost-effective way. \n- BigQuery table snapshots are a feature that allows you to capture the state of a table at a particular point in time. \n- Snapshots are incremental, so they only store the data that has changed, making them more cost-effective than full table copies. \n- In the event of a regional failure, you can quickly restore the table from a snapshot."
      },
      {
        "date": "2024-08-29T09:40:00.000Z",
        "voteCount": 1,
        "content": "A : BQ snapshot should be in same region, and if so the region fails so does the snapshot."
      },
      {
        "date": "2024-08-19T02:03:00.000Z",
        "voteCount": 1,
        "content": "Why Option A is not suitable: Restoring data from Option A would require reloading it back into BigQuery, which is time-consuming. This process cannot guarantee a recovery point objective (RPO) of less than 24 hours."
      },
      {
        "date": "2024-08-11T22:41:00.000Z",
        "voteCount": 2,
        "content": "Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page."
      },
      {
        "date": "2024-08-10T09:04:00.000Z",
        "voteCount": 1,
        "content": "C seems correct and Raaad also saying same."
      },
      {
        "date": "2024-04-23T03:36:00.000Z",
        "voteCount": 1,
        "content": "it is c"
      },
      {
        "date": "2024-02-23T11:08:00.000Z",
        "voteCount": 2,
        "content": "C. Schedule a daily BigQuery snapshot of the table.\n\nHere's why:\n\nCost-effective: BigQuery snapshots are significantly cheaper than daily exports to Cloud Storage or copying the entire dataset to a backup region. They offer point-in-time backups with minimal storage costs.\nFast recovery: Snapshots can be restored quickly, meeting your RPO requirement of less than 24 hours.\nMulti-regional: By default, BigQuery snapshots are automatically stored in a different region from the source data, ensuring redundancy and disaster recovery."
      },
      {
        "date": "2024-03-13T04:53:00.000Z",
        "voteCount": 2,
        "content": "At the beginning I also thought that \"C\" is a correct answer, but futher I found documentation https://cloud.google.com/bigquery/docs/locations. According this documentation \n\"Selecting a multi-region location does not provide cross-region replication or regional redundancy, so there is no increase in dataset availability in the event of a regional outage. Data is stored in a single region within the geographic location.\n\nData located in the EU multi-region is only stored in the europe-west1 (Belgium) or europe-west4 (Netherlands) data centers\"\nSo, multi-region dataset just means locating data inside one of US regions, hence snapshot also will be stored in the same region what means that answer C is not correct"
      },
      {
        "date": "2024-02-15T10:25:00.000Z",
        "voteCount": 1,
        "content": "A. Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket."
      },
      {
        "date": "2024-01-18T09:22:00.000Z",
        "voteCount": 2,
        "content": "Option A. Check the ref for regional loss -\nhttps://cloud.google.com/bigquery/docs/reliability-intro#scenario_loss_of_region"
      },
      {
        "date": "2024-01-15T02:24:00.000Z",
        "voteCount": 1,
        "content": "A. Schedule a daily export of the table to a Cloud Storage dual or multi-region bucket."
      },
      {
        "date": "2024-01-13T01:39:00.000Z",
        "voteCount": 2,
        "content": "A: MaxNRG and Helinia cleared the reasons very well"
      },
      {
        "date": "2024-01-06T19:23:00.000Z",
        "voteCount": 5,
        "content": "\"BigQuery does not offer durability or availability in the extraordinarily unlikely and unprecedented event of physical region loss. This is true for both \"regions and multi-region\" configurations. Hence maintaining durability and availability under such a scenario requires customer planning.\"\n\n\"To avoid data loss in the face of destructive regional loss, you need to back up data to another geographic location. For example, you could periodically export a snapshot of your data to Google Cloud Storage in another geographically distinct region.\"\n\nRef: https://cloud.google.com/bigquery/docs/reliability-intro#scenario_loss_of_region"
      },
      {
        "date": "2024-01-07T11:10:00.000Z",
        "voteCount": 1,
        "content": "Option A (Export to Cloud Storage): While exporting to Cloud Storage is a viable backup strategy, it can be more expensive and less efficient than using snapshots, especially if the table is large and updated frequently."
      },
      {
        "date": "2024-01-07T11:22:00.000Z",
        "voteCount": 1,
        "content": "I agree, It's A:\nA table snapshot must be in the same region, and under the same organization, as its base table.\nhttps://cloud.google.com/bigquery/docs/table-snapshots-intro#limitations"
      },
      {
        "date": "2024-01-07T06:51:00.000Z",
        "voteCount": 3,
        "content": "Why not C: \n\"BigQuery also supports the ability to snapshot tables. With this feature you can explicitly backup data within the same region for longer than the 7 day time travel window. A snapshot is purely a metadata operation and results in no additional storage bytes. While this can add protection against accidental deletion, it does not increase the durability of the data.\"\n\nhttps://cloud.google.com/bigquery/docs/reliability-intro#scenario_accidental_deletion_or_data_corruption"
      },
      {
        "date": "2024-01-04T20:19:00.000Z",
        "voteCount": 3,
        "content": "Option A"
      },
      {
        "date": "2024-01-04T20:20:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/reliability-intro"
      },
      {
        "date": "2023-12-30T01:34:00.000Z",
        "voteCount": 2,
        "content": "Automatically replicates data to a backup region upon each update, ensuring an RPO of less than 24 hours, even with multiple daily updates."
      },
      {
        "date": "2024-01-02T13:15:00.000Z",
        "voteCount": 4,
        "content": "Option D:\nDoubles the write load and storage costs since you are maintaining two live datasets."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 212,
    "url": "https://www.examtopics.com/discussions/google/view/129859-exam-professional-data-engineer-topic-1-question-212/",
    "body": "You are troubleshooting your Dataflow pipeline that processes data from Cloud Storage to BigQuery. You have discovered that the Dataflow worker nodes cannot communicate with one another. Your networking team relies on Google Cloud network tags to define firewall rules. You need to identify the issue while following Google-recommended networking security practices. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether your Dataflow pipeline has a custom network tag set.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 for the Dataflow network tag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 on the subnet used by Dataflow workers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDetermine whether your Dataflow pipeline is deployed with the external IP address option enabled."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T11:46:00.000Z",
        "voteCount": 8,
        "content": "The best approach would be to check if there is a firewall rule allowing traffic on TCP ports 12345 and 12346 for the Dataflow network tag.\nDataflow uses TCP ports 12345 and 12346 for communication between worker nodes. Using network tags and associated firewall rules is a Google-recommended security practice for controlling access between Compute Engine instances like Dataflow workers.\n\nSo the key things to check would be:\n\n1.\tEnsure your Dataflow pipeline is using the Dataflow network tag on the worker nodes. This tag is applied by default unless overridden.\n2.\tCheck if there is a firewall rule allowing TCP 12345 and 12346 ingress and egress traffic for instances with the Dataflow network tag. If not, add the rule.\n\nOptions A, C and D relate to other networking aspects but do not directly address the Google recommended practice of using network tags and firewall rules."
      },
      {
        "date": "2024-02-15T10:27:00.000Z",
        "voteCount": 1,
        "content": "B. Determine whether there is a firewall rule set to allow traffic on TCP ports 12345 and 12346 for the Dataflow network tag."
      },
      {
        "date": "2024-01-13T02:18:00.000Z",
        "voteCount": 1,
        "content": "B, check if there is a firewall rule allowing traffic on TCP ports 12345 and 12346 for the Dataflow network tag."
      },
      {
        "date": "2024-01-07T02:28:00.000Z",
        "voteCount": 3,
        "content": "Because network tags are used and Dataflow uses TCP ports 12345 and 12346 as stated on\nhttps://cloud.google.com/dataflow/docs/guides/routes-firewall"
      },
      {
        "date": "2024-01-02T14:00:00.000Z",
        "voteCount": 3,
        "content": "This option focuses directly on ensuring that the firewall rules are set up correctly for the network tags used by Dataflow worker nodes. It specifically addresses the potential issue of worker nodes not being able to communicate due to restrictive firewall rules blocking the necessary ports."
      },
      {
        "date": "2023-12-30T01:36:00.000Z",
        "voteCount": 2,
        "content": "Focus on Network Tags:\n\nAdheres to the recommended practice of using network tags for firewall configuration, enhancing security and flexibility.\nAvoids targeting specific subnets, which can be less secure and harder to manage."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 213,
    "url": "https://www.examtopics.com/discussions/google/view/129860-exam-professional-data-engineer-topic-1-question-213/",
    "body": "Your company's customer_order table in BigQuery stores the order history for 10 million customers, with a table size of 10 PB. You need to create a dashboard for the support team to view the order history. The dashboard has two filters, country_name and username. Both are string data types in the BigQuery table. When a filter is applied, the dashboard fetches the order history from the table and displays the query results. However, the dashboard is slow to show the results when applying the filters to the following query:<br><br><img src=\"https://img.examtopics.com/professional-data-engineer/image3.png\"><br><br>How should you redesign the BigQuery table to support faster access?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster the table by country and username fields.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCluster the table by country field, and partition by username field.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the table by country and username fields.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tPartition the table by _PARTITIONTIME."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-03-07T10:27:00.000Z",
        "voteCount": 1,
        "content": "Why not D?  if u partition by date and  int  going to be the best option??"
      },
      {
        "date": "2024-02-15T10:31:00.000Z",
        "voteCount": 2,
        "content": "If country is represented by an integer code, then partition by country and cluster by username would be a better solution. As country code is a string, available best solution is  \"A. Cluster the table by country and username fields.\""
      },
      {
        "date": "2024-01-15T03:09:00.000Z",
        "voteCount": 4,
        "content": "Correct answer: A. Cluster the table by country and username fields.\n\nWhy not B and C - &gt; Intiger is required for partitioning \nhttps://cloud.google.com/bigquery/docs/partitioned-tables#integer_range"
      },
      {
        "date": "2024-01-13T02:42:00.000Z",
        "voteCount": 3,
        "content": "A: the fields are both strings, which are not supported for partitioning. Moreover, the fields are regularly used in filters, which is where clustering really improves performance"
      },
      {
        "date": "2024-03-19T19:24:00.000Z",
        "voteCount": 1,
        "content": "Is not mandatory to have partitioning for clustering?"
      },
      {
        "date": "2024-01-08T12:00:00.000Z",
        "voteCount": 1,
        "content": "Clustering can also be done after partiton?"
      },
      {
        "date": "2024-03-08T01:33:00.000Z",
        "voteCount": 2,
        "content": "Yes but the partition is done on username field which has 10 million values. Since a BQ table can only have 4000 it is not suitable"
      },
      {
        "date": "2024-01-02T14:09:00.000Z",
        "voteCount": 4,
        "content": "- Clustering organizes the data based on the specified columns (in this case, country_name and username). \n- When a query filters on these columns, BigQuery can efficiently scan only the relevant parts of the table"
      },
      {
        "date": "2023-12-30T01:38:00.000Z",
        "voteCount": 3,
        "content": "country and username  --&gt; cluster"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 214,
    "url": "https://www.examtopics.com/discussions/google/view/129861-exam-professional-data-engineer-topic-1-question-214/",
    "body": "You have a Standard Tier Memorystore for Redis instance deployed in a production environment. You need to simulate a Redis instance failover in the most accurate disaster recovery situation, and ensure that the failover has no impact on production data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Standard Tier Memorystore for Redis instance in the development environment. Initiate a manual failover by using the limited-data-loss data protection mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease one replica to Redis instance in production environment. Initiate a manual failover by using the force-data-loss data protection mode.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInitiate a manual failover by using the limited-data-loss data protection mode to the Memorystore for Redis instance in the production environment."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T12:18:00.000Z",
        "voteCount": 11,
        "content": "The best option is B - Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode.\nThe key points are:\n\u2022\tThe failover should be tested in a separate development environment, not production, to avoid impacting real data.\n\u2022\tThe force-data-loss mode will simulate a full failover and restart, which is the most accurate test of disaster recovery.\n\u2022\tLimited-data-loss mode only fails over reads which does not fully test write capabilities.\n\u2022\tIncreasing replicas in production and failing over (C) risks losing real production data.\n\u2022\tFailing over production (D) also risks impacting real data and traffic.\nSo option B isolates the test from production and uses the most rigorous failover mode to fully validate disaster recovery capabilities."
      },
      {
        "date": "2024-08-30T22:41:00.000Z",
        "voteCount": 1,
        "content": "We are trying to simulate the disaster recovery on a redis Instance and we want minimum data loss.\nTherefore, Option A - create a test Standard Tier Memorystore for Redis instance in Dev Environment and use the limited data loss data protection mode, seems to be the correct option here."
      },
      {
        "date": "2024-07-09T11:13:00.000Z",
        "voteCount": 1,
        "content": "D seems correct. We are required to simulate and not test in a different environment.\n\"How data protection modes work\nThe limited-data-loss mode minimizes data loss by verifying that the difference in data between the primary and replica is below 30 MB before initiating the failover. The offset on the primary is incremented for each byte of data that must be synchronized to its replicas. In the limited-data-loss mode, the failover will abort if the greatest offset delta between the primary and each replica is 30MB or greater. If you can tolerate more data loss and want to aggressively execute the failover, try setting the data protection mode to force-data-loss.\n\nThe force-data-loss mode employs a chain of failover strategies to aggressively execute the failover. It does not check the offset delta between the primary and replicas before initiating the failover; you can potentially lose more than 30MB of data changes.\""
      },
      {
        "date": "2024-01-17T02:56:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/memorystore/docs/redis/about-manual-failover"
      },
      {
        "date": "2024-01-15T03:16:00.000Z",
        "voteCount": 1,
        "content": "B. Create a Standard Tier Memorystore for Redis instance in a development environment. Initiate a manual failover by using the force-data-loss data protection mode"
      },
      {
        "date": "2024-01-13T03:00:00.000Z",
        "voteCount": 1,
        "content": "Best option is B - no impact on production env and forces a full failover"
      },
      {
        "date": "2024-01-02T16:38:00.000Z",
        "voteCount": 1,
        "content": "Increasing the number of replicas in a Redis instance in a production environment means that we will have additional copies of the same data and thats why failover will not impact the production data"
      },
      {
        "date": "2024-01-07T12:19:00.000Z",
        "voteCount": 1,
        "content": "\"no impact on production data\" - not C nor D"
      },
      {
        "date": "2023-12-30T01:39:00.000Z",
        "voteCount": 1,
        "content": "Separate Development Environment:\n\nIsolates testing from production, preventing any impact on live data or services.\nProvides a safe and controlled environment for simulating failover scenarios."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 215,
    "url": "https://www.examtopics.com/discussions/google/view/129862-exam-professional-data-engineer-topic-1-question-215/",
    "body": "You are administering a BigQuery dataset that uses a customer-managed encryption key (CMEK). You need to share the dataset with a partner organization that does not have access to your CMEK. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the partner organization a copy of your CMEKs to decrypt the data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the tables to parquet files to a Cloud Storage bucket and grant the storageinsights.viewer role on the bucket to the partner organization.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the tables you need to share to a dataset without CMEKs. Create an Analytics Hub listing for this dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an authorized view that contains the CMEK to decrypt the data when accessed."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-02-17T05:06:00.000Z",
        "voteCount": 2,
        "content": "Analytics Hub"
      },
      {
        "date": "2024-01-03T16:24:00.000Z",
        "voteCount": 2,
        "content": "- Create a copy of the necessary tables into a new dataset that doesn't use CMEK, ensuring the data is accessible without requiring the partner to have access to the encryption key.\n- Analytics Hub can then be used to share this data securely and efficiently with the partner organization, maintaining control and governance over the shared data."
      },
      {
        "date": "2023-12-30T01:40:00.000Z",
        "voteCount": 2,
        "content": "Preserves Key Confidentiality:\n\nAvoids sharing your CMEK with the partner, upholding key security and control."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 216,
    "url": "https://www.examtopics.com/discussions/google/view/129863-exam-professional-data-engineer-topic-1-question-216/",
    "body": "You are developing an Apache Beam pipeline to extract data from a Cloud SQL instance by using JdbcIO. You have two projects running in Google Cloud. The pipeline will be deployed and executed on Dataflow in Project A. The Cloud SQL. instance is running in Project B and does not have a public IP address. After deploying the pipeline, you noticed that the pipeline failed to extract data from the Cloud SQL instance due to connection failure. You verified that VPC Service Controls and shared VPC are not in use in these projects. You want to resolve this error while ensuring that the data does not go through the public internet. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up VPC Network Peering between Project A and Project B. Add a firewall rule to allow the peered subnet range to access all instances on the network.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTurn off the external IP addresses on the Dataflow worker. Enable Cloud NAT in Project A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd the external IP addresses of the Dataflow worker as authorized networks in the Cloud SQL instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up VPC Network Peering between Project A and Project B. Create a Compute Engine instance without external IP address in Project B on the peered subnet to serve as a proxy server to the Cloud SQL database."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 16,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-09-14T15:30:00.000Z",
        "voteCount": 1,
        "content": "The answer is A. While D might work, it adds unnecessary complexity. Setting up a proxy is an extra layer of infrastructure that isn\u2019t required"
      },
      {
        "date": "2024-08-19T18:13:00.000Z",
        "voteCount": 1,
        "content": "It is a tie between A and D. \nOption A will definitely provide necessary connectivity but is less secure as access is enabled to \"all instances\". Which i feel is unnecessary considering industry best practices.\n\nOption D provides the necessary connectivity but brings in the unnecessary overhead of managing an extra VM and introduces a bit of extra complexity.\n\nSince the question emphasises on data not going through public internet(which is satisfied in both options), i would prioritise security over simplicity and choose option D in this case."
      },
      {
        "date": "2024-08-16T18:21:00.000Z",
        "voteCount": 1,
        "content": "If properly implemented with the right routing and firewall rules, Option A can be the correct and most straightforward solution, as it leverages VPC Peering to maintain internal traffic without going through the public internet."
      },
      {
        "date": "2024-08-10T08:43:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-07-11T13:48:00.000Z",
        "voteCount": 1,
        "content": "still confused between A and D"
      },
      {
        "date": "2024-07-05T17:45:00.000Z",
        "voteCount": 2,
        "content": "A is correct"
      },
      {
        "date": "2024-07-02T05:31:00.000Z",
        "voteCount": 1,
        "content": "no proxy needed"
      },
      {
        "date": "2024-06-08T01:14:00.000Z",
        "voteCount": 1,
        "content": "People referencing \"VPC Network Peering does not provide transitive routing. For example, if VPC networks net-a and net-b are connected using VPC Network Peering, and VPC networks net-a and net-c are also connected using VPC Network Peering, VPC Network Peering does not provide connectivity between net-b and net-c.\"\n\nthe question states that  cloud sql is running in project B. \nWhich means the instance is already part of the VPC in project B, so with Network Peering workers from A can definitely access data in B. No proxy is needed."
      },
      {
        "date": "2024-05-24T01:01:00.000Z",
        "voteCount": 2,
        "content": "Why so many people are voting for D? There's no need for a proxy, the peering is enough to allow network traffic between subnets."
      },
      {
        "date": "2024-05-24T01:14:00.000Z",
        "voteCount": 2,
        "content": "Now I see why, I put this on ChatGPT and it thinks the right answer is D. I'm pretty sure that's a hallucination."
      },
      {
        "date": "2024-05-21T03:38:00.000Z",
        "voteCount": 2,
        "content": "Proxy? no, it is not necessary..\n\nA"
      },
      {
        "date": "2024-05-18T12:12:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sql/docs/mysql/connect-multiple-vpcs"
      },
      {
        "date": "2024-02-26T12:05:00.000Z",
        "voteCount": 4,
        "content": "A - The requirement for a proxy is un-necessary: \n https://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity"
      },
      {
        "date": "2024-02-19T06:50:00.000Z",
        "voteCount": 2,
        "content": "Option D. Source: https://cloud.google.com/sql/docs/mysql/private-ip#multiple_vpc_connectivity"
      },
      {
        "date": "2024-02-17T05:07:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-25T11:04:00.000Z",
        "voteCount": 2,
        "content": "the reason : Cloud SQL supports private IP addresses through private service access. When you create a Cloud SQL instance, Cloud SQL creates the instance within its own virtual private cloud (VPC), called the Cloud SQL VPC. Enabling private IP requires setting up a peering connection between the Cloud SQL VPC and your VPC network."
      },
      {
        "date": "2024-01-16T03:17:00.000Z",
        "voteCount": 1,
        "content": "Using VPC Network Peering, Cloud SQL implements private service access internally, which allows internal IP addresses to connect across two VPC networks regardless of whether they belong to the same project or organization. \n\nHowever, since VPC Network Peering isn't transitive, it only broadcasts routes between the two VPCs that are directly peered. If you have an additional VPC, it won't be able to access your Cloud SQL resources using the connection set up with your original VPC."
      },
      {
        "date": "2024-01-15T03:23:00.000Z",
        "voteCount": 1,
        "content": "D. Set up VPC Network Peering between Project A and Project B. Create a Compute Engine instance without external IP address in Project B on the peered subnet to serve as a proxy server to the Cloud SQL database."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 217,
    "url": "https://www.examtopics.com/discussions/google/view/129864-exam-professional-data-engineer-topic-1-question-217/",
    "body": "You have a BigQuery table that contains customer data, including sensitive information such as names and addresses. You need to share the customer data with your data analytics and consumer support teams securely. The data analytics team needs to access the data of all the customers, but must not be able to access the sensitive data. The consumer support team needs access to all data columns, but must not be able to access customers that no longer have active contracts. You enforced these requirements by using an authorized dataset and policy tags. After implementing these steps, the data analytics team reports that they still have access to the sensitive columns. You need to ensure that the data analytics team does not have access to restricted data. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two separate authorized datasets; one for the data analytics team and another for the consumer support team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the data analytics team members do not have the Data Catalog Fine-Grained Reader role for the policy tags.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tReplace the authorized dataset with an authorized view. Use row-level security and apply filter_expression to limit data access.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRemove the bigquery.dataViewer role from the data analytics team on the authorized datasets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnforce access control in the policy tag taxonomy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "E",
    "answerDescription": "",
    "votes": [
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T22:52:00.000Z",
        "voteCount": 7,
        "content": "Option B &amp; E"
      },
      {
        "date": "2024-01-15T04:14:00.000Z",
        "voteCount": 5,
        "content": "B&amp; E\n https://cloud.google.com/bigquery/docs/column-level-security-intro"
      },
      {
        "date": "2024-07-05T17:48:00.000Z",
        "voteCount": 1,
        "content": "the correct options are:\n\nA. Create two separate authorized datasets; one for the data analytics team and another for the consumer support team.\nC. Replace the authorized dataset with an authorized view. Use row-level security and apply filter_expression to limit data access."
      },
      {
        "date": "2024-07-05T17:48:00.000Z",
        "voteCount": 1,
        "content": "Explanation of why other options are incorrect:\n\nB. Ensure that the data analytics team members do not have the Data Catalog Fine-Grained Reader role for the policy tags: This role relates to viewing data in Data Catalog based on policy tags, not directly controlling access to BigQuery data.\n\nD. Remove the bigquery.dataViewer role from the data analytics team on the authorized datasets: Removing this role would block all access to the dataset, which is too restrictive if they still need access to non-sensitive columns.\n\nE. Enforce access control in the policy tag taxonomy: While policy tags are used to enforce access controls, simply enforcing controls in the taxonomy does not directly address the issue of sensitive data access in BigQuery."
      },
      {
        "date": "2024-01-14T07:26:00.000Z",
        "voteCount": 3,
        "content": "B &amp; E\nB - It will ensure they don't have access to secure columns\nE- It will allow to enforce column level security\nRef - https://cloud.google.com/bigquery/docs/column-level-security-intro"
      },
      {
        "date": "2024-01-13T03:20:00.000Z",
        "voteCount": 2,
        "content": "Option B&amp; E to me"
      },
      {
        "date": "2024-01-07T23:10:00.000Z",
        "voteCount": 3,
        "content": "A &amp; B\nThe current setup is not effective because the data analytics team still has access to the sensitive columns despite using an authorized dataset and policy tags. This indicates that the policy tags are not being enforced properly, and the data analytics team members are able to view the tags and gain access to the sensitive data.\nSeparating the data into two distinct authorized datasets is a better approach because it isolates the sensitive data from the non-sensitive data. This prevents the data analytics team from accessing the sensitive columns directly, even if they have access to the authorized dataset for general customer data.\nAdditionally, revoking the Data Catalog Fine-Grained Reader role from the data analytics team members ensures that they cannot view or modify the policy tags. This limits their ability to bypass the access control imposed by the authorized dataset and policy tags."
      },
      {
        "date": "2024-01-13T03:19:00.000Z",
        "voteCount": 3,
        "content": "Max I feel like it's more B&amp;E. \nI do agree on the revoking Data Catalog Fine-grained reader role to avoid the data analytics team to read policy tags metadata, but if the tags are setup as stated, it's just missing the enforcement of the policy tags themselves.\nCreating 2 auth dataset is not efficient on big datasets and Data catalog+ policy tags are built to manage these situations. Don't you agree?"
      },
      {
        "date": "2024-01-04T06:05:00.000Z",
        "voteCount": 1,
        "content": "And the second answer? One is option B and the other is option D maybe?"
      },
      {
        "date": "2024-01-03T16:40:00.000Z",
        "voteCount": 3,
        "content": "- The Data Catalog Fine-Grained Reader role allows users to read metadata that is restricted by policy tags. \n- If members of the data analytics team have this role, they might bypass the restrictions set by policy tags. \n- Ensuring they do not have this role will help enforce the restrictions intended by the policy tags."
      },
      {
        "date": "2023-12-30T01:42:00.000Z",
        "voteCount": 2,
        "content": "Prevents data analytics team members from viewing sensitive data, even if it's tagged.\nRestricts access to policy tags themselves, ensuring confidentiality of sensitive information."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 218,
    "url": "https://www.examtopics.com/discussions/google/view/129865-exam-professional-data-engineer-topic-1-question-218/",
    "body": "You have a Cloud SQL for PostgreSQL instance in Region\u2019 with one read replica in Region2 and another read replica in Region3. An unexpected event in Region\u2019 requires that you perform disaster recovery by promoting a read replica in Region2. You need to ensure that your application has the same database capacity available before you switch over the connections. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable zonal high availability on the primary instance. Create a new read replica in a new region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a cascading read replica from the existing read replica in Region3.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two new read replicas from the new primary instance, one in Region3 and one in a new region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new read replica in Region1, promote the new read replica to be the primary instance, and enable zonal high availability."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-18T12:22:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/sql/docs/mysql/replication#cross-region-read-replicas"
      },
      {
        "date": "2024-08-25T09:41:00.000Z",
        "voteCount": 1,
        "content": "requires 2 new read replicas as the read replica that wasn't promoted isn't capable to be a replica any more as the primary isa gone"
      },
      {
        "date": "2024-04-07T14:44:00.000Z",
        "voteCount": 2,
        "content": "The best option here is C. Create two new read replicas from the new primary instance, one in Region3 and one in a new region.\n\nHere's the breakdown:\n\nCapacity Restoration: Promoting the Region2 replica makes it the new primary. You need to replicate from this new primary to maintain redundancy and capacity. Creating two replicas (Region3, new region) accomplishes this.\nGeographic Distribution: Distributing replicas across regions ensures availability if another regional event occurs.\nSpeed: Creating new replicas from the promoted primary is likely faster than promoting another existing replica."
      },
      {
        "date": "2024-04-11T23:03:00.000Z",
        "voteCount": 1,
        "content": "Who said that i can use a 4\u00b0 region? If have constraint that i can't go out from that 3 regions?\nBy My opinion will be a right solution if the new replica will in another zona of the region 1 or 3.\nmay be the best solution is the case D"
      },
      {
        "date": "2024-04-11T23:11:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/postgres/replication/cross-region-replicas?hl=en"
      },
      {
        "date": "2024-02-17T05:08:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-03T16:50:00.000Z",
        "voteCount": 4,
        "content": "After promoting the read replica in Region2 to be the new primary instance, creating additional read replicas from it can help distribute the read load and maintain or increase the database's total capacity."
      },
      {
        "date": "2023-12-30T01:43:00.000Z",
        "voteCount": 2,
        "content": "Immediate Failover:\n\nPromoting the read replica in Region2 quickly restores database operations in a different region, aligning with disaster recovery goals.\nCapacity Restoration:\n\nCreates two new read replicas from the promoted primary instance (formerly the read replica in Region2).\nThis replaces the lost capacity in Region1 and adds a read replica in a new region for further redundancy."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 219,
    "url": "https://www.examtopics.com/discussions/google/view/129866-exam-professional-data-engineer-topic-1-question-219/",
    "body": "You orchestrate ETL pipelines by using Cloud Composer. One of the tasks in the Apache Airflow directed acyclic graph (DAG) relies on a third-party service. You want to be notified when the task does not succeed. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a function with notification logic to the on_retry_callback parameter for the operator responsible for the task at risk.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure a Cloud Monitoring alert on the sla_missed metric associated with the task at risk to trigger a notification.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a function with notification logic to the on_failure_callback parameter tor the operator responsible for the task at risk.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign a function with notification logic to the sla_miss_callback parameter for the operator responsible for the task at risk."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-15T21:25:00.000Z",
        "voteCount": 1,
        "content": "What is Task is long-running and in between stuck?"
      },
      {
        "date": "2024-05-24T07:19:00.000Z",
        "voteCount": 2,
        "content": "https://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/callbacks.html"
      },
      {
        "date": "2024-02-24T08:53:00.000Z",
        "voteCount": 2,
        "content": "on_failure_callback"
      },
      {
        "date": "2024-01-15T04:20:00.000Z",
        "voteCount": 3,
        "content": "on_failure_callback is invoked when the task fails\nhttps://airflow.apache.org/docs/apache-airflow/stable/administration-and-deployment/logging-monitoring/callbacks.html"
      },
      {
        "date": "2024-01-13T03:27:00.000Z",
        "voteCount": 1,
        "content": "Option C to me"
      },
      {
        "date": "2024-01-04T04:47:00.000Z",
        "voteCount": 4,
        "content": "- The on_failure_callback is a function that gets called when a task fails. \n- Assigning a function with notification logic to this parameter is a direct way to handle task failures. \n- When the task fails, this function can trigger a notification, making it an appropriate solution for the need to be alerted on task failures."
      },
      {
        "date": "2023-12-30T01:44:00.000Z",
        "voteCount": 4,
        "content": "Direct Trigger:\n\nThe on_failure_callback parameter is specifically designed to invoke a function when a task fails, ensuring immediate notification.\nCustomizable Logic:\n\nYou can tailor the notification function to send emails, create alerts, or integrate with other notification systems, providing flexibility."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 220,
    "url": "https://www.examtopics.com/discussions/google/view/129867-exam-professional-data-engineer-topic-1-question-220/",
    "body": "You are migrating your on-premises data warehouse to BigQuery. One of the upstream data sources resides on a MySQL. database that runs in your on-premises data center with no public IP addresses. You want to ensure that the data ingestion into BigQuery is done securely and does not go through the public internet. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate your existing on-premises ETL tool to write to BigQuery by using the BigQuery Open Database Connectivity (ODBC) driver. Set up the proxy parameter in the simba.googlebigqueryodbc.ini file to point to your data center\u2019s NAT gateway.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Datastream to replicate data from your on-premises MySQL database to BigQuery. Set up Cloud Interconnect between your on-premises data center and Google Cloud. Use Private connectivity as the connectivity method and allocate an IP address range within your VPC network to the Datastream connectivity configuration. Use Server-only as the encryption type when setting up the connection profile in Datastream.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Datastream to replicate data from your on-premises MySQL database to BigQuery. Use Forward-SSH tunnel as the connectivity method to establish a secure tunnel between Datastream and your on-premises MySQL database through a tunnel server in your on-premises data center. Use None as the encryption type when setting up the connection profile in Datastream.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Datastream to replicate data from your on-premises MySQL database to BigQuery. Gather Datastream public IP addresses of the Google Cloud region that will be used to set up the stream. Add those IP addresses to the firewall allowlist of your on-premises data center. Use IP Allowlisting as the connectivity method and Server-only as the encryption type when setting up the connection profile in Datastream."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T04:50:00.000Z",
        "voteCount": 8,
        "content": "- Datastream is a serverless change data capture and replication service, which can be used to replicate data changes from MySQL to BigQuery. \n- Using Cloud Interconnect provides a private, secure connection between your on-premises environment and Google Cloud ==&gt; This method ensures that data doesn't go through the public internet and is a recommended approach for secure, large-scale data migrations. \n- Setting up private connectivity with Datastream allows for secure and direct data transfer."
      },
      {
        "date": "2024-05-18T12:29:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/datastream/docs/network-connectivity-options"
      },
      {
        "date": "2024-01-15T04:33:00.000Z",
        "voteCount": 1,
        "content": "Datastream is a seamless replication from relational databases directly to BigQuery. The source database can be hosted on-premises, on Google Cloud services such as Cloud SQL or Bare Metal Solution for Oracle, or anywhere else on any cloud.\nhttps://cloud.google.com/datastream-for-bigquery#benefits"
      },
      {
        "date": "2024-01-15T04:35:00.000Z",
        "voteCount": 1,
        "content": "It is required that the data ingestion into BigQuery is done securely and does not go through the public internet. It can be done by Interconnect."
      },
      {
        "date": "2023-12-30T01:46:00.000Z",
        "voteCount": 2,
        "content": "Secure Private Connection:\n\nCloud Interconnect establishes a direct, private connection between your on-premises network and Google Cloud, bypassing the public internet and ensuring data confidentiality.\nDatastream Integration:\n\nDatastream seamlessly replicates data from your MySQL database to BigQuery, handling the complexities of data transfer and synchronization."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 221,
    "url": "https://www.examtopics.com/discussions/google/view/129868-exam-professional-data-engineer-topic-1-question-221/",
    "body": "You store and analyze your relational data in BigQuery on Google Cloud with all data that resides in US regions. You also have a variety of object stores across Microsoft Azure and Amazon Web Services (AWS), also in US regions. You want to query all your data in BigQuery daily with as little movement of data as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service to load files from Azure and AWS into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataflow pipeline to ingest files from Azure and AWS to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad files from AWS and Azure to Cloud Storage with Cloud Shell gsutil rsync arguments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Omni functionality and BigLake tables to query files in Azure and AWS.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T01:47:00.000Z",
        "voteCount": 6,
        "content": "Direct Querying:\n\nBigQuery Omni allows you to query data in Azure and AWS object stores directly without physically moving it to BigQuery, reducing data transfer costs and delays.\nBigLake Tables:\n\nProvide a unified view of both BigQuery tables and external object storage files, enabling seamless querying across multi-cloud data."
      },
      {
        "date": "2024-01-04T04:59:00.000Z",
        "voteCount": 6,
        "content": "- BigQuery Omni allows us to analyze data stored across Google Cloud, AWS, and Azure directly from BigQuery without having to move or copy the data. \n- It extends BigQuery's data analysis capabilities to other clouds, enabling cross-cloud analytics."
      },
      {
        "date": "2024-05-18T12:32:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/data-analytics/introducing-bigquery-omni\nhttps://cloud.google.com/bigquery/docs/omni-introduction"
      },
      {
        "date": "2024-02-26T09:07:00.000Z",
        "voteCount": 2,
        "content": "Option A, B, and C all involve moving data, which is described as something that shouldn't happen."
      },
      {
        "date": "2024-02-17T05:04:00.000Z",
        "voteCount": 2,
        "content": "BigQuery Omni"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 222,
    "url": "https://www.examtopics.com/discussions/google/view/129869-exam-professional-data-engineer-topic-1-question-222/",
    "body": "You have a variety of files in Cloud Storage that your data science team wants to use in their models. Currently, users do not have a method to explore, cleanse, and validate the data in Cloud Storage. You are looking for a low code solution that can be used by your data science team to quickly cleanse and explore data within Cloud Storage. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the data science team access to Dataflow to create a pipeline to prepare and validate the raw data and load data into BigQuery for data exploration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an external table in BigQuery and use SQL to transform the data as necessary. Provide the data science team access to the external tables to explore the raw data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLoad the data into BigQuery and use SQL to transform the data as necessary. Provide the data science team access to staging tables to explore the raw data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tProvide the data science team access to Dataprep to prepare, validate, and explore the data within Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T05:02:00.000Z",
        "voteCount": 12,
        "content": "- Dataprep is a serverless, no-code data preparation tool that allows users to visually explore, cleanse, and prepare data for analysis. \n- It's designed for business analysts, data scientists, and others who want to work with data without writing code. \n- Dataprep can directly access and transform data in Cloud Storage, making it a suitable choice for a team that prefers a low-code, user-friendly solution."
      },
      {
        "date": "2024-02-17T05:13:00.000Z",
        "voteCount": 1,
        "content": "Dataprep"
      },
      {
        "date": "2024-01-16T23:43:00.000Z",
        "voteCount": 1,
        "content": "Goes without say"
      },
      {
        "date": "2024-01-13T03:39:00.000Z",
        "voteCount": 1,
        "content": "Option D - Low code and efficient way to explore and prep data"
      },
      {
        "date": "2024-01-07T07:09:00.000Z",
        "voteCount": 1,
        "content": "why you message wrong answers\ncorrect is C"
      },
      {
        "date": "2024-01-13T03:40:00.000Z",
        "voteCount": 2,
        "content": "The \"Reveal Answer\" button contains 90% of the time an incorrect answer. You should always check the community and the discussion during studying :)"
      },
      {
        "date": "2023-12-30T01:48:00.000Z",
        "voteCount": 4,
        "content": "Low-Code Interface:\n\nOffers a visual, drag-and-drop interface that empowers users with varying technical skills to cleanse and explore data without extensive coding, aligning with the low-code requirement.\nData Cleaning and Validation:\n\nProvides built-in tools for data profiling, cleaning, transformation, and validation, ensuring data quality and accuracy before model training.\nDirect Cloud Storage Access:\n\nConnects directly to Cloud Storage, allowing users to work with data in place without additional data movement or storage costs, optimizing efficiency."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 223,
    "url": "https://www.examtopics.com/discussions/google/view/129870-exam-professional-data-engineer-topic-1-question-223/",
    "body": "You are building an ELT solution in BigQuery by using Dataform. You need to perform uniqueness and null value checks on your final tables. What should you do to efficiently integrate these checks into your pipeline?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild BigQuery user-defined functions (UDFs).",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate Dataplex data quality tasks.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild Dataform assertions into your code.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a Spark-based stored procedure."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T05:04:00.000Z",
        "voteCount": 6,
        "content": "- Dataform provides a feature called \"assertions,\" which are essentially SQL-based tests that you can define to verify the quality of your data. \n- Assertions in Dataform are a built-in way to perform data quality checks, including checking for uniqueness and null values in your tables."
      },
      {
        "date": "2024-02-17T05:14:00.000Z",
        "voteCount": 2,
        "content": "https://docs.dataform.co/guides/assertions"
      },
      {
        "date": "2024-01-18T03:43:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/dataform/docs/assertions"
      },
      {
        "date": "2024-01-07T07:15:00.000Z",
        "voteCount": 1,
        "content": "Agree with C"
      },
      {
        "date": "2024-01-07T07:14:00.000Z",
        "voteCount": 1,
        "content": "agree with C"
      },
      {
        "date": "2023-12-30T01:49:00.000Z",
        "voteCount": 3,
        "content": "Native Integration:\n\nDataform assertions are designed specifically for data quality checks within Dataform pipelines, ensuring seamless integration and compatibility.\nThey leverage Dataform's execution model and configuration, aligning with the existing workflow.\nDeclarative Syntax:\n\nAssertions are defined using a simple, declarative syntax within Dataform code, making them easy to write and understand, even for users with less SQL expertise."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 224,
    "url": "https://www.examtopics.com/discussions/google/view/129871-exam-professional-data-engineer-topic-1-question-224/",
    "body": "A web server sends click events to a Pub/Sub topic as messages. The web server includes an eventTimestamp attribute in the messages, which is the time when the click occurred. You have a Dataflow streaming job that reads from this Pub/Sub topic through a subscription, applies some transformations, and writes the result to another Pub/Sub topic for use by the advertising department. The advertising department needs to receive each message within 30 seconds of the corresponding click occurrence, but they report receiving the messages late. Your Dataflow job's system lag is about 5 seconds, and the data freshness is about 40 seconds. Inspecting a few messages show no more than 1 second lag between their eventTimestamp and publishTime. What is the problem and what should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe advertising department is causing delays when consuming the messages. Work with the advertising department to fix this.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMessages in your Dataflow job are taking more than 30 seconds to process. Optimize your job or increase the number of workers to fix this.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"G\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tG.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMessages in your Dataflow job are processed in less than 30 seconds, but your job cannot keep up with the backlog in the Pub/Sub subscription. Optimize your job or increase the number of workers to fix this.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tThe web server is not pushing messages fast enough to Pub/Sub. Work with the web server team to fix this."
    ],
    "answer": "G",
    "answerDescription": "",
    "votes": [
      {
        "answer": "G",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2023-12-30T01:50:00.000Z",
        "voteCount": 9,
        "content": "System Lag vs. Data Freshness: System lag is low (5 seconds), indicating that individual messages are processed quickly. However, data freshness is high (40 seconds), suggesting a backlog in the pipeline.\nNot Advertising's Fault: The issue is upstream of their consumption, as they're already receiving delayed messages.\nNot Web Server's Fault: The lag between eventTimestamp and publishTime is minimal (1 second), meaning the server is publishing messages promptly."
      },
      {
        "date": "2024-01-04T08:12:00.000Z",
        "voteCount": 5,
        "content": "- It suggest a backlog problem. \n- It indicates that while individual messages might be processed quickly once they're handled, the job overall cannot keep up with the rate of incoming messages, causing a delay in processing the backlog."
      },
      {
        "date": "2024-01-15T04:51:00.000Z",
        "voteCount": 2,
        "content": "Why not B than?"
      },
      {
        "date": "2024-02-12T13:51:00.000Z",
        "voteCount": 2,
        "content": "I guess that's because it says in the text that \"Your Dataflow job's system lag is about 5 seconds\"."
      },
      {
        "date": "2024-09-08T19:57:00.000Z",
        "voteCount": 1,
        "content": "I don't know why you guys got the processing time is less than 30 sec. But I would consider the processing time with 40(freshness) - 5(system lag) = 35 sec. Even minus the publish time of Pub/sub which is less than 1 sec. The processing time still larger than 30 sec. I believe inspecting a few messages show no more than 1 sec lag is about pub/sub processing time. Not inspecting a few messages for dataflow. So I would choose B."
      },
      {
        "date": "2024-02-17T05:15:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T03:46:00.000Z",
        "voteCount": 3,
        "content": "Option C - low system lag (which identifies fast processing) but high data freshness (which identifies that the messages sit in the backlog a lot)"
      },
      {
        "date": "2024-01-07T07:17:00.000Z",
        "voteCount": 1,
        "content": "agree correct is C"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 225,
    "url": "https://www.examtopics.com/discussions/google/view/129872-exam-professional-data-engineer-topic-1-question-225/",
    "body": "Your organization stores customer data in an on-premises Apache Hadoop cluster in Apache Parquet format. Data is processed on a daily basis by Apache Spark jobs that run on the cluster. You are migrating the Spark jobs and Parquet data to Google Cloud. BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery. You want to use managed services, while minimizing ETL data processing changes and overhead costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data to Cloud Storage and migrate the metadata to Dataproc Metastore (DPMS). Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data to Cloud Storage and register the bucket as a Dataplex asset. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc Serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data to BigQuery. Refactor Spark pipelines to write and read data on BigQuery, and run them on Dataproc Serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data to BigLake. Refactor Spark pipelines to write and read data on Cloud Storage, and run them on Dataproc on Compute Engine."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 10,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-13T04:59:00.000Z",
        "voteCount": 1,
        "content": "A is correct"
      },
      {
        "date": "2024-06-06T03:55:00.000Z",
        "voteCount": 3,
        "content": "Option B: Registering the bucket as a Dataplex asset adds an additional layer of data governance and management. While useful, it may not be necessary for your immediate migration needs and can introduce additional complexity.\nOption C: Migrating data directly to BigQuery would require significant changes to your Spark pipelines since they would need to be refactored to read from and write to BigQuery instead of Parquet files. This approach could introduce higher costs due to BigQuery storage and querying.\nOption D: Using BigLake and Dataproc on Compute Engine is more complex and requires more management compared to Dataproc Serverless. Additionally, it might not be as cost-effective as leveraging Cloud Storage and Dataproc Serverless."
      },
      {
        "date": "2024-08-20T06:09:00.000Z",
        "voteCount": 2,
        "content": "Just adding further commentary on why A is correct while why other options are incorrect is explained above.\nParquet files have schema engrained in them. Hence Spark pipelines on Hadoop Cluster may not have needed tables at all. Hence the simplest solution would be to move it to Cloud Storage instead of BigQuery and this way there would be minimal changes to the ETL pipelines - just change HDFS file system pointer to GCS file system for read writes and no need for any additional tables"
      },
      {
        "date": "2024-05-18T13:58:00.000Z",
        "voteCount": 1,
        "content": "The question says \"You want to use managed services, while minimizing ETL data processing changes and overhead costs\". Dataproc is a managed service that doesn't need to refactor the data transformation Spark code you already have (you will have to refactor only the wrtie and read code), an it has a Big Query connector for future use. https://cloud.google.com/dataproc/docs/concepts/connectors/bigquery"
      },
      {
        "date": "2024-03-07T02:22:00.000Z",
        "voteCount": 3,
        "content": "Migrate your data directly to BigQuery.\nRefactor Spark pipelines to read from and write to BigQuery.\nRun the Spark jobs on Dataproc Serverless.\nThe best choice for ensuring data availability in BigQuery.  It allows seamless integration with BigQuery and minimizes ETL changes."
      },
      {
        "date": "2024-02-26T09:21:00.000Z",
        "voteCount": 3,
        "content": "A tricky one, because of \"you need to ensure that your data is available in BigQuery\". The easiest and most straight forward migration seems answer A to me, and then you can use external tables to make the parquet data directly available in BigQuery.\nhttps://cloud.google.com/bigquery/docs/loading-data-cloud-storage-parquet\n\nHowever creating the external tables is an extra step? So therefore maybe C is the answer?"
      },
      {
        "date": "2024-02-25T19:11:00.000Z",
        "voteCount": 1,
        "content": "I think the key phrase here is \"you need to ensure that your data is available in BigQuery\" that's why I thing C it's the best option"
      },
      {
        "date": "2024-02-17T08:31:00.000Z",
        "voteCount": 3,
        "content": "I think it's C.\n\nDataproc can use BigQuery to read and write data. \nDataproc's BigQuery connector is a library that allows Spark and Hadoop applications to process and write data from BigQuery. \n\nHere's how Dataproc can be used with BigQuery:\nProcess large datasets: Use Spark to process data stored in BigQuery.\nWrite results: Write the results back to BigQuery or other data storage for further analysis.\nRead data: The BigQuery connector can read data from BigQuery into a Spark DataFrame.\nWrite data: The connector writes data to BigQuery by buffering all the data into a Cloud Storage temporary table."
      },
      {
        "date": "2024-02-17T08:33:00.000Z",
        "voteCount": 3,
        "content": "As per question.. \"BigQuery will be used on future transformation pipelines so you need to ensure that your data is available in BigQuery. You want to use managed services (DATAPROC), while minimizing ETL data processing changes and overhead costs.\""
      },
      {
        "date": "2024-02-14T10:03:00.000Z",
        "voteCount": 3,
        "content": "I think its B and the reason is that egistering the data as a Dataplex asset enables seamless integration with BigQuery later on. Dataplex simplifies data discovery and lineage tracking, making it easier to prepare your data for BigQuery transformations."
      },
      {
        "date": "2024-02-08T04:02:00.000Z",
        "voteCount": 3,
        "content": "Why would I select A here? Why not moving the data to BigQuery and running Dataproc Serverless jobs accessing the data in BigQuery?"
      },
      {
        "date": "2024-01-04T08:19:00.000Z",
        "voteCount": 4,
        "content": "- This option involves moving Parquet files to Cloud Storage, which is a common and cost-effective storage solution for big data and is compatible with Spark jobs. \n- Using Dataproc Metastore to manage metadata allows us to keep Hadoop ecosystem's structural information. \n- Running Spark jobs on Dataproc Serverless takes advantage of managed Spark services without managing clusters. \n- Once the data is in Cloud Storage, you can also easily load it into BigQuery for further analysis."
      },
      {
        "date": "2023-12-30T01:51:00.000Z",
        "voteCount": 2,
        "content": "Managed Services: Leverages Dataproc Serverless for a fully managed Spark environment, reducing overhead and administrative tasks.\nMinimal Data Processing Changes: Keeps Spark pipelines largely intact by working with Parquet files on Cloud Storage, minimizing refactoring efforts.\nBigQuery Integration: Dataproc Serverless can directly access BigQuery, enabling future transformation pipelines without additional data movement.\nCost-Effective: Serverless model scales resources only when needed, optimizing costs for intermittent workloads."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 226,
    "url": "https://www.examtopics.com/discussions/google/view/129873-exam-professional-data-engineer-topic-1-question-226/",
    "body": "Your organization has two Google Cloud projects, project A and project B. In project A, you have a Pub/Sub topic that receives data from confidential sources. Only the resources in project A should be able to access the data in that topic. You want to ensure that project B and any future project cannot access data in the project A topic. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd firewall rules in project A so only traffic from the VPC in project A is permitted.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC Service Controls in the organization with a perimeter around project A.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Identity and Access Management conditions to ensure that only users and service accounts in project A. can access resources in project A.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tConfigure VPC Service Controls in the organization with a perimeter around the VPC of project A."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 12,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-15T12:36:00.000Z",
        "voteCount": 10,
        "content": "And I would agree with GPT. The question is about that who can do what within GCP environment. It's all about permissions and access management, not about networking."
      },
      {
        "date": "2024-08-13T03:59:00.000Z",
        "voteCount": 1,
        "content": "The best solution to prevent project B and any future projects from accessing data in project A&amp;#x27;s Pub/Sub topic is B. Configure VPC Service Controls in the organization with a perimeter around project A."
      },
      {
        "date": "2024-08-10T07:56:00.000Z",
        "voteCount": 1,
        "content": "B is correct Raaad is always right"
      },
      {
        "date": "2024-05-24T02:17:00.000Z",
        "voteCount": 2,
        "content": "Setting up a perimeter around project A is future proof, the question asks to \"ensure that project B and any future project cannot access data in the project A topic\", IAM is not future proof.\n\nReference: https://cloud.google.com/vpc-service-controls/docs/overview#isolate\n\np.s: VPC Service Controls is not the same thing as VPC, instead its a security layer on top of a VPC and it should be used together with IAM, not one or the other (https://cloud.google.com/vpc-service-controls/docs/overview#how-vpc-service-controls-works)"
      },
      {
        "date": "2024-05-20T04:19:00.000Z",
        "voteCount": 2,
        "content": "C. Use Identity and Access Management conditions to ensure that only users and service accounts in project A. can access resources in project A. [SIMPLE!!!]"
      },
      {
        "date": "2024-02-17T09:24:00.000Z",
        "voteCount": 1,
        "content": "I'll go with \"B. Configure VPC Service Controls in the organization with a perimeter around project A.\""
      },
      {
        "date": "2024-01-15T12:21:00.000Z",
        "voteCount": 1,
        "content": "GPT:\nC. Use Identity and Access Management conditions to ensure that only users and service accounts in project A can access resources in project A.\n\nAnalysis: This is the most appropriate option. IAM allows you to define who (which users or service accounts) has what access to your GCP resources. By setting IAM policies with conditions specific to Project A, you can ensure that only designated entities within Project A have access to its resources, including the Pub/Sub topic.\nD. Configure VPC Service Controls in the organization with a perimeter around the VPC of project A."
      },
      {
        "date": "2024-01-15T12:22:00.000Z",
        "voteCount": 1,
        "content": "A. Add firewall rules in project A so only traffic from the VPC in project A is permitted.\n\nAnalysis: Firewall rules in GCP are used to control traffic to and from instances within Google Cloud Virtual Private Clouds (VPCs). However, they don't specifically control access to Pub/Sub resources. Pub/Sub access is managed through IAM, not VPC firewall rules."
      },
      {
        "date": "2024-01-15T12:23:00.000Z",
        "voteCount": 1,
        "content": "B. Configure VPC Service Controls in the organization with a perimeter around project A.\n\nAnalysis: VPC Service Controls provide a security perimeter for your data, but they are more focused on preventing data exfiltration; this might be more complex and broader than necessary for the specific requirement of restricting access to a Pub/Sub topic."
      },
      {
        "date": "2024-01-16T06:42:00.000Z",
        "voteCount": 1,
        "content": "D. Configure VPC Service Controls in the organization with a perimeter around the VPC of project A.\n\nAnalysis: Similar to option B, this is focused on securing network boundaries rather than specific resource access within GCP. While it could provide an additional layer of security, it's not the most direct way to control access to a specific Pub/Sub topic."
      },
      {
        "date": "2024-01-04T08:40:00.000Z",
        "voteCount": 4,
        "content": "Option B:\n-It allows us to create a secure boundary around all resources in Project A, including the Pub/Sub topic. \n- It prevents data exfiltration to other projects and ensures that only resources within the perimeter (Project A) can access the sensitive data. \n- VPC Service Controls are specifically designed for scenarios where you need to secure sensitive data within a specific context or boundary in Google Cloud."
      },
      {
        "date": "2023-12-30T01:52:00.000Z",
        "voteCount": 3,
        "content": "VPC Service Controls enforce a security perimeter around entire projects, ensuring that resources within project A (including the Pub/Sub topic) are inaccessible from any other project, including project B and future projects.\nThis aligns with the requirement to prevent cross-project access."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 227,
    "url": "https://www.examtopics.com/discussions/google/view/129874-exam-professional-data-engineer-topic-1-question-227/",
    "body": "You stream order data by using a Dataflow pipeline, and write the aggregated result to Memorystore. You provisioned a Memorystore for Redis instance with Basic Tier, 4 GB capacity, which is used by 40 clients for read-only access. You are expecting the number of read-only clients to increase significantly to a few hundred and you need to be able to support the demand. You want to ensure that read and write access availability is not impacted, and any changes you make can be deployed quickly. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Memorystore for Redis instance with Standard Tier. Set capacity to 4 GB and read replica to No read replicas (high availability only). Delete the old instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Memorystore for Redis instance with Standard Tier. Set capacity to 5 GB and create multiple read replicas. Delete the old instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Memorystore for Memcached instance. Set a minimum of three nodes, and memory per node to 4 GB. Modify the Dataflow pipeline and all clients to use the Memcached instance. Delete the old instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate multiple new Memorystore for Redis instances with Basic Tier (4 GB capacity). Modify the Dataflow pipeline and new clients to use all instances."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T08:43:00.000Z",
        "voteCount": 7,
        "content": "- Upgrading to the Standard Tier and adding read replicas is an effective way to scale and manage increased read load. \n- The additional capacity (5 GB) provides more space for data, and read replicas help distribute the read load across multiple instances."
      },
      {
        "date": "2024-01-15T12:44:00.000Z",
        "voteCount": 3,
        "content": "Descrived here:\nhttps://cloud.google.com/memorystore/docs/redis/redis-tiers"
      },
      {
        "date": "2023-12-30T01:53:00.000Z",
        "voteCount": 3,
        "content": "Scalability for Read-Only Clients: Read replicas distribute read traffic across multiple instances, significantly enhancing read capacity to support a large number of clients without impacting write performance.\nHigh Availability: Standard Tier ensures high availability with automatic failover, minimizing downtime in case of instance failure.\nMinimal Code Changes: Redis clients can seamlessly connect to read replicas without requiring extensive code modifications, enabling a quick deployment."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 228,
    "url": "https://www.examtopics.com/discussions/google/view/129875-exam-professional-data-engineer-topic-1-question-228/",
    "body": "You have a streaming pipeline that ingests data from Pub/Sub in production. You need to update this streaming pipeline with improved business logic. You need to ensure that the updated pipeline reprocesses the previous two days of delivered Pub/Sub messages. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Pub/Sub subscription clear-retry-policy flag",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Pub/Sub Snapshot capture two days before the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Pub/Sub subscription two days before the deployment.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Pub/Sub subscription retain-acked-messages flag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Pub/Sub Seek with a timestamp."
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 24,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": false
      },
      {
        "answer": "E",
        "count": 8,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-18T08:46:00.000Z",
        "voteCount": 9,
        "content": "B and E, already tested at cloud console."
      },
      {
        "date": "2024-01-18T07:51:00.000Z",
        "voteCount": 9,
        "content": "DE\n\nAnother way to replay messages that have been acknowledged is to seek to a timestamp. To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days.\n\nYou only need to do this step if you intend to seek to a timestamp, not to a snapshot.\n\nhttps://cloud.google.com/pubsub/docs/replay-message"
      },
      {
        "date": "2024-08-26T00:17:00.000Z",
        "voteCount": 1,
        "content": "this is correct as there are 2 options (timestamp and snapshot) and foreach there are 2 stages.\nSnapshot - create ('B') and seek\nTimestamp - configure 'retain' ('D') and seek ('E')\nas shown 'B' is missing the 'seek' operation"
      },
      {
        "date": "2024-04-18T05:30:00.000Z",
        "voteCount": 2,
        "content": "Its BE.\n\nBy the way, you can seek to a snapshot yes:\n\"Seeks an existing subscription to a point in time or to a given snapshot, whichever is provided in the request\"\n\nLink:https://cloud.google.com/pubsub/docs/reference/rest/v1/projects.subscriptions/seek"
      },
      {
        "date": "2024-09-24T15:54:00.000Z",
        "voteCount": 1,
        "content": "First, read this document: https://cloud.google.com/pubsub/docs/replay-overview.\n\nKey Points:\n\nSeek to a Snapshot: Reprocesses only unacknowledged messages.\nSeek to a Timestamp: Reprocesses all messages (acknowledged and unacknowledged) after that time.\nSince the question asks for delivering all messages, option E is correct, as it includes both acknowledged and unacknowledged messages.\n\nRegarding Option D: Configuring a subscription with the retain_acked_messages property allows replaying previously acknowledged messages retained for up to 31 days. This satisfies the requirement to deliver all messages and retains them longer than the mentioned 2 days."
      },
      {
        "date": "2024-08-15T16:18:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D - based on Vertex AI feedback"
      },
      {
        "date": "2024-06-06T08:45:00.000Z",
        "voteCount": 2,
        "content": "BE\nB. Use Pub/Sub Snapshot capture two days before the deployment.\n\nPub/Sub Snapshot: Creating a snapshot captures the state of the subscription at a specific point in time. You can then seek to this snapshot to replay messages from that point onwards.\nBy capturing a snapshot two days before the deployment, you can ensure that your pipeline reprocesses messages from the past two days.\nE. Use Pub/Sub Seek with a timestamp.\n\nPub/Sub Seek: This feature allows you to reset the subscription to a specific timestamp. Messages published to the topic after this timestamp are re-delivered.\nBy seeking to the timestamp from two days ago, you can instruct Pub/Sub to start re-delivering messages from that point in time"
      },
      {
        "date": "2024-05-20T05:27:00.000Z",
        "voteCount": 2,
        "content": "D. Use the Pub/Sub subscription retain-acked-messages flag.\nE. Use Pub/Sub Seek with a timestamp."
      },
      {
        "date": "2024-02-26T13:32:00.000Z",
        "voteCount": 3,
        "content": "E for sure, you need to seek from a timestamp.\nTo accomplish to that you need to \"Set the retain-acked-messages flag to true for the subscription.\"\n\nFrom google documentation: \n\n\"Note: To seek to a previous time point, your subscription must be configured to retain acknowledged messages. You can change this setting by clicking Edit on the subscription details page, and checking the box for Retain acknowledged messages.\"\n\nhttps://cloud.google.com/pubsub/docs/replay-message"
      },
      {
        "date": "2024-02-25T20:44:00.000Z",
        "voteCount": 3,
        "content": "D and E,\nhttps://cloud.google.com/pubsub/docs/replay-message"
      },
      {
        "date": "2024-02-16T05:47:00.000Z",
        "voteCount": 3,
        "content": "B and E: The seek feature extends subscriber capabilities by allowing you to alter the acknowledgement state of messages in bulk. For example, you can replay previously acknowledged messages or purge messages in bulk. In addition, you can copy the acknowledgement state of one subscription to another by using seek in combination with a snapshot. Source: https://cloud.google.com/pubsub/docs/replay-overview"
      },
      {
        "date": "2024-01-15T05:52:00.000Z",
        "voteCount": 3,
        "content": "BE\nhttps://cloud.google.com/pubsub/docs/replay-overview"
      },
      {
        "date": "2024-01-18T07:56:00.000Z",
        "voteCount": 1,
        "content": "But There is a problem snapshot you shoudl seek by subscriptions not by timestamp"
      },
      {
        "date": "2024-01-13T04:08:00.000Z",
        "voteCount": 3,
        "content": "Option D and E"
      },
      {
        "date": "2024-01-10T06:21:00.000Z",
        "voteCount": 6,
        "content": "DE\nSet the retain-acked-messages flag to true for the subscription.\nThis instructs Pub/Sub to store acknowledged messages for a specified retention period.\n\n\nE Use Pub/Sub Seek with a timestamp.\nAfter deploying the updated pipeline, use the Seek feature to replay messages.\nSpecify a timestamp that's two days before the current time.\nThis rewinds the subscription's message cursor, making it redeliver messages from that point onward."
      },
      {
        "date": "2024-01-04T08:52:00.000Z",
        "voteCount": 6,
        "content": "- Pub/Sub Snapshots allow you to capture the state of a subscription's unacknowledged messages at a particular point in time. \n- By creating a snapshot two days before deploying the updated pipeline, you can later use this snapshot to replay the messages from that point. \n=============\nOption E:\n- Pub/Sub Seek allows us to alter the acknowledgment state of messages in bulk. \n- So we can rewind a subscription to a point in time or a snapshot. \n- Using Seek with a timestamp corresponding to two days ago would allow the updated pipeline to reprocess messages from that time."
      },
      {
        "date": "2024-01-15T12:56:00.000Z",
        "voteCount": 3,
        "content": "Creating a snapshot of the Pub/Sub subscription two days before the deployment captures the state of unacknowledged messages at that particular point in time, which would include messages from before those two days. If our objective is to reprocess the data from the last two days specifically, then capturing a snapshot two days prior wouldn't directly address this need."
      },
      {
        "date": "2024-01-15T13:04:00.000Z",
        "voteCount": 5,
        "content": "This case is described here. \nhttps://cloud.google.com/pubsub/docs/replay-message\nAnd according to this D &amp;E would be correct."
      },
      {
        "date": "2024-01-15T13:06:00.000Z",
        "voteCount": 4,
        "content": "nother way to replay messages that have been acknowledged is to seek to a timestamp. To seek to a timestamp, you must first configure the subscription to retain acknowledged messages using retain-acked-messages. If retain-acked-messages is set, Pub/Sub retains acknowledged messages for 7 days."
      },
      {
        "date": "2023-12-30T01:54:00.000Z",
        "voteCount": 4,
        "content": "BE--&gt; correct\nPub/Sub Snapshot: Captures a point-in-time snapshot of the messages in the subscription, ensuring that the previous two days of messages are available for reprocessing even after they've been acknowledged.\nRetain-Acked-Messages Flag: While this flag prevents acknowledged messages from being deleted, it's not sufficient on its own because it only retains messages going forward from when it's enabled."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 229,
    "url": "https://www.examtopics.com/discussions/google/view/129876-exam-professional-data-engineer-topic-1-question-229/",
    "body": "You currently use a SQL-based tool to visualize your data stored in BigQuery. The data visualizations require the use of outer joins and analytic functions. Visualizations must be based on data that is no less than 4 hours old. Business users are complaining that the visualizations are too slow to generate. You want to improve the performance of the visualization queries while minimizing the maintenance overhead of the data preparation pipeline. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate materialized views with the allow_non_incremental_definition option set to true for the visualization queries. Specify the max_staleness parameter to 4 hours and the enable_refresh parameter to true. Reference the materialized views in the data visualization tool.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate views for the visualization queries. Reference the views in the data visualization tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function instance to export the visualization query results as parquet files to a Cloud Storage bucket. Use Cloud Scheduler to trigger the Cloud Function every 4 hours. Reference the parquet files in the data visualization tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate materialized views for the visualization queries. Use the incremental updates capability of BigQuery materialized views to handle changed data automatically. Reference the materialized views in the data visualization tool."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-05T02:34:00.000Z",
        "voteCount": 1,
        "content": "Just a note, the question saying \"data no less than 4 hours old\" presumably means \"no more than 4 hours old\""
      },
      {
        "date": "2024-08-13T16:09:00.000Z",
        "voteCount": 2,
        "content": "Unfortunately the correct answer is B due to the limitations of materialized views, doesn't support any other join than inner and no analytical function is supported"
      },
      {
        "date": "2024-03-13T13:19:00.000Z",
        "voteCount": 3,
        "content": "A\n\nhttps://cloud.google.com/bigquery/docs/materialized-views-create#non-incremental \n\nIn scenarios where data staleness is acceptable, for example for batch data processing or reporting, non-incremental materialized views can improve query performance and reduce cost.\n\nallow_non_incremental_definition option. This option must be accompanied by the max_staleness option. To ensure a periodic refresh of the materialized view, you should also configure a refresh policy."
      },
      {
        "date": "2024-01-13T04:14:00.000Z",
        "voteCount": 2,
        "content": "Option A is better than D, since it accounts for data staleness and is better suited for heavy querying, thanks to the allow_non_incremental_definition"
      },
      {
        "date": "2024-01-05T15:37:00.000Z",
        "voteCount": 4,
        "content": "A seems right but whats wrong with option D, can anybody please explain?"
      },
      {
        "date": "2024-01-15T13:29:00.000Z",
        "voteCount": 2,
        "content": "Seems like materialiazed views can use incremental updates only if data was not delated or updated in original table. Here the data changes so I think thats the reason why its not correct answer\nhttps://cloud.google.com/bigquery/docs/materialized-views-use#incremental_updates\n\"BigQuery combines the cached view's data with new data to provide consistent query results while still using the materialized view. For single-table materialized views, this is possible if the base table is unchanged since the last refresh, or if only new data was added. For multi-table views, no more than one table can have appended data. If more than one of a multi-table view's base tables has changed, then the view cannot be incrementally updated.\""
      },
      {
        "date": "2024-01-04T09:00:00.000Z",
        "voteCount": 3,
        "content": "- Materialized views in BigQuery precompute and store the result of a base query, which can speed up data retrieval for complex queries used in visualizations. \n- The max_staleness parameter allows us to specify how old the data can be, ensuring that the visualizations are based on data no less than 4 hours old. \n- The enable_refresh parameter ensures that the materialized view is periodically refreshed.\n- The allow_non_incremental_definition is used for enabling the creation of non-incrementally refreshable materialized views."
      },
      {
        "date": "2023-12-30T01:55:00.000Z",
        "voteCount": 3,
        "content": "Precomputed Results: Materialized views store precomputed results of complex queries, significantly accelerating subsequent query performance, addressing the slow visualization issue.\nAllow Non-Incremental Views: Using allow_non_incremental_definition circumvents the limitation of incremental updates for outer joins and analytic functions, ensuring views can be created for the specified queries.\nNear-Real-Time Data: Setting max_staleness to 4 hours guarantees data freshness within the acceptable latency for visualizations.\nAutomatic Refresh: Enabling refresh with enable_refresh maintains view consistency with minimal maintenance overhead.\nMinimal Overhead: Materialized views automatically update as underlying data changes, reducing maintenance compared to manual exports or view definitions."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 230,
    "url": "https://www.examtopics.com/discussions/google/view/130174-exam-professional-data-engineer-topic-1-question-230/",
    "body": "You need to modernize your existing on-premises data strategy. Your organization currently uses:<br>\u2022\tApache Hadoop clusters for processing multiple large data sets, including on-premises Hadoop Distributed File System (HDFS) for data replication.<br>\u2022\tApache Airflow to orchestrate hundreds of ETL pipelines with thousands of job steps.<br><br>You need to set up a new architecture in Google Cloud that can handle your Hadoop workloads and requires minimal changes to your existing orchestration processes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Bigtable for your large workloads, with connections to Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Orchestrate your pipelines with Cloud Composer.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataproc to migrate Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Convert your ETL pipelines to Dataflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataproc to migrate your Hadoop clusters to Google Cloud, and Cloud Storage to handle any HDFS use cases. Use Cloud Data Fusion to visually design and deploy your ETL pipelines."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T09:01:00.000Z",
        "voteCount": 7,
        "content": "Straight forward"
      },
      {
        "date": "2024-03-04T17:16:00.000Z",
        "voteCount": 1,
        "content": "You can use Dataproc for doing Apache Hadoop process, then Cloud Storage to replace the HDFS, and using Cloud Composer (built in Apache Airflow) for orchestrator."
      },
      {
        "date": "2024-02-22T15:29:00.000Z",
        "voteCount": 2,
        "content": "Airflow -&gt; composer\nMinimum changes -&gt; Dataproc"
      },
      {
        "date": "2024-02-17T10:57:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-13T04:16:00.000Z",
        "voteCount": 2,
        "content": "definitely B"
      },
      {
        "date": "2024-01-03T04:43:00.000Z",
        "voteCount": 4,
        "content": "Cloud Composer -&gt; Airflow"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 231,
    "url": "https://www.examtopics.com/discussions/google/view/130340-exam-professional-data-engineer-topic-1-question-231/",
    "body": "You recently deployed several data processing jobs into your Cloud Composer 2 environment. You notice that some tasks are failing in Apache Airflow. On the monitoring dashboard, you see an increase in the total workers memory usage, and there were worker pod evictions. You need to resolve these errors. What should you do? (Choose two.)",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the directed acyclic graph (DAG) file parsing interval.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the Cloud Composer 2 environment size from medium to large.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the maximum number of workers and reduce worker concurrency.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory available to the Airflow workers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"E\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tE.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the memory available to the Airflow triggerer."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-16T06:58:00.000Z",
        "voteCount": 5,
        "content": "If an Airflow worker pod is evicted, all task instances running on that pod are interrupted, and later marked as failed by Airflow. The majority of issues with worker pod evictions happen because of out-of-memory situations in workers.\nYou might want to:\n- (D) Increase the memory available to workers.\n- (C) Reduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.\n\nSource: https://cloud.google.com/composer/docs/composer-2/optimize-environments"
      },
      {
        "date": "2024-05-22T21:36:00.000Z",
        "voteCount": 1,
        "content": "Answer C,D \n\nAccording to your observations, you might want to:\n\nIncrease the memory available to workers.\nReduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers."
      },
      {
        "date": "2024-05-20T05:31:00.000Z",
        "voteCount": 1,
        "content": "C. Increase the maximum number of workers and reduce worker concurrency. Most Voted\nD. Increase the memory available to the Airflow workers."
      },
      {
        "date": "2024-02-16T06:56:00.000Z",
        "voteCount": 1,
        "content": "If an Airflow worker pod is evicted, all task instances running on that pod are interrupted, and later marked as failed by Airflow. The majority of issues with worker pod evictions happen because of out-of-memory situations in workers.\nYou might want to:\n- Increase the memory available to workers.\n- Reduce worker concurrency. In this way, a single worker handles fewer tasks at once. This provides more memory or storage to each individual task. If you change worker concurrency, you might also want to increase the maximum number of workers. In this way, the number of tasks that your environment can handle at once stays the same. For example, if you reduce worker Concurrency from 12 to 6, you might want to double the maximum number of workers.\nSource: https://cloud.google.com/composer/docs/composer-2/optimize-environments"
      },
      {
        "date": "2024-01-17T08:40:00.000Z",
        "voteCount": 3,
        "content": "CD It is clear"
      },
      {
        "date": "2024-01-13T04:19:00.000Z",
        "voteCount": 2,
        "content": "C &amp; D to me"
      },
      {
        "date": "2024-01-10T16:00:00.000Z",
        "voteCount": 4,
        "content": "C and D\nCheck ref for memory optimization - https://cloud.google.com/composer/docs/composer-2/optimize-environments"
      },
      {
        "date": "2024-01-13T19:01:00.000Z",
        "voteCount": 3,
        "content": "Agree. Straightforward.\nhttps://cloud.google.com/composer/docs/composer-2/optimize-environments#monitor-scheduler\n-&gt; Figure 3. Graph that displays worker pod evictions"
      },
      {
        "date": "2024-01-08T02:24:00.000Z",
        "voteCount": 1,
        "content": "B&amp;D See this-\nhttps://cloud.google.com/composer/docs/composer-2/troubleshooting-dags#task-fails-without-logs\ngo through the suggested fixes for If there are airflow-worker pods that show Evicted"
      },
      {
        "date": "2024-01-06T09:10:00.000Z",
        "voteCount": 2,
        "content": "C and D"
      },
      {
        "date": "2024-01-04T09:09:00.000Z",
        "voteCount": 3,
        "content": "B&amp;D:\nB : \n- Scaling up the environment size can provide more resources, including memory, to the Airflow workers. If worker pod evictions are occurring due to insufficient memory, increasing the environment size to allocate more resources could alleviate the problem and improve the stability of your data processing jobs.\n\nD:\n- Increase the memory available to the Airflow workers. - Directly increasing the memory allocation for Airflow workers can address the issue of high memory usage and worker pod evictions. More memory per worker means that each worker can handle more demanding tasks or a higher volume of tasks without running out of memory."
      },
      {
        "date": "2024-01-18T03:25:00.000Z",
        "voteCount": 2,
        "content": "why not B ) It s not decreasing concurrency which may cause issue again"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 232,
    "url": "https://www.examtopics.com/discussions/google/view/130175-exam-professional-data-engineer-topic-1-question-232/",
    "body": "You are on the data governance team and are implementing security requirements to deploy resources. You need to ensure that resources are limited to only the europe-west3 region. You want to follow Google-recommended practices.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the constraints/gcp.resourceLocations organization policy constraint to in:europe-west3-locations.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy resources with Terraform and implement a variable validation rule to ensure that the region is set to the europe-west3 region for all resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the constraints/gcp.resourceLocations organization policy constraint to in:eu-locations.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Function to monitor all resources created and automatically destroy the ones created outside the europe-west3 region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T09:17:00.000Z",
        "voteCount": 10,
        "content": "- The constraints/gcp.resourceLocations organization policy constraint is used to define where resources in the organization can be created. \n- Setting it to in:europe-west3-locations would specify that resources can only be created in the europe-west3 region."
      },
      {
        "date": "2024-03-19T20:37:00.000Z",
        "voteCount": 3,
        "content": "I am new to this forum. In almost all the questions, the reveal solution is different than the once's discussed here??"
      },
      {
        "date": "2024-09-22T07:38:00.000Z",
        "voteCount": 1,
        "content": "B, D\nB - Increase the Cloud Composer 2 environment size from medium to large.\nIncreasing the environment size will provide more resources (including memory) to the entire environment, which should help mitigate memory usage issues. This will also support scaling if the jobs demand more resources.\nD. Increase the memory available to the Airflow workers.\nIncreasing memory for Airflow workers will directly address the memory usage issue that's causing the pod evictions. By allocating more memory to the workers, they can handle larger tasks or more intensive workloads without failing due to memory constraints."
      },
      {
        "date": "2024-02-19T19:28:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T04:21:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-01-03T04:47:00.000Z",
        "voteCount": 2,
        "content": "Set the constraints/gcp.resourceLocations organization policy constraint to in:europe-west3-locations."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 233,
    "url": "https://www.examtopics.com/discussions/google/view/130176-exam-professional-data-engineer-topic-1-question-233/",
    "body": "You are a BigQuery admin supporting a team of data consumers who run ad hoc queries and downstream reporting in tools such as Looker. All data and users are combined under a single organizational project. You recently noticed some slowness in query results and want to troubleshoot where the slowdowns are occurring. You think that there might be some job queuing or slot contention occurring as users run jobs, which slows down access to results. You need to investigate the query job information and determine where performance is being affected. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse slot reservations for your project to ensure that you have enough query processing capacity and are able to allocate available slots to the slower queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Monitoring to view BigQuery metrics and set up alerts that let you know when a certain percentage of slots were used.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse available administrative resource charts to determine how slots are being used and how jobs are performing over time. Run a query on the INFORMATION_SCHEMA to review query performance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Logging to determine if any users or downstream consumers are changing or deleting access grants on tagged resources."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T09:19:00.000Z",
        "voteCount": 8,
        "content": "- BigQuery provides administrative resource charts that show slot utilization and job performance, which can help identify patterns of heavy usage or contention. \n- Additionally, querying the INFORMATION_SCHEMA with the JOBS or JOBS_BY_PROJECT view can provide detailed information about specific queries, including execution time, slot usage, and whether they were queued."
      },
      {
        "date": "2024-01-16T01:10:00.000Z",
        "voteCount": 3,
        "content": "descrived here:\nhttps://cloud.google.com/blog/products/data-analytics/troubleshoot-bigquery-performance-with-these-dashboards"
      },
      {
        "date": "2024-02-17T11:17:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/blog/topics/developers-practitioners/monitor-analyze-bigquery-performance-using-information-schema"
      },
      {
        "date": "2024-01-13T04:23:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-03T04:52:00.000Z",
        "voteCount": 1,
        "content": "C. Use available administrative resource charts to determine how slots are being used and how jobs are performing over time. Run a query on the INFORMATION_SCHEMA to review query performance."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 234,
    "url": "https://www.examtopics.com/discussions/google/view/130177-exam-professional-data-engineer-topic-1-question-234/",
    "body": "You migrated a data backend for an application that serves 10 PB of historical product data for analytics. Only the last known state for a product, which is about 10 GB of data, needs to be served through an API to the other applications. You need to choose a cost-effective persistent storage solution that can accommodate the analytics requirements and the API performance of up to 1000 queries per second (QPS) with less than 1 second latency. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the historical data in BigQuery for analytics.<br>2. Use a materialized view to precompute the last state of a product.<br>3. Serve the last state data directly from BigQuery to the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the products as a collection in Firestore with each product having a set of historical changes.<br>2. Use simple and compound queries for analytics.<br>3. Serve the last state data directly from Firestore to the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the historical data in Cloud SQL for analytics.<br>2. In a separate table, store the last state of the product after every product change.<br>3. Serve the last state data directly from Cloud SQL to the API.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Store the historical data in BigQuery for analytics.<br>2. In a Cloud SQL table, store the last state of the product after every product change.<br>3. Serve the last state data directly from Cloud SQL to the API.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-08T05:49:00.000Z",
        "voteCount": 7,
        "content": "Serve the last state data directly from Cloud SQL to the API.\nHere's why this option is most suitable:\n\nBigQuery for Analytics: BigQuery is an excellent choice for storing and analyzing large datasets like your 10 PB of historical product data. It is designed for handling big data analytics efficiently and cost-effectively.\n\nCloud SQL for Last State Data: Cloud SQL is a fully managed relational database that can effectively handle the storage of the last known state of products. Storing this subset of data (about 10 GB) in Cloud SQL allows for optimized and faster query performance for your API needs. Cloud SQL can comfortably handle the requirement of up to 1000 QPS with sub-second latency.\n\nSeparation of Concerns: This approach separates the analytics workload (BigQuery) from the operational query workload (Cloud SQL). This separation ensures that analytics queries do not interfere with the operational performance of the API and vice versa."
      },
      {
        "date": "2024-01-16T01:20:00.000Z",
        "voteCount": 7,
        "content": "D. 1. Store the historical data in BigQuery for analytics.\n2. In a Cloud SQL table, store the last state of the product after every product change.\n3. Serve the last state data directly from Cloud SQL to the AP\n\nThis approach leverages BigQuery's scalability and efficiency for handling large datasets for analytics. BigQuery is well-suited for managing the 10 PB of historical product data. Meanwhile, Cloud SQL provides the necessary performance to handle the API queries with the required low latency. By storing the latest state of each product in Cloud SQL, you can efficiently handle the high QPS with sub-second latency, which is crucial for the API's performance. This combination of BigQuery and Cloud SQL offers a balanced solution for both the large-scale analytics and the high-performance API needs."
      },
      {
        "date": "2024-06-05T20:46:00.000Z",
        "voteCount": 1,
        "content": "Why not A:\nServing data directly from BigQuery to the API may not meet the low latency requirements for high QPS operations, as BigQuery is optimized for analytical queries rather than transactional workloads."
      },
      {
        "date": "2024-05-18T15:21:00.000Z",
        "voteCount": 1,
        "content": "Materialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency. Materialized views can optimize queries with high computation cost and small dataset results. https://cloud.google.com/bigquery/docs/materialized-views-intro#use_cases\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro"
      },
      {
        "date": "2024-04-07T16:53:00.000Z",
        "voteCount": 1,
        "content": "Why D is the best choice:\n\nCost-Effective Analytics: BigQuery excels at handling large datasets (10 PB) and complex analytical queries. Its columnar storage and massively parallel processing make it ideal for analyzing historical product data.\nHigh-Performance API: Cloud SQL provides a managed relational database service optimized for transactional workloads. It can easily handle the 1000 QPS requirement with low latency, ensuring fast API responses.\nSeparation of Concerns: Storing historical data in BigQuery and the last known state in Cloud SQL separates analytical and transactional workloads, optimizing performance and cost for each use case."
      },
      {
        "date": "2024-02-19T19:29:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-16T13:27:00.000Z",
        "voteCount": 1,
        "content": "BigQuery = data warehouse that is optimized for querying and analyzing large datasets using SQL. Can easily process petabytes of data.\nCloud SQL = designed for transactional workloads and traditional relational database use cases, such as web applications, e-commerce platforms, and content management systems."
      },
      {
        "date": "2024-01-13T04:30:00.000Z",
        "voteCount": 3,
        "content": "Option D is the right one, compared to option A, Cloud SQL is more efficient and cost effective for the amount of time the data needs to be accessed by the api"
      },
      {
        "date": "2024-01-03T04:56:00.000Z",
        "voteCount": 2,
        "content": "A. 1. Store the historical data in BigQuery for analytics.\n2. Use a materialized view to precompute the last state of a product.\n3. Serve the last state data directly from BigQuery to the API."
      },
      {
        "date": "2024-02-26T06:42:00.000Z",
        "voteCount": 2,
        "content": "I believe the latency of BigQuery is too high to accommodate the sub-second latency requirement."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 235,
    "url": "https://www.examtopics.com/discussions/google/view/130178-exam-professional-data-engineer-topic-1-question-235/",
    "body": "You want to schedule a number of sequential load and transformation jobs. Data files will be added to a Cloud Storage bucket by an upstream process. There is no fixed schedule for when the new data arrives. Next, a Dataproc job is triggered to perform some transformations and write the data to BigQuery. You then need to run additional transformation jobs in BigQuery. The transformation jobs are different for every table. These jobs might take hours to complete. You need to determine the most efficient and maintainable workflow to process hundreds of tables and provide the freshest data to your end users. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.<br>2. Use a single shared DAG for all tables that need to go through the pipeline.<br>3. Schedule the DAG to run hourly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Cloud Storage, Dataproc, and BigQuery operators.<br>2. Create a separate DAG for each table that needs to go through the pipeline.<br>3. Schedule the DAGs to run hourly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.<br>2. Use a single shared DAG for all tables that need to go through the pipeline.<br>3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.<br>2. Create a separate DAG for each table that needs to go through the pipeline.<br>3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 7,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-26T13:41:00.000Z",
        "voteCount": 5,
        "content": "D\n\n* Transformations are in Dataproc and BigQuery. So you don't need operators for GCS (A and B can be discard)\n* \"There is no fixed schedule for when the new data arrives.\" so you trigger the DAG when a file arrives\n* \"The transformation jobs are different for every table. \" so you need a DAG for each table.\n\nThen, D is the most suitable answer"
      },
      {
        "date": "2024-06-26T10:15:00.000Z",
        "voteCount": 1,
        "content": "This explains why it's not D:\nmaintainable workflow to process hundreds of tables and provide the freshest data to your end users\n\nHow is creating a DAG for each of the hundreds of tables maintainable?"
      },
      {
        "date": "2024-02-18T04:53:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T04:35:00.000Z",
        "voteCount": 3,
        "content": "Option D, which gets triggered when the data comes in and accounts for the fact that each table has its own set of transformations"
      },
      {
        "date": "2024-01-06T09:50:00.000Z",
        "voteCount": 3,
        "content": "why not C?"
      },
      {
        "date": "2024-02-26T13:40:00.000Z",
        "voteCount": 2,
        "content": "It says that the transformations for each table are very different"
      },
      {
        "date": "2024-01-13T20:03:00.000Z",
        "voteCount": 4,
        "content": "Same question, why not use single DAG to manage as there are hundreds of tables."
      },
      {
        "date": "2024-01-04T11:16:00.000Z",
        "voteCount": 2,
        "content": "- Option D: Tailored handling and scheduling for each table; triggered by data arrival for more timely and efficient processing."
      },
      {
        "date": "2024-01-03T05:24:00.000Z",
        "voteCount": 1,
        "content": "D. \n1. Create an Apache Airflow directed acyclic graph (DAG) in Cloud Composer with sequential tasks by using the Dataproc and BigQuery operators.\n2. Create a separate DAG for each table that needs to go through the pipeline.\n3. Use a Cloud Storage object trigger to launch a Cloud Function that triggers the DAG."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 236,
    "url": "https://www.examtopics.com/discussions/google/view/130179-exam-professional-data-engineer-topic-1-question-236/",
    "body": "You are deploying a MySQL database workload onto Cloud SQL. The database must be able to scale up to support several readers from various geographic regions. The database must be highly available and meet low RTO and RPO requirements, even in the event of a regional outage. You need to ensure that interruptions to the readers are minimal during a database failover. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a highly available Cloud SQL instance in region Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Backup the Cloud SQL instances to a multi-regional Cloud Storage bucket. Restore the Cloud SQL backup to a new instance in another region when Region A is down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in multiple regions. Promote one of the read replicas when region A is down.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a highly available Cloud SQL instance in region A. Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Promote the read replica in region B when region A is down.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a highly available Cloud SQL instance in region A. Scale up read workloads by creating read replicas in the same region. Failover to the standby Cloud SQL instance when the primary instance fails."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-02-02T19:02:00.000Z",
        "voteCount": 9,
        "content": "Option C: Because HA read replica in multiple regions. \nNotA: Coz restore from back up is time taking\nNotB: No HA in Multiple regions read replica\nNot  D: Only one region mentioned."
      },
      {
        "date": "2024-01-04T12:01:00.000Z",
        "voteCount": 5,
        "content": "- Combines high availability with geographic distribution of read workloads. \n- Promoting a highly available read replica can provide a quick failover solution, potentially meeting low RTO and RPO requirements.\n\n=====\nWhy not A:\nRestoring from backup to a new instance in another region during a regional outage might not meet low RTO and RPO requirements due to the time it takes to perform a restore."
      },
      {
        "date": "2024-01-13T20:14:00.000Z",
        "voteCount": 1,
        "content": "Why not B?"
      },
      {
        "date": "2024-01-16T01:51:00.000Z",
        "voteCount": 5,
        "content": "Why not B:\nWhile B option scales up read workloads across multiple regions, it doesn't specify high availability for the read replica in another region. In the event of a regional outage, promoting a non-highly available read replica might not provide the desired uptime and reliability."
      },
      {
        "date": "2024-01-22T01:49:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/sql/docs/mysql/replication\n\nThis option involves having read replicas in multiple regions, allowing you to promote one of them in the event of a failure in region A. While there may still be a brief interruption during the failover, it is likely to be less than the time required for the synchronization of cascading read replicas."
      },
      {
        "date": "2024-01-13T04:41:00.000Z",
        "voteCount": 1,
        "content": "To me, it's B. it provides: \nHigh availability: The highly available Cloud SQL instance in region A will ensure that the database remains accessible even if one of the zones in the region becomes unavailable.\nScalability: The read replicas in multiple regions will enable you to scale up the read capacity of the database to support the demands of readers from various geographic regions.\nMinimal interruptions: When region A is down, one of the read replicas in another region will be promoted to become the new primary instance. This will ensure that there is no interruption to the readers."
      },
      {
        "date": "2024-01-13T04:42:00.000Z",
        "voteCount": 1,
        "content": "Why not others: \nApproach A: This approach requires you to restore a backup from a different region, which could take some time. This could result in a significant RPO (Recovery Point Objective) for the database. Additionally, the restored instance may not be physically located in the same region as the readers, which could impact performance.\nApproach C: This approach requires you to promote the read replica in region B, which could result in a temporary interruption to the readers while the promotion is taking place. Additionally, the read replica in region B may not be able to handle the same level of read traffic as the primary instance in region A.\nApproach D: This approach does not provide the same level of scalability as the other approaches, as you are limited to read replicas in the same region. Additionally, failover to the standby instance could result in a temporary interruption to the readers."
      },
      {
        "date": "2024-01-13T04:46:00.000Z",
        "voteCount": 3,
        "content": "Ignore my previous messages, it's C :D"
      },
      {
        "date": "2024-01-03T05:41:00.000Z",
        "voteCount": 1,
        "content": "A. \nCreate a highly available Cloud SQL instance in region Create a highly available read replica in region B. Scale up read workloads by creating cascading read replicas in multiple regions. Backup the Cloud SQL instances to a multi-regional Cloud Storage bucket. Restore the Cloud SQL backup to a new instance in another region when Region A is down."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 237,
    "url": "https://www.examtopics.com/discussions/google/view/130180-exam-professional-data-engineer-topic-1-question-237/",
    "body": "You are planning to load some of your existing on-premises data into BigQuery on Google Cloud. You want to either stream or batch-load data, depending on your use case. Additionally, you want to mask some sensitive data before loading into BigQuery. You need to do this in a programmatic way while keeping costs to a minimum. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion to design your pipeline, use the Cloud DLP plug-in to de-identify data within your pipeline, and then move the data into BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Data Transfer Service to schedule your migration. After the data is populated in BigQuery, use the connection to the Cloud Data Loss Prevention (Cloud DLP) API to de-identify the necessary data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate your pipeline with Dataflow through the Apache Beam SDK for Python, customizing separate options within your code for streaming, batch processing, and Cloud DLP. Select BigQuery as your data sink.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Datastream to replicate your on-premise data on BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T12:13:00.000Z",
        "voteCount": 9,
        "content": "- Programmatic Flexibility: Apache Beam provides extensive control over pipeline design, allowing for customization of data transformations, including integration with Cloud DLP for sensitive data masking.\n- Streaming and Batch Support: Beam seamlessly supports both streaming and batch data processing modes, enabling flexibility in data loading patterns.\n- Cost-Effective Processing: Dataflow offers a serverless model, scaling resources as needed, and only charging for resources used, helping optimize costs.\n- Integration with Cloud DLP: Beam integrates well with Cloud DLP for sensitive data masking, ensuring data privacy before loading into BigQuery."
      },
      {
        "date": "2024-01-08T06:42:00.000Z",
        "voteCount": 2,
        "content": "In correct Option is A because you want a programatic way whereas datafusion is codeless solution and also dataflow is cost effective"
      },
      {
        "date": "2024-01-13T20:27:00.000Z",
        "voteCount": 2,
        "content": "You are saying Option C"
      },
      {
        "date": "2024-02-18T09:04:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-22T02:12:00.000Z",
        "voteCount": 2,
        "content": "C is correct. Using Dataflow as Python as programming and BQ as sink. \n\nA is incorrect - DataFusion is Code-free as the main propose"
      },
      {
        "date": "2024-01-03T05:42:00.000Z",
        "voteCount": 1,
        "content": "A. \nUse Cloud Data Fusion to design your pipeline, use the Cloud DLP plug-in to de-identify data within your pipeline, and then move the data into BigQuery."
      },
      {
        "date": "2024-05-16T07:03:00.000Z",
        "voteCount": 1,
        "content": "Incorrect, that's a low-code solution. Doesnt meet this specific requirement: \"You need to do this in a programmatic way\""
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 238,
    "url": "https://www.examtopics.com/discussions/google/view/130181-exam-professional-data-engineer-topic-1-question-238/",
    "body": "You want to encrypt the customer data stored in BigQuery. You need to implement per-user crypto-deletion on data stored in your tables. You want to adopt native features in Google Cloud to avoid custom solutions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImplement Authenticated Encryption with Associated Data (AEAD) BigQuery functions while storing your data in BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer-managed encryption key (CMEK) in Cloud KMS. Associate the key to the table while creating the table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a customer-managed encryption key (CMEK) in Cloud KMS. Use the key to encrypt data before storing in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEncrypt your data during ingestion by using a cryptographic library supported by your ETL pipeline."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T12:34:00.000Z",
        "voteCount": 9,
        "content": "- AEAD cryptographic functions in BigQuery allow for encryption and decryption of data at the column level. \n- You can encrypt specific data fields using a unique key per user and manage these keys outside of BigQuery (for example, in your application or using a key management system). \n- By \"deleting\" or revoking access to the key for a specific user, you effectively make their data unreadable, achieving crypto-deletion. \n- This method provides fine-grained encryption control but requires careful key management and integration with your applications."
      },
      {
        "date": "2024-02-18T09:21:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/aead-encryption-concepts\nhttps://cloud.google.com/bigquery/docs/reference/standard-sql/aead_encryption_functions"
      },
      {
        "date": "2024-01-03T05:46:00.000Z",
        "voteCount": 2,
        "content": "A. \nImplement Authenticated Encryption with Associated Data (AEAD) BigQuery functions while storing your data in BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 239,
    "url": "https://www.examtopics.com/discussions/google/view/130182-exam-professional-data-engineer-topic-1-question-239/",
    "body": "The data analyst team at your company uses BigQuery for ad-hoc queries and scheduled SQL pipelines in a Google Cloud project with a slot reservation of 2000 slots. However, with the recent introduction of hundreds of new non time-sensitive SQL pipelines, the team is encountering frequent quota errors. You examine the logs and notice that approximately 1500 queries are being triggered concurrently during peak time. You need to resolve the concurrency issue. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the slot capacity of the project with baseline as 0 and maximum reservation size as 3000.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate SQL pipelines to run as a batch query, and run ad-hoc queries as interactive query jobs.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the slot capacity of the project with baseline as 2000 and maximum reservation size as 3000.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate SQL pipelines and ad-hoc queries to run as interactive query jobs."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T13:37:00.000Z",
        "voteCount": 6,
        "content": "- BigQuery allows you to specify job priority as either BATCH or INTERACTIVE. \n- Batch queries are queued and then started when idle resources are available, making them suitable for non-time-sensitive workloads. \n- Running ad-hoc queries as interactive ensures they have prompt access to resources."
      },
      {
        "date": "2024-05-18T16:37:00.000Z",
        "voteCount": 2,
        "content": "You already have a 2000 slots consumption and sudden peaks, so you should use a baseline of 2000 slots and a maximum of 3000 to tackle the peak concurrent activity.\nhttps://cloud.google.com/bigquery/docs/slots-autoscaling-intro"
      },
      {
        "date": "2024-04-07T17:21:00.000Z",
        "voteCount": 2,
        "content": "Why A is the best choice:\n\nAddresses Concurrency: Increasing the maximum reservation size to 3000 slots directly addresses the concurrency issue by providing more capacity for simultaneous queries. Since the current peak usage is 1500 queries, this increase ensures sufficient headroom.\nCost Optimization: Setting the baseline to 0 means you only pay for the slots actually used, avoiding unnecessary costs for idle capacity. This is ideal for non-time-sensitive workloads where flexibility is more important than guaranteed instant availability."
      },
      {
        "date": "2024-02-19T19:34:00.000Z",
        "voteCount": 2,
        "content": "Option B"
      },
      {
        "date": "2024-04-07T17:21:00.000Z",
        "voteCount": 1,
        "content": "B: While batch queries are generally more cost-effective for large, non-interactive workloads, they don't solve the concurrency problem. If multiple batch queries are triggered simultaneously, they would still compete for the same limited slot pool."
      },
      {
        "date": "2024-01-03T05:55:00.000Z",
        "voteCount": 2,
        "content": "B. \nUpdate SQL pipelines to run as a batch query, and run ad-hoc queries as interactive query jobs."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 240,
    "url": "https://www.examtopics.com/discussions/google/view/130183-exam-professional-data-engineer-topic-1-question-240/",
    "body": "You are designing a data mesh on Google Cloud by using Dataplex to manage data in BigQuery and Cloud Storage. You want to simplify data asset permissions. You are creating a customer virtual lake with two user groups:<br><br>\u2022\tData engineers, which require full data lake access<br>\u2022\tAnalytic users, which require access to curated data<br><br>You need to assign access rights to these two groups. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the dataplex.dataOwner role to the data engineer group on the customer data lake.<br>2. Grant the dataplex.dataReader role to the analytic user group on the customer curated zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the dataplex.dataReader role to the data engineer group on the customer data lake.<br>2. Grant the dataplex.dataOwner to the analytic user group on the customer curated zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectCreator role on Cloud Storage buckets to data engineers.<br>2. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to analytic users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Grant the bigquery.dataViewer role on BigQuery datasets and the storage.objectViewer role on Cloud Storage buckets to data engineers.<br>2. Grant the bigquery.dataOwner role on BigQuery datasets and the storage.objectEditor role on Cloud Storage buckets to analytic users."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T13:50:00.000Z",
        "voteCount": 9,
        "content": "- dataplex.dataOwner: Grants full control over data assets, including reading, writing, managing, and granting access to others.\n- dataplex.dataReader: Allows users to read data but not modify it."
      },
      {
        "date": "2024-01-13T21:13:00.000Z",
        "voteCount": 4,
        "content": "Yes, https://cloud.google.com/dataplex/docs/lake-security#data-roles\nDataplex maps its roles to the data roles for each underlying storage resource (Cloud Storage, BigQuery).\n^ simplify the permissions."
      },
      {
        "date": "2024-05-18T16:47:00.000Z",
        "voteCount": 1,
        "content": "The quetion is for BigQuery AND Cloud Storage for a Data Lake, so you should assign IAM permissions for both of them. C is correct."
      },
      {
        "date": "2024-07-17T06:38:00.000Z",
        "voteCount": 1,
        "content": "Dataplex roles are mapped to roles for the underlying resources, like BQ and GCS. So A and C are functionally (almost) equivalent, but A is simpler (2 roles rather than 4). See https://cloud.google.com/dataplex/docs/lake-security#data-roles"
      },
      {
        "date": "2024-02-19T19:34:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-14T07:26:00.000Z",
        "voteCount": 3,
        "content": "A correct answer"
      },
      {
        "date": "2024-01-13T06:17:00.000Z",
        "voteCount": 2,
        "content": "Option A clearly correct"
      },
      {
        "date": "2024-01-03T06:05:00.000Z",
        "voteCount": 1,
        "content": "A. \n1. Grant the dataplex.dataOwner role to the data engineer group on the customer data lake.\n2. Grant the dataplex.dataReader role to the analytic user group on the customer curated zone."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 241,
    "url": "https://www.examtopics.com/discussions/google/view/130184-exam-professional-data-engineer-topic-1-question-241/",
    "body": "You are designing the architecture of your application to store data in Cloud Storage. Your application consists of pipelines that read data from a Cloud Storage bucket that contains raw data, and write the data to a second bucket after processing. You want to design an architecture with Cloud Storage resources that are capable of being resilient if a Google Cloud regional failure occurs. You want to minimize the recovery point objective (RPO) if a failure occurs, with no impact on applications that use the stored data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdopt multi-regional Cloud Storage buckets in your architecture.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdopt two regional Cloud Storage buckets, and update your application to write the output on both buckets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdopt a dual-region Cloud Storage bucket, and enable turbo replication in your architecture.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdopt two regional Cloud Storage buckets, and create a daily task to copy from one bucket to the other."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T14:08:00.000Z",
        "voteCount": 10,
        "content": "- Dual-region buckets are a specific type of storage that automatically replicates data between two geographically distinct regions. \n- Turbo replication is an enhanced feature that provides faster replication between the two regions, thus minimizing RPO. \n- This option ensures that your data is resilient to regional failures and is replicated quickly, meeting the needs for low RPO and no impact on application performance."
      },
      {
        "date": "2024-04-07T17:39:00.000Z",
        "voteCount": 4,
        "content": "A. Adopt multi-regional Cloud Storage buckets in your architecture.\n\nWhy A is the best choice:\n\nAutomatic Cross-Region Replication: Multi-regional buckets automatically replicate data across multiple geographically separated regions within a selected multi-region location (e.g., us). This ensures data redundancy and availability even if one region experiences an outage.\nMinimal RPO: Data written to a multi-regional bucket is synchronously replicated to at least two regions. This means that in the event of a regional failure, the RPO is essentially zero, as the data is already available in other regions.\nNo Application Changes: Applications can continue reading and writing data to the multi-regional bucket without any modifications, as the cross-region replication is handled transparently by Cloud Storage"
      },
      {
        "date": "2024-03-21T12:30:00.000Z",
        "voteCount": 1,
        "content": "vote c"
      },
      {
        "date": "2024-03-14T03:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/storage/docs/availability-durability#turbo-replication\n\n\"Default replication in Cloud Storage is designed to provide redundancy across regions for 99.9% of newly written objects within a target of one hour and 100% of newly written objects within a target of 12 hours\"\n\n\"When enabled, turbo replication is designed to replicate 100% of newly written objects to both regions that constitute the dual-region within the recovery point objective of 15 minutes, regardless of object size.\"\n\nThus, since they want to minimize RPO, should use turbo replication"
      },
      {
        "date": "2024-02-19T19:35:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T06:23:00.000Z",
        "voteCount": 4,
        "content": "Option C: https://cloud.google.com/storage/docs/dual-regions + https://cloud.google.com/storage/docs/managing-turbo-replication"
      },
      {
        "date": "2024-01-06T04:49:00.000Z",
        "voteCount": 4,
        "content": "Turbo replication provides faster redundancy across regions for data in your dual-region buckets, which reduces the risk of data loss exposure and helps support uninterrupted service following a regional outage."
      },
      {
        "date": "2024-01-03T06:06:00.000Z",
        "voteCount": 2,
        "content": "A. Adopt multi-regional Cloud Storage buckets in your architecture."
      },
      {
        "date": "2024-01-16T03:04:00.000Z",
        "voteCount": 3,
        "content": "It wont be a correct answer. Correct is C. It is required \"no impact on applications that use the stored data\""
      },
      {
        "date": "2024-02-23T13:02:00.000Z",
        "voteCount": 1,
        "content": "But multi-region is completely transparent for the application if one fails. it would need to fail all EU or US regions. I dont undertand why multi-region would have impact on that"
      },
      {
        "date": "2024-01-16T03:04:00.000Z",
        "voteCount": 4,
        "content": "Whereas with multi-region \" it can also introduce unpredictable latency into the response time and higher network egress charges for cloud workloads when multi-region data is read from remote regions\"\nhttps://cloud.google.com/blog/products/storage-data-transfer/choose-between-regional-dual-region-and-multi-region-cloud-storage"
      },
      {
        "date": "2024-02-23T13:11:00.000Z",
        "voteCount": 2,
        "content": "There is no requirment on latency, just RPO which it would be 0 since multi-region."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 242,
    "url": "https://www.examtopics.com/discussions/google/view/130186-exam-professional-data-engineer-topic-1-question-242/",
    "body": "You have designed an Apache Beam processing pipeline that reads from a Pub/Sub topic. The topic has a message retention duration of one day, and writes to a Cloud Storage bucket. You need to select a bucket location and processing strategy to prevent data loss in case of a regional outage with an RPO of 15 minutes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use a dual-region Cloud Storage bucket.<br>2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.<br>3. Seek the subscription back in time by 15 minutes to recover the acknowledged messages.<br>4. Start the Dataflow job in a secondary region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use a multi-regional Cloud Storage bucket.<br>2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.<br>3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.<br>4. Start the Dataflow job in a secondary region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use a regional Cloud Storage bucket.<br>2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.<br>3. Seek the subscription back in time by one day to recover the acknowledged messages.<br>4. Start the Dataflow job in a secondary region and write in a bucket in the same region.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use a dual-region Cloud Storage bucket with turbo replication enabled.<br>2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.<br>3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.<br>4. Start the Dataflow job in a secondary region.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T03:16:00.000Z",
        "voteCount": 5,
        "content": "D. 1. Use a dual-region Cloud Storage bucket with turbo replication enabled.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 60 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region.\n\nRPO of 15 minutes is guaranteed when turbo replication is used\nhttps://cloud.google.com/storage/docs/availability-durability"
      },
      {
        "date": "2024-02-23T13:21:00.000Z",
        "voteCount": 1,
        "content": "Why multi-region is not correct. There is no downtime in case a region goes down."
      },
      {
        "date": "2024-05-12T09:33:00.000Z",
        "voteCount": 1,
        "content": "D\nhttps://cloud.google.com/storage/docs/availability-durability#cross-region-redundancy"
      },
      {
        "date": "2024-02-19T19:41:00.000Z",
        "voteCount": 4,
        "content": "Option D is correct.\n\nNot A, because dual-region bucket WITHOUT turbo replication takes atleast 1 hour to sync data between regions. SLA for 100% data sync is 12 hours as per google."
      },
      {
        "date": "2024-01-25T04:13:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/storage/docs/availability-durability#turbo-replication says : \"When enabled, turbo replication is designed to replicate 100% of newly written objects to both regions that constitute the dual-region within the recovery point objective of 15 minutes, regardless of object size.\"\nso seems D to me"
      },
      {
        "date": "2024-01-04T14:13:00.000Z",
        "voteCount": 1,
        "content": "- Low RPO: Dual-region buckets offer synchronous replication, ensuring data is immediately available in both regions, aligning with the 15-minute RPO.\n- Turbo Replication: enabling turbo replication can further reduce replication latency to near-real-time for even stricter RPO requirements.\n- Resilient Data Storage: Dual-region buckets ensure data availability even during regional outages, protecting processed data.\n- Fast Recovery: Reprocessing from the last 15 minutes of acknowledged messages minimizes data loss and downtime."
      },
      {
        "date": "2024-01-18T20:07:00.000Z",
        "voteCount": 2,
        "content": "why not D then, if turbo replication improves RPO??"
      },
      {
        "date": "2024-01-03T06:15:00.000Z",
        "voteCount": 1,
        "content": "A. \n1. Use a dual-region Cloud Storage bucket.\n2. Monitor Dataflow metrics with Cloud Monitoring to determine when an outage occurs.\n3. Seek the subscription back in time by 15 minutes to recover the acknowledged messages.\n4. Start the Dataflow job in a secondary region."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 243,
    "url": "https://www.examtopics.com/discussions/google/view/130349-exam-professional-data-engineer-topic-1-question-243/",
    "body": "You are preparing data that your machine learning team will use to train a model using BigQueryML. They want to predict the price per square foot of real estate. The training data has a column for the price and a column for the number of square feet. Another feature column called \u2018feature1\u2019 contains null values due to missing data. You want to replace the nulls with zeros to keep more data points. Which query should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-data-engineer/image4.png\"><br>\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-data-engineer/image5.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-data-engineer/image6.png\"><br>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t<img src=\"https://img.examtopics.com/professional-data-engineer/image7.png\">"
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-16T03:29:00.000Z",
        "voteCount": 5,
        "content": "Correct answer is C. \nIt both replace NULL with 0 and pass price per square foot of real estate."
      },
      {
        "date": "2024-02-13T01:42:00.000Z",
        "voteCount": 6,
        "content": "Option C isn't a good practice. What if any 0 value is contained in the column of squre_feet, then price / 0 will throw an exception. IF(IFNULL(squre_feet, 0) = 0, 0, price/squre_feet)."
      },
      {
        "date": "2024-10-07T01:55:00.000Z",
        "voteCount": 1,
        "content": "I think the assumption here is that no houses are zero feet in size. If they are, that should be caught in preprocessing, which is outside the short scope of this question. If the answer isn't C, then it's A, which would mean the question is suggesting you need an ML model to calculate price per square for data where you already have both price and square feet as features. In that instance you clearly need to only divide one by the other. Those columns must be intended to be the target, or the whole question is nonsense."
      },
      {
        "date": "2024-02-17T03:10:00.000Z",
        "voteCount": 5,
        "content": "I didn't get why they mentioned in the task price and square feet columns. Just to irritate us? Do we need to do something with these columns or just with column feature1?"
      },
      {
        "date": "2024-03-24T06:24:00.000Z",
        "voteCount": 1,
        "content": "I think they just want us to build a \u201clabel\u201d (target) column ourselves since there\u2019s no direct value in the training set"
      },
      {
        "date": "2024-03-24T06:31:00.000Z",
        "voteCount": 1,
        "content": "But I still prefer to choose A since the square_feet column itself may have influence on price, which shouldn\u2019t be removed"
      },
      {
        "date": "2024-10-07T01:52:00.000Z",
        "voteCount": 1,
        "content": "This must be C, though the wording isn't great. If price and square foot are included in the data, they are either intended to be the target, in which case you need to create that target as per C, or if they are genuinely features, you DO NOT need a machine learning model. If you already know price and square feet, price per square foot is just price/ft2. You don't need ML to predict that, it's just a division. The only context this makes sense in is if they mean \"price and square foot are the target, and feature1 is the predictive feature\", which means C is correct. The removing nulls from feature1 and the creation of price per square foot is C."
      },
      {
        "date": "2024-07-03T09:20:00.000Z",
        "voteCount": 1,
        "content": "Font Cloude 3.5 and GPT 4o, in theoy is better to keep the less amount of features, then price_per_sqft and feature1 cleaned is the best option"
      },
      {
        "date": "2024-04-29T12:26:00.000Z",
        "voteCount": 1,
        "content": "EXCEPT means it won't select that column."
      },
      {
        "date": "2024-03-11T05:50:00.000Z",
        "voteCount": 4,
        "content": "Option A is the correct choice because it retains all the original columns and specifically addresses the issue of null values in \u2018feature1\u2019 by replacing them with zeros, without altering any other columns or performing unnecessary calculations. This makes the data ready for use in BigQueryML without losing any important information. \n\nOption C is not the best choice because it includes the EXCEPT clause for the price and square_feet columns, which would exclude these columns from the results. This is not desirable since you need these columns for the machine learning model to predict the price per square foot"
      },
      {
        "date": "2024-02-28T09:51:00.000Z",
        "voteCount": 3,
        "content": "C is not a valid answer. You are introducing a redundant variable, that could be valid, but removing from the dataset 2 variables that exactly influence in the predictions you are trying to make."
      },
      {
        "date": "2024-02-28T09:51:00.000Z",
        "voteCount": 2,
        "content": "C is not a valid answer. You are introducing a redundant variable, that could be valid, but removing from the dataset 2 variables that exactly influence in the predictions you are trying to make."
      },
      {
        "date": "2024-10-07T01:50:00.000Z",
        "voteCount": 1,
        "content": "Just to clarify, they don't \"influence\" the prediction, they are in fact the target. The model needs to predict price per square foot. If you have price, and square foot, they are either 1) the prediction target price/squarefoot, or if not then you absolutely do not need a machine learning model, you just device price by square foot."
      },
      {
        "date": "2024-02-25T07:09:00.000Z",
        "voteCount": 1,
        "content": "Option C not only handles the null values in feature1 by replacing them with zeros (using IFNULL(feature1, 0) as feature1_cleaned), but it also creates a new feature price_per_sqft by dividing the price by the number of square feet (price/square_feet as price_per_sqft). This new feature directly corresponds to what your team wants to predict (the price per square foot of real estate), and could therefore be very useful for the machine learning model."
      },
      {
        "date": "2024-02-23T13:30:00.000Z",
        "voteCount": 1,
        "content": "It should be C.\n\n\"They want to predict the price per square foot of real estate. The training data has a column for the price and a column for the number of square feet.\"\n\nYou need to create the column the model is going to predict."
      },
      {
        "date": "2024-02-19T20:51:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T06:33:00.000Z",
        "voteCount": 1,
        "content": "option A clearly"
      },
      {
        "date": "2024-01-04T14:45:00.000Z",
        "voteCount": 4,
        "content": "Straight forward"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 244,
    "url": "https://www.examtopics.com/discussions/google/view/129902-exam-professional-data-engineer-topic-1-question-244/",
    "body": "Different teams in your organization store customer and performance data in BigQuery. Each team needs to keep full control of their collected data, be able to query data within their projects, and be able to exchange their data with other teams. You need to implement an organization-wide solution, while minimizing operational tasks and costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each team to create authorized views of their data. Grant the biquery.jobUser role to each team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery scheduled query to replicate all customer data into team projects.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk each team to publish their data in Analytics Hub. Direct the other teams to subscribe to them.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable each team to create materialized views of the data they need to access in their projects."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T14:49:00.000Z",
        "voteCount": 5,
        "content": "- Analytics Hub allows organizations to create and manage exchanges where producers can publish their data and consumers can discover and subscribe to data products. \n- Asking each team to publish their data in Analytics Hub and having other teams subscribe to them is a scalable and controlled way of sharing data. \n- It minimizes operational tasks because data doesn't need to be duplicated or manually managed after setup, and teams can maintain full control over their datasets."
      },
      {
        "date": "2024-04-07T17:57:00.000Z",
        "voteCount": 1,
        "content": "Why C is the best choice:\n\nCentralized Data Exchange: Analytics Hub provides a unified platform for data sharing across teams and organizations. It simplifies the process of publishing, discovering, and subscribing to datasets, reducing operational overhead.\nData Ownership and Control: Each team retains full control over their data, deciding which datasets to publish and who can access them. This ensures data governance and security.\nCross-Project Querying: Once a team subscribes to a dataset in Analytics Hub, they can query it directly from their own BigQuery project, enabling seamless data access without data replication.\nCost Efficiency: Analytics Hub eliminates the need for data duplication or complex ETL processes, reducing storage and processing costs."
      },
      {
        "date": "2024-03-21T00:53:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2024-02-19T20:52:00.000Z",
        "voteCount": 1,
        "content": "C. Analytics Hub"
      },
      {
        "date": "2024-01-13T06:34:00.000Z",
        "voteCount": 2,
        "content": "that's what analytics hub is designed for"
      },
      {
        "date": "2023-12-30T10:24:00.000Z",
        "voteCount": 3,
        "content": "Analytics hub to reduce operational overhead of creating/maintaining views permissions etc"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 245,
    "url": "https://www.examtopics.com/discussions/google/view/129901-exam-professional-data-engineer-topic-1-question-245/",
    "body": "You are developing a model to identify the factors that lead to sales conversions for your customers. You have completed processing your data. You want to continue through the model development lifecycle. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse your model to run predictions on fresh customer input data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMonitor your model performance, and make any adjustments needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDelineate what data will be used for testing and what will be used for training the model.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tTest and evaluate your model on your curated data to determine how well the model performs."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T14:53:00.000Z",
        "voteCount": 7,
        "content": "- Before you can train a model, you need to decide how to split your dataset."
      },
      {
        "date": "2023-12-30T10:23:00.000Z",
        "voteCount": 6,
        "content": "Model doesn't seem to be trained yet"
      },
      {
        "date": "2024-08-10T05:35:00.000Z",
        "voteCount": 3,
        "content": "First ever time Exam Topic answers matching with users answer yoooo hoooooo.\n\nC"
      },
      {
        "date": "2024-09-14T10:05:00.000Z",
        "voteCount": 1,
        "content": "Yoooo hoooooo.Yep !"
      },
      {
        "date": "2024-02-19T20:52:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T06:37:00.000Z",
        "voteCount": 5,
        "content": "Option C - you've just concluded processing data, ending up with clean and prepared data for the model. Now you need to decide how to split the data for testing and for training. Only afterwards, you can train the model, evaluate it, fine tune it and, eventually, predict with it"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 246,
    "url": "https://www.examtopics.com/discussions/google/view/130188-exam-professional-data-engineer-topic-1-question-246/",
    "body": "You have one BigQuery dataset which includes customers\u2019 street addresses. You want to retrieve all occurrences of street addresses from the dataset. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tWrite a SQL query in BigQuery by using REGEXP_CONTAINS on all tables in your dataset to find rows where the word \u201cstreet\u201d appears.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a deep inspection job on each table in your dataset with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a discovery scan configuration on your organization with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a de-identification job in Cloud Data Loss Prevention and use the masking transformation."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T14:57:00.000Z",
        "voteCount": 6,
        "content": "- Cloud Data Loss Prevention (Cloud DLP) provides powerful inspection capabilities for sensitive data, including predefined detectors for infoTypes such as STREET_ADDRESS. \n- By creating a deep inspection job for each table with the STREET_ADDRESS infoType, you can accurately identify and retrieve rows that contain street addresses."
      },
      {
        "date": "2024-05-18T17:48:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/sensitive-data-protection/docs/learn-about-your-data#inspection"
      },
      {
        "date": "2024-02-19T20:53:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-15T23:40:00.000Z",
        "voteCount": 1,
        "content": "Why not C? Discovery scan configuration can also help to identify risk/sensitivity fields."
      },
      {
        "date": "2024-01-16T07:01:00.000Z",
        "voteCount": 4,
        "content": "In the question we need to retrieve all occurances of street adresses from the dataset. In C you create discovery confiuration plan on whole organization. Its not needed."
      },
      {
        "date": "2024-01-13T06:41:00.000Z",
        "voteCount": 3,
        "content": "Option B - you want to retrieve ALL occurrences within the dataset"
      },
      {
        "date": "2024-01-03T06:26:00.000Z",
        "voteCount": 2,
        "content": "B. Create a deep inspection job on each table in your dataset with Cloud Data Loss Prevention and create an inspection template that includes the STREET_ADDRESS infoType."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 247,
    "url": "https://www.examtopics.com/discussions/google/view/130189-exam-professional-data-engineer-topic-1-question-247/",
    "body": "Your company operates in three domains: airlines, hotels, and ride-hailing services. Each domain has two teams: analytics and data science, which create data assets in BigQuery with the help of a central data platform team. However, as each domain is evolving rapidly, the central data platform team is becoming a bottleneck. This is causing delays in deriving insights from data, and resulting in stale data when pipelines are not kept up to date. You need to design a data mesh architecture by using Dataplex to eliminate the bottleneck. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create one lake for each team. Inside each lake, create one zone for each domain.<br>2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.<br>3. Have the central data platform team manage all zones\u2019 data assets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create one lake for each team. Inside each lake, create one zone for each domain.<br>2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.<br>3. Direct each domain to manage their own zone\u2019s data assets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create one lake for each domain. Inside each lake, create one zone for each team.<br>2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.<br>3. Direct each domain to manage their own lake\u2019s data assets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create one lake for each domain. Inside each lake, create one zone for each team.<br>2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.<br>3. Have the central data platform team manage all lakes\u2019 data assets."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T16:15:00.000Z",
        "voteCount": 7,
        "content": "- each domain should manage their own lake\u2019s data assets"
      },
      {
        "date": "2024-01-16T00:22:00.000Z",
        "voteCount": 4,
        "content": "Agree. https://cloud.google.com/dataplex/docs/introduction#a_domain-centric_data_mesh"
      },
      {
        "date": "2024-03-20T13:25:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2024-02-19T20:54:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T06:44:00.000Z",
        "voteCount": 4,
        "content": "Option C - create a lake for each domain, each team manages its own assets"
      },
      {
        "date": "2024-01-10T22:23:00.000Z",
        "voteCount": 2,
        "content": "Separate lakes for each team\nZones within each lake dedicated to different domains"
      },
      {
        "date": "2024-01-03T06:29:00.000Z",
        "voteCount": 1,
        "content": "C. \n1. Create one lake for each domain. Inside each lake, create one zone for each team.\n2. Attach each of the BigQuery datasets created by the individual teams as assets to the respective zone.\n3. Direct each domain to manage their own lake\u2019s data assets."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 248,
    "url": "https://www.examtopics.com/discussions/google/view/130191-exam-professional-data-engineer-topic-1-question-248/",
    "body": "dataset.inventory_vm sample records:<br><br><img src=\"https://img.examtopics.com/professional-data-engineer/image8.png\"><br><br>You have an inventory of VM data stored in the BigQuery table. You want to prepare the data for regular reporting in the most cost-effective way. You need to exclude VM rows with fewer than 8 vCPU in your report. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view with a filter to drop rows with fewer than 8 vCPU, and use the UNNEST operator.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view with a filter to drop rows with fewer than 8 vCPU, and use the WITH common table expression.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to batch process and write the result to another BigQuery table."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T16:25:00.000Z",
        "voteCount": 6,
        "content": "- The table structure shows that the vCPU data is stored in a nested field within the components column. \n- Using the UNNEST operator to flatten the nested field and apply the filter."
      },
      {
        "date": "2024-03-20T13:23:00.000Z",
        "voteCount": 1,
        "content": "option A"
      },
      {
        "date": "2024-02-19T20:55:00.000Z",
        "voteCount": 1,
        "content": "Option A  - UNNEST"
      },
      {
        "date": "2024-01-14T14:13:00.000Z",
        "voteCount": 4,
        "content": "A seems to be the correct answer because of the table structure and the UNNEST operator.\nHowever, i don\u2019t understand why wouldn\u2019t we chose a materialized view"
      },
      {
        "date": "2024-01-13T06:46:00.000Z",
        "voteCount": 4,
        "content": "Option A - The regular reporting doesn't justify a materialized view, since the frequency of access is not so high; a simple view would do the trick. Moreover, the vcpu data is in a nested field and requires Unnest."
      },
      {
        "date": "2024-01-03T06:39:00.000Z",
        "voteCount": 2,
        "content": "A. Create a view with a filter to drop rows with fewer than 8 vCPU, and use the UNNEST operator."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 249,
    "url": "https://www.examtopics.com/discussions/google/view/130353-exam-professional-data-engineer-topic-1-question-249/",
    "body": "Your team is building a data lake platform on Google Cloud. As a part of the data foundation design, you are planning to store all the raw data in Cloud Storage. You are expecting to ingest approximately 25 GB of data a day and your billing department is worried about the increasing cost of storing old data. The current business requirements are:<br><br>\u2022\tThe old data can be deleted anytime.<br>\u2022\tThere is no predefined access pattern of the old data.<br>\u2022\tThe old data should be available instantly when accessed.<br>\u2022\tThere should not be any charges for data retrieval.<br><br>What should you do to optimize for cost?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the bucket with the Autoclass storage class feature.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 90 days to coldline, and 365 days to archive storage class. Delete old data as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Object Lifecycle Management policy to modify the storage class for data older than 30 days to coldline, 90 days to nearline, and 365 days to archive storage class. Delete old data as needed.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 45 days to coldline, and 60 days to archive storage class. Delete old data as needed."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-07T04:30:00.000Z",
        "voteCount": 10,
        "content": "https://cloud.google.com/storage/docs/autoclass"
      },
      {
        "date": "2024-01-04T16:36:00.000Z",
        "voteCount": 7,
        "content": "- Autoclass automatically moves objects between storage classes without impacting performance or availability, nor incurring retrieval costs. \n- It continuously optimizes storage costs based on access patterns without the need to set specific lifecycle management policies."
      },
      {
        "date": "2024-07-15T07:45:00.000Z",
        "voteCount": 1,
        "content": "the question clearly specifies there should not be any retrieval charges. so enabling autoclass is not recommended because we have to pay one time fees while retrieving the data. and usually soft delete is enable."
      },
      {
        "date": "2024-08-26T03:13:00.000Z",
        "voteCount": 1,
        "content": "A one-time pay isn't considered a retrieval charge. A is correct"
      },
      {
        "date": "2024-04-07T18:22:00.000Z",
        "voteCount": 2,
        "content": "Why B is the best choice:\n\nCost Optimization: This option leverages Cloud Storage's different storage classes to significantly reduce costs for storing older data. Nearline, coldline, and archive storage classes are progressively cheaper than the standard storage class, with trade-offs in availability and retrieval times.\nMeets Requirements:\nOld data deletion: You can manually delete old data whenever needed, fulfilling the first requirement.\nNo predefined access pattern: The policy automatically transitions data to cheaper storage classes based on age, regardless of access patterns.\nInstant availability: Nearline storage provides immediate access to data, meeting the third requirement.\nNo retrieval charges: While there are retrieval charges for coldline and archive storage, nearline storage has no retrieval fees, satisfying the fourth requirement."
      },
      {
        "date": "2024-01-09T02:19:00.000Z",
        "voteCount": 4,
        "content": "For sure A, read the documentation"
      },
      {
        "date": "2024-01-08T17:03:00.000Z",
        "voteCount": 2,
        "content": "autoclass is the correct way to handle all business cases"
      },
      {
        "date": "2024-01-06T05:02:00.000Z",
        "voteCount": 3,
        "content": "Create an Object Lifecycle Management policy to modify the storage class for data older than 30 days to nearline, 90 days to coldline, and 365 days to archive storage class. Delete old data as needed."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 250,
    "url": "https://www.examtopics.com/discussions/google/view/130198-exam-professional-data-engineer-topic-1-question-250/",
    "body": "Your company's data platform ingests CSV file dumps of booking and user profile data from upstream sources into Cloud Storage. The data analyst team wants to join these datasets on the email field available in both the datasets to perform analysis. However, personally identifiable information (PII) should not be accessible to the analysts. You need to de-identify the email field in both the datasets before loading them into BigQuery for analysts. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud Data Loss Prevention (Cloud DLP) with masking as the de-identification transformations type.<br>2. Load the booking and user profile data into a BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a pipeline to de-identify the email field by using recordTransformations in Cloud DLP with format-preserving encryption with FFX as the de-identification transformation type.<br>2. Load the booking and user profile data into a BigQuery table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.<br>2. Create a policy tag with the email mask as the data masking rule.<br>3. Assign the policy to the email field in both tables. A<br>4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.<br>2. Create a policy tag with the default masking value as the data masking rule.<br>3. Assign the policy to the email field in both tables.<br>4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts"
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 20,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-25T03:39:00.000Z",
        "voteCount": 8,
        "content": "Format-preserving encryption (FPE) with FFX in Cloud DLP is a strong choice for de-identifying PII like email addresses. FPE maintains the format of the data and ensures that the same input results in the same encrypted output consistently. This means the email fields in both datasets can be encrypted to the same value, allowing for accurate joins in BigQuery while keeping the actual email addresses hidden."
      },
      {
        "date": "2024-01-07T04:36:00.000Z",
        "voteCount": 5,
        "content": "As it states \"You need to de-identify the email field in both the datasets before loading them into BigQuery for analysts\" data masking should not be an option as the data would stored unmasked in BigQuery?"
      },
      {
        "date": "2024-06-14T02:37:00.000Z",
        "voteCount": 2,
        "content": "Option A:\nMasking: Simple masking might not preserve the uniqueness and joinability of the email field, making it difficult to perform accurate joins between datasets.\nOption C and D:\nDynamic Data Masking: These options involve masking the email field dynamically within BigQuery, which does not address the requirement to de-identify data before loading into BigQuery. Additionally, dynamic masking does not prevent access to the actual email data before it is loaded into BigQuery, potentially exposing PII during the data ingestion process."
      },
      {
        "date": "2024-05-16T07:20:00.000Z",
        "voteCount": 1,
        "content": "format-preserving encryption with FFX is required as the analysts want to perform JOINs"
      },
      {
        "date": "2024-02-19T20:57:00.000Z",
        "voteCount": 2,
        "content": "Option B\nhttps://cloud.google.com/sensitive-data-protection/docs/pseudonymization"
      },
      {
        "date": "2024-02-17T04:12:00.000Z",
        "voteCount": 3,
        "content": "A) masking = replace with a surrogate character like # or * = output not unique, so cannot apply joins\nC and D) question specifies to de-identify BEFORE loading into BQ, whereas these options perform dynamic masking IN BigQuery.\n\nTherefore, only valid option is B."
      },
      {
        "date": "2024-01-13T06:56:00.000Z",
        "voteCount": 1,
        "content": "Option C. The need is to just mask the data to Analyst, without modifying the underlying data. Moreover, it's stored on 2 separate tables and the analysts need to be able to perform joins based on the masked data. Dynamic masking is the right module and the right masking rule is email mask (https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options) which guarantees the join capabilities join"
      },
      {
        "date": "2024-01-11T01:27:00.000Z",
        "voteCount": 4,
        "content": "A  wouldn't preserve the email format\nC&amp;D maskedReader roles still grant access to the underlying values.\nthe only option is B"
      },
      {
        "date": "2024-01-11T23:48:00.000Z",
        "voteCount": 1,
        "content": "I dont't know why preserve email format is necessary to perform the join. A could be valid."
      },
      {
        "date": "2024-02-05T17:15:00.000Z",
        "voteCount": 1,
        "content": "masking only replace by specific characters, doing the field not unique and not ready for joins."
      },
      {
        "date": "2024-01-09T02:32:00.000Z",
        "voteCount": 1,
        "content": "I will go for C, because there is a separate type of masking for emails, so whe to use the dafault? https://cloud.google.com/bigquery/docs/column-data-masking-intro#masking_options"
      },
      {
        "date": "2024-01-08T17:11:00.000Z",
        "voteCount": 1,
        "content": "data masking with BQ is correct with email masking rule.\nRef - https://cloud.google.com/bigquery/docs/column-data-masking-intro"
      },
      {
        "date": "2024-02-07T07:37:00.000Z",
        "voteCount": 1,
        "content": "should be correct if they want to access tables and it's not valid for datasets"
      },
      {
        "date": "2024-01-06T12:32:00.000Z",
        "voteCount": 2,
        "content": "why not B?"
      },
      {
        "date": "2024-01-04T17:04:00.000Z",
        "voteCount": 2,
        "content": "- The reason option C works well is that dynamic data masking in BigQuery allows the underlying data to remain unaltered (thus preserving the ability to join on this field), while also preventing analysts from viewing the actual PII. \n- The analysts can query and join the data as needed for their analysis, but when they access the data, the email field will be masked according to the policy tag, and they will only see the masked version."
      },
      {
        "date": "2024-01-03T07:35:00.000Z",
        "voteCount": 1,
        "content": "D. 1. Load the CSV files from Cloud Storage into a BigQuery table, and enable dynamic data masking.\n2. Create a policy tag with the default masking value as the data masking rule.\n3. Assign the policy to the email field in both tables.\n4. Assign the Identity and Access Management bigquerydatapolicy.maskedReader role for the BigQuery tables to the analysts"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 251,
    "url": "https://www.examtopics.com/discussions/google/view/130202-exam-professional-data-engineer-topic-1-question-251/",
    "body": "You have important legal hold documents in a Cloud Storage bucket. You need to ensure that these documents are not deleted or modified. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a retention policy. Lock the retention policy.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet a retention policy. Set the default storage class to Archive for long-term digital preservation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Object Versioning feature. Add a lifecycle rule.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the Object Versioning feature. Create a copy in a bucket in a different region."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T17:12:00.000Z",
        "voteCount": 6,
        "content": "- Setting a retention policy on a Cloud Storage bucket prevents objects from being deleted for the duration of the retention period. \n- Locking the policy makes it immutable, meaning that the retention period cannot be reduced or removed, thus ensuring that the documents cannot be deleted or overwritten until the retention period expires."
      },
      {
        "date": "2024-01-17T06:10:00.000Z",
        "voteCount": 2,
        "content": "Agree. https://cloud.google.com/storage/docs/bucket-lock#overview"
      },
      {
        "date": "2024-02-19T21:07:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T07:04:00.000Z",
        "voteCount": 2,
        "content": "Option A - set retention policy to prevent deletion, lock it to make it immutable (not subject to edits)"
      },
      {
        "date": "2024-01-03T08:16:00.000Z",
        "voteCount": 1,
        "content": "A. Set a retention policy. Lock the retention policy."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 252,
    "url": "https://www.examtopics.com/discussions/google/view/130203-exam-professional-data-engineer-topic-1-question-252/",
    "body": "You are designing a data warehouse in BigQuery to analyze sales data for a telecommunication service provider. You need to create a data model for customers, products, and subscriptions. All customers, products, and subscriptions can be updated monthly, but you must maintain a historical record of all data. You plan to use the visualization layer for current and historical reporting. You need to ensure that the data model is simple, easy-to-use, and cost-effective. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a normalized model with tables for each entity. Use snapshots before updates to track historical data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a normalized model with tables for each entity. Keep all input files in a Cloud Storage bucket to track historical data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a denormalized model with nested and repeated fields. Update the table and use snapshots to track historical data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a denormalized, append-only model with nested and repeated fields. Use the ingestion timestamp to track historical data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T17:18:00.000Z",
        "voteCount": 10,
        "content": "- A denormalized, append-only model simplifies query complexity by eliminating the need for joins. \n- Adding data with an ingestion timestamp allows for easy retrieval of both current and historical states. \n- Instead of updating records, new records are appended, which maintains historical information without the need to create separate snapshots."
      },
      {
        "date": "2024-02-19T21:14:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-17T01:33:00.000Z",
        "voteCount": 1,
        "content": "Straight forward, good for costs"
      },
      {
        "date": "2024-01-09T02:37:00.000Z",
        "voteCount": 1,
        "content": "D looks logical"
      },
      {
        "date": "2024-01-08T15:14:00.000Z",
        "voteCount": 1,
        "content": "Easy, cost effective and no cpmpexity"
      },
      {
        "date": "2024-01-03T08:20:00.000Z",
        "voteCount": 2,
        "content": "D. Create a denormalized, append-only model with nested and repeated fields. Use the ingestion timestamp to track historical data."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 253,
    "url": "https://www.examtopics.com/discussions/google/view/130204-exam-professional-data-engineer-topic-1-question-253/",
    "body": "You are deploying a batch pipeline in Dataflow. This pipeline reads data from Cloud Storage, transforms the data, and then writes the data into BigQuery. The security team has enabled an organizational constraint in Google Cloud, requiring all Compute Engine instances to use only internal IP addresses and no external IP addresses. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that your workers have network tags to access Cloud Storage and BigQuery. Use Dataflow with only internal IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that the firewall rules allow access to Cloud Storage and BigQuery. Use Dataflow with only internal IPs.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a VPC Service Controls perimeter that contains the VPC network and add Dataflow, Cloud Storage, and BigQuery as allowed services in the perimeter. Use Dataflow with only internal IP addresses.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnsure that Private Google Access is enabled in the subnetwork. Use Dataflow with only internal IP addresses.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 4,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T17:33:00.000Z",
        "voteCount": 5,
        "content": "- Private Google Access for services allows VM instances with only internal IP addresses in a VPC network or on-premises networks (via Cloud VPN or Cloud Interconnect) to reach Google APIs and services. \n- When you launch a Dataflow job, you can specify that it should use worker instances without external IP addresses if Private Google Access is enabled on the subnetwork where these instances are launched. \n- This way, your Dataflow workers will be able to access Cloud Storage and BigQuery without violating the organizational constraint of no external IPs."
      },
      {
        "date": "2024-01-06T18:32:00.000Z",
        "voteCount": 2,
        "content": "why not C?"
      },
      {
        "date": "2024-01-08T15:23:00.000Z",
        "voteCount": 4,
        "content": "Even if you create VPC service control, your dataflow worker will run on google compute engine instances with private ips only after policy enforcement. \nWithout external IP addresses, you can still perform administrative and monitoring tasks. \nYou can access your workers by using SSH through the options listed in the preceding list. However, the pipeline cannot access the internet, and internet hosts cannot access your Dataflow workers."
      },
      {
        "date": "2024-01-08T15:24:00.000Z",
        "voteCount": 4,
        "content": "ref - https://cloud.google.com/dataflow/docs/guides/routes-firewall"
      },
      {
        "date": "2024-01-10T20:15:00.000Z",
        "voteCount": 3,
        "content": "VPC Service Controls are typically used to define and enforce security perimeters around APIs and services, restricting their access to a specified set of Google Cloud projects. In this scenario, the security constraint is focused on Compute Engine instances used by Dataflow, and VPC Service Controls might be considered a bit heavy-handed for just addressing the internal IP address requirement."
      },
      {
        "date": "2024-06-08T02:53:00.000Z",
        "voteCount": 1,
        "content": "No way it is C. \nLike the use case for Google VPC Service Controls perimeter is not to establish secure connectivity on its own but rather to control connectivity, like allowing vms within x premise to access, and blocking vms outside premise even if in same VPC from access. \n\nD on the other hand is completely sensical."
      },
      {
        "date": "2024-02-29T23:20:00.000Z",
        "voteCount": 1,
        "content": "According to this documentation: https://cloud.google.com/vpc-service-controls/docs/overview I think the correct answer is C. Take into account the phrase \"organizational constraint\" and the VPC Service Control allow you to do that."
      },
      {
        "date": "2024-02-29T13:01:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/private-google-access\n\n\"VM instances that only have internal IP addresses (no external IP addresses) can use Private Google Access. They can reach the external IP addresses of Google APIs and services.\""
      },
      {
        "date": "2024-01-28T13:10:00.000Z",
        "voteCount": 1,
        "content": "It should be C"
      },
      {
        "date": "2024-01-13T07:13:00.000Z",
        "voteCount": 1,
        "content": "Option D, as GCP001 said"
      },
      {
        "date": "2024-01-13T07:14:00.000Z",
        "voteCount": 1,
        "content": "Missclicked the answer &lt;.&lt;"
      },
      {
        "date": "2024-01-08T15:19:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/dataflow/docs/guides/routes-firewall"
      },
      {
        "date": "2024-01-03T08:27:00.000Z",
        "voteCount": 1,
        "content": "C. Create a VPC Service Controls perimeter that contains the VPC network and add Dataflow, Cloud Storage, and BigQuery as allowed services in the perimeter. Use Dataflow with only internal IP addresses."
      },
      {
        "date": "2024-01-10T20:15:00.000Z",
        "voteCount": 2,
        "content": "C is wrong. Option D is simple and straight forward. VPC Service Controls are typically used to define and enforce security perimeters around APIs and services, restricting their access to a specified set of Google Cloud projects. In this scenario, the security constraint is focused on Compute Engine instances used by Dataflow, and VPC Service Controls might be considered a bit heavy-handed for just addressing the internal IP address requirement."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 254,
    "url": "https://www.examtopics.com/discussions/google/view/130205-exam-professional-data-engineer-topic-1-question-254/",
    "body": "You are running a Dataflow streaming pipeline, with Streaming Engine and Horizontal Autoscaling enabled. You have set the maximum number of workers to 1000. The input of your pipeline is Pub/Sub messages with notifications from Cloud Storage. One of the pipeline transforms reads CSV files and emits an element for every CSV line. The job performance is low, the pipeline is using only 10 workers, and you notice that the autoscaler is not spinning up additional workers. What should you do to improve performance?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable Vertical Autoscaling to let the pipeline use larger workers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the pipeline code, and introduce a Reshuffle step to prevent fusion.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the job to increase the maximum number of workers.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow Prime, and enable Right Fitting to increase the worker resources."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T17:39:00.000Z",
        "voteCount": 12,
        "content": "- Fusion optimization in Dataflow can lead to steps being \"fused\" together, which can sometimes hinder parallelization. \n- Introducing a Reshuffle step can prevent fusion and force the distribution of work across more workers. \n- This can be an effective way to improve parallelism and potentially trigger the autoscaler to increase the number of workers."
      },
      {
        "date": "2024-08-10T05:02:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dataflow/docs/pipeline-lifecycle#prevent_fusion"
      },
      {
        "date": "2024-06-08T03:15:00.000Z",
        "voteCount": 1,
        "content": "Right fitting is for declaration, declaring the correct resources will not help. Reshuffling step is what can prevent fusion which can lead to unused workers."
      },
      {
        "date": "2024-02-17T05:45:00.000Z",
        "voteCount": 3,
        "content": "Fusion occurs when multiple transformations are fused into a single stage, which can limit parallelism and hinder performance, especially in streaming pipelines. By introducing a Reshuffle step, you break fusion and allow for better parallelism."
      },
      {
        "date": "2024-02-10T08:20:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dataflow/docs/guides/right-fitting"
      },
      {
        "date": "2024-01-08T15:30:00.000Z",
        "voteCount": 3,
        "content": "Problem is performnace and not using all workers properly, https://cloud.google.com/dataflow/docs/pipeline-lifecycle#fusion_optimization"
      },
      {
        "date": "2024-01-03T08:34:00.000Z",
        "voteCount": 1,
        "content": "D. Use Dataflow Prime, and enable Right Fitting to increase the worker resources."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 255,
    "url": "https://www.examtopics.com/discussions/google/view/130206-exam-professional-data-engineer-topic-1-question-255/",
    "body": "You have an Oracle database deployed in a VM as part of a Virtual Private Cloud (VPC) network. You want to replicate and continuously synchronize 50 tables to BigQuery. You want to minimize the need to manage infrastructure. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Apache Kafka in the same VPC network, use Kafka Connect Oracle Change Data Capture (CDC), and Dataflow to stream the Kafka topic to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Pub/Sub subscription to write to BigQuery directly. Deploy the Debezium Oracle connector to capture changes in the Oracle database, and sink to the Pub/Sub topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDeploy Apache Kafka in the same VPC network, use Kafka Connect Oracle change data capture (CDC), and the Kafka Connect Google BigQuery Sink Connector.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-04T17:49:00.000Z",
        "voteCount": 10,
        "content": "- Datastream is a serverless and easy-to-use change data capture (CDC) and replication service. \n- You would create a Datastream service that sources from your Oracle database and targets BigQuery, with private connectivity configuration to the same VPC. \n- This option is designed to minimize the need to manage infrastructure and is a fully managed service."
      },
      {
        "date": "2024-02-19T21:36:00.000Z",
        "voteCount": 2,
        "content": "D.  Datastream"
      },
      {
        "date": "2024-01-03T08:38:00.000Z",
        "voteCount": 2,
        "content": "D. Create a Datastream service from Oracle to BigQuery, use a private connectivity configuration to the same VPC network, and a connection profile to BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 256,
    "url": "https://www.examtopics.com/discussions/google/view/130209-exam-professional-data-engineer-topic-1-question-256/",
    "body": "You are deploying an Apache Airflow directed acyclic graph (DAG) in a Cloud Composer 2 instance. You have incoming files in a Cloud Storage bucket that the DAG processes, one file at a time. The Cloud Composer instance is deployed in a subnetwork with no Internet access. Instead of running the DAG based on a schedule, you want to run the DAG in a reactive way every time a new file is received. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable Private Google Access in the subnetwork, and set up Cloud Storage notifications to a Pub/Sub topic.<br>2. Create a push subscription that points to the web server URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable the Cloud Composer API, and set up Cloud Storage notifications to trigger a Cloud Function.<br>2. Write a Cloud Function instance to call the DAG by using the Cloud Composer API and the web server URL.<br>3. Use VPC Serverless Access to reach the web server URL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.<br>2. Create a Private Service Connect (PSC) endpoint.<br>3. Write a Cloud Function that connects to the Cloud Composer cluster through the PSC endpoint.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.<br>2. Write a Cloud Function instance to call the DAG by using the Airflow REST API and the web server URL.<br>3. Use VPC Serverless Access to reach the web server URL."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T07:12:00.000Z",
        "voteCount": 10,
        "content": "- Enable Airflow REST API: In Cloud Composer, enable the \"Airflow web server\" option.\n- Set Up Cloud Storage Notifications: Create a notification for new files, routing to a Cloud Function.\n- Create PSC Endpoint: Establish a PSC endpoint for Cloud Composer.\n- Write Cloud Function: Code the function to use the Airflow REST API (via PSC endpoint) to trigger the DAG.\n\n========\nWhy not Option D\n- Using the web server URL directly wouldn't work without internet access or a direct path to the web server."
      },
      {
        "date": "2024-01-19T23:47:00.000Z",
        "voteCount": 4,
        "content": "Why not B, use Cloud Composer API"
      },
      {
        "date": "2024-10-08T01:01:00.000Z",
        "voteCount": 1,
        "content": "This is A, as steve_pegleg says, there is no way to connect the cloud function to the Airflow instance, without first enabling private access. The pubsub pattern makes sense in this context."
      },
      {
        "date": "2024-08-07T05:58:00.000Z",
        "voteCount": 3,
        "content": "This is the guidance how to use method in A:\nhttps://cloud.google.com/composer/docs/composer-2/triggering-gcf-pubsub\n\"In this specific example, you create a Cloud Function and deploy two DAGs. The first DAG pulls Pub/Sub messages and triggers the second DAG according to the Pub/Sub message content.\"\n\nFor C &amp; D, this guidance says it can't be done when you have Private or VPS Service Controls set up:\nhttps://cloud.google.com/composer/docs/composer-2/triggering-with-gcf#check_your_environments_networking_configuration\n\"This solution does not work in Private IP and VPC Service Controls configurations because it is not possible to configure connectivity from Cloud Functions to the Airflow web server in these configurations.\""
      },
      {
        "date": "2024-05-19T10:57:00.000Z",
        "voteCount": 3,
        "content": "C is not correct because \"this solution does not work in Private IP and VPC Service Controls configurations because it is not possible to configure connectivity from Cloud Functions to the Airflow web server in these configurations\".\nhttps://cloud.google.com/composer/docs/how-to/using/triggering-with-gcf\nThe correct answer is A using Pub/Sub https://cloud.google.com/composer/docs/composer-2/triggering-gcf-pubsub"
      },
      {
        "date": "2024-03-27T07:16:00.000Z",
        "voteCount": 1,
        "content": "Why not Option C? C involves creating a Private Service Connect (PSC) endpoint, which, while viable for creating private connections to Google services, adds complexity and might not be required when simpler solutions like VPC Serverless Access (as in Option D) can suffice."
      },
      {
        "date": "2024-03-27T07:18:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/vpc/docs/serverless-vpc-access: Serverless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud (VPC) network from serverless environments such as Cloud Run, App Engine, or Cloud Functions"
      },
      {
        "date": "2024-03-24T09:09:00.000Z",
        "voteCount": 1,
        "content": "The answer should be D \nServerless VPC Access makes it possible for you to connect directly to your Virtual Private Cloud (VPC) network from serverless environments such as Cloud Run, App Engine, or Cloud Functions. Configuring Serverless VPC Access allows your serverless environment to send requests to your VPC network by using internal DNS and internal IP addresses (as defined by RFC 1918 and RFC 6598). The responses to these requests also use your internal network.\nYou can use Serverless VPC Access to access Compute Engine VM instances, Memorystore instances, and any other resources with internal DNS or internal IP address.\n(Reference: https://cloud.google.com/vpc/docs/serverless-vpc-access)\nWhen you use Airflow Rest API to tigger the job, the url is based on the private IP address of Cloud Composer Instance, so you need to use Serverless VPC Access for it."
      },
      {
        "date": "2024-03-24T09:10:00.000Z",
        "voteCount": 1,
        "content": "Why not C:\nThe reference here (https://cloud.google.com/vpc/docs/private-service-connect#published-services) limits the available use cases:\nPrivate Service Connect supports access to the following types of managed services:\nPublished VPC-hosted services, which include the following:\n\tGoogle published services, such as Apigee or the GKE control plane\n\tThird-party published services provided by Private Service Connect partners\n\tIntra-organization published services, where the consumer and producer might be two different VPC networks within the same company\nGoogle APIs, such as Cloud Storage or BigQuery\n\nUnfortunately your airflow Rest API is not published as a service in the list, so you can not use it \nThis is also one of the reasons why you should reject A"
      },
      {
        "date": "2024-03-24T09:11:00.000Z",
        "voteCount": 1,
        "content": "B is not appropriate while Cloud Composer API can really execute Airflow command\uff0cbut It\u2019s not via web server Url to run a DAG in this case, and I doubt if it is really possible"
      },
      {
        "date": "2024-01-13T07:30:00.000Z",
        "voteCount": 1,
        "content": "Option C, raaad explained well why"
      },
      {
        "date": "2024-01-03T08:56:00.000Z",
        "voteCount": 1,
        "content": "C. \n1. Enable the Airflow REST API, and set up Cloud Storage notifications to trigger a Cloud Function instance.\n2. Create a Private Service Connect (PSC) endpoint.\n3. Write a Cloud Function that connects to the Cloud Composer cluster through the PSC endpoint."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 257,
    "url": "https://www.examtopics.com/discussions/google/view/130210-exam-professional-data-engineer-topic-1-question-257/",
    "body": "You are planning to use Cloud Storage as part of your data lake solution. The Cloud Storage bucket will contain objects ingested from external systems. Each object will be ingested once, and the access patterns of individual objects will be random. You want to minimize the cost of storing and retrieving these objects. You want to ensure that any cost optimization efforts are transparent to the users and applications. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket with Autoclass enabled.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object age reaches 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Cloud Storage bucket with an Object Lifecycle Management policy to transition objects from Standard to Coldline storage class if an object is not live.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Cloud Storage buckets. Use the Standard storage class for the first bucket, and use the Coldline storage class for the second bucket. Migrate objects from the first bucket to the second bucket after 30 days."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 9,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-02T11:36:00.000Z",
        "voteCount": 2,
        "content": "Thanks to you guys, I found out about this feature :D \n\nThe feature was released on November 3, 2023. Note that enabling Autoclass on an existing bucket incurs additional charges."
      },
      {
        "date": "2024-02-19T21:42:00.000Z",
        "voteCount": 1,
        "content": "A. Autoclass"
      },
      {
        "date": "2024-01-13T07:33:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-05T07:21:00.000Z",
        "voteCount": 4,
        "content": "- Autoclass automatically analyzes access patterns of objects and automatically transitions them to the most cost-effective storage class within Standard, Nearline, Coldline, or Archive.\n- This eliminates the need for manual intervention or setting specific age thresholds.\n- No user or application interaction is required, ensuring transparency."
      },
      {
        "date": "2024-01-03T09:09:00.000Z",
        "voteCount": 1,
        "content": "A. Create a Cloud Storage bucket with Autoclass enabled."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 258,
    "url": "https://www.examtopics.com/discussions/google/view/130211-exam-professional-data-engineer-topic-1-question-258/",
    "body": "You have several different file type data sources, such as Apache Parquet and CSV. You want to store the data in Cloud Storage. You need to set up an object sink for your data that allows you to use your own encryption keys. You want to use a GUI-based solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Storage Transfer Service to move files into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion to move files into Cloud Storage.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to move files into Cloud Storage.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service to move files into BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T07:25:00.000Z",
        "voteCount": 9,
        "content": "- Cloud Data Fusion is a fully managed, code-free, GUI-based data integration service that allows you to visually connect, transform, and move data between various sources and sinks. - It supports various file formats and can write to Cloud Storage. \n- You can configure it to use Customer-Managed Encryption Keys (CMEK) for the buckets where it writes data."
      },
      {
        "date": "2024-01-19T23:55:00.000Z",
        "voteCount": 4,
        "content": "Agree. https://cloud.google.com/data-fusion/docs/how-to/customer-managed-encryption-keys#create-instance"
      },
      {
        "date": "2024-01-20T17:54:00.000Z",
        "voteCount": 6,
        "content": "Even though storage transfer service can be used in GUI, it does not support CMEK which is required in this question. \n\n\"Storage Transfer Service does not encrypt data on your behalf, such as in customer-managed encryption keys (CMEK). We only encrypt data in transit.\"\n\nRef: https://cloud.google.com/storage-transfer/docs/on-prem-security"
      },
      {
        "date": "2024-10-10T02:29:00.000Z",
        "voteCount": 1,
        "content": "I acknolwedge that the question wants the answer to be B, so I'm calling it as B. I don't like this though, as can't we just create a bucket with a CMEK upfront and then use the storage transfer service? It would be easier and cheaper, and achieve the same thing.\nThe language of \"sink\" strongly suggests to me they intend this to be B though, as datafusion uses that terminology, and the CMEK thing is probably indicating that datafusion can encrypt for you with CMEK."
      },
      {
        "date": "2024-03-19T13:03:00.000Z",
        "voteCount": 1,
        "content": "option 8"
      },
      {
        "date": "2024-02-24T05:08:00.000Z",
        "voteCount": 1,
        "content": "B just because tranfer service does not support CMEK\nA: \n* GUI + encryption but no CMEK.\nB:\n* Its GUI ETL + support CMEK but not sure why you need an ETL tool for transfering something once? (No scheduling or event-driven trigger is mentioned)"
      },
      {
        "date": "2024-02-11T03:17:00.000Z",
        "voteCount": 1,
        "content": "B if data fusion creates the bucket. We could create the bucket and associate, in this case A is better."
      },
      {
        "date": "2024-01-11T02:37:00.000Z",
        "voteCount": 3,
        "content": "A. Use Storage Transfer Service to move files into Cloud Storage.\n move files into Cloud Storage should be Storage Transfer Service \nCloud Data Fusion is like using a tank to kill an ant"
      },
      {
        "date": "2024-01-03T09:12:00.000Z",
        "voteCount": 1,
        "content": "B. Use Cloud Data Fusion to move files into Cloud Storage."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 259,
    "url": "https://www.examtopics.com/discussions/google/view/130212-exam-professional-data-engineer-topic-1-question-259/",
    "body": "Your business users need a way to clean and prepare data before using the data for analysis. Your business users are less technically savvy and prefer to work with graphical user interfaces to define their transformations. After the data has been transformed, the business users want to perform their analysis directly in a spreadsheet. You need to recommend a solution that they can use. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow to clean the data, and write the results to BigQuery. Analyze the data by using Looker Studio."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 21,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T03:11:00.000Z",
        "voteCount": 8,
        "content": "If only all the questions were like this..."
      },
      {
        "date": "2024-01-05T07:31:00.000Z",
        "voteCount": 5,
        "content": "- Allow business users to perform their analysis in a familiar spreadsheet interface via Connected Sheets."
      },
      {
        "date": "2024-05-19T11:06:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/connected-sheets\nhttps://cloud.google.com/dataprep"
      },
      {
        "date": "2024-03-19T13:01:00.000Z",
        "voteCount": 1,
        "content": "vote A"
      },
      {
        "date": "2024-01-13T07:35:00.000Z",
        "voteCount": 1,
        "content": "Clearly option A"
      },
      {
        "date": "2024-01-03T09:26:00.000Z",
        "voteCount": 3,
        "content": "A. Use Dataprep to clean the data, and write the results to BigQuery. Analyze the data by using Connected Sheets."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 260,
    "url": "https://www.examtopics.com/discussions/google/view/130213-exam-professional-data-engineer-topic-1-question-260/",
    "body": "You have two projects where you run BigQuery jobs:<br>\u2022\tOne project runs production jobs that have strict completion time SLAs. These are high priority jobs that must have the required compute resources available when needed. These jobs generally never go below a 300 slot utilization, but occasionally spike up an additional 500 slots.<br>\u2022\tThe other project is for users to run ad-hoc analytical queries. This project generally never uses more than 200 slots at a time. You want these ad-hoc queries to be billed based on how much data users scan rather than by slot capacity.<br><br>You need to ensure that both projects have the appropriate compute resources available. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a single Enterprise Edition reservation for both projects. Set a baseline of 300 slots. Enable autoscaling up to 700 slots.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two reservations, one for each of the projects. For the SLA project, use an Enterprise Edition with a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, configure on-demand billing.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, set a reservation baseline of 0 slots and set the ignore idle slots flag to False.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate two Enterprise Edition reservations, one for each of the projects. For the SLA project, set a baseline of 800 slots. For the ad-hoc project, enable autoscaling up to 200 slots."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T07:38:00.000Z",
        "voteCount": 7,
        "content": "- The SLA project gets a dedicated reservation with autoscaling to handle spikes, ensuring it meets its strict completion time SLAs. \n- The ad-hoc project uses on-demand billing, which means it will be billed based on the amount of data scanned rather than slot capacity, fitting the billing preference for ad-hoc queries."
      },
      {
        "date": "2024-03-12T19:00:00.000Z",
        "voteCount": 2,
        "content": "Critical jobs can spike up to 800 slots, making option B wrong"
      },
      {
        "date": "2024-03-20T11:59:00.000Z",
        "voteCount": 2,
        "content": "in this context, \"enable autoscaling up to 500 slots\" means that the system can add up to 500 slots beyond the baseline"
      },
      {
        "date": "2024-03-27T06:03:00.000Z",
        "voteCount": 1,
        "content": "I dont think thats correct. \"up to 500 slots\" means the maximum limit is 500 slots - it doesnt specify autoscaling an additional 500 slots."
      },
      {
        "date": "2024-04-07T20:20:00.000Z",
        "voteCount": 2,
        "content": "Separate Reservations: This approach provides tailored resource allocation and billing models to match the distinct needs of each project.\nSLA Project Reservation:\nEnterprise Edition: Guarantees consistent slot availability for your production jobs.\nBaseline of 300 slots: Ensures resources are always available to meet your core usage at a predictable cost.\nAutoscaling up to 500 slots: Accommodates bursts in workload while controlling costs.\nAd-hoc Project On-demand:\nOn-demand billing: Charges based on data scanned, ideal for unpredictable and variable query patterns by your ad-hoc users."
      },
      {
        "date": "2024-03-27T06:04:00.000Z",
        "voteCount": 1,
        "content": "Note, Option A states autoscale \"up to\" (not an additional) 500 slots, whereas the requirement is 800 slots. Making option D the only viable option."
      },
      {
        "date": "2024-03-27T07:27:00.000Z",
        "voteCount": 3,
        "content": "Scratch this - Option B: https://cloud.google.com/bigquery/docs/slots-autoscaling-intro#using_reservations_with_baseline_and_autoscaling_slots \nBaseline Slots and AutoScaling Slots are treated as two different entities in the documentation. Therefore B is right despite the horrific wording of the answers."
      },
      {
        "date": "2024-03-24T03:59:00.000Z",
        "voteCount": 3,
        "content": "\"You want these ad-hoc queries to be billed based on how much data users scan rather than by slot capacity.\" So D is out. Choose B"
      },
      {
        "date": "2024-03-20T11:59:00.000Z",
        "voteCount": 3,
        "content": "B \n\"enable autoscaling up to 500 slots\" means that the system can add up to 500 slots beyond the baseline as needed, effectively allowing for a total of 800 slots (300 baseline + 500 autoscaled) during peak usage."
      },
      {
        "date": "2024-03-19T12:59:00.000Z",
        "voteCount": 1,
        "content": "500 (additional) +300 = 800, so answer is D"
      },
      {
        "date": "2024-02-20T00:40:00.000Z",
        "voteCount": 3,
        "content": "Option B.\n\nNot D because \"In Project-2, ad-hoc queries need to be billed based on how much data users scan rather than by slot capacity.\""
      },
      {
        "date": "2024-01-18T19:13:00.000Z",
        "voteCount": 1,
        "content": "Considering the emphasis on strict completion time SLA's.I go with option D. However I think both B and D are not the best solution here."
      },
      {
        "date": "2024-01-13T07:39:00.000Z",
        "voteCount": 2,
        "content": "Option B - first project works well with dedicated reservation and autoscaling. The second one requires on demand billing, as per question requires."
      },
      {
        "date": "2024-01-10T13:50:00.000Z",
        "voteCount": 3,
        "content": "\"These jobs generally never go below a 300 slot utilization, but occasionally spike up an additional 500 slots.\" -&gt; if it spikes up an ADITIONAL 500 slots, on top of the regular 300, shouldn't we reserve at a minimum 800? open to explanations as to why this is not the case."
      },
      {
        "date": "2024-01-03T09:29:00.000Z",
        "voteCount": 2,
        "content": "B. Create two reservations, one for each of the projects. For the SLA project, use an Enterprise Edition with a baseline of 300 slots and enable autoscaling up to 500 slots. For the ad-hoc project, configure on-demand billing."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 261,
    "url": "https://www.examtopics.com/discussions/google/view/129906-exam-professional-data-engineer-topic-1-question-261/",
    "body": "You want to migrate your existing Teradata data warehouse to BigQuery. You want to move the historical data to BigQuery by using the most efficient method that requires the least amount of programming, but local storage space on your existing data warehouse is limited. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service by using the Java Database Connectivity (JDBC) driver with FastExport connection.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Teradata Parallel Transporter (TPT) export script to export the historical data, and import to BigQuery by using the bq command-line tool.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery Data Transfer Service with the Teradata Parallel Transporter (TPT) tbuild utility.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a script to export the historical data, and upload in batches to Cloud Storage. Set up a BigQuery Data Transfer Service instance from Cloud Storage to BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 19,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T07:42:00.000Z",
        "voteCount": 9,
        "content": "- Reduced Local Storage: By using FastExport, data is directly streamed from Teradata to BigQuery without the need for local storage, addressing your storage limitations.\n- Minimal Programming: BigQuery Data Transfer Service offers a user-friendly interface, eliminating the need for extensive scripting or coding."
      },
      {
        "date": "2024-01-20T21:21:00.000Z",
        "voteCount": 7,
        "content": "Agree. https://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method\nExtraction using a JDBC driver with FastExport connection. If there are constraints on the local storage space available for extracted files, or if there is some reason you can't use TPT, then use this extraction method."
      },
      {
        "date": "2023-12-30T11:32:00.000Z",
        "voteCount": 6,
        "content": "https://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method\n\nLack of local storage pushes this to JDBC driver"
      },
      {
        "date": "2024-04-08T13:16:00.000Z",
        "voteCount": 1,
        "content": "Extraction using a JDBC driver with FastExport connection. If there are constraints on the local storage space available for extracted files, or if there is some reason you can't use TPT, then use this extraction method.\nhttps://cloud.google.com/bigquery/docs/migration/teradata-overview#extraction_method"
      },
      {
        "date": "2024-01-13T07:41:00.000Z",
        "voteCount": 3,
        "content": "Option A, the JDBC driver is the key to solve the limited local storage"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 262,
    "url": "https://www.examtopics.com/discussions/google/view/130214-exam-professional-data-engineer-topic-1-question-262/",
    "body": "You are on the data governance team and are implementing security requirements. You need to encrypt all your data in BigQuery by using an encryption key managed by your team. You must implement a mechanism to generate and store encryption material only on your on-premises hardware security module (HSM). You want to rely on Google managed solutions. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the encryption key in the on-premises HSM, and import it into a Cloud Key Management Service (Cloud KMS) key. Associate the created Cloud KMS key while creating the BigQuery resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the encryption key in the on-premises HSM and link it to a Cloud External Key Manager (Cloud EKM) key. Associate the created Cloud KMS key while creating the BigQuery resources.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the encryption key in the on-premises HSM. Create BigQuery resources and encrypt data while ingesting them into BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 23,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T08:17:00.000Z",
        "voteCount": 17,
        "content": "- Cloud EKM allows you to use encryption keys managed in external key management systems, including on-premises HSMs, while using Google Cloud services. \n- This means that the key material remains in your control and environment, and Google Cloud services use it via the Cloud EKM integration. \n- This approach aligns with the need to generate and store encryption material only on your on-premises HSM and is the correct way to integrate such keys with BigQuery.\n\n======\nWhy not Option C\n- Cloud HSM is a fully managed service by Google Cloud that provides HSMs for your cryptographic needs. However, it's a cloud-based solution, and the keys generated or managed in Cloud HSM are not stored on-premises. This option doesn't align with the requirement to use only on-premises HSM for key storage."
      },
      {
        "date": "2024-08-10T04:10:00.000Z",
        "voteCount": 1,
        "content": "Option B, I agree with Raaad on the approach"
      },
      {
        "date": "2024-05-17T02:46:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/kms/docs/ekm#ekm-management-mode"
      },
      {
        "date": "2024-05-17T02:45:00.000Z",
        "voteCount": 1,
        "content": "B- https://cloud.google.com/kms/docs/ekm#ekm-management-mode\nCoordinated external keys are made possible by EKM via VPC connections that use EKM key management from Cloud KMS. If your EKM supports the Cloud EKM control plane, then you can enable EKM key management from Cloud KMS for your EKM via VPC connections to create coordinated external keys. With EKM key management from Cloud KMS enabled, Cloud EKM can request the following changes in your EKM:"
      },
      {
        "date": "2024-01-13T07:44:00.000Z",
        "voteCount": 2,
        "content": "Option B, I agree with Raaad on the approach"
      },
      {
        "date": "2024-01-03T09:44:00.000Z",
        "voteCount": 3,
        "content": "C. Create the encryption key in the on-premises HSM, and import it into Cloud Key Management Service (Cloud HSM) key. Associate the created Cloud HSM key while creating the BigQuery resources."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 263,
    "url": "https://www.examtopics.com/discussions/google/view/130215-exam-professional-data-engineer-topic-1-question-263/",
    "body": "You maintain ETL pipelines. You notice that a streaming pipeline running on Dataflow is taking a long time to process incoming data, which causes output delays. You also noticed that the pipeline graph was automatically optimized by Dataflow and merged into one step. You want to identify where the potential bottleneck is occurring. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tInsert output sinks after each key processing step, and observe the writing throughput of each block.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLog debug information in each ParDo function, and analyze the logs at execution time.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tVerify that the Dataflow service accounts have appropriate permissions to write the processed data to the output sinks."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T08:21:00.000Z",
        "voteCount": 8,
        "content": "- The Reshuffle operation is used in Dataflow pipelines to break fusion and redistribute elements, which can sometimes help improve parallelization and identify bottlenecks. \n- By inserting Reshuffle after each processing step and observing the pipeline's performance in the Dataflow console, you can potentially identify stages that are disproportionately slow or stalled. \n- This can help in pinpointing the step where the bottleneck might be occurring."
      },
      {
        "date": "2024-02-11T02:13:00.000Z",
        "voteCount": 1,
        "content": "ince we don't know for sure if fusion is the culprit,  detailed debug logging is still the top choice to find the precise slow operation(s)."
      },
      {
        "date": "2024-10-06T00:14:00.000Z",
        "voteCount": 1,
        "content": "B identifies where the problem lies."
      },
      {
        "date": "2024-02-20T06:53:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-02-10T10:23:00.000Z",
        "voteCount": 2,
        "content": "It should be C"
      },
      {
        "date": "2024-01-30T08:06:00.000Z",
        "voteCount": 1,
        "content": "The best option is B\nBecause create additional output to capturing and processing error data, will get error each step that allows you to observe the writing throughput of each block, which can help identify specific processing steps causing bottlenecks.\n\nOption A also is valid but can not directly address all bottlenecks, especially if the graph was merged."
      },
      {
        "date": "2024-01-09T05:37:00.000Z",
        "voteCount": 3,
        "content": "From the Dataflow documentation: \"There are a few cases in your pipeline where you may want to prevent the Dataflow service from performing fusion optimizations. These are cases in which the Dataflow service might incorrectly guess the optimal way to fuse operations in the pipeline, which could limit the Dataflow service's ability to make use of all available workers.\nYou can insert a Reshuffle step. Reshuffle prevents fusion, checkpoints the data, and performs deduplication of records. Reshuffle is supported by Dataflow even though it is marked deprecated in the Apache Beam documentation.\""
      },
      {
        "date": "2024-01-03T10:09:00.000Z",
        "voteCount": 2,
        "content": "A. Insert a Reshuffle operation after each processing step, and monitor the execution details in the Dataflow console."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 264,
    "url": "https://www.examtopics.com/discussions/google/view/130360-exam-professional-data-engineer-topic-1-question-264/",
    "body": "You are running your BigQuery project in the on-demand billing model and are executing a change data capture (CDC) process that ingests data. The CDC process loads 1 GB of data every 10 minutes into a temporary table, and then performs a merge into a 10 TB target table. This process is very scan intensive and you want to explore options to enable a predictable cost model. You need to create a BigQuery reservation based on utilization information gathered from BigQuery Monitoring and apply the reservation to the CDC process. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery reservation for the dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery reservation for the job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery reservation for the service account running the job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery reservation for the project.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 6,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T23:23:00.000Z",
        "voteCount": 5,
        "content": "Reserve assignments\nTo use the slot capacity you purchased, assign projects, folders, or organizations to a reservation. When a job in a project runs, it uses slots from the assigned reservation. Resources can inherit roles from their parents in the resource hierarchy. Even if a project is not assigned to a reservation, it inherits the assignment from the parent folder or organization, if any. If a project does not have an assigned or inherited reservation, the job uses on-demand pricing. For more information about the resource hierarchy, see Organizing BigQuery Resources ."
      },
      {
        "date": "2024-01-13T07:51:00.000Z",
        "voteCount": 5,
        "content": "Option D, reservation can't be applied to resources lower than projects (only to Org, folders or projects)"
      },
      {
        "date": "2024-01-20T21:44:00.000Z",
        "voteCount": 5,
        "content": "Seems correct. https://cloud.google.com/bigquery/docs/reservations-intro#understand_workload_management"
      },
      {
        "date": "2024-08-16T16:35:00.000Z",
        "voteCount": 1,
        "content": "The correct answer is B. Create a BigQuery reservation at the job level.\n\nCreate a BigQuery reservation at the job level: This is the most suitable option. By creating a job-level reservation, you can allocate resources specifically to the CDC process and improve the accuracy of cost forecasting.\n\nSteps to create and apply a BigQuery reservation:\nIdentify the job: Clearly identify the job that executes the CDC process.\nCreate a reservation: Use the BigQuery console or API to create a reservation, specifying the job's label, query text, and other details.\nApply the reservation: Assign the created reservation to the job that executes the CDC process."
      },
      {
        "date": "2024-08-10T03:59:00.000Z",
        "voteCount": 1,
        "content": "Are these questions really useful andd coming in GDE Exam? Anyone appeared recently and passed ?"
      },
      {
        "date": "2024-05-18T23:51:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas.\nQuotas can be applied on Project or User Level"
      },
      {
        "date": "2024-05-17T03:27:00.000Z",
        "voteCount": 2,
        "content": "D- choose the correct project and apply the task type background:https://cloud.google.com/bigquery/docs/reservations-intro?hl=fr#assignments"
      },
      {
        "date": "2024-02-20T19:13:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-08T12:12:00.000Z",
        "voteCount": 3,
        "content": "D.\nReservation is on project, folder or organisation level."
      },
      {
        "date": "2024-01-05T08:37:00.000Z",
        "voteCount": 3,
        "content": "C or D ?? \n\nOption C (service account) allows you to target the reservation specifically to the CDC process or any other jobs run by that service account. This is particularly useful if you have multiple processes running in the project with different performance or cost requirements.\n\nOption D (project) applies the reservation across all jobs in the project, which is a broader approach. If the CDC process is the primary or sole job running in the project and you want all jobs to share the same reservation, then this option might be more straightforward."
      },
      {
        "date": "2024-01-04T18:59:00.000Z",
        "voteCount": 1,
        "content": "D. Create a BigQuery reservation for the project."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 265,
    "url": "https://www.examtopics.com/discussions/google/view/130361-exam-professional-data-engineer-topic-1-question-265/",
    "body": "You are designing a fault-tolerant architecture to store data in a regional BigQuery dataset. You need to ensure that your application is able to recover from a corruption event in your tables that occurred within the past seven days. You want to adopt managed services with the lowest RPO and most cost-effective solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAccess historical data by using time travel in BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExport the data from BigQuery into a new table that excludes the corrupted data",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table snapshot on a daily basis.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your data to multi-region BigQuery buckets."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 17,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T08:39:00.000Z",
        "voteCount": 11,
        "content": "- Lowest RPO: Time travel offers point-in-time recovery for the past seven days by default, providing the shortest possible recovery point objective (RPO) among the given options. You can recover data to any state within that window.\n- No Additional Costs: Time travel is a built-in feature of BigQuery, incurring no extra storage or operational costs.\n- Managed Service: BigQuery handles time travel automatically, eliminating manual backup and restore processes."
      },
      {
        "date": "2024-02-11T02:16:00.000Z",
        "voteCount": 1,
        "content": "BigQuery's time travel feature typically retains history up to 7 days. However, if the corruption affects the underlying data for an extended period, the 7-day window might not be long enough."
      },
      {
        "date": "2024-04-08T13:59:00.000Z",
        "voteCount": 1,
        "content": "Meets Recovery Needs: Table snapshots provide point-in-time copies of your data, allowing you to restore data from any point within the last seven days, effectively addressing the corruption event recovery requirement.\nLow RPO: With daily snapshots, your Recovery Point Objective (RPO) is at most 24 hours, satisfying the need for a low RPO.\nManaged Service: Table snapshots are a fully managed service within BigQuery, aligning with your preference.\nCost-Effective: Snapshots only store the changes from the base table, minimizing storage costs compared to full table copies."
      },
      {
        "date": "2024-03-16T22:45:00.000Z",
        "voteCount": 1,
        "content": "vote for A"
      },
      {
        "date": "2024-02-20T19:14:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T07:54:00.000Z",
        "voteCount": 3,
        "content": "Option A, raaad explanation is perfect"
      },
      {
        "date": "2024-01-04T19:00:00.000Z",
        "voteCount": 1,
        "content": "A. Access historical data by using time travel in BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 266,
    "url": "https://www.examtopics.com/discussions/google/view/130217-exam-professional-data-engineer-topic-1-question-266/",
    "body": "You are building a streaming Dataflow pipeline that ingests noise level data from hundreds of sensors placed near construction sites across a city. The sensors measure noise level every ten seconds, and send that data to the pipeline when levels reach above 70 dBA. You need to detect the average noise level from a sensor when data is received for a duration of more than 30 minutes, but the window ends when no data has been received for 15 minutes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse session windows with a 15-minute gap duration.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse session windows with a 30-minute gap duration.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse hopping windows with a 15-minute window, and a thirty-minute period.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse tumbling windows with a 15-minute window and a fifteen-minute .withAllowedLateness operator."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 8,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 5,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-19T06:47:00.000Z",
        "voteCount": 9,
        "content": "to detect average noise levels from sensors, the best approach is to use session windows with a 15-minute gap duration (Option A). Session windows are ideal for cases like this where the events (sensor data) are sporadic. They group events that occur within a certain time interval (15 minutes in your case) and a new window is started if no data is received for the duration of the gap. This matches your requirement to end the window when no data is received for 15 minutes, ensuring that the average noise level is calculated over periods of continuous data"
      },
      {
        "date": "2024-02-16T13:57:00.000Z",
        "voteCount": 2,
        "content": "But you are not fulfilling this requirement \"You need to detect the average noise level from a sensor when data is received for a duration of more than 30 minutes\". I would say C"
      },
      {
        "date": "2024-02-08T08:23:00.000Z",
        "voteCount": 7,
        "content": "You need a window that start when data for a sensor arrives and end when there's a gap in data. That would rule out hopping and tumbling windows. \n\n- &gt; Windows need to stay open as long as there's data arriving - 30+ mins\n-&gt; Window Should close when no data has been received for 15 mins -&gt; Gap 15 mins"
      },
      {
        "date": "2024-08-24T10:27:00.000Z",
        "voteCount": 1,
        "content": "The problem requires detecting average noise levels when data is received for more than 30 minutes, but the window should end when no data has been received for 15 minutes.\nSession windows are ideal for this scenario because:\nThey are designed to capture bursts of activity followed by periods of inactivity.\nThey dynamically size based on the data received, which fits well with the variable duration of noise events.\nThe gap duration can be set to define when a session ends.\nThe 15-minute gap duration aligns perfectly with the requirement to end the window when no data has been received for 15 minutes.\nSession windows will naturally extend beyond 30 minutes if data keeps coming in, satisfying the requirement to detect levels for durations of more than 30 minutes."
      },
      {
        "date": "2024-08-16T16:51:00.000Z",
        "voteCount": 1,
        "content": "D. Using a 15-minute window with a 15-minute tumbling window withAllowedLateness is the most suitable option for the following reasons:\n\nFlexibility: By allowing a 15-minute delay, it can accommodate various situations such as network latency or sensor failures.\nProcessing efficiency: Using a fixed window improves processing efficiency.\nCompliance with conditions: The window ends if no data is received for 15 minutes, meeting the specified condition.\nImplementation points:\n\n.withAllowedLateness: This operator allows delayed events to be included in the current window.\nTrigger: When 30 minutes of data is collected, a trigger event is generated, and the average value is calculated based on this event.\nWatermark: By setting a watermark, processing of old data can be terminated."
      },
      {
        "date": "2024-08-05T11:38:00.000Z",
        "voteCount": 2,
        "content": "Without a doubt A: https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows"
      },
      {
        "date": "2024-08-02T13:56:00.000Z",
        "voteCount": 1,
        "content": "The requirements are\n- recieve data for a duration of MORE than 30 minutes\n- end the window based on inactivity"
      },
      {
        "date": "2024-05-19T12:20:00.000Z",
        "voteCount": 2,
        "content": "Correct answer: A.\nUse a session Window to cature data and create an aggregation when the Session is larger than 30 minutes. \nhttps://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows\nhttps://beam.apache.org/releases/javadoc/2.6.0/org/apache/beam/sdk/transforms/windowing/Sessions.html"
      },
      {
        "date": "2024-05-18T23:05:00.000Z",
        "voteCount": 1,
        "content": "C- Running average: https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows"
      },
      {
        "date": "2024-04-07T01:51:00.000Z",
        "voteCount": 1,
        "content": "Guys, I think its B. I was considering C, but C is calculating every 30 min, the 15min window gap data.Thats not what the questions wants. The questions wants a solution to get the average data of a 30 min window. So Its B.\n\nLook at this relating to C:\n\"Use hopping windows with a 15-minute window, and a thirty-minute period\" --&gt; Wrong\n(IS DIFFERENT THEN)\n\"Use hopping windows with a 30-minute window, and a 15-minute period\" --&gt; Right.\n\nThats why I think the B is the right answer."
      },
      {
        "date": "2024-04-19T01:10:00.000Z",
        "voteCount": 1,
        "content": "Actually I think with B, the window will never be closed because the probability of having a period of inactivity of 30 minutes is very low. In that case I think the option A is the more correct one."
      },
      {
        "date": "2024-03-06T22:04:00.000Z",
        "voteCount": 1,
        "content": "To take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds."
      },
      {
        "date": "2024-02-25T05:30:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#session-windows\nA session window contains elements within a gap duration of another element. The gap duration is an interval between new data in a data stream. If data arrives after the gap duration, the data is assigned to a new window.\nSo, we need"
      },
      {
        "date": "2024-01-25T09:11:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#hopping-windows   To take running averages of data, use hopping windows. You can use one-minute hopping windows with a thirty-second period to compute a one-minute running average every thirty seconds."
      },
      {
        "date": "2024-01-13T08:02:00.000Z",
        "voteCount": 2,
        "content": "Option D to me, It aligns with the specified criteria for detecting the average noise level within a 30-minute duration and handling the end of the window when no data is received for 15 minutes."
      },
      {
        "date": "2024-01-24T06:54:00.000Z",
        "voteCount": 1,
        "content": "Agree D.\nData comes -&gt; 30 mts duration.\nData didn't come in 15 mts -&gt; 15 mts duration"
      },
      {
        "date": "2024-01-10T21:38:00.000Z",
        "voteCount": 2,
        "content": "OPTION D is correct for the specific scenario where we want to detect the average noise level for a duration of more than 30 minutes but end the window when no data has been received for 15 minutes.\n\nExplanation:\n\n- Tumbling windows are non-overlapping windows, and in this case, you want to capture data continuously for 30-minute intervals.\n\n- Using a tumbling window with a 15-minute window size aligns with your requirement to detect the average noise level for a duration of more than 30 minutes.\n\n- Adding a .withAllowedLateness operator with a duration of fifteen minutes ensures that the window will still consider late-arriving data within that time frame. After fifteen minutes of no data, the window will be closed, and any late-arriving data will not be considered.\n\nOption A and B invalid as they capture fixed logic with 15 or 30 mins. Option C captures only 15 min average with 30 min trigger hence not suitable."
      },
      {
        "date": "2024-01-09T06:02:00.000Z",
        "voteCount": 2,
        "content": "Hopping windows (called sliding windows in Apache Beam).\nTo take running averages of data, use hopping windows."
      },
      {
        "date": "2024-01-03T10:23:00.000Z",
        "voteCount": 3,
        "content": "C. Use hopping windows with a 15-minute window, and a thirty-minute period."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 267,
    "url": "https://www.examtopics.com/discussions/google/view/130218-exam-professional-data-engineer-topic-1-question-267/",
    "body": "You are creating a data model in BigQuery that will hold retail transaction data. Your two largest tables, sales_transaction_header and sales_transaction_line, have a tightly coupled immutable relationship. These tables are rarely modified after load and are frequently joined when queried. You need to model the sales_transaction_header and sales_transaction_line tables to improve the performance of data analytics queries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a sales_transaction table that holds the sales_transaction_header information as rows and the sales_transaction_line rows as nested and repeated fields.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a sales_transaction table that holds the sales_transaction_header and sales_transaction_line information as rows, duplicating the sales_transaction_header data for each line.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a sales_transaction table that stores the sales_transaction_header and sales_transaction_line data as a JSON data type.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate separate sales_transaction_header and sales_transaction_line tables and, when querying, specify the sales_transaction_line first in the WHERE clause."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 10,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-05-19T12:28:00.000Z",
        "voteCount": 2,
        "content": "Option A https://cloud.google.com/bigquery/docs/best-practices-performance-nested"
      },
      {
        "date": "2024-03-16T22:41:00.000Z",
        "voteCount": 1,
        "content": "optional A"
      },
      {
        "date": "2024-02-20T19:15:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T08:07:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-05T09:36:00.000Z",
        "voteCount": 4,
        "content": "- In BigQuery, nested and repeated fields can significantly improve performance for certain types of queries, especially joins, because the data is co-located and can be read efficiently. - - This approach is often used in data warehousing scenarios where query performance is a priority, and the data relationships are immutable and rarely modified."
      },
      {
        "date": "2024-01-03T10:27:00.000Z",
        "voteCount": 1,
        "content": "A. Create a sales_transaction table that holds the sales_transaction_header information as rows and the sales_transaction_line rows as nested and repeated fields."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 268,
    "url": "https://www.examtopics.com/discussions/google/view/130219-exam-professional-data-engineer-topic-1-question-268/",
    "body": "You created a new version of a Dataflow streaming data ingestion pipeline that reads from Pub/Sub and writes to BigQuery. The previous version of the pipeline that runs in production uses a 5-minute window for processing. You need to deploy the new version of the pipeline without losing any data, creating inconsistencies, or increasing the processing latency by more than 10 minutes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the old pipeline with the new pipeline code.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSnapshot the old pipeline, stop the old pipeline, and then start the new pipeline from the snapshot.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tDrain the old pipeline, then start the new pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCancel the old pipeline, then start the new pipeline."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T10:04:00.000Z",
        "voteCount": 6,
        "content": "- Graceful Data Transition: Draining the old pipeline ensures it processes all existing data in its buffers and watermarks before shutting down, preventing data loss or inconsistencies.\n- Minimal Latency Increase: The latency increase will be limited to the amount of time it takes to drain the old pipeline, typically within the acceptable 10-minute threshold."
      },
      {
        "date": "2024-02-11T12:32:00.000Z",
        "voteCount": 5,
        "content": "I don't think C is correct, as it will immediately fire the window:\n\"Draining can result in partially filled windows. In that case, if you restart the drained pipeline, the same window might fire a second time, which can cause issues with your data. \"\nhttps://cloud.google.com/dataflow/docs/guides/stopping-a-pipeline#effects\n\nMaybe \"A\" means launching a replacement job?\nhttps://cloud.google.com/dataflow/docs/guides/updating-a-pipeline#Launching"
      },
      {
        "date": "2024-03-24T10:06:00.000Z",
        "voteCount": 2,
        "content": "So why not B it is the better choice to save intermediate state and easy to use"
      },
      {
        "date": "2024-08-06T07:58:00.000Z",
        "voteCount": 1,
        "content": "There is requirement to avoid data loss.\n\nhttps://cloud.google.com/dataflow/docs/guides/upgrade-guide#stop-and-replace\n\"To avoid data loss, in most cases, draining is the preferred action.\""
      },
      {
        "date": "2024-06-12T02:58:00.000Z",
        "voteCount": 1,
        "content": "- Draining the old pipeline ensures that it finishes processing all in-flight data before stopping, which prevents data loss and inconsistencies.\n- After draining, you can start the new pipeline, which will begin processing new data from where the old pipeline left off.\n- This approach maintains a smooth transition between the old and new versions, minimizing latency increases and avoiding data gaps or overlaps.\n\n==&gt; Other options, such as updating, snapshotting, or canceling, might not provide the same level of consistency and could lead to data loss or increased latency beyond the acceptable 10-minute window. Draining is the safest method to ensure a seamless transition."
      },
      {
        "date": "2024-03-24T10:09:00.000Z",
        "voteCount": 2,
        "content": "I would choose B as mentioned by Alizcert, a simple drain may cause problem \nDataflow snapshots save the state of a streaming pipeline, which lets you start a new version of your Dataflow job without losing state. Snapshots are useful for backup and recovery, testing and rolling back updates to streaming pipelines, and other similar scenarios."
      },
      {
        "date": "2024-03-15T23:30:00.000Z",
        "voteCount": 1,
        "content": "C option"
      },
      {
        "date": "2024-01-13T08:09:00.000Z",
        "voteCount": 1,
        "content": "Option C, draining the old pipeline solves all requests"
      },
      {
        "date": "2024-01-03T10:31:00.000Z",
        "voteCount": 2,
        "content": "C. Drain the old pipeline, then start the new pipeline."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 269,
    "url": "https://www.examtopics.com/discussions/google/view/130424-exam-professional-data-engineer-topic-1-question-269/",
    "body": "Your organization's data assets are stored in BigQuery, Pub/Sub, and a PostgreSQL instance running on Compute Engine. Because there are multiple domains and diverse teams using the data, teams in your organization are unable to discover existing data assets. You need to design a solution to improve data discoverability while keeping development and configuration efforts to a minimum. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Catalog to automatically catalog BigQuery datasets. Use Data Catalog APIs to manually catalog Pub/Sub topics and PostgreSQL tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use custom connectors to manually catalog PostgreSQL tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse customer connectors to manually catalog BigQuery datasets, Pub/Sub topics, and PostgreSQL tables."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 21,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-29T01:24:00.000Z",
        "voteCount": 11,
        "content": "Data Catalog is the best choice. But for catalogging PostgreSQL it is better to use a connector when available, instead of using API. \nhttps://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_unsupported_data_sources"
      },
      {
        "date": "2024-09-13T06:23:00.000Z",
        "voteCount": 1,
        "content": "I agree. On the linked page:\nIf you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries.\nAs we can find a connector there, it should be used."
      },
      {
        "date": "2024-02-01T02:48:00.000Z",
        "voteCount": 3,
        "content": "Agree. If it doesn't have a connector, it must be manually built on the Data Catalog API.\nAs PostgreSQL already has a connector it's the best option is C"
      },
      {
        "date": "2024-01-05T10:08:00.000Z",
        "voteCount": 10,
        "content": "- It utilizes Data Catalog's native support for both BigQuery datasets and Pub/Sub topics. \n- For PostgreSQL tables running on a Compute Engine instance, you'd use Data Catalog APIs to create custom entries, as Data Catalog does not automatically discover external databases like PostgreSQL."
      },
      {
        "date": "2024-01-20T22:16:00.000Z",
        "voteCount": 4,
        "content": "Agree. https://cloud.google.com/data-catalog/docs/concepts/overview#catalog-non-google-cloud-assets"
      },
      {
        "date": "2024-10-08T02:21:00.000Z",
        "voteCount": 1,
        "content": "This is C. To clarify some issues below with B, the links provided by supporters of B actually do say that it's preferable to use a community connector where available, and to only use the API when the case is genuinely not supported by community connectors. \nIn this case it's Postgresql, so it's supported, see here for full list: https://cloud.google.com/data-catalog/docs/integrate-data-sources#integrate_on-premises_data_sources\n\nSo this would be B if it was something like Q+ or some genuinely unsupported database, but postgres is supported for community connector."
      },
      {
        "date": "2024-08-24T10:35:00.000Z",
        "voteCount": 2,
        "content": "Data Catalog automatically catalogs metadata from Google Cloud sources such as BigQuery, Vertex AI, Pub/Sub, Spanner, Bigtable, and more.\n\nTo catalog metadata from non-Google Cloud systems in your organization, you can use the following:\n\nCommunity-contributed connectors to multiple popular on-premises data sources\nManually build on the Data Catalog APIs for custom entries"
      },
      {
        "date": "2024-08-24T10:37:00.000Z",
        "voteCount": 1,
        "content": "C. While similar to B, using custom connectors for PostgreSQL might involve more development effort than using the Data Catalog APIs directly."
      },
      {
        "date": "2024-08-10T03:24:00.000Z",
        "voteCount": 1,
        "content": "raaad mostly correct and we can check his description supporting his answer so we can go with it .Cheers mate"
      },
      {
        "date": "2024-07-27T12:37:00.000Z",
        "voteCount": 1,
        "content": "I\u2019m voting for C because the documentation states that Postgres is a custom connector developed by the community."
      },
      {
        "date": "2024-07-28T06:38:00.000Z",
        "voteCount": 1,
        "content": "Changed my mind. B.\n-This is not on premise, so the custom connector should not be applicable\n-Question says keep manual dev and config to a minimum"
      },
      {
        "date": "2024-06-16T21:37:00.000Z",
        "voteCount": 2,
        "content": "BigQuery Datasets and Pub/Sub Topics: Google Data Catalog can automatically catalog metadata from BigQuery and Pub/Sub, making it easy to discover and manage these data assets without additional development effort.\n\nPostgreSQL Tables: While Data Catalog does not have built-in connectors for PostgreSQL, you can use the Data Catalog APIs to manually catalog the PostgreSQL tables. This requires some custom development but is manageable compared to creating custom connectors for everything."
      },
      {
        "date": "2024-05-22T05:05:00.000Z",
        "voteCount": 1,
        "content": "B. Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables."
      },
      {
        "date": "2024-05-14T04:51:00.000Z",
        "voteCount": 1,
        "content": "Option B leverages Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics, which streamlines the process and reduces manual effort. Using Data Catalog APIs to manually catalog PostgreSQL tables ensures consistency across all data assets while minimizing development and configuration efforts."
      },
      {
        "date": "2024-04-24T23:20:00.000Z",
        "voteCount": 2,
        "content": "I vote for c as per Integrate on-premises data sources\nTo integrate on-premises data sources, you can use the corresponding Python connectors contributed by the community:\n\nunder the link\n\nhttps://cloud.google.com/data-catalog/docs/integrate-data-sources"
      },
      {
        "date": "2024-04-24T23:22:00.000Z",
        "voteCount": 1,
        "content": "data catalog api will come into effect if custom connectors are not available via community repos."
      },
      {
        "date": "2024-04-07T02:24:00.000Z",
        "voteCount": 3,
        "content": "In the opction C, the expression \"Use custom connectors to manually catalog PostgreSQL tables.\" is refering to the use case of Google when you want to use \"Community-contributed connectors to multiple popular on-premises data sources\". As you can see, this connectors are for ON-PREMISSES data sources ONLY. In this case the Postgres is in a VM in the cloud. Thus, the option correct is B."
      },
      {
        "date": "2024-04-07T02:24:00.000Z",
        "voteCount": 1,
        "content": "Link: https://cloud.google.com/data-catalog/docs/concepts/overview#catalog-non-google-cloud-assets"
      },
      {
        "date": "2024-03-15T23:27:00.000Z",
        "voteCount": 1,
        "content": "option B, there's no need to build a custom connector now, postgreSQL is now supported\nhttps://github.com/GoogleCloudPlatform/datacatalog-connectors-rdbms/tree/master/google-datacatalog-postgresql-connector"
      },
      {
        "date": "2024-03-24T10:28:00.000Z",
        "voteCount": 1,
        "content": "I think \u201ccustom connector\u201d here may just infer that this is not official tools? as the doc mentioned \u201c connectors contributed by the community\u201d \nAnd should not be B as \u201cmanually catalog by API \u201c this is a way even more basic than using connector"
      },
      {
        "date": "2024-03-13T04:20:00.000Z",
        "voteCount": 1,
        "content": "Use Data Catalog to automatically catalog BigQuery datasets and Pub/Sub topics. Use Data Catalog APIs to manually catalog PostgreSQL tables."
      },
      {
        "date": "2024-02-26T08:50:00.000Z",
        "voteCount": 1,
        "content": "Datacatalog API contain the connector for postgresql with using it developer don't have to create the custom connectors"
      },
      {
        "date": "2024-02-17T15:13:00.000Z",
        "voteCount": 4,
        "content": "Google Recommendation: If you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries. To do that, you can:\n- Use one of the Data Catalog Client Libraries in one of the following languages: C#, Go, Java, Node.js, PHP, Python, or Ruby.\n- Or manually build on the Data Catalog API.\n\nHowever, there is a connector for PostgreSQL, so option C."
      },
      {
        "date": "2024-02-09T01:26:00.000Z",
        "voteCount": 2,
        "content": "If you can't find a connector for your data source, you can still manually integrate it by creating entry groups and custom entries. To do that, you can:\n\n- Manually build on the Data Catalog API."
      },
      {
        "date": "2024-01-13T08:13:00.000Z",
        "voteCount": 3,
        "content": "Option B - Data Catalog automatically maps out GCP resources and dev efforts are minimized by leveraging the data catalog API to do the same for postgresql db"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 270,
    "url": "https://www.examtopics.com/discussions/google/view/130220-exam-professional-data-engineer-topic-1-question-270/",
    "body": "You need to create a SQL pipeline. The pipeline runs an aggregate SQL transformation on a BigQuery table every two hours and appends the result to another existing BigQuery table. You need to configure the pipeline to retry if errors occur. You want the pipeline to send an email notification after three consecutive failures. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQueryUpsertTableOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable email notifications.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 8,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T10:18:00.000Z",
        "voteCount": 5,
        "content": "- It provides a direct and controlled way to manage the SQL pipeline using Cloud Composer (Apache Airflow). \n- The BigQueryInsertJobOperator is well-suited for running SQL jobs in BigQuery, including aggregate transformations and handling of results. \n- The retry and email_on_failure parameters align with the requirements for error handling and notifications. \n- Cloud Composer requires more setup than using BigQuery's scheduled queries directly, but it offers robust workflow management, retry logic, and notification capabilities, making it suitable for more complex and controlled data pipeline requirements."
      },
      {
        "date": "2024-02-20T10:27:00.000Z",
        "voteCount": 3,
        "content": "The prompt wants an email notification sent after three failed attempts. Is there any concern that the retry parameter is set to 3, wouldn't this mean that the email is sent after 4 failed attempts (1 original + 3 retries)?"
      },
      {
        "date": "2024-05-19T12:58:00.000Z",
        "voteCount": 1,
        "content": "https://airflow.apache.org/docs/apache-airflow-providers-google/stable/_api/airflow/providers/google/cloud/operators/bigquery/index.html#airflow.providers.google.cloud.operators.bigquery.BigQueryInsertJobOperator\nhttps://cloud.google.com/composer/docs/composer-2/write-dags#notifications_on_operator_failure"
      },
      {
        "date": "2024-04-07T02:57:00.000Z",
        "voteCount": 1,
        "content": "It s B (however for me its a incomplete answers cause it does not address the schedule of every 2 hours).\n\nIts not C or D because BigQuery scheduled queries by default does not retries the queries when error occurs. Link: https://cloud.google.com/bigquery/docs/scheduling-queries"
      },
      {
        "date": "2024-02-20T19:21:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-29T01:53:00.000Z",
        "voteCount": 2,
        "content": "D. Create a BigQuery scheduled query to run the SQL transformation with schedule options that repeats every two hours, and enable notification to Pub/Sub topic. Use Pub/Sub and Cloud Functions to send an email after three failed executions\n\nThis method utilizes BigQuery's native scheduling capabilities for running the SQL job and leverages Pub/Sub and Cloud Functions for customized notification handling, including the specific requirement of sending an email after three consecutive failures."
      },
      {
        "date": "2024-02-22T14:34:00.000Z",
        "voteCount": 3,
        "content": "Option D mentions nothing about how the job retrying is put in place, so for that reason I don't think this is the correct option."
      },
      {
        "date": "2024-01-03T10:43:00.000Z",
        "voteCount": 1,
        "content": "B. Use the BigQueryInsertJobOperator in Cloud Composer, set the retry parameter to three, and set the email_on_failure parameter to true."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 271,
    "url": "https://www.examtopics.com/discussions/google/view/130427-exam-professional-data-engineer-topic-1-question-271/",
    "body": "You are monitoring your organization\u2019s data lake hosted on BigQuery. The ingestion pipelines read data from Pub/Sub and write the data into tables on BigQuery. After a new version of the ingestion pipelines is deployed, the daily stored data increased by 50%. The volumes of data in Pub/Sub remained the same and only some tables had their daily partition data size doubled. You need to investigate and fix the cause of the data increase. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.<br>2. Schedule daily SQL jobs to deduplicate the affected tables.<br>3. Share the deduplication script with the other operational teams to reuse if this occurs to other tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Check for code errors in the deployed pipelines.<br>2. Check for multiple writing to pipeline BigQuery sink.<br>3. Check for errors in Cloud Logging during the day of the release of the new pipelines.<br>4. If no errors, restore the BigQuery tables to their content before the last release by using time travel.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Check for duplicate rows in the BigQuery tables that have the daily partition data size doubled.<br>2. Check the BigQuery Audit logs to find job IDs.<br>3. Use Cloud Monitoring to determine when the identified Dataflow jobs started and the pipeline code version.<br>4. When more than one pipeline ingests data into a table, stop all versions except the latest one.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Roll back the last deployment.<br>2. Restore the BigQuery tables to their content before the last release by using time travel.<br>3. Restart the Dataflow jobs and replay the messages by seeking the subscription to the timestamp of the release."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T10:29:00.000Z",
        "voteCount": 11,
        "content": "- Detailed Investigation of Logs and Jobs Checking for duplicate rows targets the potential immediate cause of the issue.\n- Checking the BigQuery Audit logs helps identify which jobs might be contributing to the increased data volume.\n- Using Cloud Monitoring to correlate job starts with pipeline versions helps identify if a specific version of the pipeline is responsible.\n- Managing multiple versions of pipelines ensures that only the intended version is active, addressing any versioning errors that might have occurred during deployment.\n\n\n=======\nWhy not B\nWhile it addresses the symptom (excess data), it doesn't necessarily stop the problem from recurring. (The questions asked to investigate and fix)"
      },
      {
        "date": "2024-01-13T08:22:00.000Z",
        "voteCount": 1,
        "content": "Option C - agree with Raaad on the reasons"
      },
      {
        "date": "2024-01-12T00:04:00.000Z",
        "voteCount": 1,
        "content": "B. Check for code errors in the deployed pipelines, multiple writing to pipeline BigQuery sink, errors in Cloud Logging, and if necessary, restore tables using time travel.\nCheck for code errors\nCheck for multiple writes\nCheck Cloud Logging\nRestore tables if necessary:"
      },
      {
        "date": "2024-02-14T04:14:00.000Z",
        "voteCount": 2,
        "content": "This does not fix the error, it basically assumes that the error is not really there."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 272,
    "url": "https://www.examtopics.com/discussions/google/view/130221-exam-professional-data-engineer-topic-1-question-272/",
    "body": "You have a BigQuery dataset named \u201ccustomers\u201d. All tables will be tagged by using a Data Catalog tag template named \u201cgdpr\u201d. The template contains one mandatory field, \u201chas_sensitive_data\u201d, with a boolean value. All employees must be able to do a simple search and find tables in the dataset that have either true or false in the \u201chas_sensitive_data\u2019 field. However, only the Human Resources (HR) group should be able to see the data inside the tables for which \u201chas_sensitive data\u201d is true. You give the all employees group the bigquery.metadataViewer and bigquery.connectionUser roles on the dataset. You want to minimize configuration overhead. What should you do next?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the \u201cgdpr\u201d tag template with private visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the \u201cgdpr\u201d tag template with private visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the \u201cgdpr\u201d tag template with public visibility. Assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate the \u201cgdpr\u201d tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T05:26:00.000Z",
        "voteCount": 15,
        "content": "- The most straightforward solution with minimal configuration overhead.\n- By creating the \"gdpr\" tag template with public visibility, you ensure that all employees can search and find tables based on the \"has_sensitive_data\" field. \n- Assigning the bigquery.dataViewer role to the HR group on tables with sensitive data ensures that only they can view the actual data in these tables."
      },
      {
        "date": "2024-02-18T03:21:00.000Z",
        "voteCount": 1,
        "content": "Ignore the last reply. The correct answer would be C.\nTags = Custom metadata fields that you can attach to a data entry to provide context.\nTag templates = Reusable structures that you can use to rapidly create new tags.\nIn short, the employees do not need a tagTemplateViewer role because it pertains to the tag templates, not the tags themselves."
      },
      {
        "date": "2024-02-18T03:08:00.000Z",
        "voteCount": 1,
        "content": "Wouldn't employees still need the roles/datacatalog.tagTemplateViewer role to view private AND public tags?\nTo get the permissions that you need to view public and private tags on Bigtable resources, ask your administrator to grant you the following IAM roles:\n- roles/datacatalog.tagTemplateViewer\n- roles/bigtable.viewer\nSource: https://cloud.google.com/bigtable/docs/manage-data-assets-using-data-catalog#permissions-view-tags"
      },
      {
        "date": "2024-08-10T02:59:00.000Z",
        "voteCount": 1,
        "content": "This Guy Raasd is mostly correct with explanation thanks mate."
      },
      {
        "date": "2024-08-02T14:43:00.000Z",
        "voteCount": 1,
        "content": "A - employees cannot use the tag\nB - increases the configuration overhead\nC - exactly what we need\nD - unnecessary role assignment, the tag template is already visibile"
      },
      {
        "date": "2024-03-24T14:12:00.000Z",
        "voteCount": 1,
        "content": "While D works well, it is not obligated to give all employees the role of   tagTemplateViewer, as it will give them the view permission for tag templates as well as the tags created by the template.\nHowever, Tags are a type of business metadata. Adding tags to a data entry helps provide meaningful context to anyone who needs to use the asset.And public tags provide less strict access control for searching and viewing the tag as compared to private tags. Any user who has the required view permissions for a data entry can view all the public tags associated with it. View permissions for public tags are only required when you perform a search in Data Catalog using the tag: syntax or when you view an unattached tag template."
      },
      {
        "date": "2024-03-24T14:15:00.000Z",
        "voteCount": 1,
        "content": "As all employees have the role \u201c bigquery.metadataViewer\u201d they are already capable to see tags on BigQuery then"
      },
      {
        "date": "2024-02-20T22:02:00.000Z",
        "voteCount": 1,
        "content": "I'll go with raaad's answer"
      },
      {
        "date": "2024-02-01T03:44:00.000Z",
        "voteCount": 2,
        "content": "If you working with PII, We can't granted public access. So Private Visibility for the Tag Template its the best option.\n\nCheck it https://cloud.google.com/data-catalog/docs/tags-and-tag-templates"
      },
      {
        "date": "2024-01-03T10:55:00.000Z",
        "voteCount": 3,
        "content": "D. Create the \u201cgdpr\u201d tag template with public visibility. Assign the datacatalog.tagTemplateViewer role on this tag to the all employees group, and assign the bigquery.dataViewer role to the HR group on the tables that contain sensitive data."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 273,
    "url": "https://www.examtopics.com/discussions/google/view/130514-exam-professional-data-engineer-topic-1-question-273/",
    "body": "You are creating the CI/CD cycle for the code of the directed acyclic graphs (DAGs) running in Cloud Composer. Your team has two Cloud Composer instances: one instance for development and another instance for production. Your team is using a Git repository to maintain and develop the code of the DAGs. You want to deploy the DAGs automatically to Cloud Composer when a certain tag is pushed to the Git repository. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.<br>2. If the tests pass, use Cloud Build to copy the code to the bucket of the production instance.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the code to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.<br>2. If the tests pass, use the KubernetesPodOperator to deploy the container to the GKE cluster of the production instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Cloud Build to build a container and the KubernetesPodOperator to deploy the code of the DAG to the Google Kubernetes Engine (GKE) cluster of the development instance for testing.<br>2. If the tests pass, copy the code to the Cloud Storage bucket of the production instance.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Use Cloud Build to copy the code of the DAG to the Cloud Storage bucket of the development instance for DAG testing.<br>2. If the tests pass, use Cloud Build to build a container with the code of the DAG and the KubernetesPodOperator to deploy the container to the Google Kubernetes Engine (GKE) cluster of the production instance."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 20,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T04:22:00.000Z",
        "voteCount": 10,
        "content": "The Answer is A. Given that there are two instances (development and production) already available, and the goal is to deploy DAGs to Cloud Composer not entire composer infra build. \n\nExplanation:\n- This approach leverages Cloud Build to manage the deployment process.\n- It first deploys the code to the Cloud Storage bucket of the development instance for testing purposes.\n- If the tests are successful in the development environment, the same Cloud Build process is used to copy the code to the Cloud Storage bucket of the production instance.\n \nB. GKE-based approach is not standard for Cloud Composer. C. GKE used for testing is unconventional for DAG deployments. D. Involves unnecessary GKE deployment for production. Testing DAGs should use Composer instances directly, not Kubernetes containers in GKE."
      },
      {
        "date": "2024-08-10T02:51:00.000Z",
        "voteCount": 1,
        "content": "Most confusing question to confuse us why GKE needed its already mentioned they have 2 composer environment"
      },
      {
        "date": "2024-02-20T22:10:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T08:47:00.000Z",
        "voteCount": 4,
        "content": "Option A, DAGs are routinely stored in cloud storage buckets, Cloud Build act as a trigger for both the deployment process to test env and the test itslef\nhttps://cloud.google.com/composer/docs/dag-cicd-integration-guide"
      },
      {
        "date": "2024-01-09T07:23:00.000Z",
        "voteCount": 1,
        "content": "I vote fore A"
      },
      {
        "date": "2024-01-07T09:31:00.000Z",
        "voteCount": 1,
        "content": "C. \nIt looks the correct choice, first build, test and verify everything on dev enviornment and then just copy the files on prod bucket.\nhttps://cloud.google.com/composer/docs/dag-cicd-integration-guide"
      },
      {
        "date": "2024-01-09T07:22:00.000Z",
        "voteCount": 1,
        "content": "But why do we need the Google Kubernetes Engine (GKE) cluster for this?"
      },
      {
        "date": "2024-01-17T15:52:00.000Z",
        "voteCount": 1,
        "content": "Yea, it should be A"
      },
      {
        "date": "2024-01-07T09:11:00.000Z",
        "voteCount": 2,
        "content": "This approach is straightforward and leverages Cloud Build to automate the deployment process. It doesn't require containerization, making it simpler and possibly quicker."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 274,
    "url": "https://www.examtopics.com/discussions/google/view/130515-exam-professional-data-engineer-topic-1-question-274/",
    "body": "You have a BigQuery table that ingests data directly from a Pub/Sub subscription. The ingested data is encrypted with a Google-managed encryption key. You need to meet a new organization policy that requires you to use keys from a centralized Cloud Key Management Service (Cloud KMS) project to encrypt data at rest. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud KMS encryption key with Dataflow to ingest the existing Pub/Sub subscription to the existing BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Pub/Sub topic with CMEK and use the existing BigQuery table by using Google-managed encryption key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new BigQuery table and Pub/Sub topic by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 14,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-10T05:30:00.000Z",
        "voteCount": 9,
        "content": "- New BigQuery Table with CMEK: This option involves creating a new BigQuery table configured to use a CMEK from Cloud KMS. It directly addresses the need to use a CMEK for data at rest in BigQuery.\n- Migrate Data: Migrating data from the old table (encrypted with a Google-managed key) to the new table (encrypted with CMEK) ensures that all existing data complies with the new policy."
      },
      {
        "date": "2024-01-13T08:54:00.000Z",
        "voteCount": 4,
        "content": "But also pub/sub has some data at rest, e.g.  messages with retention period. \nTo comply with the organisation policy, we need to adapt also pub/sub"
      },
      {
        "date": "2024-01-20T22:27:00.000Z",
        "voteCount": 2,
        "content": "No, \"The ingested data is encrypted with a Google-managed encryption key\", target is ingested data in BigQuery."
      },
      {
        "date": "2024-02-18T04:45:00.000Z",
        "voteCount": 3,
        "content": "Correct, but the question states 'use keys from a centralized Cloud KMS project', so only D is correct."
      },
      {
        "date": "2024-08-24T10:54:00.000Z",
        "voteCount": 1,
        "content": "Requirement for Cloud KMS keys: The new organization policy requires using keys from a centralized Cloud KMS project for encrypting data at rest. This necessitates the use of customer-managed encryption keys (CMEK).\nBigQuery table encryption: The existing BigQuery table is encrypted with a Google-managed key. To meet the new policy, a new table needs to be created with CMEK.\nPub/Sub topic encryption: Since the data is ingested directly from a Pub/Sub subscription, the Pub/Sub topic also needs to use CMEK to ensure end-to-end encryption with customer-managed keys.\nData migration: The existing data in the old BigQuery table needs to be migrated to the new CMEK-encrypted table to ensure all data complies with the new policy"
      },
      {
        "date": "2024-07-14T21:05:00.000Z",
        "voteCount": 1,
        "content": "\"The best solution here is B. Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.\n\nHere's why:\n\nCustomer-Managed Encryption Keys (CMEK): CMEKs allow you to have granular control over your encryption keys, complying with the organization's policy to use keys from a centralized Cloud KMS project.\nData Migration: Since the data in the existing table is already encrypted with a Google-managed key, you cannot retroactively change the encryption key for that table. Migrating the data to a new table with the correct encryption is the most efficient way to meet compliance."
      },
      {
        "date": "2024-07-14T21:06:00.000Z",
        "voteCount": 1,
        "content": "Why other options aren't suitable:\n\nA: Dataflow can't retroactively change the encryption of data that's already in BigQuery.\nC: Creating a new Pub/Sub topic with CMEK wouldn't address the data that's already in BigQuery.\nD: While creating a new Pub/Sub topic might be useful in the long run, it's not necessary for solving the immediate compliance issue with the existing data.\""
      },
      {
        "date": "2024-08-02T14:50:00.000Z",
        "voteCount": 1,
        "content": "You have some data in Pub/Sub at rest as well which is immediate compliance issue."
      },
      {
        "date": "2024-06-15T04:06:00.000Z",
        "voteCount": 2,
        "content": "D. Create a new BigQuery table and Pub/Sub topic by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table.\n\nThis approach comprehensively addresses the requirement to use CMEK from a centralized Cloud KMS project for encrypting data at rest:\n\nCreate a new Pub/Sub topic configured to use CMEK from the centralized Cloud KMS project.\nCreate a new BigQuery table with CMEK enabled, using the same centralized Cloud KMS project.\nUpdate the ingestion process to use the new Pub/Sub topic to feed data into the new BigQuery table.\nMigrate existing data from the old BigQuery table to the new BigQuery table to ensure all data complies with the new encryption policy."
      },
      {
        "date": "2024-06-05T07:23:00.000Z",
        "voteCount": 1,
        "content": "B, been there, done that..."
      },
      {
        "date": "2024-06-05T07:24:00.000Z",
        "voteCount": 2,
        "content": "sry, I mean D"
      },
      {
        "date": "2024-05-26T10:13:00.000Z",
        "voteCount": 2,
        "content": "BigQuery and Pub/Sub shall be encrypted using CMEK using new versions of each one.\n  https://cloud.google.com/pubsub/docs/encryption#using-cmek"
      },
      {
        "date": "2024-05-18T03:22:00.000Z",
        "voteCount": 1,
        "content": "Data at rest in requirement = Big Query ONLY. \n\nPub/Sub is data in movement - overkill for the solution"
      },
      {
        "date": "2024-05-17T23:59:00.000Z",
        "voteCount": 1,
        "content": "D- BigQuery and Pub/sub are automatically encrypted but here we need to apply a more secured policy by using CMEK so we need to use it for bigquery and pub/sub to meet this policy"
      },
      {
        "date": "2024-04-25T03:26:00.000Z",
        "voteCount": 1,
        "content": "B. Create a new BigQuery table by using customer-managed encryption keys (CMEK), and migrate the data from the old BigQuery table. Most Voted"
      },
      {
        "date": "2024-04-25T03:29:00.000Z",
        "voteCount": 1,
        "content": "it should be B as the data in pub sub is already encrypted , please read it carefully and use Copilot or chat gpt to have confirmation."
      },
      {
        "date": "2024-04-03T23:30:00.000Z",
        "voteCount": 1,
        "content": "BigQuery allows you to encrypt data at rest using either Google-managed encryption keys or customer-managed encryption keys (CMEK) from Cloud KMS.\nSince the new policy requires using keys from a centralized Cloud KMS project, you need to create a new BigQuery table that is configured to use CMEK for encryption.\nAfter creating the new table with CMEK, you can migrate the data from the old table (encrypted with Google-managed keys) to the new table (encrypted with CMEK).\nThis approach ensures that the data in the BigQuery table is encrypted using the required CMEK while preserving the existing data.\n\nCreating a new BigQuery table and Pub/Sub topic with CMEK is not necessary because the focus is on encrypting the data at rest in BigQuery. The existing Pub/Sub subscription can still be used to ingest data into the new BigQuery table."
      },
      {
        "date": "2024-03-23T07:58:00.000Z",
        "voteCount": 3,
        "content": "D - 'as new organization policy that requires you to use keys from a centralized Cloud Key Management Service (Cloud KMS) project to encrypt data at rest.' Therefore, the Pub/Sub default Google-managed encryption key is not sufficient as the organization requires it's own CMEK that is to be generated from a centralized Cloud KMS project."
      },
      {
        "date": "2024-02-27T13:30:00.000Z",
        "voteCount": 1,
        "content": "Agree with ML6 and Smakyel. To encrypt data at rest we should encrypt the data in PubSub and BigQuery"
      },
      {
        "date": "2024-02-18T04:51:00.000Z",
        "voteCount": 3,
        "content": "Only option D complies with the organisation policy:\n- By creating a new Pub/Sub topic with customer-managed encryption keys (CMEK), any new data ingested into Pub/Sub will be encrypted with the (!) organization's desired encryption keys (!).\n- Creating a new BigQuery table with CMEK ensures that all data stored in BigQuery, both newly ingested and migrated historical data, is encrypted according to organizational policies.\n- Migrating the data from the old BigQuery table to the new one ensures that historical data is also encrypted with the new keys, thus meeting the organization's requirements for encryption at rest for both Pub/Sub and BigQuery."
      },
      {
        "date": "2024-01-13T08:54:00.000Z",
        "voteCount": 4,
        "content": "Option D - I get the discussion about B and D, but also pub/sub has some data at rest, e.g.  messages with retention period. \nTo comply with the organisation policy, we need to adapt also pub/sub"
      },
      {
        "date": "2024-01-07T09:38:00.000Z",
        "voteCount": 3,
        "content": "D.\n\nWe should use new CMSK for both pubsub topic and BQ tables along with migrating old data."
      },
      {
        "date": "2024-01-07T09:14:00.000Z",
        "voteCount": 3,
        "content": "This option ensures that both the ingestion mechanism (Pub/Sub) and the storage component (BigQuery) are aligned with the organization's policy of using CMEK, providing end-to-end encryption control."
      },
      {
        "date": "2024-01-09T12:16:00.000Z",
        "voteCount": 2,
        "content": "Why not B??"
      },
      {
        "date": "2024-01-09T12:18:00.000Z",
        "voteCount": 2,
        "content": "Configuring a Pub/Sub topic with a CMEK is not necessary for encrypting data at rest in BigQuery."
      },
      {
        "date": "2024-01-13T08:55:00.000Z",
        "voteCount": 3,
        "content": "to me it's D because also pub/sub has some data at rest, e.g.  messages with retention period. \nTo comply with the organisation policy, we need to adapt also pub/sub encryption"
      },
      {
        "date": "2024-02-18T04:42:00.000Z",
        "voteCount": 2,
        "content": "But they mention that ingested data is already encrypted?"
      },
      {
        "date": "2024-01-11T04:47:00.000Z",
        "voteCount": 3,
        "content": "Requirement is encrypt bq data - \" The ingested data is encrypted with a Google-managed encryption key\" so pubsub encryption from ingestion is not needed. Option B is correct."
      },
      {
        "date": "2024-01-08T08:44:00.000Z",
        "voteCount": 1,
        "content": "I considered also A as they are asking about encryption at rest. The BigQuery is the one but Pub/Sub, not sure."
      },
      {
        "date": "2024-01-09T12:17:00.000Z",
        "voteCount": 2,
        "content": "I think option A is a partial solution as using Cloud KMS key in Dataflow for ingestion does not change the encryption of the data at rest in the BigQuery table."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 275,
    "url": "https://www.examtopics.com/discussions/google/view/130517-exam-professional-data-engineer-topic-1-question-275/",
    "body": "You created an analytics environment on Google Cloud so that your data scientist team can explore data without impacting the on-premises Apache Hadoop solution. The data in the on-premises Hadoop Distributed File System (HDFS) cluster is in Optimized Row Columnar (ORC) formatted files with multiple columns of Hive partitioning. The data scientist team needs to be able to explore the data in a similar way as they used the on-premises HDFS cluster with SQL on the Hive query engine. You need to choose the most cost-effective storage and processing solution. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the ORC files to Bigtable tables for the data scientist team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tImport the ORC files to BigQuery tables for the data scientist team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the ORC files on Cloud Storage, then deploy a Dataproc cluster for the data scientist team.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy the ORC files on Cloud Storage, then create external BigQuery tables for the data scientist team.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T12:20:00.000Z",
        "voteCount": 7,
        "content": "- It leverages the strengths of BigQuery for SQL-based exploration while avoiding additional costs and complexity associated with data transformation or migration. \n- The data remains in ORC format in Cloud Storage, and BigQuery's external tables feature allows direct querying of this data."
      },
      {
        "date": "2024-08-27T22:41:00.000Z",
        "voteCount": 1,
        "content": "There is a requirement to use a 'hive query engine'', and BQ is using only the hive metastore and his own engine, so 'D' seems a better fit here."
      },
      {
        "date": "2024-03-17T08:35:00.000Z",
        "voteCount": 4,
        "content": "I think C is the correct answer, DS want to explore the data in a \"similar way as they used the on-premises HDFS cluster with SQL on the Hive query engine\". Dataproc can help to create clusters quickly with the Hadoop cluster. CMIIW"
      },
      {
        "date": "2024-03-15T22:12:00.000Z",
        "voteCount": 1,
        "content": "option d"
      },
      {
        "date": "2024-03-11T11:00:00.000Z",
        "voteCount": 2,
        "content": "it is talking about partition as well"
      },
      {
        "date": "2024-02-20T23:36:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T08:59:00.000Z",
        "voteCount": 2,
        "content": "Option D - leverages BigQuery for SQL-based exploration on direct querying to cloud storage"
      },
      {
        "date": "2024-01-07T09:17:00.000Z",
        "voteCount": 3,
        "content": "This approach leverages BigQuery's powerful analytics capabilities without the overhead of data transformation or maintaining a separate cluster, while also allowing your team to use SQL for data exploration, similar to their experience with the on-premises Hadoop/Hive environment."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 276,
    "url": "https://www.examtopics.com/discussions/google/view/130223-exam-professional-data-engineer-topic-1-question-276/",
    "body": "You are designing a Dataflow pipeline for a batch processing job. You want to mitigate multiple zonal failures at job submission time. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSubmit duplicate pipelines in two different zones by using the --zone flag.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the pipeline staging location as a regional Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSpecify a worker region by using the --region flag.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Eventarc trigger to resubmit the job in case of zonal failure when submitting the job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T09:04:00.000Z",
        "voteCount": 9,
        "content": "Option C: https://cloud.google.com/dataflow/docs/guides/pipeline-workflows#zonal-failures"
      },
      {
        "date": "2024-01-09T12:33:00.000Z",
        "voteCount": 5,
        "content": "- Specifying a worker region (instead of a specific zone) allows Google Cloud's Dataflow service to manage the distribution of resources across multiple zones within that region"
      },
      {
        "date": "2024-02-20T23:42:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-09T08:35:00.000Z",
        "voteCount": 1,
        "content": "https://cloud.google.com/dataflow/docs/guides/pipeline-workflows#zonal-failures"
      },
      {
        "date": "2024-01-03T11:21:00.000Z",
        "voteCount": 2,
        "content": "C. Specify a worker region by using the --region flag."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 277,
    "url": "https://www.examtopics.com/discussions/google/view/130262-exam-professional-data-engineer-topic-1-question-277/",
    "body": "You are designing a real-time system for a ride hailing app that identifies areas with high demand for rides to effectively reroute available drivers to meet the demand. The system ingests data from multiple sources to Pub/Sub, processes the data, and stores the results for visualization and analysis in real-time dashboards. The data sources include driver location updates every 5 seconds and app-based booking events from riders. The data processing involves real-time aggregation of supply and demand data for the last 30 seconds, every 2 seconds, and storing the results in a low-latency system for visualization. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup the data by using a tumbling window in a Dataflow pipeline, and write the aggregated data to Memorystore.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup the data by using a session window in a Dataflow pipeline, and write the aggregated data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGroup the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to BigQuery."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T13:43:00.000Z",
        "voteCount": 8,
        "content": "- Hopping Window: Hopping windows are fixed-sized, overlapping intervals. \n- Aggregate data over the last 30 seconds, every 2 seconds, as hopping windows allow for overlapping data analysis.\n- Memorystore: Ideal for low-latency access required for real-time visualization and analysis."
      },
      {
        "date": "2024-04-11T09:18:00.000Z",
        "voteCount": 2,
        "content": "Hopping windows are sliding windows. It makes sense to use that over tumbling (fixed) window because the ask is to collect last 30 seconds of data every 5 second"
      },
      {
        "date": "2024-07-23T22:00:00.000Z",
        "voteCount": 1,
        "content": "OPTION A. (IGNORE MY Previous Comment)\n\n Tumbling windows are the best choice for this ride-hailing app because they provide accurate 2-second aggregations without the complexities of overlapping data. This is crucial for real-time decision-making and ensuring accurate visualization of supply and demand.\nHopping windows introduce potential inaccuracies and complexity, making them less suitable for this scenario. While they can be useful in other situations, they are not the optimal choice for real-time aggregation with strict accuracy requirements."
      },
      {
        "date": "2024-07-23T21:59:00.000Z",
        "voteCount": 1,
        "content": "Option B.\n\n Tumbling windows are the best choice for this ride-hailing app because they provide accurate 2-second aggregations without the complexities of overlapping data. This is crucial for real-time decision-making and ensuring accurate visualization of supply and demand.\nHopping windows introduce potential inaccuracies and complexity, making them less suitable for this scenario. While they can be useful in other situations, they are not the optimal choice for real-time aggregation with strict accuracy requirements."
      },
      {
        "date": "2024-02-21T00:26:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-17T04:13:00.000Z",
        "voteCount": 1,
        "content": "hopping window is clear but memorystore vs bigquery?? Why memorystore and not bigquery?"
      },
      {
        "date": "2024-02-18T05:25:00.000Z",
        "voteCount": 1,
        "content": "Memory store is an in-memory key-value database for use cases such as real-time application."
      },
      {
        "date": "2024-04-15T12:09:00.000Z",
        "voteCount": 1,
        "content": "Let me complete your answer MS vs BQ in this case is a matter of low latence where MS is the winner but if precision were stated about a large amount of data BQ then would\"ve been the best choice."
      },
      {
        "date": "2024-01-30T10:57:00.000Z",
        "voteCount": 1,
        "content": "why not D?"
      },
      {
        "date": "2024-02-26T02:59:00.000Z",
        "voteCount": 1,
        "content": "Because BigQuery is not a low latency system..."
      },
      {
        "date": "2024-01-03T21:14:00.000Z",
        "voteCount": 2,
        "content": "B. Group the data by using a hopping window in a Dataflow pipeline, and write the aggregated data to Memorystore."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 278,
    "url": "https://www.examtopics.com/discussions/google/view/130263-exam-professional-data-engineer-topic-1-question-278/",
    "body": "Your car factory is pushing machine measurements as messages into a Pub/Sub topic in your Google Cloud project. A Dataflow streaming job, that you wrote with the Apache Beam SDK, reads these messages, sends acknowledgment to Pub/Sub, applies some custom business logic in a DoFn instance, and writes the result to BigQuery. You want to ensure that if your business logic fails on a message, the message will be sent to a Pub/Sub topic that you want to monitor for alerting purposes. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable retaining of acknowledged messages in your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the subscription/num_retained_acked_messages metric on this subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse an exception handling block in your Dataflow\u2019s DoFn code to push the messages that failed to be transformed through a side output and to a new Pub/Sub topic. Use Cloud Monitoring to monitor the topic/num_unacked_messages_by_region metric on this new topic.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable dead lettering in your Pub/Sub pull subscription, and specify a new Pub/Sub topic as the dead letter topic. Use Cloud Monitoring to monitor the subscription/dead_letter_message_count metric on your pull subscription.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a snapshot of your Pub/Sub pull subscription. Use Cloud Monitoring to monitor the snapshot/num_messages metric on this snapshot."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 16,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T14:57:00.000Z",
        "voteCount": 11,
        "content": "- Exception Handling in DoFn: Implementing an exception handling block within DoFn in Dataflow to catch failures during processing is a direct way to manage errors.\n- Side Output to New Topic: Using a side output to redirect failed messages to a new Pub/Sub topic is an effective way to isolate and manage these messages.\n- Monitoring: Monitoring the num_unacked_messages_by_region on the new topic can alert you to the presence of failed messages."
      },
      {
        "date": "2024-09-25T10:56:00.000Z",
        "voteCount": 1,
        "content": "Option C - dead letter topic is built in and requires no changes https://cloud.google.com/pubsub/docs/handling-failures\n\nEnable dead lettering in your Pub/Sub pull subscription, and specify a new Pub/Sub topic as the dead letter topic. Use Cloud Monitoring to monitor the subscription/dead_letter_message_count metric on your pull subscription."
      },
      {
        "date": "2024-09-14T02:59:00.000Z",
        "voteCount": 1,
        "content": "See here:\nhttps://cloud.google.com/dataflow/docs/concepts/streaming-with-cloud-pubsub#unsupported-features\nIt's not recommended to use Pub/Sub dead-letter topics with Dataflow (...) Instead, implement the dead-letter pattern explicitly in the pipeline"
      },
      {
        "date": "2024-07-23T22:03:00.000Z",
        "voteCount": 1,
        "content": "Option B.\n\n Here's why:\n\n    Side Output for Failed Messages: Dataflow allows you to use side outputs to handle messages that fail processing. In your DoFn , you can catch exceptions and write the failed messages to a separate PCollection . This PCollection can then be written to a new Pub/Sub topic.\n    New Pub/Sub Topic for Monitoring: Creating a dedicated Pub/Sub topic for failed messages allows you to monitor it specifically for alerting purposes. This provides a clear view of any issues with your business logic.\n    topic/num_unacked_messages_by_region Metric: This Cloud Monitoring metric tracks the number of unacknowledged messages in a Pub/Sub topic. By monitoring this metric on your new topic, you can identify when messages are failing to be processed correctly."
      },
      {
        "date": "2024-04-09T00:36:00.000Z",
        "voteCount": 1,
        "content": "I would like to know why isn't anyone considering the option C."
      },
      {
        "date": "2024-04-09T00:45:00.000Z",
        "voteCount": 2,
        "content": "I think that C is not right anyways: In order to use dead_letter feature, the message CANNOT be acknowledge (somehow) by the subscriber. In this question it says that the messages are first acknowledge and then applied the business logic. So, if there are error in the business logic we cannot use the feature dead_letter, beacuse the message was already acknowledge. Thus, option B is the right one."
      },
      {
        "date": "2024-03-15T13:48:00.000Z",
        "voteCount": 1,
        "content": "option B"
      },
      {
        "date": "2024-02-21T00:30:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-13T09:28:00.000Z",
        "voteCount": 1,
        "content": "Option B - Raaad explanation is complete"
      },
      {
        "date": "2024-01-03T21:24:00.000Z",
        "voteCount": 1,
        "content": "B. Use an exception handling block in your Dataflow\u2019s DoFn code to push the messages that failed to be transformed through a side output and to a new Pub/Sub topic. Use Cloud Monitoring to monitor the topic/num_unacked_messages_by_region metric on this new topic."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 279,
    "url": "https://www.examtopics.com/discussions/google/view/130264-exam-professional-data-engineer-topic-1-question-279/",
    "body": "You want to store your team\u2019s shared tables in a single dataset to make data easily accessible to various analysts. You want to make this data readable but unmodifiable by analysts. At the same time, you want to provide the analysts with individual workspaces in the same project, where they can create and store tables for their own use, without the tables being accessible by other analysts. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive analysts the BigQuery Data Viewer role at the project level. Create one other dataset, and give the analysts the BigQuery Data Editor role on that dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive analysts the BigQuery Data Viewer role at the project level. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the project level.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGive analysts the BigQuery Data Viewer role on the shared dataset. Create one other dataset and give the analysts the BigQuery Data Editor role on that dataset."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T15:05:00.000Z",
        "voteCount": 9,
        "content": "- Data Viewer on Shared Dataset: Grants read-only access to the shared dataset.\n- Data Editor on Individual Datasets: Giving each analyst Data Editor role on their respective dataset creates private workspaces where they can create and store personal tables without exposing them to other analysts."
      },
      {
        "date": "2024-08-10T02:15:00.000Z",
        "voteCount": 1,
        "content": "Will GO with C"
      },
      {
        "date": "2024-03-15T13:45:00.000Z",
        "voteCount": 1,
        "content": "voted C"
      },
      {
        "date": "2024-02-21T00:36:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T09:34:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-09T08:44:00.000Z",
        "voteCount": 2,
        "content": "option C, because analysts can not see the individual datasets of other analysts"
      },
      {
        "date": "2024-01-03T21:26:00.000Z",
        "voteCount": 2,
        "content": "C. Give analysts the BigQuery Data Viewer role on the shared dataset. Create a dataset for each analyst, and give each analyst the BigQuery Data Editor role at the dataset level for their assigned dataset."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 280,
    "url": "https://www.examtopics.com/discussions/google/view/130265-exam-professional-data-engineer-topic-1-question-280/",
    "body": "You are running a streaming pipeline with Dataflow and are using hopping windows to group the data as the data arrives. You noticed that some data is arriving late but is not being marked as late data, which is resulting in inaccurate aggregations downstream. You need to find a solution that allows you to capture the late data in the appropriate window. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse watermarks to define the expected data arrival window. Allow late data as it arrives.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange your windowing function to tumbling windows to avoid overlapping window periods.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange your windowing function to session windows to define your windows based on certain activity.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tExpand your hopping window so that the late data has more time to arrive within the grouping."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T15:11:00.000Z",
        "voteCount": 6,
        "content": "- Watermarks: Watermarks in a streaming pipeline are used to specify the point in time when Dataflow expects all data up to that point to have arrived. \n- Allow Late Data: configure the pipeline to accept and correctly process data that arrives after the watermark, ensuring it's captured in the appropriate window."
      },
      {
        "date": "2024-02-21T01:11:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T09:35:00.000Z",
        "voteCount": 3,
        "content": "Option A - https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks"
      },
      {
        "date": "2024-01-09T08:37:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/dataflow/docs/concepts/streaming-pipelines#watermarks"
      },
      {
        "date": "2024-01-03T21:32:00.000Z",
        "voteCount": 1,
        "content": "A. Use watermarks to define the expected data arrival window. Allow late data as it arrives."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 281,
    "url": "https://www.examtopics.com/discussions/google/view/130269-exam-professional-data-engineer-topic-1-question-281/",
    "body": "You work for a large ecommerce company. You store your customer's order data in Bigtable. You have a garbage collection policy set to delete the data after 30 days and the number of versions is set to 1. When the data analysts run a query to report total customer spending, the analysts sometimes see customer data that is older than 30 days. You need to ensure that the analysts do not see customer data older than 30 days while minimizing cost and overhead. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the expiring values of the column families to 29 days and keep the number of versions to 1.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a timestamp range filter in the query to fetch the customer's data for a specific range.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSchedule a job daily to scan the data in the table and delete data older than 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the expiring values of the column families to 30 days and set the number of versions to 2."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T09:38:00.000Z",
        "voteCount": 6,
        "content": "Agree with others https://cloud.google.com/bigtable/docs/garbage-collection"
      },
      {
        "date": "2024-01-24T22:58:00.000Z",
        "voteCount": 8,
        "content": "Agree. https://cloud.google.com/bigtable/docs/garbage-collection#data-removed\n\"Because it can take up to a week for expired data to be deleted, you should never rely solely on garbage collection policies to ensure that read requests return the desired data. Always apply a filter to your read requests that excludes the same values as your garbage collection rules. You can filter by limiting the number of cells per column or by specifying a timestamp range.\""
      },
      {
        "date": "2024-02-25T16:51:00.000Z",
        "voteCount": 5,
        "content": "Agree with MAtt_108 and AllenChen 123.\n\"Garbage collection is a continuous process in which Bigtable checks the rules for each column family and deletes expired and obsolete data accordingly. In general, it can take up to a week from the time that data matches the criteria in the rules for the data to actually be deleted. You are not able to change the timing of garbage collection.\"\n\n\"Always apply a filter to your read requests that exclude the same values as your garbage collection rules. \"\n\nRef: https://cloud.google.com/bigtable/docs/garbage-collection#data-removed"
      },
      {
        "date": "2024-01-10T00:58:00.000Z",
        "voteCount": 1,
        "content": "I will go for B too"
      },
      {
        "date": "2024-01-07T04:15:00.000Z",
        "voteCount": 3,
        "content": "B.  Use a timestamp range filter in the query to fetch the customer's data for a specific range.\n\nAlways use query filter as garbage collectore runs on it's way - https://cloud.google.com/bigtable/docs/garbage-collection"
      },
      {
        "date": "2024-01-03T22:28:00.000Z",
        "voteCount": 1,
        "content": "B. Use a timestamp range filter in the query to fetch the customer's data for a specific range."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 282,
    "url": "https://www.examtopics.com/discussions/google/view/130157-exam-professional-data-engineer-topic-1-question-282/",
    "body": "You are using a Dataflow streaming job to read messages from a message bus that does not support exactly-once delivery. Your job then applies some transformations, and loads the result into BigQuery. You want to ensure that your data is being streamed into BigQuery with exactly-once delivery semantics. You expect your ingestion throughput into BigQuery to be about 1.5 GB per second. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Storage Write API and ensure that your target BigQuery table is regional.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Storage Write API and ensure that your target BigQuery table is multiregional.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Streaming API and ensure that your target BigQuery table is regional.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Streaming API and ensure that your target BigQuery table is multiregional."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 7,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T15:56:00.000Z",
        "voteCount": 8,
        "content": "- BigQuery Storage Write API: This API is designed for high-throughput, low-latency writing of data into BigQuery. It also provides tools to prevent data duplication, which is essential for exactly-once delivery semantics.\n- Regional Table: Choosing a regional location for the BigQuery table could potentially provide better performance and lower latency, as it would be closer to the Dataflow job if they are in the same region."
      },
      {
        "date": "2024-01-24T23:04:00.000Z",
        "voteCount": 4,
        "content": "Agree.\nhttps://cloud.google.com/bigquery/docs/write-api#advantages"
      },
      {
        "date": "2024-06-05T08:12:00.000Z",
        "voteCount": 7,
        "content": "It should B, Storage Write API has \"3 GB per second throughput in multi-regions; 300 MB per second in regions\""
      },
      {
        "date": "2024-09-29T22:09:00.000Z",
        "voteCount": 1,
        "content": "To ensure that analysts do not see customer data older than 30 days while minimizing cost and overhead, the best option is:\nB. Use a timestamp range filter in the query to fetch the customer\u2019s data for a specific range.\n\nThis approach directly addresses the issue by filtering out data older than 30 days at query time, ensuring that only the relevant data is retrieved. It avoids the overhead and potential delays associated with garbage collection and manual deletion processes"
      },
      {
        "date": "2024-03-15T09:16:00.000Z",
        "voteCount": 1,
        "content": "option D"
      },
      {
        "date": "2024-04-06T23:22:00.000Z",
        "voteCount": 1,
        "content": "you are wrong!!!!!!!!!!!!"
      },
      {
        "date": "2024-01-13T09:41:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-02T19:06:00.000Z",
        "voteCount": 2,
        "content": "Voting on A"
      },
      {
        "date": "2024-01-07T09:32:00.000Z",
        "voteCount": 4,
        "content": "This option leverages the BigQuery Storage Write API's capability for exactly-once delivery semantics and a regional table setting that can meet compliance and data locality needs without impacting the delivery semantics. The BigQuery Storage Write API is more suitable for your high-throughput requirements compared to the BigQuery Streaming API."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 283,
    "url": "https://www.examtopics.com/discussions/google/view/130510-exam-professional-data-engineer-topic-1-question-283/",
    "body": "You have created an external table for Apache Hive partitioned data that resides in a Cloud Storage bucket, which contains a large number of files. You notice that queries against this table are slow. You want to improve the performance of these queries. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tChange the storage class of the Hive partitioned data objects from Coldline to Standard.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an individual external table for each Hive partition by using a common table name prefix. Use wildcard table queries to reference the partitioned data.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpgrade the external table to a BigLake table. Enable metadata caching for the table.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate the Hive partitioned data objects to a multi-region Cloud Storage bucket."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T16:01:00.000Z",
        "voteCount": 7,
        "content": "- BigLake Table: BigLake allows for more efficient querying of data lakes stored in Cloud Storage. It can handle large datasets more effectively than standard external tables.\n- Metadata Caching: Enabling metadata caching can significantly improve query performance by reducing the time taken to read and process metadata from a large number of files."
      },
      {
        "date": "2024-01-20T23:33:00.000Z",
        "voteCount": 4,
        "content": "Agree. https://cloud.google.com/bigquery/docs/biglake-intro#metadata_caching_for_performance"
      },
      {
        "date": "2024-01-24T23:08:00.000Z",
        "voteCount": 5,
        "content": "And https://cloud.google.com/bigquery/docs/external-data-cloud-storage#upgrade-external-tables-to-biglake-tables"
      },
      {
        "date": "2024-03-15T09:15:00.000Z",
        "voteCount": 1,
        "content": "vote C"
      },
      {
        "date": "2024-02-21T03:13:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T09:43:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-10T01:17:00.000Z",
        "voteCount": 1,
        "content": "agree with C"
      },
      {
        "date": "2024-01-07T08:17:00.000Z",
        "voteCount": 2,
        "content": "C. Upgrade the external table to a BigLake table. Enable metadata caching for the table. \nCheck ref - https://cloud.google.com/bigquery/docs/biglake-intro"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 284,
    "url": "https://www.examtopics.com/discussions/google/view/130273-exam-professional-data-engineer-topic-1-question-284/",
    "body": "You have a network of 1000 sensors. The sensors generate time series data: one metric per sensor per second, along with a timestamp. You already have 1 TB of data, and expect the data to grow by 1 GB every day. You need to access this data in two ways. The first access pattern requires retrieving the metric from one specific sensor stored at a specific timestamp, with a median single-digit millisecond latency. The second access pattern requires running complex analytic queries on the data, including joins, once a day. How should you store this data?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in BigQuery. Concatenate the sensor ID and timestamp, and use it as the primary key.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in Bigtable. Concatenate the sensor ID and timestamp and use it as the row key. Perform an export to BigQuery every day.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in Bigtable. Concatenate the sensor ID and metric, and use it as the row key. Perform an export to BigQuery every day.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tStore your data in BigQuery. Use the metric as a primary key."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 22,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T16:14:00.000Z",
        "voteCount": 14,
        "content": "- Bigtable excels at incredibly fast lookups by row key, often reaching single-digit millisecond latencies. \n- Constructing the row key with sensor ID and timestamp enables efficient retrieval of specific sensor readings at exact timestamps.\n- Bigtable's wide-column design effectively stores time series data, allowing for flexible addition of new metrics without schema changes.\n- Bigtable scales horizontally to accommodate massive datasets (petabytes or more), easily handling the expected data growth."
      },
      {
        "date": "2024-06-17T03:57:00.000Z",
        "voteCount": 2,
        "content": "agree with raaad"
      },
      {
        "date": "2024-03-15T09:04:00.000Z",
        "voteCount": 1,
        "content": "voted b"
      },
      {
        "date": "2024-02-21T03:16:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-01-13T09:45:00.000Z",
        "voteCount": 1,
        "content": "Option B - agree with raaad"
      },
      {
        "date": "2024-01-03T23:37:00.000Z",
        "voteCount": 3,
        "content": "B. Store your data in Bigtable. Concatenate the sensor ID and timestamp and use it as the row key. Perform an export to BigQuery every day."
      },
      {
        "date": "2024-01-07T09:38:00.000Z",
        "voteCount": 4,
        "content": "Based on your requirements, Option B seems most suitable. Bigtable's design caters to the low-latency access of time-series data (your first requirement), and the daily export to BigQuery enables complex analytics (your second requirement). The use of sensor ID and timestamp as the row key in Bigtable would facilitate efficient access to specific sensor data at specific times."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 285,
    "url": "https://www.examtopics.com/discussions/google/view/130511-exam-professional-data-engineer-topic-1-question-285/",
    "body": "You have 100 GB of data stored in a BigQuery table. This data is outdated and will only be accessed one or two times a year for analytics with SQL. For backup purposes, you want to store this data to be immutable for 3 years. You want to minimize storage costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a BigQuery table clone.<br>2. Query the clone when you need to perform analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a BigQuery table snapshot.<br>2. Restore the snapshot when you need to perform analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.<br>2. Enable versioning on the bucket.<br>3. Create a BigQuery external table on the exported files.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Perform a BigQuery export to a Cloud Storage bucket with archive storage class.<br>2. Set a locked retention policy on the bucket.<br>3. Create a BigQuery external table on the exported files.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T04:54:00.000Z",
        "voteCount": 5,
        "content": "Straight Forward"
      },
      {
        "date": "2024-06-17T03:59:00.000Z",
        "voteCount": 2,
        "content": "For data keeping till last 3 years, use bucket lock with rentention policy"
      },
      {
        "date": "2024-03-15T08:00:00.000Z",
        "voteCount": 1,
        "content": "voted D"
      },
      {
        "date": "2024-02-21T03:17:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T09:46:00.000Z",
        "voteCount": 2,
        "content": "Option D, clearly"
      },
      {
        "date": "2024-01-07T08:26:00.000Z",
        "voteCount": 2,
        "content": "D. \nFor data keeping till last 3 years, use bucket lock with rentention policy"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 286,
    "url": "https://www.examtopics.com/discussions/google/view/130287-exam-professional-data-engineer-topic-1-question-286/",
    "body": "You have thousands of Apache Spark jobs running in your on-premises Apache Hadoop cluster. You want to migrate the jobs to Google Cloud. You want to use managed services to run your jobs instead of maintaining a long-lived Hadoop cluster yourself. You have a tight timeline and want to keep code changes to a minimum. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove your data to BigQuery. Convert your Spark scripts to a SQL-based processing approach.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRewrite your jobs in Apache Beam. Run your jobs in Dataflow.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCopy your data to Compute Engine disks. Manage and run your jobs directly on those instances.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove your data to Cloud Storage. Run your jobs on Dataproc.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-08-10T01:37:00.000Z",
        "voteCount": 1,
        "content": "option D, minimum code changes"
      },
      {
        "date": "2024-03-15T07:56:00.000Z",
        "voteCount": 2,
        "content": "option D, minimum code changes"
      },
      {
        "date": "2024-02-21T03:22:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-02-18T09:59:00.000Z",
        "voteCount": 3,
        "content": "D) That is what Dataproc is made for. It is a fully managed and highly scalable service for running Apache Hadoop, Apache Spark, etc."
      },
      {
        "date": "2024-01-13T10:11:00.000Z",
        "voteCount": 2,
        "content": "Clearly D"
      },
      {
        "date": "2024-01-10T01:27:00.000Z",
        "voteCount": 2,
        "content": "of course D"
      },
      {
        "date": "2024-01-07T08:29:00.000Z",
        "voteCount": 3,
        "content": "D. Move your data to Cloud Storage. Run your jobs on Dataproc. \nDataproc is managed service and not needed much code changes."
      },
      {
        "date": "2024-01-04T02:41:00.000Z",
        "voteCount": 3,
        "content": "D. Move your data to Cloud Storage. Run your jobs on Dataproc."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 287,
    "url": "https://www.examtopics.com/discussions/google/view/130289-exam-professional-data-engineer-topic-1-question-287/",
    "body": "You are administering shared BigQuery datasets that contain views used by multiple teams in your organization. The marketing team is concerned about the variability of their monthly BigQuery analytics spend using the on-demand billing model. You need to help the marketing team establish a consistent BigQuery analytics spend each month. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEstablish a BigQuery quota for the marketing team, and limit the maximum number of bytes scanned each day.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery Standard pay-as-you go reservation with a baseline of 0 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 5,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 3,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T16:55:00.000Z",
        "voteCount": 9,
        "content": "Reservations guarantee a fixed number of slots (computational resources) for BigQuery queries, ensuring a predictable monthly cost, addressing the marketing team's concern about variability."
      },
      {
        "date": "2024-01-21T00:28:00.000Z",
        "voteCount": 3,
        "content": "Why 500 slots?"
      },
      {
        "date": "2024-01-24T23:28:00.000Z",
        "voteCount": 3,
        "content": "But seems only C makes sense.\nhttps://cloud.google.com/bigquery/quotas#query_jobs\n\"There is no limit to the number of bytes that can be processed by queries in a project.\""
      },
      {
        "date": "2024-01-29T04:48:00.000Z",
        "voteCount": 2,
        "content": "\"However, you can set limits on the amount of data users can query by creating custom quotas to control query usage per day or query usage per day per user.\"\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas\nB would be correct"
      },
      {
        "date": "2024-02-09T00:27:00.000Z",
        "voteCount": 4,
        "content": "If you use B - the marketing team wouldn't be able to run their queries when the quota is reached, which could harm the business. \n\nHaving a reservation for 500 slots and no autoscaling gives you exact predictable cost for each month without harming the business or have variable cost with autoscaling \n\nSo C should be the right answer"
      },
      {
        "date": "2024-10-08T05:02:00.000Z",
        "voteCount": 1,
        "content": "Just to clarify a point of confusion: setting a quota does not affect variability (as specified in the question). It means there is a limit to the maximum but it can still vary anywhere between zero and that maximum each month. It would also prevent the marking team actually performing the queries if set too low. C is the only one that makes sense, though the question \"why 500\" is a valid one, all the other answers simply do not deliver the requirements."
      },
      {
        "date": "2024-09-25T18:30:00.000Z",
        "voteCount": 1,
        "content": "It should be B\nC is a subset of B. I mean you can put custom quota == 500 slots and obviously there wont be any auto scaling. that exactly the purpose of quota"
      },
      {
        "date": "2024-09-25T12:15:00.000Z",
        "voteCount": 1,
        "content": "A Create a BigQuery Enterprise reservation with a baseline of 250 slots and autoscaling set to 500 for the marketing team, and bill them back accordingly.\n\nGIves a consistent baseline cost and allocation for peak times"
      },
      {
        "date": "2024-09-24T16:49:00.000Z",
        "voteCount": 1,
        "content": "The objective here is not performance. It's more concerned about the spend each month. It's not about 250 slots or 500 slots. Selecting a custom quota will let you select what ever slot you want but stay consistent with it, rather than getting stick by a particular slot option."
      },
      {
        "date": "2024-09-24T16:46:00.000Z",
        "voteCount": 1,
        "content": "Custom Quota\nIf you have multiple BigQuery projects and users, you can manage costs by requesting a custom quota that specifies a limit on the amount of query data processed per day. Daily quotas are reset at midnight Pacific Time.\n\nCustom quota is proactive, so you can't run an 11 TB query if you have a 10 TB quota. Creating a custom quota on query data lets you control costs at the project level or at the user level.\n\nProject-level custom quotas limit the aggregate usage of all users in that project."
      },
      {
        "date": "2024-09-10T19:06:00.000Z",
        "voteCount": 2,
        "content": "I think is A"
      },
      {
        "date": "2024-08-17T18:22:00.000Z",
        "voteCount": 1,
        "content": "Balance of Cost and Performance: By setting a baseline of slots to ensure a minimum cost, and enabling autoscaling for flexible scaling based on demand, we can achieve an optimal balance between cost and performance.\nCost Control: Setting a maximum number of slots allows us to clearly define a cost ceiling.\nBenefits of Enterprise Reservations: Enterprise reservations offer a higher discount rate compared to Standard reservations, and provide more stable performance."
      },
      {
        "date": "2024-08-10T01:26:00.000Z",
        "voteCount": 2,
        "content": "Yes, you can reserve slots in BigQuery for on-demand billing:\nSlot recommender\nUse the slot recommender to get cost-optimized recommendations for on-demand workloads.\nCapacity commitment\nPurchase a capacity commitment to reserve capacity for a minimum amount of time and save on costs.\nCapacity-based pricing\nUse reservations to switch to capacity-based pricing, which lets you reserve a volume of slots. You pay for that capacity continuously every second it's deployed."
      },
      {
        "date": "2024-07-21T10:44:00.000Z",
        "voteCount": 2,
        "content": "Explanation:\nPredictability: The baseline of 250 slots ensures a predictable minimum spend each month.\nFlexibility: Autoscaling up to 500 slots allows the team to handle peak workloads without interruptions.\nBalanced Cost: While Option B limits daily spend, it can lead to disruptions in service. Option A offers a consistent monthly cost while still accommodating variable workloads efficiently."
      },
      {
        "date": "2024-06-27T07:44:00.000Z",
        "voteCount": 1,
        "content": "Why 500 slots?"
      },
      {
        "date": "2024-06-17T04:19:00.000Z",
        "voteCount": 1,
        "content": "C. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly."
      },
      {
        "date": "2024-05-23T03:56:00.000Z",
        "voteCount": 1,
        "content": "C. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly."
      },
      {
        "date": "2024-05-18T06:40:00.000Z",
        "voteCount": 3,
        "content": "The question clearly mentions, that team is using the on-demand billing mode in BiqQuery, which charges for the number of bytes processed by each query. So limiting the bytes processed will be the solution.\nhttps://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas"
      },
      {
        "date": "2024-06-15T21:21:00.000Z",
        "voteCount": 1,
        "content": "C. Create a BigQuery reservation with a baseline of 500 slots with no autoscaling for the marketing team, and bill them back accordingly.\n\nThis option provides the marketing team with a predictable monthly cost by reserving a fixed number of slots, ensuring that they have dedicated resources without the variability introduced by autoscaling or on-demand pricing. This setup also simplifies budgeting and financial planning for the marketing team, as they will have a consistent expense each month."
      },
      {
        "date": "2024-04-24T01:59:00.000Z",
        "voteCount": 3,
        "content": "at first I thought C for best practices but the questions does not ask to lower the cost just to make the spend consistent"
      },
      {
        "date": "2024-04-26T03:09:00.000Z",
        "voteCount": 1,
        "content": "u spot on MissK1371"
      },
      {
        "date": "2024-04-11T14:24:00.000Z",
        "voteCount": 1,
        "content": "As wrote @Sofia98 the company using \"on-demand billing model\", so the best solution should be the B, https://cloud.google.com/blog/products/data-analytics/manage-bigquery-costs-with-custom-quotas"
      },
      {
        "date": "2024-02-18T10:17:00.000Z",
        "voteCount": 2,
        "content": "I agree that at first sight option C seems best. However, the question mentions that they are currently on the on-demand billing model and option C does not mention anything about changing the pricing model from on-demand to capacity computing (BigQuery Standard or Enterprise edition). I don't believe it is possible to reserve slots with on-demand billing."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 288,
    "url": "https://www.examtopics.com/discussions/google/view/130291-exam-professional-data-engineer-topic-1-question-288/",
    "body": "You are part of a healthcare organization where data is organized and managed by respective data owners in various storage services. As a result of this decentralized ecosystem, discovering and managing data has become difficult. You need to quickly identify and implement a cost-optimized solution to assist your organization with the following:<br><br>\u2022\tData management and discovery<br>\u2022\tData lineage tracking<br>\u2022\tData quality validation<br><br>How should you build the solution?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigLake to convert the current solution into a data lake architecture.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tBuild a new data discovery tool on Google Kubernetes Engine that helps with new source onboarding and data lineage tracking.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery to track data lineage, and use Dataprep to manage data and perform data quality validation.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataplex to manage data, track data lineage, and perform data quality validation.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 17,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-07-21T10:52:00.000Z",
        "voteCount": 1,
        "content": "D. Dataplex"
      },
      {
        "date": "2024-06-17T04:23:00.000Z",
        "voteCount": 2,
        "content": "Option D, no doubt"
      },
      {
        "date": "2024-03-15T07:43:00.000Z",
        "voteCount": 2,
        "content": "Option D, no doubt"
      },
      {
        "date": "2024-02-21T09:34:00.000Z",
        "voteCount": 2,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T10:22:00.000Z",
        "voteCount": 3,
        "content": "Clearly D"
      },
      {
        "date": "2024-01-10T01:51:00.000Z",
        "voteCount": 2,
        "content": "Agree with Dataplex option"
      },
      {
        "date": "2024-01-09T16:56:00.000Z",
        "voteCount": 4,
        "content": "Straight forward"
      },
      {
        "date": "2024-01-04T03:10:00.000Z",
        "voteCount": 1,
        "content": "D. Use Dataplex to manage data, track data lineage, and perform data quality validation."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 289,
    "url": "https://www.examtopics.com/discussions/google/view/130292-exam-professional-data-engineer-topic-1-question-289/",
    "body": "You have data located in BigQuery that is used to generate reports for your company. You have noticed some weekly executive report fields do not correspond to format according to company standards. For example, report errors include different telephone formats and different country code identifiers. This is a frequent issue, so you need to create a recurring job to normalize the data. You want a quick solution that requires no coding. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow SQL to create a job that normalizes the data, and that after the first run of the job, schedule the pipeline to execute recurrently.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Spark job and submit it to Dataproc Serverless.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse BigQuery and GoogleSQL to normalize the data, and schedule recurring queries in BigQuery."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-13T10:23:00.000Z",
        "voteCount": 7,
        "content": "Definitely A, cloud data fusion and wrangler to setup the clean up pipeline with no coding required"
      },
      {
        "date": "2024-07-21T10:59:00.000Z",
        "voteCount": 1,
        "content": "A. Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job.\n\nExplanation\nNo Coding Required: Cloud Data Fusion's Wrangler offers a no-code interface for data transformation tasks. You can visually design data normalization workflows without writing any code.\nRecurring Jobs: Cloud Data Fusion allows you to schedule these data normalization tasks to run on a recurring basis, meeting your need for automation."
      },
      {
        "date": "2024-07-16T12:41:00.000Z",
        "voteCount": 1,
        "content": "The best solution here is D. Use BigQuery and GoogleSQL to normalize the data, and schedule recurring queries in BigQuery.\n\nHere's why:\n\n* No-code solution: BigQuery's built-in capabilities and GoogleSQL offer a no-code way to transform and standardize data. You can leverage functions like REGEXP_REPLACE to normalize phone numbers and FORMAT to ensure consistent formatting across fields.\n* Recurring jobs: BigQuery allows you to schedule queries to run regularly, which is perfect for maintaining data consistency over time.\n* Quick and efficient: BigQuery is designed for large-scale data processing, making it fast and efficient for normalization tasks."
      },
      {
        "date": "2024-07-16T12:42:00.000Z",
        "voteCount": 1,
        "content": "Why other options aren't as suitable:\n\nA. Cloud Data Fusion and Wrangler: While powerful, these tools might be overkill for a simple normalization task and could involve a steeper learning curve.\nB. Dataflow SQL: Dataflow is primarily for stream processing and might not be the most efficient for batch transformations on data already in BigQuery.\nC. Dataproc Serverless: This involves using a Spark job, which requires coding and might be more complex than necessary for this task."
      },
      {
        "date": "2024-06-17T04:33:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/data-fusion/docs"
      },
      {
        "date": "2024-02-25T04:12:00.000Z",
        "voteCount": 1,
        "content": "As per chatGPT, Option D allows you to utilize BigQuery's SQL capabilities to write queries that normalize the data according to company standards.\nYou can then schedule these queries to run on a recurring basis using BigQuery's scheduled queries feature. This feature allows you to specify a schedule (e.g., weekly) for executing SQL queries automatically.\nThis approach requires no additional setup or coding outside of BigQuery, making it a quick and straightforward solution to address the issue of data normalization."
      },
      {
        "date": "2024-02-25T04:13:00.000Z",
        "voteCount": 1,
        "content": "Any views on this ?"
      },
      {
        "date": "2024-02-26T03:29:00.000Z",
        "voteCount": 6,
        "content": "Wouldn't writing the SQL transformation be considered coding? The question specifically states that a solution requiring no coding is needed."
      },
      {
        "date": "2024-03-18T23:13:00.000Z",
        "voteCount": 1,
        "content": "While Cloud Data Fusion with Wrangler offers a visual interface for data wrangling, it requires setting up the environment and potentially writing code for ransformations.  So it its not appropriate.   I think D"
      },
      {
        "date": "2024-02-21T09:35:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-10T02:04:00.000Z",
        "voteCount": 2,
        "content": "Cloud Data Fusion and Wrangler"
      },
      {
        "date": "2024-01-04T03:12:00.000Z",
        "voteCount": 2,
        "content": "A. Use Cloud Data Fusion and Wrangler to normalize the data, and set up a recurring job."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 290,
    "url": "https://www.examtopics.com/discussions/google/view/130293-exam-professional-data-engineer-topic-1-question-290/",
    "body": "You are designing a messaging system by using Pub/Sub to process clickstream data with an event-driven consumer app that relies on a push subscription. You need to configure the messaging system that is reliable enough to handle temporary downtime of the consumer app. You also need the messaging system to store the input messages that cannot be consumed by the subscriber. The system needs to retry failed messages gradually, avoiding overloading the consumer app, and store the failed messages after a maximum of 10 retries in a topic. How should you configure the Pub/Sub subscription?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tIncrease the acknowledgement deadline to 10 minutes.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse immediate redelivery as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse exponential backoff as the subscription retry policy, and configure dead lettering to the same source topic with maximum delivery attempts set to 10.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T17:13:00.000Z",
        "voteCount": 14,
        "content": "- Exponential Backoff: This retry policy gradually increases the delay between retries, which helps to avoid overloading the consumer app. \n- Dead Lettering to a Different Topic: Configuring dead lettering sends messages that couldn't be processed after the specified number of delivery attempts (10 in this case) to a separate topic. This allows for handling of failed messages without interrupting the regular flow of new messages.\n- Maximum Delivery Attempts Set to 10: This setting ensures that the system retries each message up to 10 times before considering it a failure and moving it to the dead letter topic."
      },
      {
        "date": "2024-02-21T09:43:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T10:25:00.000Z",
        "voteCount": 1,
        "content": "Option D - agree with other comments explanation"
      },
      {
        "date": "2024-01-07T08:59:00.000Z",
        "voteCount": 3,
        "content": "D. Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10\n\nBest suitable options for graceful retry and storing failed messages"
      },
      {
        "date": "2024-01-04T03:15:00.000Z",
        "voteCount": 2,
        "content": "D. Use exponential backoff as the subscription retry policy, and configure dead lettering to a different topic with maximum delivery attempts set to 10."
      },
      {
        "date": "2024-01-07T09:52:00.000Z",
        "voteCount": 2,
        "content": "Exponential backoff will help in managing the load on the consumer app by gradually increasing the delay between retries. Configuring dead lettering to a different topic after a maximum of 10 delivery attempts ensures that undeliverable messages are stored separately, preventing them from being retried endlessly and cluttering the main message flow."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 291,
    "url": "https://www.examtopics.com/discussions/google/view/130296-exam-professional-data-engineer-topic-1-question-291/",
    "body": "You designed a data warehouse in BigQuery to analyze sales data. You want a self-serving, low-maintenance, and cost- effective solution to share the sales dataset to other business units in your organization. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate an Analytics Hub private exchange, and publish the sales dataset.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable the other business units\u2019 projects to access the authorized views of the sales dataset.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate and share views with the users in the other business units.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the BigQuery Data Transfer Service to create a schedule that copies the sales dataset to the other business units\u2019 projects."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-09T17:18:00.000Z",
        "voteCount": 7,
        "content": "Analytics Hub offers a centralized platform for managing data sharing and access within the organization. This simplifies access control management."
      },
      {
        "date": "2024-07-21T11:06:00.000Z",
        "voteCount": 1,
        "content": "A. is the answer I select"
      },
      {
        "date": "2024-02-21T09:44:00.000Z",
        "voteCount": 1,
        "content": "Analytics Hub"
      },
      {
        "date": "2024-01-13T10:26:00.000Z",
        "voteCount": 1,
        "content": "Definitely A"
      },
      {
        "date": "2024-01-04T03:22:00.000Z",
        "voteCount": 1,
        "content": "A. Create an Analytics Hub private exchange, and publish the sales dataset."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 292,
    "url": "https://www.examtopics.com/discussions/google/view/130298-exam-professional-data-engineer-topic-1-question-292/",
    "body": "You have terabytes of customer behavioral data streaming from Google Analytics into BigQuery daily. Your customers\u2019 information, such as their preferences, is hosted on a Cloud SQL for MySQL database. Your CRM database is hosted on a Cloud SQL for PostgreSQL instance. The marketing team wants to use your customers\u2019 information from the two databases and the customer behavioral data to create marketing campaigns for yearly active customers. You need to ensure that the marketing team can run the campaigns over 100 times a day on typical days and up to 300 during sales. At the same time, you want to keep the load on the Cloud SQL databases to a minimum. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate BigQuery connections to both Cloud SQL databases. Use BigQuery federated queries on the two databases and the Google Analytics data on BigQuery to run these queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a job on Apache Spark with Dataproc Serverless to query both Cloud SQL databases and the Google Analytics data on BigQuery for these queries.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a Dataproc cluster with Trino to establish connections to both Cloud SQL databases and BigQuery, to execute the queries."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T05:02:00.000Z",
        "voteCount": 11,
        "content": "- Datastream: It's a fully managed, serverless service for real-time data replication. It allows to stream data from various sources, including Cloud SQL, into BigQuery.\n- Reduced Load on Cloud SQL: By replicating the required tables from both Cloud SQL databases into BigQuery, you minimize the load on the Cloud SQL instances. The marketing team's queries will be run against BigQuery, which is designed to handle high-volume analytics workloads.\n- Frequency of Queries: BigQuery can easily handle the high frequency of queries (100 times daily, up to 300 during sales events) due to its powerful data processing capabilities.\n- Combining Data Sources: Once the data is in BigQuery, you can efficiently combine it with the Google Analytics data for comprehensive analysis and campaign planning."
      },
      {
        "date": "2024-03-20T08:21:00.000Z",
        "voteCount": 1,
        "content": "Why not A ? Federrated queries will downgrade Cloud SQL perf?"
      },
      {
        "date": "2024-07-21T11:11:00.000Z",
        "voteCount": 1,
        "content": "Initially I said A, but this question was how I learned about Datastream, which I think would be the better solution in this scenario. So my answer is C"
      },
      {
        "date": "2024-06-05T08:33:00.000Z",
        "voteCount": 1,
        "content": "C, noting that federated queries on read replicas would be the ideal solution"
      },
      {
        "date": "2024-04-11T01:03:00.000Z",
        "voteCount": 1,
        "content": "Its option C.\n\n\"Performance. A federated query is likely to not be as fast as querying only BigQuery storage. BigQuery needs to wait for the source database to execute the external query and temporarily move data from the external data source to BigQuery. Also, the source database might not be optimized for complex analytical queries.\"\n\nSo, it will load the Cloud SQL external sources with the queries, impacting performance on those.\n\nLink: https://cloud.google.com/bigquery/docs/federated-queries-intro"
      },
      {
        "date": "2024-03-11T00:38:00.000Z",
        "voteCount": 1,
        "content": "C is make sense"
      },
      {
        "date": "2024-02-21T09:51:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-04T03:33:00.000Z",
        "voteCount": 3,
        "content": "C. Create streams in Datastream to replicate the required tables from both Cloud SQL databases to BigQuery for these queries."
      },
      {
        "date": "2024-01-07T09:57:00.000Z",
        "voteCount": 3,
        "content": "Datastream is a serverless, easy-to-use change data capture (CDC) and replication service. By replicating the necessary tables from the Cloud SQL databases to BigQuery, you can offload the query load from the Cloud SQL databases. The marketing team can then run their queries directly on BigQuery, which is designed for large-scale data analytics. This approach seems to balance both efficiency and performance, minimizing load on the Cloud SQL instances."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 293,
    "url": "https://www.examtopics.com/discussions/google/view/130301-exam-professional-data-engineer-topic-1-question-293/",
    "body": "Your organization is modernizing their IT services and migrating to Google Cloud. You need to organize the data that will be stored in Cloud Storage and BigQuery. You need to enable a data mesh approach to share the data between sales, product design, and marketing departments. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project for storage of the data for each of your departments.<br>2. Enable each department to create Cloud Storage buckets and BigQuery datasets.<br>3. Create user groups for authorized readers for each bucket and dataset.<br>4. Enable the IT team to administer the user groups to add or remove users as the departments\u2019 request.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create multiple projects for storage of the data for each of your departments\u2019 applications.<br>2. Enable each department to create Cloud Storage buckets and BigQuery datasets.<br>3. Publish the data that each department shared in Analytics Hub.<br>4. Enable all departments to discover and subscribe to the data they need in Analytics Hub.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a project for storage of the data for your organization.<br>2. Create a central Cloud Storage bucket with three folders to store the files for each department.<br>3. Create a central BigQuery dataset with tables prefixed with the department name.<br>4. Give viewer rights for the storage project for the users of your departments.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create multiple projects for storage of the data for each of your departments\u2019 applications.<br>2. Enable each department to create Cloud Storage buckets and BigQuery datasets.<br>3. In Dataplex, map each department to a data lake and the Cloud Storage buckets, and map the BigQuery datasets to zones.<br>4. Enable each department to own and share the data of their data lakes.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T09:10:00.000Z",
        "voteCount": 9,
        "content": "- Decentralized ownership: Each department controls its data lake, aligning with the core principle of data ownership in a data mesh.\n- Self-service data access: Departments can create and manage their own Cloud Storage buckets and BigQuery datasets within their data lakes, enabling self-service data access.\n- Interdepartmental sharing: Dataplex facilitates data sharing by enabling departments to publish their data products from their data lakes, making it easily discoverable and usable by other departments."
      },
      {
        "date": "2024-08-02T11:37:00.000Z",
        "voteCount": 1,
        "content": "B is better option as organization is migrating to google cloud, that means teams doesnt have much hands on, analytical hub is more ease to use and solved the purpose as compared to dataplex were setup itself if very complex."
      },
      {
        "date": "2024-07-21T11:18:00.000Z",
        "voteCount": 1,
        "content": "For a straightforward data mesh approach where the focus is on decentralizing data management while enabling easy data sharing and discovery, Analytics Hub is often the more appropriate choice due to its simplicity and directness. It facilitates the core objectives of a data mesh\u2014decentralized data ownership and accessible data sharing\u2014without the added complexity of managing data lakes and advanced governance features."
      },
      {
        "date": "2024-04-11T01:34:00.000Z",
        "voteCount": 1,
        "content": "I think its B. I know since we are talking about Datamesh we want to go to the Dataplex service suddenly. However, in Dataplex a Lake can only have assets (bq tables etc) that are in the same project as the Dataplex service. \n\nExample: There is bq table in project A and B. I want to to create a Lake in Dataplex in Project A that contains tables of project B. I can\u00b4t do that, i can only host tables of the Project A, since the Lake is in project A.\n\nWith this said, I think the best option is B, because the datamesh approach is related to \"to share the data between sales, product design, and marketing departments\". So the question is focusing only in the sharing part of the datamesh. Option B fits just fine."
      },
      {
        "date": "2024-04-11T01:39:00.000Z",
        "voteCount": 1,
        "content": "I was wrong in my explanation guys. Look at this link:\nhttps://cloud.google.com/dataplex/docs/add-zone\n\n\"A lake can include one or more zones. While a zone can only be part of one lake, it may contain assets that point to resources that are part of projects outside of its parent project.\"\n\nSo, option D seems good."
      },
      {
        "date": "2024-02-21T10:12:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-12T07:55:00.000Z",
        "voteCount": 2,
        "content": "that's pure data mesh, which is what dataplex has been built for"
      },
      {
        "date": "2024-01-10T09:02:00.000Z",
        "voteCount": 1,
        "content": "For me, Dataplex looks more logical"
      },
      {
        "date": "2024-01-07T03:59:00.000Z",
        "voteCount": 1,
        "content": "D.  Dataplex looks more suitable for data mesh approach, Check the ref - https://cloud.google.com/dataplex/docs/introduction"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 294,
    "url": "https://www.examtopics.com/discussions/google/view/130307-exam-professional-data-engineer-topic-1-question-294/",
    "body": "You work for a large ecommerce company. You are using Pub/Sub to ingest the clickstream data to Google Cloud for analytics. You observe that when a new subscriber connects to an existing topic to analyze data, they are unable to subscribe to older data. For an upcoming yearly sale event in two months, you need a solution that, once implemented, will enable any new subscriber to read the last 30 days of data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new topic, and publish the last 30 days of data each time a new subscriber connects to an existing topic.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the topic retention policy to 30 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet the subscriber retention policy to 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAsk the source system to re-push the data to Pub/Sub, and subscribe to it."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T09:13:00.000Z",
        "voteCount": 11,
        "content": "- Topic Retention Policy: This policy determines how long messages are retained by Pub/Sub after they are published, even if they have not been acknowledged (consumed) by any subscriber.\n- 30 Days Retention: By setting the retention policy of the topic to 30 days, all messages published to this topic will be available for consumption for 30 days. This means any new subscriber connecting to the topic can access and analyze data from the past 30 days."
      },
      {
        "date": "2024-04-11T01:54:00.000Z",
        "voteCount": 1,
        "content": "Its B. It could be C as well because subscription has message retention. However, in the subscription there is a maximum value for it: 7 days.\n\nLink:https://cloud.google.com/pubsub/docs/subscription-properties"
      },
      {
        "date": "2024-04-11T01:55:00.000Z",
        "voteCount": 1,
        "content": "In a topic the maximum value is 31 days.\n\nLink: https://cloud.google.com/pubsub/docs/topic-properties"
      },
      {
        "date": "2024-01-13T10:32:00.000Z",
        "voteCount": 2,
        "content": "Definitely B"
      },
      {
        "date": "2024-01-10T09:13:00.000Z",
        "voteCount": 4,
        "content": "https://cloud.google.com/blog/products/data-analytics/pubsub-gains-topic-retention-feature"
      },
      {
        "date": "2024-01-04T03:45:00.000Z",
        "voteCount": 1,
        "content": "B. Set the topic retention policy to 30 days."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 295,
    "url": "https://www.examtopics.com/discussions/google/view/130309-exam-professional-data-engineer-topic-1-question-295/",
    "body": "You are designing the architecture to process your data from Cloud Storage to BigQuery by using Dataflow. The network team provided you with the Shared VPC network and subnetwork to be used by your pipelines. You need to enable the deployment of the pipeline on the Shared VPC network. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the compute.networkUser role to the Dataflow service agent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the compute.networkUser role to the service account that executes the Dataflow pipeline.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the dataflow.admin role to the Dataflow service agent.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAssign the dataflow.admin role to the service account that executes the Dataflow pipeline."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T09:55:00.000Z",
        "voteCount": 8,
        "content": "- Dataflow service agent is the one responsible for setting up and managing the network resources that Dataflow requires. \n- By granting the compute.networkUser role to this service agent, we are enabling it to provision the necessary network resources within the Shared VPC for your Dataflow job."
      },
      {
        "date": "2024-09-25T18:39:00.000Z",
        "voteCount": 1,
        "content": "If you see in the comments, A was answer by people around 8 months ago but recent ones have answered B with the documentation. The GCP documentation evolves with time"
      },
      {
        "date": "2024-09-25T18:37:00.000Z",
        "voteCount": 1,
        "content": "service account that executes the Dataflow pipeline\nIt's straight forward"
      },
      {
        "date": "2024-09-24T16:52:00.000Z",
        "voteCount": 1,
        "content": "Assign the compute.networkUser role to the service account that executes the Dataflow pipeline"
      },
      {
        "date": "2024-07-23T22:57:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is B. Assign the compute.networkUser role to the service account that executes the Dataflow pipeline.\n\nHere's why:\n\n    Shared VPC and Network Access: When using a Shared VPC, you need to grant specific permissions to service accounts in the service project (where your Dataflow pipeline runs) to access resources in the host project's network.\n    compute.networkUser Role: This role grants the necessary permissions for a service account to use the network resources in the Shared VPC. This includes accessing subnets, creating instances, and communicating with other services within the network.\n    Service Account for Pipeline Execution: The service account that executes your Dataflow pipeline is the one that needs these network permissions. This is because the Dataflow service uses this account to create and manage worker instances within the Shared VPC network."
      },
      {
        "date": "2024-06-10T14:53:00.000Z",
        "voteCount": 3,
        "content": "Dataflow service agent is a role that is assigned to a service account. So is compute.networkUser.\nhttps://cloud.google.com/dataflow/docs/concepts/access-control#example"
      },
      {
        "date": "2024-05-19T16:14:00.000Z",
        "voteCount": 3,
        "content": "Option B https://cloud.google.com/knowledge/kb/dataflow-job-in-shared-vpc-xpn-permissions-000004261"
      },
      {
        "date": "2024-05-01T05:02:00.000Z",
        "voteCount": 2,
        "content": "I believe the answer is B. All authentication documentation points to Service Accounts. https://cloud.google.com/dataflow/docs/concepts/authentication#on-gcp\n\nDataflow service agent typically manages general interactions with the Dataflow service but does not execute the actual jobs."
      },
      {
        "date": "2024-02-09T00:52:00.000Z",
        "voteCount": 3,
        "content": "All projects that have used the resource Dataflow Job have a Dataflow Service Account, also known as the Dataflow service agent.\n\nMake sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet."
      },
      {
        "date": "2024-01-13T10:37:00.000Z",
        "voteCount": 3,
        "content": "Option A, I do agree with Raaad, it's the dataflow service agent that needs the networkUser role, because it's the one that provisions the network resources https://cloud.google.com/dataflow/docs/guides/specifying-networks#shared"
      },
      {
        "date": "2024-02-08T03:17:00.000Z",
        "voteCount": 1,
        "content": "But your link it's explain that \"Network User role must be assigned to the Dataflow service account\" \n\nMake sure the Shared VPC subnetwork is shared with the Dataflow service account and has the Compute Network User role assigned on the specified subnet. The Compute Network User role must be assigned to the Dataflow service account in the host project."
      },
      {
        "date": "2024-02-18T11:47:00.000Z",
        "voteCount": 2,
        "content": "All projects that have used the resource Dataflow Job have a Dataflow Service Account, also known as the Dataflow service agent.\nSource: https://cloud.google.com/dataflow/docs/concepts/security-and-permissions#df-service-account"
      },
      {
        "date": "2024-01-12T01:54:00.000Z",
        "voteCount": 3,
        "content": "compute.networkUser  to the service account that executes the Dataflow pipeline."
      },
      {
        "date": "2024-01-11T08:38:00.000Z",
        "voteCount": 1,
        "content": "Option B is Correct. \n\nExplanation:\nYou need to give compute networkuser role to service account that is processing the pipeline as it will need to deploy nessesary worker nodes on the shared vpc project. \n\nOption A is incorrect as Dataflow Service Agent is Google MGS service account that will not responsible for running or deoplying workers in shared vpc.\n\nOption C and D is incorrect as dataflow.admin is elevated privlages to create and manage all of dataflow components not deploying resources in shared vpc."
      },
      {
        "date": "2024-01-07T03:32:00.000Z",
        "voteCount": 2,
        "content": "B. Assign the compute.networkUser role to the service account that executes the Dataflow pipeline.  See the ref - https://cloud.google.com/dataflow/docs/guides/specifying-networks"
      },
      {
        "date": "2024-01-11T09:57:00.000Z",
        "voteCount": 1,
        "content": "Option A makes more sense:\n- Assigning the compute.networkUser role to the pipeline's service account grants it unnecessary and possibly excessive permissions outside its core responsibility of data processing. \n\nThe question focused specifically on the deployment aspect (i.e., provisioning of network resources like VMs) rather than what the pipeline accesses or processes once it's running."
      },
      {
        "date": "2024-01-17T12:11:00.000Z",
        "voteCount": 1,
        "content": "Yes , I agree, it should be A. Dataflow service account should be the one having this permission instaed of worker"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 296,
    "url": "https://www.examtopics.com/discussions/google/view/129909-exam-professional-data-engineer-topic-1-question-296/",
    "body": "Your infrastructure team has set up an interconnect link between Google Cloud and the on-premises network. You are designing a high-throughput streaming pipeline to ingest data in streaming from an Apache Kafka cluster hosted on- premises. You want to store the data in BigQuery, with as minimal latency as possible. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a Kafka Connect bridge between Kafka and Pub/Sub. Use a Google-provided Dataflow template to read the data from Pub/Sub, and write the data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a proxy host in the VPC in Google Cloud connecting to Kafka. Write a Dataflow pipeline, read data from the proxy host, and write the data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a Kafka Connect bridge between Kafka and Pub/Sub. Write a Dataflow pipeline, read the data from Pub/Sub, and write the data to BigQuery."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 14,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "D",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-09T23:39:00.000Z",
        "voteCount": 1,
        "content": "Going with C"
      },
      {
        "date": "2024-07-05T20:54:00.000Z",
        "voteCount": 2,
        "content": "Latency: Option C, with direct integration between Kafka and Dataflow, offers lower latency by eliminating intermediate steps.\nFlexibility: Custom Dataflow pipelines (Option C) provide more control over data processing and optimization compared to using a pre-built template."
      },
      {
        "date": "2024-04-11T14:19:00.000Z",
        "voteCount": 1,
        "content": "per the text below at https://cloud.google.com/dataflow/docs/kafka-dataflow -\n\n\"Alternatively, you might have an existing Kafka cluster that resides outside of Google Cloud. For example, you might have an existing workload that is deployed on-premises or in another public cloud.\""
      },
      {
        "date": "2024-03-01T04:47:00.000Z",
        "voteCount": 2,
        "content": "From my point of view, the best option is C taking into account this doc: https://cloud.google.com/dataflow/docs/kafka-dataflow"
      },
      {
        "date": "2024-02-26T12:45:00.000Z",
        "voteCount": 2,
        "content": "Based on the key requirements highlighted:\n\u2022\tInterconnect link between GCP and on-prem Kafka\n\u2022\tHigh throughput streaming pipeline\n\u2022\tMinimal latency\n\u2022\tData to be stored in BigQuery\nD - The key reasons this meets the requirements:\n\u2022\tKafka connect provides a reliable bridge to Pub/Sub over the interconnect\n\u2022\tReading from Pub/Sub minimizes latency vs reading directly from Kafka\n\u2022\tDataflow provides a high throughput streaming engine\n\u2022\tWriting to BigQuery gives scalable data storage\nBy leveraging these fully managed GCP services over the dedicated interconnect, a low latency streaming pipeline from on-prem Kafka into BigQuery can be implemented rapidly.\nOptions A/B/C have higher latencies or custom code requirements, so do not meet the minimal latency criteria as well as option D."
      },
      {
        "date": "2024-02-26T12:47:00.000Z",
        "voteCount": 1,
        "content": "Why choose option D over A?\nThe key advantage with option D is that by writing a custom Dataflow pipeline rather than using a Google provided template, there is more flexibility to customize performance tuning and optimization for lowest latency.\n\u2022\tSome potential optimizations:\n\u2022\tFine tuning number of workers, machine types to meet specific throughput targets\n\u2022\tCustom data parsing/processing logic if applicable\n\u2022\tExperimenting with autoscaling parameters or triggers"
      },
      {
        "date": "2024-02-26T12:47:00.000Z",
        "voteCount": 1,
        "content": "The Google template may be easier to set up initially, but a custom pipeline provides more control over optimizations specifically for low latency requirements stated in the question.\nThat being said, option A would still work reasonably well - but option D allows squeezing out that extra bit of performance if low millisecond latency is absolutely critical in the pipeline through precise tuning.\nSo in summary, option A is easier to implement but option D provides more optimization flexibility for ultra low latency streaming requirements."
      },
      {
        "date": "2024-02-26T12:47:00.000Z",
        "voteCount": 2,
        "content": "Why not C:\nAt first option C (using a Dataflow pipeline to directly read from Kafka and write to BigQuery) seems reasonable.\nHowever, the key requirement stated in the question is to have minimal latency for the streaming pipeline.\nBy reading directly from Kafka within Dataflow, there can be additional latency and processing overhead compared to reading from Pub/Sub, for a few reasons:\n1.\tPub/Sub acts as a buffer and handles scaling/reliability of streaming data automatically. This reduces processing burden on the pipeline.\n2.\tNetwork latency can be lower by leveraging Pub/Sub instead of making constant pull requests for data from Kafka within the streaming pipeline.\n3.\tAny failures have to be handled within the pipeline code itself when reading directly from Kafka. With Pub/Sub, reliability is built-in."
      },
      {
        "date": "2024-03-26T10:38:00.000Z",
        "voteCount": 3,
        "content": "You are adding an intermediate hop in between on prem kafka and Dataflow ( pubsub ). Why won't this add additional latency."
      },
      {
        "date": "2024-02-26T12:47:00.000Z",
        "voteCount": 2,
        "content": "So in summary, while option C is technically possible, option D introduces Pub/Sub as a streaming buffer which reduces overall latency for the pipeline, allowing the key requirement of minimal latency to be better satisfied."
      },
      {
        "date": "2024-02-21T11:08:00.000Z",
        "voteCount": 2,
        "content": "A Vs C  -- Not sure which one would have low latency.\n\nPoints related to option C:\n\"Yes, Dataflow can read events from Kafka. Dataflow is a fully-managed, serverless streaming analytics service that supports both batch and stream processing. It can read events from Kafka, process them, and write the results to a BigQuery table for further analysis. \"\n\n\"Dataflow supports Kafka support, which was added to Apache Beam in 2016. Google provides a Dataflow template that configures a Kafka-to-BigQuery pipeline. The template uses the BigQueryIO connector provided in the Apache Beam SDK. \""
      },
      {
        "date": "2024-02-21T19:59:00.000Z",
        "voteCount": 2,
        "content": "Going with C"
      },
      {
        "date": "2024-02-22T08:42:00.000Z",
        "voteCount": 1,
        "content": "Final???"
      },
      {
        "date": "2024-02-02T09:12:00.000Z",
        "voteCount": 3,
        "content": "Option C makes more sense to me because of the \"minimal latency as possible\".\nI would have chosen option A if it were \"less CODING as possible\"."
      },
      {
        "date": "2024-01-13T10:39:00.000Z",
        "voteCount": 3,
        "content": "Option A, leverage dataflow template for Kafka https://cloud.google.com/dataflow/docs/kafka-dataflow"
      },
      {
        "date": "2024-01-21T01:02:00.000Z",
        "voteCount": 1,
        "content": "Agree. \"Google provides a Dataflow template that configures a Kafka-to-BigQuery pipeline. The template uses the BigQueryIO connector provided in the Apache Beam SDK.\""
      },
      {
        "date": "2024-02-18T12:02:00.000Z",
        "voteCount": 1,
        "content": "But it includes setting up a Kafka Connect bridge while an interconnect link has already been set up. https://cloud.google.com/dataflow/docs/kafka-dataflow#connect_to_an_external_cluster"
      },
      {
        "date": "2024-01-04T04:10:00.000Z",
        "voteCount": 4,
        "content": "C. Use Dataflow, write a pipeline that reads the data from Kafka, and writes the data to BigQuery."
      },
      {
        "date": "2023-12-30T13:07:00.000Z",
        "voteCount": 3,
        "content": "Dataflow has templates to read from Kafka. Other options are too complicated\nhttps://cloud.google.com/dataflow/docs/kafka-dataflow"
      },
      {
        "date": "2024-01-10T09:16:00.000Z",
        "voteCount": 2,
        "content": "so, this is the answer A, whe C?"
      },
      {
        "date": "2024-01-13T10:39:00.000Z",
        "voteCount": 1,
        "content": "Yeah, the answer is A. C requires you to develop the pipeline yourself and ensure minimal latency, which means that you perform better than a pre-built template from Google...not really the case most of the times :)"
      },
      {
        "date": "2024-01-29T07:55:00.000Z",
        "voteCount": 7,
        "content": "but Option A introduces additional replication into Pub/Sub and the question states with minimal latency. In my opinion subscribing to Kafka via Dataflow has a lower latency than replicating the messages first to Pub/Sub and subscribing with Dataflow to it."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 297,
    "url": "https://www.examtopics.com/discussions/google/view/130313-exam-professional-data-engineer-topic-1-question-297/",
    "body": "You migrated your on-premises Apache Hadoop Distributed File System (HDFS) data lake to Cloud Storage. The data scientist team needs to process the data by using Apache Spark and SQL. Security policies need to be enforced at the column level. You need a cost-effective solution that can scale into a data mesh. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Deploy a long-living Dataproc cluster with Apache Hive and Ranger enabled.<br>2. Configure Ranger for column level security.<br>3. Process with Dataproc Spark or Hive SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Define a BigLake table.<br>2. Create a taxonomy of policy tags in Data Catalog.<br>3. Add policy tags to columns.<br>4. Process with the Spark-BigQuery connector or BigQuery SQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Load the data to BigQuery tables.<br>2. Create a taxonomy of policy tags in Data Catalog.<br>3. Add policy tags to columns.<br>4. Process with the Spark-BigQuery connector or BigQuery SQL.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Apply an Identity and Access Management (IAM) policy at the file level in Cloud Storage.<br>2. Define a BigQuery external table for SQL processing.<br>3. Use Dataproc Spark to process the Cloud Storage files."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 19,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T10:03:00.000Z",
        "voteCount": 13,
        "content": "- BigLake Integration: BigLake allows you to define tables on top of data in Cloud Storage, providing a bridge between data lake storage and BigQuery's powerful analytics capabilities. This approach is cost-effective and scalable.\n- Data Catalog for Governance: Creating a taxonomy of policy tags in Google Cloud's Data Catalog and applying these tags to specific columns in your BigLake tables enables fine-grained, column-level access control. \n- Processing with Spark and SQL: The Spark-BigQuery connector allows data scientists to process data using Apache Spark directly against BigQuery (and BigLake tables). This supports both Spark and SQL processing needs.\n- Scalability into a Data Mesh: BigLake and Data Catalog are designed to scale and support the data mesh architecture, which involves decentralized data ownership and governance."
      },
      {
        "date": "2024-02-21T11:19:00.000Z",
        "voteCount": 1,
        "content": "Going with 'B' based on the comments"
      },
      {
        "date": "2024-01-13T10:41:00.000Z",
        "voteCount": 1,
        "content": "Option B, agree with comments explanation"
      },
      {
        "date": "2024-01-07T17:57:00.000Z",
        "voteCount": 4,
        "content": "BigLake leverages existing Cloud Storage infrastructure, eliminating the need for a dedicated Dataproc cluster, reducing costs significantly."
      },
      {
        "date": "2024-01-04T04:15:00.000Z",
        "voteCount": 1,
        "content": "C. \n1. Load the data to BigQuery tables.\n2. Create a taxonomy of policy tags in Data Catalog.\n3. Add policy tags to columns.\n4. Process with the Spark-BigQuery connector or BigQuery SQL."
      },
      {
        "date": "2024-01-11T10:03:00.000Z",
        "voteCount": 7,
        "content": "- Option B offers a serverless approach that integrates Cloud Storage (as a data lake), BigLake (for table definition), Data Catalog (for data mesh), and BigQuery (for analytics), all of which are essential components of a flexible, scalable, and secure data platform."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 298,
    "url": "https://www.examtopics.com/discussions/google/view/129911-exam-professional-data-engineer-topic-1-question-298/",
    "body": "One of your encryption keys stored in Cloud Key Management Service (Cloud KMS) was exposed. You need to re- encrypt all of your CMEK-protected Cloud Storage data that used that key, and then delete the compromised key. You also want to reduce the risk of objects getting written without customer-managed encryption key (CMEK) protection in the future. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRotate the Cloud KMS key version. Continue to use the same Cloud Storage bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud KMS key. Set the default CMEK key on the existing Cloud Storage bucket to the new one.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud KMS key. Create a new Cloud Storage bucket. Copy all objects from the old bucket to the new one bucket while specifying the new Cloud KMS key in the copy command.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Cloud KMS key. Create a new Cloud Storage bucket configured to use the new key as the default CMEK key. Copy all objects from the old bucket to the new bucket without specifying a key.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 21,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T14:04:00.000Z",
        "voteCount": 9,
        "content": "- New Key Creation: A new Cloud KMS key ensures a secure replacement for the compromised one.\n- New Bucket: A separate bucket prevents potential conflicts with existing objects and configurations.\n- Default CMEK: Setting the new key as default enforces encryption for all objects in the bucket, reducing the risk of unencrypted data.\n- Copy Without Key Specification: Copying objects without specifying a key leverages the default key, simplifying the process and ensuring consistent encryption.\n- Old Key Deletion: After copying, the compromised key can be safely deleted."
      },
      {
        "date": "2023-12-30T13:18:00.000Z",
        "voteCount": 8,
        "content": "Wrong:\nA - rotating external key doesn't trigger re-encryption of data already in GCS: https://cloud.google.com/kms/docs/rotate-key#rotate-external-coordinated\nC - Setting key during copy doesn't take care of objects that are later uploaded to the bucket, that will still use the default key"
      },
      {
        "date": "2024-02-21T20:11:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-02-18T14:11:00.000Z",
        "voteCount": 3,
        "content": "The correct answer is D. Rotating the key does not seem to re-encrypt:\n\nIn the event that a key is compromised, regular rotation (!!) limits the number of actual messages vulnerable to compromise (!!).\nIf you suspect that a key version is compromised, disable it and revoke access to it as soon as possible.\nSource: https://cloud.google.com/kms/docs/key-rotation#why_rotate_keys"
      },
      {
        "date": "2024-02-18T14:13:00.000Z",
        "voteCount": 3,
        "content": "Note: When you rotate a key, data encrypted with previous key versions is not automatically re-encrypted with the new key version. You can learn more about re-encrypting data.\nSource: https://cloud.google.com/kms/docs/key-rotation#how_often_to_rotate_keys"
      },
      {
        "date": "2024-01-24T13:02:00.000Z",
        "voteCount": 2,
        "content": "I don't understand why only Matt select A\n\nhttps://cloud.google.com/sdk/gcloud/reference/kms/keys/update\n\nThis seems to do the job, am I wrong ?"
      },
      {
        "date": "2024-01-13T10:43:00.000Z",
        "voteCount": 1,
        "content": "Definitely A"
      },
      {
        "date": "2024-02-18T14:10:00.000Z",
        "voteCount": 1,
        "content": "Rotating does not mean you re-encrypt data."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 299,
    "url": "https://www.examtopics.com/discussions/google/view/130328-exam-professional-data-engineer-topic-1-question-299/",
    "body": "You have an upstream process that writes data to Cloud Storage. This data is then read by an Apache Spark job that runs on Dataproc. These jobs are run in the us-central1 region, but the data could be stored anywhere in the United States. You need to have a recovery process in place in case of a catastrophic single region failure. You need an approach with a maximum of 15 minutes of data loss (RPO=15 mins). You want to ensure that there is minimal latency when reading the data. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create two regional Cloud Storage buckets, one in the us-central1 region and one in the us-south1 region.<br>2. Have the upstream process write data to the us-central1 bucket. Use the Storage Transfer Service to copy data hourly from the us-central1 bucket to the us-south1 bucket.<br>3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in that region.<br>4. In case of regional failure, redeploy your Dataproc clusters to the us-south1 region and read from the bucket in that region instead.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a Cloud Storage bucket in the US multi-region.<br>2. Run the Dataproc cluster in a zone in the us-central1 region, reading data from the US multi-region bucket.<br>3. In case of a regional failure, redeploy the Dataproc cluster to the us-central2 region and continue reading from the same bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.<br>2. Enable turbo replication.<br>3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the us-south1 region.<br>4. In case of a regional failure, redeploy your Dataproc cluster to the us-south1 region and continue reading from the same bucket.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.<br>2. Enable turbo replication.<br>3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.<br>4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 11,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T13:57:00.000Z",
        "voteCount": 5,
        "content": "- Rapid Replication: Turbo replication ensures near-real-time data synchronization between regions, achieving an RPO of 15 minutes or less.\n- Minimal Latency: Dataproc clusters can read from the bucket in the same region, minimizing data transfer latency and optimizing performance.\n- Disaster Recovery: In case of regional failure, Dataproc clusters can seamlessly redeploy to the other region and continue reading from the same bucket, ensuring business continuity."
      },
      {
        "date": "2024-02-21T20:20:00.000Z",
        "voteCount": 1,
        "content": "Option D"
      },
      {
        "date": "2024-01-13T10:45:00.000Z",
        "voteCount": 2,
        "content": "Option D, answers all needs from the request"
      },
      {
        "date": "2024-01-04T06:34:00.000Z",
        "voteCount": 3,
        "content": "D. \n1. Create a dual-region Cloud Storage bucket in the us-central1 and us-south1 regions.\n2. Enable turbo replication.\n3. Run the Dataproc cluster in a zone in the us-central1 region, reading from the bucket in the same region.\n4. In case of a regional failure, redeploy the Dataproc clusters to the us-south1 region and read from the same bucket."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 300,
    "url": "https://www.examtopics.com/discussions/google/view/130318-exam-professional-data-engineer-topic-1-question-300/",
    "body": "You currently have transactional data stored on-premises in a PostgreSQL database. To modernize your data environment, you want to run transactional workloads and support analytics needs with a single database. You need to move to Google Cloud without changing database management systems, and minimize cost and complexity. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate and modernize your database with Cloud Spanner.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your workloads to AlloyDB for PostgreSQL.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate to BigQuery to optimize analytics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMigrate your PostgreSQL database to Cloud SQL for PostgreSQL."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 15,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 14,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-10-08T06:22:00.000Z",
        "voteCount": 1,
        "content": "In real life clearly how performant it needed to be would be a massive factor. AlloyDB is more expensive (see https://cloud.google.com/alloydb/pricing, vs https://cloud.google.com/sql/pricing), but when they say \"minimise cost\" is that per query, or is it per year assuming similar instance size. There's no way for us to know, we have to guess. I'm guessing AlloyDB, as the question seem to be telegraphing that, but it could just as easily be CloudSQL postgres based on the cheaper costs. We simply cannot know."
      },
      {
        "date": "2024-07-12T09:51:00.000Z",
        "voteCount": 1,
        "content": "Because  AlloyDB is optimised for hybrid transactional and analytical processing (HTAP), meaning you can run both transactional workloads and analytics on the same database with excellent performance."
      },
      {
        "date": "2024-07-11T21:09:00.000Z",
        "voteCount": 1,
        "content": "AlloyDB"
      },
      {
        "date": "2024-06-27T09:59:00.000Z",
        "voteCount": 2,
        "content": "Minimize cost. https://cloud.google.com/alloydb?hl=en\n\nAlloyDB offers superior performance, 4x faster than standard PostgreSQL for transactional workloads. That does not come without cost."
      },
      {
        "date": "2024-06-25T10:12:00.000Z",
        "voteCount": 1,
        "content": "It's a little complicated, considering it says minimize costs (Cloud SQL) and run transactional workloads and support analytics needs (AlloyDB). I consider B. because you can minimize costs in the long-term instead of doing it immediately with possible extra costs in the long-term. Think about it"
      },
      {
        "date": "2024-06-10T15:23:00.000Z",
        "voteCount": 1,
        "content": "AlloyDB is for large scale and more expensive. We want to minimize cost and complexity, so the answer is D."
      },
      {
        "date": "2024-05-29T00:25:00.000Z",
        "voteCount": 1,
        "content": "B. Migrate your workloads to AlloyDB for PostgreSQL."
      },
      {
        "date": "2024-05-29T00:28:00.000Z",
        "voteCount": 1,
        "content": "Sorry its D. Migrate your PostgreSQL database to Cloud SQL for PostgreSQL."
      },
      {
        "date": "2024-03-27T22:34:00.000Z",
        "voteCount": 2,
        "content": "They currently have transactional data stored on-premises in a PostgreSQL database and they want to modernize their database that supports transactional workloads and analytics .If they select cloud Sql (postgreSQL) it will minimize the cost and complexity.\nand for analytics purpose they can create federated queries over cloudSql(postgreSql)\nhttps://cloud.google.com/bigquery/docs/federated-queries-intro\nThis approach will minimze the cost"
      },
      {
        "date": "2024-03-23T10:29:00.000Z",
        "voteCount": 1,
        "content": "B - minimize cost\n\nCloud SQL for PostgreSQL: Generally less expensive than AlloyDB, especially for smaller deployments.\nAlloyDB: Can be significantly more expensive due to its advanced features and high performance capabilities."
      },
      {
        "date": "2024-04-24T02:49:00.000Z",
        "voteCount": 2,
        "content": "so answer D?"
      },
      {
        "date": "2024-03-03T05:41:00.000Z",
        "voteCount": 3,
        "content": "minimize cost and complexity"
      },
      {
        "date": "2024-02-21T21:48:00.000Z",
        "voteCount": 2,
        "content": "Considering the cost factor, I'll go with D. \nIf \"minimize cost\" is not present in the question, then I'd go with 'B' AlloyDB. \n\nCloud SQL for PostgreSQL: Generally less expensive than AlloyDB.\nAlloyDB: Can be significantly more expensive due to its advanced features and high performance capabilities."
      },
      {
        "date": "2024-02-09T08:25:00.000Z",
        "voteCount": 4,
        "content": "AlloyDB for PostgreSQL is a fully managed, PostgreSQL-compatible database service that's designed for your most demanding workloads, including hybrid transactional and analytical processing.\nref: https://cloud.google.com/alloydb/docs/overview"
      },
      {
        "date": "2024-01-29T06:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/alloydb#all-features\nThe requirement is to minimize cost and complexity. Cloud SQL would be the best choice."
      },
      {
        "date": "2024-01-18T22:57:00.000Z",
        "voteCount": 1,
        "content": "Database Migration Service makes it easier for you to migrate your data to Google Cloud. This service helps you lift and shift your PostgreSQL workloads into Cloud SQL."
      },
      {
        "date": "2024-01-06T13:51:00.000Z",
        "voteCount": 4,
        "content": "- AlloyDB is a fully managed, PostgreSQL-compatible database service with industry-leading performance."
      },
      {
        "date": "2024-01-21T01:20:00.000Z",
        "voteCount": 1,
        "content": "Why not D"
      },
      {
        "date": "2024-01-04T04:51:00.000Z",
        "voteCount": 1,
        "content": "B. Migrate your workloads to AlloyDB for PostgreSQL."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 301,
    "url": "https://www.examtopics.com/discussions/google/view/129913-exam-professional-data-engineer-topic-1-question-301/",
    "body": "You are architecting a data transformation solution for BigQuery. Your developers are proficient with SQL and want to use the ELT development technique. In addition, your developers need an intuitive coding environment and the ability to manage SQL as code. You need to identify a solution for your developers to build these pipelines. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataform to build, manage, and schedule SQL pipelines.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow jobs to read data from Pub/Sub, transform the data, and load the data to BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Data Fusion to build and execute ETL pipelines.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Cloud Composer to load data and run SQL pipelines by using the BigQuery job operators."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T13:36:00.000Z",
        "voteCount": 6,
        "content": "- Aligns with ELT Approach: Dataform is designed for ELT (Extract, Load, Transform) pipelines, directly executing SQL transformations within BigQuery, matching the developers' preference.\n-SQL as Code: It enables developers to write and manage SQL transformations as code, promoting version control, collaboration, and testing.\n- Intuitive Coding Environment: Dataform provides a user-friendly interface and familiar SQL syntax, making it easy for SQL-proficient developers to adopt.\n- Scheduling and Orchestration: It includes built-in scheduling capabilities to automate pipeline execution, simplifying pipeline management."
      },
      {
        "date": "2024-02-22T00:21:00.000Z",
        "voteCount": 1,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T10:08:00.000Z",
        "voteCount": 2,
        "content": "Definitely A"
      },
      {
        "date": "2024-01-04T05:57:00.000Z",
        "voteCount": 3,
        "content": "A. Use Dataform to build, manage, and schedule SQL pipelines."
      },
      {
        "date": "2023-12-30T13:51:00.000Z",
        "voteCount": 1,
        "content": "Dataform = transformations in SQL"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 302,
    "url": "https://www.examtopics.com/discussions/google/view/130327-exam-professional-data-engineer-topic-1-question-302/",
    "body": "You work for a farming company. You have one BigQuery table named sensors, which is about 500 MB and contains the list of your 5000 sensors, with columns for id, name, and location. This table is updated every hour. Each sensor generates one metric every 30 seconds along with a timestamp, which you want to store in BigQuery. You want to run an analytical query on the data once a week for monitoring purposes. You also want to minimize costs. What data model should you use?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a metrics column in the sensors table.<br>2. Set RECORD type and REPEATED mode for the metrics column.<br>3. Use an UPDATE statement every 30 seconds to add new metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a metrics column in the sensors table.<br>2. Set RECORD type and REPEATED mode for the metrics column.<br>3. Use an INSERT statement every 30 seconds to add new metrics.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a metrics table partitioned by timestamp.<br>2. Create a sensorId column in the metrics table, that points to the id column in the sensors table.<br>3. Use an INSERT statement every 30 seconds to append new metrics to the metrics table.<br>4. Join the two tables, if needed, when running the analytical query.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t1. Create a metrics table partitioned by timestamp.<br>2. Create a sensorId column in the metrics table, which points to the id column in the sensors table.<br>3. Use an UPDATE statement every 30 seconds to append new metrics to the metrics table.<br>4. Join the two tables, if needed, when running the analytical query."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 9,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 3,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 2,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T13:31:00.000Z",
        "voteCount": 9,
        "content": "Partitioned Metrics Table: Creating a separate metrics table partitioned by timestamp is a standard practice for time-series data like sensor readings. Partitioning by timestamp allows for more efficient querying, especially when you're only interested in a specific time range (like weekly monitoring).\nReference to Sensors Table: Including a sensorId column that references the id column in the sensors table allows you to maintain a relationship between the metrics and the sensors without duplicating sensor information.\nINSERT Every 30 Seconds: Using an INSERT statement every 30 seconds to the partitioned metrics table is a standard approach for time-series data ingestion in BigQuery. It allows for efficient data storage and querying.\nJoin for Analysis: When you need to analyze the data, you can join the metrics table with the sensors table based on the sensorId, allowing for comprehensive analysis with sensor details."
      },
      {
        "date": "2024-09-14T06:37:00.000Z",
        "voteCount": 1,
        "content": "Because \"Minimize costs\" was requested, i would go for C. \nStorage cost will be lower for partitions where no writes took place for a certain amount of time, see https://cloud.google.com/bigquery/pricing#storage\nPartitioning by timestamp can be configured to use daily, hourly, monthly, or yearly partitioning - so if you choose daily partitioning, the number of partitions should not be an issue.\nWorking with RECORDS (A,B) would be an option if performance was in focus."
      },
      {
        "date": "2024-08-03T12:26:00.000Z",
        "voteCount": 2,
        "content": "Option C will not violate partitioning limit of 4000 as the lowest grain of partitioning is hourly"
      },
      {
        "date": "2024-06-21T19:40:00.000Z",
        "voteCount": 2,
        "content": "Here's my logic (some people have already said same thing)\n\nCannot be C and D\n- Total 5000 sensors are sending new timestamp every 30 seconds. If you partition this table with timestamp, you are getting partitions above 4000 (single job) or 10000 (partition limit) so option C and D don't look correct\n- For C and D, also need to consider that BigQuery best practices advise to avoid JOINs and use STRUCT and RECORD types to solve the parent-child join issue.\n\nNow coming back to A and B, we will be adding sensor readings for every sensor. I don't think this is a transactional type database where you need to update data. You will add new data for more accurate analysis later so A is discarded. BigQuery best practices also advise to avoid UPDATE statements since its an Analytical columnar database\n\nB is the correct option."
      },
      {
        "date": "2024-05-30T12:48:00.000Z",
        "voteCount": 3,
        "content": "Since BigQuery tables are limited to 4000 partitions, options C &amp; D are discarded. Option B is wrong as insertion is invalid too. So option A."
      },
      {
        "date": "2024-04-11T18:33:00.000Z",
        "voteCount": 4,
        "content": "I'm in favor of Option B\nReason: BQ has nested columns feature specifically to address these scenarios where a join would be needed in a traditional/ relational data model. Nesting field will reduce the need to join tables, performance will be high and design will be simple"
      },
      {
        "date": "2024-02-15T02:58:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-13T10:07:00.000Z",
        "voteCount": 2,
        "content": "Option C"
      },
      {
        "date": "2024-03-21T08:30:00.000Z",
        "voteCount": 3,
        "content": "Why C. Partitioning by timestamp could breach the 4000 cap of number of partitions easily. And with soo much less data, why partitioning is required in the first place. Ans should be B"
      },
      {
        "date": "2024-01-06T13:31:00.000Z",
        "voteCount": 4,
        "content": "Option C"
      },
      {
        "date": "2024-01-04T05:54:00.000Z",
        "voteCount": 1,
        "content": "C. \n1. Create a metrics table partitioned by timestamp.\n2. Create a sensorId column in the metrics table, that points to the id column in the sensors table.\n3. Use an INSERT statement every 30 seconds to append new metrics to the metrics table.\n4. Join the two tables, if needed, when running the analytical query."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 303,
    "url": "https://www.examtopics.com/discussions/google/view/130326-exam-professional-data-engineer-topic-1-question-303/",
    "body": "You are managing a Dataplex environment with raw and curated zones. A data engineering team is uploading JSON and CSV files to a bucket asset in the curated zone but the files are not being automatically discovered by Dataplex. What should you do to ensure that the files are discovered by Dataplex?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tMove the JSON and CSV files to the raw zone.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tEnable auto-discovery of files for the curated zone.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the bg command-line tool to load the JSON and CSV files into BigQuery tables.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tGrant object level access to the CSV and JSON files in Cloud Storage."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 36,
        "isMostVoted": true
      },
      {
        "answer": "B",
        "count": 18,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-17T11:40:00.000Z",
        "voteCount": 22,
        "content": "Should be A.  Curated zone need Parquet, Avro, ORC format not CSV or JSON. Check the ref - https://cloud.google.com/dataplex/docs/add-zone#curated-zones"
      },
      {
        "date": "2024-01-06T05:57:00.000Z",
        "voteCount": 6,
        "content": "- Auto-Discovery Feature: Dataplex has an auto-discovery feature that, when enabled, automatically discovers and catalogs data assets within a zone. \n- Appropriate for Both Raw and Curated Zones: This feature is applicable to both raw and curated zones, and it should be tailored to the specific data governance and cataloging needs of the organization."
      },
      {
        "date": "2024-08-21T07:17:00.000Z",
        "voteCount": 1,
        "content": "Discovery configuration\nDiscovery is enabled by default when you create a new zone or asset. You can disable Discovery at the zone or asset level.\n\nFor each Dataplex asset with Discovery enabled, Dataplex does the following:\n\nScans the data associated with the asset.\nGroups structured and semi-structured files into tables.\nCollects technical metadata, such as table name, schema, and partition definition.\nFor unstructured data, such as images and videos, Dataplex Discovery automatically detects and registers groups of files sharing media type as filesets. For example, if gs://images/group1 contains GIF images, and gs://images/group2 contains JPEG images, Dataplex Discovery detects and registers two filesets. For structured data, such as Avro, Discovery detects files only if they are located in folders that contain the same data format and schema.\n\nReference : https://cloud.google.com/dataplex/docs/discover-data#exclude-files-from-Discovery"
      },
      {
        "date": "2024-07-03T11:15:00.000Z",
        "voteCount": 2,
        "content": "While JSON and CSV can technically be stored in curated zones, it is not a common practice due to the reasons mentioned above. no where in the mention link its mention that there is a restriction."
      },
      {
        "date": "2024-06-16T20:18:00.000Z",
        "voteCount": 4,
        "content": "While none of the original options (A, B, C, or D) directly address the issue, the closest solution is:\n\nMove the JSON and CSV files to a raw zone. (This was previously marked as the most voted option, but it's not ideal due to data organization disruption)\nHere's why this approach might be necessary (but not ideal):\n\nDataplex curated zones currently don't support native processing of JSON and CSV formats. They are designed for structured data formats like Parquet, Avro, or ORC."
      },
      {
        "date": "2024-05-01T23:24:00.000Z",
        "voteCount": 1,
        "content": "Option A\nhttps://cloud.google.com/dataplex/docs/add-zone#raw-zones \n\nRaw zones are the only zones that support CSV &amp; JSON"
      },
      {
        "date": "2024-04-12T09:44:00.000Z",
        "voteCount": 1,
        "content": "Its B guys, i encounter this in my job, and I had to do B to make it work"
      },
      {
        "date": "2024-04-12T09:46:00.000Z",
        "voteCount": 1,
        "content": "Actually I did this in a Raw zone, not Curated."
      },
      {
        "date": "2024-04-12T09:57:00.000Z",
        "voteCount": 3,
        "content": "Its A :)"
      },
      {
        "date": "2024-03-05T10:33:00.000Z",
        "voteCount": 2,
        "content": "GCP001  agree with him"
      },
      {
        "date": "2024-02-29T18:16:00.000Z",
        "voteCount": 2,
        "content": "The answer can be found reading a common config of Dataplex in this URL: https://medium.com/google-cloud/google-cloud-dataplex-part-1-lakes-zones-assets-and-discovery-5f288486cb2f"
      },
      {
        "date": "2024-02-28T03:53:00.000Z",
        "voteCount": 1,
        "content": "Dataplex does not allow users to create CSV files within a \u201ccurated zone\u201d"
      },
      {
        "date": "2024-02-21T18:20:00.000Z",
        "voteCount": 2,
        "content": "According to this URL: https://cloud.google.com/dataplex/docs/discover-data, the auto-discovery can support CSV and Json in both Raw-Zone and Curated-Zone. I also open a console the verify it, both Raw and Curated zone can set up csv&amp;json auto-discovery."
      },
      {
        "date": "2024-02-11T18:33:00.000Z",
        "voteCount": 3,
        "content": "Discovery raises the following administrator actions whenever data-related issues are detected during scans : Inconsistent data format in a table. For example, files of different formats exist with the same table prefix.  Inconsistent data format in a table. For example, files of different formats exist with the same table prefix."
      },
      {
        "date": "2024-02-11T18:35:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/dataplex/docs/discover-data#invalid_data_format"
      },
      {
        "date": "2024-01-13T10:02:00.000Z",
        "voteCount": 4,
        "content": "I'd go for Option B, auto-discovery is enabled by default for any zone, including curated ones, so if a file is not automatically discovered it's due to the disabled auto-discovery"
      },
      {
        "date": "2024-02-18T15:25:00.000Z",
        "voteCount": 1,
        "content": "In this case, it would be because of invalid data format in curated zones (data not in Avro, Parquet, or ORC formats)."
      },
      {
        "date": "2024-01-11T01:11:00.000Z",
        "voteCount": 3,
        "content": "I will go with A, check the ref. Curated zones only store Parquet, Avro, and ORC in CS, and well-defined schema and Hive-style partitions in the BigQuery:\nhttps://cloud.google.com/dataplex/docs/add-zone#curated-zones"
      },
      {
        "date": "2024-01-04T05:45:00.000Z",
        "voteCount": 1,
        "content": "A. Move the JSON and CSV files to the raw zone."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 304,
    "url": "https://www.examtopics.com/discussions/google/view/130325-exam-professional-data-engineer-topic-1-question-304/",
    "body": "You have a table that contains millions of rows of sales data, partitioned by date. Various applications and users query this data many times a minute. The query requires aggregating values by using AVG, MAX, and SUM, and does not require joining to other tables. The required aggregations are only computed over the past year of data, though you need to retain full historical data in the base tables. You want to ensure that the query results always include the latest data from the tables, while also reducing computation cost, maintenance overhead, and duration. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view to aggregate the base table data. Include a filter clause to specify the last one year of partitions.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view to aggregate the base table data. Configure a partition expiration on the base table to retain only the last one year of partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a view to aggregate the base table data. Include a filter clause to specify the last year of partitions.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new table that aggregates the base table data. Include a filter clause to specify the last year of partitions. Set up a scheduled query to recreate the new table every hour."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 13,
        "isMostVoted": true
      },
      {
        "answer": "C",
        "count": 2,
        "isMostVoted": false
      },
      {
        "answer": "B",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T10:13:00.000Z",
        "voteCount": 7,
        "content": "- Materialized View: Materialized views in BigQuery are precomputed views that periodically cache the result of a query for increased performance and efficiency. They are especially beneficial for heavy and repetitive aggregation queries.\n- Filter for Recent Data: Including a clause to focus on the last year of partitions ensures that the materialized view is only storing and updating the relevant data, optimizing storage and refresh time.\n- Always Up-to-date: Materialized views are maintained by BigQuery and automatically updated at regular intervals, ensuring they include the latest data up to a certain freshness point."
      },
      {
        "date": "2024-02-22T03:39:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-02-19T15:58:00.000Z",
        "voteCount": 2,
        "content": "materialized view requires refreshing so it might not fulfill the requirement: \"results always include the latest data from the tables\". Opt. C will give you the newest data every time you execute the query but it will have to be computed every time"
      },
      {
        "date": "2024-03-25T12:29:00.000Z",
        "voteCount": 1,
        "content": "Agree, these questions always play with words, making many of options seem plausible"
      },
      {
        "date": "2024-03-25T12:33:00.000Z",
        "voteCount": 2,
        "content": "But materialized view always returns fresh data \nFresh data. Materialized views return fresh data. If changes to base tables might invalidate the materialized view, then data is read directly from the base tables. If the changes to the base tables don't invalidate the materialized view, then rest of the data is read from the materialized view and only the changes are read from the base tables.\nhttps://cloud.google.com/bigquery/docs/materialized-views-intro"
      },
      {
        "date": "2024-02-03T11:31:00.000Z",
        "voteCount": 1,
        "content": "A We can do aggregations, bit if not specified table will not be partitioned on the view. \nB partition expiration is not possible, as expiration is the same as base table\nC It might be the right one, although not specific savings vs the original query, but here we would guarantee accessing only last year data. \nD not a good one in any sense \n\nA and C might be equally good solutions depending on some understandings. Would probably opt for A"
      },
      {
        "date": "2024-01-12T08:00:00.000Z",
        "voteCount": 2,
        "content": ". Create a materialized view to aggregate the base table data. Include a filter clause to specify the last one year of partitions."
      },
      {
        "date": "2024-01-11T01:23:00.000Z",
        "voteCount": 2,
        "content": "To preserve the historical data"
      },
      {
        "date": "2024-01-04T05:41:00.000Z",
        "voteCount": 1,
        "content": "B. Create a materialized view to aggregate the base table data. Configure a partition expiration on the base table to retain only the last one year of partitions."
      },
      {
        "date": "2024-01-11T10:14:00.000Z",
        "voteCount": 3,
        "content": "Why not B\n- Configuring partition expiration on the BASE TABLE is a way to manage storage and costs by automatically dropping old data. However, the question specifies the need to retain full historical data, making this approach not suitable since it doesnt keep all historical records."
      },
      {
        "date": "2024-01-11T01:22:00.000Z",
        "voteCount": 4,
        "content": "Don't agree, it is said thad that we need to store the historical data, so answer A is correct"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 305,
    "url": "https://www.examtopics.com/discussions/google/view/129916-exam-professional-data-engineer-topic-1-question-305/",
    "body": "Your organization uses a multi-cloud data storage strategy, storing data in Cloud Storage, and data in Amazon Web Services\u2019 (AWS) S3 storage buckets. All data resides in US regions. You want to query up-to-date data by using BigQuery, regardless of which cloud the data is stored in. You need to allow users to query the tables from BigQuery without giving direct access to the data in the storage buckets. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSetup a BigQuery Omni connection to the AWS S3 bucket data. Create BigLake tables over the Cloud Storage and S3 data and query the data using BigQuery directly.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up a BigQuery Omni connection to the AWS S3 bucket data. Create external tables over the Cloud Storage and S3 data and query the data using BigQuery directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create BigLake tables over the Cloud Storage data and query the data using BigQuery directly.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Storage Transfer Service to copy data from the AWS S3 buckets to Cloud Storage buckets. Create external tables over the Cloud Storage data and query the data using BigQuery directly."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 15,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T05:36:00.000Z",
        "voteCount": 8,
        "content": "- BigQuery Omni: This is an extension of BigQuery that allows you to analyze data across Google Cloud, AWS, and Azure without having to manage the infrastructure or move data across clouds. It's suitable for querying data stored in AWS S3 buckets directly.\n- BigLake: Allows you to create a logical abstraction (table) over data stored in Cloud Storage and S3, so you can query data using BigQuery without moving it.\n- Unified Querying: By setting up BigQuery Omni to connect to AWS S3 and creating BigLake tables over both Cloud Storage and S3 data, you can query all data using BigQuery directly."
      },
      {
        "date": "2024-02-19T02:45:00.000Z",
        "voteCount": 3,
        "content": "I wonder, why BigLake tables (A) over external tables (B)?"
      },
      {
        "date": "2024-08-20T19:02:00.000Z",
        "voteCount": 1,
        "content": "external tables can be created only on data residing in Cloud Storage, BigTable or Google Drive: https://cloud.google.com/bigquery/docs/external-tables. Hence creating external tables WITHOUT BQ Omni is not an option"
      },
      {
        "date": "2024-01-25T05:41:00.000Z",
        "voteCount": 6,
        "content": "Agree. https://cloud.google.com/bigquery/docs/omni-introduction\n\"To run BigQuery analytics on your external data, you first need to connect to Amazon S3 or Blob Storage. If you want to query external data, you would need to create a BigLake table that references Amazon S3 or Blob Storage data.\""
      },
      {
        "date": "2024-02-22T03:41:00.000Z",
        "voteCount": 2,
        "content": "Option A"
      },
      {
        "date": "2024-01-13T09:56:00.000Z",
        "voteCount": 2,
        "content": "Option A - clearly explained in comments"
      },
      {
        "date": "2023-12-30T14:03:00.000Z",
        "voteCount": 3,
        "content": "A - BigLake tables work for S3 and GCS"
      },
      {
        "date": "2023-12-30T14:04:00.000Z",
        "voteCount": 2,
        "content": "https://cloud.google.com/bigquery/docs/external-data-sources#external_data_source_feature_comparison"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 306,
    "url": "https://www.examtopics.com/discussions/google/view/130321-exam-professional-data-engineer-topic-1-question-306/",
    "body": "You are preparing an organization-wide dataset. You need to preprocess customer data stored in a restricted bucket in Cloud Storage. The data will be used to create consumer analyses. You need to comply with data privacy requirements.<br><br>What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse customer-managed encryption keys (CMEK) to directly encrypt the data in Cloud Storage. Use federated queries from BigQuery. Share the encryption key by following the principle of least privilege.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse the Cloud Data Loss Prevention API and Dataflow to detect and remove sensitive fields from the data in Cloud Storage. Write the filtered data in BigQuery.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Dataflow and Cloud KMS to encrypt sensitive fields and write the encrypted data in BigQuery. Share the encryption key by following the principle of least privilege."
    ],
    "answer": "A",
    "answerDescription": "",
    "votes": [
      {
        "answer": "A",
        "count": 18,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-06T05:31:00.000Z",
        "voteCount": 12,
        "content": "- Prioritizes Data Privacy: It protects sensitive information by masking it, reducing the risk of exposure in case of unauthorized access or accidental leaks.\n- Reduces Data Sensitivity: Masking renders sensitive data unusable for attackers, even if they gain access to it.\n- Preserves Data Utility: Masked data can still be used for consumer analyses, as patterns and relationships are often preserved, allowing meaningful insights to be derived."
      },
      {
        "date": "2024-02-11T19:00:00.000Z",
        "voteCount": 2,
        "content": "why not d ?"
      },
      {
        "date": "2024-02-19T03:37:00.000Z",
        "voteCount": 2,
        "content": "Data in Cloud Storage is encrypted by default."
      },
      {
        "date": "2024-06-05T09:04:00.000Z",
        "voteCount": 3,
        "content": "What made me decide on A instead of C was the \"The data will be used to create consumer analyses\" sentence. Having all the PIIs completely redacted from the records, we were unable to distinguish between the individual customers."
      },
      {
        "date": "2024-01-13T09:54:00.000Z",
        "voteCount": 1,
        "content": "Option A, agree with raaad explanation"
      },
      {
        "date": "2024-01-04T05:21:00.000Z",
        "voteCount": 2,
        "content": "A. Use Dataflow and the Cloud Data Loss Prevention API to mask sensitive data. Write the processed data in BigQuery."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 307,
    "url": "https://www.examtopics.com/discussions/google/view/130320-exam-professional-data-engineer-topic-1-question-307/",
    "body": "You need to connect multiple applications with dynamic public IP addresses to a Cloud SQL instance. You configured users with strong passwords and enforced the SSL connection to your Cloud SQL instance. You want to use Cloud SQL public IP and ensure that you have secured connections. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd CIDR 0.0.0.0/0 network to Authorized Network. Use Identity and Access Management (IAM) to add users.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd all application networks to Authorized Network and regularly update them.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLeave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tAdd CIDR 0.0.0.0/0 network to Authorized Network. Use Cloud SQL Auth proxy on all applications."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 12,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-11T10:21:00.000Z",
        "voteCount": 6,
        "content": "- Using the Cloud SQL Auth proxy is a recommended method for secure connections, especially when dealing with dynamic IP addresses. \n- The Auth proxy provides secure access to your Cloud SQL instance without the need for Authorized Networks or managing IP addresses. \n- It works by encapsulating database traffic and forwarding it through a secure tunnel, using Google's IAM for authentication.\n- Leaving the Authorized Networks empty means you're not allowing any direct connections based on IP addresses, relying entirely on the Auth proxy for secure connectivity. This is a secure and flexible solution, especially for applications with dynamic IPs."
      },
      {
        "date": "2024-02-22T03:53:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-01-18T08:31:00.000Z",
        "voteCount": 3,
        "content": "The Cloud SQL Auth Proxy is a Cloud SQL connector that provides secure access to your instances without a need for Authorized networks or for configuring SSL.\nhttps://cloud.google.com/sql/docs/mysql/sql-proxy"
      },
      {
        "date": "2024-01-12T07:57:00.000Z",
        "voteCount": 1,
        "content": "always use Cloud SQL Auth proxy if possible"
      },
      {
        "date": "2024-01-11T05:38:00.000Z",
        "voteCount": 4,
        "content": "https://stackoverflow.com/questions/27759356/how-to-authorize-my-dynamic-ip-network-address-in-google-cloud-sql\nhttps://stackoverflow.com/questions/24749810/how-to-make-a-google-cloud-sql-instance-accessible-for-any-ip-address"
      },
      {
        "date": "2024-02-22T03:52:00.000Z",
        "voteCount": 4,
        "content": "Links also say not to go with option D.\n0.0.0.0/0 which includes all possible IP Addresses is not recommended for security reasons. You have to keep access as restricted as possible."
      },
      {
        "date": "2024-01-11T05:37:00.000Z",
        "voteCount": 1,
        "content": "As for me, after reading documentation, option D looks appropriate"
      },
      {
        "date": "2024-04-12T19:27:00.000Z",
        "voteCount": 1,
        "content": "Save your shxx answer in your dxxb head."
      },
      {
        "date": "2024-01-04T05:13:00.000Z",
        "voteCount": 1,
        "content": "C. Leave the Authorized Network empty. Use Cloud SQL Auth proxy on all applications."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 308,
    "url": "https://www.examtopics.com/discussions/google/view/130319-exam-professional-data-engineer-topic-1-question-308/",
    "body": "You are migrating a large number of files from a public HTTPS endpoint to Cloud Storage. The files are protected from unauthorized access using signed URLs. You created a TSV file that contains the list of object URLs and started a transfer job by using Storage Transfer Service. You notice that the job has run for a long time and eventually failed. Checking the logs of the transfer job reveals that the job was running fine until one point, and then it failed due to HTTP 403 errors on the remaining files. You verified that there were no changes to the source system. You need to fix the problem to resume the migration process. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tSet up Cloud Storage FUSE, and mount the Cloud Storage bucket on a Compute Engine instance. Remove the completed files from the TSV file. Use a shell script to iterate through the TSV file and download the remaining URLs to the FUSE mount point.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRenew the TLS certificate of the HTTPS endpoint. Remove the completed files from the TSV file and rerun the Storage Transfer Service job.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUpdate the file checksums in the TSV file from using MD5 to SHA256. Remove the completed files from the TSV file and rerun the Storage Transfer Service job."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 13,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-05T10:35:00.000Z",
        "voteCount": 6,
        "content": "- It addresses the likely issue: that the signed URLs have expired or are otherwise invalid. By creating a new TSV file with freshly generated signed URLs (with a longer validity period), you're ensuring that the Storage Transfer Service has valid authorization to access the files.\n- Splitting the TSV file and running parallel jobs might help in managing the workload more efficiently and overcoming any limitations related to the number of files or transfer speed."
      },
      {
        "date": "2024-08-04T01:13:00.000Z",
        "voteCount": 2,
        "content": "got this one on the exam, aug 2024, passed"
      },
      {
        "date": "2024-02-10T07:42:00.000Z",
        "voteCount": 4,
        "content": "C. Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel.\n\nHere's why:\n\nHTTP 403 errors: These errors indicate unauthorized access, but since you verified the source system and signed URLs, the issue likely lies with expired signed URLs. Renewing the URLs with a longer validity period prevents this issue for the remaining files.\nSeparate jobs: Splitting the file into smaller chunks and submitting them as separate jobs improves parallelism and potentially speeds up the transfer process.\nAvoid manual intervention: Options A and D require manual intervention and complex setups, which are less efficient and might introduce risks.\nLonger validity: While option B addresses expired URLs, splitting the file offers additional benefits for faster migration."
      },
      {
        "date": "2024-01-13T09:50:00.000Z",
        "voteCount": 3,
        "content": "Option C - agree with Raaad"
      },
      {
        "date": "2024-01-04T05:05:00.000Z",
        "voteCount": 2,
        "content": "C. Create a new TSV file for the remaining files by generating signed URLs with a longer validity period. Split the TSV file into multiple smaller files and submit them as separate Storage Transfer Service jobs in parallel."
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 309,
    "url": "https://www.examtopics.com/discussions/google/view/132182-exam-professional-data-engineer-topic-1-question-309/",
    "body": "You work for an airline and you need to store weather data in a BigQuery table. Weather data will be used as input to a machine learning model. The model only uses the last 30 days of weather data. You want to avoid storing unnecessary data and minimize costs. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table where each record has an ingestion timestamp. Run a scheduled query to delete all the rows with an ingestion timestamp older than 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table partitioned by datetime value of the weather date. Set up partition expiration to 30 days.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table partitioned by ingestion time. Set up partition expiration to 30 days.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a BigQuery table with a datetime column for the day the weather data refers to. Run a scheduled query to delete rows with a datetime value older than 30 days."
    ],
    "answer": "B",
    "answerDescription": "",
    "votes": [
      {
        "answer": "B",
        "count": 10,
        "isMostVoted": true
      },
      {
        "answer": "D",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-08-04T01:14:00.000Z",
        "voteCount": 2,
        "content": "got this one on the exam, aug 2024, passed"
      },
      {
        "date": "2024-03-25T13:23:00.000Z",
        "voteCount": 3,
        "content": "https://cloud.google.com/bigquery/docs/partitioned-tables\nHere it mentions \u201c For TIMESTAMP and DATETIME columns, the partitions can have either hourly, daily, monthly, or yearly granularity.l\nSo you should not calculate the amount of partitions on second granularity"
      },
      {
        "date": "2024-03-08T00:36:00.000Z",
        "voteCount": 1,
        "content": "Skeptical about Option B as maximum partitions in a BQ table is 4000.Since Datetime value is a timestamp it will have more than 4000 values in a duration for 30 days (30*24*60*60 = 259,200 ). So Option D is right imo"
      },
      {
        "date": "2024-03-13T19:52:00.000Z",
        "voteCount": 1,
        "content": "This is a good point"
      },
      {
        "date": "2024-04-15T01:49:00.000Z",
        "voteCount": 4,
        "content": "It's not a good point. The granularity goes to DAYs, not SECONDs. So, the right answer is B."
      },
      {
        "date": "2024-02-22T04:14:00.000Z",
        "voteCount": 1,
        "content": "Option B"
      },
      {
        "date": "2024-02-02T05:23:00.000Z",
        "voteCount": 3,
        "content": "We need the last 30 days, we don't care about ingestion time"
      },
      {
        "date": "2024-01-25T16:30:00.000Z",
        "voteCount": 4,
        "content": "Partitioned based on weather date, with partition expiration set"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 310,
    "url": "https://www.examtopics.com/discussions/google/view/132186-exam-professional-data-engineer-topic-1-question-310/",
    "body": "You need to look at BigQuery data from a specific table multiple times a day. The underlying table you are querying is several petabytes in size, but you want to filter your data and provide simple aggregations to downstream users. You want to run queries faster and get up-to-date insights quicker. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tRun a scheduled query to pull the necessary data at specific intervals dally.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a cached query to accelerate time to results.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tLimit the query columns being pulled in the final result.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a materialized view based off of the query being run.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>"
    ],
    "answer": "D",
    "answerDescription": "",
    "votes": [
      {
        "answer": "D",
        "count": 8,
        "isMostVoted": true
      }
    ],
    "comments": [
      {
        "date": "2024-01-25T16:53:00.000Z",
        "voteCount": 6,
        "content": "Create a materialized view as query source.\nMaterialized views are precomputed views that periodically cache the results of a query for increased performance and efficiency."
      },
      {
        "date": "2024-02-28T12:33:00.000Z",
        "voteCount": 1,
        "content": "Option D. Materialized view"
      },
      {
        "date": "2024-02-02T05:43:00.000Z",
        "voteCount": 1,
        "content": "materialized view"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  },
  {
    "topic": 1,
    "index": 311,
    "url": "https://www.examtopics.com/discussions/google/view/132198-exam-professional-data-engineer-topic-1-question-311/",
    "body": "Your chemical company needs to manually check documentation for customer order. You use a pull subscription in Pub/Sub so that sales agents get details from the order. You must ensure that you do not process orders twice with different sales agents and that you do not add more complexity to this workflow. What should you do?",
    "options": [
      "<span class=\"multi-choice-letter\" data-choice-letter=\"A\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tA.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse a Deduplicate PTransform in Dataflow before sending the messages to the sales agents.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"B\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tB.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a transactional database that monitors the pending messages.",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"C\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tC.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tUse Pub/Sub exactly-once delivery in your pull subscription.\n\t\t\t\t\t\t\t\t\t\t<span class=\"badge badge-success most-voted-answer-badge\" title=\"\" style=\"display: none;\" data-original-title=\"This answer is currently the most voted for in the discussion\">\n                Most Voted\n            </span>",
      "<span class=\"multi-choice-letter\" data-choice-letter=\"D\">\n\t\t\t\t\t\t\t\t\t\t\t\t\tD.\n\t\t\t\t\t\t\t\t\t\t\t\t</span>\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\tCreate a new Pub/Sub push subscription to monitor the orders processed in the agent's system."
    ],
    "answer": "C",
    "answerDescription": "",
    "votes": [
      {
        "answer": "C",
        "count": 11,
        "isMostVoted": true
      },
      {
        "answer": "A",
        "count": 1,
        "isMostVoted": false
      }
    ],
    "comments": [
      {
        "date": "2024-01-29T05:25:00.000Z",
        "voteCount": 5,
        "content": "I remember seeing this in the exam."
      },
      {
        "date": "2024-01-30T16:15:00.000Z",
        "voteCount": 3,
        "content": "how many questions were from here?"
      },
      {
        "date": "2024-08-04T01:15:00.000Z",
        "voteCount": 2,
        "content": "also got this one. about 70%"
      },
      {
        "date": "2024-08-11T22:40:00.000Z",
        "voteCount": 4,
        "content": "Believe me all questions were from Exam topic all were there yesterday in exam. But yes dont go with starting questions mainly focus questions after 200 and latest questions are at last page."
      },
      {
        "date": "2024-07-23T21:57:00.000Z",
        "voteCount": 1,
        "content": "Why not C - Exactly-once delivery in Pub/Sub guarantees that a message is delivered to a subscriber exactly once. However, it doesn't prevent multiple subscribers from processing the same message."
      },
      {
        "date": "2024-02-22T04:32:00.000Z",
        "voteCount": 1,
        "content": "Option C"
      },
      {
        "date": "2024-02-02T05:44:00.000Z",
        "voteCount": 1,
        "content": "C, of course"
      },
      {
        "date": "2024-01-25T21:50:00.000Z",
        "voteCount": 4,
        "content": "Straightforward.\nhttps://cloud.google.com/pubsub/docs/exactly-once-delivery"
      }
    ],
    "examNameCode": "professional-data-engineer",
    "topicNumber": "1"
  }
]